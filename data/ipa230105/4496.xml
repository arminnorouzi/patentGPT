<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004497A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004497</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17872744</doc-number><date>20220725</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0862</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0862</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>602</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SMART PREFETCHING FOR REMOTE MEMORY</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17367078</doc-number><date>20210702</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11442865</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17872744</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>VMware, Inc.</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CALCIU</last-name><first-name>Irina</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>NOWATZYK</last-name><first-name>Andreas</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>AKKAWI</last-name><first-name>Isam Wadih</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>PEDDAMALLU</last-name><first-name>Venkata Subhash Reddy</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>SUBRAHMANYAM</last-name><first-name>Pratap</first-name><address><city>Saratoga</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of prefetching memory pages from remote memory includes detecting that a cache-line access made by a processor executing an application program is an access to a cache line containing page table data of the application program, identifying data pages that are referenced by the page table data, initiating a fetch of a data page, which is one of the identified data pages, and starting a timer. If the fetch completes prior to expiration of the timer, the data page is stored in a local memory. On the other hand, if the fetch does not complete prior to expiration of timer, a presence bit of the data page in the page table data is set to indicate that the data page is not present.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="217.76mm" wi="158.75mm" file="US20230004497A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="245.11mm" wi="171.28mm" file="US20230004497A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="207.26mm" wi="168.99mm" file="US20230004497A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="153.75mm" wi="152.65mm" file="US20230004497A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="193.29mm" wi="137.84mm" orientation="landscape" file="US20230004497A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="215.98mm" wi="150.96mm" orientation="landscape" file="US20230004497A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="231.22mm" wi="145.63mm" file="US20230004497A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="245.19mm" wi="144.02mm" file="US20230004497A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="89.58mm" wi="118.45mm" file="US20230004497A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="172.30mm" wi="106.00mm" file="US20230004497A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="235.29mm" wi="161.12mm" file="US20230004497A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/367,078, filed Jul. 2, 2021, which application is incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Remote memory techniques have been employed to pool memory from multiple hosts connected via a fast network. Even though the network has high bandwidth and low latency, the cost of a remote memory access is still high relative to local memory accesses. To avoid delays in remote memory accesses, memory pages may be prefetched before they are accessed. Various prediction models have been developed to be employed in prefetching memory pages from a remote host, but they have generally been inadequate. What is needed is an improved way of prefetching memory pages from a remote host that is able to better predict what memory locations are to be accessed by application programs before they are actually accessed.</p><p id="p-0004" num="0003">In addition, a fetch from remote memory is subject to certain failures because the fetch occurs over the network connecting the multiple hosts. The network can encounter delays, and the hosts for the remote memory can fail. In these cases, it is desirable to have a mechanism for guarding against these failures.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">A method of prefetching memory pages from remote memory, according to an embodiment, includes detecting that a cache-line access made by a processor executing an application program is an access to a cache line containing page table data of the application program, identifying data pages that are referenced by the page table data, initiating a fetch of a data page, which is one of the identified data pages, and starting a timer. If the fetch completes prior to expiration of the timer, the data page is stored in a local memory. On the other hand, if the fetch does not complete prior to expiration of timer, a presence bit of the data page in the page table data is set to indicate that the data page is not present.</p><p id="p-0006" num="0005">Further embodiments include a device configured to carry out one or more aspects of the above method and a computer system configured to carry out one or more aspects of the above method.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> depicts a block diagram of a computer system that is representative of a virtualized computer architecture in which embodiments may be implemented.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> depicts a block diagram of a computer system that is representative of a non-virtualized computer architecture in which embodiments may be implemented.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a block diagram of a pair of CPU sockets for a CPU and an FPGA, in an embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a plurality of hosts connected to each other for memory pooling.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts the components involved in an operation to fetch pages of memory from a remote host</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a flow of operations for an initializing an application program to prefetch memory pages from remote memory according to embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a flow of operations of a main program for managing the process to prefetch memory pages from remote memory according to embodiments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a flow of operations of a function to prefetch memory pages from remote memory.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a flow of operations of a function to gather statistics about prefetched memory pages.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a flow of operations of a function to clean unused memory pages that have been prefetched.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a flow of operations of a module that listens to activity on a coherence interconnect.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">To improve prefetching from remote memory, one or more embodiments herein allocate page tables of an application to an FPGA-attached memory to enable the FPGA to track all of the CPU accesses to cache lines containing page table data. When a CPU accesses a cache line containing page table data from the FPGA-attached memory, the FPGA uses this information to prefetch data pages referenced by the page table data from the remote memory into a local memory. A timer governs the prefetching to avoid machine check faults if a page does not arrive in time from the remote memory. In addition, the FPGA tracks cache-line accesses to the prefetched data pages and uses this data to prioritize and filter subsequent prefetches of data pages from remote memory and to clean up unused data pages from the local memory.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a block diagram of a computer system that is representative of a virtualized computer architecture in which embodiments may be implemented. As is illustrated, computer system <b>100</b> hosts multiple virtual machines (VMs) <b>118</b><sub>1</sub>-<b>118</b><sub>N </sub>that run on and share a common hardware platform <b>102</b>. Hardware platform <b>102</b> includes conventional computer hardware components, such as one or more items of processing hardware such as central processing units (CPUs) <b>104</b>, random access memory (RAM) <b>106</b> as system memory, one or more network interfaces <b>108</b> for connecting to a network, and one or more host bus adapters (HBA) <b>110</b> for connecting to a storage system. In addition, hardware platform <b>102</b> includes a field-programmable gate array (FPGA) <b>112</b>, which is installed in a CPU socket and communicates with one or more CPUs <b>104</b> via coherence interconnect <b>114</b>. Coherence interconnect <b>114</b> may be any point-to-point coherence interconnect between processors, such as the Intel&#xae; UltraPath Interconnect (UPI), which is a successor to the Intel&#xae; QuickPath Interconnect (QPI), which supports a cache coherence protocol. In another embodiment, coherence interconnect <b>114</b> is a Compute Express Link&#x2032; (OM&#x2032;) operating with CXL-cache/mem protocols. Though the following description has the FPGA occupying a CPU socket, the embodiments are not so limited; any arrangement of the FGPA that includes a connection to the coherence interconnect among processors present in computer system <b>100</b> is sufficient.</p><p id="p-0020" num="0019">A virtualization software layer, referred to hereinafter as hypervisor <b>111</b>, is installed on top of hardware platform <b>102</b>. Hypervisor <b>111</b> makes possible the concurrent instantiation and execution of one or more VMs <b>118</b><sub>1</sub>-<b>118</b><sub>N</sub>. The interaction of a VM <b>118</b> with hypervisor <b>111</b> is facilitated by the virtual machine monitors (VMMs) <b>134</b>. Each VMM <b>134</b><sub>1</sub>-<b>134</b><sub>N </sub>is assigned to and monitors a corresponding VM <b>118</b><sub>1</sub>-<b>118</b><sub>N</sub>. In one embodiment, hypervisor <b>111</b> may be a hypervisor implemented as a commercial product in VMware's vSphere&#xae; virtualization product, available from VMware Inc. of Palo Alto, Calif. In an alternative embodiment, hypervisor <b>111</b> runs on top of a host operating system which itself runs on hardware platform <b>102</b>. In such an embodiment, hypervisor <b>111</b> operates above an abstraction level provided by the host operating system.</p><p id="p-0021" num="0020">After instantiation, each VM <b>118</b><sub>1</sub>-<b>118</b><sub>N </sub>encapsulates a virtual hardware platform that is executed under the control of hypervisor <b>111</b>, in particular the corresponding VMM <b>134</b><sub>1</sub>-<b>134</b><sub>N</sub>. For example, virtual hardware devices of VM <b>118</b><sub>1 </sub>in virtual hardware platform <b>120</b> include one or more virtual CPUs (vCPUs) <b>1221</b>-<b>122</b>N, a virtual random access memory (vRAM) <b>124</b>, a virtual network interface adapter (vNIC) <b>126</b>, and virtual HBA (vHBA) <b>128</b>. Virtual hardware platform <b>120</b> supports the installation of a guest operating system (guest OS) <b>130</b>, on top of which applications <b>132</b> are executed in VM <b>118</b><sub>1</sub>. Examples of guest OS <b>130</b> include any of the well-known commodity operating systems, such as the Microsoft Windows&#xae; operating system, the Linux&#xae; operating system, and the like.</p><p id="p-0022" num="0021">It should be recognized that the various terms, layers, and categorizations used to describe the components in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> may be referred to differently without departing from their functionality or the spirit or scope of the disclosure. For example, VMMs <b>134</b><sub>1</sub>-<b>134</b><sub>N </sub>may be considered separate virtualization components between VMs <b>118</b><sub>1</sub>-<b>118</b><sub>N </sub>and hypervisor <b>111</b> since there exists a separate VMM for each instantiated VM. Alternatively, each VMM may be considered to be a component of its corresponding virtual machine since each VMM includes the hardware emulation components for the virtual machine.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a block diagram of a computer system that is representative of an alternative computer architecture in which embodiments may be implemented. As illustrated, computer system <b>150</b> includes a hardware platform <b>152</b> that is similar to hardware platform <b>102</b>. Hardware platform <b>152</b> includes conventional computer hardware components, such as one or more items of processing hardware such as central processing units (CPUs) <b>154</b>, random access memory (RAM) <b>156</b> as system memory, one or more network interfaces <b>158</b> for connecting to a network, and one or more host bus adapters (HBA) <b>110</b> for connecting to a storage system. In addition, hardware platform <b>152</b> includes an FPGA <b>162</b>, which is installed in a CPU socket and communicates with one or more CPUs <b>154</b> via coherence interconnect <b>164</b>. Coherence interconnect <b>164</b> may be any point-to-point coherence interconnect between processors such as those identified above for coherence interconnect <b>114</b>. Though the following description has the FPGA occupying a CPU socket, the embodiments are not so limited; any arrangement of the FGPA that includes a connection to the coherence interconnect among processors present in computer system <b>150</b> is sufficient. Hardware platform <b>152</b> supports the installation of an operating system <b>186</b>, on top of which applications <b>182</b> are executed in computer system <b>150</b>. Examples of an operating system <b>186</b> include any of the well-known commodity operating systems, such as the Microsoft Windows&#xae; operating system, the Linux&#xae; operating system, and the like.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a block diagram of a pair of CPU sockets, a first socket <b>202</b><i>a </i>for a representative one of CPUs <b>104</b> (or CPUs <b>154</b>), and a second socket <b>202</b><i>b </i>for an FPGA (e.g., FPGA <b>112</b> or FPGA <b>162</b>). The CPU includes one or more processing cores <b>204</b>, caches <b>205</b> for cores <b>204</b> (often implemented as a hierarchy of multiple cache levels), a cache protocol agent <b>209</b> for enforcing the cache coherence protocol, a communications port <b>208</b> connected to a coherence interconnect (e.g., coherence interconnect <b>114</b> or coherence interconnect <b>164</b>), and a memory interface <b>210</b> connected via memory bus <b>232</b> to CPU memory <b>206</b>, which is allocated from RAM <b>106</b> or RAM <b>156</b>. The FPGA is configured with one or more application-specific modules <b>218</b>, a cache protocol agent <b>220</b> for enforcing the cache coherence protocol, a communications port <b>212</b> connected to coherence interconnect <b>114</b>, <b>164</b>, and a memory interface <b>222</b> connected via memory bus <b>236</b> to FPGA memory <b>216</b>, which is allocated from RAM <b>106</b> or RAM <b>156</b>. Other modules <b>214</b>, whose operations are unrelated to application-specific modules <b>218</b>, also may be configured in FPGA <b>112</b>. In one embodiment, each CPU socket, <b>202</b><i>a</i>, <b>202</b><i>b</i>, represents a separate Non-Uniform Memory Access (NUMA) domain.</p><p id="p-0025" num="0024">In the embodiments, page tables <b>240</b> of VMs or applications running in a host that rely on remote memory accesses are moved from CPU memory <b>206</b> to FPGA memory <b>216</b>. Having the page tables in FPGA memory <b>216</b> enables the FPGA to track all of the CPU accesses to cache lines containing page table data and prefetch from remote memory the pages of data referenced by the page table data contained in the accessed cache lines. In addition, the FPGA maintains a list of the accessed data pages, determines access patterns from the list, and formulates future requests to prefetch data pages from remote memory based on the access patterns.</p><p id="p-0026" num="0025">As is well known, caches <b>205</b> are used to reduce the average cost to access data from memory. Data is transferred between CPU memory <b>206</b> and caches <b>205</b> in blocks of fixed size, called cache lines or cache blocks. When a cache line is copied from CPU memory <b>206</b> into caches <b>205</b>, a cache entry is created, which includes both the copied data and the requested memory location (called a tag). When the CPU requests to read or write a location in CPU memory <b>206</b>, caches <b>205</b> first check for a corresponding entry contained therein. That is, caches <b>205</b> search for the contents of the requested memory location in any cache lines that might contain that address. If the CPU finds that the memory location resides in caches <b>205</b>, a cache hit has occurred, and the CPU immediately reads or writes the data in the cache line. However, if the CPU does not find the memory location in caches <b>205</b>, a cache miss has occurred. For a cache miss, caches <b>205</b> allocate a new entry and copy data from CPU memory <b>206</b>. The request is then fulfilled from the contents of caches <b>205</b>.</p><p id="p-0027" num="0026">Communication ports <b>208</b>, <b>212</b>, mentioned above, support a coherence protocol, which is designed to maintain cache coherence in a system with many processors, each having its own cache or caches. With the FPGA residing in one socket <b>202</b><i>b </i>of the CPU sockets and having its own communication port <b>212</b> that supports the coherence protocol, the FPGA can monitor and participate in the coherency protocol that keeps the processor caches coherent.</p><p id="p-0028" num="0027">Cache coherence on the coherence interconnect is maintained according to a standard coherence protocol, such as modified, exclusive, shared, invalid (MESI) protocol or modified, exclusive, shared, invalid, forwarded (MESIF) protocol. In these protocols, cache lines marked invalid signify that the cache line has invalid data, and fresh data must be brought into caches <b>205</b> from CPU memory <b>206</b>. Cache lines marked exclusive, shared, and forwarded (in the MESIF protocol) all signify that the cache line has valid data, but the cache line is clean (not modified), so the cache line can be discarded from the cache without writing data of the cache line back to CPU memory <b>206</b>. A cache line marked as modified signifies the cache line is modified or dirty, and data of the cache line must be written back to CPU memory <b>206</b> before the cache line is discarded from caches <b>205</b>.</p><p id="p-0029" num="0028">Each cache protocol agent can initiate and respond to transactions on the coherence interconnect by sending and receiving messages on the coherence interconnect. In the embodiments illustrated herein, cache protocol agent <b>209</b> cooperates with cache protocol agent <b>220</b> by sending messages, including broadcast messages, over the coherence interconnect. In the protocol, one of the cache protocol agents is an owner of a set of cache lines and contains information regarding those cache lines. The other cache protocol agents send messages to the owner agent requesting a cache line or to find the status of the cache line owned by the owner agent. The owner agent may service the request directly or request that another cache protocol agent satisfy the request.</p><p id="p-0030" num="0029">When the CPU accesses a cache line that is not in its caches <b>205</b>, at any level of the cache hierarchy, it is cache protocol agent <b>209</b> of the CPU that requests the cache line from CPU memory <b>206</b>. Thus, cache protocol agent <b>209</b> in CPU <b>104</b> issues a load cache line transaction on the coherence interconnect. The transaction can be &#x2018;Load Shared&#x2019; for sharing the cache line or &#x2018;Load Exclusive&#x2019; for cache lines that will be modified. A cache line that is loaded as &#x2018;Shared&#x2019; means that the line probably will not be modified. In contrast, a cache line that is loaded as &#x2018;Exclusive&#x2019; is considered potentially dirty because it is not certain the cache line will be modified. When a cache line gets evicted from caches <b>205</b> to CPU memory <b>206</b>, if it is modified, it must be written back to CPU memory <b>206</b> from which it originated. The operation of writing the cache line is performed on the coherence interconnect as a write-back transaction and can be monitored for tracking dirty cache lines. In the case of a write-back transaction, the cache line is actually dirty rather than potentially dirty. In the description that follows, a writeback transaction is converted to and handled as a message, &#x2018;WB_Data_CL.&#x2019;</p><p id="p-0031" num="0030">To confirm whether a cache line is dirty or not, a cache protocol agent, such as cache protocol agent <b>220</b> in the FPGA, can snoop the cache line in accordance with the coherence interconnect protocol. If the cache line is dirty, the snoop triggers a write-back transaction, thereby exposing the dirty cache line that was residing in the processor cache. Cache protocol agents <b>209</b> and <b>220</b> also have information regarding the cache lines that are resident in the processor caches. This information is accessible via the coherence interconnect.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a plurality of hosts <b>302</b><sub>1-N </sub>that are running a VM <b>118</b> as in computer system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> or an application <b>182</b> as in computer system <b>150</b> of <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. Each host <b>302</b> includes system memory (e.g., RAM <b>106</b> or <b>156</b>) and is interconnected to the other hosts by a network <b>308</b>, which can be a local area network or a wide area network. For purposes of illustration, it is assumed that the system memory of the plurality of hosts <b>302</b><sub>1-N </sub>is pooled for sharing, and a VM or an application running in host <b>302</b><sub>2 </sub>depends on pages of memory that are not present on host <b>302</b><sub>2 </sub>but contained in one or more other hosts <b>302</b><sub>1 </sub>and <b>302</b><sub>3-N</sub>, each of which is hereinafter referred to as a remote host.</p><p id="p-0033" num="0032">In the embodiments described below, the VM or the application that relies on pages of memory contained in a remote host is referred to as an application program. The host that is running the application program is referred to as a local host, and the hypervisor or the operating system running in the local host is referred to as system software.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts the components involved in an operation to fetch pages of memory from a remote host. The components include a main program <b>402</b> and several functions, which include an initialize function <b>404</b> (described in reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>), a Prefetch Pages function <b>406</b> (described in reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>), a Gather Statistics (GatherStats) function <b>410</b> (described in reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>), a Clean Unused function <b>412</b> (described in reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>), a dataCL module <b>408</b>, a buffer <b>414</b>, and a timer <b>416</b>. Timer <b>416</b> is set with a timer period during which a fetch of a page is expected to complete and to have a value less than a timeout interval of the coherence interconnect. In one embodiment, the timeout interval is determined from empirically observations of cache-line accesses on the coherence interconnect. Without the timer, if there is a delay in fetching of the page, the cache coherence interconnect and cache protocol agents may time out, which would cause a machine check exception (MCE). The MCE would then be handled by the hypervisor, but handling the MCE is costly and may shut down the host. On the other hand, with timer <b>416</b>, if a fetch of a page does not complete within the time period of the timer, the presence bit for the page is set to indicate that the page is not present. If the CPU tries to access the missing page, an exit to the hypervisor occurs, and the hypervisor handles the missing page as a normal page fault.</p><p id="p-0035" num="0034">DataCL module <b>408</b>, further described in reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, listens to activity on the coherence interconnect, such activity including a &#x2018;Load_Data_CL&#x2019; event and a &#x2018;WB_Data_CL&#x2019; event. The Load_Data_CL&#x2032; event indicates that a cache line is loaded into a CPU cache and the &#x2018;WB_Data_CL&#x2019; event indicates that a cache line of the CPU cache is written back to system memory. DataCL module <b>408</b> may also receive a trackCL(page) command, which tells dataCL module <b>408</b> to track any cache line, loaded or written back, in the specified page. Any cache line that is tracked is added to buffer <b>414</b> so that any patterns among the tracked cache lines can be detected. In one embodiment, dataCL module <b>408</b> is one of the application-specific modules <b>218</b> configured in the FPGA, and buffer <b>414</b> is configured in FPGA memory <b>216</b>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a flow of operations for an initialize function, which is executed by system software running in the local host when an application program is allocated remote memory. The initialize function in step <b>502</b> moves all of the page tables of the application program from CPU memory <b>206</b> to FPGA memory <b>216</b>. As a result, when the CPU accesses a cache line from page table data, the cache-line access appears as a &#x2018;Load_Data_CL&#x2019; event on the coherence interconnect, and dataCL module <b>408</b> is able to recognize that the cache line is for page table data. In step <b>504</b>, the initialize function sends a reset command to clear buffer <b>414</b>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a flow of operations of a main program, which is executed as one of application-specific modules <b>218</b> running in the local host for managing the prefetching process. The main program awaits the receipt of a cache line containing page table data from dataCL module <b>408</b> in step <b>602</b>. Upon receipt, the main program converts the page table data to a set of data pages referenced by the page table data in step <b>604</b>. In step <b>606</b>, the main program calls the GatherStats function to acquire usage data on the data pages of the application program. Based on the usage data, which in the embodiments illustrated herein is the number of cache-line accesses per data page, the main program in step <b>607</b> prioritizes the data pages to prefetch. In one embodiment, the main program assigns a higher fetching priority to data pages with a higher number of cache-line accesses. During the initial stages of executing the application program, step <b>607</b> may be skipped and replaced with a prioritization scheme based on whether or not the accessed bit for any of the data pages is set. The main program assigns a higher priority to the data pages having the accessed bit set and a lower priority to the data pages that do not have the accessed bit set. In step <b>608</b>, the main program calls the Prefetch function with the set of pages to be fetched that are prioritized as described above.</p><p id="p-0038" num="0037">While waiting for the receipt of a cache line containing page table data from dataCL module <b>408</b> in step <b>602</b>, the main program determines in step <b>610</b> whether or not the condition for cleaning prefetched data pages that are unused is satisfied. The condition may be an expiration of a time interval that is set upon initialization of the application program, and each time the Clean Unused function is called in step <b>612</b> to remove prefetched data pages that are unused.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a flow of operations of the Prefetch function, which is executed as one of application-specific modules <b>218</b> running in the local host. The Prefetch function is called by the main program with a set of data pages that are ordered based on their priority. The Prefetch function iterates in step <b>702</b> over each data page in the set. In step <b>704</b>, the Prefetch function determines if the data page has been fetched into local memory already. If not, the Prefetch function starts a timer in step <b>706</b> and initiates the fetch of the data page in step <b>708</b>. In steps <b>710</b> and <b>712</b>, the Prefetch function determines whether the fetch completes while the timer is still running. If so (step <b>710</b>: Yes; step <b>712</b>: Yes), the Prefetch function adds the fetched data page to the local memory in step <b>714</b>. In step <b>716</b>, the Prefetch function sends a TrackCL(page) command to the dataCL module to cause cache lines of the fetched data page to be tracked. In step <b>718</b>, the Prefetch function cancels the timer (if it is still running) and steps to the next page in the iteration in step <b>702</b>.</p><p id="p-0040" num="0039">Continuing with <figref idref="DRAWINGS">FIG. <b>7</b></figref>, if the timer expires and the fetch has not completed (step <b>710</b>: No; step <b>712</b>: No), the Prefetch function sets the presence bit for the page to False in step <b>720</b> and terminates the fetch from remote memory to prevent a machine check error. The Prefetch function then steps to the next page in the set of pages in step <b>702</b>.</p><p id="p-0041" num="0040">In one embodiment, the Prefetch function may be multi-threaded so that the Prefetch function can run concurrently for each page in the set of pages. Concurrent running of the function allows concurrent fetching from multiple remote hosts to obtain the required data pages.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a flow of operations of the GatherStats function, which is executed as one of application-specific modules <b>218</b> running in the local host. In step <b>802</b>, the GatherStats function retrieves a set of tracked cache lines from buffer <b>414</b>. In step <b>804</b>, the GatherStats function counts the number of cache-line accesses for each data page that is being tracked by dataCL module <b>408</b>. In step <b>806</b>, the GatherStats function returns the counts for each data page as usage data to the caller.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a flow of operations of the Clean Unused function, which is executed as one of application-specific modules <b>218</b> running in the local host. In step <b>902</b>, the Clean Unused calls the GatherStats function to acquire usage data on the data pages of the application program. The GatherStats function iterates in step <b>903</b> over each prefetched data page. If the prefetched data page has had zero cache-line accesses (step <b>904</b>; Yes), the data page is removed from FPGA memory <b>216</b> of the local host in step <b>906</b>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a flow of operations of the dataCL module, which is one of application-specific modules <b>218</b> running in the local host. The operations begin in step <b>1002</b>, when dataCL module <b>408</b> receives a message on the coherence interconnect or from one of the other functions. Then, in step <b>1004</b>, dataCL module <b>408</b> matches the message to one of two events on the coherence interconnect or to a message sent from other functions.</p><p id="p-0045" num="0044">If the message is a Load_Data_CL event, step <b>1005</b> is executed. In step <b>1005</b>, the dataCL module tests whether the cache line being accessed is a cache line is page table data, which is stored in page tables <b>240</b> that have been moved into FPGA memory <b>216</b>. If so (step <b>1005</b>; Yes), it returns the cache line to main program <b>402</b> in step <b>1008</b>. Otherwise (step <b>1005</b>; No), it checks to see if it is the cache line of a data page that is being tracked (step <b>1010</b>). If the cache line is of a data page that is being tracked (step <b>1010</b>; Yes), the dataCL module adds the cache line to buffer <b>414</b> in step <b>1012</b> and returns to step <b>1002</b> to wait for the next message. If not (step <b>1010</b>; No), step <b>1012</b> is skipped, and the dataCL module returns to step <b>1002</b> to wait for the next message.</p><p id="p-0046" num="0045">If the message is a WB_Data_CL event, the dataCL module proceeds to step <b>1010</b> directly and executes step <b>1010</b> to see if it the cache line of a data page that is being tracked. As described above, if the cache line is of a data page that is being tracked (step <b>1010</b>; Yes), the dataCL module adds the cache line to buffer <b>414</b> in step <b>1012</b> and returns to step <b>1002</b> to wait for the next message. If not (step <b>1010</b>; No), step <b>1012</b> is skipped, and the dataCL module returns to step <b>1002</b> to wait for the next message.</p><p id="p-0047" num="0046">If the message is a trackCL command, dataCL module <b>408</b> executes step <b>1014</b> to record the data page identified in the trackCL command as a data page for which cache-line accesses are to be tracked by dataCL module <b>408</b>. After step <b>1014</b>, dataCL module <b>408</b> returns to step <b>1002</b> to wait for the next message.</p><p id="p-0048" num="0047">If the message is a reset command, then dataCL module <b>408</b> clears buffer <b>414</b> in step <b>1016</b>. After step <b>1016</b>, dataCL module <b>408</b> returns to step <b>1002</b> to wait for the next message.</p><p id="p-0049" num="0048">Certain embodiments as described above involve a hardware abstraction layer on top of a host computer. The hardware abstraction layer allows multiple contexts to share the hardware resource. These contexts are isolated from each other in one embodiment, each having at least a user application program running therein. The hardware abstraction layer thus provides benefits of resource isolation and allocation among the contexts. In the foregoing embodiments, virtual machines are used as an example for the contexts and hypervisors as an example for the hardware abstraction layer. As described above, each virtual machine includes a guest operating system in which at least one application program runs. It should be noted that these embodiments may also apply to other examples of contexts, such as containers not including a guest operating system, referred to herein as &#x201c;OS-less containers&#x201d; (see, e.g., www.docker.com). OS-less containers implement operating system-level virtualization, wherein an abstraction layer is provided on top of the kernel of an operating system on a host computer. The abstraction layer supports multiple OS-less containers, each including an application program and its dependencies. Each OS-less container runs as an isolated process in user space on the host operating system and shares the kernel with other containers. The OS-less container relies on the kernel's functionality to make use of resource isolation (CPU, memory, block 1/0, network, etc.) and separate namespaces and to completely isolate the application program's view of the operating environments. By using OS-less containers, resources can be isolated, services restricted, and processes provisioned to have a private view of the operating system with their own process ID space, file system structure, and network interfaces. Multiple containers can share the same kernel, but each container can be constrained only to use a defined amount of resources such as CPU, memory, and 1/0.</p><p id="p-0050" num="0049">Certain embodiments may be implemented in a host computer without a hardware abstraction layer or an OS-less container. For example, certain embodiments may be implemented in a host computer running a Linux&#xae; or Windows&#xae; operating system.</p><p id="p-0051" num="0050">The various embodiments described herein may be practiced with other computer system configurations, including hand-held devices, microprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, and the like.</p><p id="p-0052" num="0051">One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in one or more computer-readable media. The term computer-readable medium refers to any data storage device that can store data which can thereafter be input to a computer system. Computer-readable media may be based on any existing or subsequently developed technology for embodying computer programs in a manner that enables them to be read by a computer. Examples of a computer-readable medium include a hard drive, network-attached storage (NAS), read-only memory, random-access memory (e.g., a flash memory device), a CD (Compact Discs)&#x2014;CD-ROM, a CDR, or a CD-RW, a DVD (Digital Versatile Disc), a magnetic tape, and other optical and non-optical data storage devices. The computer-readable medium can also be distributed over a network-coupled computer system so that the computer-readable code is stored and executed in a distributed fashion.</p><p id="p-0053" num="0052">Although one or more embodiments of the present invention have been described in some detail for clarity of understanding, it will be apparent that certain changes and modifications may be made within the scope of the claims. Accordingly, the described embodiments are to be considered as illustrative and not restrictive, and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims, elements and/or steps do not imply any particular order of operation unless explicitly stated in the claims.</p><p id="p-0054" num="0053">Plural instances may be provided for components, operations, or structures described herein as a single instance. Finally, boundaries between various components, operations, and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention(s). In general, structures and functionality presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionality presented as a single component may be implemented as separate components. These and other variations, modifications, additions, and improvements may fall within the scope of the appended claim(s).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer system for prefetching pages from a remote memory, the system comprising:<claim-text>a processor installed in a first CPU socket and executing an application program;</claim-text><claim-text>a local memory; and</claim-text><claim-text>a field programmable gate array (FPGA) installed in a second CPU socket and connected to the processor via a coherence interconnect, wherein the FPGA is configured to:<claim-text>detect on the coherence interconnect that a cache-line access made by a processor executing an application program is an access to a cache line containing page table data of the application program;</claim-text><claim-text>identify data pages that are referenced by the page table data;</claim-text><claim-text>initiate a fetch of a data page, which is one of the identified data pages; and</claim-text><claim-text>store the fetched data page in the local memory.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the FPGA is further configured to:<claim-text>assign priorities to the identified data pages, wherein a fetch of the identified data pages are to be initiated in the order of the assigned priorities.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the FPGA is further configured to:<claim-text>track cache-line accesses to the identified data pages, wherein the priorities to the identified data pages are assigned based on the number of cache-line accesses.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer system of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the identified data pages having a higher number of cache-line accesses relative to the other identified data pages are assigned a higher priority than the other identified data pages.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the FPGA is further configured to:<claim-text>track cache-line accesses to the identified data pages that have been fetched and stored in the local memory;</claim-text><claim-text>determine that some of the identified data pages are unused based on the cache-line accesses; and</claim-text><claim-text>remove the identified data pages that are determined to be unused from the local memory.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer system of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the local memory includes a first local memory of the processor and a second local memory of the FPGA and the identified data pages are removed from the second local memory.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the coherence interconnect has a timeout interval, and</claim-text><claim-text>the FPGA is further configured to store in the local memory additional data pages that are fetched within the timeout interval of the coherence interconnect.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method of prefetching pages from a remote memory for a processor installed in a first CPU socket, by a field programmable gate array (FPGA) installed in a second CPU socket and connected to the processor via a coherence interconnect, the method comprising:<claim-text>detecting on the coherence interconnect that a cache-line access made by the processor executing an application program is an access to a cache line containing page table data of the application program;</claim-text><claim-text>identifying data pages that are referenced by the page table data;</claim-text><claim-text>initiating a fetch of a data page, which is one of the identified data pages; and</claim-text><claim-text>storing the fetched data page in a local memory.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>assigning priorities to the identified data pages, wherein a fetch of the identified data pages are to be initiated in the order of the assigned priorities.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>tracking cache-line accesses to the identified data pages, wherein the priorities to the identified data pages are assigned based on the number of cache-line accesses.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the identified data pages having a higher number of cache-line accesses relative to the other identified data pages are assigned a higher priority than the other identified data pages.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising:<claim-text>tracking cache-line accesses to the identified data pages that have been fetched and stored in the local memory;</claim-text><claim-text>determining that some of the identified data pages are unused based on the cache-line accesses; and</claim-text><claim-text>removing the identified data pages that are determined to be unused from the local memory.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the local memory includes a first local memory of the processor and a second local memory of the FPGA and the identified data pages are removed from the second local memory.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the coherence interconnect has a timeout interval, and</claim-text><claim-text>additional data pages that are fetched within the timeout interval of the coherence interconnect are stored in the local memory.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable medium comprising instructions to be executed in a field programmable gate array (FPGA) that is connected to a processor executing an application program, wherein the processor is installed in a first CPU socket and the FPGA is installed in a second CPU socket and connected to the processor via a coherence interconnect, and the instructions, when executed in the FPGA, causes the FPGA to carry out a method of prefetching pages from a remote memory, the method comprising:<claim-text>detecting on the coherence interconnect that a cache-line access made by the processor executing an application program is an access to a cache line containing page table data of the application program;</claim-text><claim-text>identifying data pages that are referenced by the page table data;</claim-text><claim-text>initiating a fetch of a data page, which is one of the identified data pages; and</claim-text><claim-text>storing the fetched data page in a local memory.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the method further comprising:<claim-text>assigning priorities to the identified data pages, wherein a fetch of the identified data pages are to be initiated in the order of the assigned priorities.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, the method further comprising:<claim-text>tracking cache-line accesses to the identified data pages, wherein the priorities to the identified data pages are assigned based on the number of cache-line accesses.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the identified data pages having a higher number of cache-line accesses relative to the other identified data pages are assigned a higher priority than the other identified data pages.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the method further comprising:<claim-text>tracking cache-line accesses to the identified data pages that have been fetched and stored in the local memory;</claim-text><claim-text>determining that some of the identified data pages are unused based on the cache-line accesses; and</claim-text><claim-text>removing the identified data pages that are determined to be unused from the local memory.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein<claim-text>the coherence interconnect has a timeout interval, and</claim-text><claim-text>additional data pages that are fetched within the timeout interval of the coherence interconnect are stored in the local memory.</claim-text></claim-text></claim></claims></us-patent-application>