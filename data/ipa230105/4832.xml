<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004833A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004833</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364732</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Methods, Systems, And Apparatuses For Model Selection And Content Recommendations</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>ON24, Inc.</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Sahasi</last-name><first-name>Jayesh</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Diaz</last-name><first-name>Jairo</first-name><address><city>Barranquilla</city><country>CO</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Streit</last-name><first-name>Brian</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and apparatuses for improved model selection and content recommendations are described herein. A distribution platform may comprise a system of computing devices, servers, software, etc., that is configured to present media assets (e.g., content) at user devices. In one example embodiment, an analytics subsystem may provide at least one content recommendation to a user device using a machine learning model. The machine learning model may be selected based on a clustering method using an unsupervised machine learning model.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.92mm" wi="158.75mm" file="US20230004833A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="244.09mm" wi="155.96mm" orientation="landscape" file="US20230004833A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="226.48mm" wi="151.72mm" file="US20230004833A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="182.37mm" wi="162.22mm" file="US20230004833A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="194.23mm" wi="128.86mm" orientation="landscape" file="US20230004833A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="182.12mm" wi="87.55mm" file="US20230004833A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="244.09mm" wi="155.96mm" orientation="landscape" file="US20230004833A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="241.98mm" wi="148.84mm" file="US20230004833A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="99.48mm" wi="124.54mm" file="US20230004833A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="184.91mm" wi="151.81mm" file="US20230004833A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="115.49mm" wi="116.25mm" file="US20230004833A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="118.45mm" wi="116.25mm" file="US20230004833A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="185.00mm" wi="128.52mm" file="US20230004833A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="232.58mm" wi="125.39mm" file="US20230004833A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="210.57mm" wi="144.78mm" file="US20230004833A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="229.79mm" wi="161.04mm" file="US20230004833A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="222.17mm" wi="143.17mm" orientation="landscape" file="US20230004833A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="239.95mm" wi="135.21mm" file="US20230004833A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="246.30mm" wi="117.26mm" orientation="landscape" file="US20230004833A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="150.96mm" wi="147.57mm" file="US20230004833A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="180.42mm" wi="147.57mm" file="US20230004833A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="181.19mm" wi="147.57mm" file="US20230004833A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Content platforms may provide a variety of content for users, such as for example, videos, slideshows, reading material, audio presentations, webinars, etc. When a plethora of content is available for users to consume, content platforms may use recommendation systems when providing content recommendations to their users. Such existing recommendation systems may provide content recommendations based on user surveys, viewership/participant statistics, etc. However, these existing recommendation systems do not adequately match content to users. These and other considerations are discussed herein.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">It is to be understood that both the following general description and the following detailed description are exemplary and explanatory only and are not restrictive. Methods, systems, and apparatuses for improved model selection and content recommendations are described herein. A distribution platform may comprise a system of computing devices, servers, software, etc., that is configured to present media assets (e.g., content) at user devices.</p><p id="p-0004" num="0003">An analytics subsystem of the distribution platform may determine at least one content recommendation associated with a first client identifier. The analytics subsystem may determine the at least one content recommendation using a machine learning model. The machine learning model may be selected based on a clustering method that considers similar groups of media assets and/or user activity data associated with a plurality of client identifiers. The first client identifier may be one of the plurality of client identifiers. The selected machine learning model may also be selected based on a recommendation type associated with the at least one content recommendation. The analytics subsystem may cause a user device to output a media asset associated with the at least one content recommendation. For example, the analytics subsystem may cause a client application to output (e.g., present, display, show, etc.) the media asset associated with the at least one content recommendation.</p><p id="p-0005" num="0004">Additional advantages will be set forth in part in the description which follows or may be learned by practice. The advantages will be realized and attained by means of the elements and combinations particularly pointed out in the appended claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005">The accompanying drawings, which are incorporated in and constitute a part of the present description serve to explain the principles of the methods, systems, and apparatuses described herein:</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of an operational environment that includes a presentation platform for presentation of digital content, in accordance with one or more embodiments of this disclosure;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example of an analytics subsystem included in a presentation platform for presentation of digital content, in accordance with one or more embodiments of this disclosure;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates an example of a storage subsystem included in a presentation platform for presentation of digital content, in accordance with one or more embodiments of this disclosure;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates an example visual representation of a user interest cloud, in accordance with one or more embodiments of this disclosure;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a user interface (UI) that presents various types of engagement data for a user device, in accordance with one or more embodiments of this disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically depicts engagement scores for example functionality features available per digital experience (or media asset), for a particular end-user, in accordance with one or more embodiments of this disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of an operational environment that includes integration with third-party subsystems, in accordance with one or more embodiments of this disclosure:</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates another example of an operational environment for integration with a third-party subsystem, in accordance with one or more embodiments of this disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates example components of an integration subsystem, in accordance with one or more embodiments of this disclosure:</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example of a UI representing a landing page for configuration of aspects of a digital experience, in accordance with one or more embodiments of this disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of a subsystem for configuration of aspects of a digital experience, in accordance with one or more embodiments of this disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates a schematic example of a layout template for presentation of a media asset and directed content, in accordance with one or more embodiments of this disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b>I</figref> illustrates another schematic example of a layout template for presentation of a media asset and directed content, in accordance with one or more embodiments of this disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of a personalization subsystem in a presentation platform for presentation of digital content, in accordance with one or more embodiments of this disclosure:</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> illustrates example components of a content management subsystem, in accordance with one or more embodiments of this disclosure;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrates an example of a digital experience, in accordance with one or more embodiments of this disclosure;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b>C</figref> illustrates another example of a digital experience, in accordance with one or more embodiments of this disclosure;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> illustrates a virtual environment module, in accordance with one or more embodiments of this disclosure;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> illustrates an example of an interactive virtual environment, in accordance with one or more embodiments of this disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. <b>15</b>A and <b>15</b>B</figref> illustrate an example systems, in accordance with one or more embodiments of this disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example system, in accordance with one or more embodiments of this disclosure:</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates a flowchart for an example method, in accordance with one or more embodiments of this disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an example system, in accordance with one or more embodiments of this disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates a flowchart for an example method, in accordance with one or more embodiments of this disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates a flowchart for an example method, in accordance with one or more embodiments of this disclosure; and</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates a flowchart for an example method, in accordance with one or more embodiments of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0033" num="0032">As used in the specification and the appended claims, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d;, and &#x201c;the&#x201d; include plural referents unless the context clearly dictates otherwise. Ranges may be expressed herein as from &#x201c;about&#x201d; one particular value, and/or to &#x201c;about&#x201d; another particular value. When such a range is expressed, another configuration includes from the one particular value and/or to the other particular value. Similarly, when values are expressed as approximations, by use of the antecedent &#x201c;about&#x201d;, it will be understood that the particular value forms another configuration. It will be further understood that the endpoints of each of the ranges are significant both in relation to the other endpoint, and independently of the other endpoint.</p><p id="p-0034" num="0033">&#x201c;Optional&#x201d; or &#x201c;optionally&#x201d; means that the subsequently described event or circumstance may or may not occur, and that the description includes cases where said event or circumstance occurs and cases where it does not.</p><p id="p-0035" num="0034">Throughout the description and claims of this specification, the word &#x201c;comprise&#x201d; and variations of the word, such as &#x201c;comprising&#x201d; and &#x201c;comprises&#x201d;, means &#x201c;including but not limited to&#x201d;, and is not intended to exclude, for example, other components, integers or steps. &#x201c;Exemplary&#x201d; means &#x201c;an example of&#x201d; and is not intended to convey an indication of a preferred or ideal configuration. &#x201c;Such as&#x201d; is not used in a restrictive sense, but for explanatory purposes.</p><p id="p-0036" num="0035">It is understood that when combinations, subsets, interactions, groups, etc. of components are described that, while specific reference of each various individual and collective combinations and permutations of these may not be explicitly described, each is specifically contemplated and described herein. This applies to all parts of this application including, but not limited to, steps in described methods. Thus, if there are a variety of additional steps that may be performed it is understood that each of these additional steps may be performed with any specific configuration or combination of configurations of the described methods.</p><p id="p-0037" num="0036">As will be appreciated by one skilled in the art, hardware, software, or a combination of software and hardware may be implemented. Furthermore, a computer program product on a computer-readable storage medium (e.g., non-transitory) having processor-executable instructions (e.g., computer software) embodied in the storage medium. Any suitable computer-readable storage medium may be utilized including hard disks, CD-ROMs, optical storage devices, magnetic storage devices, memristors, Non-Volatile Random-Access Memory (NVRAM), flash memory, or a combination thereof.</p><p id="p-0038" num="0037">Throughout this application reference is made to block diagrams and flowcharts. It will be understood that each block of the block diagrams and flowcharts, and combinations of blocks in the block diagrams and flowcharts, respectively, may be implemented by processor-executable instructions. These processor-executable instructions may be loaded onto a general-purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the processor-executable instructions which execute on the computer or other programmable data processing apparatus create a device for implementing the functions specified in the flowchart block or blocks.</p><p id="p-0039" num="0038">These processor-executable instructions may also be stored in a computer-readable memory that may direct a computer or other programmable data processing apparatus to function in a particular manner, such that the processor-executable instructions stored in the computer-readable memory produce an article of manufacture including processor-executable instructions for implementing the function specified in the flowchart block or blocks. The processor-executable instructions may also be loaded onto a computer or other programmable data processing apparatus to cause a series of operational steps to be performed on the computer or other programmable apparatus to produce a computer-implemented process such that the processor-executable instructions that execute on the computer or other programmable apparatus provide steps for implementing the functions specified in the flowchart block or blocks.</p><p id="p-0040" num="0039">Blocks of the block diagrams and flowcharts support combinations of devices for performing the specified functions, combinations of steps for performing the specified functions and program instruction means for performing the specified functions. It will also be understood that each block of the block diagrams and flowcharts, and combinations of blocks in the block diagrams and flowcharts, may be implemented by special purpose hardware-based computer systems that perform the specified functions or steps, or combinations of special purpose hardware and computer instructions.</p><p id="p-0041" num="0040">Methods, systems, and apparatuses for improved model selection and content recommendations are described herein. A distribution platform may comprise a system of computing devices, servers, software, etc., that is configured to present media assets (e.g., content) at user devices. In one example embodiment, an analytics subsystem may provide at least one content recommendation to a user device.</p><p id="p-0042" num="0041">The analytics subsystem may receive activity data indicative of a plurality of engagements of the user device with a plurality of media assets. The analytics subsystem may receive the activity data via a client application executing on the user device. The analytics subsystem may generate a user interest cloud associated with the user device.</p><p id="p-0043" num="0042">The analytics subsystem may determine at least one content recommendation based on the user interest cloud and the activity data. For example, the analytics subsystem may determine the at least one content recommendation using a first machine learning model.</p><p id="p-0044" num="0043">The first machine learning model trained using historical user activity data. The first machine learning model may not be client-specific. For example, the a plurality of machine learning models may be associated with associated with a plurality of client identifiers. The analytics subsystem may select the first machine learning model from the plurality of machine learning models based on clusters of clients with similar characteristics.</p><p id="p-0045" num="0044">Additionally. or in the alternative, the analytics subsystem may select the first machine learning model based on a particular use case. For example, the analytics subsystem may select the first machine learning model from the plurality of machine learning models based on a recommendation type associated with the at least one content recommendation. The recommendation type may comprise, for example, a media asset recommendation, an engagement recommendation, a combination thereof, and/or the like.</p><p id="p-0046" num="0045">The analytics subsystem may cause the user device to output the at least one content recommendation. For example, the analytics subsystem may cause the client application to output (e.g., present, display, show, etc.) a media asset associated with the at least one content recommendation. Other examples are possible as well, as described herein.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of an operational environment <b>100</b> that includes a presentation platform for presentation of digital content, in accordance with one or more embodiments of this disclosure. The presentation platform can include backend platform devices <b>130</b> and, in some cases, distribution platform devices <b>160</b>. In other cases, the distribution platform devices <b>160</b> can pertain to a third-party provider. Regardless of its type, the backend platform devices <b>130</b> and the distribution platform devices <b>160</b> can be functionally coupled by a network architecture <b>155</b>. The network architecture <b>155</b> can include one or a combination of networks (wireless or wireline) that permit one-way and/or two-way communication of data and/or signaling. The digital content can include, for example, 2D content, 3D content, or 4D content or another type of immersive content. Besides digital content that is static and, thus, can be consumed in time-shifted fashion, digital content that can be created and consumed contemporaneously also is contemplated.</p><p id="p-0048" num="0047">The digital content can be consumed by a user device of a group of user devices <b>102</b>. The user device can consume the content as part of a presentation that is individual or as part of a presentation involving multiple parties. Regardless of its type a presentation can take place within a session to consume content. Such a session can include, for example, a call session, videoconference, a downstream lecture (a seminar, a class, a tutorial, or the like, for example).</p><p id="p-0049" num="0048">The group of user devices <b>102</b> can include various types of user devices, each having a particular amount of computing resources (e.g., processing resources, memory resources, networking resources, and I/O elements) to consume digital content via a presentation. In some cases, the group of user devices <b>102</b> can be homogeneous, including devices of a particular type, such as high-end to medium-end mobile devices, IoT devices <b>120</b>, or wearable devices <b>122</b>. A mobile device can be embodied in, for example, a handheld portable device <b>112</b> (e.g., a smartphone, a tablet, or a gaming console); a non-handheld portable device <b>118</b> (e.g., a laptop); a tethered device <b>116</b> (such as a personal computer); or an automobile <b>114</b> having an in-car infotainment system (IVS) having wireless connectivity. A wearable device can be embodied in goggles (such as augmented-reality (AR) goggles) or a helmet mounted display device, for example. An IoT device can include an appliance having wireline connectivity and/or wireless connectivity. In other cases, the group of user device <b>102</b> can be heterogeneous, including devices of a various types, such as a combination of high-end to medium-end mobile devices, wearable devices, and IoT devices.</p><p id="p-0050" num="0049">To consume digital content, a user device of the group of user devices <b>102</b> can execute a client application <b>106</b> retained in a memory device <b>104</b> that can be present in the user device. A processor (not depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) integrated into the user device can execute the application <b>106</b>. The client application <b>106</b> can include a mobile application or a web browser, for example. Execution of the client application <b>106</b> can cause initiation of a presentation session. Accordingly, execution of the client application <b>106</b> can result in the exchange of data and/or signaling with a user gateway <b>132</b> included in the backend platform devices <b>130</b>. The user device and the user gateways <b>132</b> can be functionally coupled by a network architecture <b>125</b> that can include one or a combination of networks (wireless or wireline) that permit one-way and/or two-way communication of data and/or signaling. Specifically, the user device can receive data defining the digital content. Such data can be embodied in one or multiple streams defining respective elements of the digital content. For instance, a first stream can define imaging data corresponding to video content, and a second stream can defined audio data corresponding to an audio channel of the digital content. In some cases, a third stream defining haptic data also can be received. The haptic data can dictate elements of 4D content or another type of immersive content.</p><p id="p-0051" num="0050">The user gateway <b>132</b> can provide data defining the digital content by identifying a particular deliver server of multiple delivery servers <b>162</b> included in the distribution platform devices <b>160</b>, and then supplying a request for content to that particular delivery server. That particular delivery server can be embodied in an edge server in cases in which the distributed platform devices <b>160</b> include a content delivery network (CDN). In some configurations, the particular delivery server can have a local instance of digital content to be provided to a user device. The local instance of digital content can be obtained from one or several media repositories <b>164</b>, where each one of the media repositories <b>164</b> contain media assets <b>166</b>. Such assets can be static and can be consumed in time-shifted fashion. At least some of the media assets <b>166</b> can be specific to a media repository or can be replicated across two or more media repositories. The media assets <b>166</b> can include, for example, a video segment, a webcast, an RSS feed, or another type of digital content that can be streamed by the user gateway <b>132</b> and/or other devices of the backend platform devices <b>130</b>. The media assets <b>166</b> are not limited to digital content that can be streamed. In some cases, at least some of the media assets <b>166</b> can include static digital content, such as an image or a document.</p><p id="p-0052" num="0051">The particular delivery server can provide digital content to the user gateway <b>132</b> in response to the request for content. The user gateway <b>132</b> can then send the digital content to a user device. The user gateway <b>132</b> can send the digital content according to one of several communication protocols (e.g., IPv4 or IPv6, for example).</p><p id="p-0053" num="0052">In some embodiments, the digital content that is available to a user device or set of multiple user devices (e.g., a virtual classroom or a recital) can be configured by content management subsystem <b>140</b>. To that end, the content management subsystem <b>140</b> can identify corpora of digital content applicable to the user device(s). Execution of the client application <b>106</b> can result in access to a specific corpus of digital content based on attributes of the user device or a combination of the set of multiple devices.</p><p id="p-0054" num="0053">The subsystems <b>136</b> also include an analytics subsystem <b>142</b> that can generate intelligence and/or knowledge about content consumption behavior of a user device (e.g., one of the user devices <b>102</b>). The analytics subsystem <b>142</b> can retain the intelligence and/or knowledge in a storage subsystem <b>144</b>. Both the intelligence and knowledge can be generated using historical data identifying one or different types of activities of the user device. The activities can be related to consumption of digital content. In some configurations, the client application <b>106</b> can send activity data during consumption of digital content. The activity data can identify an interaction or a combination of interactions of the user device with the digital content. An example of an interaction is trick play (e.g., fast-forward or rewind) of the digital content. Another example of an interaction is reiterated playback of the digital content. Another example of an interaction is aborted playback, e.g., playback that is terminated before the endpoint of the digital content. Yet another example of the interaction is submission (or &#x201c;share&#x201d;) of the digital content to a user account in a social media platform. Thus, the activity data can characterize engagement with the digital content.</p><p id="p-0055" num="0054">The analytics subsystem <b>142</b> can then utilize the activity data to assess a degree of interest of the user device on the digital content (e.g., media assets). To that end, in some embodiments, the analytics subsystem <b>142</b> can train a machine learning model to discern a degree of interest on digital content among multiple interest levels. The machine learning model can be trained using unsupervised training, for example, and multiple features determined using digital content and the activity data. By applying the trained machine learning model to new activity data, an interest attribute can be generated. An interest attribute may represent one of the multiple interest levels and, thus, quantifies interest on the digital content on part of the user device.</p><p id="p-0056" num="0055">By evaluating interest of a user device on different types of digital content, the analytics subsystem <b>142</b> can generate a user profile for the user device. Such an evaluation can be implemented for multiple user devices and therefore multiple user profiles can be generated. A user profile may comprise a user interest cloud (UIC). A UIC can identify types of digital content&#x2014;and/or features thereof&#x2014;likely to be of interest to a user corresponding to a UIC and therefore likely to be consumed by the user via their user device. For example, a UIC may comprise a tag cloud that includes interest tags, which correspond to respective interests of a user. An interest of a user may be derived from user activity data. For example, the analytics subsystem <b>142</b> may receive activity data indicative of a plurality of engagements of a user device with a plurality of media assets (e.g., digital content). The analytics subsystem <b>142</b> may receive the activity data via the client application <b>106</b> executing on the user device. Each of the plurality of media assets may comprise a plurality of content features, as further described herein. The analytics subsystem <b>142</b> may generate a UIC associated with that particular user and/or user device. The UIC may include at least one content feature of the plurality of content features (e.g., representing content features associated with content with which the user has engaged). The UIC may also include, as further described herein, at least one interest attribute representing a level of interest for each of the media assets consumed by the user/user device. As further described herein, the UIC can be used by a machine learning model (e.g., client model <b>1550</b>) to identify one or more of the media assets <b>166</b> that are likely to be of interest to a user corresponding to the UIC.</p><p id="p-0057" num="0056">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the analytics subsystem <b>142</b> can include multiple units that permit generating a user profile. The analytics subsystem <b>142</b> can include a feature extraction unit <b>210</b> that can receive media asset data <b>204</b> defining a media asset of the media assets <b>166</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). As mentioned, the media asset can be a webinar, a video, a document, a webpage, a promotional webpage, or similar asset. The feature extraction unit <b>210</b> can then determine one or several content features for the media asset. Examples of content features that can be determined for the media asset include, content type (video, webinar, pdf, web page, etc.), content rating; author information (e.g., academic biography of a lecturer); date of creation; content tag; content category; content filter; language of the content; content description</p><p id="p-0058" num="0057">Simply as an example, the content description can include an abstract or a summary, such as a promotional summary, a social media summary, and an on-demand summary. The feature extraction unit <b>210</b> can determine the content feature(s) for the media asset prior to consumption of the media asset. In this way, the determination of a user profile can be more efficient. The feature extraction unit <b>210</b> can retain data indicative of the determined content feature(s) in storage <b>240</b>, within memory elements <b>246</b> (represented features <b>246</b>).</p><p id="p-0059" num="0058">In addition, the analytics subsystem <b>142</b> can include an activity monitoring unit <b>220</b> that can receive user activity data <b>224</b> for a user device. As mentioned, the client application <b>106</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) includes in the user device can send the user activity data <b>224</b>. The user activity data <b>224</b> can identify an interaction or a combination of interactions of the user device with the media asset. Again, an interaction can include one of trick play, reiterated playback, aborted play, social media share, or similar. The activity monitoring unit <b>220</b> can then generate one or several engagement features using the user activity data <b>224</b>. In some configurations, an engagement feature can quantify the engagement of the user device with the media asset. For instance, the engagement feature can be a numerical weight ascribed to a particular type of user activity data <b>224</b>. For example, aborted playback can be ascribed a first numerical weight and social media share can be ascribed a second numerical weight, where the first numerical weight is less than the second numerical weight. Other numerical weights can be ascribed to reiterated playback and trick-play. For such interactions, the number of reiterations and the time spent consuming the media asset due to trick-play can determine the magnitude of respective numerical weights. The feature extraction unit <b>210</b> can retain data indicative of the determined engagement feature(s) in the storage <b>240</b>, within the features <b>244</b>.</p><p id="p-0060" num="0059">The analytics subsystem <b>142</b> also can include a scoring unit <b>230</b> that can determine an interest level for the media asset corresponding to the determined content feature(s) and engagement feature(s). To that end, the scoring unit can apply a scoring model <b>248</b> to those features, where the scoring model <b>248</b> can be a trained machine learning model that resolves a multi-class classification task. Specifically, in some embodiments, the scoring unit <b>230</b> can generate a feature vector including determined content feature(s) and engagement feature(s) for the media asset. A feature vector may be associated with a particular user device(s). A feature vector may comprise a quantification of a level/amount of engagement with each media asset and/or a numerical weight associated with an engagement feature as described herein. The number and arrangement of items in such a feature vector may be the same as those of features vectors used during training of the scoring model <b>248</b>. The scoring unit <b>230</b> can then apply the scoring model <b>248</b> to the feature vector to generate an interest attribute representing a level of interest on the media asset. The interest attribute can be a numerical value (e.g., an integer number) or textual label that indicates the level of interest (e.g., &#x201c;high&#x201d;, &#x201c;moderate&#x201d;, and &#x201c;low&#x201d;).</p><p id="p-0061" num="0060">A profile generation unit <b>250</b> can determine, in some instances, that an interest attribute for a media asset meets or exceeds a defined level of interest. In those instances, the profile generation unit <b>250</b> can select words or phrases, or both, from content features determined for the media asset. Simply for purposes of illustrations, the profile generation unit <b>250</b> can select one or more categories of the media asset and a title of the media asset as is defined within a description of the media asset. A selected word or phrase may, for example, represent an interest of the user device on the media asset. The profile generation unit <b>250</b> can then generate a user profile <b>270</b> that includes multiple entries <b>276</b>, each one corresponding to a selected word or phrase. The profile generation unit <b>250</b> can then retain the user profile <b>270</b> in the storage subsystem <b>144</b>.</p><p id="p-0062" num="0061">By receiving user activity data <b>224</b> from different user devices, the analytics subsystem <b>142</b> can generate respective user profiles for those user devices. Thus, as is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the storage subsystem <b>144</b> can include user profiles <b>310</b>. In addition, or in some embodiments, the content management subsystem <b>140</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) can then configure digital content (e.g., media assets) that are of interest to the user device. As a result, a particular group of the media assets <b>166</b> can be made available to a particular user device. Such a group may define a corpus of digital content.</p><p id="p-0063" num="0062">In some embodiments, a user profile and a corpus of digital content for a user device also can comprise a UIC for the user device. In addition, or in other embodiments, the content management subsystem <b>140</b> can configure one or more functions to interact with digital content. Those function(s) can include, for example, one or a combination of translation functionality (automated or otherwise), social-media distribution, formatting functionality, or the like. The content management subsystem <b>140</b> can include at least one of the function(s) in the user interest cloud.</p><p id="p-0064" num="0063">The content management subsystem <b>140</b> can retain data defining a UIC within the storage subsystem <b>144</b>. Accordingly, the storage subsystem <b>144</b> can include asset corpora <b>320</b> (<figref idref="DRAWINGS">FIG. <b>3</b>A</figref>) that retains a corpora of media assets <b>324</b> for respective user profiles <b>310</b>. Multiple memory devices can comprise the asset corpora <b>320</b>. Those memory devices can be distributed geographically, in some embodiments. One or many database management servers (not depicted in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>) can manage the cloud storage <b>320</b>. The database management server(s) can be included in the content management subsystem <b>140</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0065" num="0064">At least a subset of the user profiles <b>320</b> can correspond to respective ones of the interest cumuli <b>314</b>. In other words, a first user profile of the user profiles <b>320</b> can be logically associated with a first interest cumulus of the interest cumuli <b>314</b>, a second user profile can be logically associated with a second interest cumulus of the interest cumuli <b>316</b>, and so forth. A logical association can be provided by a unique identifier (ID) for an interest cumulus corresponding to a user profile. The unique ID can be retained in the user profile.</p><p id="p-0066" num="0065">As described herein, each UIC may be derived from user activity data <b>224</b> indicative of a plurality of engagements of a user device with a plurality of media assets (e.g., digital content). The analytics subsystem <b>142</b> may receive the activity data via the client application <b>106</b> executing on the user device. The analytics subsystem <b>142</b> may generate a UIC associated with that particular user and/or user device. The UIC may include at least one content feature of a plurality of content features (e.g., representing content features associated with content with which the user has engaged). The UIC may also include, as further described herein, at least one interest attribute representing a level of interest for each of the media assets consumed by the user/user device. Each of the plurality of media assets <b>166</b> may comprise a plurality of content features including, but not limited to, at least one of: content format/type (e.g., video, audio, webcast, webinar. PDF, webpage, etc.); content rating (e.g., an audience/aggregated review score, such as 4/5 stars, 88%, etc.); demographic information associated with presenters; date of creation/upload/availability; engagement score of other users (e.g., as described herein with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>); metadata (e.g., tags, categories, filters, etc.); description/abstract/summary; language(s) spoken/shown; functionality feature(s), as further described herein; a combination thereof, and/or the like.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> shows an example visual representation <b>325</b> of a UIC. As shown in the visual representation <b>325</b>, the UIC may be based on, for example, the user activity data <b>224</b> indicative of the plurality of engagements with one or more of the plurality of media assets. The media assets may include, as an example only, downloaded resources (e.g., media assets and related content); videos; webcasts/webinars; questions asked (e.g., via the client application <b>106</b>): and slides. As further described herein, a user profile, which may comprise the UIC, may include multiple entries <b>276</b> of words and/or phrases. An example of words and/or phrases that may be included in the multiple entries <b>276</b> is shown in the right-hand side of the visual representation <b>325</b> of the UIC. These words and/or phrases may represent interests of the corresponding user that are derived as described herein based on the user activity data <b>224</b>.</p><p id="p-0068" num="0067">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, multiple source devices <b>150</b> can create digital content for presentation at a user device (e.g., one of the user devices <b>102</b>). At least a subset of the source devices <b>150</b> can comprise a source platform. Such digital content can include, for example, static assets that can be retained in a media repository, as part of the media assets <b>166</b>. The source device can provide the created digital content to a source gateway <b>146</b>. The source device can be coupled to the source gateway by a network architecture <b>145</b>. The network architecture <b>145</b> can include one or a combination of networks (wireless or wireline) that permit one-way and/or two-way communication of data and/or signaling. The source gateway <b>140</b> can send the digital content to the content management subsystem <b>140</b> for provisioning of the digital content in one or several of the media repositories <b>164</b>.</p><p id="p-0069" num="0068">In addition, or in some cases, a source device can configure the manner of creating digital content contemporaneously by means of the client application <b>106</b> and other components available to a user device. That is, the source device can build the client application <b>106</b> to have specific functionality for generation of digital content. The source device can then supply an executable version of the client device to a user device. Digital content created contemporaneously can be retained in the storage subsystem <b>144</b>, for example.</p><p id="p-0070" num="0069">The subsystems <b>136</b> also can include a service management subsystem <b>138</b> than can provide several administrative functionalities. For instance, the service management subsystem <b>138</b> can provide onboarding for new service providers. The service management subsystem <b>138</b> also can provide billing functionality for extant service providers. Further, the service management subsystem can host an executable version of the client application <b>106</b> for provision to a user device. In other words, the service management subsystem <b>136</b> can permit downloading the executable version of the client application <b>106</b>.</p><p id="p-0071" num="0070">With further reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the analytics subsystem <b>142</b> can retain user activity data <b>224</b> over time in an activity data repository <b>244</b> (referred to as activity data <b>244</b>). The time during which the user activity data <b>224</b> can be retained can vary, ranging from a few days to several weeks.</p><p id="p-0072" num="0071">The analytics subsystem <b>142</b> can include a report unit <b>260</b> that can generate various views of the activity data <b>244</b> and can operate on at least a subset of the activity data <b>244</b>. The report unit <b>260</b> also can cause a user device to present a data view and/or one or several results from respective operations on the activity data <b>244</b>. To that end, the user device can include the application <b>106</b> and the report unit <b>260</b> can receive from the application <b>106</b> a request message to provide the data view or the result(s), or both. Further, in response to the request message, the report unit <b>260</b> generate the data view and the result(s) and can then cause the application <b>106</b> to direct the user device to present a user interface conveying the data view or the result(s). The UI can be presented in a display device integrated into, or functionally coupled to, the user device. The user device can be one of the user devices <b>102</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0073" num="0072">The request message can be formatted according to one of several communication protocols (e.g., HTTP) and can control the number and type of data views and results to be presented in the user device. The request message can thus include payload data identifying a data view and/or a result being requested. In some cases, the request message can be general, where the payload data identify data view(s) and result(s) defined by the analytics subsystem. For instance, the payload data can be a string, such as &#x201c;report_all&#x201d; or &#x201c;dashboard&#x201d;, or another alphanumeric code that conveys that a preset reporting option is being requested. In other cases, the request message can be customized, where the payload data can include one or more first codes identifying respective data views and/or one or more second codes identifying a particular operation on available activity data <b>244</b>.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a U <b>1400</b> that presents various types of engagement data that can be obtained from the activity data <b>244</b> for a particular end-user, in accordance with one or more embodiments of this disclosure. The UI <b>400</b> can be referred to as engagement dashboard. The data conveyed in the UI <b>400</b> can be obtained in response to a request message including the &#x201c;dashboard&#x201d; code or a similar payload data. As is illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the UI <b>400</b> includes indicia <b>404</b>: various panes are displayed, each presenting a particular data view or an aggregated result for a particular end-user. Specifically, the UI <b>400</b> includes a first pane <b>410</b> that presents engagement level <b>412</b> and engagement time <b>414</b>. The UI <b>400</b> also includes a second pane <b>420</b> that presents engagement activity and a third pane <b>430</b> that presents buying activity. In addition.</p><p id="p-0075" num="0074">UI <b>400</b> includes a fourth pane <b>440</b> that presents a menu of content recommendations and a fifth pane <b>450</b> that presents at least some of the words/phrases <b>276</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) pertaining to the end-user. The words and phrases that are presented can be formatted in a way that pictorially ranks the interests of the end-user (e.g., greater font size represents greater interest). Further, the UI <b>400</b> also includes a sixth pane <b>460</b> that presents an amount of content consumed as a function of time. Such temporal dependence of content consumption can be referred to as &#x201c;content journey&#x201d;. By making available the types of engagement data illustrated in the UI <b>400</b>, a source device can access valuable and actionable insights to optimize a digital experience (or media asset).</p><p id="p-0076" num="0075">The analytics subsystem <b>142</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) also can contain other scoring models besides the scoring model that can be applied to generate an interest level for particular content (e.g., a media asset). By using those other scoring models, the analytics subsystem <b>142</b> can generate information identifying features of a digital experience (or media asset(s)) that may cause satisfactory engagement (e.g., most engagement, second most engagement, or similar) with an end-user. Accordingly, the analytics subsystem <b>142</b> can predict how best to personalize digital experiences (or media assets) for particular customers based on their prior behavior and interactions with media assets supplied by the distribution platform devices <b>160</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). Accordingly, a source device can access valuable and actionable insights to optimize a digital experience.</p><p id="p-0077" num="0076">More specifically, in some embodiments, the scoring unit <b>230</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) can apply a defined scoring model to user activity data <b>224</b> to evaluate a set of functionality features present in several media assets. Evaluating a functionality feature f includes generating a score S for f. Thus, for a set of multiple functionality features {f<sub>0</sub>, f<sub>1</sub>, f<sub>2 </sub>. . . , f<sub>N-1</sub>}, with N a natural number greater than unity, application of defined scoring model can result in a set of respective scores {S<sub>0</sub>, S<sub>1</sub>, S<sub>2</sub>, . . . , S<sub>N-1</sub>}. The defined scoring model can be one of the scoring models <b>248</b> and can be trained using historical user activity data for many users and media assets.</p><p id="p-0078" num="0077">Simply for purposes of illustration, the functionality features can include (i) real-time translation, (ii) real-time transcription (e.g., captioning) in same language; (iii) real-time transcription in a different language; (iv) access to documents (scientific publications, scientific preprints, or whitepapers, for example) mentioned in a presentation; (v) detection of haptic capable device and provisioning of 4D experience during presentation; (vi) &#x201c;share&#x201d; function to custom set of recipients within or outside a social network; (vii) access to recommended content, such as copies of or links to similar presentations and/or links to curated content (e.g., &#x201c;because you watched &#x201c;Content A&#x201d; you might enjoy &#x201c;Content B&#x201d;); (viii) messaging with links to cited, recommended, or curated content; (ix) scheduler function that prompts to add, adds, or sends invites for, live presentations of interest that occur during times that end-user is free; automatically populates a portion of the calendar with those presentations, amount of calendar that can be populated is determined by end-user; or similar functions. Access to a document can include provision of a copy of the document or provision of a link to the document. Similarly, access to content can include provision of a copy of the content or provision of a link to the content.</p><p id="p-0079" num="0078">Diagram <b>510</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically depicts engagement scores for an example case in which N=8 functionality features are available per digital experience (or media asset), for a particular end-user. Each of the features f<sub>0</sub>, f<sub>1</sub>, f<sub>2</sub>, f<sub>3</sub>, f<sub>4</sub>, f<sub>5</sub>, f<sub>6</sub>, and f<sub>7 </sub>have respective scores. Some of the scores are less than a threshold score S<sub>th </sub>and other scores are greater than S<sub>th</sub>. The threshold score is a configurable parameter that the profile generation unit <b>250</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) can apply to determine if a functionality feature is preferred by the particular end-user. As is depicted with a dotted area in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a functionality feature f is preferred if the corresponding engagement score S is greater than or equal to S<sub>th</sub>. The score structure for that set of functionality features can differ from end-user to end-user, thus revealing which functionality features are preferred for the end-user. The profile generation unit <b>250</b> can determine that respective engagement scores for one or several functionality features are greater than S<sub>th</sub>. In response, the profile generation unit <b>250</b> can update a user profile <b>520</b> with preference data identifying the functionality feature(s). Thus, the user profile <b>520</b> can include words/phrases <b>276</b> and functionality preference <b>530</b> including that preference data.</p><p id="p-0080" num="0079">In the example depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, functionality features f<sub>2</sub>, f<sub>3 </sub>and f<sub>7 </sub>have engagement scores greater than S<sub>th</sub>. Thus, the profile generation unit <b>250</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) can determine that those features are preferred by the particular end-user. In one example, h can be real-time translation, f<sub>3 </sub>can be real-time transcription in a different language from the language of a presentation, and f<sub>7 </sub>can be access to documents. The profile generation unit <b>250</b> can determine that respective engagement scores for those features are greater than S<sub>th</sub>, and can then update a user profile <b>520</b> with preference data identifying features functionality features, f<sub>2</sub>, f<sub>3 </sub>and f<sub>7</sub>. As such, the user profile <b>520</b> can include words/phrases <b>276</b> and functionality preference <b>530</b> including that preference data.</p><p id="p-0081" num="0080">The content management subsystem <b>140</b> can personalize the digital experiences for an end-user by including the functionality features <b>531</b>) defined in the user profile <b>520</b> pertaining to the end-user. In some embodiments, the content management subsystem <b>140</b> can include a media provisioning unit <b>540</b> that access the functionality preferences <b>530</b> and can then generate a UI that is personalized according to the functionality preferences <b>530</b>. That personalized UI can include the functionality features identified in the functionality preferences <b>530</b>.</p><p id="p-0082" num="0081">In addition, or in other embodiments, the media provisioning unit <b>540</b> also can generate a layout of content areas that is personalized to end-user. The personalized layout can include a particular arrangement of one or several UI elements for respective preferred functionalities of the end-user. Further, or in other embodiments, the media provisioning unit <b>540</b> can generate a presentation ticker (such as a carousel containing indicia) identifying live-action presentations near a location of a user device presenting the personalized UI. In addition, or in some cases, the presentation ticker also can include indicia identifying digital experiences (or media assets) that occur during times shown as available in a calendar application of the end-user.</p><p id="p-0083" num="0082">It is noted that the analytics subsystem <b>142</b> is not limited to scoring models. Indeed, the analytics subsystem <b>142</b> can include and utilize other machine-learning (ML) models to provide various types of predictive functionalities. Examples of those functionalities include predictive engagement levels for end-users; Q&#x26;A autonomous modules to answer routine support questions; and platform audience and presenter load predictions. The service management subsystem <b>138</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) can use load predictions to identify and configure operational resources and provide oversight. The operational resources include computing resources, such as processing units, storage units, and cloud services, for example.</p><p id="p-0084" num="0083">The presentation platform described in this disclosure can be integrated with a third-party platform. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of an operational environment <b>600</b> that includes a presentation platform integrated with third-party subsystems <b>610</b>, in accordance with one or more embodiments of this disclosure. Integration of the presentation platform can be accomplished by functional coupling with third-party subsystems <b>610</b> via a third-party gateway <b>612</b> and a network architecture <b>615</b>. The network architecture <b>615</b> can include one or a combination of networks (wireless or wireline) that permit one-way and/or two-way communication of data and/or signaling.</p><p id="p-0085" num="0084">The third-party subsystem <b>610</b> can include various type of subsystems that permit first-person insights generated by the analytics subsystem <b>142</b> to be extracted and leveraged across business systems of a source platform. As is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the third-party subsystems <b>610</b> can include a Customer Relationship Management (CRM) subsystem <b>620</b>, a business intelligence (BI) subsystem <b>630</b>, and a marketing automation subsystem <b>640</b>. Each third-party subsystem <b>610</b> may be referred to herein as a &#x201c;client system&#x201d; or simply as a &#x201c;client&#x201d;. The presentation platform described herein may access, control, etc., each of the third-party subsystems <b>610</b>.</p><p id="p-0086" num="0085">As is illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a source device <b>704</b> can access an API server device <b>710</b> within the backend platform device <b>130</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref> or <figref idref="DRAWINGS">FIG. <b>4</b></figref>) by means of the source gateway <b>146</b>. The API server device <b>710</b> can expose multiple application programming interfaces (APIs) <b>724</b> retained in API storage <b>720</b>. One or many of the APIs <b>724</b> can be exposed to the source device <b>704</b>, in order to access a third-party subsystem <b>730</b> and functionality provided by such subsystem. The exposed API(s) can permit executing respective sets of function calls. That is, a first exposed API can permit accessing a first group of function calls for defined functionality, and a second exposed API can permit accessing a second group of function calls for defined second functionality. The function calls can operate on data that is contained in the soured device <b>704</b> and/or a storage system functionally coupled to the source device <b>704</b>. The function calls also can operate on activity data <b>244</b>, with result being pushed to the source device <b>704</b>.</p><p id="p-0087" num="0086">Data and/or signaling associated with execution of such function calls can be exchanged between the API server device <b>710</b> and the third-party subsystem <b>730</b> via a third-party gateway <b>612</b>. In addition, other data and/or signaling can be exchanged between the API server device <b>710</b> and the source device <b>704</b> via the source gateway <b>146</b>.</p><p id="p-0088" num="0087">In some cases, the API server device <b>710</b> also can expose one or many of the APIs <b>726</b> to the third-party subsystem <b>730</b>. In that way, the third-party subsystem <b>730</b> (or, in some cases, a third-party device, such as a developer device) can create applications that utilize some of the functionality of the backend platform devices <b>130</b>.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates example components of the integration subsystem <b>740</b>. The integration subsystem <b>740</b> supports an ecosystem of third-party application integrations and APIs that enable the first-person insights generated by the analytics subsystem <b>142</b> to be extracted and leveraged across customer business systems for more intelligent sales and marketing. The integration subsystem <b>741</b>) can include an API <b>744</b> that may be configured to exchange data with one or more third-party applications <b>750</b>. The one or more third-party applications <b>750</b> may be, for example, a sales application, a marketing automation application, a CRM application, a Business Intelligence (BI) application, and/or the like. The third-party applications <b>750</b> may be configured to leverage data received from and/or sent to the integration subsystem <b>740</b>, via the API <b>744</b>.</p><p id="p-0090" num="0089">In order to exchange data and provide control over certain functionality via the API <b>744</b>, the integration subsystem <b>744</b> may use an authentication and authorization unit <b>748</b> to generate an access token. The access token may comprise a token key and a token secret. The access token may be associated with a client identifier. Authentication for API requests may be handled via custom HTTP request headers corresponding to the token key and the token secret. The client identifier may be included in the path of an API request URL.</p><p id="p-0091" num="0090">The API <b>744</b> may comprise a set of routines, protocols, and/or tools for building software applications. The API <b>744</b> may specify how software components should interact. In an embodiment, the API <b>744</b> may be configured to send data <b>766</b>, receive data <b>768</b>, and/or synchronize data <b>770</b>. In some cases, the API <b>744</b> may be configured to send data <b>766</b>, receive data <b>768</b>, and/or synchronize data <b>770</b> in substantially real-time, at regular intervals, as requested, and/or the like. The API <b>744</b> may be configured to provide the one or more third party applications <b>750</b> the ability to access a digital experience (or media asset) functionality, including, for example, event management (e.g., create a webinar, delete a webinar), analytics, account level functions (e.g., event, registrants, attendees), event level functions (e.g., metadata, usage, registrants, attendees), and/or registration (e.g., webinar, or an online portal product as is described below).</p><p id="p-0092" num="0091">The integration subsystem <b>740</b>, via the API <b>744</b>, may be configured to deliver attendance/registration information to the third party application <b>750</b> to update contact information for Leads <b>752</b>. The third-party application <b>750</b> can use attendance/registration information for lead segmentation, lead scoring, lead qualification, and/or targeted campaigns. Engagement data (such as viewing duration, engagement scores, resource downloads, poll/survey responses) associated with webinars may be provided to the third-party application <b>750</b> for use in lead scoring and lead qualification to identify leads and ensure effective communication with prospects and current customers.</p><p id="p-0093" num="0092">The integration subsystem <b>740</b>, via the API <b>744</b>, may be configured to enable the third-party application <b>750</b> to use data provided by the integration subsystem <b>740</b>, via the API <b>744</b>, to automate workflows. Engagement data (such as viewing duration, engagement scores, resource downloads, poll/survey responses) associated with webinars may be provided to the third-party application <b>750</b> for use in setting one or more triggers <b>754</b>, filters <b>756</b>, and/or actions <b>758</b>. The third-party application <b>750</b> may configure a trigger <b>754</b>. The trigger <b>754</b> may be a data point and/or an event, the existence of which may cause an action <b>758</b> to occur. The third-party application <b>750</b> may configure a filter <b>754</b>. The filter <b>754</b> may be a threshold or similar constraint applied to the data point and/or the event to determine whether any action <b>758</b> should be taken based on occurrence of the trigger <b>758</b> or determine which action <b>758</b> to take based on occurrence of the trigger <b>756</b>. The third-party application <b>750</b> may configure an action <b>758</b>. The action <b>758</b> may be an execution of a function, such as updating a database, sending an email, activating a campaign, etc. The third-party application <b>750</b> may receive data (such as engagement data) from the integration subsystem <b>740</b>, via the API <b>744</b>, determine if the data relates to a trigger <b>754</b>, apply any filters <b>756</b>, and initiate any actions <b>758</b>. As an example, the third-party application <b>750</b> may receive engagement data from the integration subsystem <b>740</b> that indicates a user from a specific company watched 30 minutes of a 40-minute video. A trigger <b>754</b> may be configured to identify any engagement data associated with the specific company. A filter <b>756</b> may be configured to filter out any engagement data associated with viewing times of less than 50% of a video. An action <b>758</b> may be configured to send an e-mail to the user inviting the user to watch a related video.</p><p id="p-0094" num="0093">In some embodiments, the content management subsystem <b>140</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) can provide an online resource portal product that permits providing rich digital experiences for an audience of prospective end-user to find, consume, and engage with interactive webinar experiences and other media assets, such as videos and whitepapers. The online resource portal product can be referred to as an &#x201c;engagement hub&#x201d;, simply for the sake of nomenclature.</p><p id="p-0095" num="0094">The online portal product provides various functionalities to generate a digital experience (or media asset). As an illustration, <figref idref="DRAWINGS">FIG. <b>8</b></figref> presents an example of a UI <b>810</b> representing a landing page of the online portal product, and <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of a portal subsystem <b>900</b> that provides the functionality of the online portal product. As is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the landing page include a pane <b>812</b> that includes a title and a UI element <b>814</b> that includes digital content describing the functionality of the online portal product. The title is depicted as &#x201c;Welcome to Digital Experience Constructor Portal&#x201d;, simply as an example. A landing unit <b>904</b> in the portal subsystem <b>900</b> (<figref idref="DRAWINGS">FIG. <b>9</b></figref>) can cause the presentation of the UI <b>900</b> in response to receiving a request message to access the online portal product from a source device.</p><p id="p-0096" num="0095">The UI <b>810</b> (<figref idref="DRAWINGS">FIG. <b>8</b></figref>) also includes several selectable UI elements identifying respective examples of the functionalities that can be provided by the online portal product. Specifically, the selectable UI elements include, for example, a selectable UI element <b>816</b> corresponding to a search function; a selectable UI element <b>818</b> corresponding to a branding function; a selectable UI element <b>820</b> corresponding to a categorization function; a selectable UI element <b>822</b> corresponding to a layout selection function (from defined content layouts), a website embedding function, a curation function, and a provisioning function. The provisioning function also can be referred to a publication function.</p><p id="p-0097" num="0096">Selection of the selectable UI element <b>816</b> can cause the source device that presents the UI <b>810</b> to present another UI (not depicted) to search for a media asset to be augmented with directed content. To that end, in some embodiments, the portal subsystem <b>900</b> can include a search unit <b>916</b>. In this disclosure, directed content refers to digital media configured for a particular audience, or a particular outlet channel (such as a website, a streaming service, or a mobile application), or both. Directed content can include, for example, digital media of various types, such as advertisement; surveys or other types of questionnaires; motion pictures, animations, or other types of video segments; podcasts; audio segments of defined durations (e.g., a portion of a speech or tutorial; and similar media.</p><p id="p-0098" num="0097">Selection of the selectable UI element <b>818</b> can cause the source device to present another UI (not depicted) that permits obtaining digital content to incorporate into a particular media asset. The digital content can identify the particular media asset as pertaining to a source platform that includes the source device. In some cases, the digital content can be embodied in as a still image (e.g., a logotype), an audio segment (e.g., a jingle), or an animation. In some embodiments, the portal subsystem <b>900</b> can include a branding unit <b>920</b> that can direct the source device to present a UI in response to selection of the selectable UI element <b>818</b>. The portal subsystem <b>900</b> also can include an ingestion unit <b>908</b> that can obtain the digital content from the storage subsystem <b>144</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) for example.</p><p id="p-0099" num="0098">Selection of the selectable UI element <b>820</b> can cause the source device to present another UI (not depicted) to categorize multiple media assets according to multiple categories. In some embodiments, the portal subsystem <b>900</b> can include a categorization unit <b>924</b> that can cause presentation of the other UI in response to selection of the selectable UI element <b>820</b>. The categorization unit <b>924</b> also can classify a media asset according to one of the several categories.</p><p id="p-0100" num="0099">Selection of the selectable UI element <b>822</b> can cause the source device to present another UI (not depicted) to select a layout of areas for presentation of digital content. A first area of the layout of areas can be assigned for presentation of a media asset that is being augmented with directed content. At least one second area of the layout of areas can be assigned for presentation of the directed content. In some embodiments, the portal subsystem <b>900</b> can include a layout selection unit <b>928</b> that can cause presentation of the other UI in response to selection of the selectable UI element <b>822</b>. The layout selection unit <b>928</b> can cause presentation of a menu of defined layout templates. Data defining such a menu can be retained in a layout template storage <b>948</b>. In response to receiving input information identifying a selection of the particular defined layout template, the layout selection unit <b>928</b> can configure that particular defined layout for presentation of the media asset and directed content.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>10</b></figref> and <figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrate respective examples of layout templates. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, an example layout template <b>1000</b> includes a first area <b>1010</b> that can be allocated to the media asset and a second area <b>1020</b> that can be allocated to the directed content. As is shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the directed content can be overlaid on the media asset. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, an example layout template <b>1100</b> includes a first area <b>1110</b> that can be allocated to the media asset and a second area <b>1120</b> that can be allocated to the directed content. The second area <b>1120</b> is adjacent to first area <b>1110</b>. Thus, rather than presenting the directed content as an overlay, the directed content is presented adjacent to the media asset.</p><p id="p-0102" num="0101">With further reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, selection of the selectable UI element <b>824</b> can cause the source device that presents the UI <b>810</b> to present another UI (not depicted) to configure website-embedding of directed content. To that end, in some embodiments, the portal subsystem <b>900</b> can include a website embedding unit <b>932</b>.</p><p id="p-0103" num="0102">Selection of the selectable UI element <b>826</b> can cause the source device to present another UI (not depicted) to curate directed content that can be presented in conjunction with media assets. In some embodiments, the ingestion unit <b>908</b> can obtain multiple directed content assets and can cause the source device to present such assets. The multiple directed content assets can be presented in various formats. In one example, the multiple directed content assets can be presented as respective thumbnails. In another example, the multiple directed content assets can be presented in a selectable carousel area. The portal subsystem <b>900</b> also can include a curation unit <b>936</b> that cause presentation of the other UI in response to selection of the selectable UI element <b>826</b>. In addition, in some cases, the curation unit <b>936</b> can receive input information indicating approval of one or several directed content assets for presentation with media assets. In other cases, the curation unit <b>936</b> can evaluate each one the multiple directed content assets obtained by the ingestion component <b>908</b>. An evaluation that satisfies one or more defined criteria results in the directed content asset being approved for presentation with media assets.</p><p id="p-0104" num="0103">Regardless of approval mechanism, the curation unit <b>936</b> can then configure each one of the approved directed content asset(s) as being available for presentation. The approval and configuration represent the curation of those assets. The curation unit <b>936</b> can update a corpus of curated directed content assets <b>956</b> within a curated asset storage <b>952</b> in response to curation of one or many directed content assets.</p><p id="p-0105" num="0104">The portal subsystem <b>900</b> also can include a media provisioning unit <b>940</b> that can configure presentation of a media asset based on one or a combination of the selected digital content that identifies the source platform, one or several curated directed content assets, and a selected defined layout. To that end, in some cases, the media provisioning unit <b>940</b> can generate formatting information identifying the media asset, the selected digital content, the curated directed content asset(s), and the selected defined layout. In addition, or in other cases, the media provisioning unit <b>940</b> also can configure a group of rules that controls presentation of directed content during the presentation of the media asset. As an example, the media provisioning unit <b>940</b> can define a rule that dictates an instant in which the presentation of the directed content begins and a duration of that presentation. Further, or as another example, the media provisioning unit <b>940</b> can configure another rule that dictates a condition for presentation of the directed content and a duration of the presentation of the directed content. Examples of the condition include presence of a defined keyword or keyphrase, or both, in the media asset; presence of defined attributes of an audience consuming the media asset; or similar conditions. An attribute of an audience includes, for example, location of the audience, size of the audience, type of the audience (e.g., students or C-suite executives, for example), or level of engagement of the audience. In some embodiments, an autonomous component (referred to as bot) can listen to a presentation and can perform keyword spotting or more complete speech recognition to detect defined keywords or keyphrases.</p><p id="p-0106" num="0105">The media provisioning unit <b>940</b> can integrate the formatting information into the media asset as metadata. The metadata can control some aspects of the digital experience that includes the presentation of the media asset. As a result, the online portal product provides a straightforward and efficient way for a source device to seamlessly publish, curate, and promote their interactive webinar experiences alongside directed content that a source device can upload and host inside presentation platform described herein in connection with <figref idref="DRAWINGS">FIG. <b>1</b></figref> or <figref idref="DRAWINGS">FIG. <b>4</b></figref>, or both.</p><p id="p-0107" num="0106">Besides the online portal product, or in some embodiments, the content management subsystem <b>130</b> can include a personalization subsystem <b>1200</b> as is illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. The personalization subsystem <b>1200</b> can be a part of the content management subsystem <b>140</b> and can permit creating a personalized media asset that incorporates directed content. The personalization subsystem <b>1200</b> can permit, for example, generating, curating, and/or disseminating interactive webinar and video experiences and other multimedia content to distributed audience segments with relevant messaging, offers, and calls-to-action (e.g., view video, listen to podcast, signup for newsletter, attend a tradeshow, etc.).</p><p id="p-0108" num="0107">The personalization subsystem <b>1200</b> can include a directed content selection unit <b>1210</b> that can identify directed content assets that can be relevant to a user device consuming a media asset. To that end, the content selection unit <b>1210</b> can direct an ingestion unit <b>1220</b> to obtain a group of directed content assets from directed content storage <b>1280</b> retaining a corpus of directed content assets <b>1284</b>. In some cases, the corpus of directed content assets <b>1264</b> can be categorized according to attributes of an end-user. The attributes can include, for example, market type, market segment, geography, business size, business type, revenue, profits, and similar. Accordingly, for a particular user device for which the personalization is being implemented, the content selection unit <b>1210</b> can direct the ingestions unit <b>1220</b> to obtain directed content assets having a particular set of attributes. Simply as an illustration, the ingestion unit <b>1220</b> can obtain multiple directed content assets having the following attributes: industrial equipment, small-medium business (SMB), and U.S. Midwest.</p><p id="p-0109" num="0108">In some cases, the ingestion unit <b>1220</b> can cause a source device to present the multiple directed content assets according to one of various formats. As mentioned, the multiple directed content assets can be presented as respective thumbnails or in a selectable carousel area.</p><p id="p-0110" num="0109">The personalization subsystem <b>1200</b> also can include a curation unit <b>1230</b> that can receive input information indicating approval of one or several directed content assets for presentation with media assets. The input information can be received from the source device that personalizes the media asset. In other cases, the curation unit <b>1230</b> can evaluate each one the multiple directed content assets obtained by the ingestion unit <b>1220</b>. An evaluation that satisfies one or more defined criteria results in the directed content asset being approved for presentation with media assets.</p><p id="p-0111" num="0110">Regardless of approval mechanism, the curation unit <b>936</b> can then configure each one of the approved directed content asset(s) as being available for personalization. As mentioned, the approval and configuration represent the curation of those assets. The ingestion unit <b>1220</b> can update a corpus of personalization assets <b>1278</b> to include directed content assets that have been curated for a particular user-device, within a storage <b>1260</b>.</p><p id="p-0112" num="0111">The personalization subsystem <b>1200</b> also can include a generation unit <b>1240</b> that can select one or several personalization assets of the personalization assets <b>1278</b> and can then incorporate the personalization asset(s) into a media asset being personalized.</p><p id="p-0113" num="0112">Incorporation of a personalization asset into the media asset can include, in some cases, adding one or several overlays to the media asset. A first overlay can include notes on a product described in the media asset. The overlay can be present for a defined duration that can be less than or equal to the duration of the media asset. Simply as an illustration, for industrial equipment, the note can be a description of capacity of a mining sifter or stability features of vibrating motor. A second overlay can include one or several links to respective documents (e.g., product whitepaper) related to the product. Further, or as another alternative, a third overlay can include a call-to-action related to the product.</p><p id="p-0114" num="0113">Further, or in some cases, the generation unit <b>1240</b> can configure one or several functionality features to be made available during presentation of the media asset. Examples of the functionality features include translation, transcription, read-aloud, live chat, trainer/presenter scheduler, or similar. The type and number of functionality features that are configured can be based on the respective scores as is described above.</p><p id="p-0115" num="0114">The generation unit <b>1240</b> can generate formatting information defining presentation attributes of one or several overlays to be included in the media asset being personalized. In addition, or in some cases, the generation unit <b>1240</b> also can generate second formatting information identifying the group of functionality features to be included with the media asset.</p><p id="p-0116" num="0115">The media provisioning unit <b>940</b> can integrate available formatting information into the media asset as metadata. The metadata can control some aspects of the personalized digital experience that includes the presentation of the media asset. The media provisioning unit <b>1260</b>, in some cases, also can configure one or more platforms/channels (web, mobile web, mobile app) to present the media asset. In addition, or in other cases, the media provisioning unit <b>1250</b> also can configure a group of rules that controls presentation of the media asset. As an example, the media provisioning unit <b>940</b> can define a rule that dictates that directed content is presented during specific time intervals during certain days. Further, or as another example, the media provisioning unit <b>1250</b> can configure another rule that dictates that directed content is presented during a particular period. For example, the particular period can be a defined number of days after initial consumption of the media asset. As yet another example, the media provisioning unit <b>1250</b> can define yet another rule that dictates that directed content is presented a defined number of times during a particular period.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> shows example components of the content management subsystem <b>140</b>. Digital content (e.g., the media assets <b>166</b>) as described herein may be provided by a presentation module <b>1300</b> of the content management subsystem <b>140</b>. For example, the media assets <b>166</b> may comprise interactive webinars. The webinars may comprise web-based presentations, livestreams, webcasts, etc. The phrases &#x201c;webinar&#x201d; and &#x201c;communication session&#x201d; may be used interchangeably herein. A communication session may comprise an entire webinar or a portion (e.g., component) of a webinar, such as a corresponding chat room/box. The presentation module <b>1300</b> may provide webinars at the user devices <b>102</b> via the client application <b>106</b>. As further described herein, the webinars may be provided via a user interface(s) <b>1301</b> of the client application <b>106</b>.</p><p id="p-0118" num="0117">The webinars may comprise linear content (e.g., live, real-time content) and/or on-demand content (e.g., pre-recorded content). For example, the webinars may be livestreamed. As another example, the webinars may have been previously livestreamed and recorded. Previously recorded webinars may be stored in the media repository <b>164</b> and accessible on-demand via the client application <b>106</b>. As further described herein, a plurality of controls provided via the client application <b>106</b> may allow users of the user devices <b>102</b> to pause, fast-forward, and/or rewind previously recorded webinars that are accessed/consumed on-demand.</p><p id="p-0119" num="0118">As shown in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>, the content management subsystem <b>140</b> may comprise a studio module <b>1304</b>. The studio module <b>1304</b> may comprise a production environment (not shown). The production environment may comprise a plurality of tools that administrators and/or presenters of a webinar may use to record, livestream, and/or upload multimedia presentations/content for the webinar.</p><p id="p-0120" num="0119">The studio module <b>1304</b> may comprise a template module <b>1304</b>A. The template module <b>1304</b>A may be used to customize the user experience for a webinar using a plurality of stored templates (e.g., layout templates). For example, administrators and/or presenters of a webinar may use the template module <b>1304</b>A to select a template from the plurality of stored templates for the webinar. The stored templates may comprise various configurations of user interface elements, as further described below with respect to <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>. For example, each template of the plurality of stored templates may comprise a particular background, font, font size, color scheme, theme, pattern, a combination thereof, and/or the like. The studio module <b>1304</b> may comprise a storage repository <b>13048</b> that allows any customization and/or selection made within the studio module <b>1304</b> to be saved (e.g., as a template).</p><p id="p-0121" num="0120"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> shows an example of a user interface <b>1301</b> of an example webinar. The user interface <b>1301</b> may be generated by the presentation module <b>1300</b> and presented at the user devices <b>102</b> via the client application <b>106</b>. The user interface <b>1301</b> for a particular webinar may comprise a background, font, font size, color scheme, theme, pattern, a combination thereof, and/or the like. The user interface <b>1301</b> may comprise a plurality of interface elements (e.g., &#x201c;widgets&#x201d;) <b>1301</b>A-<b>1301</b>F. The user interface <b>1301</b> and the plurality of interface elements <b>1301</b>A-<b>1301</b>F may be configured for use on any computing device, mobile device, media player, etc. that supports rich web/Internet applications (e.g., HTML5, Adobe Flash&#x2122;, Microsoft Silverlight&#x2122;, etc.).</p><p id="p-0122" num="0121">As shown in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>, the user interface <b>1301</b> may comprise a media player element <b>1301</b>A. The media player element <b>1301</b>A may stream audio and/or video presented during a webinar. The media player element <b>1301</b>A may comprise a plurality of controls (not shown) that allow users of the client application <b>106</b> to adjust a volume level, adjust a quality level (e.g., a bitrate), and/or adjust a window size. For webinars that are provided on-demand, the plurality of controls of the media player element <b>1301</b>A may allow users of the client application <b>106</b> to pause, fast-forward, and/or rewind content presented via the media player element <b>1301</b>A.</p><p id="p-0123" num="0122">As another example, as shown in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>, the user interface <b>1301</b> may comprise a Q&#x26;A element <b>1301</b>B. The Q&#x26;A element <b>1301</b>B may comprise a chat room/box that allows users of the client application <b>106</b> to interact with other users, administrators, and/or presenters of the webinar. The user interface <b>1301</b> may also comprise a resources element <b>1301</b>C. The resources element <b>1301</b>C may include a plurality of internal or external links to related content associated with the webinar, such as other webinars, videos, audio, images, documents, websites, a combination thereof, and/or the like.</p><p id="p-0124" num="0123">The user interface <b>1301</b> may comprise a communication element <b>1301</b>D. The communication element <b>1301</b>D may allow users of the client application <b>106</b> to communicate with an entity associated with the webinar (e.g., a company, person, website, etc.). For example, the communication element <b>1301</b>D may include links to email addresses, websites, telephone numbers, a combination thereof, and/or the like.</p><p id="p-0125" num="0124">The user interface <b>1301</b> may comprise a survey/polling element <b>1301</b>E. The survey/polling element <b>1301</b>E may comprise a plurality of surveys and/or polls of various forms. The surveys and/or polls may allow users of the client application <b>106</b> to submit votes, provide feedback, interact with administrators and/or presenters (e.g., for a live webinar), interact with the entity associated with the webinar (e.g., a company, person, website, etc.), a combination thereof, and/or the like.</p><p id="p-0126" num="0125">The user interface <b>1301</b> may comprise a plurality of customization elements <b>1301</b>F. The plurality of customization elements <b>1301</b>F may be associated with one or more customizable elements of the webinar, such as backgrounds, fonts, font sizes, color schemes, themes, patterns, combinations thereof, and/or the like. For example, the plurality of customization elements <b>1301</b>F may allow the webinar to be customized via the studio module <b>1304</b>. The plurality of customization elements <b>1301</b>F may be customized to enhance user interaction with any of the plurality of interface elements (e.g., &#x201c;widgets&#x201d;) described herein. For example, the plurality of customization elements <b>1301</b>F may comprise a plurality of control buttons associated with the webinar, such as playback controls (e.g., pause, FF, RWD, etc.), internal and/or external links (e.g., to content within the webinar and/or online), communication links (e.g., email links, chat room/box links), a combination thereof, and/or the like.</p><p id="p-0127" num="0126">Users may interact with the webinars via the user devices <b>102</b> and the client application <b>106</b>. User interaction with the webinars may be monitored by the client application <b>106</b>. For example, the user activity data <b>224</b> associated with the webinars provided by the presentation module <b>1300</b> may be monitored via the activity monitoring engine <b>220</b>. Examples of the user activity data <b>224</b> associated with the webinars includes, but is not limited to, interaction with the user interface <b>1301</b> (e.g., one or more of the elements <b>1301</b>A-<b>1301</b>F), interaction with the studio module <b>1304</b>, a duration of a webinar consumed (e.g., streamed, played), a duration of inactivity during a webinar (e.g., inactivity indicated by the user device <b>102</b>), a frequency or duration of movement (e.g., movement indicated by indicated by the user device <b>102</b>), a combination thereof, and/or the like. The user activity data <b>224</b> associated with the webinars may be provided to the analytics subsystem <b>142</b> via the activity monitoring engine <b>220</b>.</p><p id="p-0128" num="0127">As shown in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>, the presentation module <b>1300</b> may comprise a captioning module <b>1302</b>. The captioning module <b>1302</b> may receive user utterance data and/or audio data of a webinar. The user utterance data may comprise one or more words spoken by a presenter(s) (e.g., speaker(s)) and/or an attendee(s) of a webinar. The audio data may comprise audio portions of any media content provided during a webinar, such as an audio track(s) of video content played during a webinar. The captioning module <b>1302</b> may convert the user utterance data and/or the audio data into closed captioning/subtitles. For example, the captioning module <b>1302</b> may comprise&#x2014;or otherwise be in communication with&#x2014;an automated speech recognition engine (not shown).</p><p id="p-0129" num="0128">The automated speech recognition engine may process the user utterance data and output a transcription(s) of the one or more words spoken by the presenter(s) and/or the attendee(s) of the webinar in real-time or near real-time (e.g., for livestreamed content). Similarly, the automated speech recognition engine may process the audio data and output a transcription(s) of the audio portions of the media content provided during the webinar in real-time or near real-time (e.g., for livestreamed content). The captioning module <b>1302</b> may generate closed captioning/subtitles corresponding to the transcription(s) output by the automated speech recognition engine. The closed captioning/subtitles may be provided as an overlay <b>1302</b>A of a webinar, as shown in <figref idref="DRAWINGS">FIG. <b>13</b>C</figref>.</p><p id="p-0130" num="0129"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> shows a virtual environment module <b>1400</b>. The virtual environment module <b>1400</b> may be a component of the content management subsystem <b>140</b>. The virtual environment module <b>1400</b> may facilitate presentation of, and interactive with, a plurality of the media assets <b>166</b> in an interactive virtual environment <b>1401</b>, as shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>. For example, the virtual environment module <b>1400</b> may facilitate presentation of, and interactive with, a plurality of webinars at the user devices <b>102</b> via the client application <b>106</b> within the interactive virtual environment <b>1401</b>. For example, as described herein, the media assets <b>166</b> may comprise interactive webinars (e.g., web-based presentations, livestreams, webcasts, etc.) that may be provided via the client application <b>106</b> by the presentation module <b>1300</b> within the interactive virtual environment <b>1401</b>.</p><p id="p-0131" num="0130">As shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, the virtual environment module <b>1400</b> may comprise a plurality of presentation modules <b>1402</b>A, <b>1402</b>B, <b>1402</b>N. Each presentation module of the plurality of presentation modules <b>1402</b>A, <b>14028</b>, <b>1402</b>N may comprise an individual session, instance, virtualization, etc., of the presentation module <b>1300</b>. For example, the plurality of presentation modules <b>1402</b>A, <b>1402</b>B. <b>1402</b>N may comprise a plurality of simultaneous webinars (e.g., media assets <b>166</b>) that are provided by the presentation module <b>1300</b> and via the client application <b>106</b>. The virtual environment module <b>1400</b> may enable users of the user devices <b>102</b> to interact with each webinar via the interactive virtual environment <b>1401</b> and the client application <b>106</b>.</p><p id="p-0132" num="0131">Each of the plurality of presentation modules <b>1402</b>A, <b>1402</b>B, <b>1402</b>N may comprise a communication session/webinar, such as a chat room/box, an audio call/session, a video call/session, a combination thereof, and/or the like. As an example, and as further described herein, the interactive virtual environment <b>1401</b> may comprise a virtual conference/tradeshow, and each of the plurality of presentation modules <b>1402</b>A, <b>1402</b>B, <b>1402</b>N may comprise a communication session that may function as a virtual &#x201c;vendor booth&#x201d;, &#x201c;lounge&#x201d;, &#x201c;meeting room&#x201d;, &#x201c;auditorium&#x201d;, etc., at the virtual conference/tradeshow. In this way, the plurality of presentation modules <b>1402</b>A, <b>1402</b>B, <b>1402</b>N may enable users at the user devices <b>102</b> to communicate with other users and/or devices via the interactive virtual environment <b>1401</b> and the client application <b>106</b>.</p><p id="p-0133" num="0132">Users of the user devices <b>102</b> may interact with the interactive virtual environment <b>1401</b> via the client application. The service management subsystem <b>138</b> may administer (e.g., control) such interactions between the user devices <b>102</b> and the interactive virtual environment <b>1401</b>. For example, the service management subsystem <b>138</b> may generate a session identifier (or any other suitable identifier) for each of the communication sessions (e.g., webinars) or components thereof (e.g., chat rooms/boxes)&#x2014;within the interactive virtual environment <b>1401</b>. The service management subsystem <b>138</b> may use the session identifiers to ensure that only the user devices <b>102</b> associated with a particular communication session (e.g., via registration/sign-up, etc.) may interact with the particular communication session.</p><p id="p-0134" num="0133">As described herein, the media assets <b>166</b> may comprise interactive webinars (e.g., web-based presentations, livestreams, webcasts, etc.) that may be provided via the client application <b>106</b> by the presentation module <b>1300</b> within the interactive virtual environment <b>1401</b>. The media assets <b>166</b> may comprise linear content (e.g., live, real-time content) and/or on-demand (e.g., pre-recorded content). For example, the media assets <b>166</b> may be livestreamed within the interactive virtual environment <b>1401</b> according to a schedule of a corresponding virtual conference/tradeshow (e.g., a &#x201c;live&#x201d; conference/tradeshow). As another example, the media assets <b>166</b> corresponding to another virtual conference/tradeshow may be pre-recorded, and the media assets <b>166</b> may be accessible via the media repository <b>164</b> on-demand via the client application <b>106</b>. For virtual conferences/tradeshows that are not live or real-time (e.g., the corresponding media assets are pre-recorded), the interactive virtual environment <b>1401</b> may nevertheless allow a user(s) of a user device(s) <b>102</b> to interact with the virtual conference/tradeshow as if it were live or being held in real-time. As an example, the interactive virtual environment <b>1401</b> may allow the user(s) of the user device(s) <b>102</b> to interact with an on-demand virtual conference/tradeshow as if the user(s) were actually present when the corresponding communication sessions (e.g., webinars) were being held/recorded. In this way, the user(s) of the user device(s) <b>102</b> may interact with the on-demand virtual conference/tradeshow as an observer in simulated-real-time. The user(s) may navigate to different communication sessions of the on-demand virtual conference/tradeshow via the interactive virtual environment <b>1401</b>, and the user-experience may only be limited in that certain aspects, such as chat rooms/boxes, may not be available for direct interaction. The user(s) may navigate within the on-demand virtual conference/tradeshow via the interactive virtual environment <b>1401</b> in 1:1 simulated-real-time or in compressed/shifted time. For example, the user(s) may &#x201c;fast-forward&#x201d; or &#x201c;rewind&#x201d; to different portions of the on-demand virtual conference/tradeshow via the interactive virtual environment <b>1401</b>. In this way, the user(s) may be able to skip certain portions of a communication session and/or re-experience certain portions of a communication session of the on-demand virtual conference/tradeshow.</p><p id="p-0135" num="0134">As shown in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, the virtual environment module <b>1400</b> may comprise a studio module <b>1404</b>. The studio module <b>1404</b> may function like the studio module <b>1304</b> described herein. For example, the studio module <b>1404</b> may allow administrators and/or presenters of a virtual conference/tradeshow&#x2014;or a session/webinar thereof&#x2014;to record, livestream, and/or upload multimedia presentations/content for the virtual conference/tradeshow. The studio module <b>1404</b> may allow administrators and/or presenters of a virtual conference/tradeshow&#x2014;or a session/webinar thereof&#x2014;to customize the user experience using the template module <b>1304</b>A and the plurality of templates (e.g., layouts) stored in the storage repository <b>1304</b>B. For example, administrators and/or presenters of a virtual conference/tradeshow&#x2014;or a session/webinar thereof&#x2014;may use the studio module <b>1404</b> to select a template from the plurality of templates stored in the storage repository <b>1304</b>B. The studio module <b>1404</b> may store/save any customization and/or selection made within the studio module <b>1404</b> to the storage repository <b>1304</b>B.</p><p id="p-0136" num="0135">User interaction with virtual conferences/tradeshows via the interactive virtual environment <b>1401</b>, whether the virtual conferences/tradeshows are real-time or on-demand, may be monitored by the client application <b>106</b>. For example, user interaction with virtual conferences/tradeshows via the interactive virtual environment <b>1401</b> may be monitored via the activity monitoring engine <b>220</b> and stored as user activity data <b>224</b>. The user activity data <b>224</b> associated with the virtual conferences/tradeshows may include, as an example, interaction with the user interface <b>1301</b> (e.g., one or more of the elements <b>1301</b>A-<b>401</b>F) within a particular communication session/webinar. As another example, the user activity data <b>224</b> associated with the virtual conferences/tradeshows may include interaction with the studio module <b>1404</b>. Further examples of the user activity data <b>224</b> associated with the virtual conferences/tradeshows include, but are not limited to, a duration of a communication session/webinar consumed (e.g., streamed, played), a duration of inactivity during a communication session/webinar (e.g., inactivity indicated by the user device <b>102</b>), a frequency or duration of movement (e.g., movement indicated by indicated by the user device <b>102</b>), a combination thereof, and/or the like. The user activity data <b>224</b> associated with the virtual conferences/tradeshows may be provided to the analytics subsystem <b>142</b> via the activity monitoring engine <b>220</b>.</p><p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> shows an example lobby <b>1405</b> of a virtual conference/tradeshow within the interactive virtual environment <b>1401</b>. The interactive virtual environment <b>1401</b> provided via the client application <b>106</b> may enable a visual, audible, and/or physical interaction between the users of the user devices <b>102</b> and areas/events within a virtual conference/tradeshow, as indicated by the lobby <b>1405</b>. For example, as shown in the lobby <b>1405</b> in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, the interactive virtual environment <b>1401</b> may provide the users of the user devices <b>102</b> with a rendered scene of a virtual conference/tradeshow. As discussed above, the interactive virtual environment <b>1401</b> may allow the users of the user devices <b>102</b> to interact with the virtual conference/tradeshow in real-time or on-demand. The manner in which the users of the user devices <b>102</b> interact with the virtual conference/tradeshow may correspond to capabilities of the user devices <b>102</b>. For example, if a particular user device <b>102</b> is a smart phone, user interaction may be facilitated by a user interacting with a touch screen of the smart phone. As another example, if a particular user device <b>102</b> is a computer or gaming console, user interaction may be facilitated by a user via a keyboard, mouse, and/or a gaming controller. Other examples are possible as well. The user devices <b>102</b> may include additional components that enable user interaction, such as sensors, cameras, speakers, etc. The interactive virtual environment <b>1401</b> of a virtual conference/tradeshow may be presented via the client application <b>106</b> in various formats such as, for example, two-dimensional or three-dimensional visual displays (including projections), sound, haptic feedback, and/or tactile feedback. The interactive virtual environment <b>1401</b> may comprise, for example, portions using augmented reality, virtual reality, a combination thereof, and/or the like.</p><p id="p-0138" num="0137">A user may interact with the lobby <b>1405</b> via the interactive virtual environment <b>1401</b> and the user interface(s) <b>1301</b> of the client application <b>106</b>. As an example, as shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>, the lobby <b>1405</b> may allow a user to navigate to a virtual attendee lounge <b>1405</b>A, meeting rooms <b>1405</b>B, a plurality of presentations <b>1405</b>C at a virtual auditorium (&#x201c;Center Stage&#x201d;) <b>1405</b>D, an information desk <b>1405</b>E, and breakout sessions <b>1405</b>F. The virtual attendee lounge <b>1405</b>A, the meeting rooms <b>1405</b>B, each of the plurality of presentations <b>1405</b>C at the virtual auditorium <b>1405</b>D, the information desk <b>1405</b>D, and the breakout sessions <b>1405</b>F may be facilitated by the virtual environment module <b>1400</b> and the plurality of presentation modules <b>1402</b>A, <b>1402</b>B, <b>1402</b>N.</p><p id="p-0139" num="0138">The presentation module <b>1402</b>A may be associated with a first part of the virtual conference/tradeshow, such as the virtual attendee lounge <b>1405</b>A, the presentation module <b>1402</b>B may be associated with another part of the virtual conference/tradeshow, such one or more of the breakout sessions <b>1405</b>F, and the presentation module <b>1402</b>N may be associated with a further part of the virtual conference/tradeshow, such as one or more of the plurality of presentations <b>1405</b>C in the virtual auditorium (&#x201c;Center Stage&#x201d;) <b>1405</b>D. As an example, a user may choose to view one of the plurality of presentations <b>1405</b>C. As discussed herein, the user device(s) <b>102</b> may be smart phones, in which case the user may touch an area of a screen of the smart phone displaying the particular presentation of the plurality of presentations <b>1405</b>C he or she wishes to view. The presentation module <b>1402</b>N may receive a request from the smart phone via the client device <b>106</b> indicating that the user wishes to view the particular presentation. The presentation module <b>1402</b>N may cause the smart phone, via the client application <b>106</b>, to render a user interface associated with the particular presentation, such as the user interface <b>1301</b>. The user may view the particular presentation and interact therewith via the user interface in a similar manner as described herein with respect to the user interface <b>1301</b>. The user interface associated with the presentation may comprise an exit option, such as a button (e.g., a customization element <b>1301</b>F), which may cause the smart phone, via the client application <b>106</b>, to &#x201c;leave&#x201d; the presentation and &#x201c;return&#x201d; the user to the lobby <b>1405</b>. For example, the user may press on an area of the smart phone's screen displaying the exit option/button, and the presentation module <b>1402</b>N may cause the smart phone, via the client application <b>106</b>, to render the lobby <b>1405</b> (e.g., &#x201c;returning&#x201d; the user to the lobby of the virtual conference/tradeshow).</p><p id="p-0140" num="0139">In some embodiments, the analytics subsystem <b>142</b> also can determine digital content (e.g., media assets) that is/are like other digital content that is present in a corpus digital content for a user device (e.g., associated with a user profile/UIC). For example, the analytics subsystem <b>142</b> can generate a recommendation for the similar content and can then send the recommendation to a user device.</p><p id="p-0141" num="0140"><figref idref="DRAWINGS">FIGS. <b>15</b>A and <b>15</b>B</figref> show example systems for improved model selection and content recommendations. As described herein, a third-party subsystem <b>610</b> can include various type of subsystems that permit first-person insights generated by the analytics subsystem <b>142</b> to be extracted and leveraged across business systems of a source platform. Each third-party subsystem <b>610</b> may be referred to herein as a &#x201c;client system&#x201d; or simply as a &#x201c;client&#x201d;. The presentation platform described herein may access, control, etc., each of the third-party subsystems <b>610</b>. <figref idref="DRAWINGS">FIG. <b>15</b>A</figref> shows an example system for providing a recommendation for similar content as described herein. Each third-party subsystem <b>610</b> may be associated with a unique client model <b>1550</b>, and each unique client model <b>1550</b> may be maintained by the analytics subsystem <b>142</b> (e.g., each client model <b>1550</b> may be accessible by the analytics subsystem <b>142</b>). Each client model <b>1550</b> may comprise at least one of: the feature extraction unit <b>210</b>, the activity monitoring unit <b>220</b>, the scoring unit <b>230</b>, the scoring model(s) <b>248</b>, or the profile generation unit <b>250</b> as described herein. Each client model <b>1550</b> may be customized based on a number of media assets (e.g., digital content) the particular client has uploaded/produced for distribution by the present distribution platform.</p><p id="p-0142" num="0141">The system shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref> may relate to an &#x201c;online&#x201d; or &#x201c;live&#x201d; service. For example, the &#x201c;online&#x201d; or &#x201c;live&#x201d; service may provide a recommendation for content (e.g., a media asset(s)) in real-time as users associated with the particular client are engaging with the client application <b>106</b>. The system shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref> may relate to &#x201c;offline&#x201d; services. For example, the &#x201c;offline&#x201d; services may be used to update UICs and each client model <b>1550</b> periodically or based on a triggering event as described herein.</p><p id="p-0143" num="0142">As shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, a user <b>1510</b> may generate activity data <b>1520</b> (&#x201c;visited contents) indicative of a plurality of engagements with a plurality of media assets via a user device associated with the user <b>1510</b> (e.g., via the client application <b>106</b>). The analytics subsystem <b>142</b> may receive the activity data <b>1520</b> via the client application <b>106</b> executing on the user device. The analytics subsystem <b>142</b> may generate a user interest cloud <b>1530</b> (business interest cloud, or &#x201c;BIC&#x201d;) associated with the user device. For example, the analytics subsystem <b>142</b> may generate the user interest cloud <b>1530</b> based on the activity data <b>1520</b>. The user interest cloud <b>1530</b> may include at least one content feature of a plurality of content features as well as at least one interest attribute of a plurality of interest attributes, as described herein. The plurality of content features and the plurality of interest attributes may be associated with each media asset of the plurality media assets. The client model <b>1550</b> (e.g., a machine learning model) may determine at least one content recommendation <b>1560</b> based on the user interest cloud <b>1530</b> and the activity data <b>1520</b>. The client model <b>1550</b> may have been trained using historical user activity data, as further described herein. The client model <b>1550</b> may determine the at least one content recommendation <b>1560</b> based on activity/interest data <b>1540</b> associated with other users. For example, the activity/interest data <b>1540</b> may comprise activity data for one or more other users who engaged with one or more media assets that the user <b>1510</b> engaged with. As another example, the activity/interest data <b>1540</b> may comprise UICs for other users that are similar to the UIC of the user <b>1510</b> in terms of interests, etc. The analytics subsystem <b>142</b> may select the client model <b>1550</b> based on the particular client associated with the user <b>1510</b>. In this way, the present distribution platform, may use both a content-based filtering (CBF) and collaborative filtering (CF) approach when determining the at least one content recommendation <b>1560</b>.</p><p id="p-0144" num="0143">In another example, the client model <b>1550</b> may not be client-specific. For example, the client model <b>1550</b> may comprise a first machine learning model of a plurality of machine learning models. The plurality of machine learning models may be associated with a plurality of client identifiers. The analytics subsystem <b>142</b> may select the client model <b>1550</b> from the plurality of machine learning models based on a size and dispersion of the activity data for each client.</p><p id="p-0145" num="0144">Additionally, or in the alternative, the analytics subsystem <b>142</b> may select the client model <b>1550</b> from the plurality of machine learning models based on clusters of clients with similar characteristics. For example, the particular client associated with the user <b>1510</b> may be associated with a first client identifier of a plurality of client identifiers. The analytics subsystem <b>142</b> may determine a first client interest cloud associated with the first client identifier. The analytics subsystem <b>142</b> may also determine a plurality of client interest clouds associated with each remaining client identifier of the plurality of client identifiers. Each client interest cloud may be based on a corresponding plurality of media assets (e.g., media assets associated with each client identifier) and corresponding activity data (e.g., activity data associated with each client identifier). The corresponding activity data may be indicative of corresponding engagements of user devices associated with the corresponding client identifier. Based on the first client interest cloud and the plurality of client interest clouds, the analytics subsystem <b>142</b> may determine a plurality of client clusters. The analytics subsystem <b>142</b> may determine the plurality of client clusters using an unsupervised machine learning model. Each client cluster may be indicative of one or more client identifiers that are associated with similar client interest clouds. For example, one or more client identifiers may be associated with similar client interest clouds based on similar media assets offered by each client (e.g., based on content features, interest attributes, etc.) and/or similar engagements associated with each client (e.g., based on user interactions).</p><p id="p-0146" num="0145">The analytics subsystem <b>142</b> may select the client model <b>1550</b> based on the particular client cluster in which the first client identifier is clustered/associated with. For example, the first client identifier may be part of a first client cluster comprising at least one additional client identifier. The first client identifier and the at least one additional client identifier may each be associated with separate machine learning models. The analytics subsystem <b>142</b> may select one of the separate machine learning models as the client model <b>1550</b> based on an Area Under the Curve (AUC) calculation and/or a Mean Average Recall at K (MAR@K) calculation, where K is defined depending on the cluster. Additionally, or in the alternative, the analytics subsystem <b>142</b> may select one of the separate machine learning models as the client model <b>1550</b> based on a particular use case. For example, the analytics subsystem <b>142</b> may select the client model <b>1550</b> from the plurality of machine learning models based on a recommendation type associated with the at least one content recommendation. The recommendation type may comprise, for example, a media asset recommendation, an engagement recommendation, a combination thereof, and/or the like.</p><p id="p-0147" num="0146">The client model <b>1550</b> may determine the at least one content recommendation <b>1560</b> based on the plurality of engagements (e.g., activity data <b>1520</b>) and the user interest cloud <b>1530</b>. The at least one content recommendation <b>1560</b> may include&#x2014;or be associated with&#x2014;at least one media asset of the plurality of media assets. The client model <b>1550</b> may then cause the user device to output the at least one content recommendation <b>1560</b>. For example, the client model <b>1550</b> may cause the client application <b>106</b> to output (e.g., present, display, show, etc.) a media asset associated with the at least one content recommendation <b>1560</b>.</p><p id="p-0148" num="0147">The at least one content recommendation <b>1560</b> may be accompanied by a certainty match (e.g., cosine similarity between two embedding vectors). As long as the certainty match is above a configurable threshold (e.g., 0.8), it may be considered suitable for presentation to the user <b>1510</b>. In some cases, multiple recommendations <b>1560</b> may be presented, and in others, only one may be presented. These recommendations <b>1560</b> may also be filtered for content the user <b>1510</b> has already encountered in the past, so the user <b>1510</b> is not provided with a recommendation <b>1560</b> that includes content with which the user <b>1510</b> has already engaged.</p><p id="p-0149" num="0148">As described herein, the analytics subsystem <b>142</b> may retrain the machine learning model. For example, the analytics subsystem <b>142</b> may retrain the client model <b>1550</b> on a periodic basis or in response to a triggering event. The analytics subsystem <b>142</b> may receive further activity data <b>1580</b> associated with a plurality of users <b>1570</b> of a client <b>1560</b> (e.g., a third-party subsystem <b>610</b>). The further activity data <b>1580</b> may be indicative of at least one further engagement of a user device associated with at least one of the user <b>1570</b> with at least one further media asset of a plurality media assets <b>1590</b>.</p><p id="p-0150" num="0149">The analytics subsystem <b>142</b> may update the user interest clouds corresponding to the users <b>1570</b> based on the further activity data <b>1580</b>. The user interest clouds may each be indicative of a level of interest associated with each media asset of the plurality of media assets <b>1590</b>. The analytics subsystem <b>142</b> may determine at least one triggering event associated with the client application <b>106</b>. For example, the analytics subsystem <b>142</b> may determine (e.g., identify/detect) the at least one triggering event based on a threshold quantity of time that the client application <b>106</b> is inactive at one or more of the user devices associated with the users <b>1570</b>. As another example, the analytics subsystem <b>142</b> may determine the at least one triggering event based on an expiration of a quantity of time since the client model <b>1550</b> was trained (e.g., a predetermined/preset amount of time). As a further example, the analytics subsystem <b>142</b> may determine the at least one triggering event based on a threshold quantity of new media assets <b>1590</b> associated with the client application <b>106</b> and/or the particular client <b>1560</b>. In still a further example, the analytics subsystem <b>142</b> may determine the at least one triggering event based on the plurality of further activity data <b>1580</b> (e.g., new activity data). In response to determining the at least one triggering event, the analytics subsystem <b>142</b> may retain the client model <b>1550</b>.</p><p id="p-0151" num="0150">Any of the machine learning models or scoring models described herein, such as the scoring models <b>248</b> or the client model <b>1550</b>, may be trained and/or retrained using training datasets comprising user activity data and/or UICs. The training datasets may comprise UICs associated with users who interacted with (e.g., engaged with) a plurality of media assets. The UICs that are used during training and/or retraining may comprise interest attributes, interest levels, functionality features, a content features, a combination thereof, and/or the like. A training module, such as the training module <b>1620</b> shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, may then determine which features in the UICs correlate with the particular features of the plurality of media assets. The machine learning models, once trained (or retrained as the case may be), may provide a recommendation for a user(s) and a media asset(s) based on the corresponding UIC(s) and the features of that media asset.</p><p id="p-0152" num="0151">Any of the machine learning models or scoring models described herein may be referred to as &#x201c;at least one machine learning model <b>1630</b>&#x201d; or simply the &#x201c;machine learning model <b>1630</b>&#x201d;, as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref> The at least one machine learning model <b>1630</b> may be trained by a system <b>1600</b> shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>. The system <b>1600</b> may be configured to use machine learning techniques to train, based on an analysis of one or more training datasets <b>1610</b>A-<b>1610</b>B by a training module <b>1620</b>, the at least one machine learning model <b>1630</b>. The at least one machine learning model <b>1630</b>, once trained, may be configured to determine a prediction that a media asset is of interest to a particular user or not of interest to the particular user. A dataset indicative of a plurality of media assets and a labeled (e.g., predetermined/known) prediction indicating whether the corresponding media assets are of interest to a particular user or not may be used by the training module <b>1620</b> to train the at least one machine learning model <b>1630</b>. Each of the plurality of media assets in the dataset may be associated with a plurality of features that are present within each corresponding media asset. The plurality of features and the labeled predictions may be used to train the at least one machine learning model <b>1630</b>.</p><p id="p-0153" num="0152">The training dataset <b>1610</b>A may comprise a first portion of the plurality of media assets in the dataset. Each media asset in the first portion may have a labeled (e.g., predetermined) prediction and one or more labeled features. The training dataset <b>1610</b>B may comprise a second portion of the plurality of media assets in the dataset. Each media asset in the second portion may have a labeled (e.g., predetermined) prediction and one or more labeled features. The plurality of media assets may be randomly assigned to the training dataset <b>1610</b>A, the training dataset <b>1610</b>B, and/or to a testing dataset. In some implementations, the assignment of media assets to a training dataset or a testing dataset may not be completely random. In this case, one or more criteria may be used during the assignment, such as ensuring that similar numbers of media assets with different predictions and/or features are in each of the training and testing datasets. In general, any suitable method may be used to assign the media assets to the training or testing datasets, while ensuring that the distributions of predictions and/or features are somewhat similar in the training dataset and the testing dataset.</p><p id="p-0154" num="0153">The training module <b>1620</b> may use the first portion and the second portion of the plurality of media assets to determine one or more features that are indicative of a high prediction. That is, the training module <b>1620</b> may determine which features present within the plurality of media assets are correlative with a high prediction. The one or more features indicative of a high prediction may be used by the training module <b>1620</b> to train the machine learning model <b>1630</b>. For example, the training module <b>1620</b> may train the machine learning model <b>1630</b> by extracting a feature set (e.g., one or more features) from the first portion in the training dataset <b>1610</b>A according to one or more feature selection techniques. The training module <b>1620</b> may further define the feature set obtained from the training dataset <b>1610</b>A by applying one or more feature selection techniques to the second portion in the training dataset <b>1610</b>B that includes statistically significant features of positive examples (e.g., high predictions) and statistically significant features of negative examples (e.g., low predictions). The training module <b>1620</b> may train the machine learning model <b>1630</b> by extracting a feature set from the training dataset <b>1610</b>B that includes statistically significant features of positive examples (e.g., high predictions) and statistically significant features of negative examples (e.g., low predictions).</p><p id="p-0155" num="0154">The training module <b>1620</b> may extract a feature set from the training dataset <b>1610</b>A and/or the training dataset <b>1610</b>B in a variety of ways. For example, the training module <b>1620</b> may extract a feature set from the training dataset <b>1610</b>A and/or the training dataset <b>1610</b>B using a classification model (e.g., a machine learning model). The training module <b>1620</b> may perform feature extraction multiple times, each time using a different feature-extraction technique. In one example, the feature sets generated using the different techniques may each be used to generate different machine learning models <b>1640</b>. For example, the feature set with the highest quality features (e.g., most indicative of interest or not of interest to a particular user(s)) may be selected for use in training. The training module <b>1620</b> may use the feature set(s) to build one or more machine learning models <b>1640</b>A-<b>1640</b>N that are configured to determine a prediction for a new, unseen media asset.</p><p id="p-0156" num="0155">The training dataset <b>1610</b>A and/or the training dataset <b>1610</b>B may be analyzed to determine any dependencies, associations, and/or correlations between features and the labeled predictions in the training dataset <b>1610</b>A and/or the training dataset <b>1610</b>B. The identified correlations may have the form of a list of features that are associated with different labeled predictions (e.g., of interest to a particular user vs. not of interest to a particular user). The term &#x201c;feature&#x201d;, as used herein, may refer to any characteristic of an item of data that may be used to determine whether the item of data falls within one or more specific categories or within a range. By way of example, the features described herein may comprise one or more features present within each of the media assets that may be correlative (or not correlative as the case may be) with a particular media asset being of interest to a particular user or not. As another example, the features described herein may comprise an interest attribute, an interest level, a functionality feature, or a content feature as further described and defined herein.</p><p id="p-0157" num="0156">A feature selection technique may comprise one or more feature selection rules. The one or more feature selection rules may comprise a feature occurrence rule. The feature occurrence rule may comprise determining which features in the training dataset <b>1610</b>A occur over a threshold number of times and identifying those features that satisfy the threshold as candidate features. For example, any features that appear greater than or equal to 5 times in the training dataset <b>1610</b>A may be considered as candidate features. Any features appearing less than, for example, 5 times may be excluded from consideration as a candidate feature. Other threshold numbers may be used as well.</p><p id="p-0158" num="0157">A single feature selection rule may be applied to select features or multiple feature selection rules may be applied to select features. The feature selection rules may be applied in a cascading fashion, with the feature selection rules being applied in a specific order and applied to the results of the previous rule. For example, the feature occurrence rule may be applied to the training dataset <b>1610</b>A to generate a first list of features. A final list of features may be analyzed according to additional feature selection techniques to determine one or more candidate feature groups (e.g., groups of features that may be used to determine a prediction). Any suitable computational technique may be used to identify the feature groups using any feature selection technique such as filter, wrapper, and/or embedded methods. One or more candidate feature groups may be selected according to a filter method. Filter methods include, for example, Pearson's correlation, linear discriminant analysis, analysis of variance (ANOVA), chi-square, combinations thereof, and the like. The selection of features according to filter methods are independent of any machine learning algorithms used by the system <b>1600</b>. Instead, features may be selected on the basis of scores in various statistical tests for their correlation with the outcome variable (e.g., a prediction).</p><p id="p-0159" num="0158">As another example, one or more candidate feature groups may be selected according to a wrapper method. A wrapper method may be configured to use a subset of features and train the machine learning model <b>1630</b> using the subset of features. Based on the inferences that may be drawn from a previous model, features may be added and/or deleted from the subset. Wrapper methods include, for example, forward feature selection, backward feature elimination, recursive feature elimination, combinations thereof, and the like. For example, forward feature selection may be used to identify one or more candidate feature groups. Forward feature selection is an iterative method that begins with no features. In each iteration, the feature which best improves the model is added until an addition of a new variable does not improve the performance of the model. As another example, backward elimination may be used to identify one or more candidate feature groups. Backward elimination is an iterative method that begins with all features in the model. In each iteration, the least significant feature is removed until no improvement is observed on removal of features. Recursive feature elimination may be used to identify one or more candidate feature groups. Recursive feature elimination is a greedy optimization algorithm which aims to find the best performing feature subset. Recursive feature elimination repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. Recursive feature elimination constructs the next model with the features remaining until all the features are exhausted. Recursive feature elimination then ranks the features based on the order of their elimination.</p><p id="p-0160" num="0159">As a further example, one or more candidate feature groups may be selected according to an embedded method. Embedded methods combine the qualities of filter and wrapper methods. Embedded methods include, for example, Least Absolute Shrinkage and Selection Operator (LASSO) and ridge regression which implement penalization functions to reduce overfitting. For example, LASSO regression performs L1 regularization which adds a penalty equivalent to absolute value of the magnitude of coefficients and ridge regression performs L2 regularization which adds a penalty equivalent to square of the magnitude of coefficients.</p><p id="p-0161" num="0160">After the training module <b>1620</b> has generated a feature set(s), the training module <b>1620</b> may generate the one or more machine learning models <b>1640</b>A-<b>1640</b>N based on the feature set(s). A machine learning model (e.g., any of the one or more machine learning models <b>1640</b>A-<b>1640</b>N) may refer to a complex mathematical model for data classification that is generated using machine-learning techniques as described herein. In one example, a machine learning model may include a map of support vectors that represent boundary features. By way of example, boundary features may be selected from, and/or represent the highest-ranked features in, a feature set.</p><p id="p-0162" num="0161">The training module <b>1620</b> may use the feature sets extracted from the training dataset <b>1610</b>A and/or the training dataset <b>1610</b>B to build the one or more machine learning models <b>1640</b>A-<b>1640</b>N for each classification category (e.g., &#x201c;of interest to a particular user media asset&#x201d; and &#x201c;not of interest to the particular user media asset&#x201d;). In some examples, the one or more machine learning models <b>1640</b>A-<b>340</b>N may be combined into a single machine learning model <b>1640</b> (e.g., an ensemble model). Similarly, the machine learning model <b>1630</b> may represent a single classifier containing a single or a plurality of machine learning models <b>1640</b> and/or multiple classifiers containing a single or a plurality of machine learning models <b>1640</b> (e.g., an ensemble classifier).</p><p id="p-0163" num="0162">The extracted features (e.g., one or more candidate features) may be combined in the one or more machine learning models <b>1640</b>A-<b>1640</b>N that are trained using a machine learning approach such as discriminant analysis; decision tree; a nearest neighbor (NN) algorithm (e.g., k-NN models, replicator NN models, etc.); statistical algorithm (e.g., Bayesian networks, etc.); clustering algorithm (e.g., k-means, mean-shift, etc.); neural networks (e.g., reservoir networks, artificial neural networks, etc.); support vector machines (SVMs); logistic regression algorithms; linear regression algorithms; Markov models or chains; principal component analysis (PCA) (e.g., for linear models); multi-layer perceptron (MLP) ANNs (e.g., for non-linear models); replicating reservoir networks (e.g., for non-linear models, typically for time series); random forest classification; a combination thereof and/or the like. The resulting machine learning model <b>1630</b> may comprise a decision rule or a mapping for each candidate feature in order to assign a prediction to a class (e.g., of interest to a particular user vs. not of interest to the particular user). As described herein, the machine learning model <b>1630</b> may be used to determine predictions for media assets. The candidate features and the machine learning model <b>1630</b> may be used to determine predictions for media assets in the testing dataset (e.g., a third portion of the plurality of media assets).</p><p id="p-0164" num="0163"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a flowchart illustrating an example training method <b>1600</b> for generating the machine learning model <b>1630</b> using the training module <b>1620</b>. The training module <b>1620</b> may implement supervised, unsupervised, and/or semi-supervised (e.g., reinforcement based) machine learning models <b>1640</b>A-<b>1640</b>N. The method <b>1700</b> illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> is an example of a unsupervised learning method; variations of this example of training method are discussed below, however, other training methods may be analogously implemented to train unsupervised and/or semi-supervised machine learning models. The method <b>1700</b> may be implemented by any one of the devices, components, units, or modules shown in <figref idref="DRAWINGS">FIG. <b>1</b>-<b>3</b>, <b>5</b>-<b>7</b>, <b>9</b></figref>, or <b>12</b>-<b>16</b>.</p><p id="p-0165" num="0164">At step <b>1710</b>, the training method <b>1700</b> may determine (e.g., access, receive, retrieve, etc.) first media assets and second media assets. The first media assets and the second media assets may each comprise one or more features and a predetermined prediction (e.g., a recommendation). The training method <b>1700</b> may generate, at step <b>1720</b>, a training dataset and a testing dataset. The training dataset and the testing dataset may be generated by randomly assigning media assets from the first media assets and/or the second media assets to either the training dataset or the testing dataset. In some implementations, the assignment of media assets as training or test samples may not be completely random. As an example, only the media assets for a specific feature(s) and/or range(s) of predetermined predictions may be used to generate the training dataset and the testing dataset. As another example, a majority of the media assets for the specific feature(s) and/or range(s) of predetermined predictions may be used to generate the training dataset. For example, 75% of the media assets for the specific feature(s) and/or range(s) of predetermined predictions may be used to generate the training dataset and 25% may be used to generate the testing dataset.</p><p id="p-0166" num="0165">The training method <b>1700</b> may determine (e.g., extract, select, etc.), at step <b>1730</b>, one or more features that may be used by, for example, a classifier to differentiate among different classifications (e.g., predictions/recommendations). The one or more features may comprise a set of features. As an example, the training method <b>1700</b> may determine a set features from the first media assets. As another example, the training method <b>1700</b> may determine a set of features from the second media assets. In a further example, a set of features may be determined from other media assets of the plurality of media assets (e.g., a third portion) associated with a specific feature(s) and/or range(s) of predetermined predictions that may be different than the specific feature(s) and/or range(s) of predetermined predictions associated with the media assets of the training dataset and the testing dataset. In other words, the other media assets (e.g., the third portion) may be used for feature determination/selection, rather than for training. The training dataset may be used in conjunction with the other media assets to determine the one or more features. The other media assets may be used to determine an initial set of features, which may be further reduced using the training dataset.</p><p id="p-0167" num="0166">The training method <b>1700</b> may train one or more machine learning models (e.g., one or more machine learning models, neural networks, deep-learning models, etc.) using the one or more features at step <b>1740</b>. In one example, the machine learning models may be trained using supervised learning. In another example, other machine learning techniques may be used, including unsupervised learning and semi-supervised. The machine learning models trained at step <b>1740</b> may be selected based on different criteria depending on the problem to be solved and/or data available in the training dataset. For example, machine learning models may suffer from different degrees of bias. Accordingly, more than one machine learning model may be trained at <b>1740</b>, and then optimized, improved, and cross-validated at step <b>1750</b>.</p><p id="p-0168" num="0167">The training method <b>1700</b> may select one or more machine learning models to build the machine learning model <b>1630</b> at step <b>1760</b>. The machine learning model <b>1630</b> may be evaluated using the testing dataset. The machine learning model <b>1630</b> may analyze the testing dataset and generate classification values and/or predicted values (e.g., predictions) at step <b>1770</b>. Classification and/or prediction values may be evaluated at step <b>1780</b> to determine whether such values have achieved a desired accuracy level. Performance of the machine learning model <b>1630</b> may be evaluated in a number of ways based on a number of true positives, false positives, true negatives, and/or false negatives classifications of the plurality of data points indicated by the machine learning model <b>1630</b>.</p><p id="p-0169" num="0168">For example, the false positives of the machine learning model <b>1630</b> may refer to a number of times the machine learning model <b>1630</b> incorrectly assigned a high prediction to a media asset associated with a low predetermined prediction. Conversely, the false negatives of the machine learning model <b>1630</b> may refer to a number of times the machine learning model assigned a low prediction to a media asset associated with a high predetermined prediction. True negatives and true positives may refer to a number of times the machine learning model <b>1630</b> correctly assigned predictions to media assets based on the known, predetermined prediction for each media asset. Related to these measurements are the concepts of recall and precision. Generally, recall refers to a ratio of true positives to a sum of true positives and false negatives, which quantifies a sensitivity of the machine learning model <b>1630</b>. Similarly, precision refers to a ratio of true positives a sum of true and false positives. When such a desired accuracy level is reached, the training phase ends and the machine learning model <b>1630</b> may be output at step <b>1790</b>; when the desired accuracy level is not reached, however, then a subsequent iteration of the training method <b>1700</b> may be performed starting at step <b>1610</b> with variations such as, for example, considering a larger collection of media assets. The machine learning model <b>1630</b> may be output at step <b>1790</b>. The machine learning model <b>1630</b> may be configured to determine predicted predictions for media assets that are not within the plurality of media assets used to train the machine learning model.</p><p id="p-0170" num="0169">As discussed herein, the present methods and systems may be computer-implemented. <figref idref="DRAWINGS">FIG. <b>18</b></figref> shows a block diagram depicting an environment <b>1800</b> comprising non-limiting examples of a computing device <b>1801</b> and a server <b>1802</b> connected through a network <b>1804</b>, such as the network <b>106</b>. The computing device <b>1801</b> and/or the server <b>1802</b> may be any one of the devices, components, units, or modules shown in <figref idref="DRAWINGS">FIG. <b>1</b>-<b>3</b>, <b>5</b>-<b>7</b>, <b>9</b></figref>, or <b>12</b>-<b>16</b>. In an aspect, some or all steps of any described method herein may be performed on a computing device as described herein. The computing device <b>1801</b> may comprise one or multiple computers configured to store one or more of a machine learning module <b>1820</b>, content and user data <b>1815</b>, and the like. The server <b>1802</b> may comprise one or multiple computers configured to store one or more of the machine learning module <b>1820</b>, the content and user data <b>1815</b>, and the like. Multiple servers <b>1802</b> may communicate with the computing device <b>1801</b> via the through the network <b>1804</b>.</p><p id="p-0171" num="0170">The computing device <b>1801</b> and the server <b>1802</b> may each be a digital computer that, in terms of hardware architecture, generally includes a processor <b>1808</b>, memory system <b>1810</b>, input/output (I/O) interfaces <b>1812</b>, and network interfaces <b>1814</b>. These components (<b>608</b>, <b>1810</b>, <b>1812</b>, and <b>1814</b>) are communicatively coupled via a local interface <b>1816</b>. The local interface <b>1816</b> may be, for example, but not limited to, one or more buses or other wired or wireless connections, as is known in the art. The local interface <b>1816</b> may have additional elements, which are omitted for simplicity, such as controllers, buffers (caches), drivers, repeaters, and receivers, to enable communications. Further, the local interface may include address, control, and/or data connections to enable appropriate communications among the aforementioned components.</p><p id="p-0172" num="0171">The processor <b>1808</b> may be a hardware device for executing software, particularly that stored in memory system <b>1810</b>. The processor <b>1808</b> may be any custom made or commercially available processor, a central processing unit (CPU), an auxiliary processor among several processors associated with the computing device <b>1801</b> and the server <b>1802</b>, a semiconductor-based microprocessor (in the form of a microchip or chip set), or generally any device for executing software instructions. When the computing device <b>1801</b> and/or the server <b>1802</b> is in operation, the processor <b>1808</b> may be configured to execute software stored within the memory system <b>1810</b>, to communicate data to and from the memory system <b>1810</b>, and to generally control operations of the computing device <b>1801</b> and the server <b>1802</b> pursuant to the software.</p><p id="p-0173" num="0172">The I/O interfaces <b>1812</b> may be used to receive user input from, and/or for providing system output to, one or more devices or components. User input may be received via, for example, a keyboard and/or a mouse. System output may comprise a display device and a printer (not shown). I/O interfaces <b>1812</b> may include, for example, a serial port, a parallel port, a Small Computer System Interface (SCSI), an infrared (I R) interface, a radio frequency (R F) interface, and/or a universal serial bus (USB) interface.</p><p id="p-0174" num="0173">The network interface <b>1814</b> may be used to transmit and receive from the computing device <b>1801</b> and/or the server <b>1802</b> on the network <b>1804</b>. The network interface <b>1814</b> may include, for example, a 10BaseT Ethernet Adaptor, a 100BaseT Ethernet Adaptor, a LAN PHY Ethernet Adaptor, a Token Ring Adaptor, a wireless network adapter (e.g., WiFi, cellular, satellite), or any other suitable network interface device. The network interface <b>1814</b> may include address, control, and/or data connections to enable appropriate communications on the network <b>1804</b>.</p><p id="p-0175" num="0174">The memory system <b>1810</b> may include any one or combination of volatile memory elements (e.g., random access memory (RAM, such as DRAM, SRAM, SDRAM, etc.)) and nonvolatile memory elements (e.g., ROM, hard drive, tape, CDROM, DVDROM, etc.). Moreover, the memory system <b>1810</b> may incorporate electronic, magnetic, optical, and/or other types of storage media. Note that the memory system <b>1810</b> may have a distributed architecture, where various components are situated remote from one another, but may be accessed by the processor <b>1808</b>.</p><p id="p-0176" num="0175">The software in memory system <b>1810</b> may include one or more software programs, each of which comprises an ordered listing of executable instructions for implementing logical functions. In the example of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the software in the memory system <b>1810</b> of the computing device <b>1801</b> may comprise the training module <b>1620</b> (or subcomponents thereof), the training data <b>320</b>, and a suitable operating system (O/S) <b>1818</b>. In the example of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the software in the memory system <b>1810</b> of the server <b>1802</b> may comprise, the video data <b>1824</b>, and a suitable operating system (O/S) <b>1818</b>. The operating system <b>1818</b> essentially controls the execution of other computer programs and provides scheduling, input-output control, file and data management, memory management, and communication control and related services.</p><p id="p-0177" num="0176">For purposes of illustration, application programs and other executable program components such as the operating system <b>1818</b> are illustrated herein as discrete blocks, although it is recognized that such programs and components may reside at various times in different storage components of the computing device <b>1801</b> and/or the server <b>1802</b>. An implementation of the training module <b>1620</b> may be stored on or transmitted across some form of computer readable media. Any of the disclosed methods may be performed by computer readable instructions embodied on computer readable media. Computer readable media may be any available media that may be accessed by a computer. By way of example and not meant to be limiting, computer readable media may comprise &#x201c;computer storage media&#x201d; and &#x201c;communications media&#x201d;. &#x201c;Computer storage media&#x201d; may comprise volatile and non-volatile, removable and non-removable media implemented in any methods or technology for storage of information such as computer readable instructions, data structures, program modules, or other data. Exemplary computer storage media may comprise RAM, ROM, EEPROM, flash memory or other memory technology, CD-ROM, digital versatile disks (DVD) or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other medium which may be used to store the desired information and which may be accessed by a computer.</p><p id="p-0178" num="0177"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows a flowchart of an example method <b>1900</b> for improved model selection and content recommendations. The method <b>1900</b> may be performed in whole or in part by a single computing device, a plurality of computing devices, and the like. For example, any one of the devices, components, units, or modules shown in <figref idref="DRAWINGS">FIG. <b>1</b>-<b>3</b>, <b>5</b>-<b>7</b>, <b>9</b></figref>, or <b>12</b>-<b>16</b> may be configured to perform the method <b>1900</b>. The computing device(s) that performs the steps of the method <b>1900</b> may comprise a machine learning model, such as the client model <b>1550</b>.</p><p id="p-0179" num="0178">At step <b>1910</b>, the computing device may receive first activity data. For example, the computing device may receive an indication of an interaction, via the first user device and the client application, of a user with at least one media asset associated a first client identifier of a plurality of client identifiers. The first activity data may be indicative of at least one first engagement of a first user device with a client application. The first user device may be associated with a first user profile of a plurality of user profiles. The at least one first engagement may be associated with the first client identifier.</p><p id="p-0180" num="0179">At step <b>1920</b>, the computing device may determine a first machine learning model of a plurality of machine learning models associated with the plurality of client identifiers. For example, the computing device may determine the first machine learning model based on the first client identifier and a first plurality of media assets associated with the first client identifier. The computing device may determine, for each remaining client identifier of the plurality of client identifiers, a plurality of client interest clouds. Each client interest cloud may be based on a corresponding plurality of media assets and corresponding activity data indicative of corresponding engagements of user devices associated the corresponding client identifier. The computing device may determine a plurality of client clusters. For example, the computing device may determine the plurality of client clusters using the first machine learning model, which may comprise an unsupervised machine learning model.</p><p id="p-0181" num="0180">The unsupervised machine learning model may determine the plurality of client clusters based on a first client interest cloud associated with the first client identifier as well as the plurality of client interest clouds associated with the remaining client identifiers. Each client cluster may be indicative of one or more client identifiers that are associated with similar client interest clouds. The computing device may determine a recommendation type associated with at least one content recommendation. The recommendation type may comprise at least one of: a media asset recommendation or an engagement recommendation. The first machine learning model may be selected by the based on the plurality of client clusters and the recommendation type. The first machine learning model may be associated with a first client cluster of the plurality of client clusters. The first client cluster may be indicative of the first client identifier and at least one additional client identifier.</p><p id="p-0182" num="0181">At step <b>1930</b>, the computing device may determine a subset of the plurality of user profiles associated with corresponding interest clouds that are similar to the first interest cloud. For example, the computing device may determine the subset of the plurality of user profiles based on a first interest cloud associated with the first user profile. Additionally, or in the alternative, the computing device may determine the subset of the plurality of user profiles based on the first client identifier. The subset of the plurality of user profiles may be associated with the first client identifier.</p><p id="p-0183" num="0182">At step <b>1940</b>, the computing device may determine the at least one content recommendation. For example, the computing device may determine the at least one content recommendation using the first machine learning model. The first machine learning model may determine the at least one content recommendation based on the at least one first engagement, the first interest cloud, and the corresponding interest clouds. At step <b>1950</b>, the computing device may cause the first user device to output the at least one content recommendation.</p><p id="p-0184" num="0183"><figref idref="DRAWINGS">FIG. <b>20</b></figref> shows a flowchart of an example method <b>2000</b> for improved model selection and content recommendations. The method <b>2000</b> may be performed in whole or in part by a single computing device, a plurality of computing devices, and the like. For example, any one of the devices, components, units, or modules shown in <figref idref="DRAWINGS">FIG. <b>1</b>-<b>3</b>, <b>5</b>-<b>7</b>, <b>9</b></figref>, or <b>12</b>-<b>16</b> may be configured to perform the method <b>2000</b>. The computing device(s) that performs the steps of the method <b>2000</b> may comprise a machine learning model, such as the client model <b>1550</b>.</p><p id="p-0185" num="0184">At step <b>2010</b>, the computing device may receive a plurality of activity data associated with a plurality of engagements of a plurality of user devices with a plurality media assets via a client application. The plurality of engagements may be associated with a plurality of client identifiers. At step <b>2020</b>, the computing device may determine a feature vector associated with each user device of the plurality of user devices. For example, the computing device may determine the feature vector based on the plurality of activity data and the plurality of engagements. Each feature vector associated with each user device of the plurality of user devices may comprise at least one content feature and at least one engagement feature associated with at least one media asset of the plurality media assets. The at least one content feature may comprise at least one: a content type, a content rating, content metadata, a date of creation, a content tag, a content category, a content filter, a language, or one or more words of a content description. The at least one engagement feature of each feature vector may comprise at least one of: a quantification of an engagement with the at least one media asset for a numerical weight associated with an engagement type.</p><p id="p-0186" num="0185">At step <b>2030</b>, the computing device may determine a plurality of user profiles and corresponding interest clouds for each user device of the plurality of user devices. For example, the computing device may determine the plurality of user profiles and corresponding interest clouds based on each feature vector associated with each user device of the plurality of user devices. The corresponding interest clouds may comprise the at least one content feature and an associated interest attribute.</p><p id="p-0187" num="0186">The computing device may determine the plurality of user profiles using a scoring model. For example, the computing device may determine the plurality of user profiles using the scoring model based on each feature vector associated with each user device of the plurality of user devices. The associated interest attribute for each of the corresponding interest clouds may comprise at least one of: a numerical indication of a level of interest associated with the at least one media asset or a textual indication of the level of interest associated with the at least one media asset. The computing device may determine the plurality of user profiles using the scoring model and the associated interest attribute for each of the corresponding interest clouds.</p><p id="p-0188" num="0187">At step <b>2040</b>, the computing device may train a plurality of machine learning models. For example, the computing device may train the plurality of machine learning models based on the plurality of user profiles and the corresponding interest clouds for each user device of the plurality of user devices. Each machine learning model of the plurality of machine learning models may be associated with a client identifier of the plurality of client identifiers. Each client identifier of the plurality of client identifiers may identify a unique instance of the client application comprising a plurality of unique media assets.</p><p id="p-0189" num="0188">At step <b>2050</b>, the computing device may determine at least one triggering event associated with the client application. The at least one triggering event may be one of a plurality of triggering events. For example, each triggering event of the plurality of triggering events may be associated with a client identifier of the plurality of client identifiers and a machine learning model of the plurality of machine learning models. The computing device may determine at least one triggering event based on a threshold quantity of time that the client application may be inactive at each user device of the plurality of user devices. As another example, the computing device may determine at least one triggering event based on an expiration of a quantity of time associated with training the plurality of machine learning models, the at least one triggering event. As another example, the computing device may determine at least one triggering event based on a threshold quantity of new media assets associated with the client application, the at least one triggering event. As another example, the computing device may determine at least one triggering event based on the plurality of further activity data, the at least one triggering event. As another example, the computing device may determine at least one triggering event based on the plurality of further engagements, the at least one triggering event.</p><p id="p-0190" num="0189">At step <b>2060</b>, the computing device may retrain the plurality of machine learning models. For example, the computing device may retrain the plurality of machine learning models based on the at least one triggering event. Additionally, or in the alternative, the computing device may retrain the plurality of machine learning models based on a plurality of further activity data associated with a plurality of further engagements of the plurality of user devices.</p><p id="p-0191" num="0190"><figref idref="DRAWINGS">FIG. <b>21</b></figref> shows a flowchart of an example method <b>2100</b> for improved model selection and content recommendations. The method <b>2100</b> may be performed in whole or in part by a single computing device, a plurality of computing devices, and the like. For example, any one of the devices, components, units, or modules shown in <figref idref="DRAWINGS">FIG. <b>1</b>-<b>3</b>, <b>5</b>-<b>7</b>, <b>9</b></figref>, or <b>12</b>-<b>16</b> may be configured to perform the method <b>2100</b>. The computing device(s) that performs the steps of the method <b>2100</b> may comprise a machine learning model (e.g., a machine learning model, such as the client model <b>1550</b>.</p><p id="p-0192" num="0191">At step <b>2110</b>, the computing device may receive first activity data indicative of at least one first engagement of a first user device with a client application and a client identifier. The first user device may be associated with a first user profile. The first activity data may comprise an indication of at least one interaction, via the first user device and the client application, of a user with at least one media asset.</p><p id="p-0193" num="0192">At step <b>2120</b>, the computing device may determine a first machine learning model. For example, the computing device may determine the first machine learning model based on the client identifier associated with the first activity data. At step <b>2130</b>, the computing device may determine a first content recommendation. For example, the computing device may determine the first content recommendation using the first machine learning model, first machine learning model may determine the first content recommendation based on the first user profile and the first activity data.</p><p id="p-0194" num="0193">At step <b>2140</b>, the computing device may determine at least one triggering event associated with the client application. The at least one triggering event may be one of a plurality of triggering events. For example, each triggering event of the plurality of triggering events may be associated with a client identifier of the plurality of client identifiers and a machine learning model of the plurality of machine learning models. The computing device may determine at least one triggering event based on a threshold quantity of time that the client application may be inactive at each user device of the plurality of user devices. As another example, the computing device may determine at least one triggering event based on an expiration of a quantity of time associated with training the plurality of machine learning models, the at least one triggering event. As another example, the computing device may determine at least one triggering event based on a threshold quantity of new media assets associated with the client application, the at least one triggering event. As another example, the computing device may determine at least one triggering event based on the plurality of further activity data, the at least one triggering event. As another example, the computing device may determine at least one triggering event based on the plurality of further engagements, the at least one triggering event.</p><p id="p-0195" num="0194">At step <b>2150</b>, the computing device may retrain the first machine learning model. For example, the computing device may retrain the first machine learning model based on the at least one triggering event. Additionally, or in the alternative, the computing device may retrain the first machine learning model based on further activity data associated with at least one further engagement of the first user device with the client application. The further activity data may comprise an indication of at least one interaction, via the first user device and the client application, of the user with at least one further media asset. At step <b>2160</b>, the computing device may determine a second content recommendation. For example, the computing device may determine the second content recommendation using the retrained first machine learning model. The retrained first machine learning model may determine the second content recommendation based on the first user profile and the further activity data. The first content recommendation may differ from the second content recommendation.</p><p id="p-0196" num="0195">While specific configurations have been described, it is not intended that the scope be limited to the particular configurations set forth, as the configurations herein are intended in all respects to be possible configurations rather than restrictive. Unless otherwise expressly stated, it is in no way intended that any method set forth herein be construed as requiring that its steps be performed in a specific order. Accordingly, where a method claim does not actually recite an order to be followed by its steps or it is not otherwise specifically stated in the claims or descriptions that the steps are to be limited to a specific order, it is in no way intended that an order be inferred, in any respect. This holds for any possible non-express basis for interpretation, including: matters of logic with respect to arrangement of steps or operational flow; plain meaning derived from grammatical organization or punctuation: the number or type of configurations described in the specification.</p><p id="p-0197" num="0196">It will be apparent to those skilled in the art that various modifications and variations may be made without departing from the scope or spirit. Other configurations will be apparent to those skilled in the art from consideration of the specification and practice described herein. It is intended that the specification and described configurations be considered as exemplary only, with a true scope and spirit being indicated by the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>receiving, by an analytics subsystem of a computing device, first activity data indicative of at least one first engagement of a first user device with a client application, wherein the first user device is associated with a first user profile of a plurality of user profiles, and wherein the at least one first engagement is associated with a first client identifier of a plurality of client identifiers;</claim-text><claim-text>determining, based on the first client identifier and a first plurality of media assets associated with the first client identifier, a first machine learning model of a plurality of machine learning models associated with the plurality of client identifiers;</claim-text><claim-text>determining, based on a first interest cloud associated with the first user profile, and based on the first client identifier, a subset of the plurality of user profiles associated with corresponding interest clouds that are similar to the first interest cloud, wherein the subset of the plurality of user profiles are associated with the first client identifier;</claim-text><claim-text>determining, by the first machine learning model, at least one content recommendation based on: the at least one first engagement, the first interest cloud, and the corresponding interest clouds; and</claim-text><claim-text>causing the first user device to output the at least one content recommendation.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein receiving the first activity data indicative of the at least one first engagement of the first user device with the client application comprises receiving an indication of an interaction, via the first user device and the client application, of a user with at least one media asset associated with the first client identifier.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving a plurality of activity data associated with a plurality of engagements of a plurality of user devices with the client application, wherein the plurality of engagements are associated with the plurality of client identifiers.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, further comprising:<claim-text>training, based on the plurality of engagements and the plurality of client identifiers, each machine learning model of the plurality of machine learning models.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the subset of the plurality of user profiles associated with the corresponding interest clouds that are similar to the first interest cloud comprises:<claim-text>determining, based on the first interest cloud, at least one content feature associated with at least one media asset associated with the first client identifier and the at least one first engagement, wherein the at least one content feature is associated with a first interest attribute;</claim-text><claim-text>determining, based on the plurality of user profiles and the at least one content feature, the subset of the plurality of user profiles, wherein the subset of the plurality of user profiles are associated with the corresponding interest clouds; and</claim-text><claim-text>determining, based on the corresponding interest clouds comprising the at least one content feature, that the corresponding interest clouds are similar to the first interest cloud.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining, based on the first client identifier and the first plurality of media assets associated with the first client identifier, the first machine learning model of the plurality of machine learning models associated with the plurality of client identifiers comprises:<claim-text>determining, for each remaining client identifier of the plurality of client identifiers, a plurality of client interest clouds, wherein each client interest cloud is based on a corresponding plurality of media assets and corresponding activity data indicative of corresponding engagements of user devices associated the corresponding client identifier; and</claim-text><claim-text>determining, by an unsupervised machine learning model, based on a first client interest cloud associated with the first client identifier, and based on the plurality of client interest clouds associated with the remaining client identifiers, a plurality of client clusters, wherein each client cluster is indicative of one or more client identifiers that are associated with similar client interest clouds.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining, based on the at least one content recommendation, a recommendation type, wherein the recommendation type comprises at least one of: a media asset recommendation or an engagement recommendation; and</claim-text><claim-text>determining, based on the plurality of client clusters and the recommendation type, the first machine learning model, wherein the first machine learning model is associated with a first client cluster of the plurality of client clusters, and wherein the first client cluster is indicative of the first client identifier and at least one additional client identifier.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A method comprising:<claim-text>receiving, by an analytics subsystem of a computing device, a plurality of activity data associated with a plurality of engagements of a plurality of user devices with a plurality media assets via a client application, wherein the plurality of engagements are associated with a plurality of client identifiers;</claim-text><claim-text>determining, based on the plurality of activity data and the plurality of engagements, a feature vector associated with each user device of the plurality of user devices, wherein each feature vector associated with each user device of the plurality of user devices comprises at least one content feature and at least one engagement feature associated with at least one media asset of the plurality media assets;</claim-text><claim-text>determining, based on each feature vector associated with each user device of the plurality of user devices, a plurality of user profiles and corresponding interest clouds for each user device of the plurality of user devices, wherein the corresponding interest clouds comprise the at least one content feature and an associated interest attribute;</claim-text><claim-text>training, based on the plurality of user profiles and the corresponding interest clouds for each user device of the plurality of user devices, a plurality of machine learning models, wherein each machine learning model of the plurality of machine learning models is associated with a client identifier of the plurality of client identifiers;</claim-text><claim-text>determining at least one triggering event associated with the client application; and</claim-text><claim-text>retraining, based on the at least one triggering event, and based on a plurality of further activity data associated with a plurality of further engagements of the plurality of user devices, the plurality of machine learning models.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein each client identifier of the plurality of client identifiers identifies a unique instance of the client application comprising a plurality of unique media assets.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one content feature comprises at least one: a content type, a content rating, content metadata, a date of creation, a content tag, a content category, a content filter, a language, or one or more words of a content description.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one engagement feature of each feature vector comprises at least one of: a quantification of an engagement with the at least one media asset for a numerical weight associated with an engagement type.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining the plurality of user profiles and corresponding interest clouds for each user device of the plurality of user devices comprises:<claim-text>determining, by a scoring model, based on each feature vector associated with each user device of the plurality of user devices, the associated interest attribute for each of the corresponding interest clouds, wherein the associated interest attribute for each of the corresponding interest clouds comprises at least one of: a numerical indication of a level of interest associated with the at least one media asset or a textual indication of the level of interest associated with the at least one media asset.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the at least one triggering event is one of a plurality of triggering events, wherein each triggering event of the plurality of triggering events is associated with a client identifier of the plurality of client identifiers and a machine learning model of the plurality of machine learning models.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining the at least one triggering event comprises at least one of:<claim-text>determining, based on a threshold quantity of time that the client application is inactive at each user device of the plurality of user devices, the at least one triggering event;</claim-text><claim-text>determining, based on an expiration of a quantity of time associated with training the plurality of machine learning models, the at least one triggering event;</claim-text><claim-text>determining, based on a threshold quantity of new media assets associated with the client application, the at least one triggering event;</claim-text><claim-text>determining, based on the plurality of further activity data, the at least one triggering event; or</claim-text><claim-text>determining, based on the plurality of further engagements, the at least one triggering event.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method comprising:<claim-text>receiving, by an analytics subsystem of a computing device, first activity data indicative of at least one first engagement of a first user device with a client application and a client identifier, wherein the first user device is associated with a first user profile;</claim-text><claim-text>determining, based on the client identifier associated with the first activity data, a first machine learning model;</claim-text><claim-text>determining, by the first machine learning model, based on the first user profile and the first activity data, a first content recommendation;</claim-text><claim-text>determining at least one triggering event associated with the client application;</claim-text><claim-text>retraining, based on the at least one triggering event, and based on further activity data associated with at least one further engagement of the first user device with the client application, the first machine learning model; and</claim-text><claim-text>determining, by the retrained first machine learning model, based on the first user profile and the further activity data, a second content recommendation.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first content recommendation differs from the second content recommendation.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first activity data comprises an indication of at least one interaction, via the first user device and the client application, of a user with at least one media asset.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the further activity data comprises an indication of at least one interaction, via the first user device and the client application, of the user with at least one further media asset.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>causing the first user device, via the client application, to output a user interface object associated with the first content recommendation or the second content recommendation;</claim-text><claim-text>causing a notification associated with the first content recommendation or the second content recommendation to be output at the first user device; or</claim-text><claim-text>causing the first user device, via the client application, to output at least one media asset associated with the first content recommendation or the second content recommendation.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining the at least one triggering event comprises at least one of:<claim-text>determining, based on a threshold quantity of time that the client application is inactive at the first user device, the at least one triggering event;</claim-text><claim-text>determining, based on an expiration of a quantity of time associated with training the first machine learning model, the at least one triggering event;</claim-text><claim-text>determining, based on a threshold quantity of new media assets associated with the client application, the at least one triggering event;</claim-text><claim-text>determining, based on the further activity data, the at least one triggering event; or</claim-text><claim-text>determining, based on the at least one further engagement, the at least one triggering event.</claim-text></claim-text></claim></claims></us-patent-application>