<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000376A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000376</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780988</doc-number><date>20201201</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>024</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>1455</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>02416</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>14551</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>021</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR PHYSIOLOGICAL MEASUREMENTS FROM OPTICAL DATA</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62942247</doc-number><date>20191202</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>BINAH.AI LTD</orgname><address><city>Tel Aviv</city><country>IL</country></address></addressbook><residence><country>IL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MAMAN</last-name><first-name>David</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>GEDALIN</last-name><first-name>Konstantin</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MARKZON</last-name><first-name>Michael</first-name><address><city>Tel Aviv</city><country>IL</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/IL2020/051238</doc-number><date>20201201</date></document-id><us-371c12-date><date>20220529</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A new system and method is provided for improving the accuracy of pulse rate detection. Various aspects contribute to the greater accuracy, including but not limited to pre-processing of the camera output/input, extracting the pulsatile signal from the preprocessed camera signals, followed by post-filtering of the pulsatile signal. This improved information may then be used for such analysis as HRV determination, which is not possible with inaccurate methods for optical pulse rate detection.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="93.56mm" wi="149.01mm" file="US20230000376A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="166.03mm" wi="133.94mm" orientation="landscape" file="US20230000376A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="145.46mm" wi="82.63mm" file="US20230000376A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="97.79mm" wi="151.05mm" file="US20230000376A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="175.43mm" wi="128.10mm" file="US20230000376A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="97.87mm" wi="127.68mm" file="US20230000376A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="180.51mm" wi="159.09mm" file="US20230000376A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="104.99mm" wi="158.41mm" file="US20230000376A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="238.08mm" wi="117.35mm" file="US20230000376A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="219.12mm" wi="137.84mm" file="US20230000376A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="128.35mm" wi="128.10mm" file="US20230000376A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="224.20mm" wi="133.10mm" file="US20230000376A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="145.03mm" wi="120.23mm" file="US20230000376A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="140.29mm" wi="120.23mm" file="US20230000376A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The present invention is of a system and method for physiological measurements as determined from optical data, and in particular, for such a system and method for determining such measurements from video data of a subject.</p><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">Heart rate measurement devices date back to 1870's with the first electrocardiogram (ECG or EKG), measuring the electric voltage changes due to heart cardiac cycle (or heart beat). The EKG signal is com-posed from three main components: P wave which represents the atria depolarization; the QRS complex represents ventricles depolarization; and T wave represents ventricles re-polarization.</p><p id="p-0004" num="0003">A second pulse rate detection technique is optical measurement that detects blood volume changes in the microvascular bed of tissue named photo-plethysmography (PPG). In PPG measurement the peripheral pulse wave characteristically exhibits systolic and diastolic peaks. The systolic peak is a result of direct pressure wave traveling from the left ventricle to the periphery of the body, and the diastolic peak (or inflection) is a result of reflections of the pressure wave by arteries of the lower body.</p><p id="p-0005" num="0004">There are two categories of PPG based devices: contact-based and remote (rPPG). The contact based device typically is used on the finger and measures the light reflection typically at red and IR (infrared) wave-lengths. On the other hand, the remote PPG device measures the light reflected from skin surface typically of the face. Most rPPG algorithms use RGB cameras, and do not use IR cameras.</p><p id="p-0006" num="0005">The PPG signal comes from the light&#x2014;biological tissue interaction, thus depends on (multiple) scattering, absorption, reflection, transmission and fluorescence. Different effects are important depending on the type of device, for contact based or remote PPG measurement. In rPPG analysis a convenient first order decomposition of the signal is to intensity fluctuations, scattering (which did not interact with biological tissues), and the pulsatile signal. The instantaneous pulse time is set from the R&#x2014;time in EKG measurement or the systolic peak in a PPG measurements. The EKG notation is used to refer the systolic peak of the rPPG measurement as R time. The instantaneous heart rate is evaluated from the difference between successive R times, RR(n)=R(n)&#x2212;R(n&#x2212;1), as 60/RR(n) in beats per minutes.</p><p id="p-0007" num="0006">Fluctuations in RR interval indicate how the cardiovascular system is adjusted to sudden physical and psychological challenges to homeostasis. The measure of these fluctuations is referred to as heart rate variability (HRV).</p><heading id="h-0003" level="1">BRIEF SUMMARY OF THE INVENTION</heading><p id="p-0008" num="0007">Accurate optical pulse rate detection unfortunately has suffered from various technical problems. The major difficulty is the low signal to noise achieved and therefore failure to detect the pulse rate. Accurate pulse rate detection is needed to be able to determine heart rate variability (HRV).</p><p id="p-0009" num="0008">HRV is the extraction of statistical parameters from the pulse rate over a long duration. Traditionally the measured time varies from 0.5-24 hours, but in recent years researchers have extracted HRV also from substantial shorter time duration. The statistical information derived from the HRV may provide a general indicator of the subject's well being, including for example with regard to stress estimation.</p><p id="p-0010" num="0009">The presently claimed invention overcomes these difficulties by providing a new system and method for improving the accuracy of pulse rate detection. Various aspects contribute to the greater accuracy, including but not limited to pre-processing of the camera output/input, extracting the pulsatile signal from the preprocessed camera signals, followed by post-filtering of the pulsatile signal. This improved information may then be used for such analysis as HRV determination, which is not possible with inaccurate methods for optical pulse rate detection.</p><p id="p-0011" num="0010">The HRV parameters relate the status of the sympathetic nervous system (SNS) and parasympathetic nervous system (PNS) activities. The SNS and PNS are indicators for the individual stress level, allowing to estimate the stress index.</p><p id="p-0012" num="0011">According to at least some embodiments, there is provided a method for obtaining a physiological signal from a subject, the method comprising obtaining optical data from a face of the subject with a camera, analyzing the optical data to select data related to the face of the subject with a computational device in communication with said camera, detecting optical data from a skin of the face, determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and calculating the physiological signal from the time series.</p><p id="p-0013" num="0012">Optionally the optical data comprises video data, and wherein said obtaining said optical data comprises obtaining video data of the face of the subject. Optionally obtaining said optical data further comprises obtaining video data from a mobile phone camera, such that said camera comprises a mobile phone camera. Optionally said computational device comprises a mobile communication device. Optionally said mobile phone camera comprises a front facing camera. Optionally said computational device is physically separate from, but in communication with, said mobile phone camera. Optionally in combination with any method or a portion thereof as described herein, said detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, selecting the face boundary with the highest probability and applying a histogram analysis to video data from the face.</p><p id="p-0014" num="0013">Optionally said determining said plurality of face boundaries comprises applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries. Optionally in combination with any method or a portion thereof as described herein, said obtaining said optical data further comprises obtaining video data of the skin of a finger of the subject. Optionally said obtaining said video data comprises obtaining video data of the skin of a fingertip of the subject by placing said fingertip on said camera. Optionally said camera for obtaining video data of said fingertip comprises a mobile phone camera. Optionally said mobile phone camera comprises a rear facing camera. Optionally said fingertip on said mobile phone camera further comprises activating a flash associated with said mobile phone camera to provide light.</p><p id="p-0015" num="0014">Optionally in combination with any method or a portion thereof as described herein, said detecting said optical data from said skin of the face comprises determining a plurality of face or fingertip boundaries, selecting the face or fingertip boundary with the highest probability and applying a histogram analysis to video data from the face or fingertip. Optionally said determining said plurality of face or fingertip boundaries comprises applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face or fingertip boundaries. Optionally the method may further comprise combining analyzed data from images of the face and fingertip to determine the physiological measurement.</p><p id="p-0016" num="0015">Optionally in combination with any method as described herein, said determining the physiological signal further comprises combining meta data with measurements from said at least one physiological signal, wherein said meta data comprises one or more of weight, age, height, biological gender, body fat percentage and body muscle percentage of the subject. Optionally in combination with any method as described herein, said physiological signal is selected from the group consisting of stress, blood pressure, breath volume, and pSO2 (oxygen saturation).</p><p id="p-0017" num="0016">According to at least some embodiments, there is provided a system for obtaining a physiological signal from a subject, the system comprising: a camera for obtaining optical data from a face of the subject, a user computational device for receiving optical data from said camera, wherein said user computational device comprises a processor and a memory for storing a plurality of instructions, wherein said processor executes said instructions for analyzing the optical data to select data related to the face of the subject, detecting optical data from a skin of the face, determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and calculating the physiological signal from the time series. Optionally said memory is configured for storing a defined native instruction set of codes and said processor is configured to perform a defined set of basic operations in response to receiving a corresponding basic instruction selected from the defined native instruction set of codes stored in said memory; wherein said memory stores a first set of machine codes selected from the native instruction set for analyzing the optical data to select data related to the face of the subject, a second set of machine codes selected from the native instruction set for detecting optical data from a skin of the face, a third set of machine codes selected from the native instruction set for determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and a fourth set of machine codes selected from the native instruction set for calculating the physiological signal from the time series.</p><p id="p-0018" num="0017">Optionally said detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, selecting the face boundary with the highest probability and applying a histogram analysis to video data from the face, such that said memory further comprises a fifth set of machine codes selected from the native instruction set for detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, a sixth set of machine codes selected from the native instruction set for selecting the face boundary with the highest probability and a seventh set of machine codes selected from the native instruction set for applying a histogram analysis to video data from the face.</p><p id="p-0019" num="0018">Optionally said determining said plurality of face boundaries comprises applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries, such that said memory further comprises an eighth set of machine codes selected from the native instruction set for applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries.</p><p id="p-0020" num="0019">Optionally in combination with any system or a portion thereof as described herein, said camera comprises a mobile phone camera and wherein said optical data is obtained as video data from said mobile phone camera. Optionally said computational device comprises a mobile communication device. Optionally said mobile phone camera comprises a rear facing camera and a fingertip of the subject is placed on said camera for obtaining said video data. Optionally the system further comprises a flash associated with said mobile phone camera to provide light for obtaining said optical data.</p><p id="p-0021" num="0020">Optionally said memory further comprises a ninth set of machine codes selected from the native instruction set for determining a plurality of face or fingertip boundaries, a tenth set of machine codes selected from the native instruction set for selecting the face or fingertip boundary with the highest probability, and an eleventh set of machine codes selected from the native instruction set for applying a histogram analysis to video data from the face or fingertip.</p><p id="p-0022" num="0021">Optionally said memory further comprises a twelfth set of machine codes selected from the native instruction set for applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face or fingertip boundaries. Optionally the system further comprises combining analyzed data from images of the face and fingertip to determine the physiological measurement according to said instructions executed by said processor. Optionally in combination with any system or a portion thereof as described herein, the system further comprises a display for displaying the physiological measurement and/or signal. Optionally said user computational device further comprises said display.</p><p id="p-0023" num="0022">Optionally in combination with any system or a portion thereof as described herein, said user computational device further comprises a transmitter for transmitting said physiological measurement and/or signal. Optionally in combination with any system or a portion thereof as described herein, said determining the physiological signal further comprises combining meta data with measurements from said at least one physiological signal, wherein said meta data comprises one or more of weight, age, height, biological gender, body fat percentage and body muscle percentage of the subject. Optionally in combination with any system or a portion thereof as described herein, said physiological signal is selected from the group consisting of stress, blood pressure, breath volume, and pSO2 (oxygen saturation).</p><p id="p-0024" num="0023">According to at least some embodiments, there is provided a system for obtaining a physiological signal from a subject, the system comprising: a rear facing camera for obtaining optical data from a finger of the subject, a user computational device for receiving optical data from said camera, wherein said user computational device comprises a processor and a memory for storing a plurality of instructions, wherein said processor executes said instructions for analyzing the optical data to select data related to the face of the subject, detecting optical data from a skin of the finger, determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and calculating the physiological signal from the time series. Optionally the system further comprises any system or portion thereof as described herein.</p><p id="p-0025" num="0024">According to at least some embodiments, there is provided a method for obtaining a physiological signal from a subject, comprising operating any system as described herein, to obtain said physiological signal from said subject.</p><p id="p-0026" num="0025">Implementation of the method and system of the present invention involves performing or completing certain selected tasks or steps manually, automatically, or a combination thereof. Moreover, according to actual instrumentation and equipment of preferred embodiments of the method and system of the present invention, several selected steps could be implemented by hardware or by software on any operating system of any firmware or a combination thereof. For example, as hardware, selected steps of the invention could be implemented as a chip or a circuit. As software, selected steps of the invention could be implemented as a plurality of software instructions being executed by a computer using any suitable operating system. In any case, selected steps of the method and system of the invention could be described as being performed by a data processor, such as a computing platform for executing a plurality of instructions.</p><p id="p-0027" num="0026">Although the present invention is described with regard to a &#x201c;computing device&#x201d;, a &#x201c;computer&#x201d;, or &#x201c;mobile device&#x201d;, it should be noted that optionally any device featuring a data processor and the ability to execute one or more instructions may be described as a computer, including but not limited to any type of personal computer (PC), a server, a distributed server, a virtual server, a cloud computing platform, a cellular telephone, an IP telephone, a smartphone, or a PDA (personal digital assistant). Any two or more of such devices in communication with each other may optionally comprise a &#x201c;network&#x201d; or a &#x201c;computer network&#x201d;.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0028" num="0027">The invention is herein described, by way of example only, with reference to the accompanying drawings. With specific reference now to the drawings in detail, it is stressed that the particulars shown are by way of example and for purposes of illustrative discussion of the preferred embodiments of the present invention only, and are presented in order to provide what is believed to be the most useful and readily understood description of the principles and conceptual aspects of the invention. In this regard, no attempt is made to show structural details of the invention in more detail than is necessary for a fundamental understanding of the invention, the description taken with the drawings making apparent to those skilled in the art how the several forms of the invention may be embodied in practice. In the drawings:</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> show exemplary non-limiting illustrative systems for obtaining video data of a user and for analyzing the video data to determine one or more biological signals;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a non-limiting exemplary method for performing signal analysis;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> show non-limiting exemplary methods for enabling the user to use the app to obtain biological statistics;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a non-limiting exemplary process for creating detailed biological statistics;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> show a non-limiting, exemplary method for obtaining video data and then performing the initial processing;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> relates to a non-limiting exemplary method for pulse rate estimation and determination of the rPPG, while <figref idref="DRAWINGS">FIGS. <b>6</b>B-<b>6</b>C</figref> relate to some results of this method;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a non-limiting exemplary method for performing an HRV or heart rate variability time domain analysis; and</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a non-limiting exemplary method for calculating the heart rate variability or HRV frequency domain.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION OF AT LEAST SOME EMBODIMENTS</heading><p id="p-0037" num="0036">A key underlying problem for rPPG mechanisms is accurate face detection and precise skin surface selection suitable for analysis, if images of the face are used. Similar problems are encountered if images of the fingertip are used, for example for images taken with the rear facing camera of a mobile device, such as a smart phone for example. The presently claimed invention overcomes this problem for face, finger and skin detection based on neural network methodology. Non-limiting examples are provided below. Preferably, for the skin selection, a histogram based algorithm used. Applying this procedure on part of the video frame containing the face only (or alternatively the finger only), the mean values for each channel, Red, Green, and Blue (RGB) construct the frame data. When using above procedures continuously for consequent video frames, the time series of RGB data is obtained. Each element of these time series represented by RGB values is obtained frame by frame, with time stamps used to determine elapsing time from the first occurrence of the first element. Then, the rPPG analysis begins when the total elapsed time reaches the averaging period used for the pulse rate estimation defined external parameter, for a complete a time window (Lalgo). Taking into account the variable frame acquisition rate, the time series data has to be interpolated with respect to the fixed given frame rate.</p><p id="p-0038" num="0037">After interpolation, a pre-processing mechanism is applied to construct more suitable three dimensional signal (RGB). Such pre-processing may include for example normalization and filtering. Following pre-processing, the rPPG trace signal is calculated, including estimating the mean pulse rate.</p><p id="p-0039" num="0038">Turning now to the drawings, <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> show exemplary non-limiting illustrative systems for obtaining video data of a user and for analyzing the video data to determine one or more biological signals.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> shows a system <b>100</b> featuring a user computational device <b>102</b>, communicating with a server <b>118</b>. The user computational device <b>102</b> preferably communicates with a server <b>118</b> through a computer network <b>116</b>. User computational device <b>102</b> preferably includes user input device <b>106</b>, which may include, for example, a pointing device such as a mouse, keyboard, and/or other input device.</p><p id="p-0041" num="0040">In addition, user computational device <b>102</b> preferably includes a camera <b>114</b>, for obtaining video data of a face of the user. The camera may also be separate from the user computational device. Optionally camera <b>114</b> comprises a rear facing camera of a mobile device, or another type of camera that is suitable for obtaining video data of a finger of the user, preferably of a portion of the finger such as a fingertip for example. User computational device <b>102</b> may comprise one or both of such cameras. The user interacts with a user app interface <b>104</b>, for providing commands for determining the type of signal analysis, for starting the signal analysis, and for also receiving the results of the signal analysis.</p><p id="p-0042" num="0041">For example, the user may, through user computational device <b>102</b>, start recording video data through camera <b>114</b>, either by separately activating camera <b>114</b>, or by recording such data by issuing a command through user app interface <b>104</b>.</p><p id="p-0043" num="0042">Next, the video data is preferably sent to server <b>118</b>, where it is received by server app interface <b>120</b>. It is then analyzed by signal analyzer engine <b>122</b>. Signal analyzer engine <b>122</b> preferably includes detection of the face in the video signals, followed by skin detection. Alternatively or additionally, signal analyzer engine <b>122</b> preferably includes detection of the finger or a portion thereof, such as the fingertip, in the video signals, followed by skin detection. As described in detail below, various non-limiting algorithms are preferably applied to support obtaining the pulse signals from this information. Next, the pulse signals are preferably analyzed according to time, frequency and non-linear filters to support the determination of HRV. Further analyses may then be performed from the HRV determination.</p><p id="p-0044" num="0043">User computational device <b>102</b> preferably features a processor <b>110</b>A, and a memory <b>112</b>A. Server <b>118</b> preferably features a processor <b>110</b>B, and a memory <b>112</b>B.</p><p id="p-0045" num="0044">As used herein, a processor such as processor <b>110</b>A or <b>110</b>B generally refers to a device or combination of devices having circuitry used for implementing the communication and/or logic functions of a particular system. For example, a processor may include a digital signal processor device, a microprocessor device, and various analog-to-digital converters, digital-to-analog converters, and other support circuits and/or combinations of the foregoing. Control and signal processing functions of the system are allocated between these processing devices according to their respective capabilities. The processor may further include functionality to operate one or more software programs based on computer-executable program code thereof, which may be stored in a memory, such as memory <b>112</b>A or <b>112</b>B in this non-limiting example. As the phrase is used herein, the processor may be &#x201c;configured to&#x201d; perform a certain function in a variety of ways, including, for example, by having one or more general-purpose circuits perform the function by executing particular computer-executable program code embodied in computer-readable medium, and/or by having one or more application-specific circuits perform the function.</p><p id="p-0046" num="0045">Optionally, memory <b>112</b>A or <b>112</b>B is configured for storing a defined native instruction set of codes. Processor <b>110</b>A or <b>110</b>B is configured to perform a defined set of basic operations in response to receiving a corresponding basic instruction selected from the defined native instruction set of codes stored in memory <b>112</b>A or <b>112</b>B. Optionally memory <b>112</b>A or <b>112</b>B stores a first set of machine codes selected from the native instruction set for analyzing the optical data to select data related to the face of the subject, a second set of machine codes selected from the native instruction set for detecting optical data from a skin of the face, a third set of machine codes selected from the native instruction set for determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and a fourth set of machine codes selected from the native instruction set for calculating the physiological signal from the time series.</p><p id="p-0047" num="0046">Optionally memory <b>112</b>A or <b>112</b>B further comprises a fifth set of machine codes selected from the native instruction set for detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, a sixth set of machine codes selected from the native instruction set for selecting the face boundary with the highest probability and a seventh set of machine codes selected from the native instruction set for applying a histogram analysis to video data from the face.</p><p id="p-0048" num="0047">Optionally memory <b>112</b>A or <b>112</b>B further comprises an eighth set of machine codes selected from the native instruction set for applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries.</p><p id="p-0049" num="0048">In addition, user computational device <b>102</b> may feature user display device <b>108</b> for displaying the results of the signal analysis, the results of one or more commands being issued and the like.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> shows a system <b>150</b>, in which the above described functions are performed by user computational device <b>102</b>. For either of <figref idref="DRAWINGS">FIG. <b>1</b>A or <b>1</b>B</figref>, user computational device <b>102</b> may comprise a mobile phone. In <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the previously described signal analyzer engine is now operated by user computational device <b>102</b> as signal analyzer engine <b>152</b>. Signal analyzer engine <b>152</b> may have the same or similar functions to those described for signal analyzer engine in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. In <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, user computational device <b>102</b> may be connected to a computer network such as the internet (not shown) and may also communicate with other computational devices. In at least some embodiments, some of the functions are performed by user computational device <b>102</b> while others are performed by a separate computational device, such as a server for example (not shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, see <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>).</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a non-limiting exemplary method for performing signal analysis. A process <b>200</b> begins by initiating the process of obtaining data at block <b>202</b>, for example, by activating a video camera <b>204</b>. Face recognition is then optionally performed at <b>206</b>, to first of all locate the face of the user. This may, for example, be performed through a deep learning face detection module <b>208</b>, and also through a tracking process <b>210</b>. It is important to locate the face of the user, as the video data is preferably of the face of the user in order to obtain the most accurate results for signal analysis. Tracking process <b>210</b> is based on a continuous features matching mechanism. The features represent a previously detected face in a new frame. The features are determined according to the position in the frame and from the output of an image recognition process, such as a CNN (convolutional neural network). When only one face appears in the frame, tracking process <b>210</b> can be simplified to face recognition within the frame.</p><p id="p-0052" num="0051">As a non-limiting example, optionally, a Multi-task Convolutional Network algorithm is applied for face detection which achieves state-of-the-art accuracy under real-time conditions. It is based on the network cascade that was introduced in a publication by Li et al (Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, and Gang Hua. A convolutional neural network cascade for face detection. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015).</p><p id="p-0053" num="0052">Next, the skin of the face of the user is located within the video data at <b>212</b>. Preferably, for the skin selection, a histogram based algorithm used. Applying this procedure on part of the video frame containing the face only, as determined according to the previously described face detection algorithm, the mean values for each channel, Red, Green, and Blue (RGB) are preferably used to construct the frame data.</p><p id="p-0054" num="0053">Alternatively or additionally, the same or a similar process may be used for analysis of images of the finger or a portion thereof, such as the fingertip for example. For example, video data is obtained in a data block as described above, but of images of the finger or a portion thereof, such as the fingertip. The process of face recognition is then preferably adapted at <b>206</b>, to first of all locate the finger or the portion thereof of the user. This process may, for example, be performed through an adapted finger or finger portion detection module (not shown), and also through an adapted tracking process (not shown) which is adapted to track the finger or the portion thereof through different images. Optionally, if the fingertip is pressed directly against the rear camera of a mobile device, tracking may be less necessary, although preferably fingertip recognition is still performed. In any case, preferably the skin of the fingertip is located as described for example for the above process for face recognition. Also preferably, if images of the finger or a portion thereof are to be analyzed, preferably the skin thereof is located as described for example for the above process for face recognition.</p><p id="p-0055" num="0054">When using above procedures continuously for consequent video frames, a time series of RGB data is obtained. Each frame, with its RGB values, represents an element of these time series. Each element has a time stamp determined according to elapsed time from the first occurrence. The collected elements may be described as being in a scaled buffer having L algo elements. The frames are preferably collected until sufficient elements are collected. The sufficiency of the number of elements is preferably determined according to the total elapsed time. The rPPG analysis of <b>214</b> begins when the total elapsed time reaches the length of time required for the averaging period used for the pulse rate estimation. The collected data elements may be interpolated. Following interpolation, the pre-processing mechanism is preferably applied to construct a more suitable three dimensional signal (RGB).</p><p id="p-0056" num="0055">A PPG signal is created at <b>214</b> from the three dimensional signal and specifically from the elements of the RGB data. For example, the pulse rate may be determined from a single calculation or from a plurality of cross-correlated calculations, as described in greater detail below. This may be then normalized and filtered at <b>216</b>, and may be used to reconstruct PSO<sub>2</sub>, ECG, and breath at <b>218</b>. A fundamental frequency is found at <b>220</b>, and the statistics are created such as heart rate, PSO<sub>2</sub>, and breath rates and so forth at <b>222</b>.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIGS. <b>3</b>A and <b>3</b>B</figref> show non-limiting exemplary methods for enabling the user to use the app to obtain biological statistics. <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> shows a non-limiting exemplary method using optical images of the face alone. <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> shows a similar, non-limiting, exemplary method for analyzing video data of the fingertip of the user, for example from the rear camera of a mobile device as previously described. Optionally both methods may be combined.</p><p id="p-0058" num="0057">Turning now to the drawings, as shown in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, in a method <b>300</b>, the user registers with the app at <b>302</b>. Next, images are obtained with the video camera, for example as attached to or formed with user computational device at <b>304</b>. The video camera is preferably a RGB camera as described herein.</p><p id="p-0059" num="0058">The face is located within the images <b>306</b>. This may be performed on the user computational device, at a server, or optionally at both. Furthermore, this process may be performed as previously described, with regard to a multi-task convolutional neural net. Skin detection is then performed, by applying a histogram to the RGB signal data. Only the video data relating to light reflected from the skin is preferably analyzed for optical pulse detection and HRV determination.</p><p id="p-0060" num="0059">The time series for the signals are determined at <b>308</b>, for example as previously described. Taking into account the variable frame acquisition rate, the time series data is preferably interpolated with respect to the fixed given frame rate. Before running the interpolation procedure, preferably the following conditions are analyzed so that interpolation can be performed. First, preferably the number of frames is analyzed to verify that after interpolation and pre-processing, there will be enough frames for the rPPG analysis.</p><p id="p-0061" num="0060">Next, the frames per second are considered, to verify that the measured frames per second in the window is above a minimum threshold. After that, the time gap between frames, if any, is analyzed to ensure that it is less than some externally set threshold, which for example may be 0.5 seconds.</p><p id="p-0062" num="0061">If any of the above conditions not satisfied, then the procedure preferably terminates with full data reset and restarts from the last valid frame, for example to return to <b>304</b> as described above.</p><p id="p-0063" num="0062">Next the video signals are preferably pre-processed at <b>310</b>, following interpolation. The pre-processing mechanism is applied to construct a more suitable three dimensional signal (RGB). The pre-processing preferably includes normalizing each channel to the total power; scaling the channel value by its mean value (estimated by low pass filter) and subtracting by one; and then passing the data through a Butterworth band pass IIR filter.</p><p id="p-0064" num="0063">Statistical information is extracted at <b>312</b>. A heartbeat is then reconstructed at <b>314</b>. Breath signals are determined at <b>316</b>, and then the pulse rate is measured at <b>318</b>. After this, the blood oxidation is measured at <b>320</b>.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> shows a similar, non-limiting, exemplary method for analyzing video data of the fingertip of the user, for example from the rear camera of a mobile device as previously described. This process may be used for example if sufficient video data cannot be captured from the front facing camera, for the face of the user. Optionally both methods may be combined.</p><p id="p-0066" num="0065">In a method <b>340</b>, the method begins by placing the fingertip of the user on or near the camera at <b>342</b>. If near the camera, then the fingertip needs to be visible to the camera. This placement may be accomplished for example in a mobile device, by having the user place the fingertip on the rear camera of the mobile device. The camera is already in a known geometric position in relation to placement of the fingertip, which encourages correct placement of the fingertip in terms of collecting accurate video data. Optionally the flash of the mobile device may be enabled in a longer mode (&#x201c;torch&#x201d; or &#x201c;flashlight&#x201d; mode) to provide sufficient light. Enabling the flash may be performed automatically if sufficient light is not detected by the camera for accurate video data of the fingertip to be obtained.</p><p id="p-0067" num="0066">At <b>344</b>, images of the finger, and preferably of the fingertip, are obtained with the camera. Next the finger, and preferably the fingertip, is located within the images at <b>346</b>. This process may be performed as previously described with regard to location of the face within the images. However, if a neural net is used, it will need to be trained specifically to locate fingers and preferably fingertips. Hand tracking from optical data is known in the art; a modified hand tracking algorithm could be used to track fingertips within a series of images.</p><p id="p-0068" num="0067">At <b>348</b>, the skin is found within the finger, and preferably fingertip, portion of the image. Again, this process may be performed generally as described above for skin location, optionally with adjustments for finger or fingertip skin. The time series for the signals are determined at <b>350</b>, for example as previously described but preferably adjusted for any characteristics of using the rear camera and/or the direct contact of the fingertip skin on the camera. Taking into account the variable frame acquisition rate, the time series data is preferably interpolated with respect to the fixed given frame rate. Before running the interpolation procedure, preferably the following conditions are analyzed so that interpolation can be performed. First, preferably the number of frames is analyzed to verify that after interpolation and pre-processing, there will be enough frames for the rPPG analysis.</p><p id="p-0069" num="0068">Next, the frames per second are considered, to verify that the measured frames per second in the window is above a minimum threshold. After that, the time gap between frames, if any, is analyzed to ensure that it is less than some externally set threshold, which for example may be 0.5 seconds.</p><p id="p-0070" num="0069">If any of the above conditions is not satisfied, then the procedure preferably terminates with full data reset and restarts from the last valid frame, for example to return to <b>344</b> as described above.</p><p id="p-0071" num="0070">Next the video signals are preferably pre-processed at <b>352</b>, following interpolation. The pre-processing mechanism is applied to construct a more suitable three dimensional signal (RGB). The pre-processing preferably includes normalizing each channel to the total power; scaling the channel value by its mean value (estimated by low pass filter) and subtracting by one; and then passing the data through a Butterworth band pass IIR filter. Again, this process is preferably adjusted for the fingertip data. At <b>354</b>, statistical information is extracted, after which the process may proceed for example as described with regard to <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> above, from <b>314</b>, to reconstruct the heart beat and to perform other measurements as described herein.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a non-limiting exemplary process for creating detailed biological statistics. In a process <b>400</b>, user video data is obtained through a user computational device <b>402</b>, with a camera <b>404</b>. A face detection model <b>406</b> is then used to find the face. For example, after face video data has been detected for a plurality of different face boundaries, all but the highest-scoring face boundary is preferably discarded. Its bounding box is cropped out of the input image, such that data related to the user's face is preferably separated from other video data. Skin pixels are preferably collected using a histogram based classifier with a soft thresholding mechanism, as previously described. From the remaining pixels, the mean value is computed per channel, and then passed on to the rPPG algorithm at <b>410</b>. This process enables skin color to be determined, such that the effect of the pulse on the optical data can be separated from the effect of the underlying skin color. The process tracks the face at <b>408</b> according to the highest scoring face bounding box.</p><p id="p-0073" num="0072">As noted above, this process may be adapted to detect the finger or portion thereof, such as the fingertip for example. Preferably a boundary detecting algorithm is also used to detect the boundaries of the finger or portion thereof, such as the fingertip. The subsequent processes, such as cropping out the bounding box to separate the relevant portion of the user's anatomy, such as the finger or portion thereof, such as the fingertip for example. An adapted histogram based classifier may also be used, given that the relevant portions of the anatomy being detected, such as the fingertip for example, comprise skin. The process at <b>408</b> may be adapted if the user presses a fingertip against the rear camera, for example to accommodate a reduced need for tracking, given the direct placement of the fingertip against the rear camera.</p><p id="p-0074" num="0073">Next, the PPG signals are created at <b>410</b>. Following pre-processing, the rPPG trace signal is calculated using a L algo elements of the scaled buffer. The procedure is described as follows: The mean pulse rate is estimated using a match filter between two rPPG different analytic signals constructed from raw interpolated data (CHROM like and Projection Matrix (PM)). Then the cross-correlation is calculated on which the mean instantaneous pulse rate is searched. Frequency estimation is based on non-linear least square (NLS) spectral decomposition with additional lock-in mechanism. The rPPG signal, then is derived from the PM method applying adaptive Wiener filtering and with initial guess signal to be the dependent on instantaneous pulse rate frequency (vpr): sin(2&#x3c0;vprn). Further, an additional filter in the frequency domain used to force signal reconstruction. Lastly, the exponential filter applied on instantaneous RR values obtained by procedure discussed in greater detail below.</p><p id="p-0075" num="0074">The signal processor at <b>412</b> then preferably performs a number of different functions, based on the PPG signals. These preferably include reconstructing an ECG-like signal at <b>414</b>, computing the HRV (heart rate variability) parameters at <b>416</b>, and then computing a stress index at <b>418</b>.</p><p id="p-0076" num="0075">HRV is the physiological phenomenon of variation in the time interval between heartbeats. It is measured by the variation in the beat-to-beat interval. Other terms used include: &#x201c;cycle length variability&#x201d;, &#x201c;RR (NN) variability&#x201d; (where R is a point corresponding to the peak of the QRS complex of the ECG wave; and RR is the interval between successive Rs), and &#x201c;heart period variability&#x201d;.</p><p id="p-0077" num="0076">As described in greater detail below, it is possible to calculate 24 h, semi(&#x2dc;15 min), short-term (ST, &#x2dc;5 min) or brief, and ultra-short-term (UST, &#x3c;5 min) HRV using time-domain, frequency-domain, and non-linear measurements.</p><p id="p-0078" num="0077">In addition, the instant blood pressure may be created at <b>420</b>, followed by blood pressure statistics <b>422</b>. Optionally metadata at <b>424</b> is included in this calculation. The metadata may for example relate to height, weight, gender or other physiological or demographic data. At <b>426</b>, the PS0<sub>2 </sub>signal is reconstructed, followed by computing the PS0<sub>2 </sub>statistics at <b>428</b>. The statistics at <b>428</b> may then lead to the further blood pressure analysis as previously described with regard to <b>420</b> and <b>422</b>.</p><p id="p-0079" num="0078">Optionally a breath signal is reconstructed at <b>430</b> by the previously described signal processor <b>412</b>, followed by computing the breath variability at <b>432</b>. The breath rate and volume are then preferably calculated at <b>434</b>.</p><p id="p-0080" num="0079">From the instant blood pressure calculations at <b>420</b>, optionally a blood pressure model is calculated at <b>436</b>. The calculation of the blood pressure model may be influenced or adjusted according to historical data at <b>438</b>, such as previously determined blood pressure, breath rate and volume, PS0<sub>2</sub>, or other calculations.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIGS. <b>5</b>A-<b>5</b>E</figref> show a non-limiting, exemplary method for obtaining video data and then performing the initial processing, which preferably includes interpolation, pre-processing and rPPG signal determination, with some results from such initial processing. Turning now to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, in a process <b>500</b>, video data is obtained in <b>502</b>, for example as previously described.</p><p id="p-0082" num="0081">Next the camera channels input buffer data is obtained at <b>504</b>, for example as previously described. Next a constant and predefined acquisition rate is preferably determined at <b>506</b>. For example, the constant and predefined acquisition rate may be set at &#x394;t=1/fps&#x2dc;=33 ms. At <b>508</b>, each channel is preferably interpolated separately to the time buffer with the constant and predefined acquisition rate. This step removes the input time jitter. Even though the interpolation procedure adds aliasing (and/or frequency folding), aliasing (and/or frequency folding) has already occurred once the images were taken by the camera. The importance of interpolating into a constant sample rate is that it satisfies a basic assumption of quasi-stationarity of the heart rate in accordance to the acquisition time. The method used for interpolation may for example be based on cubic Hermite interpolation.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIGS. <b>5</b>B-<b>5</b>D</figref> show data relating to different stages of the scaling procedure. The color coding corresponds to the colors of each channel, i.e. red corresponds to the red channel (top line; middle line is green; lower line is blue) and so forth. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> shows the camera channel data after interpolation.</p><p id="p-0084" num="0083">Turning back to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, at <b>510</b>-<b>514</b>, after interpolating each of the colored channels (vec(c)), pre-processing is performed to enhance the pulsatile modulations. The pre-processing preferably incorporates three steps. At <b>510</b>, normalization of each channel to the total power is performed, which reduces noise due to overall external light modulation.</p><p id="p-0085" num="0084">The power normalization is given by</p><p id="p-0086" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mover>       <mi>c</mi>       <semantics definitionURL="">        <mo>&#x2192;</mo>        <annotation encoding="Mathematica">"\[Rule]"</annotation>       </semantics>      </mover>      <mi>p</mi>     </msub>     <mo>=</mo>     <mfrac>      <mover>       <mi>c</mi>       <semantics definitionURL="">        <mo>&#x2192;</mo>        <annotation encoding="Mathematica">"\[Rule]"</annotation>       </semantics>      </mover>      <msqrt>       <mrow>        <msubsup>         <mi>c</mi>         <mi>r</mi>         <mn>2</mn>        </msubsup>        <mo>+</mo>        <msubsup>         <mi>c</mi>         <mi>g</mi>         <mn>2</mn>        </msubsup>        <mo>+</mo>        <msubsup>         <mi>c</mi>         <mi>b</mi>         <mn>2</mn>        </msubsup>       </mrow>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0087" num="0085">with&#x2014;&#x2192;c p is the power normalized camera channel vector, and&#x2014;&#x2192;c is the interpolated input vector as described. For brevity reason, the frame index was removed from both sides.</p><p id="p-0088" num="0086">Next, at <b>512</b>, scaling is performed. For example, such scaling may be performed by the mean value i and subtracted by one, which reduces effects of stationary light source and its brightness level. The mean value is set by the segment length (Lalgo), but this type of a solution can enhance low frequency components. Alternatively, instead of scaling by the mean value, it is possible to scale by a low pass FIR filter.</p><p id="p-0089" num="0087">Using a low pass filter adds an inherent latency, which requires compensation on M/2 frames. The scaled signal is given by:</p><p id="p-0090" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>c</mi>       <mi>s</mi>      </msub>      <mo>(</mo>      <mrow>       <mi>n</mi>       <mo>-</mo>       <mfrac>        <mi>M</mi>        <mn>2</mn>       </mfrac>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <msub>         <mi>c</mi>         <mi>p</mi>        </msub>        <mo>(</mo>        <mrow>         <mi>n</mi>         <mo>-</mo>         <mfrac>          <mi>M</mi>          <mn>2</mn>         </mfrac>        </mrow>        <mo>)</mo>       </mrow>       <mrow>        <munderover>         <mo>&#x2211;</mo>         <mrow>          <mi>m</mi>          <mo>=</mo>          <mn>0</mn>         </mrow>         <mrow>          <mi>m</mi>          <mo>=</mo>          <mi>M</mi>         </mrow>        </munderover>        <mrow>         <mrow>          <mi>b</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>m</mi>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <msub>           <mi>c</mi>           <mi>p</mi>          </msub>          <mo>(</mo>          <mrow>           <mi>n</mi>           <mo>-</mo>           <mi>m</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mfrac>      <mo>-</mo>      <mn>1</mn>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0091" num="0088">with cs(n) is a single channel scaled value of frame n, and b is the lowpass FIR coefficients. The channel color notation was removed from the above formula for brevity.</p><p id="p-0092" num="0089">At <b>514</b>, the scaled data is passed through Butterworth band pass IIR filter.</p><p id="p-0093" num="0090">This filter is defined as:</p><p id="p-0094" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mrow>       <mi>s</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>n</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mstyle><mtext>?</mtext></mstyle>        <mrow>         <mi>b</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mi>m</mi>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <msub>          <mi>c</mi>          <mi>s</mi>         </msub>         <mo>(</mo>         <mrow>          <mi>n</mi>          <mo>-</mo>          <mi>m</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>-</mo>       <mrow>        <munderover>         <mo>&#x2211;</mo>         <mrow>          <mi>l</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mrow>          <mi>l</mi>          <mo>=</mo>          <msub>           <mi>M</mi>           <mi>fb</mi>          </msub>         </mrow>        </munderover>        <mrow>         <mrow>          <mi>a</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>l</mi>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <mi>s</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>n</mi>           <mo>-</mo>           <mi>l</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00003-2" num="00003.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0095" num="0091">The output of the scaling procedure is&#x2014;&#x2192;s each new frame adds a new frame with latency for each camera channel. Note that for brevity the frame index n is used but it actually refers to frame n&#x2014;M/2 (due to the low pass filter).</p><p id="p-0096" num="0092"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> shows power normalization of the camera input, plot of the low-pass scaled data before the band-pass filter (top line is red; middle line is green; lower line is blue). <figref idref="DRAWINGS">FIG. <b>5</b>D</figref> shows a plot of the power scaled data before the band-pass filter. <figref idref="DRAWINGS">FIG. <b>5</b>E</figref> shows a comparison of the mean absolute deviation for all subjects using the two normalization procedures, with the filter response given as <figref idref="DRAWINGS">FIG. <b>5</b>E-<b>1</b></figref> and the weight response (averaging by the mean) given as <figref idref="DRAWINGS">FIG. <b>5</b>E-<b>2</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b>E-<b>1</b></figref> shows the magnitude and frequency response of the pre-processing filters. The blue line represents the M=33 tap low pass FIR filter (line starting at the left most side), while the red line shows the third order IIR Butterworth filter. <figref idref="DRAWINGS">FIG. <b>5</b>E-<b>2</b></figref> shows the <b>64</b> long Hann window weight response used for averaging the rPPG trace.</p><p id="p-0097" num="0093">At <b>516</b> the CHROM algorithm is applied to determine the pulse rate. This algorithm is applied by projecting the signals onto two planes defined by</p><p id="p-0098" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>S</mi>      <mrow>       <mi>c</mi>       <mo>,</mo>       <mn>1</mn>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mn>3</mn>       <mo>&#x2062;</mo>       <msub>        <mi>s</mi>        <mi>r</mi>       </msub>      </mrow>      <mo>-</mo>      <mrow>       <mn>2</mn>       <mo>&#x2062;</mo>       <msub>        <mi>s</mi>        <mi>q</mi>       </msub>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00004-2" num="00004.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>S</mi>      <mrow>       <mi>c</mi>       <mo>,</mo>       <mn>2</mn>      </mrow>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mn>1.5</mn>       <msub>        <mi>s</mi>        <mi>r</mi>       </msub>      </mrow>      <mo>+</mo>      <msub>       <mi>s</mi>       <mi>g</mi>      </msub>      <mo>-</mo>      <mrow>       <mn>1.5</mn>       <msub>        <mi>s</mi>        <mi>b</mi>       </msub>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00004-3" num="00004.3"><math overflow="scroll"> <mtable>  <mtr>   <mtd/>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0099" num="0094">Then the rPPG signal is taken as the difference between the two</p><p id="p-0100" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>chrom</mi>     <mo>=</mo>     <mrow>      <msub>       <mi>S</mi>       <mrow>        <mi>c</mi>        <mo>,</mo>        <mn>1</mn>       </mrow>      </msub>      <mo>-</mo>      <mrow>       <mfrac>        <mrow>         <mi>&#x3c3;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <msub>          <mi>S</mi>          <mrow>           <mi>c</mi>           <mo>,</mo>           <mn>1</mn>          </mrow>         </msub>         <mo>)</mo>        </mrow>        <mrow>         <mi>&#x3c3;</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <msub>          <mi>S</mi>          <mrow>           <mi>c</mi>           <mo>,</mo>           <mn>2</mn>          </mrow>         </msub>         <mo>)</mo>        </mrow>       </mfrac>       <mo>&#x2062;</mo>       <msub>        <mi>S</mi>        <mrow>         <mi>c</mi>         <mo>,</mo>         <mn>2</mn>        </mrow>       </msub>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0101" num="0095">with &#x3c3;( . . . ) is the standard deviation of the signal. Note that the two projected signals were normalized by their maximum fluctuation. The CHROM method is derived to minimize the specular light reflection.</p><p id="p-0102" num="0096">Next at <b>518</b> the projection matrix is applied to determine the pulse rate. For the projection matrix (PM) method the signal is projected to the pulsatile direction. Even though the three elements are not orthogonal, it was surprisingly found that this projection gives a very stable solution with better signal to noise than CHROM. To derive the PM method, the matrix elements of the intensity, specular, and pulsatile elements of the RGB signal are determined:</p><p id="p-0103" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>M</mi>      <mi>measured</mi>     </msub>     <mo>=</mo>     <mrow>      <mo>[</mo>      <mtable>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mn>0.77</mn>        </mtd>        <mtd>         <mn>0.33</mn>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mn>0.51</mn>        </mtd>        <mtd>         <mn>0.77</mn>        </mtd>       </mtr>       <mtr>        <mtd>         <mn>1</mn>        </mtd>        <mtd>         <mn>0.38</mn>        </mtd>        <mtd>         <mn>0.53</mn>        </mtd>       </mtr>      </mtable>      <mo>]</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0104" num="0097">The above matrix elements may be determined for example from a paper by de Haan and van Leest (G de Haan and A van Leest. Improved motion robustness of remote-ppg by using the blood volume pulse signature. Physiological Measurement, 35(9):1913, 2014). In this paper, the signals from arterial blood (and hence from the pulse) are determined from the RGB signals, and can be used to determine the blood volume spectra.</p><p id="p-0105" num="0098">For this example the intensity is normalized to one. The projection to the pulsatile direction is found by inverting the above matrix and choosing the vector corresponding to the pulsatile. This gives:</p><p id="p-0106" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>pm=&#x2212;</i>0.26<i>s</i><sub>r</sub>&#x2212;0.83<i>s</i><sub>g</sub>&#x2212;0.50<i>s</i><sub>b </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0107" num="0099">At <b>520</b>, the two pulse rate results are cross-correlated to determine the rPPG. The determination of the rPPG is explained in greater detail with regard to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0108" num="0100"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> relates to a non-limiting exemplary method for pulse rate estimation and determination of the rPPG, while <figref idref="DRAWINGS">FIGS. <b>6</b>B-<b>6</b>C</figref> relate to some results of this method. The method uses the output of the CHROM and PM rPPG methods, described above with regard to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, to find the pulse rate frequency vpr. This method involves searching for the mean pulse rate over the past Lalgo frames. The frequency is extracted from the output of a match filter (between the CHROM and PM), by using non-linear least square spectral decomposition with the application of a lock-in mechanism.</p><p id="p-0109" num="0101">Turning now to <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, in a method <b>600</b>, the process begins at <b>602</b> by calculating the match filter between the CHROM and PM output. The match filter is simply done by calculating the correlation between CHROM and PM methods output. Next at <b>604</b>, the cost function of a non-linear least squares (NLS) frequency estimation is calculated, based on a periodic function with its harmonics. In the above equation, x is the model output, al and bl are the weight of the frequency components, 1 is its harmonic order, L is number of orders in the model, v is the frequency, and (n) is the additive noise component. Then the log likelihood spectrum is calculated at <b>606</b> by adapting the algorithm given in Nielsen et. al (Jesper Kj&#xe6;r Nielsen, Tobias Lindstrom Jensen, Jesper Rindom Jensen, Mads Gr&#xe6;esb&#xf8;ll Christensen, and Soren Holdt Jensen.</p><p id="p-0110" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mrow>       <mi>x</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>n</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mstyle><mtext>?</mtext></mstyle>        <mo>[</mo>        <mrow>         <mrow>          <mstyle><mtext>?</mtext></mstyle>          <mrow>           <mi>cos</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>2</mn>            <mo>&#x2062;</mo>            <mi>&#x3c0;</mi>            <mo>&#x2062;</mo>            <mi>l</mi>            <mo>&#x2062;</mo>            <mi>&#x3bd;</mi>            <mo>&#x2062;</mo>            <mi>n</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>-</mo>         <mrow>          <mstyle><mtext>?</mtext></mstyle>          <mrow>           <mi>sin</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mn>2</mn>            <mo>&#x2062;</mo>            <mi>&#x3c0;</mi>            <mo>&#x2062;</mo>            <mi>l</mi>            <mo>&#x2062;</mo>            <mi>&#x3bd;</mi>            <mo>&#x2062;</mo>            <mi>n</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>        </mrow>        <mo>]</mo>       </mrow>       <mo>+</mo>       <mrow>        <mi>&#x3f5;</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mi>n</mi>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>10</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00007-2" num="00007.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0111" num="0102">Fast fundamental frequency estimation: Making a statistically efficient estimator computationally efficient. Signal Processing, 135:188-197, 2017) in a computational complexity of O(N log N)+O(NL).</p><p id="p-0112" num="0103">In Nielsen et. A, the frequency is set as the frequency of the maximum peak out of all harmonic orders. The method itself is a general method, which can be adapted in this case by altering the band frequency parameters. An inherent feature of the model is that higher order will have more local maximum peaks in the cost function spectra than lower order. This feature is used for the lock-in procedure.</p><p id="p-0113" num="0104">At <b>608</b>, the lock-in mechanism gets as input the target pulse rate frequency vtraget. Then at <b>610</b>, the method finds all the local maximum peaks amplitude (Ap) and frequency (vp) of the cost function spectrum of order 1=L. For each local maximum, the following function is estimated:</p><p id="p-0114" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>f</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <msub>        <mi>A</mi>        <mi>p</mi>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3bd;</mi>        <mi>p</mi>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3bd;</mi>        <mi>traget</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <msub>       <mi>A</mi>       <mi>p</mi>      </msub>      <mrow>       <semantics definitionURL="">        <mo>&#x2758;</mo>        <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>       </semantics>       <mrow>        <msub>         <mi>&#x3bd;</mi>         <mi>p</mi>        </msub>        <mo>-</mo>        <msub>         <mi>&#x3bd;</mi>         <mi>traget</mi>        </msub>       </mrow>       <semantics definitionURL="">        <mo>&#x2758;</mo>        <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>       </semantics>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>11</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0115" num="0105">This function takes a balance between the signal strength and distance from the target frequency. At <b>610</b>, the output pulse rate is set as local peak vp which maximize the above function f (Ap,vp,vtraget)</p><p id="p-0116" num="0106"><figref idref="DRAWINGS">FIGS. <b>6</b>B and <b>6</b>C</figref> show an exemplary reconstructed rPPG trace (blue line), of an example run. The red circles show the peak R time. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> shows the trace from run start at time t=0s till time t=50s. <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> shows a zoom of the trace and showing also RR interval times in milliseconds.</p><p id="p-0117" num="0107">Next at <b>612</b>-<b>614</b>, the instantaneous rPPG signal is filtered, with two dynamic filters around the mean pulse rate frequency (vpr): Wiener filter, and FFT Gaussian filter. At <b>612</b>, the</p><p id="p-0118" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mtext>                </mtext>     <mrow>      <mrow>       <mi>g</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mi>&#x3bd;</mi>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mstyle><mtext>?</mtext></mstyle>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>12</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00009-2" num="00009.2"><math overflow="scroll"> <mrow>  <mstyle><mtext>?</mtext></mstyle>  <mtext mathsize="6pt">indicates text missing or illegible when filed</mtext> </mrow></math></maths></p><p id="p-0119" num="0000">Wiener filter is applied. The desired target is sin(2&#x3c0;vprn), with n is the index number (representing the time). At <b>614</b>, the FFT Gaussian filter aims to clean the signal around vpr, thus a Gaussian shape of the form<br/>is used with &#x3c3;g as its width. As the name suggests, the filtering is done by transforming the signal to its frequency domain (FFT) and multiplying it by g (v) and transforming back to the time domain and taking the real part component.</p><p id="p-0120" num="0108">The output of the above procedure is a filtered rPPG trace (pm) of length Lalgo with mean pulse rate of vpr. The output is obtained for each observed video frame and constructing the overlapping time series of pulse. These time series must be averaged to produce mean final rPPG trace suitable for HRV processing. This is done using overlapping and addition of filtered rPPG signal (pm) using following formula (n represents time) from a paper by Wang et al (W. Wang, A. C. den Brinker, S. Stuijk, and G. de Haan. Algorithmic principles of remote ppg. IEEE Transactions on Biomedical Engineering, 64(7):1479-1491, July 2017):</p><p id="p-0121" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>t</i>(<i>n</i>&#x2212;Lalgo+1)=<i>t</i>(<i>n</i>&#x2212;Lalgo+1)+<i>w</i>(1)<i>pm</i>(1)&#x2003;&#x2003;(13)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0122" num="0000">with 1 is a running index between 0 and Lalgo; where w(i) is a weight function, that sets the configuration and latency of the output trace. Obtaining then consequent peaks (maxima that represents systolic peak) it is possible construct so called RR intervals as distance in time. Using series of RR intervals is possible to retrieve HRV parameters as statistical measurements in both time and frequency domains.</p><p id="p-0123" num="0109"><figref idref="DRAWINGS">FIGS. <b>7</b> and <b>8</b></figref> relate to methods for creating statistical measures for various parameters, which can then be used for providing the above information, such as for example calculating respiratory rate (RR). The tables relate to the standard set of HRV parameters and are calculated directly from RR intervals aggregated for different time periods. Most of these parameters refer to the statistical presentation of the HR variation in time.</p><p id="p-0124" num="0110"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a non-limiting exemplary method for performing an HRV or heart rate variability time domain analysis. As shown in a method <b>700</b>, processed video signals are obtained at <b>702</b>. The processed video signals are then calculated to determine a heart rate (HR) at <b>703</b>.</p><p id="p-0125" num="0111">The SDRR is calculated at <b>704</b>. The PRR50 is calculated at <b>706</b>. The RMSSD is calculated at <b>708</b>. The triangle is calculated at <b>710</b>. The TINN is calculated at <b>712</b>. The HRV heart rate variability time domain is calculated <b>714</b>.</p><p id="p-0126" num="0112">Steps <b>702</b>-<b>712</b> are preferred repeated at <b>716</b>. The SDARR is calculated at <b>718</b>. The SDRRI is calculated at <b>720</b>. Steps <b>714</b>-<b>720</b> is optionally repeated at <b>722</b>. Then steps <b>702</b>-<b>704</b> are optionally repeated at <b>724</b>. Finally, steps <b>708</b>-<b>714</b> are optionally repeated at <b>726</b>.</p><p id="p-0127" num="0113">The meaning of the acronyms for the HRV time-domain measures are described below:</p><p id="p-0128" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="49pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><colspec colname="3" colwidth="126pt" align="left"/><colspec colname="4" colwidth="77pt" align="left"/><thead><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry/><entry>Calculated after X</entry></row><row><entry>Parameter</entry><entry>Unit</entry><entry>Description</entry><entry>minutes?</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>SDRR</entry><entry>Ms</entry><entry>Standard deviation of RR intervals</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry/><entry>a different meaning)</entry></row><row><entry>SDARR</entry><entry>Ms</entry><entry>Standard deviation of the average RR</entry><entry>5 min</entry></row><row><entry/><entry/><entry>intervals for each 5 min segment of a 24 h</entry></row><row><entry/><entry/><entry>HRV recording</entry></row><row><entry>SDRR index</entry><entry>Ms</entry><entry>Mean of the standard deviations of all the</entry><entry>5 min</entry></row><row><entry>(SDRRI)</entry><entry/><entry>RR intervals for each 5 min segment of a</entry></row><row><entry/><entry/><entry>24 h HRV recording</entry></row><row><entry>pRR50</entry><entry>%</entry><entry>Percentage of successive RR intervals that</entry><entry>2, 5 min (each with a</entry></row><row><entry/><entry/><entry>differ by more than 50 ms</entry><entry>different meaning)</entry></row><row><entry>RMSSD</entry><entry>Ms</entry><entry>Root mean square of successive RR</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>interval differences</entry><entry>a different meaning)</entry></row><row><entry>HRV triangular</entry><entry/><entry>Integral of the density of the RR interval</entry><entry>2, 5, 15 min (each with</entry></row><row><entry>index</entry><entry/><entry>histogram divided by its height</entry><entry>a different meaning)</entry></row><row><entry>TINN</entry><entry>Ms</entry><entry>Baseline width of the RR interval</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>histogram</entry><entry>a different meaning)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry namest="1" nameend="4" align="left" id="FOO-00001">*Inter-beat interval, time interval between successive heartbeats; NN intervals, inter-beat intervals from which artifacts have been removed; RR intervals, inter-beat intervals between all successive heartbeats.</entry></row></tbody></tgroup></table></tables></p><p id="p-0129" num="0114">The following parameters may be calculated according to information provided in F. Shaffer and J. P. Ginsberg (An Overview of Heart Rate Variability Metrics and Norms, Front Public Health. 2017; 5: 258), which is hereby incorporated by reference as if fully set forth herein: SDRR, RMSSD, triangle (HRV triangular index), and TINN.</p><p id="p-0130" num="0115">The following parameter may be calculated according to information provided in Umetani et al (Twenty-four hour time domain heart rate variability and heart rate: relations to age and gender over nine decades, J Am Coll Cardiol. 1998 Mar. 1;31(3):593-601): HRV time domain.</p><p id="p-0131" num="0116">The following parameters may be calculated according to information provided in O. Murray (The Correlation Between Heart Rate Variability and Diet, Proceedings of The National Conference On Undergraduate Research (NCUR) 2016, North Carolina): SDRRI (SDRR index). SDARR and pRR50.</p><p id="p-0132" num="0117"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a non-limiting exemplary method for calculating the heart rate variability or HRV frequency domain. In a method <b>800</b>, processed video signals are obtained as previously described at <b>802</b>. Heart rate is calculated as previously described at <b>803</b>. The ULF is calculated at <b>804</b>. The VLF is calculated at <b>806</b>. The LF peak is calculated at <b>808</b>.</p><p id="p-0133" num="0118">LF power is calculated at <b>810</b>. The HF peak is calculated at <b>812</b>. HF power is calculated at <b>814</b>. The ratio of LF to HF is calculated at <b>816</b>. The HRV or heart rate variability frequency is calculated at <b>814</b>. Steps <b>802</b>-<b>818</b> are optionally repeated at a first interval at <b>820</b>. Then, steps <b>802</b>-<b>808</b> are optionally repeated at a second interval at <b>822</b>.</p><p id="p-0134" num="0119">The meaning of the acronyms for the HRV frequency-domain measures are described in greater detail below:</p><p id="p-0135" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><colspec colname="3" colwidth="147pt" align="left"/><colspec colname="4" colwidth="77pt" align="left"/><thead><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry/><entry>Calculated after X</entry></row><row><entry>Parameter</entry><entry>Unit</entry><entry>Description</entry><entry>minutes?</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>ULF power</entry><entry>ms2</entry><entry>Absolute power of the ultra-low-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(&#x2264;0.003 Hz)</entry><entry>a different meaning)</entry></row><row><entry>VLF power</entry><entry>ms2</entry><entry>Absolute power of the very-low-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.0033-0.04 Hz)</entry><entry>a different meaning)</entry></row><row><entry>LF peak</entry><entry>Hz</entry><entry>Peak frequency of the low-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.04-0.15 Hz)</entry><entry>a different meaning)</entry></row><row><entry>LF power</entry><entry>ms2</entry><entry>Absolute power of the low-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.04-0.15 Hz)</entry><entry>a different meaning)</entry></row><row><entry>LF power</entry><entry>nu</entry><entry>Relative power of the low-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.04-0.15 Hz) in normal units</entry><entry>a different meaning)</entry></row><row><entry>LF power</entry><entry>%</entry><entry>Relative power of the low-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.04-0.15 Hz)</entry><entry>a different meaning)</entry></row><row><entry>HF peak</entry><entry>Hz</entry><entry>Peak frequency of the high-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.15-0.4 Hz)</entry><entry>a different meaning)</entry></row><row><entry>HF power</entry><entry>ms2</entry><entry>Absolute power of the high-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.15-0.4 Hz)</entry><entry>a different meaning)</entry></row><row><entry>HF power</entry><entry>nu</entry><entry>Relative power of the high-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.15-0.4 Hz) in normal units</entry><entry>a different meaning)</entry></row><row><entry>HF power</entry><entry>%</entry><entry>Relative power of the high-frequency band</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry>(0.15-0.4 Hz)</entry><entry>a different meaning)</entry></row><row><entry>LF/HF</entry><entry>%</entry><entry>Ratio of LF-to-HF power</entry><entry>2, 5, 15 min (each with</entry></row><row><entry/><entry/><entry/><entry>a different meaning)</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0136" num="0120">Additionally or alternatively, various non-linear measures may be determined for calculating HRV:</p><p id="p-0137" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="21pt" align="left"/><colspec colname="3" colwidth="147pt" align="left"/><colspec colname="4" colwidth="56pt" align="left"/><thead><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry/><entry/><entry>Calculated after X</entry></row><row><entry>Parameter</entry><entry>Unit</entry><entry>Description</entry><entry>minutes?</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>S</entry><entry>ms</entry><entry>Area of the ellipse which represents total HRV</entry><entry>over 5 min</entry></row><row><entry>SD1</entry><entry>ms</entry><entry>Poincar&#xe9; plot standard deviation perpendicular</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>the line of identity</entry></row><row><entry>SD2</entry><entry>ms</entry><entry>Poincar&#xe9; plot standard deviation along the line of</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>identity</entry></row><row><entry>SD1/SD2</entry><entry>%</entry><entry>Ratio of SD1-to-SD2</entry><entry>over 5 min</entry></row><row><entry>ApEn</entry><entry/><entry>Approximate entropy, which measures the</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>regularity and complexity of a time series</entry></row><row><entry>SampEn</entry><entry/><entry>Sample entropy, which measures the regularity</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>and complexity of a time series</entry></row><row><entry>DFA &#x3b1;1</entry><entry/><entry>Detrended fluctuation analysis, which describes</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>short-term fluctuations</entry></row><row><entry>DFA &#x3b1;2</entry><entry/><entry>Detrended fluctuation analysis, which describes</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>long-term fluctuations</entry></row><row><entry>D2</entry><entry/><entry>Correlation dimension, which estimates the</entry><entry>over 5 min</entry></row><row><entry/><entry/><entry>minimum number of variables required to</entry></row><row><entry/><entry/><entry>construct a model of system dynamics</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0138" num="0121">The following parameters may be calculated according to information provided in the previously described paper by F. Shaffer and J. P. Ginsberg: ULF, VLF, LF peak, LF power, HF peak, HF power, LF/HF and HRV frequency.</p><p id="p-0139" num="0122">It is appreciated that certain features of the invention, which are, for clarity, described in the context of separate embodiments, may also be provided in combination in a single embodiment. Conversely, various features of the invention, which are, for brevity, described in the context of a single embodiment, may also be provided separately or in any suitable sub-combination.</p><p id="p-0140" num="0123">Although the invention has been described in conjunction with specific embodiments thereof, it is evident that many alternatives, modifications and variations will be apparent to those skilled in the art. Accordingly, it is intended to embrace all such alternatives, modifications and variations that fall within the spirit and broad scope of the appended claims. All publications, patents and patent applications mentioned in this specification are herein incorporated in their entirety by reference into the specification, to the same extent as if each individual publication, patent or patent application was specifically and individually indicated to be incorporated herein by reference. In addition, citation or identification of any reference in this application shall not be construed as an admission that such reference is available as prior art to the present invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230000376A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.81mm" wi="76.20mm" file="US20230000376A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230000376A1-20230105-M00002.NB"><img id="EMI-M00002" he="14.48mm" wi="76.20mm" file="US20230000376A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US20230000376A1-20230105-M00003.NB"><img id="EMI-M00003" he="13.04mm" wi="76.20mm" file="US20230000376A1-20230105-M00003.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004 MATH-US-00004-2 MATH-US-00004-3" nb-file="US20230000376A1-20230105-M00004.NB"><img id="EMI-M00004" he="12.02mm" wi="76.20mm" file="US20230000376A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230000376A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.01mm" wi="76.20mm" file="US20230000376A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230000376A1-20230105-M00006.NB"><img id="EMI-M00006" he="8.81mm" wi="76.20mm" file="US20230000376A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007 MATH-US-00007-2" nb-file="US20230000376A1-20230105-M00007.NB"><img id="EMI-M00007" he="7.37mm" wi="76.20mm" file="US20230000376A1-20230105-M00007.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230000376A1-20230105-M00008.NB"><img id="EMI-M00008" he="6.35mm" wi="76.20mm" file="US20230000376A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009 MATH-US-00009-2" nb-file="US20230000376A1-20230105-M00009.NB"><img id="EMI-M00009" he="7.37mm" wi="76.20mm" file="US20230000376A1-20230105-M00009.TIF" alt="text missing or illegible when filed" img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-01-37" num="01-37"><claim-text><b>1</b>-<b>37</b>. (canceled)</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. A method for obtaining a physiological signal from a subject, the method comprising obtaining optical data from a face of the subject with a camera, analyzing the optical data to select data related to the face of the subject with a computational device in communication with said camera, detecting optical data from a skin of the face, determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and calculating the physiological signal from the time series; wherein said detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, selecting the face boundary with the highest probability and applying a histogram analysis to video data from the face; wherein said histogram analysis comprises a histogram based classifier with a soft thresholding mechanism.</claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein the optical data comprises video data, and wherein said obtaining said optical data comprises obtaining video data of the face of the subject.</claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The method of <claim-ref idref="CLM-00039">claim 39</claim-ref>, wherein said obtaining said optical data further comprises obtaining video data from a mobile phone camera, such that said camera comprises a mobile phone camera.</claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. The method of <claim-ref idref="CLM-00040">claim 40</claim-ref>, wherein said computational device comprises a mobile communication device.</claim-text></claim><claim id="CLM-00042" num="00042"><claim-text><b>42</b>. The method of <claim-ref idref="CLM-00041">claim 41</claim-ref>, wherein said mobile phone camera comprises a front facing camera.</claim-text></claim><claim id="CLM-00043" num="00043"><claim-text><b>43</b>. The method of <claim-ref idref="CLM-00042">claim 42</claim-ref>, wherein said computational device is physically separate from, but in communication with, said mobile phone camera.</claim-text></claim><claim id="CLM-00044" num="00044"><claim-text><b>44</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein said determining said plurality of face boundaries comprises applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries.</claim-text></claim><claim id="CLM-00045" num="00045"><claim-text><b>45</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref> wherein said obtaining said optical data further comprises obtaining video data of the skin of a finger of the subject.</claim-text></claim><claim id="CLM-00046" num="00046"><claim-text><b>46</b>. The method of <claim-ref idref="CLM-00045">claim 45</claim-ref>, wherein said obtaining said video data comprises obtaining video data of the skin of a fingertip of the subject by placing said fingertip on said camera.</claim-text></claim><claim id="CLM-00047" num="00047"><claim-text><b>47</b>. The method of <claim-ref idref="CLM-00046">claim 46</claim-ref>, wherein said camera for obtaining video data of said fingertip comprises a mobile phone camera.</claim-text></claim><claim id="CLM-00048" num="00048"><claim-text><b>48</b>. The method of <claim-ref idref="CLM-00047">claim 47</claim-ref> wherein said fingertip on said mobile phone camera further comprises activating a flash associated with said mobile phone camera to provide light.</claim-text></claim><claim id="CLM-00049" num="00049"><claim-text><b>49</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein said detecting said optical data from said skin of the face comprises determining a plurality of face or fingertip boundaries, selecting the face or fingertip boundary with the highest probability and applying a histogram analysis to video data from the face or fingertip.</claim-text></claim><claim id="CLM-00050" num="00050"><claim-text><b>50</b>. The method of <claim-ref idref="CLM-00049">claim 49</claim-ref>, wherein said determining said plurality of face or fingertip boundaries comprises applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face or fingertip boundaries.</claim-text></claim><claim id="CLM-00051" num="00051"><claim-text><b>51</b>. The method of <claim-ref idref="CLM-00050">claim 50</claim-ref>, further comprising combining analyzed data from images of the face and fingertip to determine the physiological measurement.</claim-text></claim><claim id="CLM-00052" num="00052"><claim-text><b>52</b>. The method of <claim-ref idref="CLM-00051">claim 51</claim-ref>, wherein said determining the physiological signal further comprises combining meta data with measurements from said at least one physiological signal, wherein said meta data comprises one or more of weight, age, height, biological gender, body fat percentage and body muscle percentage of the subject.</claim-text></claim><claim id="CLM-00053" num="00053"><claim-text><b>53</b>. The method of <claim-ref idref="CLM-00052">claim 52</claim-ref>, wherein said physiological signal is selected from the group consisting of stress, blood pressure, breath volume, and pSO2 (oxygen saturation).</claim-text></claim><claim id="CLM-00054" num="00054"><claim-text><b>54</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, further comprising creating PPG signals from said detected optical data by calculating an rPPG trace signal using said time series;<claim-text>estimating mean pulse rate from rPPG trace signals; calculating a mean instantaneous pulse rate; determining a rPPG signal according to PM (projection matrix) applying adaptive Wiener filtering and with an initial signal determined according to said instantaneous pulse rate frequency.</claim-text></claim-text></claim><claim id="CLM-00055" num="00055"><claim-text><b>55</b>. The method of <claim-ref idref="CLM-00054">claim 54</claim-ref>, wherein said mean pulse rate is estimated using a match filter between two rPPG different analytic signals constructed from raw interpolated data (CHROM like and Projection Matrix (PM)).</claim-text></claim><claim id="CLM-00056" num="00056"><claim-text><b>56</b>. The method of <claim-ref idref="CLM-00055">claim 55</claim-ref>, wherein a cross-correlation is calculated to determine said mean instantaneous pulse rate, wherein frequency estimation is calculated according to non-linear least square (NLS) spectral decomposition.</claim-text></claim><claim id="CLM-00057" num="00057"><claim-text><b>57</b>. The method of <claim-ref idref="CLM-00056">claim 56</claim-ref>, wherein said PPG signals are further determined by applying an additional filter in the frequency domain to force signal reconstruction and an exponential filter applied on instantaneous RR values.</claim-text></claim><claim id="CLM-00058" num="00058"><claim-text><b>58</b>. A system for obtaining a physiological signal from a subject, the system comprising: a camera for obtaining optical data from a face of the subject, a user computational device for receiving optical data from said camera, wherein said user computational device comprises a processor and a memory for storing a plurality of instructions, wherein said processor executes said instructions for analyzing the optical data to select data related to the face of the subject, detecting optical data from a skin of the face, determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and calculating the physiological signal from the time series; wherein said detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, selecting the face boundary with the highest probability and applying a histogram analysis to video data from the face; wherein said histogram analysis comprises a histogram based classifier with a soft thresholding mechanism.</claim-text></claim><claim id="CLM-00059" num="00059"><claim-text><b>59</b>. The system of <claim-ref idref="CLM-00058">claim 58</claim-ref>, wherein said memory is configured for storing a defined native instruction set of codes and said processor is configured to perform a defined set of basic operations in response to receiving a corresponding basic instruction selected from the defined native instruction set of codes stored in said memory; wherein said memory stores a first set of machine codes selected from the native instruction set for analyzing the optical data to select data related to the face of the subject, a second set of machine codes selected from the native instruction set for detecting optical data from a skin of the face, a third set of machine codes selected from the native instruction set for determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and a fourth set of machine codes selected from the native instruction set for calculating the physiological signal from the time series.</claim-text></claim><claim id="CLM-00060" num="00060"><claim-text><b>60</b>. The system of <claim-ref idref="CLM-00059">claim 59</claim-ref>, wherein said determining said plurality of face boundaries comprises applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries, such that said memory further comprises an eighth set of machine codes selected from the native instruction set for applying a multi-parameter convolutional neural net (CNN) to said video data to determine said face boundaries.</claim-text></claim><claim id="CLM-00061" num="00061"><claim-text><b>61</b>. The system of <claim-ref idref="CLM-00060">claim 60</claim-ref>, wherein said camera comprises a mobile phone camera and wherein said optical data is obtained as video data from said mobile phone camera.</claim-text></claim><claim id="CLM-00062" num="00062"><claim-text><b>62</b>. The system of <claim-ref idref="CLM-00061">claim 61</claim-ref>, wherein said computational device comprises a mobile communication device.</claim-text></claim><claim id="CLM-00063" num="00063"><claim-text><b>63</b>. The system of <claim-ref idref="CLM-00062">claim 62</claim-ref>, wherein said mobile phone camera comprises a rear facing camera and a fingertip of the subject is placed on said camera for obtaining said video data.</claim-text></claim><claim id="CLM-00064" num="00064"><claim-text><b>64</b>. The system of <claim-ref idref="CLM-00063">claim 63</claim-ref>, further comprising a flash associated with said mobile phone camera to provide light for obtaining said optical data.</claim-text></claim><claim id="CLM-00065" num="00065"><claim-text><b>65</b>. The system of <claim-ref idref="CLM-00064">claim 64</claim-ref>, wherein said memory further comprises a ninth set of machine codes selected from the native instruction set for determining a plurality of face or fingertip boundaries, a tenth set of machine codes selected from the native instruction set for selecting the face or fingertip boundary with the highest probability, and an eleventh set of machine codes selected from the native instruction set for applying a histogram analysis to video data from the face or fingertip.</claim-text></claim><claim id="CLM-00066" num="00066"><claim-text><b>66</b>. The system of <claim-ref idref="CLM-00065">claim 65</claim-ref>, further comprising combining analyzed data from images of the face and fingertip to determine the physiological measurement according to said instructions executed by said processor.</claim-text></claim><claim id="CLM-00067" num="00067"><claim-text><b>67</b>. The system of <claim-ref idref="CLM-00066">claim 66</claim-ref>, further comprising a display for displaying the physiological measurement and/or signal; wherein said user computational device further comprises said display; and wherein said user computational device further comprises a transmitter for transmitting said physiological measurement and/or signal.</claim-text></claim><claim id="CLM-00068" num="00068"><claim-text><b>68</b>. A system for obtaining a physiological signal from a subject, the system comprising: a rear facing camera for obtaining optical data from a finger of the subject, a user computational device for receiving optical data from said camera, wherein said user computational device comprises a processor and a memory for storing a plurality of instructions, wherein said processor executes said instructions for analyzing the optical data to select data related to the face of the subject, detecting optical data from a skin of the finger, determining a time series from the optical data by collecting the optical data until an elapsed period of time has been reached and then calculating the time series from the collected optical data for the elapsed period of time; and calculating the physiological signal from the time series; wherein said detecting said optical data from said skin of the face comprises determining a plurality of face boundaries, selecting the face boundary with the highest probability and applying a histogram analysis to video data from the face; wherein said histogram analysis comprises a histogram based classifier with a soft thresholding mechanism.</claim-text></claim></claims></us-patent-application>