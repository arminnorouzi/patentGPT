<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005576A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005576</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782344</doc-number><date>20201204</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>10</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>10</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e43">ANALYSIS OF SELECTIVELY NORMALIZED SPATIAL REPRESENTATIONS OF DATA</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62944063</doc-number><date>20191205</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Multiplai Health Ltd.</orgname><address><city>London</city><country>GB</country></address></addressbook><residence><country>GB</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Miriuka</last-name><first-name>Santiago Gabriel</first-name><address><city>Buenos Aires</city><country>AR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Ramondt</last-name><first-name>Mark Paul</first-name><address><city>Buenos Aires</city><country>AR</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/IB2020/061534</doc-number><date>20201204</date></document-id><us-371c12-date><date>20220603</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer that analyzes data is described. During operation, the computer may access the data in the memory. Then, the computer may transform the data into a spatial representation. For example, for biological data, the transformation may be based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome. Moreover, the computer may selectively normalize the transformed data to obtain normalized transformed data. Notably, the selective normalization may use different normalization ranges based at least in part on expression levels in a type of biological sequencing. Next, the processor may convert the normalized transformed data into an output image. Furthermore, the processor may analyze the image using an image-analysis technique (such as a pretrained neural network) to determine a classification. Additionally, the processor may perform: storing the classification; displaying the classification; and/or providing the classification to an electronic device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="136.74mm" wi="133.18mm" file="US20230005576A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="149.44mm" wi="135.21mm" file="US20230005576A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="144.95mm" wi="87.29mm" file="US20230005576A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="99.91mm" wi="138.26mm" file="US20230005576A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="178.82mm" wi="101.18mm" orientation="landscape" file="US20230005576A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="211.24mm" wi="132.76mm" file="US20230005576A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="188.38mm" wi="102.36mm" orientation="landscape" file="US20230005576A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="113.37mm" wi="156.55mm" file="US20230005576A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="220.90mm" wi="150.45mm" file="US20230005576A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The application claims the priority benefit of U.S. Provisional Patent Application Ser. No. 62/944,063, filed Dec. 5, 2019, the contents of which are herein incorporated by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Field</heading><p id="p-0003" num="0002">The described embodiments relate, generally, to techniques for analyzing data by transforming the data into a selectively normalized, two-dimensional (2D) spatial representation.</p><heading id="h-0004" level="1">Related Art</heading><p id="p-0004" num="0003">Recent advances in the sequencing of biological samples has resulted in ever-larger datasets. For example, biological samples can be sequenced to determine genomic, transcriptomic, epigenomic, proteomic, and/or metabolic information. In principle, these large datasets can be analyzed to increase understanding of underlying biologic processes, to identify the pathophysiology of disease, to diagnose medical conditions, and to guide patient care.</p><p id="p-0005" num="0004">However, in practice it is typically difficult to analyze biological datasets. Notably, the large size of biological datasets is often a computational challenge. Moreover, gaps in existing knowledge usually complicate or obstruct the analysis.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">A computer that analyzes data is described. This computer includes: an interface circuit, a processor, and memory that stores the data. During operation, the processor may access the data in the memory. Then, the processor may transform the data into a spatial representation. Moreover, the processor may selectively normalize the transformed data to obtain normalized transformed data. Next, the processor may convert the normalized transformed data into an output image. Furthermore, the processor may analyze the image using an image-analysis technique to determine a classification. Additionally, the processor may perform at least one of: storing the classification in the memory; displaying the classification on a display; or providing, from the interface circuit, the classification addressed to an electronic device.</p><p id="p-0007" num="0006">Note that the data may include temporal data (such as longitudinal data).</p><p id="p-0008" num="0007">Moreover, the data may include spatial data.</p><p id="p-0009" num="0008">Furthermore, the data may include biological data, and the transformation may be based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome. For example, the biological data may include: genomic data, proteomics data, transcriptomic data, epigenomic data, mitochondria data, electrical signals, and/or metabolic data.</p><p id="p-0010" num="0009">Additionally, a given pixel in the image may represent a predefined or predetermined range of bases in the genome. Moreover, the image may have a dynamic or a variable encoding, so that at least some pixels represent different ranges of bases in the genome. For example, the variable encoding may be based at least in part on different information content in different portions of the data (such as mutual information or entropy). Alternatively, the image may have a fixed encoding, so that the given pixel represents a constant range of bases in the genome.</p><p id="p-0011" num="0010">In some embodiments, the image-analysis technique includes a pretrained neural network.</p><p id="p-0012" num="0011">Note that a given pixel in the image may represent data from multiple biological samples.</p><p id="p-0013" num="0012">Moreover, different pixels in the image may correspond to different types of tissue or different types of biological samples.</p><p id="p-0014" num="0013">Furthermore, the selective normalization may involve: using a first normalization range for data having a first intensity less than a threshold; and using a second (different) normalization range for data having a second intensity greater than the threshold. Note that a given intensity may correspond to an expression level in a type of biological sequencing.</p><p id="p-0015" num="0014">In some embodiments, a given pixel in the image has an associated intensity and a color in a color space (such as RGB or YCbCr).</p><p id="p-0016" num="0015">In some embodiments, a method of diagnosing a condition is also performed by a computer. The method may include any one or more of the preceding steps or features.</p><p id="p-0017" num="0016">Another embodiment provides a computer-readable storage medium for use with the computer. This computer-readable storage medium includes program instructions for at least some of the operations performed by the computer.</p><p id="p-0018" num="0017">Another embodiment provides a method for analyzing data. This method includes at least some of the operations performed by the computer.</p><p id="p-0019" num="0018">Another embodiment provides an integrated circuit that performs at least some of the operations performed by the computer.</p><p id="p-0020" num="0019">This Summary is provided merely for purposes of illustrating some exemplary embodiments, so as to provide a basic understanding of some aspects of the subject matter described herein. Accordingly, it will be appreciated that the above-described features are merely examples and should not be construed to narrow the scope or spirit of the subject matter described herein in any way. Other features, aspects, and advantages of the subject matter described herein will become apparent from the following Detailed Description, Figures, and Claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example of communication among electronic devices in accordance with an embodiment of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow diagram illustrating an example method for analyzing data in accordance with an embodiment of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a drawing illustrating example communication among components in an electronic device in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with an embodiment of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a drawing illustrating an example of transforming data into a spatial representation in accordance with an embodiment of the present disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a drawing illustrating an example of selective normalization in accordance with an embodiment of the present disclosure.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a drawing illustrating an example of converting transformed data into an image in accordance with an embodiment of the present disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating an example of one of the electronic devices of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance with an embodiment of the present disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flow diagram illustrating an example method of diagnosing a condition in accordance with an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0029" num="0028">Note that like reference numerals refer to corresponding parts throughout the drawings. Moreover, multiple instances of the same part are designated by a common prefix separated from an instance number by a dash.</p><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0030" num="0029">A computer that analyzes data is described. During operation, the computer may access the data in the memory. Then, the computer may transform the data into a spatial representation. For example, the data may include biological data, and the transformation may be based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome. Moreover, the computer may selectively normalize the transformed data to obtain normalized transformed data. Notably, the selective normalization may use different normalization ranges based at least in part on expression levels in a type of biological sequencing. Next, the processor may convert the normalized transformed data into an output image. Furthermore, the processor may analyze the image using an image-analysis technique (e.g., deep learning, pretrained or not pretrained neural network) to determine a classification. Additionally, the processor may perform: storing the classification in memory; displaying the classification on a display; and/or providing the classification to an electronic device.</p><p id="p-0031" num="0030">By analyzing the data, the analysis techniques may facilitate faster, cheaper and more-accurate classification. For example, by transforming the data into the image, the analysis techniques may leverage one or more image-analysis techniques to perform the classification. This capability may simplify the computations involved in analyzing the data. Consequently, the analysis techniques may improve the user experience when analyzing data.</p><p id="p-0032" num="0031">In the discussion that follows electronic devices and computers may include radios or, more generally, network interfaces that communicate packets or frames in accordance with one or more communication protocols, such as: an Institute of Electrical and Electronics Engineers (IEEE) 802.11 standard (which is sometimes referred to as &#x2018;Wi-Fi&#xae;,&#x2019; from the Wi-Fi&#xae; Alliance of Austin, Tex.), Bluetooth&#x2122; (from the Bluetooth Special Interest Group of Kirkland, Wash.), a cellular-telephone communication protocol, another type of wireless interface, a wired network communication protocol (e.g., Ethernet, Ethernet II or an IEEE 802.3 standard, which are individually or collectively henceforth referred to as &#x2018;Ethernet&#x2019;) and/or another network communication protocol. For example, the cellular-telephone communication protocol may include or may be compatible with: a 2<sup>nd </sup>generation or mobile telecommunication technology, a 3<sup>rd </sup>generation of mobile telecommunications technology (such as a communication protocol that complies with the International Mobile Telecommunications-2000 specifications by the International Telecommunication Union of Geneva, Switzerland), a 4<sup>th </sup>generation of mobile telecommunications technology (such as a communication protocol that complies with the International Mobile Telecommunications Advanced specification by the International Telecommunication Union of Geneva, Switzerland), a 5<sup>th </sup>generation of mobile telecommunications technology (International Mobile Telecommunications (IMT)-2020 network), and/or another cellular-telephone communication technique. In some embodiments, the communication protocol includes Long Term Evolution or LTE. However, a wide variety of communication protocols may be used. In addition, the communication may occur via a wide variety of frequency bands.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> presents a block diagram illustrating an example of communication between an electronic device <b>110</b> (such as a computer, a portable electronic device, a cellular telephone, a tablet computer, a smartwatch, a wearable device, etc.) and the computer <b>112</b> (such as a cloud-based computer or server). Electronic device <b>110</b> and computer may communicate with each other using wired (or non-wireless communication) via network <b>114</b> (such as the Internet) and/or optional wireless communication via a cellular-telephone network <b>118</b> (e.g., via an optional base station <b>116</b>), a wireless local area network (e.g., via an optional access point <b>120</b>) and/or another wireless communication technique. Note that computer <b>112</b> may be located remotely from electronic device <b>110</b> (such as at a different geographic location) or in proximity to electronic device <b>110</b>. Moreover, note that the optional access point <b>120</b> may provide access to network <b>114</b>, such as the Internet, via an Ethernet protocol, and may be a physical access point or a virtual or &#x2018;software&#x2019; access point that is implemented on a computer or an electronic device.</p><p id="p-0034" num="0033">As described further below with reference to <figref idref="DRAWINGS">FIGS. <b>2</b>-<b>6</b></figref>, computer <b>112</b> may transform data into a spatial representation (such as a vector). Then, an image may be determined by selectively normalizing the transformed data. For example, the selective normalization may involve: using a first normalization range for data having a first read count less than a threshold, the first read count being transformed into a first intensity; and using a second (different) normalization range for data having a second read count greater than the threshold, the second read count being transformed into a second intensity. Note that a given intensity may correspond to an expression level in a type of biological sequencing. In some embodiments, a given pixel in the image may have an associated intensity and a color in a color space (such as RGB or YCbCr).</p><p id="p-0035" num="0034">Moreover, computer <b>112</b> may perform an image-analysis technique on the image in order to classify the data. More generally, the image-analysis technique may be performed by one or more computers or electronic devices. In some embodiments, the image-analysis uses a pretrained neural network.</p><p id="p-0036" num="0035">Additionally, computer <b>112</b> may perform at least one of: storing the classification in the memory; displaying the classification on a display; or providing the classification to electronic device <b>110</b>.</p><p id="p-0037" num="0036">Note that the data may include temporal data (such as longitudinal data) and/or spatial data. Furthermore, the data may include biological data, and the transformation may be based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome. For example, the biological data may include: genomic data, transcriptomics data, proteomics data, epigenomic data, mitochondria data, microbiome data, electrical signals and/or metabolic data. In some embodiments, a given pixel in the image may represent data from multiple biological samples. Alternatively, or additionally, different pixels in the image may correspond to different types of tissue or different types of biological samples.</p><p id="p-0038" num="0037">In these ways, the analysis techniques may be used to improve analysis of the data. Notably, the analysis techniques may allow one or more image-analysis techniques to be used to perform the classification. This capability may simplify the computations involved in analyzing the data, may reduce a cost of the analysis, may decrease a run time of the analysis, and/or may improve an accuracy of the classification. These capabilities may enhance the user experience of an individual or a user when using computer <b>112</b>.</p><p id="p-0039" num="0038">As noted previously, in some embodiments, communication among components in <figref idref="DRAWINGS">FIG. <b>1</b></figref> involves wired and/or wireless communication. During the wireless communication, electronic device <b>110</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> may: transmit advertising frames on wireless channels, detect one another by scanning wireless channels, establish wireless connections (for example, by transmitting association requests), and/or transmit and receive packets or frames (which may include the association requests and/or additional information as payloads). Moreover, during the wired communication, electronic device <b>110</b> and/or the computer <b>112</b> may receive packets or frames using a wired communication technique or protocol (e.g., Ethernet II or an IEEE 802.3 standard). In some embodiments, the optional base station <b>116</b> and/or the optional access point <b>120</b> may convert packets or frames that are received using the wired communication technique to a WLAN communication technique or protocol (such as an IEEE 802.11 standard or an LTE standard), and may wirelessly transmit the packets or frames. Similarly, the optional base station <b>116</b> and/or the optional access point <b>120</b> may: receive packets or frames using a wireless communication technique; convert the packets or frames to the wired communication technique; and transmit the packets or frames. Thus, the optional base station <b>116</b> and/or the optional access point <b>120</b> may perform the functions of an access point.</p><p id="p-0040" num="0039">As described further below with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, electronic device <b>110</b>, computer <b>112</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> may include subsystems, such as: a networking subsystem, a memory subsystem and a processor subsystem. In addition, electronic devices <b>110</b>, computer <b>112</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> may include radios <b>122</b> in the networking subsystems. (Note that radios <b>122</b> may be instances of the same radio or may be different from each other.) More generally, electronic devices <b>110</b>, computer <b>112</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> can include (or can be included within) any electronic devices with the networking subsystems that enable electronic devices <b>110</b>, computer <b>112</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> to communicate with each other using wired communication (e.g., a non-wireless communication technique) and/or wireless communication. The wireless communication can comprise transmitting advertisements on wireless channels to enable electronic devices to make initial contact or detect each other, followed by exchanging subsequent data/management frames (such as association requests and responses) to establish a wireless connection, configure security options (e.g., Internet Protocol Security), and transmit and receive packets or frames via the wireless connection, etc.</p><p id="p-0041" num="0040">As can be seen in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, wireless signals <b>124</b> (represented by a jagged line) are optionally transmitted from radio <b>122</b>-<b>1</b> in electronic device <b>110</b>. These wireless signals are optionally received by at least the optional access point <b>120</b>. Notably, electronic device <b>110</b> may optionally transmit packets. In turn, these packets may be optionally received by a radio <b>122</b>-<b>2</b> in the optional access point <b>120</b>. This may allow electronic device <b>110</b> to wirelessly communicate information to the optional access point <b>120</b>. While <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates electronic device <b>110</b> transmitting packets, note that electronic device <b>110</b> may also receive packets from the optional access point <b>120</b>.</p><p id="p-0042" num="0041">In the described embodiments, processing of a packet or frame in electronic device <b>110</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> includes: receiving signals (such as wireless signals <b>124</b>) with the packet or frame; decoding/extracting the packet or frame from the received signals to acquire the packet or frame; and processing the packet or frame to determine information contained in the packet or frame.</p><p id="p-0043" num="0042">Note that the communication among electronic device <b>110</b>, computer <b>112</b>, the optional base station <b>116</b> and/or the optional access point <b>120</b> may be characterized by a variety of performance metrics, such as: a data rate, a data rate for successful communication (which is sometimes referred to as a &#x2018;throughput&#x2019;), an error rate (such as a retry or resend rate), a mean-square error of equalized signals relative to an equalization target, intersymbol interference, multipath interference, a signal-to-noise ratio, a width of an eye pattern, a ratio of number of bytes successfully communicated during a time interval (such as 1-10 s) to an estimated maximum number of bytes that can be communicated in the time interval (the latter of which is sometimes referred to as the &#x2018;capacity&#x2019; of a channel or link), and/or a ratio of an actual data rate to an estimated data rate (which is sometimes referred to as &#x2018;utilization&#x2019;).</p><p id="p-0044" num="0043">Although we describe the network environment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> as an example, in alternative embodiments, different numbers or types of electronic devices may be present. For example, some embodiments comprise more or fewer electronic devices. As another example, in another embodiment, different electronic devices are transmitting and/or receiving packets or frames. While electronic device <b>110</b> and optional access point <b>120</b> are illustrated with a single instance of radios <b>122</b>, in other embodiments electronic device <b>110</b>, optional access point <b>120</b> and/or another component in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may include multiple radios.</p><p id="p-0045" num="0044">We now describe embodiments of the analysis techniques. <figref idref="DRAWINGS">FIG. <b>2</b></figref> presents a flow diagram illustrating example method <b>200</b> for analyzing data, which may be performed by a computer, such as computer <b>112</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. During operation, the computer may access data (operation <b>210</b>) in memory. For example, the data may include temporal data (such as longitudinal data) and/or spatial data. For example, continuous time data may be segmented into temporal subsets based at least in part on a delimiter, such as fixed events in a timeline. Moreover, the data may include biological data, and the transformation may be based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome. In some embodiments, the biological data includes: genomic data, proteomics data, transcriptomic data, epigenomic data, mitochondria data, microbiome data, electrical signals, and/or metabolic data.</p><p id="p-0046" num="0045">Then, the computer may transform the data into a spatial representation (operation <b>212</b>). Moreover, the computer may selectively normalize the transformed data (operation <b>214</b>) to obtain normalized transformed data. For example, the selective normalization may involve: using a first normalization range for data having a first read count less than a threshold (such as, e.g., a threshold of 10 or 100 in a range of 0 to 255); and using a second (different) normalization range for data having a second read count greater than the threshold. The first normalization range may be transformed into a first intensity range (e.g., 0 to 255 for RGB) and the second normalization range may be transformed in a second intensity range (e.g., 128 to 255 for RGB). Note that a given intensity may correspond to an expression level in a type of biological sequencing.</p><p id="p-0047" num="0046">Next, the computer may convert the normalized transformed data into an output image (operation <b>216</b>). In some embodiments, a given pixel in the image has an associated intensity and a color in a color space (such as RGB or YCbCr).</p><p id="p-0048" num="0047">Note that a given pixel in the image may represent a predefined or predetermined range of bases in the genome. Moreover, the image may have a dynamic or a variable encoding, so that at least some pixels represent different ranges of bases in the genome. For example, the variable encoding may be based at least in part on different information content in different portions of the data (such as mutual information or entropy). Alternatively, the image may have a fixed encoding, so that the given pixel represents a constant range of bases in the genome. In some embodiments, a given pixel in the image may represent data from multiple biological samples, and/or different pixels in the image may correspond to different types of tissue or different types of biological samples.</p><p id="p-0049" num="0048">Furthermore, the computer may analyze the image using an image-analysis technique to determine a classification (operation <b>218</b>). For example, the image-analysis technique may include a pretrained neural network, an untrained neural network, or any other machine learning technique for data classification (i.e., supported vector machines).</p><p id="p-0050" num="0049">Additionally, the computer may perform one or more additional operations (operation <b>220</b>), including at least one of: storing the classification in the memory; displaying the classification on a display; or providing, from the interface circuit, the classification to an electronic device.</p><p id="p-0051" num="0050">Note that method <b>200</b> may include additional or fewer operations. Moreover, there may be different operations. Furthermore, the order of the operations may be changed, and/or two or more operations may be combined into a single operation or performed at least partially in parallel.</p><p id="p-0052" num="0051">While the preceding discussion illustrated the analysis techniques using a pretrained neural network, in other embodiments the analysis techniques may include or may use one or more pretrained machine-learning models. For example, during the analysis of the image (operation <b>218</b>), the computer may perform one or more feature extraction techniques on the image to determine one or more features. For example, the one or more feature extraction techniques may include: a discrete Fourier transform, principal component analysis and/or JPEG (or compression) analysis. In some embodiments, the one or more feature extraction techniques includes one or more of: an edge or a line-segment detector (such as a Sobel-Feldman operator or Sobel Filter), a texture-based feature detector, a texture-less feature detector, a scale invariant feature transform (SIFT)-like object-detector, a speed-up robust-features (SURF) detector, a binary-descriptor (such as ORB) detector, a binary robust invariant scalable keypoints (BRISK) detector, a fast retinal keypoint (FREAK) detector, a binary robust independent elementary features (BRIEF) detector, a histogram of oriented gradients (HOG), a features from accelerated segment test (FAST) detector, a motion detector (such as a Gaussian-mixture model), etc. After the one or more features are determined, the computer may select a subset of the one or more features. The selected one or more features may be used as inputs to a pretrained machine-learning model that outputs the classification.</p><p id="p-0053" num="0052">Note that the pretrained machine-learning model may be trained using a training dataset and, e.g., gradient descent optimization. For example, the computer may analyze the image using a pretrained classifier or a regression model that was trained using a supervised learning technique (such as a support vector machine, a classification and regression tree, logistic regression, LASSO, linear regression and/or another linear or nonlinear supervised-learning technique). In some embodiments, the one or more pretrained machine-learning models are trained using an unsupervised learning technique (such as a clustering technique).</p><p id="p-0054" num="0053">Moreover, note that the image may be compatible with a wide variety of different resolutions and/or file formats, such as one or more of: a JPEG or JPEG File Interchange format, JPEG 2000, an Exchangeable image file format (Exif), a Tagged Image File Format (TIFF), a Graphics Interchange Format (GIF), a bitmap file format (such as BMP), a Portable Network Graphics (PNG) file format, a Netpbm format, a WebP format, a Better Portable Graphics (BPG) format, a Photoshop file format (from Adobe Systems of San Jose, Calif.), a High Efficiency Image File Format (HEIF) and/or another image file format. Alternatively or additionally, in embodiments where the image includes a video (such as a sequences of images corresponding to a sequence of data), the video may be compatible with a variety of different resolutions and/or file formats, such as one or more of: an Audio Video Interleave (AVI) format, a Flash Video Format (FVF or SWF), a Windows Media Video (WMV), a Quick Time video format, Moving Pictures Expert Group 4 (MPEG 4 or MP4), an MOV format, a matroska (MKV) format, an advanced vide coding, high definition (AVCHD) format, and/or another video file format.</p><p id="p-0055" num="0054">While <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrated the computer performing the operations in method <b>200</b>, in other embodiments two or more computers may perform some or all of the operations. For example, after generating the image (operation <b>216</b>), the computer may provide the image to another computer (or computer system) that performs the analysis of operation <b>218</b>.</p><p id="p-0056" num="0055">Moreover, in some embodiments, the computer may automatically perform one or more measurements (e.g., using a sensor or a measurement device) that determine the data. For example, the computer may sequence a biological sample to determine the data.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>3</b></figref> presents a drawing illustrating example communication among components in computer <b>112</b> while performing method <b>200</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) and between computer <b>112</b> and electronic device <b>110</b>. During operation, processor <b>310</b> in computer <b>112</b> may access data <b>312</b> in memory <b>314</b> in computer <b>112</b>. Then, processor <b>310</b> may transform <b>316</b> data <b>312</b> into a spatial representation (SR). Moreover, processor <b>310</b> may selectively normalize <b>318</b> the transformed data to obtain normalized transformed data.</p><p id="p-0058" num="0057">Next, processor <b>310</b> may convert the normalized transformed data into an output image <b>320</b>. Furthermore, processor <b>310</b> may analyze image <b>320</b> using an image-analysis technique to determine a classification <b>322</b> (and, more generally, an analysis result).</p><p id="p-0059" num="0058">Additionally, processor <b>310</b> may perform at least one of: storing classification <b>322</b> in memory <b>314</b>; displaying classification <b>322</b> on a display <b>324</b> in computer <b>112</b> (such as by providing an instruction <b>326</b> to display <b>324</b> to display classification <b>322</b> or information corresponding to classification <b>322</b>); or providing, from an interface circuit <b>328</b>, classification <b>322</b> to electronic device <b>110</b> (such as by providing an instruction <b>330</b> to interface circuit <b>328</b> to provide one or more packets <b>332</b> or frames with classification <b>322</b> to electronic device <b>110</b>).</p><p id="p-0060" num="0059">While <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates particular operations involving unilateral or bilateral communication (as illustrated by single-arrow and dual-arrow lines), in general each of the operations illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may involve unilateral or bilateral communication.</p><p id="p-0061" num="0060">In some embodiments, the analysis techniques are used to concurrently analyze the data. For example, the analysis techniques may use deep learning to: classify colormap images into different cancer classes; diagnose cancers; determine a colormap of transcripts in a biological sample; and/or expression at a point or locus of a genome of a biological lifeform (such as a human, an animal, a plant, an insect, etc.). In general, the analysis techniques may be used with a wide variety of data, such as: DNA, RNA, proteomic data, epigenomic data, etc.</p><p id="p-0062" num="0061">During the analysis techniques, the spatial representation may align the data with a genome, may convert the transformed data into a colormap (e.g., an image), and then may analyze the image to determine the classification (with high accuracy, e.g., close to 100%). This analysis approach may significantly reduce a data file size from gigabytes to kilobytes. Note that each pixel in the image may represent N bases in the genome, where N is an integer. For example, each pixel may represent 10,000 bases in the genome. Alternatively, each pixel may represent a given chromosome, a single base, or another subset of information.</p><p id="p-0063" num="0062">For example, the computer may receive or may access Fast-Seq raw data. After verifying the quality of the raw data, the computer may determine how to align the raw data to a genome. This calculation may involve the computer quantifying a number of transcripts/gene (e.g., there may be up to 200,000 different transcripts). Then, the computer may transform the data into an image, and align the transcripts with actual genomic positions in the image. Note that the output may indicate a level of expression at the actual position in genome in an image.</p><p id="p-0064" num="0063">Using genomic data as an example, the computer may receive genes aligned to genome and transform them into an image in which each pixel equals a gene or N bases (where N is an integer) or a chromosome. In some embodiments, there may be 2 million pixels in an image.</p><p id="p-0065" num="0064">The image may use an RGB color space and, more generally, may use a multi-level machine-readable code (e.g., similar to a QR code) to represent a relationship between transcripts and the genome. For example, the intensity of the RBG color may correspond to level of expression. An arbitrary color in the color space may be specified by combining red, green and/or blue colors. Notably, each red, green or blue color may be, e.g., represented by a number between 0 and 255, where &#x2018;0&#x2019; is turned off and &#x2018;255&#x2019; is maximum brightness. Consequently, each pixel may be represented by three numbers between 0 and 255. Thus, r:250, g:10 and b:240 (or 250, 10, 240) may specify purple, or r:100, g:100 and b:0 (or 100, 100, 10) may specify dark yellow. In some embodiments, a YCbCr color space may be used.</p><p id="p-0066" num="0065">In an image, a given pixel may represent the same information across one or more biological samples. In addition, the given pixel may represent the same information in a training dataset for the image-analysis technique, such as a machine-learning model, a neural network, etc. Thus, a range of bases in chromosome 1 may represented by a pixel in an upper left-hand corner of an image, etc. Note that no expression may be represented by the color black. It may be possible that only 2% of a genome may be a color other than black, since only a minor part of the gene expression is detected, based, at least in part, on the depth of sequencing.</p><p id="p-0067" num="0066">In some embodiments, a given pixel, which corresponds to a genomic location and transcript expression, may also represent: a time and a transcript relationship, a position and a transcript relationship, a tissue location and a transcript relationship (e.g., based at least in part on the expression of tissue specific genes), etc.</p><p id="p-0068" num="0067">Because an image may have one or more layers, for example three layers, e.g., red, green and blue, where each color has the same information but potentially the same or different orientations, an image may include redundant information. In principle, different colors may be used to represent different information (such as genome, transcriptome, and proteome). For example, a given color may represent a subset of the total genome (e.g., chromosome 1-7 may be represented by one color, chromosome 8-16 by another color, and chromosome 17-23 and mitochondria by a third color). Consequently, an image may include one or more layers (such as one layer or three layers). Even more, an image may be formed by any number of overlaid layers, each one representing different information (for example, one layer per chromosome) and each one on a different or same color space and/or range. For example, chromosome 1 may be a whole layer as described elsewhere herein, but also a subset of a color dimension, ranging from 0 to 56 in the red channel, then chromosome 2 from 57 to 128, and so on, resulting in pixel intensity differences.</p><p id="p-0069" num="0068">The biological sample(s) may be obtained from a variety of sources (such as RNAseq, short read sequencing, long read sequencing, of a biopsy sample). For example, the layers in an image may represent a cell from a single-cell RNAseq. The biological sample may be processed by using different technologies. For example, in some embodiments the biological sample may be analyzed using short-read cDNA reading (e.g., using Illumina technology) or using direct-RNA long reading (e.g., using Oxford Nanopore technology). The processing of the biological sample may or may not include preprocessing modifications. For example, a ribosomal and/or globin RNA depletion techniques may or may not be applied. In some embodiments, the biological sample may be obtained from one or more of: blood, urine, tissue, etc. Moreover, the data may be geo-localized to a position, such as microbiome expression at a particular location in the gut, electrical signals (such as frequency or voltage data) for a particular gastrointestinal location, voltage data for particular portion of heart, etc. Furthermore, the data may correspond to one or more of: RNA (such as transcriptomics, single-cell RNAseq, small RNAseq, circular RNA, and/or linear RNA), DNA, proteomics, epigenomics, etc. However, more generally, the data may include one or more of: in-situ stained sections, MRI data, EEG data, other electrical signals, etc. Additionally, a multi-layer image may be used to represent one or more types of data, such as genomic data, metabolic data, MRI data, EEG data, etc.</p><p id="p-0070" num="0069">In some embodiments, the analysis techniques may use a pretrained or not pretrained neural network that may be further trained on a particular dataset. For example, 80% of images may be used for training and another 20% may be used for testing. Note that images may need to be processed similarly during training and analysis, so that comparisons can be made or pattern analysis can occur. Because it can be difficult to train a machine-learning model with a large data file, training may be facilitated by transforming the initial data to smaller image file without losing information using the analysis techniques described herein.</p><p id="p-0071" num="0070">In an exemplary embodiment of the analysis techniques, a pipeline may start with a raw RNAseq data file. This input data file may be, e.g., several gigabytes in size, depending on how deep sequencing was performed. The data file may undergo transcriptomic analysis. For example, a quality-control check may be performed using a quality-control program, such as FASTQC (from Babraham Institute, Cambridge, UK). The adaptor sequences of the output file may be trimmed off using trimmomatic (Bolger, et al. (2014). &#x201c;Trimmomatic: A flexible trimmer for Illumina Sequence Data.&#x201d; <i>Bioinformatics</i>, Vol. 30: Issue 15: pp. 2114-20, the disclosure of which is herein incorporated by reference in its entirety).</p><p id="p-0072" num="0071">Next, Spliced Transcripts Alignment to a Reference or Spliced Transcripts Alignment to a Reference (STAR) (&#xa9; Alexander Dobin) may be used to align the data to the genome, and the reads may be counted using HTSeq (from the Genome Biology Unit at the European Molecular Biology Laboratory of Heidelberg, Germany) for linear RNA (coding and noncoding) and DCC (from the Dieterich Lab at the University Hospital Heidelberg of Heidelberg, Germany) for circular RNA.</p><p id="p-0073" num="0072">Moreover, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, which presents a drawing illustrating an example of transforming data into a spatial representation, the gene expression level data may be combined with or aligned with genome position. For example, a file combining the HTSeq-counts with genecode (from the Wellcome Trust Sanger Institute of Hinxton, United Kingdom), which may contain gene names and their chromosome positions) may be generated. Notably, when perform the transformation, matching genes in the HTSeq counts and genecode files may be identified. For each match, the genome position (chromosome, start and end bases) and gene expression level may be included in the spatial representation.</p><p id="p-0074" num="0073">Note that a number of bins in the spatial representation may be determined from the genome length divided by the number of pixels of the image. Consequently, the number of bins P in the spatial representation may equal the number of bases in chromosomes divided by the number of pixels in a given image. For example, for an image with 1024&#xd7;1024 pixels, then for 3,088,286,401 bases in a genome, each bin may represent 2,945 bases. Thus, the number of bins may indicate a portion of the genome. Moreover, each bin may represent the RNA expression level in a particular portion.</p><p id="p-0075" num="0074">Table 1 provides program instructions in bash to generate the spatial representation.</p><p id="p-0076" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="259pt" align="left"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>! / bin /bash</entry></row><row><entry>gtf=annotation. gtf</entry></row><row><entry>mkdir files ready</entry></row><row><entry>out=files_ready</entry></row><row><entry>mkdir normcoverage</entry></row><row><entry>out1=norm_coverage</entry></row><row><entry>gtf_temp=$ (cat $ gtf | 'awk BEGIN{FS-&#x201c;\t&#x201d;} {split ($9, a, &#x201c;;&#x201d; ($3~&#x201c;gene&#x201d;)</entry></row><row><entry>print a[1] &#x201c;\ t &#x201d;a[3] &#x201c;\ t &#x201d;$1 &#x201c;\ t &#x201d;$4&#x201c;\ t &#x201d;$5&#x201c;\ t &#x201d;a[5] &#x201c;\ t &#x201d;$7} &#x2018; | \</entry></row><row><entry>sed 's / gene_id&#x201d; &#x201c;//&#x2019; sed &#x2019;s / gene_id &#x201d;// &#x2019; sed &#x2019;s / gene_biotype &#x201c;/ / &#x2019; sed &#x2019;</entry></row><row><entry>&#x2003;s/gene name &#x2033;// &#x2019; | \</entry></row><row><entry>sed &#x2018;s /gene_biotype &#x201c;// &#x2019; | sed &#x2019;s /&#x201d;//g &#x2018; | sed &#x2019;s///g &#x2018; | awk -F&#x201c;\ t&#x201d; &#x2019;{split ($1, a,print a[1], $2,</entry></row><row><entry>$3, $4, $5}&#x2019;)</entry></row><row><entry>for i in raw_data / * .counts</entry></row><row><entry>do</entry></row><row><entry>base=$(basename $ i.counts)</entry></row><row><entry>awk -F&#x201c;\ t&#x201d; &#x2019;{split ($1, a, &#x201c;.&#x201d;); print a[1], $2}&#x2019; $i&#x3e; raw_data/ $base.counts_tmp</entry></row><row><entry>&#x2003;&#x26;&#x26; mv raw_data/$base.counts_tmp raw_data/$base. counts</entry></row><row><entry>echo &#x201c;$gtf temp&#x201d; | awk 'FNR==NR{a[$1]=$2; next} (if (a[$1]==&#x201c;&#x201d;)</entry></row><row><entry>&#x2003;{a[$1]=0};</entry></row><row><entry>print $1&#x201c;\ t &#x201d;$2&#x201c;\ t &#x201d;$3&#x201c;\ t &#x201d;$4&#x201c;\ t &#x201d;$5&#x201c;\ t &#x201d;a[$1] &#x201c;\ t&#x201d; (a[$1] / (($5-$4)/1000))}&#x2019;</entry></row><row><entry>&#x2003;$i - &#x3e; $out/$base.txt</entry></row><row><entry>sum=$ (cat $out/$base.txt | awk -F &#x201c;\ t&#x201d; &#x2018;{s+=$7} END {print s} &#x2019;) # sum total</entry></row><row><entry>&#x2003;counts per sample (for normalization)</entry></row><row><entry>awk -v var=&#x201c;$sum&#x201d; &#x2018;{print $1&#x201c;\ t &#x201d;$2&#x201c;\ t &#x201d;$3&#x201c;\ t &#x201d;$4&#x201c;\ t &#x201d;$5&#x201c;\ t &#x201d;$6&#x201c;\ t&#x201d; (($7</entry></row><row><entry>&#x2003;/(var/1000000)))}&#x2019; $out / $base.txt &#x3e; $out/$base.norm.txt</entry></row><row><entry>rm $out/$base.txt</entry></row><row><entry>echo &#x201c;cove rage with counts (field6=fila 5)/tpm(field 7=fila 6) on file</entry></row><row><entry>&#x2003;$base.norm&#x201d;</entry></row><row><entry>python test_3.py Sout/Sbase.norm. txt $outl/$base.csv</entry></row><row><entry>done</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><heading id="h-0008" level="2">Table 1 (Continued).</heading><p id="p-0077" num="0075">Table 2 provides program instructions in python to use the output of the program instructions in Table 1 to generate the spatial representation by merging gtf and a gene expression file. These program instructions include the binning definition.</p><p id="p-0078" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="217pt" align="left"/><colspec colname="2" colwidth="70pt" align="left"/><thead><row><entry namest="1" nameend="2" rowsep="1">TABLE 2</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>import sys</entry><entry/></row><row><entry>chromos={ }</entry><entry/></row><row><entry>length =[248956422, 242193529, 198295559, 190214555, 181538259,</entry><entry/></row><row><entry>&#x2003;170805979, 159345973, 145138636, 138394717, 133797422, 135086622,</entry><entry/></row><row><entry>&#x2003;133275309, 114364328, 107043718, 101991189, 90338345, 83257441,</entry><entry/></row><row><entry>&#x2003;80373285, 58617616, 64444167, 46709983, 50818468, 156040895,</entry><entry/></row><row><entry>&#x2003;57227415, 16569]</entry><entry/></row><row><entry>bining=1710</entry><entry/></row><row><entry>def filecounter(filename, chromos):</entry><entry/></row><row><entry>&#x2003;filepath = filename</entry><entry/></row><row><entry>&#x2003;with open(filepath) as fp:</entry><entry/></row><row><entry>&#x2003;&#x2003;for line in fp:</entry><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;row=line.split( )</entry><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;start=int (int (row[3]) / bining ) * bining</entry><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;chromos [row[2] ] [ str (start)]+=float (row[6])</entry><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;end=int (int (row [4]) / bining ) * bining</entry><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;for j in range (start+bining, end+bining, bining):</entry><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;chromos [row[2] ] [str(j)]+=float (row[6])</entry><entry/></row><row><entry>for i in range (1, 26):</entry><entry/></row><row><entry>&#x2003;if i==23:</entry><entry/></row><row><entry>&#x2003;&#x2003;chromo=&#x2018;X&#x2019;</entry><entry/></row><row><entry>&#x2003;elif i==24:</entry><entry/></row><row><entry>&#x2003;&#x2003;chromo=&#x2018;Y&#x2019;</entry><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="217pt" align="left"/><colspec colname="2" colwidth="56pt" align="left"/><colspec colname="3" colwidth="14pt" align="left"/><tbody valign="top"><row><entry>&#x2003;elif i==25:</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromo=&#x2018;M&#x2019;</entry><entry/><entry/></row><row><entry>&#x2003;else:</entry><entry/><entry/></row><row><entry>&#x2003;chromo=str(i)</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromos.update({ &#x2018;chr&#x2019;+chromo: { } })</entry><entry/><entry/></row><row><entry>for j in range (0, largos [i -1], bining):</entry><entry/><entry/></row><row><entry>chromos [&#x2018;chr&#x2019;+chromo].update ({str (j): 0})</entry><entry/><entry/></row><row><entry>filename_in=sy s. argv [1]</entry><entry/><entry/></row><row><entry>filename_out=sys. argv[2]</entry><entry/><entry/></row><row><entry>filecounter(fdename_in, chromos)</entry><entry/><entry/></row><row><entry>f = open(filename_out , &#x2019;a&#x2019;)</entry><entry/><entry/></row><row><entry>for i in range (1, 26):</entry><entry>&#x2019;+ str (j+bining) +&#x2019;</entry><entry>&#x2019;+</entry></row><row><entry>&#x2003;if i==23:</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromo=&#x2018;X&#x2018;</entry><entry/><entry/></row><row><entry>&#x2003;elif i==24:</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromo=&#x2018;Y&#x2018;</entry><entry/><entry/></row><row><entry>&#x2003;elif i==25:</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromo=&#x2018;M&#x2018;</entry><entry/><entry/></row><row><entry>&#x2003;else :</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromo=str(i)</entry><entry/><entry/></row><row><entry>&#x2003;for j in range (0, length[i-l], bining):</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;chromos[&#x2018;chr&#x2018;+chromo] [str(j)]</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;f.write(&#x2019; \n &#x2019; + &#x2019; chr &#x2019;+chromo +&#x2019; , &#x2019;+ str(j) +&#x2019;</entry><entry/><entry/></row><row><entry>&#x2003;&#x2003;&#x2003;str(chromos[&#x2019; chr &#x2019;+chromo] [str(j)]))</entry><entry/><entry/></row><row><entry>f.close()</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><heading id="h-0009" level="2">Table 2 (Continued).</heading><p id="p-0079" num="0076">Moreover, the file with the name of each gene, position, and expression level may be normalized according to its expression level. Normalization may be performed based on a count number (e.g., level of expression, number of transcripts per million, number of reads, etc.) and transformed into a reference range of 0 to 256, which are values that corresponds to an RGB image. In some embodiments of the analysis techniques, a double normalization is performed according to the gene expression level. For example, genes with a gene expression level that is less than a threshold value may be normalized from, e.g., 0 to 256. Moreover, genes with a gene expression level that is greater than the threshold value may be normalized to a number between, e.g., 128 to 256. The double normalization may enhance the representation of low-expressed genes. Otherwise, many pixels in the image may be black.</p><p id="p-0080" num="0077">This is illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, which presents a drawing illustrating an example of selective normalization. Notably, in the initial transcriptomic distribution, most of the reads have a low number of counts. If this data is normalized to a range from 0 to 255, the resulting distribution may be skewed, with most genes having very low values. Consequently, the resulting image may be mostly black. In addition, genes with low expression are often long noncoding RNA, which may account for most differences between different types of tissue. Without the double normalization, highly expressed genes (such as coding RNA) may obscure the long noncoding RNA.</p><p id="p-0081" num="0078">In order to avoid these problems, the data may be split between high and low-expressed genes. For example, 100 transcripts/million or 100 reads may be used as a split point between coding genes and noncoding genes. Alternatively, a split point or threshold value may be determined for a particular distribution. After splitting, the data may be separately normalized, such as from 0 to 255 for low expressed genes, and from 128 to 255 in highly expressed genes. Next, both normalizations may be joined in a single distribution.</p><p id="p-0082" num="0079">Furthermore, after the double normalization, the transformed and normalized data may be converted into an image. Notably, the gene expression level may be normalized to an RGB image value. An RGB image may be formed by overlapping three different layers of numbers called channels. A first channel may be used for red, a second channel may be used for green and a third channel may be used for blue. The number of each layer may span from 0 (darkest) to 255 (lighter). Thus, a pixel with a value of (0,0,0) may be black, while a value of (255,255,255) may be white, and a pixel with a value of (255,0,0) may be pure red.</p><p id="p-0083" num="0080">This is illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, which presents an example of converting transformed data into an image. Notably, a bin file may be translated into an image by converting each row into a pixel. For example, a first row may take a first position, and so on. Moreover, a red channel may start at the upper left corner, a green channel may start at the upper right, and a blue channel may start at a lower right. In some embodiments, after the bins in chromosome 1 have been placed, the process may continue with the next chromosome. Note that the bin file may include information for somatic, sexual, and mitochondrial chromosomes.</p><p id="p-0084" num="0081">Table 3 provides program instructions in python to generate the image. Notably, the program instructions may generate a function that is applied to every.csv file that is created by the program instructions in Tables 1 and 2.</p><p id="p-0085" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 3</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>import numpy as np</entry></row><row><entry>from PIL import Image, ImageFilter</entry></row><row><entry>import cv2 as cv</entry></row><row><entry>import pandas as pd</entry></row><row><entry>from scipy import stats</entry></row><row><entry>import os</entry></row><row><entry>from sklearn.preprocessing import MinMaxScaler</entry></row><row><entry>&#x2003;def create_img (indir, outdir):</entry></row><row><entry>&#x2003;&#x2003;for root, dirs, filenames in os.walk (indir):</entry></row><row><entry>&#x2003;&#x2003;&#x2003;for fde in filenames:</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;if file.endswith(&#x2018;.csv&#x2019;):</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;path_file = os.path.join(root, file)</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;df = pd.read csv(path file, header=0, names=[&#x2018;Chr&#x2019;,&#x2018; Start</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2018;,&#x2019;End &#x2019;,&#x2018;Counts&#x2019;])</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;cutoff = 100</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;less cutoff = df.loc[df [&#x2019;Counts &#x2019;] &#x3c;= cutoff]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;df.loc[ df [&#x2018;Counts&#x2019;] &#x3c;= cutoff, &#x2018;norm&#x2019;] = (less_cutoff[&#x2018;Counts&#x2019;]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;/(less_cutoff[&#x2019;Counts&#x2019;].max( )*1)) *255</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;high cutoff = df.loc[df [&#x2019;Counts&#x2019;] &#x3e; cutoff]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;df.loc[ df [&#x2019;Counts &#x2019;] &#x3e; cutoff, &#x2019;norm&#x2019;] = (((high_cutoff[&#x2019;Counts&#x2019;</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;] - high_cutoff [&#x2018;Counts']min() )/((high_cutoff[&#x2019;Counts&#x2019;]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;.max() - high_cutoff [&#x2018;Counts']min( ))*2) + 0.5))*255</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;df array = df.drop'Chr &#x2019;,&#x2019; Start &#x2019;,&#x2019;End&#x2019;, &#x2019;Counts&#x2019;],</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;axis=l). values</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;df_zeros = np.zeros((311, 1))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;df_plus = np.concatenate((df_array, df_zeros))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;y = 1806337</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;subred = df_plus[0:y]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;sub_green = df_plus [0:y]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;sub blue = df_plus [0:y]</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;z = 1344</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;red = sub_red.reshape((z , z))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;green = sub_green.reshape((z , z))</entry></row><row><entry>&#x2003;&#x2003;&#x2003;&#x2003;&#x2003;blue = sub_blue.reshape((z , z))</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;green = np.rot90(green, 2)</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;blue = np.fliplr(blue)</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;arr = np.zerosz , z , 3))</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;arr [:, :, 0] = red</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;arr [:, :, 1]=green</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;arr [:, :,2] = blue</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;img = Image.fromarray(arr.astype(int), &#x2018;RGB&#x2019;)</entry></row><row><entry>&#x2003;&#x2003;&#x2002;&#x2002;img.save(outdir+file+&#x2018;.png&#x2019;)</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><heading id="h-0010" level="2">Table 3 (Continued).</heading><p id="p-0086" num="0082">After an image is created, a deep-learning technique may be used to perform the classification. For example, a convolutional neural network such as a residual neural network (ResNet) or a densely connected neural network (DenseNet), may be used to classify images created from one or more transcriptomic datasets. In some embodiments, using cancer data from the Genome Data Commons (from the National Cancer Institute of Bethesda, Md.), the analysis techniques may be able to classify transcriptomics with an accuracy of 97.1%. In general, the classification may detect a disease, a trait, a state, a prognosis, a response to treatment, and/or the recurrence of a disease.</p><p id="p-0087" num="0083">We now describe embodiments of an electronic device, which may perform the analysis techniques. <figref idref="DRAWINGS">FIG. <b>7</b></figref> presents a block diagram of an example of an electronic device <b>700</b>, such as one of: one of electronic devices <b>110</b>, computer <b>112</b>, the optional base station <b>116</b> or the optional access point <b>120</b>. This electronic device includes processing subsystem <b>710</b>, memory subsystem <b>712</b>, networking subsystem <b>714</b> and optional measurement subsystem <b>732</b>. Processing subsystem <b>710</b> includes one or more devices configured to perform computational operations. For example, processing subsystem <b>710</b> can include one or more microprocessors, application-specific integrated circuits (ASICs), microcontrollers, programmable-logic devices, one or more GPUs, and/or one or more digital signal processors (DSPs). In some embodiments, processing subsystem <b>710</b> includes a mean for processing or performing computations.</p><p id="p-0088" num="0084">Memory subsystem <b>712</b> includes one or more devices for storing data and/or instructions for processing subsystem <b>710</b>, networking subsystem <b>714</b> and/or measurement subsystem <b>732</b>. For example, memory subsystem <b>712</b> can include dynamic random access memory (DRAM), static random access memory (SRAM), a read-only memory (ROM), flash memory, and/or other types of memory. In some embodiments, instructions for processing subsystem <b>710</b> in memory subsystem <b>712</b> include: one or more program modules or sets of instructions (such as program instructions <b>722</b> or operating system <b>724</b>), which may be executed by processing subsystem <b>710</b>. For example, a ROM can store programs, utilities or processes to be executed in a non-volatile manner, and DRAM can provide volatile data storage, and may store instructions related to the operation of electronic device <b>700</b>. Note that the one or more computer programs may constitute a computer-program mechanism, a computer-readable storage medium or software. Moreover, instructions in the various modules in memory subsystem <b>712</b> may be implemented in: a high-level procedural language, an object-oriented programming language, and/or in an assembly or machine language. Furthermore, the programming language may be compiled or interpreted, e.g., configurable or configured (which may be used interchangeably in this discussion), to be executed by processing subsystem <b>710</b>. In some embodiments, the one or more computer programs are distributed over a network-coupled computer system so that the one or more computer programs are stored and executed in a distributed manner.</p><p id="p-0089" num="0085">In addition, memory subsystem <b>712</b> can include mechanisms for controlling access to the memory. In some embodiments, memory subsystem <b>712</b> includes a memory hierarchy that comprises one or more caches coupled to a memory in electronic device <b>700</b>. In some of these embodiments, one or more of the caches is located in processing subsystem <b>710</b>.</p><p id="p-0090" num="0086">In some embodiments, memory subsystem <b>712</b> is coupled to one or more high-capacity mass-storage devices (not shown). For example, memory subsystem <b>712</b> can be coupled to a magnetic or optical drive, a solid-state drive, or another type of mass-storage device. In these embodiments, memory subsystem <b>712</b> can be used by electronic device <b>700</b> as fast-access storage for often-used data, while the mass-storage device is used to store less frequently used data.</p><p id="p-0091" num="0087">Networking subsystem <b>714</b> includes one or more devices configured to couple to and communicate on a wired and/or wireless network (i.e., to perform network operations), including: control logic <b>716</b>, an interface circuit <b>718</b> and a set of antennas <b>720</b> (or antenna elements) in an adaptive array that can be selectively turned on and/or off by control logic <b>716</b> to create a variety of optional antenna patterns or &#x2018;beam patterns.&#x2019; (While <figref idref="DRAWINGS">FIG. <b>7</b></figref> includes set of antennas <b>720</b>, in some embodiments electronic device <b>700</b> includes one or more nodes, such as nodes <b>708</b>, e.g., a pad, which can be coupled to set of antennas <b>720</b>. Thus, electronic device <b>700</b> may or may not include set of antennas <b>720</b>.) For example, networking subsystem <b>714</b> can include a Bluetooth networking system, a cellular networking system (e.g., a 3G/4G/5G network such as UMTS, LTE, etc.), a universal serial bus (USB) networking system, a networking system based on the standards described in IEEE 802.11 (e.g., a Wi-Fi&#xae; networking system), an Ethernet networking system, and/or another networking system.</p><p id="p-0092" num="0088">Networking subsystem <b>714</b> includes processors, controllers, radios/antennas, sockets/plugs, and/or other devices used for coupling to, communicating on, and handling data and events for each supported networking system. Note that mechanisms used for coupling to, communicating on, and handling data and events on the network for each network system are sometimes collectively referred to as a &#x2018;network interface&#x2019; for the network system. Moreover, in some embodiments a &#x2018;network&#x2019; or a &#x2018;connection&#x2019; between the electronic devices does not yet exist. Therefore, electronic device <b>700</b> may use the mechanisms in networking subsystem <b>714</b> for performing simple wireless communication between the electronic devices, e.g., transmitting advertising or beacon frames and/or scanning for advertising frames transmitted by other electronic devices.</p><p id="p-0093" num="0089">Moreover, measurement subsystem <b>732</b> may include one or more sensors that can acquire one or more data (such as an image, an electrical signal, information corresponding to RNA or DNA, etc.). Furthermore, measurement subsystem <b>732</b> may optionally include one or more integrated circuits that perform at least some of the operations in an embodiment of the analysis techniques.</p><p id="p-0094" num="0090">Within electronic device <b>700</b>, processing subsystem <b>710</b>, memory subsystem <b>712</b>, networking subsystem <b>714</b> and measurement subsystem <b>732</b> are coupled together using bus <b>728</b> that facilitates data transfer between these components. Bus <b>728</b> may include an electrical, optical, and/or electro-optical connection that the subsystems can use to communicate commands and data among one another. Although only one bus <b>728</b> is shown for clarity, different embodiments can include a different number or configuration of electrical, optical, and/or electro-optical connections among the subsystems.</p><p id="p-0095" num="0091">In some embodiments, electronic device <b>700</b> includes a display subsystem <b>726</b> for displaying information on a display, which may include a display driver and the display, such as a liquid-crystal display, a multi-touch touchscreen, a heads-up display, an augmented reality display, a virtual reality display, another type of display, etc. For example, the display be an HDMI display. Display subsystem <b>726</b> may be controlled by processing subsystem <b>710</b> to display information to a user. In some embodiments, display subsystem <b>726</b> is used to display processed or cleaned-up images following application of the analysis techniques.</p><p id="p-0096" num="0092">Electronic device <b>700</b> can also include a user-input subsystem <b>730</b> that allows a user of the electronic device <b>700</b> to interact with electronic device <b>700</b>. For example, user-input subsystem <b>730</b> can take a variety of forms, such as: a button, keypad, dial, touch screen, audio input interface, etc.</p><p id="p-0097" num="0093">Electronic device <b>700</b> can be (or can be included in) any electronic device with at least one network interface. For example, electronic device <b>700</b> may include: a cellular telephone or a smartphone, a smartwatch, a wearable device (such as smart glasses or a helmet camera), a camera (such as a dashboard camera), a tablet computer, a laptop computer, a notebook computer, a personal or desktop computer, a netbook computer, a media player device, an electronic book device, a smartwatch, a wearable computing device, a portable computing device, a consumer-electronic device, a vehicle (such as a car, bus or truck), as well as any other type of electronic computing device having measurement and/or analysis capability.</p><p id="p-0098" num="0094">Although specific components are used to describe electronic device <b>700</b>, in alternative embodiments, different components and/or subsystems may be present in electronic device <b>700</b>. For example, electronic device <b>700</b> may include one or more additional processing subsystems, memory subsystems, networking subsystems, imaging subsystems, measurement subsystems and/or display subsystems. Additionally, one or more of the subsystems may not be present in electronic device <b>700</b>. Moreover, in some embodiments, electronic device <b>700</b> may include one or more additional subsystems that are not shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Also, although separate subsystems are shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in some embodiments some or all of a given subsystem or component can be integrated into one or more of the other subsystems or component(s) in electronic device <b>700</b>. For example, in some embodiments program instructions <b>722</b> are included in operating system <b>724</b> and/or control logic <b>716</b> is included in interface circuit <b>718</b>.</p><p id="p-0099" num="0095">Moreover, the circuits and components in electronic device <b>700</b> may be implemented using any combination of analog and/or digital circuitry, including: bipolar, PMOS and/or NMOS gates or transistors. Furthermore, signals in these embodiments may include digital signals that have approximately discrete values and/or analog signals that have continuous values. Additionally, components and circuits may be single-ended or differential, and power supplies may be unipolar or bipolar.</p><p id="p-0100" num="0096">An integrated circuit (which is sometimes referred to as a &#x2018;communication circuit&#x2019;) may implement some or all of the functionality of networking subsystem <b>714</b>. This integrated circuit may include hardware and/or software mechanisms that are used for transmitting wireless signals from electronic device <b>700</b> and receiving signals at electronic device <b>700</b> from other electronic devices. Aside from the mechanisms herein described, radios are generally known in the art and hence are not described in detail. In general, networking subsystem <b>714</b> and/or the integrated circuit can include any number of radios. Note that the radios in multiple-radio embodiments function in a similar way to the described single-radio embodiments.</p><p id="p-0101" num="0097">In some embodiments, networking subsystem <b>714</b> and/or the integrated circuit include a configuration mechanism (such as one or more hardware and/or software mechanisms) that configures the radio(s) to transmit and/or receive on a given communication channel (e.g., a given carrier frequency). For example, in some embodiments, the configuration mechanism can be used to switch the radio from monitoring and/or transmitting on a given communication channel to monitoring and/or transmitting on a different communication channel. (Note that &#x2018;monitoring&#x2019; as used herein comprises receiving signals from other electronic devices and possibly performing one or more processing operations on the received signals).</p><p id="p-0102" num="0098">In some embodiments, an output of a process for designing an integrated circuit, or a portion of an integrated circuit, which includes one or more of the circuits described herein may be a computer-readable medium such as, for example, a magnetic tape or an optical or magnetic disk. The computer-readable medium may be encoded with data structures or other information describing circuitry that may be physically instantiated as the integrated circuit or the portion of the integrated circuit. Although various formats may be used for such encoding, these data structures are commonly written in: Caltech Intermediate Format (CIF), Calma GDS II Stream Format (GDSII) or Electronic Design Interchange Format (EDIF). Those of skill in the art of integrated circuit design can develop such data structures from schematic diagrams of the type detailed above and the corresponding descriptions and encode the data structures on the computer-readable medium. Those of skill in the art of integrated circuit fabrication can use such encoded data to fabricate integrated circuits that include one or more of the circuits described herein.</p><p id="p-0103" num="0099">While some of the operations in the preceding embodiments of the analysis techniques were implemented in hardware or software, in general the operations in the preceding embodiments of the analysis techniques can be implemented in a wide variety of configurations and architectures. Therefore, some or all of the operations in the preceding embodiments of the analysis techniques may be performed in hardware, in software or both. For example, at least some of the operations in the analysis techniques may be implemented using program instructions <b>722</b>, operating system <b>724</b> (such as a driver for interface circuit <b>718</b>) or in firmware in measurement subsystem <b>732</b> or interface circuit <b>718</b>. Alternatively or additionally, at least some of the operations in the communication technique may be implemented in a physical layer, such as hardware in interface circuit <b>718</b>.</p><p id="p-0104" num="0100">While the preceding embodiments are applicable to many biological samples (for example, blood, urine, tissue biopsies, etc.) from many different sources, an implementation may be the analysis of a blood sample. In such an embodiment, the sample is obtained by standard venipuncture in a peripheral vein and stored in a stabilization buffer (e.g., RNA-later) for long periods of time (e.g., days, months or years), for later analysis. The sample can be processed and analyzed on site or alternatively shipped to a central facility. The sample may be then processed to isolate RNA using any standard routine procedures (e.g., organic extraction methods, a spin basket format method, magnetic particles, direct lysis methods, etc.). Then the sample may be further processed to build up a cDNA library for RNA-sequencing. This step may or may not include any RNA specific separation technique, including Ribosomal or Globin RNA depletion. A cDNA library may be then built using reverse transcription from the RNA, and then a fragmentation and sequencer adapters may or may be not applied for obtaining a final sample. Then the sample can be sequenced for different number of reads (e.g., 40 million reads). According to the library preparation method of choice, either a short read RNA sequencing (for example, the Illumina technology) or a long-read RNA sequencing technique (using for example the Oxford Nanopore technology) may be applied.</p><p id="p-0105" num="0101">Using any of the preceding embodiments or methods, a method of diagnosing, prognosing, monitoring, guiding treatment of, assessing, etc. a condition, disease, health state, etc. may be performed, as shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Method <b>800</b> may be performed by a computer, such as computer <b>112</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. During operation, the computer may receive biological data from a biological sample (operation <b>810</b>). As described elsewhere herein, biological data may include, but not be limited to: genomic data, proteomics data, transcriptomic data, epigenomic data, mitochondria data, electrical signals, and/or metabolic data. The biological sample may include, but not be limited to, blood, saliva, tissue, stool, cerebral spinal fluid (CSF), urine, or any other type of tissue or fluid from a living being (e.g., animal, human, vertebrate, invertebrate, fish, reptile, bird, etc.). In some embodiments, the method <b>800</b> may optionally include receiving the biological sample, for example via collection, postal carrier, medical procedure, biopsy, swab, voiding, etc. In some embodiments, the method <b>800</b> may optionally include isolating the biological data from the biological sample. Isolation may include, but not be limited to, any one or more of: cell isolation; cell lysis; DNA isolation; protein isolation; RNA isolation; size-based exclusion or isolation; species based exclusion or isolation of DNA, RNA, and/or protein; genetic material amplification; genetic material synthesis; etc. In some embodiments, the computer, or another computer communicatively coupled to the computer, may optionally sequence the biological data from the biological sample, for example using long reads, short reads, complementary DNA (cDNA) methods, tunneling currents, antibody-based sequencing, etc.</p><p id="p-0106" num="0102">In some embodiments, the computer may transform the biological data into a spatial representation (operation <b>812</b>); selectively normalize the transformed data to obtain normalized transformed data (operation <b>814</b>); convert the normalized transformed data into an output image (operation <b>816</b>); and analyze the image using an image-analysis technique to determine a classification (operation <b>818</b>), each of which have been described elsewhere herein, for example in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0107" num="0103">In some embodiments, the computer may output an indication of a condition based on the determined classification (operation <b>820</b>). Outputting an indication may include displaying a message, alert, pop-up, badge, or other notification on a display of a computing device, for example a mobile computing device, personal digital assistant, desktop computer, server, laptop, wearable, etc. The indication may include an audio (e.g., beeping, chime, spoken language, etc.), visual (e.g., light, text-based notification, color activation or change, etc.), or haptic indication (e.g., vibration, piezo based haptics, etc.).</p><p id="p-0108" num="0104">In some embodiments, the condition, disease, health state, etc. includes, but is not limited to, a diagnosis, a disease state, a genetic status, a prognosis, etc. For example, a disease state may include an indication of abnormal versus normal, healthy versus not healthy, benign versus malignant, etc. Further for example, a genetic status may include, but not be limited to, one or more genetic markers, a mutation presence or absence, a gene presence or absence, a transposon presence or absence, a long non-coding RNA presence or absence, etc., any of which may or may not be indicative of a condition or disease. Further still, for example, a prognosis may include, but not be limited to, a life expectancy, a disease course, a likelihood of treatment success or failure, an expected or anticipated pain level, a likelihood of metastasis, a likelihood of acquiring or being diagnosed with a condition or disease or cancer, etc.</p><p id="p-0109" num="0105">While examples of numerical values are provided in the preceding discussion, in other embodiments different numerical values are used. Consequently, the numerical values provided are not intended to be limiting.</p><p id="p-0110" num="0106">In the preceding description, we refer to &#x2018;some embodiments.&#x2019; Note that &#x2018;some embodiments&#x2019; describes a subset of all of the possible embodiments but does not always specify the same subset of embodiments.</p><p id="p-0111" num="0107">The foregoing description is intended to enable any person skilled in the art to make and use the disclosure and is provided in the context of a particular application and its requirements. Moreover, the foregoing descriptions of embodiments of the present disclosure have been presented for purposes of illustration and description only. They are not intended to be exhaustive or to limit the present disclosure to the forms disclosed. Accordingly, many modifications and variations will be apparent to practitioners skilled in the art, and the general principles defined herein may be applied to other embodiments and applications without departing from the spirit and scope of the present disclosure. Additionally, the discussion of the preceding embodiments is not intended to limit the present disclosure. Thus, the present disclosure is not intended to be limited to the embodiments shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer, comprising:<claim-text>an interface circuit configured to communicate with an electronic device;</claim-text><claim-text>a processor; and</claim-text><claim-text>memory configured to store biological data and program instructions, wherein, when executed by the processor, the program instructions cause the computer to analyze the biological data by performing operations comprising:<claim-text>accessing the biological data in the memory;</claim-text><claim-text>transforming the biological data into a spatial representation, wherein the transformation is based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome;</claim-text><claim-text>selectively normalizing the transformed biological data to obtain normalized transformed data, wherein the selective normalization comprises using a first normalization range for data having a first read count less than a threshold, the first read count being transformed into a first intensity, and using a second normalization range for data having a second read count greater than the threshold, the second read count being transformed into a second intensity;</claim-text><claim-text>converting the normalized transformed biological data into an output image, wherein a given pixel in the output image represents a predefined or predetermined range of bases in the genome;</claim-text><claim-text>analyzing the image using an image-analysis technique to determine a classification to detect a disease, a trait, a state, a prognosis, a response to treatment, and/or the recurrence of a disease; and</claim-text><claim-text>performing at least one of: storing the classification in the memory; displaying the classification on a display; or providing, from the interface circuit, the classification addressed to an electronic device.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the biological data comprises: temporal data or spatial data.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. (canceled)</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. (canceled)</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image has a dynamic or a variable encoding, so that at least some pixels represent different ranges of bases in the genome.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the variable encoding is based at least in part on different information content in different portions of the biological data.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image-analysis technique comprises a neural network.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a given pixel in the image represents data from multiple biological samples.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein different pixels in the image correspond to different types of tissue or different types of biological samples.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. (canceled)</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein a given intensity corresponds to an expression level in a type of biological sequencing.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A non-transitory computer-readable storage medium for use in conjunction with a computer, the computer-readable storage medium storing program instructions that, when executed by the computer, cause the computer to carry out operations to analyze biological data by performing operations comprising:<claim-text>accessing the biological data in memory;</claim-text><claim-text>transforming the biological data into a spatial representation, wherein the transformation is based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome;</claim-text><claim-text>selectively normalizing the transformed data to obtain normalized transformed data, wherein the selective normalization comprises using a first normalization range for data having a first read count less than a threshold, the first read count being transformed into a first intensity, and using a second normalization range for data having a second read count greater than the threshold, the second read count being transformed into a second intensity;</claim-text><claim-text>converting the normalized transformed data into an output image, wherein a given pixel in the output image represents a predefined or predetermined range of bases in the genome;</claim-text><claim-text>analyzing the image using an image-analysis technique to determine a classification to detect a disease, a trait, a state, a prognosis, a response to treatment, and/or the recurrence of a disease; and</claim-text><claim-text>performing at least one of: storing the classification in the memory; displaying the classification on a display; or providing, from the interface circuit, the classification addressed to an electronic device.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the biological data comprises: temporal data or spatial data.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. (canceled)</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the image has a dynamic or a variable encoding, so that at least some pixels represent different ranges of bases in the genome.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the image-analysis technique comprises a pretrained neural network.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein a given intensity corresponds to an expression level in a type of biological sequencing.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A method of diagnosing a condition, comprising:<claim-text>receiving biological data from a biological sample;</claim-text><claim-text>transforming the biological data into a spatial representation, wherein the transformation is based at least in part on a predefined relationship between the biological data and corresponding spatial locations in a genome;</claim-text><claim-text>selectively normalizing the transformed data to obtain normalized transformed data, wherein the selective normalization comprises using a first normalization range for data having a first read count less than a threshold, the first read count being transformed into a first intensity, and using a second normalization range for data having a second read count greater than the threshold, the second read count being transformed into a second intensity;</claim-text><claim-text>converting the normalized transformed data into an output image, wherein a given pixel in the output image represents a predefined or predetermined range of bases in the genome;</claim-text><claim-text>analyzing the image using an image-analysis technique to determine a classification to detect a disease, a trait, a state, a prognosis, a response to treatment, and/or the recurrence of a disease; and</claim-text><claim-text>outputting an indication of a condition based on the determined classification.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, further comprising:<claim-text>receiving the biological sample;</claim-text><claim-text>isolating the biological data from the biological sample; and</claim-text><claim-text>sequencing the biological data from the biological sample.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the biological sample comprises one of: blood, urine, tissue, stool, saliva, or cerebral spinal fluid.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the biological data comprises one of: genomic data, transcriptomic data, proteomic data, epigenomic data, mitochondria data, microbiome data, electrical signals, or metabolic data.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the condition comprises one of: a diagnosis, a disease state, or a genetic status.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein outputting further comprises displaying the indication on a display.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00021">claim 21</claim-ref>, wherein the indication is one or more of: an audio indication, a visual indication, or a haptic indication.</claim-text></claim></claims></us-patent-application>