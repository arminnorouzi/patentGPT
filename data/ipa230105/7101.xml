<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007102A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007102</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17777648</doc-number><date>20191120</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>505</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">REQUEST SCHEDULING</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Telefonaktiebolaget LM Ericsson (publ)</orgname><address><city>Stockholm</city><country>SE</country></address></addressbook><residence><country>SE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Klein</last-name><first-name>Cristian</first-name><address><city>Ume&#xe5;</city><country>SE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hassan</last-name><first-name>Ahmed</first-name><address><city>Ume&#xe5;</city><country>SE</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2019/081928</doc-number><date>20191120</date></document-id><us-371c12-date><date>20220518</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for end-to-end request scheduling for a distributed system comprising a plurality of hosts via which one or more requests are transmitted. The method comprises: receiving the one or more requests; assigning global scheduling information to each of the one or more requests; transmitting, for each of the one or more requests, respectively assigned global scheduling information with the request, such that respective global scheduling information is made available to a local scheduling unit corresponding to a host via which each of the plurality of requests is transmitted; and determining, for each of one or more one or more requests received at each of the plurality of hosts, an order in which at least one of: a computation operation, a communication operation, and an input/output operation associated with the request is performed, based on the global scheduling information assigned to the respective request.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="146.30mm" wi="138.18mm" file="US20230007102A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="171.62mm" wi="178.48mm" file="US20230007102A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="193.72mm" wi="113.62mm" file="US20230007102A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="164.93mm" wi="118.79mm" file="US20230007102A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="174.33mm" wi="140.21mm" file="US20230007102A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="149.61mm" wi="136.14mm" file="US20230007102A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="159.26mm" wi="136.91mm" file="US20230007102A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="164.08mm" wi="149.18mm" file="US20230007102A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="159.26mm" wi="136.23mm" file="US20230007102A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="160.78mm" wi="139.62mm" file="US20230007102A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to the field of request scheduling. In particular, the present disclosure relates to the field of end-to-end request scheduling for a distributed system which includes a number of hosts.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Several studies of users' Quality-of-Experience highlight that large application response times lead to a decrease in revenue. For example, many companies found that increased latency leads to dropped traffic and reduced sales.</p><p id="p-0004" num="0003">Besides attempts to reduce the average response time for applications, controlling tail response times (e.g. the 99<sup>th </sup>percentile response times) of web applications is also central to providing good Quality-of-Service for web applicant clients. Due to the complexity of the software and hardware used to build distributed systems, events such as scheduling delays, garbage collection, energy saving, and background tasks may cause &#x201c;hiccups&#x201d; in the execution of an application, which leads to some requests being served orders of magnitude slower than on average. In particular, scheduling delays are a major source of increased tail response times occurring on the operating system level. Operating system scheduling delays cause long tail response times as most operating systems schedule tasks, such as runnable threads or backlogged network packets, in an arbitrary order based on interrupts, instead of an order that minimises tail response time.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">Many currently available techniques focus on reducing delays for single application components, for example by replicating requests. Techniques for doing so include dedicating a separate CPU core to network interrupts, using real-time priority and changing the default scheduler from Completely Fair Scheduler (CFS) to Borrowed Virtual Time (BVT). However, such techniques have not been validated on applications composed of multiple components, as featured by modern micro-service-based applications. In fact, one can show using queuing theory that, without specifically addressing the distributed nature of multi-tiered applications, modularising a monolithic application leads to increased tail response times.</p><p id="p-0006" num="0005">Thus, there remains a need for application-agnostic end-to-end request scheduling techniques. Embodiments of the present disclosure implement an end-to-end scheduling method which can reduce tail response times in micro-service-based applications. In more detail, embodiments of the present disclosure track information about an original user request throughout the distributed system, both horizontally (along the distribution application call-chain) and vertically (from the micro-service request to the thread serving the request to the CPU scheduler). This allows CPU schedulers to complement the information about runnable threads with information about the user requests they are servicing, in order to enforce the same ordering of work as if the application was monolithic.</p><p id="p-0007" num="0006">One aspect of the present disclosure provides a method for end-to-end request scheduling for a distributed system which comprises a plurality of hosts via which one or more requests are transmitted. The method comprises: receiving the one or more requests; assigning global scheduling information to each of the one or more requests; transmitting, for each of the one or more requests, respectively assigned global scheduling information with the respective request, such that respective global scheduling information is made available to a local scheduling unit corresponding to a host via which each of the plurality of requests is transmitted; and determining, for each of one or more one or more requests received at each of the plurality of hosts, an order in which at least one of: a computation operation, a communication operation, and an input/output operation associated with the respective request is performed, wherein the determination is based on the global scheduling information assigned to the respective request.</p><p id="p-0008" num="0007">Another aspect of the disclosure provides a scheduling system for performing end-to-end request scheduling at a distributed system comprising a plurality of hosts. The scheduling system extends a network communication protocol implemented by the distributed system and comprises: an entry-point configured to receive one or more requests and to assign global scheduling information to each of the one or more requests, and a plurality of local scheduling units, wherein each of the plurality of local scheduling units corresponds to one of the plurality of hosts in the distributed system. The scheduling system is configured to extend the network communication protocol to perform the following: transmitting, for each of the one or more requests, respectively assigned global scheduling information with the respective request via a protocol extension associated with the network communication protocol, such that the respective global scheduling information is made available to a local scheduling unit corresponding to a host via which the respective request is transmitted. Each of the plurality of local scheduling units is configured to determine, for each of one or more requests received at the corresponding host, an order in which at least one of a computation operation, a communication operation, and an input/output operation associated with the respective request is performed, wherein the determination is based on the global scheduling information assigned to the respective request.</p><p id="p-0009" num="0008">Another aspect of the disclosure provides a computer program product comprising a computer readable medium, the computer readable medium having computer readable code embodied therein, the computer readable code being configured such that, on execution by a suitable computer or processor, the computer or processor is caused to perform the method as described herein.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">For a better understanding of examples of the present invention, and to show more clearly how the examples may be carried into effect, reference will now be made, by way of example only, to the following drawings in which:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a distributed system capable of end-to-end request scheduling according to embodiments of the disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram illustrating the structure of an execution stack, according to embodiments of the disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram illustrating the structure of a network communication protocol stack, according to embodiments of the disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating a method for end-to-end request scheduling for a distributed system according to embodiments of the disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a graph illustrating results achieved by the method with increasing load, according to embodiments of the disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a graph illustrating results achieved by the method with increasing number of CPUs and constant load, according to embodiments of the disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a graph illustrating results achieved by the method with co-located components, according to embodiments of the disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a graph illustrating results achieved by the method according to embodiments of the disclosure as compared with alternative approaches; and</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a graph illustrating results achieved by the method over three physical machines, according to embodiments of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a scheduling system capable of end-to-end request scheduling according to embodiments of the disclosure. Specifically, the scheduling system <b>100</b> is capable of performing end-to-end request scheduling at a distributed system <b>110</b>, which comprises a plurality of hosts via which one or more requests is transmitted. The end-to-end request scheduling is based on global transmission of scheduling information, which will be referred to as &#x201c;global scheduling information&#x201d; hereafter. The term &#x201c;global&#x201d; in this context refers to information that is available to the plurality of hosts in the distributed system <b>110</b> (and components that use such information), in contrast to &#x201c;local&#x201d; information that is available to a single host (and components that use such information). In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the plurality of hosts of the distributed system <b>110</b> are represented as a first host <b>112</b><i>a </i>and a second host <b>112</b><i>b</i>. It will be appreciated that in some embodiments there may be provided more than two hosts at the distributed system <b>110</b>. In the present embodiment, the scheduling system <b>100</b> extends a network communication protocol implemented by the distributed system <b>110</b>.</p><p id="p-0021" num="0020">The scheduling system <b>100</b> comprises an entry-point <b>120</b> and a plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b</i>. The entry-point <b>120</b> is configured to receive the one or more requests <b>140</b><i>a</i>, <b>140</b><i>b</i>, <b>140</b><i>c</i>, and to assign global scheduling information <b>150</b> to each of the one or more requests <b>140</b><i>a</i>, <b>140</b><i>b</i>, <b>140</b><i>c</i>. The global scheduling information can either be generated when the request arrives to the distributed system <b>110</b>, or produced by a trusted entity outside the distributed system <b>110</b>. In some embodiments, the entry-point <b>120</b> may be configured to discard, for each of the plurality of requests, information associated with an arrival time of the request. This discarding operation may be performed prior to assigning of the global scheduling information. Thus, the entry-point <b>120</b> can provide security in the manner that potentially malicious or suspicious data contained in the information associated with arrivals times can be discarded.</p><p id="p-0022" num="0021">Each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>corresponds to one of the plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b </i>in the distributed system <b>110</b>. In the present embodiment, a first local scheduling unit <b>130</b><i>a </i>of the plurality of local scheduling units corresponds to the first host <b>112</b><i>a</i>, and a second local scheduling unit <b>130</b><i>b </i>of the plurality of local scheduling units corresponds to the second host <b>112</b><i>b</i>. In some embodiments, each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>implemented in at least one of a level of an execution stack. The execution stack may comprise at least one of the following levels: a runtime environment, an operating system, and a hypervisor. Moreover, in some embodiments each of the plurality of local scheduling units may be implemented in a single level of the execution stack, without requiring any support or modifications in the other levels of the execution stack. For example, in some embodiments each of the plurality of local scheduling unit may be implemented in a single level of the execution stack without requiring further interface or functionality provided by a different level of the execution stack.</p><p id="p-0023" num="0022">In some embodiments, the global scheduling information <b>150</b> assigned to each of the one or more requests <b>140</b> may comprise an arrival time of the respective request at the entry-point <b>120</b>. This is illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> which shows that in the global scheduling information <b>150</b> for each of the requests <b>140</b><i>a</i>, <b>140</b><i>b</i>, <b>140</b><i>c</i>, an &#x201c;arrival time&#x201d; (of the request at the entry-point <b>120</b>) is provided. For example, for a first request <b>140</b><i>a </i>the global scheduling information <b>150</b> comprises an arrival time represented by &#x201c;1&#x201d;, for a second request <b>140</b><i>b </i>the global scheduling information <b>150</b> comprises an arrival time represented by &#x201c;2&#x201d;, and for a third request <b>140</b><i>c </i>the global scheduling information <b>150</b> comprises an arrival time represented by &#x201c;3&#x201d;. In this case, the number representing the arrival time indicates an order in which the request is received at the entry-point <b>120</b>.</p><p id="p-0024" num="0023">Although not illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in some embodiment the arrival time of a respective request <b>140</b> may be expressed in the format of signed or unsigned integers in 8-bit, 16-bit, 32-bit, or 64-bit. A number of advantages are associated with this particular format for expressing the arrival time. For example, with 64-bit CPUs being widespread, arrival times that are expressed in 64-bit integers can be compared quickly. As another example, the granularity offered by this format make it unlikely for two requests to share an arrival time. Also, as another example, arrival times expressed in this type of format can be quickly generated, for example at any level in the execution stack, e.g. those illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0025" num="0024">The scheduling system <b>100</b> is configured to extend the network communication protocol to transmit, for each of the one or more requests <b>140</b><i>a</i>, <b>140</b><i>b</i>, <b>140</b><i>c</i>, respectively assigned global scheduling information <b>150</b> with the respect request via protocol extension associated with the network communication protocol, such that the respective global scheduling information <b>150</b> is made available to a local scheduling unit <b>130</b> corresponding to a host <b>112</b> via which the respective request <b>140</b> is transmitted.</p><p id="p-0026" num="0025">As mentioned above, each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>corresponds to one of the plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b</i>. According to the present embodiment, each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>is configured to determine, for each of the one or more requests <b>140</b> received at the corresponding host <b>112</b>, an order in which at least one of a computation operation, a communication operation, and an input/output operation associated with the respective request <b>140</b> is performed. The determination performed by the local scheduling unit <b>130</b> is based on the global scheduling information <b>150</b> assigned to the respective request <b>140</b>.</p><p id="p-0027" num="0026">In some embodiments, each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>may be configured to determine the order in which at least one of a computation operation, a communication operation, an input/output operation associated with the respective request <b>140</b> is performed based on a first-come-first-served scheduling policy or an earliest-deadline-first scheduling policy, by prioritising requests <b>140</b> associated with the earliest arrival times. Alternatively or in addition, in some embodiments, each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>may be configured to determine the order in which at least one of a computation operation, a communication operation, and an input/output operation associated with the respective request <b>140</b> is performed based on a least-attained-service scheduling policy, by prioritising requests <b>140</b> associated with at least one of: shorter amount of time serving the request and smaller amount of network data transmitted on behalf of the request.</p><p id="p-0028" num="0027">Furthermore, in some embodiments, each of the plurality of local scheduling units <b>130</b><i>a</i>, <b>130</b><i>b </i>may be configured to update, for each of the requests received at the respective corresponding host <b>112</b>, the respective global scheduling information <b>150</b> such that it comprises information associated with at least one of: an amount of time spent serving the respective request and an amount of network data transmitted on behalf of the respective request. For example, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the first local scheduling unit <b>130</b><i>a </i>is configured to update the global scheduling information <b>150</b> such that the global scheduling information <b>150</b> of the first request <b>140</b><i>a </i>comprises &#x201c;service time&#x201d; (i.e. an amount of time spent serving the respective request <b>140</b>) which is represented by &#x201c;5&#x201d;, the global scheduling information <b>150</b> of the second request <b>140</b><i>b </i>comprises &#x201c;service time&#x201d; which is represented by &#x201c;5&#x201d;, and the global scheduling information <b>150</b> of the third request <b>140</b><i>c </i>comprises &#x201c;service time&#x201d; which is represented by &#x201c;1&#x201d;.</p><p id="p-0029" num="0028">In the present embodiment as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the scheduling system <b>100</b> comprises the distributed system <b>110</b>, and therefore the plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b </i>of the distributed system <b>110</b>. The plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b </i>may be configured to host one or more applications. Each of the one or more applications may comprise one or more application components. Furthermore, for each of the one or more applications hosted by the plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b</i>, the one or more application components may be configured to perform at least one of a computation operation, a communication operation, and an input/output operation corresponding to each of the one or more requests <b>140</b> received at the corresponding hosts <b>112</b> in the order determined by the corresponding local scheduling unit <b>130</b>.</p><p id="p-0030" num="0029">Those skilled in the art would appreciate that in alternative embodiments at least part of the distributed system <b>110</b> may not be part of the scheduling system <b>100</b>. For example, in some embodiments, the plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b </i>may not be part of the scheduling system <b>100</b>.</p><p id="p-0031" num="0030">Although not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the plurality of hosts <b>112</b><i>a</i>, <b>112</b><i>b </i>may be connected through a network stack comprising a plurality of layers. In these embodiments, the protocol extension may be implemented in at least one of the plurality of layers within the network stack. An exemplary structure of a network communication protocol stack is shown in the schematic diagram of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In more detail, as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the exemplary network communication protocol stack <b>300</b> comprises an application layer <b>310</b>, a transport layer <b>320</b>, a network layer <b>330</b>, and a link layer <b>340</b>. Therefore, in some embodiments, the protocol extension may be implemented in at least one of the application layer <b>310</b>, the transport layer <b>320</b>, the network layer <b>330</b>, and the link layer <b>340</b>. In some embodiments a link layer tagging of IEEE 802.3 Ethernet Frames with global scheduling information, similar to 802.1q, may be implemented. In some other embodiments, a network layer extension, such as an IPv4 option or and IPv6 extension header may be implemented. In some embodiments, global scheduling information may be transmitted via a transport layer extension, such as a TCP option. Furthermore, in some embodiments global scheduling information may be transmitted via an application layer extension, such as an HTTP header.</p><p id="p-0032" num="0031">It will be appreciated that <figref idref="DRAWINGS">FIG. <b>1</b></figref> only shows the components required to illustrate an aspect of the scheduling system <b>100</b> and, in a practical implementation, the scheduling system <b>100</b> may comprise alternative or additional components to those shown.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram illustrating the structure of an execution stack, according to embodiments of the disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the execution stack <b>200</b> comprises application <b>210</b>, runtime <b>220</b>, operating system virtualisation <b>230</b>, operating system <b>240</b>, hardware virtualisation <b>250</b>, and hardware <b>260</b>. In some embodiments, local scheduling unit(s) may be implemented at one or more levels of the execution stack as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0034" num="0033">In more detail, in some embodiments computation, communication, and input/output operations issued by the application to the runtime <b>220</b> may be encapsulated in lightweight threads, such as events or go-routines. The runtime <b>220</b> may act as a local scheduling unit for the application. The runtime <b>220</b> may then issues system calls to the (potentially virtualized) operating system <b>240</b>, said system calls being issues within one or more kernel threads. The operating system <b>240</b> kernel may act as a local scheduling unit ordering the kernel threads created by the runtime <b>220</b>. Finally, the operating system <b>240</b> may run the operations on (potentially virtual) CPUs. If the CPUs are virtual, then a hypervisor may act as the local scheduling unit for ordering virtual CPU operations onto the hardware CPUs. Finally, in some embodiments the hardware CPU itself may schedule instructions onto the underlying arithmetic, memory or I/O units. The method according to the present disclosure may be implemented at one or more of these levels, whenever a local scheduling decision is involved. The place to implement ordering may be chosen depending on the information available from the upper execution level, on the level of congestion on the resources of the lower execution level, and/or on the network stack layer at which the global scheduling information is encapsulated. For example, in some embodiments global scheduling information may be transmitted via HTTP headers and perform local scheduling decisions in the runtime <b>220</b>, whereas in other embodiments global scheduling information may be transmitted via IPv4 options and perform local scheduling decisions in the operating system <b>240</b> kernel.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is flowchart illustrating a method <b>400</b> for end-to-end request scheduling for a distributed system according to embodiments of the disclosure. The illustrated method can generally performed by or under the control of a scheduling system, for example the scheduling system <b>100</b> as described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For purposes of illustration, the method <b>400</b> will be described with reference to the various components of the scheduling system <b>100</b> and the distributed system <b>110</b> as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0036" num="0035">With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, at step <b>410</b>, one or more requests are received. These requests correspond to the one or more requests that are transmitted via the plurality of hosts <b>112</b> in the distributed system <b>110</b>. Upon receiving the one or more requests at the scheduling system <b>100</b>, at step <b>420</b> global scheduling information is assigned to each of the one or more requests received at step <b>410</b>. In some embodiments, the method <b>400</b> may be implemented at the scheduling system <b>100</b> which comprises an entry-point <b>120</b> and a plurality of local scheduling units <b>130</b> each corresponding to one of the plurality of hosts <b>112</b> in the distributed system <b>110</b>. In these embodiments, the steps of receiving the one or more requests at step <b>410</b> and assigning global scheduling information at step <b>420</b> may be performed at the entry-point <b>120</b>.</p><p id="p-0037" num="0036">In some embodiments, global scheduling information assigned to each of the one or more requests at step <b>420</b> may comprise an arrival time of the respective request at the entry-point. An arrival time of a respective request at the entry-point may be expressed in the format of integers in 8-bit, or 16-bit, or 32-bit, or 64-bit.</p><p id="p-0038" num="0037">Subsequent to global scheduling information being assigned, at step <b>430</b> for each of the one or more requests, respective assigned global scheduling information <b>150</b> is transmitted with the respective request, such that respective global scheduling information <b>150</b> is made available to a local scheduling unit <b>130</b> corresponding to a host <b>112</b> via which each of the plurality of requests <b>140</b> is transmitted. For example, for the first request <b>140</b><i>a </i>in the plurality of requests, the global scheduling information of the first request <b>140</b><i>a </i>can be made available to each of the one or more hosts <b>112</b> via which the first request <b>140</b><i>a </i>is transmitted. In embodiments where the scheduling system <b>110</b> extends a network communication protocol implemented by the distributed system <b>110</b>, the transmission of the one or more requests may be performed via the network communication protocol implemented by the distributed system <b>110</b>, and the transmission of the respectively assigned global scheduling information may be performed via a protocol extension associated with the network communication protocol.</p><p id="p-0039" num="0038">Then, at step <b>440</b>, for each of the one or more requests received at each of the plurality of hosts <b>112</b>, an order in which at least one of: a computation operation, a communication operation, and an input/output operation associated with the respective request <b>140</b> is performed. This determination at <b>450</b> is based on the global scheduling information <b>150</b> assigned to the respective request. In some embodiments, the determination may be performed by a respective local scheduling unit <b>130</b> corresponding to the host <b>112</b>.</p><p id="p-0040" num="0039">In some embodiments, determining the order at step <b>440</b> may be based on a first-come-first-served scheduling policy or earliest-deadline-first scheduling policy. In more detail, requests associated with earlier arrival times are prioritised, i.e. the earlier the arrival time, the more the respective request is prioritised in the determined order. For example, referring to the exemplary arrival times shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the first request <b>140</b><i>a</i>, the second request <b>140</b><i>b</i>, and the third request <b>140</b><i>c</i>, it is shown that the first request <b>140</b><i>a </i>has an earlier arrival time than the second request <b>140</b><i>b</i>, and the second request <b>140</b><i>b </i>has an earlier arrival time than the third request <b>140</b><i>c</i>. Therefore, if the determination at step <b>450</b> is at least in part based on a first-come-first-served scheduling policy or earliest-deadline-first scheduling policy, the determined order may be (from most prioritised to least prioritised): the first request <b>140</b><i>a</i>, the second request <b>140</b><i>b</i>, and the third request <b>140</b><i>c. </i></p><p id="p-0041" num="0040">In some embodiments, determining the order at step <b>440</b> may be based on a least-attained-service scheduling policy. In more detail, requests associated with at least one of: shorter amount of time serving the request and smaller amount of network data transmitted on behalf of the request are prioritised, i.e. the shorter the amount of time spent serving the request, and/or the smaller the amount of network data transmitted on behalf of the request, the more the respective request is prioritised in the determined order. For example, referring to the exemplary service times shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> (between the first host <b>112</b><i>a </i>and the second host <b>112</b><i>b</i>), it is shown that the third request <b>140</b><i>c </i>has a shorter service time than the first request <b>140</b><i>a </i>or the second request <b>140</b><i>b</i>. Therefore, if the determination at step <b>450</b> is at least in part based on a least-attained-service scheduling policy, the determined order may prioritise the third request <b>140</b><i>c </i>over the first request <b>140</b><i>a </i>and the second request <b>140</b><i>b. </i></p><p id="p-0042" num="0041">The method may include an optional method step <b>450</b> in which at least one of a computation operation, a communication operation, and an input/output communication is performed by one or more application components, in the order determined by the corresponding local scheduling unit <b>130</b> at step <b>440</b>. The one or more application components may be part of one or more applications that are hosted by the plurality of hosts <b>112</b>.</p><p id="p-0043" num="0042">Although not illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the method <b>400</b> may further comprise a step of updating, for each of the requests <b>140</b> received at the respective corresponding host <b>112</b>, the respective global scheduling information <b>150</b> such that it comprises information associated with at least one of: an amount of time spent serving the respective request <b>140</b> and an amount of network data transmitted on behalf of the respective request <b>140</b>. This updating step may be performed by a respective local scheduling unit <b>130</b> corresponding to the host <b>112</b>.</p><p id="p-0044" num="0043">Moreover, although not illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the method may further comprise a step of discarding, for each of the plurality of requests, information associated with an arrival time of the request. The discarding step is performed prior to assigning the global scheduling information, and the step may be performed by the entry-point <b>120</b> of the scheduling system <b>100</b>.</p><p id="p-0045" num="0044">Those who are skilled in the art would appreciate that in some embodiments the method steps illustrated in steps <b>430</b> to <b>450</b> may be performed in a different order. For example, in some embodiments after determining an order in which at least one of: a computation operation, a communication operation, and an input/output operation associated with a respective request is performed (step <b>440</b>), and performing such operation(s) in the determined order (step <b>450</b>), the method may return to step <b>430</b> at which the respective request is transmitted to another host with the respectively assigned global scheduling information.</p><p id="p-0046" num="0045">For at least some of the embodiments of the disclosure, by tracking the global scheduling information (e.g. arrival time) of each user request throughout the distributed system and enforcing an end-to-end servicing order of requests, performance loss due to modularisation in multi-tiered or micro-service-based applications can be reduced.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b></figref> to <figref idref="DRAWINGS">FIG. <b>9</b></figref> are graphs illustrating results achieved by the method according to embodiments of the disclosure. In more detail, <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates results achieved by the method with increasing load, <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates results achieved by the method with increasing number of CPUs and constant load, <figref idref="DRAWINGS">FIG. <b>7</b></figref> is illustrate results achieved by the method with co-located components, <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates results achieved by the method as compared with alternative approaches, and <figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates results achieved by the method over three physical machines. The results show that the end-to-end scheduling technique according to embodiments of the disclosure can reduce tail response time by up to 50%, even when compared to a &#x201c;near-ideal&#x201d; approach that locally runs each request to completion.</p><p id="p-0048" num="0047">The graphs as shown in <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>9</b></figref> are based on evaluation of performance gains that can be achieved using the method according to at least some embodiments of the disclosure, based on a number of experiments. The method according to embodiments of the disclosure will be referred to as &#x201c;TailTamer&#x201d; in the present context for <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>9</b></figref>. In the TailTamer embodiment, IPv4 options are used as network protocol extension to transmit global scheduling information, perform local scheduling decisions in the operating system kernel, and order computation, communication, and/or input/output operations based on earliest arrival time at the entry-point expressed as a 64-bit integer representing the number of nanoseconds elapsed since the Unix epoch (Jan. 1, 1970 at 0:00:00 UTC), herein called Universal Arrival Time (UAT). It is noted that the UAT is not necessarily unique within the distributed system. The word &#x201c;universal&#x201d; in the present context denotes that said arrival time should be considered for local scheduling by all hosts in the distributed system. Although UAT is used in these evaluations, similar benefits can be expected with other embodiments. The focus of the evaluation is to compare the response time obtained using the default Linux scheduler (as is commonly deployed in the art) with TailTamer. The technique associated with embodiments of the present disclosure involves (1) reducing service time by reducing the number of context switches, and (2) using the same priority (based on global scheduling information) for all the queues throughout the system.</p><p id="p-0049" num="0048">To better understand the contribution of each of two aspects, in some experiments (i.e. the experiments associated with the graphs shown in <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref>) the results associated with a third scheduler is included. The third scheduler reduces context switches in the same manner as TailTamer, but does not propagate the arrival time (in the global scheduling information) to the next tier or component. This scheduler resembled a tandem queue with first-come-first-served (FCFS) per server, but not system-wide. In other words, each thread is prioritised based on the earliest arrival time of the request it served at each component. This is in contrast to TailTamer which prioritises threads based on the earliest arrival time at the first tier of the whole application pipeline.</p><p id="p-0050" num="0049">The experiments associated with the graphs shown in <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>8</b></figref> were conducted on a single physical machine equipped with one Intel&#xae; Core&#x2122; i7-6700 processor and 32 GB of memory. Kernel-based Virtual Machine (KVM) was used as hypervisor, CoreOS was run as the guest operating system, and RUBiS (an e-commerce prototype) was deployed. In order to test that the arrival times (of the global scheduling information) are transmitted correctly over the (virtual) network, RUBiS was deployed in three Virtual Machines (VMs): lighttpd for load-balancing, Apache with mod-php for web serving, and MySQL for database.</p><p id="p-0051" num="0050">In order to ensure that the results are reliable and unbiased, vmtouch utility was used to hold the database files in-memory, thus avoiding variance due to disk latency. Furthermore, in order to ensure the load generated in the same way during each experiment, the httpmon workload generator in open system model and the same sequence of exponentially distributed inter-arrival times were used. Also, no non-essential processes or cron scripts were running at the time of the experiments. For example, some Core OS services that might interfere with the experiments were masked.</p><p id="p-0052" num="0051">Since the experimental setup for all of these experiments included diverse application structures (non-threaded event-driven, non-threaded process pool, thread-pool), the evaluation results are relevant for a wide range of applications.</p><p id="p-0053" num="0052">In the graphs as shown in <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>9</b></figref>, the x-axis represents an experimental parameter that is varied, e.g. the arrival rate or the number of CPU cores, and the results for various schedulers (including TailTamer) are presented along the x-axis to facilitate comparison. The y-axis represents the response time in milliseconds, with the response-time distribution presented as a violin plot (without trimming, i.e. including minimum and maximum values). The horizontal thickness of the area represents the relative number of response time values on the y-axis recorded throughout an experiment. The 99<sup>th </sup>percentile response time is highlighted using a dash. It is noted that log scales are employed in <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>8</b></figref> in the y-axis.</p><p id="p-0054" num="0053">Referring to the graph shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, TailTamer was tested as the load is increasing. Each VM is allocated one virtual CPU core and each virtual CPU is pinned to a separate physical core. The load is increased from 10 requests per second to 60 requests per second. As can be predicted using queuing theory, as the arrival rate and the load are increasing, the response time (both in average and tail) increases. With a higher load, TailTamer performs better at reducing tail response time. For example, when the arrival rate is 60 requests per second, the 99<sup>th </sup>percentile response time is reduced from 3.13 s to 1.57 s, a reduction of almost 50%. The results in <figref idref="DRAWINGS">FIG. <b>5</b></figref> also highlight that reducing context switches, without consistently ordering requests throughout the system, only partially contributes to the performance of the technique as discussed in embodiments of the present disclosure. Indeed, besides reducing the number of context switches and the overall service time, TailTamer also reduces tail response times via propagation of arrival times, by ensuring that user requests are prioritised the same way at each component.</p><p id="p-0055" num="0054">Referring to the graph shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, TailTamer was tested as the number of cores allocated to a tier increase, but the load per core is kept constant at 50 requests per core per second. Only the database component is scaled, as it is the bottleneck of the application. TailTamer was tested with arrival rates to 50, 100, 150, and 200 requests per second, allocating one, two, three, and 4 CPUs to the database component, respectively. As can be predicted using queuing theory, increasing the number of CPUs (or servers, in queuing theory terminology) reduces both average and tail response times. However, ensuring consistent order based on arrival times (in the global scheduling information), as done by TailTamer, further reduces tail response time. For example, with four CPUs, the 99<sup>th </sup>percentile response time is reduced from 349 ms to 220 ms, a reduction of over 30%. As with the experiment discussed with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the results highlight that both service time reduction and arrival times are contributing to response time reduction.</p><p id="p-0056" num="0055">One of the advantages of implementing TailTamer is that the technique runs in the kernel. This means that, in contrast to application-level scheduler, TailTamer is insensitive to several processes being co-located on the same resources an can potentially better deal with self-interference, i.e. the undesirable phenomenon of components of an application causing performance interference among themselves due to co-location. To illustrate this, the experiment associated with the graph of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is one that was based on the experiment discussed with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, but ran with Apache and MySQL running in the same VM, which has a single virtual CPU. No CPU pinning is performed within the VM, to ensure that the CPU of the VM can be fully utilised. In contrast, having Apache and MySQL pinned to virtual CPUs would lead to either of the two acting as a soft bottleneck, i.e. a bottleneck that appears despite idle physical resources.</p><p id="p-0057" num="0056">Referring to the graph of <figref idref="DRAWINGS">FIG. <b>7</b></figref>, as expected both schedulers (i.e. default and TailTamer) are affected by co-location, since fewer physical CPUs need to be shared among Apache and MySQL. However, it is clear that TailTamer outperforms the default scheduler. Furthermore, compared to the experimented discussed with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the method employed for the current experiment (<figref idref="DRAWINGS">FIG. <b>7</b></figref>) is less affected by co-location, e.g. at an arrival rate of 200 requests per second and four cores, the 99<sup>th </sup>percentile response time increases by 50% compared to the previously discussed experiments, whereas for the default scheduler the increase is around 66%. It can therefore be concluded that TailTamer can reduce self-interference, and that the resulting increase in tail response time (as compared to the case when no co-location is performed) is less compared to the default scheduler, despite the reduced capacity available to the application.</p><p id="p-0058" num="0057">Referring to the graph of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the associated evaluation is based on comparison of TailTamer with alternative approaches for dealing with scheduling delays. The graph in <figref idref="DRAWINGS">FIG. <b>8</b></figref> shows the results which compare the performance of RUBiS when deployed with the default Linux scheduler, TailTamer, and the Linux Real-time scheduler. As can be observed, the real-time approach addresses some sources of scheduling delay, but it cannot effectively cope with tail response time reduction in distributed systems. Indeed, TailTamer outperforms the realtime scheduler at high loads. For example, when the arrival rate is 60 requests per second, the 99<sup>th </sup>percentile response time is 3.15 s for the realtime scheduler, whereas TailTamer reduces response time to 1.71, representing a reduction of over 45%.</p><p id="p-0059" num="0058">It is noted that the experiments associated with the graphs illustrated in <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>8</b></figref> were performed in a controlled network environment, i.e. the virtual network within a single physical machine. For the experiment associated with the graphs of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, TailTamer was evaluated on a real local network with physical machines. In more detail, the experiment was conducted on three physical machines A, B, and C which are equipped with two AMD Operton&#x2122; 6272 processors and 56 GB of memory. The three machines were connected through a non-dedicated Gigabit switch. Ubuntu 16.04.3 LTS was used on top of which a custom Linux kernel with TailTamer, a Docer Engine 17.10.0-ce were deployed. Also, the workload generator and lighttpd were deployed to A, Apache and PHP were deployed to B, and MySQL was deployed to C. To make it easier to bring the system close to saturation, each container was limited to a single, exclusively-allocated CPU core.</p><p id="p-0060" num="0059">Given the fact that experiments over a real network imply more variability, 10 repetitions for each arrival rate were performed. The focus of the experiments is on the 99<sup>th </sup>percentile response time, and each measurement, the mean of the measurements and the 95% confidence intervals computed using the loess method were reported.</p><p id="p-0061" num="0060">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, it is shown that as the arrival time increases, the 99<sup>th </sup>percentile response time increases for both the default scheduler and TailTamer, in agreement with queuing theory. It is observed that the 99<sup>th </sup>percentile response time for TailTamer increases slower. In fact, the mean of the measurements for TailTamer are always lower than the default scheduler, no matter the arrival rate. At low arrival rates the confidence intervals overlap, which means it cannot be concluded (at 95% confidence) that TailTamer outperforms the default scheduler. However, starting with 15 requests per second, the two confidence intervals no longer overlap, and therefore it can be concluded (at 95% confidence) that TailTamer outperforms the default scheduler at high load. For example, at an arrival rate of 20 requests per second, the 99<sup>th </sup>response time is reduced from 4.00 s to 2.89 s, representing a 27% improvement.</p><p id="p-0062" num="0061">In summary of the results presented in the graphs of <figref idref="DRAWINGS">FIGS. <b>5</b> to <b>9</b></figref>, TailTamer significantly improves tail latencies compared to currently known scheduling algorithms by up to 50%, in particular when the system is operating at high utilisation or when application components are co-located.</p><p id="p-0063" num="0062">Thus, embodiments of the present disclosure provide a method for end-to-end request scheduling which can reduce tail response time and can be readily employed without requiring changes to the application source code, and they are particularly beneficial at high load or when the application components are co-located. This translates into lower infrastructure cost and, given the lack of energy-proportional hardware, higher energy efficiency while ensuring good user experience without resorting to computing capacity over-provisioning. The method reduces tail response time at the boundary of the distributed system instead of reducing tail response time for individual components. Moreover, the method is application agnostic, hence applications can directly benefit from it without source code modifications and without using the same communication framework.</p><p id="p-0064" num="0063">Embodiments of the disclosure also provide a scheduling system for performing end-to-end request scheduling at a distributed system comprising a plurality of hosts. The scheduling system extends a network communication protocol implemented by the distributed system.</p><p id="p-0065" num="0064">There is also provided a computer program product comprising a computer readable medium, the computer readable medium having computer readable code embodied therein, the computer readable code being configured such that, on execution by a suitable computer or processor, the computer or processor is caused to perform the method or methods described herein. Thus, it will be appreciated that the disclosure also applies to computer programs, particularly computer programs on or in a carrier, adapted to put embodiments into practice. The program may be in the form of a source code, an object code, a code intermediate source and an object code such as in a partially compiled form, or in any other form suitable for use in the implementation of the method according to the embodiments described herein.</p><p id="p-0066" num="0065">It will also be appreciated that such a program may have many different architectural designs. For example, a program code implementing the functionality of the method or system may be sub-divided into one or more sub-routines. Many different ways of distributing the functionality among these sub-routines will be apparent to the skilled person. The sub-routines may be stored together in one executable file to form a self-contained program. Such an executable file may comprise computer-executable instructions, for example, processor instructions and/or interpreter instructions (e.g. Java interpreter instructions). Alternatively, one or more or all of the sub-routines may be stored in at least one external library file and linked with a main program either statically or dynamically, e.g. at run-time. The main program contains at least one call to at least one of the sub-routines. The sub-routines may also comprise function calls to each other.</p><p id="p-0067" num="0066">An embodiment relating to a computer program product comprises computer-executable instructions corresponding to each processing stage of at least one of the methods set forth herein. These instructions may be sub-divided into sub-routines and/or stored in one or more files that may be linked statically or dynamically. Another embodiment relating to a computer program product comprises computer-executable instructions corresponding to each means of at least one of the systems and/or products set forth herein. These instructions may be sub-divided into sub-routines and/or stored in one or more files that may be linked statically or dynamically.</p><p id="p-0068" num="0067">The carrier of a computer program may be any entity or device capable of carrying the program. For example, the carrier may include a data storage, such as a ROM, for example, a CD ROM or a semiconductor ROM, or a magnetic recording medium, for example, a hard disk. Furthermore, the carrier may be a transmissible carrier such as an electric or optical signal, which may be conveyed via electric or optical cable or by radio or other means. When the program is embodied in such a signal, the carrier may be constituted by such a cable or other device or means. Alternatively, the carrier may be an integrated circuit in which the program is embedded, the integrated circuit being adapted to perform, or used in the performance of, the relevant method.</p><p id="p-0069" num="0068">Variations to the disclosed embodiments can be understood and effected by those skilled in the art in practicing the claimed invention, from a study of the drawings, the disclosure and the appended claims. In the claims, the word &#x201c;comprising&#x201d; does not exclude other elements or steps, and the indefinite article &#x201c;a&#x201d; or &#x201c;an&#x201d; does not exclude a plurality. A single processor or other unit may fulfil the functions of several items recited in the claims. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage. A computer program may be stored/distributed on a suitable medium, such as an optical storage medium or a solid-state medium supplied together with or as part of other hardware, but may also be distributed in other forms, such as via the Internet or other wired or wireless telecommunication systems. Any reference signs in the claims should not be construed as limiting the scope.</p><p id="p-0070" num="0069">The above disclosure sets forth specific details, such as particular embodiments or examples for purposes of explanation and not limitation. It will be appreciated by one skilled in the art that other examples may be employed apart from these specific details.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for end-to-end request scheduling for a distributed system, the distributed system comprising a plurality of hosts via which one or more requests are transmitted, the method comprising:<claim-text>receiving the one or more requests;</claim-text><claim-text>assigning global scheduling information to each of the one or more requests;</claim-text><claim-text>transmitting, for each of the one or more requests, respectively assigned global scheduling information with the respective request, such that respective global scheduling information is made available to a local scheduling unit corresponding to a host via which each of the plurality of requests is transmitted; and</claim-text><claim-text>determining, for each of one or more one or more requests received at each of the plurality of hosts, an order in which at least one of: a computation operation, a communication operation, and an input/output operation associated with the respective request is performed, wherein the determination is based on the global scheduling information assigned to the respective request.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method is implemented at a scheduling system comprising an entry-point and a plurality of local scheduling units each corresponding to one of the plurality of hosts in the distributed system, wherein the steps of receiving the one or more requests and assigning global scheduling information are performed at the entry-point, and the step of determining an order in which at least one of a computation operation, a communication operation, and an input/output operation associated with a request is performed by a local scheduling unit corresponding to the respective host at which the respective request is received.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each of the plurality of hosts is configured to host one or more applications, each of the one or more applications comprising one or more application components, the method further comprising:<claim-text>performing, by the one or more application components, at least one of a computation operation, a communication operation, and an input/output operation corresponding to each of the one or more requests received at the respective corresponding hosts in the order determined by the corresponding local scheduling unit.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the global scheduling information assigned to each of the one or more requests comprises an arrival time of the respective request at the entry-point.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein an arrival time of a respective request at the entry-point is expressed in the format of integers in 8-bit, or 16-bit, or 32-bit, or 64-bit.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein determining the order in which at least one of a computation operation, a communication operation, and an input/output operation associated with the respective request is performed based on a first-come-first-served scheduling policy or earliest-deadline-first scheduling policy, by prioritizing requests associated with the earliest arrival times.</claim-text></claim><claim id="CLM-007-9" num="007-9"><claim-text><b>7</b>-<b>9</b>. (canceled)</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A computer program product comprising a computer readable medium, the computer readable medium having computer readable code embodied therein, the computer readable code being configured such that, on execution by a suitable computer or processor, the computer or processor is caused to:<claim-text>receive one or more requests;</claim-text><claim-text>assign global scheduling information to each of the one or more requests;</claim-text><claim-text>transmit, for each of the one or more requests, respectively assigned global scheduling information with the respective request, such that respective global scheduling information is made available to a local scheduling unit corresponding to a host via which each of the plurality of requests is transmitted; and</claim-text><claim-text>determine, for each of one or more one or more requests received at each of the plurality of hosts, an order in which at least one of: a computation operation, a communication operation, and an input/output operation associated with the respective request is performed, wherein the determination is based on the global scheduling information assigned to the respective request.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A scheduling system for performing end-to-end request scheduling at a distributed system comprising a plurality of hosts via which one or more requests are transmitted, wherein the scheduling system extends a network communication protocol implemented by the distributed system and comprises:<claim-text>an entry-point configured to receive the one or more requests and to assign global scheduling information to each of the one or more requests; and</claim-text><claim-text>a plurality of local scheduling units, wherein each of the plurality of local scheduling units corresponds to one of the plurality of hosts in the distributed system,</claim-text><claim-text>wherein the scheduling system is configured to extend the network communication protocol to perform the following:<claim-text>transmitting, for each of the one or more requests, respectively assigned global scheduling information with the respective request via a protocol extension associated with the network communication protocol, such that the respective global scheduling information is made available to a local scheduling unit corresponding to a host via which the respective request is transmitted,</claim-text><claim-text>wherein each of the plurality of local scheduling units is configured to determine, for each of one or more one or more requests received at the corresponding host, an order in which at least one of a computation operation, a communication operation, and an input/output operation associated with the respective request is performed, wherein the determination is based on the global scheduling information assigned to the respective request.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The scheduling system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising the plurality of hosts, wherein each of the plurality of hosts is configured to host one or more applications, each of the one or more applications comprising one or more application components,<claim-text>wherein, for each of the one or more applications, the one or more application components are configured to perform at least one of a computation operation, a communication operation, and an input/output operation corresponding to each of the one or more requests received at the respective corresponding hosts in the order determined by the corresponding local scheduling unit.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The scheduling system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the global scheduling information assigned to each of the one or more requests comprises an arrival time of the respective request at the entry-point.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The scheduling system according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein an arrival time of a respective request at the entry-point is expressed in the format of integers in 8-bit, or 16-bit, or 32-bit, or 64-bit.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The scheduling system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein each of the plurality of local scheduling units is configured to determine the order in which at least one of a computation operation, a communication operation, an input/output operation associated with the respective request is performed based on a first-come-first served scheduling policy or an earlier-deadline-first scheduling policy, by prioritizing requests associated with the earliest arrival times.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The scheduling system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein each of the plurality of local scheduling units is configured to update, for each of the requests received at the respective corresponding host, the respective global scheduling information such that it comprises information associated with at least one of: an amount of time spent serving the respective request and an amount of network data transmitted on behalf of the respective request.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The scheduling system according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein each of the plurality of local scheduling units is configured to determine the order in which at least one of a computation operation, a communication operation, and an input/output operation associated with the respective request is performed based on a least-attained-service scheduling policy, by prioritizing requests associated with at least one of: shorter amount of time serving the request and smaller amount of network data transmitted on behalf of the request.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The scheduling system according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the each of the plurality of local scheduling units is implemented in at least one of a level of an execution stack, wherein the execution stack comprises at least one of: a runtime environment, an operating system, and a hypervisor.</claim-text></claim><claim id="CLM-19-21" num="19-21"><claim-text><b>19</b>-<b>21</b>. (canceled)</claim-text></claim></claims></us-patent-application>