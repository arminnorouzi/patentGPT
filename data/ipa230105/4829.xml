<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004830A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004830</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363248</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AI-Based Cognitive Cloud Service</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Oracle International Corporation</orgname><address><city>Redwood Shores</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>AREVALO</last-name><first-name>Orlando</first-name><address><city>Hurth</city><country>DE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments provide cognitive cloud services. Embodiments receive, via an input Application Programming Interface (&#x201c;API&#x201d;), input data, the input data including one or more of text data, picture data, audio data and video data. Embodiments determine one or more formats of the input data and, based on the determined formats, select one or more of artificial intelligence based modules for processing of the input data. Embodiments collect an output resulting from the processing of the input data and enrich the output. Embodiments then provide the enriched output via an output API.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="89.66mm" wi="158.75mm" file="US20230004830A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="268.39mm" wi="178.99mm" orientation="landscape" file="US20230004830A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="238.25mm" wi="154.18mm" orientation="landscape" file="US20230004830A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="268.39mm" wi="151.47mm" orientation="landscape" file="US20230004830A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="251.88mm" wi="164.42mm" file="US20230004830A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="206.59mm" wi="196.68mm" file="US20230004830A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">One embodiment is directed generally to artificial intelligence, and in particular to an artificial intelligence-based cognitive cloud service.</p><heading id="h-0002" level="1">BACKGROUND INFORMATION</heading><p id="p-0003" num="0002">Cognitive computing or cognitive services refers to technology platforms that are generally based on the scientific disciplines of artificial intelligence and signal processing. These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (i.e., object recognition), human-computer interaction, dialog and narrative generation, among other technologies.</p><p id="p-0004" num="0003">Further, cognitive computing has been used to refer to new hardware and/or software that mimics the functioning of the human brain and helps to improve human decision-making. In this sense, cognitive computing is a new type of computing with the goal of more accurate models of how the human brain/mind senses, reasons, and responds to stimulus.</p><p id="p-0005" num="0004">Cognitive computing systems/services may be adaptive, in that they may learn as information changes, and as goals and requirements evolve, they may resolve ambiguity and tolerate unpredictability, and they may be engineered to feed on dynamic data in real time, or near real time. Cognitive computing systems/services may be interactive in that they may interact easily with users so that those users can define their needs comfortably, and they may also interact with other processors, devices, and cloud services, as well as with people.</p><p id="p-0006" num="0005">Cognitive computing systems/services may be iterative and stateful in that they may aid in defining a problem by asking questions or finding additional source input if a problem statement is ambiguous or incomplete, and they may &#x201c;remember&#x201d; previous interactions in a process and return information that is suitable for the specific application at that point in time. Cognitive computing systems/services may be contextual in that they may understand, identify, and extract contextual elements such as meaning, syntax, time, location, appropriate domain, regulations, user's profile, process, task and goal. They may draw on multiple sources of information, including both structured and unstructured digital information, as well as sensory inputs (visual, gestural, auditory, or sensor-provided).</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0007" num="0006">Embodiments provide cognitive cloud services. Embodiments receive, via an input Application Programming Interface (&#x201c;API&#x201d;), input data, the input data including one or more of text data, picture data, audio data and video data. Embodiments determine one or more formats of the input data and, based on the determined formats, select one or more of artificial intelligence based modules for processing of the input data. Embodiments collect an output resulting from the processing of the input data and enrich the output. Embodiments then provide the enriched output via an output API.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an overview diagram of elements of an AI based cognitive cloud service/system that can implement embodiments of the invention.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of one or more components of system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the form of a computer server/system in accordance with an embodiment of the present invention.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a high level diagram of the functionality of the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance to embodiments.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of the functionality of the AI cognitive cloud service module of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for performing AI cognitive cloud services in accordance with one embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example input to demonstrate unexpected results of embodiments of the invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0013" num="0012">One embodiment is an integrated artificial intelligence (&#x201c;AI&#x201d;) based cognitive system/service that provides cognitive analysis of audio and video based sources and generates an enriched file based on the cognitive analysis.</p><p id="p-0014" num="0013">Reference will now be made in detail to the embodiments of the present disclosure, examples of which are illustrated in the accompanying drawings. In the following detailed description, numerous specific details are set forth in order to provide a thorough understanding of the present disclosure. However, it will be apparent to one of ordinary skill in the art that the present disclosure may be practiced without these specific details. In other instances, well-known methods, procedures, components, and circuits have not been described in detail so as not to unnecessarily obscure aspects of the embodiments. Wherever possible, like reference numbers will be used for like elements.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an overview diagram of elements of an AI based cognitive cloud service/system <b>150</b> that can implement embodiments of the invention. In general, system <b>150</b> is a cloud based AI service available via an Application Programming Interface (&#x201c;API&#x201d;) call. System <b>150</b> provides an integrated cognitive analysis of audio and video files as well as streams, and include the following functionalities: (1) audio to text translation; (2) language recognition; (3) object detection, recognition and classification; (4) scene description (e.g., natural language generation based on what is happening in the video); (5) text to audio translation (e.g., reading texts on pictures or scenes); (6) entity recognition and classification; (7) audio and text anonymization based on a given entity based filtering (e.g., bleep out all names of persons); (8) content search based on semantic and syntactic queries (e.g., &#x201c;find scenes when a person crosses a street and phones at the same time&#x201d;); and (9) any further capabilities to produce a machine based understanding and analysis of audio and video content.</p><p id="p-0016" num="0015">System <b>150</b> is implemented on a cloud <b>110</b> so that it functions as a Software as a service (&#x201c;SaaS&#x201d;). Cloud computing in general is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. In one embodiment, cloud <b>110</b> is implemented by the Oracle Cloud Infrastructure (&#x201c;OCI&#x201d;) by Oracle Corp.</p><p id="p-0017" num="0016">System <b>150</b> receives, as input data <b>100</b>, text data <b>101</b>, picture data <b>102</b>, audio data <b>103</b> and/or video data <b>104</b>. Input data <b>100</b> can be on-demand or live streamed. Subsequently, system <b>150</b> outputs an enriched file <b>120</b>. Enriched file <b>120</b> is structured to provide information as if a person would have analyzed all the inputs and provided a detailed description of what the entire input content conveys.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of one or more components of system <b>150</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the form of a computer server/system <b>10</b> in accordance with an embodiment of the present invention. Although shown as a single system, the functionality of system <b>10</b> can be implemented as a distributed system. Further, the functionality disclosed herein can be implemented on separate servers or devices that may be coupled together over a network. Further, one or more components of system <b>10</b> may not be included. System <b>10</b> can be used to implement any of the components/elements shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and/or interact with any of the components.</p><p id="p-0019" num="0018">System <b>10</b> includes a bus <b>12</b> or other communication mechanism for communicating information, and a processor <b>22</b> coupled to bus <b>12</b> for processing information. Processor <b>22</b> may be any type of general or specific purpose processor. System <b>10</b> further includes a memory <b>14</b> for storing information and instructions to be executed by processor <b>22</b>. Memory <b>14</b> can be comprised of any combination of random access memory (&#x201c;RAM&#x201d;), read only memory (&#x201c;ROM&#x201d;), static storage such as a magnetic or optical disk, or any other type of computer readable media. System <b>10</b> further includes a communication device <b>20</b>, such as a network interface card, to provide access to a network. Therefore, a user may interface with system <b>10</b> directly, or remotely through a network, or any other method.</p><p id="p-0020" num="0019">Computer readable media may be any available media that can be accessed by processor <b>22</b> and includes both volatile and nonvolatile media, removable and non-removable media, and communication media. Communication media may include computer readable instructions, data structures, program modules, or other data in a modulated data signal such as a carrier wave or other transport mechanism, and includes any information delivery media.</p><p id="p-0021" num="0020">Processor <b>22</b> is further coupled via bus <b>12</b> to a display <b>24</b>, such as a Liquid Crystal Display (&#x201c;LCD&#x201d;) and includes a microphone for receiving user utterances. A keyboard <b>26</b> and a cursor control device <b>28</b>, such as a computer mouse, are further coupled to bus <b>12</b> to enable a user to interface with system <b>10</b>.</p><p id="p-0022" num="0021">In one embodiment, memory <b>14</b> stores software modules that provide functionality when executed by processor <b>22</b>. The modules include an operating system <b>15</b> that provides operating system functionality for system <b>10</b>. The modules further include an AI cognitive services module <b>16</b> that implements AI cognitive services, and all other functionality disclosed herein. System <b>10</b> can be part of a larger system. Therefore, system <b>10</b> can include one or more additional functional modules <b>18</b> to include the additional functionality. A file storage device or database <b>17</b> is coupled to bus <b>12</b> to provide centralized storage for modules <b>16</b> and <b>18</b>. In one embodiment, database <b>17</b> is a relational database management system (&#x201c;RDBMS&#x201d;) that can use Structured Query Language (&#x201c;SQL&#x201d;) to manage the stored data.</p><p id="p-0023" num="0022">In one embodiment, particularly when there are a large number of distributed files at a single device, database <b>17</b> is implemented as an in-memory database (&#x201c;IMDB&#x201d;). An IMDB is a database management system that primarily relies on main memory for computer data storage. It is contrasted with database management systems that employ a disk storage mechanism. Main memory databases are faster than disk-optimized databases because disk access is slower than memory access, the internal optimization algorithms are simpler and execute fewer CPU instructions. Accessing data in memory eliminates seek time when querying the data, which provides faster and more predictable performance than disk.</p><p id="p-0024" num="0023">In one embodiment, database <b>17</b>, when implemented as an IMDB, is implemented based on a distributed data grid. A distributed data grid is a system in which a collection of computer servers work together in one or more clusters to manage information and related operations, such as computations, within a distributed or clustered environment. A distributed data grid can be used to manage application objects and data that are shared across the servers. A distributed data grid provides low response time, high throughput, predictable scalability, continuous availability, and information reliability. In particular examples, distributed data grids, such as, e.g., the &#x201c;Oracle Coherence&#x201d; data grid from Oracle Corp., store information in-memory to achieve higher performance, and employ redundancy in keeping copies of that information synchronized across multiple servers, thus ensuring resiliency of the system and continued availability of the data in the event of failure of a server.</p><p id="p-0025" num="0024">In one embodiment, system <b>10</b> is a computing/data processing system including an application or collection of distributed applications for enterprise organizations, and may also implement logistics, manufacturing, and inventory management functionality. The applications and computing system <b>10</b> may be configured to operate with or be implemented as a cloud-based system, a software-as-a-service (&#x201c;SaaS&#x201d;) architecture, or other type of computing solution.</p><p id="p-0026" num="0025">In general, with known solutions, the capabilities of machine learning (&#x201c;ML&#x201d;) and AI have been exploited in relatively isolated and specialized domains, for very localized use cases and for separated different formats of data. However, text, audio, pictures and video data are being generated in immense volumes and speed every day, either on-demand or in live streaming.</p><p id="p-0027" num="0026">In contrast, embodiments are directed to a cloud based, fully integrated and human like cognitive service, able to analyze and enrich those contents, which can open the doors for solving innumerable challenges of using machines and unfold the power of interaction between human and machines. Embodiments unify disparate AI and ML solutions to ultimately resemble the real ways the human brain works (i.e., by considering all kinds of inputs in parallel and understanding the context it is exposed to as a whole).</p><p id="p-0028" num="0027">Referring again to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, system <b>150</b> includes an AI cognitive cloud service module <b>140</b> that the cloud-based AI service available via API calls. Module <b>150</b> provides an integrated cognitive analysis of text, image, audio and video files, input on demand, as well as in streams. An integration module <b>140</b> provides for the automatic format detection and forwarding to relevant satellites service modules <b>130</b>-<b>139</b> and the orchestration of satellite service modules <b>130</b>-<b>139</b> that perform the functionality of (1) speech to text translation; (2) language recognition and translation; (3) topic and sentiment analysis; (4) object detection, recognition and classification; (4) text to speech translation; (5) read texts on pictures or scenes; (6) entity recognition, classification and anonymization (i.e. entity filtering); (7) scene description via natural-language generation (&#x201c;NLG&#x201d;) made on what is happening in the scenes; and (8) content search based on semantic and syntactic queries. The output of module <b>140</b> is a comprehensive audiovisual material, where an enriched, human-like summary of the input content <b>100</b> is automatically produced.</p><p id="p-0029" num="0028">The satellite service modules <b>130</b>-<b>139</b> provide data processing and data transformation using trained AI and ML models. Each of modules <b>130</b>-<b>139</b> are implemented with state of the art ML and AI models required for all the functionalities of the service. Embodiments collect enough data relevant to train (and regularly re-train) the collected models. Embodiments serialize the trained models in an efficient format, in order to used them for producing predictions close to real time. Model serialization is performed in embodiments by writing objects (within the &#x201c;object oriented programming&#x201d; context) into a byte-stream, which can be stored onto a non-volatile computer memory device. Once stored, this file can be read at any later point in time, thus retrieving the stored objects to reuse them in new programming routines or algorithms. Standard methods for model serialization in ML and AI include, for example, &#x201c;pickle&#x201d;, &#x201c;joblib&#x201d;, &#x201c;hdf5&#x201d; for the Python language or &#x201c;POJO&#x201d;, &#x201c;MOJO&#x201d; for the Java language.</p><p id="p-0030" num="0029">Embodiments embed the whole model training and prediction into a continuous integration (&#x201c;CI&#x201d;) and continuous delivery (&#x201c;CD&#x201d;) framework, in order to allow for a continuous development and release cycle of the service version. Embodiments embed the whole model training and prediction into an infrastructure as code (&#x201c;IaC&#x201d;) framework, in order to efficiently manage the hardware and software resources for training the models and offering predictions close to real time.</p><p id="p-0031" num="0030">An entity recognizer <b>131</b> provides entity recognition, classification and anonymization. Named-entity recognition (&#x201c;NER&#x201d;) (also known as named entity identification, entity chunking, and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities, e.g. mentioned in unstructured text, into pre-defined categories such as person names, organizations/institutions, locations, time expressions, quantities, monetary values, percentages, medical codes, etc.</p><p id="p-0032" num="0031">NER generally entails identifying names (one or more words) in text and assigning them a type (e.g., person, location, organization). State-of-the-art supervised approaches use statistical models that incorporate a name's form, its linguistic context, and its compatibility with known names. These models are typically trained using supervised machine learning and rely on large collections of text where each name has been manually annotated, specifying the word span and named entity type.</p><p id="p-0033" num="0032">A language recognizer <b>132</b> provides language recognition and translation. In natural language processing, language identification or language guessing is the process of determining which natural language the given content is in. Computational approaches to this problem view it as a special case of text categorization, solved with various statistical methods.</p><p id="p-0034" num="0033">Embodiments can use several statistical approaches to language identification using different techniques to classify the data. One technique is to compare the compressibility of the text to the compressibility of texts in a set of known languages. This approach is known as mutual information based distance measure. The same technique can also be used to empirically construct family trees of languages which closely correspond to the trees constructed using historical methods. Mutual information based distance measure is essentially equivalent to more conventional model-based methods.</p><p id="p-0035" num="0034">Another technique is to create a language n-gram model from a &#x201c;training text&#x201d; for each of the languages. These models can be based on characters or encoded bytes. In the latter, language identification and character encoding detection are integrated. Then, for any piece of text needing to be identified, a similar model is made, and that model is compared to each stored language model. The most likely language is the one with the model that is most similar to the model from the text needing to be identified. This approach can be problematic when the input text is in a language for which there is no model. In that case, the method may return another, &#x201c;most similar&#x201d; language as its result.</p><p id="p-0036" num="0035">An optical character recognizer <b>133</b> provides optical character recognition (&#x201c;OCR&#x201d;). OCR is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (e.g., the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (e.g., from a television broadcast).</p><p id="p-0037" num="0036">Embodiments can use two different types of OCR algorithms which may produce a ranked list of candidate characters. Specifically, matrix matching involves comparing an image to a stored glyph on a pixel-by-pixel basis. It is also known as &#x201c;pattern matching&#x201d;, &#x201c;pattern recognition&#x201d;, or &#x201c;image correlation&#x201d;. This relies on the input glyph being correctly isolated from the rest of the image, and on the stored glyph being in a similar font and at the same scale. This technique works best with typewritten text and does not work well when new fonts are encountered.</p><p id="p-0038" num="0037">Feature extraction decomposes glyphs into &#x201c;features&#x201d; such as lines, closed loops, line direction, and line intersections. The extraction features reduces the dimensionality of the representation and makes the recognition process computationally efficient. These features are compared with an abstract vector-like representation of a character, which might reduce to one or more glyph prototypes. Nearest neighbor classifiers such as the k-nearest neighbors algorithm are used to compare image features with stored glyph features and choose the nearest match.</p><p id="p-0039" num="0038">An optical recognizer <b>134</b> provides object detection. Object detection is a technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class, such as humans, buildings, cars, etc., in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.</p><p id="p-0040" num="0039">Embodiments can implement object detection either using neural network-based or non-neural approaches. For non-neural approaches, it becomes necessary to first define features and then using a technique such as support vector machine (&#x201c;SVM&#x201d;) to do the classification. On the other hand, neural techniques are able to do end-to-end object detection without specifically defining features, and can be based on convolutional neural networks (&#x201c;CNN&#x201d;).</p><p id="p-0041" num="0040">A speech to text converter <b>135</b> provides speech recognition. Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers. It is also known as automatic speech recognition, computer speech recognition or speech to text. It incorporates knowledge and research in the computer science, linguistics, statistical learning and software engineering fields.</p><p id="p-0042" num="0041">One embodiment uses Hidden Markov Models (&#x201c;HMM&#x201d;) for the speech recognition. These are statistical models that output a sequence of symbols or quantities. HMMs are used in speech recognition because a speech signal can be viewed as a piecewise stationary signal or a short-time stationary signal. In a short time-scale (e.g., 10 milliseconds), speech can be approximated as a stationary process. Speech can be thought of as a Markov model for many stochastic purposes. Further, HMMs can be trained automatically and are simple and computationally feasible to use. In speech recognition, the hidden Markov model would output a sequence of n-dimensional real-valued vectors (with n being a small integer, such as 10), outputting one of these every 10 milliseconds. The vectors would consist of cepstral coefficients, which are obtained by taking a Fourier transform of a short time window of speech and decorrelating the spectrum using a cosine transform, then taking the first (most significant) coefficients. The hidden Markov model will tend to have in each state a statistical distribution that is a mixture of diagonal covariance Gaussians, which will give a likelihood for each observed vector. Each word, or (for more general speech recognition systems), each phoneme, will have a different output distribution; a hidden Markov model for a sequence of words or phonemes is made by concatenating the individual trained hidden Markov models for the separate words and phonemes.</p><p id="p-0043" num="0042">A syntax parsing based search engine <b>136</b> provides information retrieval. An information retrieval query language is used to make queries for searching on indexed content. A query language is formally defined in a context-free grammar and can be used by users in a textual, visual/UI or speech form. Advanced query languages are often defined for professional users in vertical search engines, so they get more control over the formulation of queries. For instance, natural query language supporting human-like querying by parsing the natural language query to a form that can be best used to retrieve relevant contents inside documents, for example with question-answering systems or conversational search.</p><p id="p-0044" num="0043">Syntax parser <b>136</b> in embodiments that takes input data (frequently text) and builds a data structure&#x2014;often some kind of parse tree, abstract syntax tree or other hierarchical structure, giving a structural representation of the input while checking for correct syntax. The parsing may be preceded or followed by other steps, or these may be combined into a single step. The parser is often preceded by a separate lexical analyzer, which creates tokens from the sequence of input characters; alternatively, these can be combined in scannerless parsing. Parsers may be programmed by hand or may be automatically or semi-automatically generated by a parser generator. Parsing is complementary to templating, which produces formatted output. These may be applied to different domains, but often appear together, such as the scanf/printf pair, or the input (front end parsing) and output (back end code generation) stages of a compiler.</p><p id="p-0045" num="0044">The input to a parser is often text in some computer language, but may also be text in a natural language or less structured textual data, in which case generally only certain parts of the text are extracted, rather than a parse tree being constructed. Parsers range from very simple functions such as scanf, to complex programs such as the frontend of a C++ compiler or the HTML parser of a web browser. An important class of simple parsing is done using regular expressions, in which a group of regular expressions defines a regular language and a regular expression engine automatically generating a parser for that language, allowing pattern matching and extraction of text. In other contexts regular expressions are instead used prior to parsing, as the lexing step whose output is then used by the parser.</p><p id="p-0046" num="0045">A natural language generator <b>137</b> provides NLG. NLG is a software-based process that produces natural language output. Common applications of NLG methods include the production of various reports, for example weather and patient reports, image captions, landscape description and chatbots. Automated NLG can be compared to the process that humans use when they turn ideas into writing or speech. Psycholinguists prefer the term language production for this process, which can also be described in mathematical terms, or modeled in a computer for psychological research. In embodiments, natural language generation uses character-based recurrent neural networks with finite-state prior knowledge.</p><p id="p-0047" num="0046">Text to speech converter <b>138</b> provides text to speech conversion. Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer. It can be implemented in software and/or hardware products. A text-to-speech (&#x201c;TTS&#x201d;) system converts normal language text into speech. Other systems render symbolic linguistic representations like phonetic transcriptions into speech.</p><p id="p-0048" num="0047">Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely &#x201c;synthetic&#x201d; voice output.</p><p id="p-0049" num="0048">A topic and sentiment analyzer <b>139</b> provides sentiment extraction. In ML and NLP, a topic model is a type of statistical model for discovering the abstract &#x201c;topics&#x201d; occurring in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one expects that particular words appear in the document more or less frequently. A document typically concerns multiple topics in different proportions. The &#x201c;topics&#x201d; produced by topic modeling techniques are clusters of similar words, captured in a mathematical framework allowing examination and discovery, based on the statistics of the words in the whole text corpus.</p><p id="p-0050" num="0049">Sentiment analysis (i.e., opinion mining or emotion AI) is the use of NLP, text analysis, computational linguistics and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. It is widely applied to people's feedbacks such as reviews, survey responses, online and social media, marketing campaigns, etc.</p><p id="p-0051" num="0050">In one embodiment, topic and sentiment analyzer <b>139</b> generates sentiment analysis and generates a corresponding polarity score. For example, if the polarity is &#x3e;0, the sentiment of the input text is considered positive, &#x3c;0 is considered negative, and =0 is considered neutral. In one embodiment, an artificial neural network or other type of artificial intelligence is used for the semantic analysis of <b>139</b> as disclosed, for example, in U.S. Pat. Pub. Nos. 2020/0394478. In this embodiment, a word embedding model including a first plurality of features is generated. A value indicating sentiment for the words in the first data set can be determined using a convolutional neural network (&#x201c;CNN&#x201d;). A second plurality of features are generated based on bigrams identified in the data set. The bigrams can be generated using a co- occurrence graph. The model is updated to include the second plurality of features, and sentiment analysis can be performed on a second data set using the updated model. In other embodiments, other techniques for using a neural network for semantic analysis and polarity assignment, such as disclosed in U.S. Pat. Pub. Nos. 2017/0249389 and 2020/0286000, are implemented.</p><p id="p-0052" num="0051">In one embodiment, each of modules <b>130</b>-<b>139</b> are implemented by a separately trained neural network. The training of the neural network from a given example is conducted by determining the difference between the processed output of the network (often a prediction) and a target output, which is the &#x201c;error&#x201d;. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training is terminated based upon certain criteria, known as &#x201c;supervised learning.&#x201d;</p><p id="p-0053" num="0052">System <b>150</b> further includes a cloud API input <b>111</b> that provides an input to module <b>140</b>. In one embodiment, API input <b>111</b> is a representational state transfer (&#x201c;REST&#x201d;) API service, able to receive a request with a header and a payload. The header and payload are used for specifying usage options of the service, as well as the audiovisual content to be analyzed by the central component. The endpoint of this API resides on cloud <b>110</b>. API <b>111</b> interacts with several standard programming languages for machines, websites and mobile applications (e.g., JAVA, Python, Scala, Ruby, Go, etc.).</p><p id="p-0054" num="0053">System <b>150</b> further includes a cloud API output <b>112</b> that provides an API output. In one embodiment, API output <b>112</b> is a REST API service able to return requests containing a service response. The service response includes metadata from the initial request and the performed calculation itself, as well as the comprehensive audiovisual file resulting from the analysis of the central component.</p><p id="p-0055" num="0054">APIs <b>111</b>, <b>112</b> in embodiments can be accessed and queried via HTTPS requests, offering the cognitive service in a standard and universally integrable manner.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a high level diagram of the functionality of system <b>150</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in accordance to embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, at <b>301</b>, AI cognitive cloud service module <b>140</b> performs data input using data <b>100</b> and logical data pre-processing. At <b>302</b>, one or more of modules <b>130</b>-<b>139</b> perform data processing and data transformation using ML and AI models. At <b>303</b>, AI cognitive cloud service module <b>140</b> performs data consolidation and data enrichment and outputs the results <b>120</b>.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of the functionality of AI cognitive cloud service module <b>140</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for performing AI cognitive cloud services in accordance with one embodiment. In one embodiment, the functionality of the flow diagram of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is implemented by software stored in memory or other computer readable or tangible medium, and executed by a processor. In other embodiments, the functionality may be performed by hardware (e.g., through the use of an application specific integrated circuit (&#x201c;ASIC&#x201d;), a programmable gate array (&#x201c;PGA&#x201d;), a field programmable gate array (&#x201c;FPGA&#x201d;), etc.), or any combination of hardware and software.</p><p id="p-0058" num="0057">At <b>402</b>, module <b>150</b> receives a REST API input call <b>111</b>. API input call <b>111</b> in embodiments include the following API parameters:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0058">username: Registered name of user.</li>        <li id="ul0002-0002" num="0059">password: User's authentication passphrase.</li>        <li id="ul0002-0003" num="0060">input_uri: Location of input file, e.g. local, web, streaming endpoint, etc.</li>        <li id="ul0002-0004" num="0061">translate_to: If translation into another language is required.</li>        <li id="ul0002-0005" num="0062">text_to_speech: If output should include audio from found text.</li>        <li id="ul0002-0006" num="0063">output_uri: Location of output like input_uri.</li>        <li id="ul0002-0007" num="0064">other_options: Another desired processing parameters.</li>    </ul>    </li></ul></p><p id="p-0059" num="0065">The following pseudo-code is an example of API input call <b>111</b>:</p><p id="p-0060" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><thead><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>import oracle_cognitive_service as ocg;</entry></row><row><entry>my_session = ocg.session(username = &#x201c;john_doe&#x201d;, password = &#x201c;mY-53cr3t5&#x201d;</entry></row><row><entry>my_request = my_session(</entry></row><row><entry>&#x2003;input_uri = {</entry></row><row><entry>&#x2003;&#x2003;&#x201c;path_from_input_file&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;input_url&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;other_service_endpoint&#x201d;</entry></row><row><entry>&#x2003;)</entry></row><row><entry>&#x2003;translate_to = {&#x201c;en&#x201d;, &#x201c;de&#x201d;, &#x201c;es&#x201d;, &#x201c;fr&#x201d;, &#x201c;it&#x201d;, &#x201c;nl&#x201d;, &#x201c;jp&#x201d;, &#x201c;ch&#x201d;, . . .},</entry></row><row><entry>&#x2003;text_to_speech = {&#x201c;no&#x201d;, &#x201c;yes&#x201d;},</entry></row><row><entry>&#x2003;output_uri = {</entry></row><row><entry>&#x2003;&#x2003;&#x201c;path_to_output_file&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;output_url&#x201d;,</entry></row><row><entry>&#x2003;&#x2003;&#x201c;other_service_endpoint&#x201d;</entry></row><row><entry>&#x2003;},</entry></row><row><entry>&#x2003;other_options = {&#x201c;value_1&#x201d;, &#x201c;value_2&#x201d;, . . .}</entry></row><row><entry>};</entry></row><row><entry>my_request.send();</entry></row><row><entry>my_request.get_result;</entry></row><row><entry>&#x2003;* Strings inside keys are mutually exclusive possible option values! *</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0061" num="0066">The input content to be analyzed by module <b>140</b> can be provided in different formats (e.g., .txt, .doc, .pdf, etc.; jpeg, .png, .gif, etc.; mp3, mp4, .avi, .mpeg, .webm, etc.). The input can be provided from a local data source or streamed on-demand or live.</p><p id="p-0062" num="0067">At <b>404</b>, module <b>140</b> recognizes the format(s) of the input data and based on the format picks one or more of modules <b>130</b>-<b>139</b> for further processing based on the input data and the content of the REST API. Format recognition in one embodiment is performed by analyzing the metadata of any file or data transfer protocol. For example, a given file extension found in the file metadata determines the format of the content. Based on industry standards, it is possible to identify if a file is audio (*.mp3, *.wav, *.ogg, *.wav, *.wma, *.m4p, etc.), or video (*.avi, *.wmv, *.webm, *.mov, *. mp4, etc.), or image (*.tiff, *.gif, *.png, *.jpeg, *.bmp, etc.) or text (*.txt, *.doc, *.rtf, etc.). With this mechanism, module <b>140</b> categorizes the given input and passes it to the further applicable content processing modules.</p><p id="p-0063" num="0068">At <b>406</b>, if the input data is text, NLP and NLG is applied using modules <b>130</b>-<b>139</b>. The AI based functionality applied by modules <b>130</b>-<b>139</b> includes: (1) recognize the language; (2) recognize entities; (3) recognize topics; (4) analyze sentiments; (5) if requested, provide language translation; (6) if requested, perform text to speech conversion.</p><p id="p-0064" num="0069">At <b>408</b>, if the input data is audio, voice recognition, NLP and NLG is applied using modules <b>130</b>-<b>139</b>. The AI based functionality applied by modules <b>130</b>-<b>139</b> includes: (1) speech to text; (2) recognize the language; (3) recognize entities; (4) recognize topics; (5) analyze sentiments; (6) if requested, provide language translation.</p><p id="p-0065" num="0070">At <b>410</b>, if the input data is image, image processing, NLP and NLG is applied using modules <b>130</b>-<b>139</b>. The AI based functionality applied by modules <b>130</b>-<b>139</b> includes: (1) recognize objects; (2) recognize characters; (3) recognize language; (4) recognize entities; (5) recognize topics; (6) analyze sentiments; (7) if requested, provide language translation; (8) if requested, perform text to speech conversion.</p><p id="p-0066" num="0071">At <b>412</b>, if the input data is video, on a frame by frame basis, image processing, NLP and NLG is applied using modules <b>130</b>-<b>139</b>. The AI based functionality applied by modules <b>130</b>-<b>139</b> includes: (1) recognize objects; (2) recognize characters; (3) speech to text conversion; (4) recognize language; (5) recognize entities; (6) recognize topics; (7) analyze sentiments; (8) if requested, provide language translation; (9) if requested, perform text to speech conversion.</p><p id="p-0067" num="0072">At <b>414</b>, module <b>140</b> collects the output from each of the models of modules <b>130</b>-<b>139</b> and enriches the outputs with natural language. The enriching includes syntax parsing, and text enrichment with NLG. The result is then output via API output <b>112</b>. In embodiments, the enrichment includes applying automatic text summarization, which is the process of producing a machine-generated, concise and meaningful summary of text from multiple text resources such as books, news articles, blog posts, research papers, emails, tweets, etc. In embodiments, the text resources are the ones being previously generated by all other modules <b>130</b>-<b>139</b>, except module <b>137</b>, which covers the NLG tasks as discussed above.</p><p id="p-0068" num="0073">The output <b>112</b> is provided by request against a user authenticated session of the REST API. The resulting output is a set of standard file formats combining all results (i.e., .json with results metadata, .text with summarized analysis, (e.g., text transcriptions of speeches), .mp3 with speech from the .text if specified in options).</p><heading id="h-0006" level="1">Use Cases</heading><p id="p-0069" num="0074">One example use case involves a brand positioning study for brand XYZ based on openly posted videos. Company XYZ, which exhibits a widely known market branding, is very interested on keeping and improving its public image. System <b>150</b> is to be used to provide answers to the following questions: (1) Are customers and consumers from the brand XYZ satisfied with the products and services? (2) How is XYZ positioned in comparison to competitors? (3) What are the most relevant public opinions about XYZ? (4) Which are the segments of people reached by the brand XYZ?</p><p id="p-0070" num="0075">Input into system <b>150</b> are thousands of openly accessible videos where the brand XYZ is mentioned. System <b>150</b> then performs a holistic analysis of all the videos and produce a complete summary of what is mentioned in relation to the brand XYZ and in with which kind of opinion. Modules <b>130</b>-<b>139</b> (except module <b>137</b>) analyze the video extracting many specific details such as which objects are on the scenes, which text, what is said during the video, which entities are mentioned (e.g., other brands), which sentiments are being expressed, etc. Module <b>140</b> collects all those outcomes and reroutes them into module <b>137</b> which then generates an automatic summary using NLG techniques, which provides a holistic text expressing what people are saying, doing and feeling about brand XYZ.</p><p id="p-0071" num="0076">Another example use case is an automated summary of worldwide sport events for a television channel which offers sport news on-demand via a website and a mobile app. System <b>150</b> is used to determine how to efficiently manage the content generation and publishing for the highlights and summaries of hundreds of sport events taking place every weekend.</p><p id="p-0072" num="0077">Embodiments provide an automated, machine based report generation, made possible by system <b>150</b>. Sport, cultural, social and political events, weather predictions, news, trending topics, are happening everywhere with accelerating pace. The day is overflowed with content and information. Processing it manually has become unmanageable. The holistic, audiovisual analysis offered by system <b>150</b> can help to mitigate the spreading of negatively, locally biased social media content. By analyzing the content from many different sources in a machine based manner, more sources, from different locations, languages and tendencies can be merged together to provide a balanced, more objective overview. Modules <b>130</b>-<b>139</b> (except module <b>137</b>) gather all specific aspects from those different sources and provide them via module <b>140</b> into module <b>137</b>. At module <b>137</b>, a summary is generated, where all angles and perspectives are weighted and imprinted into a short text. In this final text, the wide spectrum is shown, instead of a single, strongly biased content. The principle of &#x201c;the wisdom of the crowd&#x201d;, which is one of the foundations of democracy, is thus here applied, by democratizing the content of information.</p><p id="p-0073" num="0078">Another example use case is automatically digitizing old registry documents stored in analog formats for a public office needing to deal with registry data collected in the many decades before the new digital technologies arrived. In order to process any request of service made by citizens, companies and other institutions, the public administration need to double check data stored in analogical form in their registry. This process is manual, time and resource consuming and even unreliable. An efficient solution is absolutely necessary.</p><p id="p-0074" num="0079">The majority of newly generated information is already made available in digital form. However, not all, and even all data collected until 10 to 15 years ago resides in analog format, which system <b>150</b> can extract and process automatically to adapt it to the new digital formats. After, for example, taking pictures, scanning or even videos of different old documents and sources of information, which remain stored in their analog format, these pictures, scans and videos can be processed with module <b>140</b>. It will activate and pass the data through the different applicable modules <b>130</b>-<b>139</b> (except module <b>137</b>). In that sense, a holistic, machine-generated digital format of all those old sources can be produced and stored for a non-invasive and modern way of analysis. Examples include old manuscripts and art pieces from museums (a vast amount of them remain uncategorized), old civil registries (properties, infrastructure, population, environmental, etc.), old library sources, old legal registry (old court cases and decisions, laws valid since long ago) and so on.</p><p id="p-0075" num="0080">Another example use case is infrastructure planning based on information enriching from unstructured data for a regional government which needs to implement a sustainable development planning according to current population needs. Approximately 20% of the data being collected is structured, such as census data, infrastructure databases, etc. However, 80% of the collected data is unstructured, such as aerial pictures of traffic, residential and green areas, public office reports, news. There is a need to enrich the existent structure data to fill the gaps of real need in the sustainable development of the region.</p><p id="p-0076" num="0081">System <b>150</b> can provide the processing, images, videos, text reports, local news as audio and video, etc., that can complete the picture of what are the real pain points of the current infrastructure. With system <b>150</b>, structured data can be extracted from all those input media to measure the real needs for a future sustainable infrastructure. Typical data about infrastructure of a city is stored in charts, plans and documents with a static point of view. Recent and actual data coming from aerial pictures for instance, can be processed by modules <b>130</b>-<b>139</b> (except module <b>137</b>) in order to, for example, recognize and quantify green areas, peoples flux, traffic flux, night illumination gaps, etc. Embodiments apply well established object and entity recognition techniques. Module <b>137</b> can then summarize the current actual situation and even describe its evolution over a given period of time. This will substantially enrich the structure static data already existing in the public registries of city infrastructure.</p><p id="p-0077" num="0082">Embodiments, by implementing a holistic analysis approach using a technological solution of combining multiple models <b>130</b>-<b>139</b>, can provide results that cannot be provided merely by combining individual components. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example input to demonstrate unexpected results of embodiments of the invention. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates the following elements of an image: (1) buildings with cylindrical shape (<b>501</b>); (2) the word ORACLE (<b>502</b>); (3) a lake (<b>503</b>); and (4) a regatta boat (<b>504</b>).</p><p id="p-0078" num="0083">In contrast to individual models, system <b>150</b> can determine, using <figref idref="DRAWINGS">FIG. <b>5</b></figref> as an input, that it is a picture of the Californian headquarters from ORACLE, a global software, hardware and IT services US company, which also competes in regatta yacht racings. Specifically, embodiments uses modules <b>130</b>-<b>139</b> (except module <b>137</b>) for recognizing objects, writings and even entities such as companies or locations. Once this basic information is given back to module <b>140</b>, it reroutes that to module <b>137</b> where all pieces are put together via automatic summarizing. Module <b>137</b> can for instance connect to sources of general knowledge for enriching the summary with references such as Wikipedia, Scholarpedia, etc. Similar to a chatbot, module <b>140</b> can receive the input picture with the implicit answer, what is on that picture. Any known approach will merely list independent, unlinked objects or facts. In contrast, module <b>140</b> will be able to provide a more natural answer to what is there, by combining and linking all those elements on the picture into a single holistic description.</p><p id="p-0079" num="0084">As disclosed, embodiments integrate all selected models in a higher level intelligence management algorithm, which orchestrates the combination of all specialized ML and AI algorithms to provide robust and self-consistent predictions out of input data in different formats. Based on all data processed, measured, analyzed and summarized, embodiments can be used, for example, to provide output as if a human were describing what he/she feels, when seeing a video of a beautiful natural landscape with sounds of birds singing or water from a stream is running.</p><p id="p-0080" num="0085">The features, structures, or characteristics of the disclosure described throughout this specification may be combined in any suitable manner in one or more embodiments. For example, the usage of &#x201c;one embodiment,&#x201d; &#x201c;some embodiments,&#x201d; &#x201c;certain embodiment,&#x201d; &#x201c;certain embodiments,&#x201d; or other similar language, throughout this specification refers to the fact that a particular feature, structure, or characteristic described in connection with the embodiment may be included in at least one embodiment of the present disclosure. Thus, appearances of the phrases &#x201c;one embodiment,&#x201d; &#x201c;some embodiments,&#x201d; &#x201c;a certain embodiment,&#x201d; &#x201c;certain embodiments,&#x201d; or other similar language, throughout this specification do not necessarily all refer to the same group of embodiments, and the described features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p id="p-0081" num="0086">One having ordinary skill in the art will readily understand that the embodiments as discussed above may be practiced with steps in a different order, and/or with elements in configurations that are different than those which are disclosed. Therefore, although this disclosure considers the outlined embodiments, it would be apparent to those of skill in the art that certain modifications, variations, and alternative constructions would be apparent, while remaining within the spirit and scope of this disclosure. In order to determine the metes and bounds of the disclosure, therefore, reference should be made to the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of providing cognitive cloud services comprising:<claim-text>receiving, via an input Application Programming Interface (API), input data, the input data comprising one or more of text data, picture data, audio data and video data;</claim-text><claim-text>determining one or more formats of the input data;</claim-text><claim-text>based on the determined formats, selecting one or more of artificial intelligence based modules for processing of the input data;</claim-text><claim-text>collecting an output resulting from the processing of the input data;</claim-text><claim-text>enriching the output; and</claim-text><claim-text>providing the enriched output via an output API.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining the formats of the input data comprises analyzing metadata corresponding to the input data, further comprising based on a payload of the API and the determined formats, selecting functionality performed by the artificial intelligence based modules.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method <claim-ref idref="CLM-00001">claim 1</claim-ref>, the enriching comprising text enriching using a natural language generator.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, the artificial intelligence based modules comprising trained models that each perform one of: speech to text translation; language recognition and translation; topic and sentiment analysis; object detection, recognition and classification; text to speech translation; reading texts on pictures or scenes; entity recognition, classification and anonymization; scene description via natural-language generation; or content search based on semantic and syntactic queries.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising serializing the trained models.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the input data comprises video data and the enriched output comprises a summary of the video data.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the input API comprises a representational state transfer (REST) API comprising a header and payload and having an endpoint that resides on the cloud.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An artificial intelligence cognitive cloud system comprising:<claim-text>an input Application Programming Interface (API) configured to receive input data, the input data comprising one or more of text data, picture data, audio data and video data;</claim-text><claim-text>one or more processors configured to:<claim-text>determine one or more formats of the input data;</claim-text><claim-text>based on the determined formats, selecting one or more of artificial intelligence based modules for processing of the input data;</claim-text><claim-text>collecting an output resulting from the processing of the input data;</claim-text></claim-text><claim-text>enriching the output; and<claim-text>providing the enriched output via an output API.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the determining the formats of the input data comprises analyzing metadata corresponding to the input data, further comprising based on a payload of the API and the determined formats, selecting functionality performed by the artificial intelligence based modules.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, the enriching comprising text enriching using a natural language generator.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, the artificial intelligence based modules comprising trained models that each perform one of: speech to text translation; language recognition and translation; topic and sentiment analysis; object detection, recognition and classification; text to speech translation; reading texts on pictures or scenes; entity recognition, classification and anonymization; scene description via natural-language generation; or content search based on semantic and syntactic queries.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, the processors further configured to serializing the trained models.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the input data comprises video data and the enriched output comprises a summary of the video data.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the input API comprises a representational state transfer (REST) API comprising a header and payload and having an endpoint that resides on the cloud.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A computer-readable medium storing instructions which, when executed by at least one of a plurality of processors, cause the processors to provide cognitive cloud services, the providing comprising:<claim-text>receiving, via an input Application Programming Interface (API), input data, the input data comprising one or more of text data, picture data, audio data and video data;</claim-text><claim-text>determining one or more formats of the input data;</claim-text><claim-text>based on the determined formats, selecting one or more of artificial intelligence based modules for processing of the input data;</claim-text><claim-text>collecting an output resulting from the processing of the input data;</claim-text><claim-text>enriching the output; and</claim-text><claim-text>providing the enriched output via an output API.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the determining the formats of the input data comprises analyzing metadata corresponding to the input data, further comprising based on a payload of the API and the determined formats, selecting functionality performed by the artificial intelligence based modules.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the enriching comprising text enriching using a natural language generator.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the artificial intelligence based modules comprising trained models that each perform one of: speech to text translation; language recognition and translation; topic and sentiment analysis; object detection, recognition and classification; text to speech translation; reading texts on pictures or scenes; entity recognition, classification and anonymization; scene description via natural-language generation; or content search based on semantic and syntactic queries.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-readable medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, the providing further comprising serializing the trained models.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the input data comprises video data and the enriched output comprises a summary of the video data.</claim-text></claim></claims></us-patent-application>