<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004713A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004713</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363883</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>274</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>274</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>1831</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MACHINE LEARNING RECOMMENDATION ENGINE FOR CONTENT ITEM DATA ENTRY BASED ON MEETING MOMENTS AND PARTICIPANT ACTIVITY</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Dropbox, Inc.</orgname><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Broussard</last-name><first-name>Matthew</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Highley</last-name><first-name>Emma</first-name><address><city>Santa Monica</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Khorashadi</last-name><first-name>Behrooz</first-name><address><city>Davis</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Vivrekar</last-name><first-name>Devangi</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Arnold</last-name><first-name>Hudson</first-name><address><city>Alameda</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Daredia</last-name><first-name>Shehzad</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A content management system obtains at least a portion of a meeting transcript based on an audio stream of a meeting attended by a plurality of users, the meeting transcript obtained in an ongoing manner as words are uttered during the meeting. The content management system detects text entered by a user of the plurality of users into a content item during the meeting. The content management system matches the detected text to at least part of the at least the portion of the meeting transcript. The content management system provides the at least part of the at least the portion of the meeting transcript to the user as a suggested subsequent text.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="109.73mm" wi="158.75mm" file="US20230004713A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="234.53mm" wi="163.58mm" file="US20230004713A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="128.78mm" wi="159.09mm" file="US20230004713A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="213.53mm" wi="167.89mm" file="US20230004713A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="180.68mm" wi="165.52mm" file="US20230004713A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="176.45mm" wi="136.40mm" file="US20230004713A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="212.60mm" wi="156.04mm" orientation="landscape" file="US20230004713A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="212.60mm" wi="156.04mm" orientation="landscape" file="US20230004713A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="212.60mm" wi="156.04mm" orientation="landscape" file="US20230004713A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="174.41mm" wi="163.07mm" orientation="landscape" file="US20230004713A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="191.94mm" wi="118.11mm" file="US20230004713A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The disclosed embodiments generally relate to computer-assisted note taking, and particularly to a content management system that predicts text for a document based on a transcript using machine learning.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">It is common for one or more attendees of a meeting to take notes during the meeting. Notes entered into a content item, such as a text document, by an attendee of the meeting often correspond to words spoken during the meeting. Entering notes into the content item as the meeting progresses can be difficult due to the rate at which information is conveyed during the meeting.</p><p id="p-0004" num="0003">Machine learning techniques can be used for natural language processing, such as to convert audio data into a text document. However, simple transcription of a meeting can be insufficient for the purpose of note taking, as the transcription may include significant amounts of text unimportant to the note taker, e.g., text unrelated to a purpose of the meeting. Moreover, such models are noisy, as they are trained on generic data that is not tuned to subjective preferences of meeting attendees.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">Systems and methods are disclosed herein for a content management system that obtains at least a portion of a meeting transcript based on an audio stream of a meeting attended by a plurality of users. The meeting transcript is obtained in an ongoing manner as words are uttered during the meeting. The content management system detects text entered by a user into a content item during the meeting (e.g., as the user takes notes). The content management system matches the detected text to at least part of the portion of the meeting transcript. The content management system provides at least part of the portion of the meeting transcript to the user as a suggested subsequent text (e.g., for entry into the user's notes).</p><p id="p-0006" num="0005">Contextualizing the relative relevance or importance of utterances in audio data such that a machine learning model can transcribe more relevant or important utterances (e.g., those with at least a threshold score assigned by the model) and not transcribe less relevant or important utterances (e.g., those with less than a threshold score assigned by the model) is difficult due to the imprecise and volatile nature of qualities such as relevance and importance. This is exacerbated by the difficulty in procuring training data for the machine learning model, where significant time and resources would be needed to produce labeled transcripts indicating the relevance or importance of utterances in the transcripts. The systems and methods disclosed herein provide various technical advantages, including solutions to these technical difficulties. For example, the systems and methods disclosed herein provide for machine learning-assisted note taking, enabling the production of meeting notes that have fewer recordation errors and are more compact with less redundant or irrelevant features than a simple autogenerated meeting transcript.</p><p id="p-0007" num="0006">The features and advantages described in this summary and the following detailed description are not all-inclusive. Many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings, specification, and claims hereof.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a diagram of a system environment of a content management system and a collaborative content management system according to one embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of components of a client device, according to one example embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a block diagram of a content management system, according to one example embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a block diagram of a collaborative content management system, according to one example embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a block diagram of an auto-complete module in a content management system, according to one example embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>C</figref> show exemplary user interfaces for a content item with transcript-based auto-complete, according to one example embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a data flow diagram for transcript-based auto-complete, according to one example embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an exemplary process for performing transcript-based auto-complete, according to one example embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0016" num="0015">The figures depict various embodiments of the present invention for purposes of illustration only. One skilled in the art will readily recognize from the following description that other alternative embodiments of the structures and methods illustrated herein may be employed without departing from the principles of the invention described herein.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">System Overview</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a system environment including content management system <b>100</b>, collaborative content management system <b>130</b>, and client devices <b>120</b><i>a</i>, <b>120</b><i>b</i>, and <b>120</b><i>c </i>(collectively or individually &#x201c;<b>120</b>&#x201d;). Content management system <b>100</b> provides functionality for sharing content items with one or more client devices <b>120</b> and synchronizing content items between content management system <b>100</b> and one or more client devices <b>120</b>.</p><p id="p-0019" num="0018">The content stored by content management system <b>100</b> can include any type of content items, such as documents, spreadsheets, collaborative content items, text files, audio files, image files, video files, webpages, executable files, binary files, placeholder files that reference other content items, etc. In some implementations, a content item can be a portion of another content item, such as an image that is included in a document. Content items can also include collections, such as folders, namespaces, playlists, albums, etc., that group other content items together. The content stored by content management system <b>100</b> may be organized in one configuration in folders, tables, or in other database structures (e.g., object oriented, key/value etc.).</p><p id="p-0020" num="0019">In one embodiment, the content stored by content management system <b>100</b> includes content items created by using third party applications, e.g., word processors, video and image editors, database management systems, spreadsheet applications, code editors, and so forth, which are independent of content management system <b>100</b>.</p><p id="p-0021" num="0020">In some embodiments, content stored by content management system <b>100</b> includes content items, e.g., collaborative content items, created using a collaborative interface provided by collaborative content management system <b>130</b>. In various implementations, collaborative content items can be stored by collaborative content item management system <b>130</b>, with content management system <b>100</b>, or external to content management system <b>100</b>. A collaborative interface can provide an interactive content item collaborative platform whereby multiple users can simultaneously create and edit collaborative content items, comment in the collaborative content items, and manage tasks within the collaborative content items.</p><p id="p-0022" num="0021">Users may create accounts at content management system <b>100</b> and store content thereon by sending such content from client device <b>120</b> to content management system <b>100</b>. The content can be provided by users and associated with user accounts that may have various privileges. For example, privileges can include permissions to: see content item titles, see other metadata for the content item (e.g. location data, access history, version history, creation/modification dates, comments, file hierarchies, etc.), read content item contents, modify content item metadata, modify content of a content item, comment on a content item, read comments by others on a content item, or grant or remove content item permissions for other users.</p><p id="p-0023" num="0022">Client devices <b>120</b> communicate with content management system <b>100</b> and collaborative content management system <b>130</b> through network <b>110</b>. The network may be any suitable communications network for data transmission. In one embodiment, network <b>110</b> is the Internet and uses standard communications technologies and/or protocols. Thus, network <b>110</b> can include links using technologies such as Ethernet, 802.11, worldwide interoperability for microwave access (WiMAX), 3G, 4G, digital subscriber line (DSL), asynchronous transfer mode (ATM), InfiniBand, PCI Express Advanced Switching, etc. Similarly, the networking protocols used on network <b>110</b> can include multiprotocol label switching (MPLS), the transmission control protocol/Internet protocol (TCP/IP), the User Datagram Protocol (UDP), the hypertext transport protocol (HTTP), the simple mail transfer protocol (SMTP), the file transfer protocol (FTP), etc. The data exchanged over network <b>110</b> can be represented using technologies and/or formats including the hypertext markup language (HTML), the extensible markup language (XML), JavaScript Object Notation (JSON), etc. In addition, all or some of links can be encrypted using conventional encryption technologies such as the secure sockets layer (SSL), transport layer security (TLS), virtual private networks (VPNs), Internet Protocol security (IPsec), etc. In another embodiment, the entities use custom and/or dedicated data communications technologies instead of, or in addition to, the ones described above.</p><p id="p-0024" num="0023">In some embodiments, content management system <b>100</b> and collaborative content management system <b>130</b> are combined into a single system. The system may include one or more servers configured to provide the functionality discussed herein for the systems <b>100</b> and <b>130</b>.</p><p id="p-0025" num="0024">Client Device</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of the components of a client device <b>120</b> according to one embodiment. Client devices <b>120</b> generally include devices and modules for communicating with content management system <b>100</b> and a user of client device <b>120</b>. Client device <b>120</b> includes display <b>210</b> for providing information to the user, and in certain client devices <b>120</b> includes a touchscreen. Client device <b>120</b> also includes network interface <b>220</b> for communicating with content management system <b>100</b> via network <b>110</b>. There are additional components that may be included in client device <b>120</b> but that are not shown, for example, one or more computer processors, local fixed memory (RAM and ROM), as well as optionally removable memory (e.g., SD-card), power sources, and audio-video outputs.</p><p id="p-0027" num="0026">In certain embodiments, client device <b>120</b> includes additional components such as camera <b>230</b> and location module <b>240</b>. Location module <b>240</b> determines the location of client device <b>120</b>, using, for example, a global positioning satellite signal, cellular tower triangulation, or other methods. Location module <b>240</b> may be used by client application <b>200</b> to obtain location data and add the location data to metadata about a content item.</p><p id="p-0028" num="0027">Client devices <b>120</b> maintain various types of components and modules for operating the client device and accessing content management system <b>100</b>. The software modules can include operating system <b>250</b> or a collaborative content item editor <b>270</b>. Collaborative content item editor <b>270</b> is configured for creating, viewing and modifying collaborative content items such as text documents, code files, mixed media files (e.g., text and graphics), presentations or the like. Operating system <b>250</b> on each device provides a local file management system and executes the various software modules such as content management system client application <b>200</b> and collaborative content item editor <b>270</b>. A contact directory <b>290</b> stores information on the user's contacts, such as name, telephone numbers, company, email addresses, physical address, website URLs, and the like.</p><p id="p-0029" num="0028">Client devices <b>120</b> access content management system <b>100</b> and collaborative content management system <b>130</b> in a variety of ways. Client device <b>120</b> may access these systems through a native application or software module, such as content management system client application <b>200</b>. Client device <b>120</b> may also access content management system <b>100</b> through web browser <b>260</b>. As an alternative, the client application <b>200</b> may integrate access to content management system <b>100</b> with the local file management system provided by operating system <b>250</b>. When access to content management system <b>100</b> is integrated in the local file management system, a file organization scheme maintained at the content management system is represented at the client device <b>120</b> as a local file structure by operating system <b>250</b> in conjunction with client application <b>200</b>.</p><p id="p-0030" num="0029">Client application <b>200</b> manages access to content management system <b>100</b> and collaborative content management system <b>130</b>. Client application <b>200</b> includes user interface module <b>202</b> that generates an interface to the content accessed by client application <b>200</b> and is one means for performing this function. The generated interface is provided to the user by display <b>210</b>. Client application <b>200</b> may store content accessed from a content storage at content management system <b>100</b> in local content <b>204</b>. While represented here as within client application <b>200</b>, local content <b>204</b> may be stored with other data for client device <b>120</b> in non-volatile storage. When local content <b>204</b> is stored this way, the content is available to the user and other applications or modules, such as collaborative content item editor <b>270</b>, when client application <b>200</b> is not in communication with content management system <b>100</b>. Content access module <b>206</b> manages updates to local content <b>204</b> and communicates with content management system <b>100</b> to synchronize content modified by client device <b>120</b> with content maintained on content management system <b>100</b>, and is one means for performing this function. Client application <b>200</b> may take various forms, such as a stand-alone application, an application plug-in, or a browser extension.</p><p id="p-0031" num="0030">Content Management System</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a block diagram of the content management system <b>100</b> according to one embodiment. To facilitate the various content management services, a user can create an account with content management system <b>100</b>. The account information can be maintained in user account database <b>316</b>, and is one means for performing this function. User account database <b>316</b> can store profile information for registered users. In some cases, the only personal information in the user profile is a username and/or email address. However, content management system <b>100</b> can also be configured to accept additional user information, such as password recovery information, demographics information, payment information, and other details. Each user is associated with a userID and a username. For purposes of convenience, references herein to information such as collaborative content items or other data being &#x201c;associated&#x201d; with a user are understood to mean an association between a collaborative content item and either of the above forms of user identifier for the user. Similarly, data processing operations on collaborative content items and users are understood to be operations performed on derivative identifiers such as collaborativeContentItemID and userIDs. For example, a user may be associated with a collaborative content item by storing the information linking the userID and the collaborativeContentItemID in a table, file, or other storage formats. For example, a database table organized by collaborativeContentItemlIDs can include a column listing the userID of each user associated with the collaborative content item. As another example, for each userID, a file can list a set of collaborativeContentItemID associated with the user. As another example, a single file can list key values pairs such as &#x3c;userID, collaborativeContentItemID&#x3e;representing the association between an individual user and a collaborative content item. The same types of mechanisms can be used to associate users with comments, threads, text elements, formatting attributes, and the like.</p><p id="p-0033" num="0032">User account database <b>316</b> can also include account management information, such as account type, e.g. free or paid; usage information for each user, e.g., file usage history; maximum storage space authorized; storage space used; content storage locations; security settings; personal configuration settings; content sharing data; etc. Account management module <b>304</b> can be configured to update and/or obtain user account details in user account database <b>316</b>. Account management module <b>304</b> can be configured to interact with any number of other modules in content management system <b>100</b>.</p><p id="p-0034" num="0033">An account can be used to store content items, such as collaborative content items, audio files, video files, etc., from one or more client devices associated with the account. Content items can be shared with multiple users and/or user accounts. In some implementations, sharing a content item can include associating, using sharing module <b>310</b>, the content item with two or more user accounts and providing for user permissions so that a user that has authenticated into one of the associated user accounts has a specified level of access to the content item. That is, the content items can be shared across multiple client devices of varying type, capabilities, operating systems, etc. The content items can also be shared across varying types of user accounts.</p><p id="p-0035" num="0034">Individual users can be assigned different access privileges to a content item shared with them, as discussed above. In some cases, a user's permissions for a content item can be explicitly set for that user. A user's permissions can also be set based on: a type or category associated with the user (e.g., elevated permissions for administrator users or manager), the user's inclusion in a group or being identified as part of an organization (e.g., specified permissions for all members of a particular team), and/or a mechanism or context of a user's accesses to a content item (e.g., different permissions based on where the user is, what network the user is on, what type of program or API the user is accessing, whether the user clicked a link to the content item, etc.). Additionally, permissions can be set by default for users, user types/groups, or for various access mechanisms and contexts.</p><p id="p-0036" num="0035">In some implementations, shared content items can be accessible to a recipient user without requiring authentication into a user account. This can include sharing module <b>310</b> providing access to a content item through activation of a link associated with the content item or providing access through a globally accessible shared folder.</p><p id="p-0037" num="0036">The content can be stored in content storage <b>318</b>, which is one means for performing this function. Content storage <b>318</b> can be a storage device, multiple storage devices, or a server. Alternatively, content storage <b>318</b> can be a cloud storage provider or network storage accessible via one or more communications networks. The cloud storage provider or network storage may be owned and managed by the content management system <b>100</b> or by a third party. In one configuration, content management system <b>100</b> stores the content items in the same organizational structure as they appear on the client device. However, content management system <b>100</b> can store the content items in its own order, arrangement, or hierarchy.</p><p id="p-0038" num="0037">Content storage <b>318</b> can also store metadata describing content items, content item types, and the relationship of content items to various accounts, folders, or groups. The metadata for a content item can be stored as part of the content item or can be stored separately. In one configuration, each content item stored in content storage <b>318</b> can be assigned a system-wide unique identifier.</p><p id="p-0039" num="0038">In one embodiment, content storage <b>318</b> may be a distributed system that stores data as key-value pairs in tables distributed across multiple nodes, where a node may be a system or a device (such as a computer or a server) that stores a portion of the data. In one embodiment, a data table (or table) is a collection of key-value pairs (may also be referred to as entries) that are stored in one node or distributed across multiple nodes. A set of related tables may be grouped as a family of tables.</p><p id="p-0040" num="0039">Content storage <b>318</b> can decrease the amount of storage space required by identifying duplicate files or duplicate segments of files. Instead of storing multiple copies of an identical content item, content storage <b>318</b> can store a single copy and then use a pointer or other mechanism to link the duplicates to the single copy. Similarly, content storage <b>318</b> stores files using a file version control mechanism that tracks changes to files, different versions of files (such as a diverging version tree), and a change history. The change history can include a set of changes that, when applied to the original file version, produces the changed file version.</p><p id="p-0041" num="0040">Content storage <b>318</b> may further decrease the amount of storage space required by deleting content items based on expiration time of the content items. An expiration time for a content item may indicate that the content item is no longer needed after the expiration time and may therefore be deleted. Content storage <b>318</b> may periodically scan through the content items and compare expiration time with current time. If the expiration time of a content item is earlier than the current time, content storage <b>318</b> may delete the content item from content storage <b>318</b>.</p><p id="p-0042" num="0041">Content management system <b>100</b> automatically synchronizes content from one or more client devices, using synchronization module <b>312</b>, which is one means for performing this function. The synchronization is platform agnostic. That is, the content is synchronized across multiple client devices <b>120</b> of varying type, capabilities, operating systems, etc. For example, client application <b>200</b> synchronizes, via synchronization module <b>312</b> at content management system <b>100</b>, content in client device <b>120</b>'s file system with the content in an associated user account on system <b>100</b>. Client application <b>200</b> synchronizes any changes to content in a designated folder and its sub-folders with the synchronization module <b>312</b>. Such changes include new, deleted, modified, copied, or moved files or folders. Synchronization module <b>312</b> also provides any changes to content associated with client device <b>120</b> to client application <b>200</b>. This synchronizes the local content at client device <b>120</b> with the content items at content management system <b>100</b>.</p><p id="p-0043" num="0042">Conflict management module <b>314</b> determines whether there are any discrepancies between versions of a content item located at different client devices <b>120</b>. For example, when a content item is modified at one client device and a second client device, differing versions of the content item may exist at each client device. Synchronization module <b>312</b> determines such versioning conflicts, for example by identifying the modification time of the content item modifications. Conflict management module <b>314</b> resolves the conflict between versions by any suitable means, such as by merging the versions, or by notifying the client device of the later-submitted version.</p><p id="p-0044" num="0043">A user can also view or manipulate content via a web interface generated by user interface module <b>302</b>. For example, the user can navigate in web browser <b>260</b> to a web address provided by content management system <b>100</b>. Changes or updates to content in content storage <b>318</b> made through the web interface, such as uploading a new version of a file, are synchronized back to other client devices <b>120</b> associated with the user's account. Multiple client devices <b>120</b> may be associated with a single account and files in the account are synchronized between each of the multiple client devices <b>120</b>.</p><p id="p-0045" num="0044">Content management system <b>100</b> includes communications interface <b>300</b> for interfacing with various client devices <b>120</b>, and with other content and/or service providers via an Application Programming Interface (API), which is one means for performing this function. Certain software applications access content storage <b>318</b> via an API on behalf of a user. For example, a software package, such as an app on a smartphone or tablet computing device, can programmatically make calls directly to content management system <b>100</b>, when a user provides credentials, to read, write, create, delete, share, or otherwise manipulate content. Similarly, the API can allow users to access all or part of content storage <b>318</b> through a web site.</p><p id="p-0046" num="0045">Content management system <b>100</b> can also include authenticator module <b>306</b>, which verifies user credentials, security tokens, API calls, specific client devices, etc., to determine whether access to requested content items is authorized, and is one means for performing this function. Authenticator module <b>306</b> can generate one-time use authentication tokens for a user account. Authenticator module <b>306</b> assigns an expiration period or date to each authentication token. In addition to sending the authentication tokens to requesting client devices, authenticator module <b>306</b> can store generated authentication tokens in authentication token database <b>320</b>. After receiving a request to validate an authentication token, authenticator module <b>306</b> checks authentication token database <b>320</b> for a matching authentication token assigned to the user. Once the authenticator module <b>306</b> identifies a matching authentication token, authenticator module <b>306</b> determines if the matching authentication token is still valid. For example, authenticator module <b>306</b> verifies that the authentication token has not expired or was not marked as used or invalid. After validating an authentication token, authenticator module <b>306</b> may invalidate the matching authentication token, such as a single-use token. For example, authenticator module <b>306</b> can mark the matching authentication token as used or invalid, or delete the matching authentication token from authentication token database <b>320</b>.</p><p id="p-0047" num="0046">In some embodiments, content management system <b>100</b> includes a content item management module <b>308</b> for maintaining a content directory that identifies the location of each content item in content storage <b>318</b>, and allows client applications to request access to content items in the storage <b>318</b>, and which is one means for performing this function. A content entry in the content directory can also include a content pointer that identifies the location of the content item in content storage <b>318</b>. For example, the content entry can include a content pointer designating the storage address of the content item in memory. In some embodiments, the content entry includes multiple content pointers that point to multiple locations, each of which contains a portion of the content item.</p><p id="p-0048" num="0047">In addition to a content path and content pointer, a content entry in some configurations also includes user account identifier that identifies the user account that has access to the content item. In some embodiments, multiple user account identifiers can be associated with a single content entry indicating that the content item has shared access by the multiple user accounts.</p><p id="p-0049" num="0048">In some embodiments, the content management system <b>100</b> can include a mail server module <b>322</b>. The mail server module <b>322</b> can send (and receive) collaborative content items to (and from) other client devices using the collaborative content management system <b>100</b>. The mail server module can also be used to send and receive messages between users in the content management system.</p><p id="p-0050" num="0049">Content management system <b>100</b> additionally includes auto-complete module <b>324</b>. Auto-complete module <b>324</b> suggests subsequent texts to a user typing into content items based on a meeting transcript (e.g., of a meeting the user is attending). The auto-complete module <b>324</b> is described in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Although described herein with reference to content items, the techniques put forth herein also apply to collaborative content items.</p><p id="p-0051" num="0050">Collaborative Content Management System</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a block diagram of the collaborative content management system <b>130</b>, according to one embodiment. Collaborative content items can be files that users can create and edit using a collaborative content items editor <b>270</b> and can contain collaborative content item elements. Collaborative content item elements may include any type of content such as text; images, animations, videos, audio, or other multi-media; tables; lists; references to external content; programming code; tasks; tags or labels; comments; or any other type of content. Collaborative content item elements can be associated with an author identifier, attributes, interaction information, comments, sharing users, etc. Collaborative content item elements can be stored as database entities, which allows for searching and retrieving the collaborative content items. As with other types of content items, collaborative content items may be shared and synchronized with multiple users and client devices <b>120</b>, using sharing <b>310</b> and synchronization <b>312</b> modules of content management system <b>100</b>. Users operate client devices <b>120</b> to create and edit collaborative content items, and to share collaborative content items with other users of client devices <b>120</b>. Changes to a collaborative content item by one client device <b>120</b> are propagated to other client devices <b>120</b> of users associated with that collaborative content item.</p><p id="p-0053" num="0052">In the embodiment of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, collaborative content management system <b>130</b> is shown as separate from content management system <b>100</b> and can communicate with it to obtain its services. In other embodiments, collaborative content management system <b>130</b> is a subsystem of the component of content management system <b>100</b> that provides sharing and collaborative services for various types of content items. User account database <b>316</b> and authentication token database <b>320</b> from content management system <b>100</b> are used for accessing collaborative content management system <b>130</b> described herein.</p><p id="p-0054" num="0053">Collaborative content management system <b>130</b> can include various servers for managing access and edits to collaborative content items and for managing notifications about certain changes made to collaborative content items. Collaborative content management system <b>130</b> can include proxy server <b>402</b>, collaborative content item editor <b>404</b>, backend server <b>406</b>, and collaborative content item database <b>408</b>, access link module <b>410</b>, copy generator <b>412</b>, collaborative content item differentiator <b>414</b>, settings module <b>416</b>, metadata module <b>418</b>, revision module <b>420</b>, notification server <b>422</b>, and notification database <b>424</b>. Proxy server <b>402</b> handles requests from client applications <b>200</b> and passes those requests to the collaborative content item editor <b>404</b>. Collaborative content item editor <b>404</b> manages application level requests for client applications <b>200</b> for editing and creating collaborative content items, and selectively interacts with backend servers <b>406</b> for processing lower level processing tasks on collaborative content items, and interfacing with collaborative content items database <b>408</b> as needed. Collaborative content items database <b>408</b> contains a plurality of database objects representing collaborative content items, comment threads, and comments. Each of the database objects can be associated with a content pointer indicating the location of each object within the CCI database <b>408</b>. Notification server <b>422</b> detects actions performed on collaborative content items that trigger notifications, creates notifications in notification database <b>424</b>, and sends notifications to client devices.</p><p id="p-0055" num="0054">Client application <b>200</b> sends a request relating to a collaborative content item to proxy server <b>402</b>. Generally, a request indicates the userID (&#x201c;UID&#x201d;) of the user, and the collaborativeContentItemID (&#x201c;NID&#x201d;) of the collaborative content item, and additional contextual information as appropriate, such as the text of the collaborative content item. When proxy server <b>402</b> receives the request, the proxy server <b>402</b> passes the request to the collaborative content item editor <b>404</b>. Proxy server <b>402</b> also returns a reference to the identified collaborative content items proxy server <b>402</b> to client application <b>200</b>, so the client application can directly communicate with the collaborative content item editor <b>404</b> for future requests. In an alternative embodiment, client application <b>200</b> initially communicates directly with a specific collaborative content item editor <b>404</b> assigned to the userID.</p><p id="p-0056" num="0055">When collaborative content item editor <b>404</b> receives a request, it determines whether the request can be executed directly or by a backend server <b>406</b>. When the request adds, edits, or otherwise modifies a collaborative content item the request is handled by the collaborative content item editor <b>404</b>. If the request is directed to a database or index inquiry, the request is executed by a backend server <b>406</b>. For example, a request from client device <b>120</b> to view a collaborative content item or obtain a list of collaborative content items responsive to a search term is processed by backend server <b>406</b>.</p><p id="p-0057" num="0056">The access module <b>410</b> receives a request to provide a collaborative content item to a client device. In one embodiment, the access module generates an access link to the collaborative content item, for instance in response to a request to share the collaborative content item by an author. The access link can be a hyperlink including or associated with the identification information of the CCI (i.e., unique identifier, content pointer, etc.). The hyperlink can also include any type of relevant metadata within the content management system (i.e., author, recipient, time created, etc.). In one embodiment, the access module can also provide the access link to user accounts via the network <b>110</b>, while in other embodiments the access link can be provided or made accessible to a user account and is accessed through a user account via the client device. In one embodiment, the access link will be a hyperlink to a landing page (e.g., a webpage, a digital store front, an application login, etc.) and activating the hyperlink opens the landing page on a client device. The landing page can allow client devices not associated with a user account to create a user account and access the collaborative content item using the identification information associated with the access link. Additionally, the access link module can insert metadata into the collaborative content item, associate metadata with the collaborative content item, or access metadata associated with the collaborative content item that is requested.</p><p id="p-0058" num="0057">The access module <b>410</b> can also provide collaborative content items via other methods. For example, the access module <b>410</b> can directly send a collaborative content item to a client device or user account, store a collaborative content item in a database accessible to the client device, interact with any module of the collaborative content management system to provide modified versions of collaborative content items (e.g., the copy generator <b>412</b>, the CCI differentiator <b>414</b>, etc.), sending content pointer associated with the collaborative content item, sending metadata associated with the collaborative content item, or any other method of providing collaborative content items between devices in the network. The access module can also provide collaborative content items via a search of the collaborative content item database (i.e., search by a keyword associated with the collaborative content item, the title, or a metadata tag, etc.).</p><p id="p-0059" num="0058">The copy generator <b>412</b> can duplicate a collaborative content item. Generally, the copy generator duplicates a collaborative content item when a client device selects an access link associated with the collaborative content item. The copy generator <b>412</b> accesses the collaborative content item associated with the access link and creates a derivative copy of the collaborative content item for every request received. The copy generator <b>412</b> stores each derivative copy of the collaborative content item in the collaborative content item database <b>408</b>. Generally, each copy of the collaborative content item that is generated by the copy generator <b>412</b> is associated with both the client device from which the request was received and the user account associated with the client device requesting the copy. When the copy of the collaborative content item is generated it can create a new unique identifier and content pointer for the copy of the collaborative content item. Additionally, the copy generator <b>412</b> can insert metadata into the collaborative content item, associate metadata with the copied collaborative content item, or access metadata associated with the collaborative content item that was requested to be copied.</p><p id="p-0060" num="0059">The collaborative content item differentiator <b>414</b> determines the difference between two collaborative content items. In one embodiment, the collaborative content item differentiator <b>414</b> determines the difference between two collaborative content items when a client device selects an access hyperlink and accesses a collaborative content item that the client device has previously used the copy generator <b>412</b> to create a derivative copy. The content item differentiator can indicate the differences between the content elements of the compared collaborative content items. The collaborative content item differentiator <b>414</b> can create a collaborative content item that includes the differences between the two collaborative content items, i.e. a differential collaborative content item. In some embodiments, the collaborative content item differentiator provides the differential collaborative content item to a requesting client device <b>120</b>. The differentiator <b>414</b> can store the differential collaborative content item in the collaborative content item database <b>408</b> and generate identification information for the differential collaborative content item. Additionally, the differentiator <b>414</b> can insert metadata into the accessed and created collaborative content items, associate metadata with the accessed and created collaborative content item, or access metadata associated with the collaborative content items that were requested to be differentiated.</p><p id="p-0061" num="0060">The settings and security module <b>416</b> can manage security during interactions between client devices <b>120</b>, the content management system <b>100</b>, and the collaborative content management system <b>130</b>. Additionally, the settings and security module <b>416</b> can manage security during interactions between modules of the collaborative content management system. For example, when a client device <b>120</b> attempts to interact within any module of the collaborative content management system <b>100</b>, the settings and security module <b>416</b> can manage the interaction by limiting or disallowing the interaction. Similarly, the settings and security module <b>416</b> can limit or disallow interactions between modules of the collaborative content management system <b>130</b>. Generally, the settings and security module <b>416</b> accesses metadata associated with the modules, systems <b>100</b> and <b>130</b>, devices <b>120</b>, user accounts, and collaborative content items to determine the security actions to take. Security actions can include: requiring authentication of client devices <b>120</b> and user accounts, requiring passwords for content items, removing metadata from collaborative content items, preventing collaborative content items from being edited, revised, saved or copied, or any other security similar security action. Additionally, settings and security module can access, add, edit or delete any type of metadata associated with any element of content management system <b>100</b>, collaborative content management system <b>130</b>, client devices <b>120</b>, or collaborative content items.</p><p id="p-0062" num="0061">The metadata module <b>418</b> manages metadata within with the collaborative content management system. Generally, metadata can take three forms within the collaborative content management system: internal metadata, external metadata, and device metadata. Internal metadata is metadata within a collaborative content item, external metadata is metadata associated with a CCI but not included or stored within the CCI itself, and device metadata is associated with client devices. At any point the metadata module can manage metadata by changing, adding, or removing metadata.</p><p id="p-0063" num="0062">Some examples of internal metadata can be: identifying information within collaborative content items (e.g., email addresses, names, addresses, phone numbers, social security numbers, account or credit card numbers, etc.); metadata associated with content elements (e.g., location, time created, content element type; content element size; content element duration, etc.); comments associated with content elements (e.g., a comment giving the definition of a word in a collaborative content item and its attribution to the user account that made the comment); or any other metadata that can be contained within a collaborative content item.</p><p id="p-0064" num="0063">Some examples of external metadata can be: content tags indicating categories for the metadata; user accounts associated with a CCI (e.g., author user account, editing user account, accessing user account etc.); historical information (e.g., previous versions, access times, edit times, author times, etc.); security settings; identifying information (e.g., unique identifier, content pointer); collaborative content management system <b>130</b> settings; user account settings; or any other metadata that can be associated with the collaborative content item.</p><p id="p-0065" num="0064">Some examples of device metadata can be: device type; device connectivity; device size; device functionality; device sound and display settings; device location; user accounts associated with the device; device security settings; or any other type of metadata that can be associated with a client device <b>120</b>.</p><p id="p-0066" num="0065">The collaborative content item revision module <b>420</b> manages application level requests for client applications <b>200</b> for revising differential collaborative content items and selectively interacts with backend servers <b>406</b> for processing lower level processing tasks on collaborative content items, and interfacing with collaborative content items database <b>408</b> as needed. The revision module can create a revised collaborative content item that is some combination of the content elements from the differential collaborative content item. The revision module <b>420</b> can store the revised collaborative content item in the collaborative content item database or provide the revised collaborative content item to a client device <b>120</b>. Additionally, the revision module <b>420</b> can insert metadata into the accessed and created collaborative content items, associate metadata with the accessed and created collaborative content item, or access metadata associated with the collaborative content items that were requested to be differentiated.</p><p id="p-0067" num="0066">Content management system <b>100</b> and collaborative content management system <b>130</b> may be implemented using a single computer, or a network of computers, including cloud-based computer implementations. The operations of content management system <b>100</b> and collaborative content management system <b>130</b> as described herein can be controlled through either hardware or through computer programs installed in computer storage and executed by the processors of such server to perform the functions described herein. These systems include other hardware elements necessary for the operations described here, including network interfaces and protocols, input devices for data entry, and output devices for display, printing, or other presentations of data, but which are not described herein. Similarly, conventional elements, such as firewalls, load balancers, collaborative content items servers, failover servers, network management tools and so forth are not shown so as not to obscure the features of the system. Finally, the functions and operations of content management system <b>100</b> and collaborative content management system <b>130</b> are sufficiently complex as to require implementation on a computer system, and cannot be performed in the human mind simply by mental steps.</p><p id="p-0068" num="0067">Content Item Management Module</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a block diagram of an auto-complete module <b>324</b> in a content management system, according to one example embodiment. The auto-complete module <b>324</b> generates suggested subsequent texts for detected text in content items. The auto-complete module may generate a suggested subsequent text for detected text from one user, or may generate different suggested subsequent texts in parallel for a plurality of users, either for the same content item or for different content items. The auto-complete module <b>324</b> includes a transcription module <b>502</b>, a text analysis module <b>504</b>, an auxiliary signal module <b>506</b>, a heuristics module <b>508</b>, a machine learning module <b>510</b>, a postprocessing module <b>512</b>, and a training database <b>514</b>.</p><p id="p-0070" num="0069">The transcription module <b>502</b> orchestrates transcription of one or more audio streams as words are uttered in the audio streams. The transcription module <b>502</b> sends the one or more audio streams to a transcription engine and receives one or more respective transcripts from the transcription engine. Each received transcript is a portion of a full transcription of the audio stream, e.g., a transcription of a particular quantity of the audio stream, such as audio data from a most recent one second time period. Each transcript may include timestamps indicating times at which each word in the transcript was uttered, and can include indications of which user spoke (which may be determined based off the client device <b>120</b> from which the audio data including the utterance was received).</p><p id="p-0071" num="0070">The transcription engine is either a local system upon the content management system <b>100</b> or a remote entity to which the one or more audio streams are sent, e.g., to an endpoint exposed by an application programming interface (API) of the remote entity. Alternatively, a hybrid approach involving a local system and a remote entity may be employed. The transcription module <b>502</b> passes the one or more transcripts to the text analysis module <b>504</b> either as they are received or periodically.</p><p id="p-0072" num="0071">The text analysis module <b>504</b> indexes the transcripts for referencing against detected text. The index may be a Markov model constructed by the text analysis module <b>504</b> and updated as additional transcripts are received from the transcription module <b>502</b>. Alternatively, the index may be another data structure. In an embodiment, the text analysis module <b>504</b> may divide the text in the index according to one or more factors, such as a change of speaker, a gap in time between utterances, a maximum number of words for one division of text in the index, or so on. In an embodiment, the index is a single-level map, where all of the keys are pairs of words. Alternative embodiments may include a prefix tree with variable length prefixes (e.g., detected text).</p><p id="p-0073" num="0072">The text analysis module <b>504</b> also receives detected text from a content item editor <b>270</b> or detects text in an active content item at the content item editor <b>270</b> that matches one or more auto-complete parameters. The text analysis module <b>504</b> checks text from the content item editor <b>270</b> against the one or more auto-complete parameters to determine whether the text qualifies for transcription-based autocomplete. For example, the auto-complete parameter may be that the detected text includes a predefined number of (e.g., pair of) adjacent words typed within a threshold amount of time of entry from one another. The one or more auto-complete parameters may alternatively or additionally include a minimum number of characters in the detected text (e.g., a minimum of four characters). The auto-complete parameter being that the detected text includes a pair of adjacent words may be beneficial in that it typically leads to a plurality of candidate suggested subsequent texts without leading to many irrelevant candidate suggested subsequent texts.</p><p id="p-0074" num="0073">The text analysis module <b>504</b> matches the detected text to text in the index, e.g., performs a string match function upon the index using the detected text as an argument. Alternatively, the text analysis module <b>504</b> checks the detected text against the index in chunks via a sliding window. Each chunk is a number of characters or words against which the detected text is checked by the text analysis module <b>504</b> at one time to determine whether the detected text matches the chunk. Depending upon the embodiment, the text analysis module <b>504</b> may check the detected text against a chunk by comparing the detected text and the chunk for an exact match or for a partial match, or by comparing the detected text and/or one or more synonyms and/or abbreviations of some or all of the detected text to the chunk and/or one or more synonyms and/or abbreviations of some or all of the text in the chunk. The text analysis module <b>504</b> may fetch the synonyms and/or abbreviations by referencing the detected text and/or chunk against a reference document, such as a thesaurus or an abbreviations sheet. One or more reference documents may be stored by the content management system <b>100</b>, e.g., at the content storage <b>318</b>. If the detected text matches the text of the chunk in the sliding window, the text analysis module <b>504</b> generates a candidate suggested subsequent text based off text in the index near the sliding window, e.g., the rest of the sentence including the text in the sliding window.</p><p id="p-0075" num="0074">The auxiliary signal module <b>506</b> fetches auxiliary data from one or more locations, e.g., a client device <b>120</b> of the user that typed the detected text, the content storage <b>318</b>, the collaborative content item database <b>408</b>, and/or training database <b>514</b>. As described below, auxiliary data can provide additional discriminating features for a machine-learning model <b>511</b> of the machine learning module <b>510</b>. Depending upon the embodiment, auxiliary data can include a representation of a global notetaking style, a representation of a notetaking style of the user, a type of meeting, an agenda for the meeting, a document related to the meeting, metadata of the meeting, user names from a meeting invitation and respective roles within an organization, results of a prior suggested subsequent text, a past document generated by the user, a past meeting transcription and a respective content item, a dictionary of common phrases, a manually labeled portion of a meeting transcript, a manually labeled portion of a content item, and/or other data from the client device <b>120</b>, content management system <b>100</b>, and/or collaborative content management system <b>130</b>. The auxiliary data may include an identifier of the user that entered the detected text into the content item.</p><p id="p-0076" num="0075">The representation of the global notetaking style may be a document in the content storage <b>318</b> detailing a notetaking ruleset. Alternatively, the representation of the global notetaking style may be a feature vector generated by a machine-learning model <b>511</b> to represent a notetaking style of members of an organization (e.g., a plurality of users attending a meeting or employees of an organization). Similarly, the representation of the notetaking style of the user may be a document in the content storage <b>318</b> detailing a notetaking ruleset for the user or a feature vector generated by a machine-learning model <b>511</b> to represent a notetaking style of the user.</p><p id="p-0077" num="0076">The type of meeting may be one of a plurality of meeting types, such as an internal meeting or an external meeting. The agenda for the meeting and/or the document related to the meeting may be a document associated with a meeting whose audio is being transcribed, e.g., a document included in a meeting invitation received by the user and/or sharing a title with the meeting. The metadata of the meeting may be metadata of the meeting invitation, such as a number of invitees, a length of the meeting, a title of the meeting, a time of the meeting, and a date of the meeting. The dictionary of common phrases may be a document stored by the content storage <b>318</b>, such as a document providing key-value pairs of phrases and their synonyms and/or definitions.</p><p id="p-0078" num="0077">The auxiliary signal module <b>506</b> may determine that one or more auxiliary data are relevant to a particular instance of transcript-based auto-complete by evaluating relationships among the auxiliary data and the particular instance of transcript-based auto-complete. For example, the auxiliary signal module <b>506</b> may determine that a meeting is in progress based on the user's calendar and scrape related auxiliary data, such as documents attached to the meeting invitation, or by searching the content storage <b>318</b>, the collaborative content item database <b>408</b>, and/or training database <b>514</b> for documents to which the user has access that include text that matches text from the calendar item, such as a document that shares at least part of a title with the calendar item. The auxiliary signal module <b>506</b> may determine that a document was shared with at least a predefined threshold of attendees of a meeting, and thereby identify the document as auxiliary data relevant to an instance of transcript-based auto-complete during the meeting by a user attending the meeting. The auxiliary signal module <b>506</b> may additionally identify as auxiliary data one or more emails exchanged among attendees to a meeting during which an instance of transcript-based auto-complete is to be performed.</p><p id="p-0079" num="0078">In an embodiment, the auxiliary signal module <b>506</b> identifies a current speaker (e.g., a user that most recently uttered) and generates a tag (e.g., an at-mention) for the current speaker as part of the suggested subsequent text. In an embodiment, the transcript includes, for each portion of text, an indication of a speaker that uttered the words of the text. The auxiliary signal module <b>506</b> may search a staff directory or other document that identifies users (e.g., such a document stored at the content storage <b>318</b>) for a user that matches the identity of the current speaker as indicated by the transcript. The auxiliary signal module <b>506</b> then scrapes from the document contact information for the current speaker, which the auxiliary signal module <b>506</b> uses to generate the tag.</p><p id="p-0080" num="0079">In an embodiment, the auxiliary data includes an importance signal received from one or more users. The importance signal is an input to the machine learning model that distinguishes a portion of the transcript as particularly important. For example, the importance signal may be a visual indication of importance and/or an audio indication of importance, such as a key word or a particular hand motion, that indicates recent utterances (e.g., utterances from the past ten seconds) in an audio stream are important. This type of auxiliary data can be used to increase weighting in the machine learning model for the respective recent portion of the transcript. For example, the importance signal can be provided as a feature as part of the auxiliary data input to the machine-learning model <b>511</b>. The importance signal may alternatively be a particular keystroke, or receipt of user selection of an importance signal widget (e.g., a button) in a user interface of the content item editor <b>270</b>.</p><p id="p-0081" num="0080">In an embodiment, the auto-complete module <b>324</b> includes the heuristics module <b>508</b> and not the machine learning module <b>510</b>. The heuristics module <b>508</b> identifies a suggested subsequent text by evaluating the candidate suggested subsequent texts using one or more heuristics.</p><p id="p-0082" num="0081">In an embodiment, the auto-complete module <b>324</b> includes the machine learning module <b>510</b> and not the heuristics module <b>508</b>. The machine learning module <b>510</b> includes one or more trained machine-learning models <b>511</b>. The machine learning module <b>510</b> receives one or more candidate suggested subsequent texts and scores them each, using the machine-learning model <b>511</b>, for potential provision as a suggested subsequent text. The machine learning module <b>510</b> sends one or more candidate suggested subsequent texts and, in some embodiments, respective scores to the postprocessing module <b>512</b>. The machine learning module <b>510</b> may discard one or more candidate suggested subsequent texts, such as candidate suggested subsequent texts with scores below a threshold score, before sending the remaining candidate suggested subsequent texts to the postprocessing module <b>512</b>.</p><p id="p-0083" num="0082">The machine learning module <b>510</b> can train the machine-learning model <b>511</b>, and can train multiple machine-learning models <b>511</b>, e.g., one for each user of an organization using the content management system <b>100</b> and/or collaborative content management system <b>130</b>. User-specific machine-learning models <b>511</b> can be used to identify suggested subsequent texts that are particular to the user, e.g., that relate to features of the user such as a role within an organization. The machine learning module <b>510</b> trains a machine-learning model <b>511</b> on data from the training database <b>514</b>, which is described below.</p><p id="p-0084" num="0083">The machine-learning model <b>511</b> can be any of a variety of machine-learning models, such as a neural network (e.g., a convolutional neural network). The machine-learning model <b>511</b> takes as input one or more transcripts (and/or portions thereof) and/or candidate suggested subsequent texts identified by the text analysis module <b>504</b>, a detected text, and, in some embodiments, auxiliary data, and outputs scores for each of the candidate suggested subsequent texts.</p><p id="p-0085" num="0084">The training database <b>514</b> is a data store that includes data used to train the machine-learning model <b>511</b>. The training database <b>514</b> may store transcripts (e.g., a past meeting transcription), respective content items (e.g., a past document generated by the user), respective suggested subsequent texts, and records of whether those suggested subsequent texts were incorporated into the content item, which can be used to further train the machine-learning model <b>511</b> over time as the data is accumulated. Text typed into a content item by a user can be weighted more highly than utterances from the transcript that were not typed into the content item because text typed by a user indicates it is text the user would have incorporated into the content item if it were suggested.</p><p id="p-0086" num="0085">Depending upon the embodiment, the training database <b>514</b> may additionally or alternatively include auxiliary data used to train the machine-learning model <b>511</b>, and can be used to further train the machine-learning model <b>511</b> over time as auxiliary data is accumulated. Training data in the training database <b>514</b> may be labeled manually or automatically, with positive and/or negative labels, for use in training machine-learning models <b>511</b> to discriminate candidate suggested subsequent texts based on a likelihood that the user will apply the candidate suggested subsequent text to the content item. The training database <b>514</b> can be one data store or a distributed set of data stores.</p><p id="p-0087" num="0086">The postprocessing module <b>512</b> ranks the candidate suggested subsequent texts by their scores and sends one or more to the content item editor <b>270</b>. The postprocessing module <b>514</b> may adjust scores according to one or more bias factors before ranking the candidate suggested subsequent texts (alternatively or additionally, these biases may be incorporated into the machine-learning model <b>511</b> by assigning or adjusting one or more weights). For example, the postprocessing module <b>514</b> may bias towards (e.g., increase the score for suggested subsequent texts that are based on) utterances from recent speakers and/or against (e.g., decrease the score for suggested subsequent texts that are based on) utterances by the user from whom the detected text originated. The postprocessing module <b>514</b> may bias towards utterances corresponding to importance signals. The postprocessing module <b>514</b> may bias towards a portion of the transcript corresponding to a timestamp where at least a predefined threshold of users (e.g., two users) input the same text into the content item close to the time of the timestamp and within a threshold amount of time of one another (e.g., ten seconds). The bias may be increased proportional to the number of users that have input the same text into the content item close to the time of the timestamp and within the threshold amount of time of one another. The postprocessing module <b>512</b> may bias towards suggested subsequent texts with timestamps closer to a current time more than suggested subsequent texts with timestamps less close to the current time. For example, a suggested subsequent text corresponding to an utterance timestamped five seconds in the past may be assigned a greater weight than a suggested subsequent text corresponding to an utterance timestamped ten minutes in the past.</p><p id="p-0088" num="0087">The postprocessing module <b>514</b> may identify a highest ranked candidate suggested subsequent text as the suggested subsequent text that it sends to the content item editor <b>270</b>. Alternatively, the postprocessing module <b>514</b> may send some or all of the candidate suggested subsequent texts to the content item editor <b>270</b> for display (e.g., ordered in accordance with their ranking), where the user can select one for insertion into the content item. The one or more suggested subsequent texts displayed to the user may or may not be visible to other users accessing the content item, depending upon the embodiment. The content item editor <b>270</b> may send a notification to the machine learning module <b>510</b> identifying whether the user accepted the suggested subsequent text for insertion into the content item.</p><p id="p-0089" num="0088">In an embodiment, users editing disparate documents may opt into receiving suggested subsequent texts based on one or more of the disparate documents being edited by the other users. The auto-complete module <b>324</b> may send a user editing a document an offer to receive suggested subsequent texts based on other documents being edited by other users. If the user accepts the offer, the auto-complete module <b>324</b> may apply the other documents as auxiliary data to the machine-learning model <b>511</b>.</p><p id="p-0090" num="0089">Exemplary Illustration of Suggested Text</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIGS. <b>6</b>A-<b>6</b>C</figref> show exemplary user interfaces for a content item with transcript-based auto-complete, according to one example embodiment. In <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, a user interface <b>600</b>A is displayed, e.g., a user interface provided by the user interface module <b>302</b> via the communication interface <b>300</b>. The user interface <b>600</b>A includes a representation of a text document including text <b>602</b> and recent text <b>604</b>, e.g., a text document of the content item editor <b>270</b> or collaborative content item editor <b>404</b>. The recent text <b>604</b> is text entered by a user into the text document that satisfies an auto-complete parameter. As such, the content management system <b>100</b> generates a suggested subsequent text for the recent text <b>604</b>.</p><p id="p-0092" num="0091">In <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, a user interface <b>600</b>B is displayed, particularly at a time when suggested subsequent text <b>606</b>A is presented in association with the recent text <b>604</b>. The user interface <b>600</b>B still includes the text <b>602</b>. The suggested subsequent text <b>606</b>A is visually distinguished from text <b>602</b> and recent text <b>604</b> entered by the user. In the example of the figure, the suggested subsequent text <b>606</b>A is displayed in a different font color than text <b>602</b> and recent text <b>604</b>; in alternative embodiments, one or more alternative or additional visual distinctions may be employed without departing from the techniques put forth herein.</p><p id="p-0093" num="0092">In <figref idref="DRAWINGS">FIG. <b>6</b>C</figref>, a user interface <b>600</b>C is displayed, particularly at a time after the user has accepted the suggested subsequent text. The user interface <b>600</b>C still includes text <b>602</b> and recent text <b>604</b>. The user interface <b>600</b>C additionally includes subsequent text <b>606</b>B, which is input to the text document subsequent to the recent text <b>604</b> upon user acceptance of the suggested subsequent text <b>606</b>A, e.g., via a particular user input from a user input device to a computer displaying the user interfaces <b>600</b>. In an embodiment, the particular user input is a keystroke, e.g., a tab or enter keystroke.</p><p id="p-0094" num="0093">Exemplary Process Flows</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a data flow diagram for transcript-based auto-complete, according to one example embodiment. The transcription module <b>502</b> receives stream of data forming a transcript <b>702</b> of audio data as it is uttered (e.g., after an utterance in the audio data, the transcription module <b>502</b> receives a transcription of the utterance). The transcription module <b>502</b> receives the stream of data from a transcription engine, which may be a component of the content management system <b>100</b> and/or collaborative content management system <b>130</b>. Alternatively, the transcription engine may be a third-party service, e.g., a software application at a remote server accessible via an application programming interface (API).</p><p id="p-0096" num="0095">The transcription module <b>502</b> sends <b>712</b> the transcript <b>702</b> to the text analysis module <b>504</b>, which indexes the transcript <b>702</b> for referencing against detected text. The index <b>718</b> may be a Markov model constructed by the text analysis module <b>504</b> and updated as additional portions of the transcript <b>702</b> are received from the transcription module <b>502</b>. In the example of the figure, a detected text <b>704</b> &#x201c;summer campaign be&#x201d; satisfies an auto-complete parameter. As such, the text analysis module <b>504</b> matches the detected text to portions <b>720</b> of the transcript <b>702</b>. The portions <b>720</b> include portion <b>720</b>A, &#x201c;begins Jun. 21, 2021,&#x201d; and portion <b>720</b>B, &#x201c;best if we discuss this now.&#x201d; Some or all of the portions <b>720</b> are sent by the text analysis module <b>504</b> to the machine learning module <b>510</b>. In an embodiment, only portions <b>720</b> uttered less than a threshold time ago are sent to the machine learning module <b>510</b>. In this manner, portions <b>720</b> unlikely to be valid subsequent texts are ignored, improving the efficiency of the system by reducing the data to be analyzed.</p><p id="p-0097" num="0096">The detected text <b>704</b> is sent <b>714</b> from the content item editor <b>270</b> to the text analysis module <b>504</b>. For example, the content item editor <b>270</b> may include an event listener/handler that listens for text entry and sends text as it is typed to the text analysis module <b>504</b>, or, in an embodiment, only text matching the auto-complete parameter. Alternatively, the content item editor <b>270</b> may stream text to the text analysis module <b>504</b>, or the text analysis module <b>504</b> may periodically fetch text from the content item editor <b>270</b>, e.g., via an endpoint exposed via an API of the content item editor <b>270</b>. In an embodiment, the content item editor <b>270</b> only sends <b>714</b> a portion of the detected text <b>704</b> to the content item editor <b>270</b>.</p><p id="p-0098" num="0097">The text analysis module <b>504</b> sends <b>722</b> the portions <b>720</b> to the machine learning module <b>510</b>. The machine learning module <b>510</b> includes a trained machine-learning model <b>724</b>. The machine learning module <b>510</b> applies the machine-learning model <b>724</b> to the portions <b>720</b> to produce a suggested subsequent text or scores for each of the portions <b>720</b>. In embodiments where the machine learning module <b>510</b> does not directly identify a suggested subsequent text, the machine learning module <b>510</b> sends <b>726</b> the portions <b>720</b> and respective scores to the postprocessing module <b>512</b>.</p><p id="p-0099" num="0098">In an embodiment, the machine learning module <b>510</b> also receives <b>716</b> auxiliary data <b>706</b> from the auxiliary signal module <b>506</b>. The auxiliary signal module <b>506</b> fetches auxiliary data <b>706</b> from one or more locations, e.g., a client device <b>120</b> of the user that typed the detected text <b>704</b>. For example, the auxiliary data <b>706</b> may be a meeting invitation from an electronic mail application at the client device <b>120</b>, where the meeting invitation corresponds to a meeting whose audio data is being transcribed. Auxiliary data <b>706</b> can improve the accuracy of the machine-learning model <b>724</b> by providing additional discriminating features.</p><p id="p-0100" num="0099">The machine learning module <b>510</b> either identifies a suggested subsequent text, which is sent to the content item editor <b>270</b> for display adjacent to the detected text <b>704</b>, or sends the portions <b>720</b> and respective scores to the postprocessing module <b>514</b>. The postprocessing module <b>514</b> ranks the portions, e.g. portion <b>720</b>A, by their scores, e.g., score <b>729</b>, which is the score of portion <b>720</b>A. The postprocessing module <b>514</b> may adjust scores according to one or more bias factors before ranking the portions <b>720</b>. The postprocessing module <b>514</b> may identify a highest ranked portion <b>720</b> as the suggested subsequent text that it sends <b>730</b> to the content item editor <b>270</b>. For example, if portion <b>720</b>A's score <b>729</b> of 0.91 is greater than a score of portion <b>720</b>B, then portion <b>720</b>A is sent <b>730</b> by the postprocessing module <b>512</b> to the content item editor <b>270</b>. Alternatively, the postprocessing module <b>514</b> may send some or all of the portions <b>720</b> to the content item editor <b>270</b>, where the user can select one for insertion into the content item. The content item editor <b>270</b> may send a notification to the machine learning module <b>510</b> identifying whether the user accepted the suggested subsequent text for insertion into the content item, which can be used to further refine the machine learning model <b>724</b> by weighting the accepted suggested subsequent text higher than other text from the transcript and/or weighting suggested subsequent text rejected by the user lower than other text from the transcript. In this manner, the content management system <b>100</b> may improve the functioning of the auto-complete module <b>324</b> over time.</p><p id="p-0101" num="0100">In an alternative embodiment, instead of or in addition to the machine learning module <b>510</b>, the system includes a heuristics module that identifies a suggested subsequent text by evaluating the candidate suggested subsequent texts using one or more heuristics. In this embodiment, the heuristics module sends the suggested subsequent text to the content item editor <b>270</b>.</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an exemplary process for performing transcript-based auto-complete, according to one example embodiment. The content management system <b>100</b> obtains <b>802</b> (e.g., using the transcription module <b>502</b>) at least a portion of a meeting transcript (e.g., the text shown with reference sign <b>702</b>) based on an audio stream of a meeting attended by a plurality of users, the meeting transcript obtained in an ongoing manner as words are uttered during the meeting.</p><p id="p-0103" num="0102">The content management system detects <b>804</b> (e.g., using the text analysis module <b>504</b> and/or content item editor <b>270</b>), text entered by a user of the plurality of users into a content item during the meeting (e.g., the text shown with reference sign <b>704</b>). In an embodiment, the detected text is text that satisfies an auto-complete parameter. Depending upon the embodiment, there may be one or more auto-complete parameters.</p><p id="p-0104" num="0103">The content management system <b>100</b> matches <b>806</b> (e.g., using the text analysis module <b>504</b>) the detected text to at least part of the at least the portion of the meeting transcript. In one embodiment, the content management system matches <b>806</b> the detected text to at least part of the at least the portion of the meeting transcript using one or more heuristics.</p><p id="p-0105" num="0104">In a second embodiment, the content management system <b>100</b> inputs (e.g., using the text analysis module <b>504</b>) the detected text and at least part of the at least the portion of the meeting transcript associated with the detected text into a trained machine-learning model (e.g., the model shown with reference sign <b>724</b> at the machine learning module <b>510</b>). The content management system <b>100</b> may also input (e.g., using auxiliary signal module <b>506</b>) auxiliary data into the machine-learning model. Inputting into the machine-learning model may be responsive to the detected text satisfying the one or more auto-complete parameters. The content management system <b>100</b> determines (e.g., using the machine learning module <b>510</b>) one or more candidate suggested subsequent texts (e.g., the text shown with reference sign <b>720</b>A) based on output from the machine-learning model. If there is one candidate suggested subsequent text, it is the suggested subsequent text. If there are multiple candidate suggested subsequent texts, the machine-learning model may score (e.g., the number shown with reference sign <b>729</b>) each of the candidate suggested subsequent texts. The content management system <b>100</b> ranks (e.g., using the postprocessing module <b>512</b>) the candidate suggested subsequent texts based on their scores.</p><p id="p-0106" num="0105">The content management system provides <b>808</b> (e.g., using the postprocessing module <b>512</b>) the at least part of the at least the portion of the meeting transcript to the user as a suggested subsequent text. In an embodiment where there are multiple candidate suggested subsequent texts ranked by the machine-learning model, the at least part of the at least the portion of the meeting transcript is the suggested subsequent text, which may be selected (e.g., using the postprocessing module <b>512</b>) from the candidate suggested subsequent texts by user input or by having a highest score among the candidate suggested subsequent texts.</p><heading id="h-0006" level="1">ADDITIONAL CONSIDERATIONS</heading><p id="p-0107" num="0106">Reference in the specification to &#x201c;one embodiment&#x201d; or to &#x201c;an embodiment&#x201d; means that a particular feature, structure, or characteristic described in connection with the embodiments is included in at least one embodiment. The appearances of the phrase &#x201c;in one embodiment&#x201d; in various places in the specification are not necessarily all referring to the same embodiment.</p><p id="p-0108" num="0107">In this description, the term &#x201c;module&#x201d; refers to a physical computer structure of computational logic for providing the specified functionality. A module can be implemented in hardware, firmware, and/or software. In regards to software implementation of modules, it is understood by those of skill in the art that a module comprises a block of code that contains the data structure, methods, classes, header and other code objects appropriate to execute the described functionality. Depending on the specific implementation language, a module may be a package, a class, or a component. It will be understood that any computer programming language may support equivalent structures using a different terminology than &#x201c;module.&#x201d;</p><p id="p-0109" num="0108">It will be understood that the named modules described herein represent one embodiment of such modules, and other embodiments may include other modules. In addition, other embodiments may lack modules described herein and/or distribute the described functionality among the modules in a different manner. Additionally, the functionalities attributed to more than one module can be incorporated into a single module. Where the modules described herein are implemented as software, the module can be implemented as a standalone program, but can also be implemented through other means, for example as part of a larger program, as a plurality of separate programs, or as one or more statically or dynamically linked libraries. In any of these software implementations, the modules are stored on the computer readable persistent storage devices of a system, loaded into memory, and executed by the one or more processors of the system's computers.</p><p id="p-0110" num="0109">The operations herein may also be performed by an apparatus. This apparatus may be specially constructed for the required purposes, or it may comprise a general-purpose computer selectively activated or reconfigured by a computer program stored in the computer. Such a computer program may be stored in a computer readable storage medium, such as, but is not limited to, any type of disk including optical disks, CD-ROMs, read-only memories (ROMs), random access memories (RAMs), magnetic or optical cards, or any type of media suitable for storing electronic instructions, and each coupled to a computer system bus. Furthermore, the computers referred to in the specification may include a single processor or may be architectures employing multiple processor designs for increased computing capability.</p><p id="p-0111" num="0110">The algorithms presented herein are not inherently related to any particular computer or other apparatus. Various general-purpose systems may also be used with programs in accordance with the teachings herein, or it may prove convenient to construct more specialized apparatus to perform the required method steps. The required structure for a variety of these systems will appear from the description above. In addition, the present invention is not described with reference to any particular programming language. It will be appreciated that a variety of programming languages may be used to implement the teachings of the present invention as described herein, and any references above to specific languages are provided for disclosure of enablement and best mode of the present invention.</p><p id="p-0112" num="0111">While the invention has been particularly shown and described with reference to a preferred embodiment and several alternate embodiments, it will be understood by persons skilled in the relevant art that various changes in form and details can be made therein without departing from the spirit and scope of the invention.</p><p id="p-0113" num="0112">As used herein, the word &#x201c;or&#x201d; refers to any possible permutation of a set of items. Moreover, claim language reciting &#x2018;at least one of&#x2019; an element or another element refers to any possible permutation of the set of elements.</p><p id="p-0114" num="0113">Although this description includes a variety of examples and other information to explain aspects within the scope of the appended claims, no limitation of the claims should be implied based on particular features or arrangements these examples. This disclosure includes specific embodiments and implementations for illustration, but various modifications can be made without deviating from the scope of the embodiments and implementations. For example, functionality can be distributed differently or performed in components other than those identified herein. This disclosure includes the described features as non-exclusive examples of systems components, physical and logical structures, and methods within its scope.</p><p id="p-0115" num="0114">Finally, it should be noted that the language used in the specification has been principally selected for readability and instructional purposes, and may not have been selected to delineate or circumscribe the inventive subject matter. Accordingly, the disclosure of the present invention is intended to be illustrative, but not limiting, of the scope of the invention, which is set forth in the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>obtaining at least a portion of a meeting transcript based on an audio stream of a meeting attended by a plurality of users, the at least the portion of the meeting transcript obtained in an ongoing manner as words are uttered during the meeting;</claim-text><claim-text>detecting text entered by a user from the plurality of users into a content item during the meeting, the detected text satisfying an auto-complete parameter;</claim-text><claim-text>responsive to the detected text satisfying the auto-complete parameter, inputting at least a portion of the detected text and at least part of the at least the portion of the meeting transcript associated with the detected text into a trained machine-learning model;</claim-text><claim-text>determining, based on output from the trained machine-learning model, a suggested subsequent text;</claim-text><claim-text>generating for display, in visual association with the detected text, a selectable option to incorporate the suggested subsequent text into the content item; and</claim-text><claim-text>responsive to receiving a selection of the selectable option from the user, adding the suggested subsequent text to the content item.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the auto-complete parameter comprises the detected text including a predefined number of adjacent words entered into the content item within a threshold time of one another.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the trained machine-learning model generates a score for each of a plurality of candidate suggested subsequent texts, and wherein the method further comprises:<claim-text>generating for display, in visual association with the detected text, a subset of the plurality of candidate suggested subsequent texts in descending order by score, wherein candidate suggested subsequent texts of the plurality of candidate suggested subsequent texts are included in the subset of the plurality of candidate suggested subsequent texts when respective scores exceed a threshold score value; and</claim-text><claim-text>receiving selection of a candidate suggested subsequent text from the subset of the plurality of candidate suggested subsequent texts, wherein the selected candidate suggested subsequent text is the suggested subsequent text.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>wherein inputting the at least the portion of the detected text and at least part of the at least the portion of the meeting transcript associated with the detected text into a trained machine-learning model further comprises inputting auxiliary data into the trained machine-learning model; and</claim-text><claim-text>wherein the auxiliary data comprises one or more of a representation of a global notetaking style, a representation of a notetaking style of the user, a type of meeting, an agenda for the meeting, a document related to the meeting, metadata of the meeting, user names from a meeting invitation and respective roles within an organization, results of a prior suggested subsequent text, a past document generated by the user, a past meeting transcription and a respective content item, a dictionary of common phrases, a manually labeled portion of a meeting transcript, and a manually labeled portion of a content item.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the meeting transcript comprises timestamps representing when portions of the meeting transcript were uttered, wherein the at least part of the at least the portion of the meeting transcript associated with the detected text comprises a first part with a first time stamp and a second part with a second time stamp nearer to a current time than the first time stamp, and wherein the trained machine-learning model assigns the second part a higher weight than the first part based on the respective time stamps.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the meeting transcript comprises indications of which user uttered which portions of the meeting transcript, wherein the trained machine-learning model weights portions of the meeting transcript uttered by a first user with a higher weight than portions of the meeting transcript uttered by a second user based on the first user having uttered a given portion of the meeting transcript more recently than the second user, and wherein the trained machine-learning model weights portions of the meeting transcript uttered by the user that entered the detected text with a lower weight than is applied to portions of the meeting transcript uttered by other users of the plurality of users.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving, from the user, an importance signal; and</claim-text><claim-text>responsive to receiving the importance signal, increasing bias for the trained machine-learning model towards more recent utterances.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>generating different suggested subsequent texts for a plurality of users in parallel based on respective detected texts from each of the plurality of users.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving at least a portion of a second meeting transcript based on a second audio stream in parallel to the at least the portion of the meeting transcript based on the audio stream; and</claim-text><claim-text>wherein inputting the at least the portion of the detected text and at least part of the at least the portion of the meeting transcript associated with the detected text into a trained machine-learning model further comprises inputting into the trained machine-learning model the at least the portion of the second meeting transcript.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving, from at least a predefined threshold of users of the plurality of users, at least one of a visual indication of importance and an audio indication of importance; and</claim-text><claim-text>responsive to receiving, from at least the predefined threshold of users of the plurality of users, at least one of a visual indication of importance and an audio indication of importance, increasing bias for the trained machine-learned model towards a recent portion of the meeting transcript.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>receiving, from at least a predefined threshold of users of the plurality of users, the same detected text within a threshold time of one another; and</claim-text><claim-text>responsive to receiving, from at least the predefined threshold of users of the plurality of users, the same detected text within the threshold time period of one another, increasing one or more weights for the trained machine-learning model towards a portion of the meeting transcript spanning the threshold time period.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the suggested subsequent text is one of a plurality of candidate suggested subsequent texts, the method further comprising:<claim-text>identifying the plurality of candidate suggested subsequent texts, the identifying comprising matching the detected text to a plurality of portions of the meeting transcript that include text similar to the detected text.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the trained machine-learning model is trained on at least one stored meeting transcript and respective content item, wherein text in the at least one meeting transcript is timestamped to indicate a time of utterance and text in the respective content item is timestamped to indicate a time of entry to the content item.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A non-transitory computer-readable storage medium storing computer program instructions executable by at least one processor to perform operations, the instructions comprising instructions to:<claim-text>obtain at least a portion of a meeting transcript based on an audio stream of a meeting attended by a plurality of users, the at least the portion of the meeting transcript obtained in an ongoing manner as words are uttered during the meeting;</claim-text><claim-text>detect text entered by a user from the plurality of users into a content item during the meeting, the detected text satisfying an auto-complete parameter;</claim-text><claim-text>responsive to the detected text satisfying the auto-complete parameter, input at least a portion of the detected text, at least part of the at least the portion of the meeting transcript associated with the detected text, and auxiliary data corresponding to the meeting into a trained machine-learning model;</claim-text><claim-text>receive, from the trained machine-learning model, for each of a plurality of candidate suggested subsequent texts, a respective score;</claim-text><claim-text>rank the plurality of candidate suggested subsequent texts using the respective scores to identify a highest ranked candidate suggested subsequent text as a suggested subsequent text;</claim-text><claim-text>generate for display, in visual association with the detected text, a selectable option to incorporate the suggested subsequent text into the content item; and</claim-text><claim-text>responsive to receiving a selection of the selectable option from the user, add the suggested subsequent text to the content item.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the auto-complete parameter comprises the detected text including a pair of adjacent words entered into the content item within a threshold time of one another.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, the instructions further comprising instructions to:<claim-text>generate for display, in visual association with the detected text, a subset of the plurality of candidate suggested subsequent texts in descending order by score; and</claim-text><claim-text>receive selection of a candidate suggested subsequent text from the subset of the plurality of candidate suggested subsequent texts, wherein the selected candidate suggested subsequent text is the suggested subsequent text.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the auxiliary data comprises one or more of a representation of a global notetaking style, a representation of a notetaking style of the user, a type of meeting, an agenda for the meeting, a document related to the meeting, metadata of the meeting, user names from a meeting invitation and respective roles within an organization, results of a prior suggested subsequent text, a past document generated by the user, a past meeting transcription and a respective content item, a dictionary of common phrases, a manually labeled portion of a meeting transcript, and a manually labeled portion of a content item.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00014">claim 14</claim-ref>, the instructions further comprising instructions to:<claim-text>receive, from the user, an importance signal; and</claim-text><claim-text>responsive to receiving the importance signal, increase scores from the trained machine-learning model for more recent utterances.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A system comprising:<claim-text>at least one processor; and</claim-text><claim-text>a non-transitory computer-readable storage medium storing computer program instructions executable by the at least one processor, the instructions comprising instructions to:<claim-text>obtain at least a portion of a meeting transcript based on an audio stream of a meeting attended by a plurality of users, the meeting transcript obtained in an ongoing manner as words are uttered during the meeting;</claim-text><claim-text>detect text entered by a user of the plurality of users into a content item during the meeting;</claim-text><claim-text>match the detected text to at least part of the at least the portion of the meeting transcript; and</claim-text><claim-text>provide the at least part of the at least the portion of the meeting transcript to the user as a suggested subsequent text.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the detected text satisfies an auto-complete parameter comprising the detected text including a pair of adjacent words entered into the content item within a threshold time of one another.</claim-text></claim></claims></us-patent-application>