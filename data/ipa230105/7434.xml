<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007435A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007435</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940871</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>20163153.8</doc-number><date>20200313</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>7</main-group><subgroup>308</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>S</subclass><main-group>2420</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">Apparatus and Method for Rendering a Sound Scene Using Pipeline Stages</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/EP2021/056363</doc-number><date>20210312</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17940871</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>Fraunhofer-Gesellschaft zur Foerderung der angewandten Forschung e.V.</orgname><address><city>Munich</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WEFERS</last-name><first-name>Frank</first-name><address><city>Erlangen</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SCHWAER</last-name><first-name>Simon</first-name><address><city>Erlangen</city><country>DE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Apparatus for rendering a sound scene, including: a first pipeline stage including a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor; a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage including a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor; and a central controller for controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and wherein the central controller is configured to control the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="70.87mm" wi="158.75mm" file="US20230007435A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="234.70mm" wi="122.43mm" orientation="landscape" file="US20230007435A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="195.83mm" wi="117.69mm" orientation="landscape" file="US20230007435A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="200.15mm" wi="133.27mm" orientation="landscape" file="US20230007435A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="189.40mm" wi="134.54mm" orientation="landscape" file="US20230007435A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="163.15mm" wi="143.17mm" file="US20230007435A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="221.32mm" wi="146.39mm" orientation="landscape" file="US20230007435A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="221.32mm" wi="154.69mm" orientation="landscape" file="US20230007435A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="221.32mm" wi="153.75mm" orientation="landscape" file="US20230007435A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="237.49mm" wi="115.65mm" file="US20230007435A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCES TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of copending International Application No. PCT/EP2021/056363, filed Mar. 12, 2021, which is incorporated herein by reference in its entirety, and additionally claims priority from European Application No. EP 20 163 153.8, filed Mar. 13, 2020, which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0003" num="0002">The present invention relates to audio processing and, particularly, to audio signal processing of sound scenes occurring, for example, in virtual reality or augmented reality applications.</p><p id="p-0004" num="0003">Geometrical Acoustics are applied in auralization, i.e., real-time and offline audio rendering of auditory scenes and environments. This includes Virtual Reality (VR) and Augmented Reality (AR) systems like the MPEG-I 6-DoF audio renderer. For rendering complex audio scenes with six degrees of freedom (DoF), the field of Geometrical Acoustics is applied, where the propagation of sound data is modeled using methods known from optics such as ray-tracing. Particularly, the reflections at walls are modeled based on models derived from optics, in which the angle of incidence of a ray that is reflected at the wall results in a reflection angle being equal to the angle of incidence.</p><p id="p-0005" num="0004">Real-time auralization systems, like the audio renderer in a Virtual Reality (VR) or Augmented Reality (AR) system, usually render early reflections based on geometry data of the reflective environment. A Geometrical Acoustics method like the image source method in combination with ray-tracing is then used to find valid propagation paths of the reflected sound. These methods are valid, if the reflecting planar surfaces are large compared to the wave length of incident sound. The distance of the reflection point on the surface to the boundaries of the reflecting surface also has to be large compared to the wave length of incident sound.</p><p id="p-0006" num="0005">Sound in Virtual Reality (VR) and Augmented Reality (AR) is rendered for a listener (user). The inputs to this process are (typically anechoic) audio signals of sound sources. A multitude of signal processing techniques is then applied to these input signals, simulating and incorporating relevant acoustic effects such as sound transmission through walls/windows/doors, diffraction around and occlusion by solid or permeable structures, the propagation of sound over longer distances, reflections in half-open and enclosed environments, Doppler shifts of moving sources/listeners, etc. The output of the audio rendering are audio signals that create a realistic, three-dimensional acoustic impression of the presented VR/AR scene when delivered to the listener via headphones or loudspeakers.</p><p id="p-0007" num="0006">The rendering is performed listener-centric and the system has to react to user motion and interaction instantaneously, without significant delays. Hence the processing of the audio signals has to be performed in real-time. User input manifests in changes of the signal processing (e.g., different filters). These changes are to be incorporated in the rendering without audible artifacts.</p><p id="p-0008" num="0007">Most audio renderers used a pre-defined fixed signal processing structure (block diagram applied to multiple channels, see for example [1]) with a fixed computation time budget for each individual audio source (e.g. 16&#xd7; object source, 2&#xd7; third-order Ambisonics). These solutions enable rendering dynamic scenes, by updating location-dependent filters and reverb parameters, but they do not allow for sources to be dynamically added/removed during runtime.</p><p id="p-0009" num="0008">Moreover, a fixed signal processing architecture can be rather ineffective when rendering complex scenes, as a large number of sources has to be processed in the same way. Newer rendering concepts facilitate clustering and level-of-detail concepts (LOD), where, depending on the perception, sources are combined and rendered with different signal processing. Source clustering (see [2]) can enable renderers to handle complex scenes with hundreds of objects. In such a setup, the cluster budget is still fixed which may lead to audible artifacts of extensive clustering in complex scenes.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0010" num="0009">According to an embodiment, an apparatus for rendering a sound scene may have a first pipeline stage including a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor; a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage including a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor; and a central controller for controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and wherein the central controller is configured to control the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant.</p><p id="p-0011" num="0010">Another embodiment may have a method of rendering a sound scene using an apparatus including a first pipeline stage including a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor; a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage including a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor, the method having the steps of: controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and controlling the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant.</p><p id="p-0012" num="0011">Another embodiment may have a non-transitory digital storage medium having a computer program stored thereon to perform the method of rendering a sound scene using an apparatus including a first pipeline stage including a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor; a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage including a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor, the method having the steps of: controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and controlling the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant, when said computer program is run by a computer.</p><p id="p-0013" num="0012">The present invention is based on the finding that, for the purpose of rendering a complex sound scene with many sources in an environment, where frequent changes of the sound scene can occur, a pipeline-like rendering architecture is useful. The pipeline-like rendering architecture comprises a first pipeline stage comprising a first control layer and a reconfigurable first audio data processor. Furthermore a second pipeline stage that is located, with respect to a pipeline flow, subsequent to the first pipeline stage is provided.</p><p id="p-0014" num="0013">This second pipeline stage again comprises a second control layer and a reconfigurable second audio data processor. Both the first and the second pipeline stages are configured to operate in accordance with a certain configuration of the reconfigurable first audio data processor at a certain time in the processing. In order to control the pipeline-architecture, a central controller for controlling the first control layer and the second control layer is provided. The control takes place in response to the sound scene, i.e., in response to an original sound scene or a change of the sound scene.</p><p id="p-0015" num="0014">In order to achieve a synchronized operation of the apparatus among all pipeline stages, and when a reconfiguration task for the first or the second reconfigurable audio data processor is required, the central controller controls the control layers of the pipeline stages so that the first control layer or the second control layer prepares another configuration such as a second configuration of the first or the second reconfigurable audio data processor during or subsequent to an operation of the reconfigurable audio data processor in the first configuration. Hence, a new configuration for the reconfigurable first or second audio data processor is prepared while the reconfigurable audio data processor belonging to this pipeline stage is still operating in accordance with a different configuration or is configured in a different configuration in case the processing task with the earlier configuration is already done. In order to make sure that both pipeline stages operate synchronized in order to obtain the so-called &#x201c;atomic operation&#x201d; or &#x201c;atomic updates&#x201d;, the central controller controls the first and the second control layers using a switch control to reconfigure the reconfigurable first audio data processor or the reconfigurable second audio data processor to the second different configuration at a certain time instant. Even when only a single pipeline stage is reconfigured, embodiments of the present invention nevertheless guarantee that due to the switch control at the certain time instance, the correct audio sample data is processed in the audio workflow via the provision of the audio stream input or output buffers included in the corresponding render lists.</p><p id="p-0016" num="0015">Advantageously, the apparatus for rendering the sound scene has a higher number of pipeline stages than a first and a second pipeline stage, but already in a system with a first and a second pipeline stage and no additional pipeline stage, the synchronized switching of the pipeline stages in response to the switch control is necessary for obtaining an improved high quality audio rendering operation that, at the same time, is highly flexible.</p><p id="p-0017" num="0016">In particular, in complex virtual reality scenes, where a user can move in three directions and where, additionally, the user can move her or his head in three additional directions, i.e., in a six degrees of freedom (6-DoF) scenario, frequent and sudden changes of filters in the rendering pipeline, for example for switching from one head-related transfer function to another head-related transfer function in case of the moving of the listener's head or the walking around of the listener requires such a change of head-related transfer functions will take place.</p><p id="p-0018" num="0017">Other problematic situations with respect to a flexible rendering with high quality are that when a listener moves around in a virtual or augmented reality scene, the number of sources to be rendered will change all the time. This can for example occur due to the fact that certain image sources become visible at a certain position of the user or due to the fact that additional diffraction effects have to be considered. Furthermore, other procedures are that in certain situations, a clustering of many different closely spaced sources is possible while, when the user moves closer to these sources, then the clustering is not feasible anymore, since the user is so close that it is necessary that each source is rendered at its distinct position. Thus, such audio scenes are problematic in that changing filters or a changing number of sources to be rendered or, in general, changing parameters is required all the time. On the other hand, it is useful to distribute the different operations for rendering onto different pipeline stages so that an efficient and high speed rendering is possible, in order to make sure that a real time rendering in complex audio environments is achievable.</p><p id="p-0019" num="0018">A further example for a thoroughly changing parameter is that as soon as a user comes closer to a source or an image source, the frequency-dependent distance attenuation and propagation delay changes with the distance between the user and sound source. Similarly, the frequency-dependent characteristics of the reflective surface may change depending on the configuration between the user and a reflecting object. Furthermore, depending on whether a user is close to a diffracting object or further away from the diffracting object or at a different angle, the frequency-dependent diffraction characteristics will also change. Thus, if all these tasks are distributed to different pipeline stages, continuing changes of these pipeline stages have to be possible and have to be performed synchronously. All this is achieved by means of the central controller that controls the control layers of the pipeline stages to prepare for a new configuration during or subsequent to an operation of the corresponding configurable audio data processor in the earlier configuration. In response to a switch control for all stages in the pipeline effected by a control update via the switch control, the reconfiguration takes place a certain time instant being identical or being at least very similar among the pipeline stages in the apparatus for rendering the sound scene.</p><p id="p-0020" num="0019">The present invention is advantageous since it allows a high quality real-time auralization of auditory scenes with dynamically changing elements, for example moving sources and listeners. Thus, the present invention contributes to the achievement of perceptually convincing soundscapes that are a significant factor for the immersive experience of a virtual scene.</p><p id="p-0021" num="0020">Embodiments of the present invention apply separate and concurrent workflows, threads or processes that very well fit to the situation of rendering dynamic auditory scenes.<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0021">1. The interaction workflow: handling of changes in the virtual scene (e.g., user motion, user interaction, scene animations, etc.) that occur at arbitrary points in time.</li>    <li id="ul0001-0002" num="0022">2. the control workflow: a snapshot of the current state of the virtual scene results in updates of the signal processing and its parameters.</li>    <li id="ul0001-0003" num="0023">3. the processing workflow: execution of the real-time signal processing, i.e., taking a frame of input samples and computing the corresponding frame of output samples.</li></ul></p><p id="p-0022" num="0024">Executions of the control workflow vary in run time, depending on which necessary computations a change is triggered, similar to the frame loop in visual computing. Advantageous embodiments of the invention are advantageous in that such variations of executions of the control workflow do not at all adversely affect the processing workflow, which is concurrently executed in the background. As real-time audio is processed block-wise, the acceptable computation time of the processing workflow is typically limited to usually a few milliseconds.</p><p id="p-0023" num="0025">The processing workflow that is concurrently executed in the background is processed by the first and the second reconfigurable audio data processors, and the control workflow is initiated by the central controller and is then implemented, on the pipeline stage level, by the control layers of the pipeline stages parallel to the background operation of the processing workflow. The interaction workflow is implemented, on the pipelined rendering apparatus level, by an interface of the central controller to external devices such as a head tracker or a similar device or is controlled by the audio scene having a moving source or geometry that represents a change of the sound scene as well as a change in the user orientation or location, i.e., generally in the user position.</p><p id="p-0024" num="0026">The present invention is advantageous in that multiple objects in the scene can be changed coherently and sample synchronously due to the centrally controlled switch control procedure. Furthermore, this procedure allows so-called atomic updates of multiple elements that have to be supported by the control workflow and the processing workflow in order to not interrupt the audio processing due to changes on the highest level, i.e., in the interaction workflow or in the intermediate level, i.e., the control workflow.</p><p id="p-0025" num="0027">Advantageous embodiments of the present invention relate to the apparatus for rendering the sound scene implementing a modular audio rendering pipeline, where the necessary steps for auralization of virtual auditory scenes are partitioned into several stages which are each independently responsible for certain perceptual effects. The individual partitioning into at least two or advantageously even more individual pipeline stages depends on the application and is advantageously defined by the author of the rendering system as is illustrated later.</p><p id="p-0026" num="0028">The present invention provides a generic structure for the rendering pipeline that facilitates parallel processing and dynamic reconfiguration of the signal processing parameters depending on the current state of the virtual scene. In that process, embodiments of the present invention ensure<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0029">a) that each stage can change their DSP processing dynamically (e.g., number of channels, updated filter coefficients) without producing audible artifacts and that any update of the rendering pipeline, based on recent changes in the scene, is handled synchronously and atomically if required</li>    <li id="ul0002-0002" num="0030">b) that changes in the scene (e.g., listener movement) can be received at arbitrary points in time and do not influence the real-time performance of the system and particularly the DSP processing, and</li>    <li id="ul0002-0003" num="0031">c) that individual stages can profit from the functionality of other stages in the pipeline (e.g., a unified directivity rendering for primary and image sources or opaque clustering for complexity reduction).</li></ul></p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0027" num="0032">Embodiments of the present invention will be detailed subsequently referring to the appended drawings, in which:</p><p id="p-0028" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a render stage input/output illustration;</p><p id="p-0029" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a state transition of render items;</p><p id="p-0030" num="0035"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a render pipeline overview;</p><p id="p-0031" num="0036"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example structure for a virtual reality auralization pipeline;</p><p id="p-0032" num="0037"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a advantageous implementation of the apparatus for rendering a sound scene;</p><p id="p-0033" num="0038"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example implementation for changing metadata for existing render items;</p><p id="p-0034" num="0039"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates another example for the reduction of render items, for example by clustering;</p><p id="p-0035" num="0040"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates another example implementation for adding new render items such as for early reflections; and</p><p id="p-0036" num="0041"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a flow chart for illustrating a control flow from a high level event being an audio scene (change) to a low level fade-in or fade-out of old or new items or a cross fade of filters or parameters.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading><p id="p-0037" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an apparatus for rendering a sound scene or audio scene received by a central controller <b>100</b>. The apparatus comprises a first pipeline stage <b>200</b> with a first control layer <b>201</b> and a reconfigurable first audio data processor <b>202</b>. Furthermore, the apparatus comprises a second pipeline stage <b>300</b> located, with respect to a pipeline flow, subsequent to the first pipeline stage <b>200</b>. The second pipeline stage <b>300</b> can be placed immediately following the first pipeline stage <b>200</b> or can be placed with one or more pipeline stages in between the pipeline stage <b>300</b> and the pipeline stage <b>200</b>. The second pipeline stage <b>300</b> comprises a second control layer <b>301</b> and a reconfigurable second audio data processor <b>302</b>. Furthermore, an optional n-th pipeline stage <b>400</b> is illustrated that comprises an n-th control layer <b>401</b> and the reconfigurable n-th audio data processor <b>402</b>. In the exemplary embodiment in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the result of the pipeline stage <b>400</b> is the already rendered audio scene, i.e., the result of the whole processing of the audio scene or the audio scene changes that have arrived at the central controller <b>100</b>. The central controller <b>100</b> is configured for controlling the first control layer <b>201</b> and the second control layer <b>301</b> in response to the sound scene.</p><p id="p-0038" num="0043">In response to the sound scene means in response to a whole scene input at a certain initialization or beginning time instant or in response to sound scene changes that, together with a preceding scene existing before the sound scene changes again, represent a full sound scene that is to be processed by the central controller <b>100</b>. In particular, the central controller <b>100</b> controls the first and the second control layers and if available, any other control layers such the n-th control layer <b>401</b> so that a new or second configuration of the first, the second and/or the n-th reconfigurable audio data processor is prepared while the corresponding reconfigurable audio data processor operates in the background in accordance with an earlier or first configuration. For this background mode, it is not decisive whether the reconfigurable audio data processor still operates, i.e., receives input samples and calculates output samples. Instead, it can also be the situation that a certain pipeline stage has already completed its tasks. Thus, the preparation of the new configuration takes place during or subsequent to an operation of the corresponding reconfigurable audio data processor in the earlier configuration.</p><p id="p-0039" num="0044">In order to make sure that atomic updates of the individual pipeline stages <b>200</b>, <b>300</b>, <b>400</b> are possible, the central controller outputs a switch control <b>110</b> in order to reconfigure the individual reconfigurable first or second audio data processors at a certain time instant. Depending on the specific application or sound scene change, only a single pipeline stage can be reconfigured at the certain time instant or two pipeline stages such as pipeline stages <b>200</b>, <b>300</b> are both reconfigured at the certain time instant or all pipeline stages of the whole apparatus for rendering the sound scene or only a subgroup having more than two pipeline stages but less than all pipeline stages can also be provided with the switch control to be reconfigured at the certain time instant. To this end, the central controller <b>100</b> has a control line to each control layer of the corresponding pipeline stage in addition to the processing workflow connection serially connecting the pipeline stages. Furthermore, the control workflow connection that is discussed later can either be provided also via the first structure for the central switch control <b>110</b>. In advantageous embodiments, however, the control workflow is also performed via the serial connection among the pipeline stages so that the central connection between each control layer of the individual pipeline stage and the central controller <b>100</b> is only reserved for the switch control <b>110</b> to obtain atomic updates and, therefore, a correct and high quality audio rendering even in complex environments.</p><p id="p-0040" num="0045">The following section describes a general audio rendering pipeline, composed of independent render stages, each with separated, synchronized control and processing workflows (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). A superordinate controller ensures that all stages in the pipeline can be updated together atomically.</p><p id="p-0041" num="0046">Every render stage has a control part and a processing part with separate inputs and outputs corresponding to the control and processing workflow respectively. In the pipeline, the outputs of one render stage are the inputs of a succeeding render stage, while a common interface guarantees that render stages can be reorganized and replaced, depending on the application.</p><p id="p-0042" num="0047">This common interface is described as a flat list of render items that is provided to the render stage in the control workflow. A render item combines processing instructions (i.e., metadata, such as position, orientation, equalization, etc.) with an audio stream buffer (single- or multichannel). The mapping of buffers to render items is arbitrary, such that multiple render items can refer to the same buffer.</p><p id="p-0043" num="0048">Every render stage ensures that succeeding stages can read the correct audio samples from the audio stream buffers corresponding to the connected render items at the rate of the processing workflow. To achieve this, every render stage creates a processing diagram from the information in the render items that describes the necessary DSP steps and its input and output buffers. Additional data may be required to construct the processing diagram (e.g., geometry in the scene or personalized HRIR sets) and is provided by the controller. The processing diagrams are lined up for synchronization and handed over to the processing workflow simultaneously for all render stages, after the control update is propagated through the whole pipeline. The exchange of processing diagrams is triggered without interfering with the real-time audio block rate, while the individual stages have to guarantee that no audible artifacts occur due to the exchange. If a render stage only acts on metadata, the DSP workflow can be a no-operation.</p><p id="p-0044" num="0049">The controller maintains a list of render items corresponding to actual audio sources in the virtual scene. In the control workflow, the controller starts a new control update by passing a new list of render items to the first render stage, atomically cumulating all metadata changes resulting from user interaction and other changes in the virtual scene. Control updates are triggered at a fixed rate that may depend on the available computational resources, but only after the previous update is finished. A render stage creates a new list of output render items from the input list. In that process, it can modify existing metadata (e.g., add an equalization characteristic), as well as add new and deactivate or remove existing render items. Render items follow a defined life cycle (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) that is communicated via a state indicator on each render item (e.g., &#x201c;activate&#x201d;, &#x201c;deactivate&#x201d;, &#x201c;active&#x201d;, &#x201c;inactive&#x201d;). This allows subsequent render stages to update their DSP diagrams according to newly created or obsolete render items. Artifact-free fade-in and fade-out of the render items on state change are handled by the controller.</p><p id="p-0045" num="0050">In a real-time application, the processing workflow is triggered by the callback from the audio hardware. When a new block of samples is requested, the controller fills the buffers of the render items it maintains with input samples (e.g., from disk or from incoming audio streams). The controller then triggers the processing part of the render stages sequentially, which act on the audio stream buffers according to their current processing diagrams.</p><p id="p-0046" num="0051">The render pipeline may contain one or more spatializers (<figref idref="DRAWINGS">FIG. <b>3</b></figref>) that are similar to a render stage, but the output of their processing part is a mixed representation of the whole virtual auditory scene as described by the final list of render items and can directly be played over a specified playback method (e.g., binaural over headphones or multichannel loudspeaker setups). However, additional render stages may follow after a spatializer (e.g., for limiting the dynamic range of the output signal).</p><heading id="h-0006" level="2">Advantages of the Proposed Solution</heading><p id="p-0047" num="0052">Compared to the state of the art, the inventive audio rendering pipeline can handle highly dynamic scenes with the flexibility to adapt processing to different hardware or user requirements. In this section, several advances over established methods are listed.<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0053">New audio elements can be added to and removed from the virtual scene at runtime. Similarly, render stages can dynamically adjust the level of detail of their rendering based on available computational resources and perceptual requirements.</li>        <li id="ul0004-0002" num="0054">Depending on the application, render stages can be reordered or new render stages can be inserted at arbitrary positions in the pipeline (e.g., a clustering or visualization stage) without changing other parts of the software. Individual render stage implementations can be changed without having to change other render stages.</li>        <li id="ul0004-0003" num="0055">Multiple spatializers can share a common processing pipeline, enabling for example multi-user VR setups or headphone and loudspeaker rendering in parallel with minimal computational effort.</li>        <li id="ul0004-0004" num="0056">Changes in the virtual scene (for example caused by a high-rate head-tracking device) are cumulated with a dynamically adjustable control rate, reducing the computational effort, e.g., for filter switching. At the same time, scene updates that explicitly require atomicity (e.g., parallel movement of audio sources) are guaranteed to be executed at the same time across all render stages.</li>        <li id="ul0004-0005" num="0057">The control and processing rate can be adjusted separately, based on the requirements of the user and (audio playback) hardware.</li>    </ul>    </li></ul></p><heading id="h-0007" level="1">Example</heading><p id="p-0048" num="0058">A practical example for a rendering pipeline to create virtual acoustic environments for VR applications may contain the following render stages in the given order (see also <figref idref="DRAWINGS">FIG. <b>4</b></figref>):<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0059">1. Transmission: Reducing a complex scene with multiple adjoint subspaces by downmixing signals and reverb of distant parts from the listener into a single render item (possibly with spatial extent).    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0060">Processing part: Downmix of signals into combined audio stream buffers and processing the audio samples with established techniques for creating late reverb</li>    </ul>    </li>    <li id="ul0005-0002" num="0061">2. Extent: Rendering the perceptual effect of spatially extended sound sources by creating multiple, spatially disjunct render items.    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0062">Processing part: distribution of the input audio signal to several buffers for the new render items (possibly with additional processing like decorrelation)</li>    </ul>    </li>    <li id="ul0005-0003" num="0063">3. Early Reflections: Incorporating perceptually relevant geometric reflections on surfaces by creating representative render items with corresponding equalization and position metadata.    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0064">Processing part: distribution of the input audio signal to several buffers for the new render items</li>    </ul>    </li>    <li id="ul0005-0004" num="0065">4. Clustering: Combining multiple render items with perceptually indistinguishable positions into a single render item to reduce the computational complexity for subsequent stages.    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="0066">Processing part: Downmix of signals into combined audio stream buffers</li>    </ul>    </li>    <li id="ul0005-0005" num="0067">5. Diffraction: Adding perceptual effects of occlusion and diffraction of propagation paths by geometry.</li>    <li id="ul0005-0006" num="0068">6. Propagation: Rendering perceptual effects on the propagation path (e.g., direction-dependent radiation characteristics, medium absorption, propagation delay, etc.)</li></ul></p><p id="p-0049" num="0069">Processing part: filtering, fractional delay lines, etc.<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0070">7. Binaural Spatializer: Rendering the remaining render items to a listener-centric binaural sound output.    <ul id="ul0011" list-style="none">        <li id="ul0011-0001" num="0071">Processing part: HRIR filtering, downmixing, etc.</li>    </ul>    </li></ul></p><p id="p-0050" num="0072">Subsequently, <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>4</b></figref> are described in other words. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates, for example, the first pipeline stage <b>200</b> also termed to be a &#x201c;render stage&#x201d; that comprises the control layer <b>201</b> indicated as &#x201c;controller&#x201d; in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the reconfigurable first audio data processor <b>202</b> indicated to be a &#x201c;DSP&#x201d; (digital signal processor). The pipeline stage or render stage <b>200</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> can, however, also be considered to be the second pipeline stage <b>300</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the n-th pipeline stage <b>400</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0051" num="0073">The pipeline stage <b>200</b> receives, as an input via an input interface, an input render list <b>500</b> and outputs, via an output interface, an output render list <b>600</b>. In case of a directly subsequent connection of the second pipeline stage <b>300</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the input render list for the second pipeline stage <b>300</b> will then be the output render list <b>600</b> of the first pipeline stage <b>200</b>, since the pipeline stages are serially connected to for the pipeline flow.</p><p id="p-0052" num="0074">Each render list <b>500</b> comprises a selection of render items illustrated by a column in the input render list <b>500</b> or the output render list <b>600</b>. Each render item comprises a render item identifier <b>501</b>, render item metadata <b>502</b> indicated as &#x201c;x&#x201d; in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and one or more audio stream buffers depending on how many audio objects or individual audio streams belong to the render item. The audio stream buffers are indicated by &#x201c;0&#x201d; and are advantageously implemented by memory references to actual physical buffers in a wording memory part of the apparatus for rendering the sound scene that can, for example, be managed by the central controller or can be managed in any other way of memory management.</p><p id="p-0053" num="0075">Alternatively, the render list can comprise audio stream buffers representing physical memory portions, but it is advantageous to implement the audio stream buffers <b>503</b> as said references to a certain physical memory.</p><p id="p-0054" num="0076">Similarly, the output render list <b>600</b> again has one column for each render item and the corresponding render item is identified by a render item identification <b>601</b>, corresponding metadata <b>602</b> and audio stream buffers <b>603</b>. Metadata <b>502</b> or <b>602</b> for the render items can comprise a position of a source, a type of a source, an equalizer associated with a certain source or, generally, a frequency-selective behavior associated with a certain source. Thus, the pipeline stage <b>200</b> receives, as an input, the input render list <b>500</b> and generates, as an output, the output render list <b>600</b>. Within the DSP <b>202</b>, audio sample values identified by the corresponding audio stream buffers are processed as required by the corresponding configuration of the reconfigurable audio data processor <b>202</b>, for example as indicated by a certain processing diagram generated by the control layer <b>201</b> for the digital signal processor <b>202</b>. Since the input render list <b>500</b> comprises, for example, three render items, and the output render list <b>600</b> comprises, for example, four render items, i.e., more render items than the input, the pipeline stage <b>202</b> could perform an upmix, for example. Another implementation could, for example, be that the first render item with the four audio signals is downmixed into a render item with a single channel. The second render item could be left untouched by the processing, i.e., could, for example, be only copied from the input to the output, and the third render item could also be, for example, left untouched by the render stage. Only the last output render item in the output render list <b>600</b> could be generated by the DSP, for example, by combining the second and the third render items of the input render list <b>500</b> into a single output audio stream for the corresponding audio stream buffer for the fourth render item of the output render list.</p><p id="p-0055" num="0077"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a state diagram for defining the &#x201c;live&#x201d; of a render item. It is advantageous that the corresponding state of the state diagram is also stored in the metadata <b>502</b> of the render item or in the identification field of the render item. In start node <b>510</b>, two different ways of activation can be performed. One way is a normal activation in order to come to an activate state <b>511</b>. The other way is an immediate activation procedure in order to already arrive at the active state <b>512</b>. The difference between both procedures is that from the activate state <b>511</b> to the active state <b>512</b>, a fade in procedure is performed.</p><p id="p-0056" num="0078">If a render item is active, it is processed and it can be either immediately deactivated or normally deactivated. In the latter case, a deactivate state <b>514</b> is obtained and a fade out procedure is performed in order to come from the deactivate state <b>514</b> to the inactive state <b>513</b>. In case of an immediate deactivation, a direct transition from state <b>512</b> to state <b>513</b> is performed. The inactive state can either come back to an immediate reactivation or into a reactivate instruction in order to arrive at the activate state <b>511</b> or, if neither a reactivate control nor an immediate reactivation control is obtained, control can proceed to the disposed output node <b>515</b>.</p><p id="p-0057" num="0079"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a render pipeline overview where the audio scene is illustrated at block <b>50</b> and where the individual control flows are illustrated as well. The central switch control flow is illustrated at <b>110</b>. The control workflow <b>130</b> is illustrated to take place from the controller <b>100</b> into the first stage <b>200</b> and, from there, via the corresponding serial control workflow line <b>120</b>. Thus, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the implementation where the control workflow is also fed in into the start stage of the pipeline and is, from there, propagated in a serial manner to the last stage. Similarly, the processing workflow <b>120</b> starts from the controller <b>120</b> via the reconfigurable audio data processors of the individual pipeline stages into the final stages where <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates two final stages, one loudspeaker output stage or specializer one stage <b>400</b><i>a </i>or a headphone specializer output stage <b>400</b><i>b. </i></p><p id="p-0058" num="0080"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an exemplary virtual reality rendering pipeline having the audio scene representation <b>50</b>, the controller <b>100</b> and, as the first pipeline stage, a transmission pipeline stage <b>200</b>. The second pipeline stage <b>300</b> is implemented as an extent render stage. A third pipeline stage <b>400</b> is implemented as an early reflection pipeline stage. A fourth pipeline stage is implemented as a clustering pipeline stage <b>551</b>. A fifth pipeline stage is implemented as a diffraction pipeline stage <b>552</b>. A sixth pipeline stage is implemented as a propagation pipeline stage <b>553</b>, and a final seventh pipeline stage <b>554</b> is implemented as a binaural spatializer in order to finally obtain headphone signals for a headphone to be worn by a listener navigating in the virtual reality or augmented reality audio scene.</p><p id="p-0059" num="0081">Subsequently, <figref idref="DRAWINGS">FIGS. <b>6</b>, <b>7</b> and <b>8</b></figref> are illustrated and discussed in order to give certain examples for how the pipeline stages can be configured and how the pipeline stages can be reconfigured.</p><p id="p-0060" num="0082"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates the procedure of changing meta data for existing render items.</p><heading id="h-0008" level="2">Scenario</heading><p id="p-0061" num="0083">Two object audio sources are represented as two Render Items (RIs). The Directivity Stage is responsible for directional filtering of the sound source signal. The Propagation Stage is responsible for rendering a propagation delay based on the distance to the listener. The Binaural Spatializer is responsible for binauralization and downmixing the scene to a binaural stereo signal.</p><p id="p-0062" num="0084">At a certain control step, the RI positions change with regard to previous control steps, thus requiring changes in the DSP processing of each individual stage. The acoustic scene should update synchronously, so that e.g. the perceptual effect of a changing distance is synchronous with the perceptual effect of a change in the listener-relative angle of incidence.</p><heading id="h-0009" level="2">Implementation</heading><p id="p-0063" num="0085">The Render List is propagated through the complete pipeline at each control step. During the control step, the parameters of the DSP processing stay constant for all stages, until the last Stage/Spatializer has processed the new Render List. After that, all Stages change their DSP parameters synchronously at the beginning of the next DSP step.</p><p id="p-0064" num="0086">It is each Stage's responsibility to update the parameters of the DSP processing without noticeable artifacts (e.g. output crossfade for FIR filter updates, linear interpolation for delay lines).</p><p id="p-0065" num="0087">RIs can contain fields for metadata pooling. This way, for example the Directivity stage does not need to filter the signal itself, but can update an EQ field in the RI metadata. A subsequent EQ stage then applies the combined EQ field of all preceding stages to the signal.</p><heading id="h-0010" level="2">Key Advantages</heading><p id="p-0066" num="0000"><ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0000">    <ul id="ul0013" list-style="none">        <li id="ul0013-0001" num="0088">Guaranteed atomicity of scene changes (both across Stages and across RIs)</li>        <li id="ul0013-0002" num="0089">Larger DSP reconfigurations do not block the audio processing and are synchronously executed when all Stages/Spatializers are ready</li>        <li id="ul0013-0003" num="0090">With clearly defined responsibilities, other Stages of the pipeline are independent of the algorithm used for a specific task (e.g. the method or even availability of clustering)</li>        <li id="ul0013-0004" num="0091">Metadata pooling allows many Stages (Directivity, Occlusion, etc.) to operate only in the control step.</li>    </ul>    </li></ul></p><p id="p-0067" num="0092">Particularly, the input render list is the same as the output render list <b>500</b> in the <figref idref="DRAWINGS">FIG. <b>6</b></figref> example. Particularly, the render list has a first render item <b>511</b> and a second render item <b>512</b> where each render item has a single audio stream buffer.</p><p id="p-0068" num="0093">In the first render or pipeline stage <b>200</b> which is the directivity stage in this example, a first FIR filter <b>211</b> is applied to the first render item and another directivity filter or FIR filter <b>212</b> is applied to the second render item <b>512</b>. Furthermore, within the second render stage or second pipeline stage <b>33</b>, which is the propagation stage in this embodiment, a first interpolating delay line <b>311</b> is applied to the first render item <b>511</b>, and another second interpolating delay line <b>312</b> is applied to the second render item <b>512</b>.</p><p id="p-0069" num="0094">Furthermore, in the third pipeline stage <b>400</b> connected subsequent to the second pipeline stage <b>300</b>, a first stereo FIR filter <b>411</b> for the first render item <b>511</b> is used, and a second FIR filter <b>412</b> or the second render item <b>512</b> is used. In the binaural specializer, a downmix of the two filter output data is performed in the adder <b>413</b> in order to have the binaural output signal. Thus, the two object signals indicated by the render items <b>511</b>, <b>512</b>, binaural signal at the output of the adder <b>413</b> (not illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) is generated. Thus, as discussed, all the elements <b>211</b>, <b>212</b>, <b>311</b>, <b>312</b>, <b>411</b>, <b>412</b> are changed in response to the switch control at the same certain time instant under the control of the control layer <b>201</b>, <b>301</b>, <b>401</b>. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a situation, where the number of objects indicated in the render list <b>500</b> remains the same, but the meta data for the objects have changed due to a different position of the object. Or, alternatively, the meta data for the objects and, particularly, the position of the object has remained the same, but, in view of the listener movement, the relation between the listener and the corresponding (fixed) object has changed resulting in changes of the FIR filters <b>211</b>, <b>212</b>, and changes in the delay lines <b>311</b>, <b>312</b>, and changes in the FIR filters <b>411</b>, <b>412</b> that are, for example, implemented as head related transfer function filters that change with each change of the source or object position or the listener position as, for example, measured by a head tracker, for example.</p><p id="p-0070" num="0095"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a further example related to the reduction of render items (by clustering).</p><heading id="h-0011" level="2">Scenario</heading><p id="p-0071" num="0096">In a complex auditory scene, the Render List may contain many RIs that are perceptually close-by, i.e. their difference in position cannot be distinguished by the listener. To reduce the computational load for subsequent Stages, a Clustering Stage may replace multiple individual RIs by a single representative RI.</p><p id="p-0072" num="0097">At a certain control step, the scene configuration may change so that the clustering is no longer perceptually feasible. In this case, the Clustering Stage will become inactive and passes the Render List unchanged.</p><heading id="h-0012" level="2">Implementation</heading><p id="p-0073" num="0098">When some incoming RIs are clustered, the original RIs are deactivated in the outgoing Render List. The reduction is opaque for subsequent Stages and the Clustering Stage needs to guarantee that as soon as the new outgoing Render List becomes active, valid samples are provided in the buffers associated with the representative RI.</p><p id="p-0074" num="0099">When the cluster becomes infeasible, the new outgoing Render List of the Clustering stage contains the original, unclustered RIs. Subsequent stages need to process them individually starting with the next DSP parameter change (e.g. by adding a new FIR filter, delay line, etc. to their DSP diagram).</p><heading id="h-0013" level="2">Key Advantages</heading><p id="p-0075" num="0000"><ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0000">    <ul id="ul0015" list-style="none">        <li id="ul0015-0001" num="0100">The opaque reduction of RIs reduces the computational load for subsequent stages without explicit reconfiguration</li>        <li id="ul0015-0002" num="0101">Due to atomicity of the DSP parameter change, Stages can handle varying numbers of incoming and outgoing RIs without artifacts</li>    </ul>    </li></ul></p><p id="p-0076" num="0102">In the <figref idref="DRAWINGS">FIG. <b>7</b></figref> example, the input render list <b>500</b> comprises 3 render item <b>521</b>, <b>522</b>, <b>523</b>, and the output renderer <b>600</b> comprises two render items <b>623</b>, <b>624</b>.</p><p id="p-0077" num="0103">The first render item <b>521</b> comes from an output of the FIR filter <b>221</b>. The second render item <b>522</b> is generated by an output of the FIR filter <b>222</b> of the directivity stage, and the third render item <b>523</b> is obtained at the output of the FIR filter <b>223</b> of the first pipeline stage <b>200</b> being the directivity stage. It is to be noted that, when it is outlined that a render item is at the output of a filter, this refers to the audio samples for the audio stream buffer of the corresponding render item.</p><p id="p-0078" num="0104">In the example in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, render item <b>523</b> remains untouched by the clustering state <b>300</b> and becomes output render item <b>623</b>. However, render item <b>521</b> and render item <b>522</b> are downmixed into dowmixed render item <b>324</b> that occurs in the renderer <b>600</b> as output render item <b>624</b>. The downmixing in the clustering stage <b>300</b> is indicated by a place <b>321</b> for the first render item <b>521</b> and a place <b>322</b> for the second render item <b>522</b>.</p><p id="p-0079" num="0105">Again, the third pipeline stage in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a binaural spatializer <b>400</b> and the render item <b>624</b> is processed by the first stereo FIR filter <b>424</b>, and the render item <b>623</b> is processed by the stereo filter FIR filter <b>423</b>, and the output of both filters is added in adder <b>413</b> to give the binaural output.</p><p id="p-0080" num="0106"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates another example illustrating the addition of new render items (for early reflections).</p><heading id="h-0014" level="2">Scenario</heading><p id="p-0081" num="0107">In geometric room acoustics, it may be beneficial to model reflected sound as image sources (i.e. two point sources with the same signal and their position mirrored on a reflective surface). If the configuration between listener, source and a reflecting surface in the scene is favorable for reflection, the Early Reflections Stage adds a new RI to its outgoing Render List that represents the image source.</p><p id="p-0082" num="0108">The audibility of image sources typically changes rapidly when the listener moves. The Early Reflections Stage can activate and deactivate the RIs at each control step and subsequent Stages should adjust their DSP processing accordingly.</p><heading id="h-0015" level="2">Implementation</heading><p id="p-0083" num="0109">Stages after the Early Reflections Stage can process the reflection RI normally, as the Early Reflections Stage guarantees that the associated audio buffer contains the same samples as the original RI. This way, perceptual effects like propagation delay can be handled for original RIs and reflections alike without explicit reconfiguration. For increased efficiency when the activity status of RIs changes often, the Stages can keep required DSP artifacts (like FIR filter instances) for reuse.</p><p id="p-0084" num="0110">Stages can handle Render Items with certain properties differently. For example, a Render Item created by a Reverb Stage (depicted by item <b>532</b> in <figref idref="DRAWINGS">FIG. <b>8</b></figref>) may not be processed by the Early Reflections Stage and will only be processed by the Spatializer. In this way, a Render Item can provide the functionality of a downmix bus. In a similar way, a Stage may handle Render Items generated by the Early Reflections Stage with a lower quality DSP algorithm as they are typically less prominent acoustically.</p><heading id="h-0016" level="2">Key Advantages</heading><p id="p-0085" num="0000"><ul id="ul0016" list-style="none">    <li id="ul0016-0001" num="0000">    <ul id="ul0017" list-style="none">        <li id="ul0017-0001" num="0111">Different Render Items can be treated differently based on their properties</li>        <li id="ul0017-0002" num="0112">A Stage that creates new Render Items can profit from the processing of subsequent Stages without explicit reconfiguration</li>    </ul>    </li></ul></p><p id="p-0086" num="0113">The render list <b>500</b> comprises a first render item <b>531</b> and a second render item <b>532</b>. Each has a single audio stream buffer that can carry mono or a stereo signal, for example.</p><p id="p-0087" num="0114">The first pipeline stage <b>200</b> is a reverb stage that has, for example, generated render item <b>531</b>. The render list <b>500</b> additionally has render item <b>532</b>. In the earlier deflection stage <b>300</b>, render item <b>531</b> and, particularly, the audio samples thereof are represented by an input <b>331</b> for a copy operation. The input <b>331</b> of the copy operation is copied into the output audio stream buffer <b>331</b> corresponding to the audio stream buffer of render item <b>631</b> of the output render list <b>600</b>. Furthermore, the other copied audio object <b>333</b> corresponds to the render item <b>633</b>. Furthermore, as stated, render item <b>532</b> of the input render list <b>500</b> is simply copied or fed through to the render item <b>632</b> of the output render list.</p><p id="p-0088" num="0115">Then, in the third pipeline stage that is, in the above example, binaural spatializer, the stereo FIR filter <b>431</b> is applied to the first render item <b>631</b>, the stereo FIR filter <b>433</b> is applied to the second render item <b>633</b>, and the third stereo FIR filter <b>432</b> is applied to the third render item <b>632</b>. Then, the contributions of all three filters are correspondingly added, i.e., channel-by-channel by the adder <b>413</b> and the output of the adder <b>413</b> are a left signal on the one hand and a right signal on the other hand for a headphone or, generally, for a binaural reproduction.</p><p id="p-0089" num="0116"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an overview of the individual control procedures from a high level control by an audio scene interface of the central controller until a low level control performed by the control layer of a pipeline stage.</p><p id="p-0090" num="0117">At certain instants that may be time instants that are irregular and depend on a listener behavior, as, for example, determined by a head tracker, a central controller receives an audio scene or an audio scene change as indicated by step <b>91</b>. In step <b>92</b>, the central controller determines a render list for each pipeline stage under the control of the central controller. Particularly, the control updates that are then sent from the central controller to the individual pipeline stages are triggered at regular rates, i.e., with a certain update rate or update frequency.</p><p id="p-0091" num="0118">As illustrated in step <b>93</b>, the central controller sends the individual render list to each respective pipeline stage control layer. This can be done centrally via the switch control infrastructure, for example, but it is advantageous to perform this serially via the first pipeline stage and from there to the next pipeline stage and so on as indicated by the control workflow line <b>130</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In a further step <b>94</b>, each control layer builds its corresponding processing diagram for the new configuration for the corresponding reconfigurable audio data processor as illustrated in step <b>94</b>. The old configuration is also indicated to be the &#x201c;first configuration&#x201d;, and the new configuration is indicated to be the &#x201c;second configuration&#x201d;.</p><p id="p-0092" num="0119">In step <b>95</b>, the control layer receives the switch control from the central controller and reconfigures its associated reconfigurable audio data processor to the new configuration. This control layer switch control reception in step <b>95</b> can take place in response to a reception of a ready message of all pipeline stages by the central controller or can be done in response to a sending out of the central controller of the corresponding switch control instruction after a certain time duration with respect to the update trigger as done in step <b>93</b>. Then, in step <b>96</b>, the control layer of the corresponding pipeline stage cares for the fade-out of items that do not exist in the new configuration or cares for the fade-in of new items that have not existed in the old configuration. In case of the same objects in the old configuration and the new configuration, and in case of meta data changes such as with respect to the distance to a source or a new HRTF filter due to a movement of the listener's head and so on, a cross-fade of filters or a cross-fade of filtered data in order to smoothly come from one distance, for example, to the other distance is also controlled by the control layer in step <b>96</b>.</p><p id="p-0093" num="0120">The actual processing in the new configuration is started via a call back from the audio hardware. Thus, in other words, the processing workflow is triggered subsequent to the reconfiguration to the new configuration in a advantageous embodiment. When a new block of samples is requested, the central controller fills the audio stream buffers of the render items it maintains with input samples such as from a disc or from incoming audio streams. The controller then triggers the process part of the render stages, i.e., the reconfigurable audio data processors sequentially, and the reconfigurable audio data processors act on the audio stream buffers according to their current configuration, i.e., to their current processing diagrams. Thus, the central controller fills the audio stream buffers of the first pipeline stage in the apparatus for rendering a sound scene. However, there will also be the situation, where input buffers of other pipeline stages are to be filled from the central controller. This situation can, for example, arise when there have not been spatially extended sound sources in earlier situations of the audio scene. Thus, in this earlier situation, stage <b>300</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> was not present. Then, however, the listener has moved to a certain place in the virtual audio scene where a spatially extended sound source is visible or has to be rendered as a spatially extended sound source since the listener is quite close to this sound source. Then, at this point in time, in order to introduce this spatially extended sound source via block <b>300</b>, the central controller <b>100</b> will feed, typically via the transmission stage <b>200</b>, the new render list for the extend render stage <b>300</b>.</p><p id="p-0094" num="0121">While this invention has been described in terms of several embodiments, there are alterations, permutations, and equivalents which fall within the scope of this invention. It should also be noted that there are many alternative ways of implementing the methods and compositions of the present invention. It is therefore intended that the following appended claims be interpreted as including all such alterations, permutations and equivalents as fall within the true spirit and scope of the present invention.</p><heading id="h-0017" level="1">REFERENCES</heading><p id="p-0095" num="0000"><ul id="ul0018" list-style="none">    <li id="ul0018-0001" num="0122">[1] Wenzel, E. M., Miller, J. D., and Abel, J. S. &#x201c;Sound Lab: A real-time, software-based system for the study of spatial hearing.&#x201d; Audio Engineering Society Convention 108. Audio Engineering Society, 2000.</li>    <li id="ul0018-0002" num="0123">[2] Tsingos, N., Gallo, E., and Drettakis, G &#x201c;Perceptual audio rendering of complex virtual environments.&#x201d; ACM Transactions on Graphics (TOG) 23.3 (2004): 249-258.</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. Apparatus for rendering a sound scene, comprising.<claim-text>a first pipeline stage comprising a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor;</claim-text><claim-text>a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage comprising a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor; and</claim-text><claim-text>a central controller for controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and</claim-text><claim-text>wherein the central controller is configured to control the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the central controller is configured for controlling the first control layer to prepare the second configuration of the reconfigurable first audio data processor during an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, and<claim-text>for controlling the second control layer to prepare the second configuration of the reconfigurable second audio data processor during the operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and</claim-text><claim-text>for controlling the first control layer and the second control layer using the switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor and to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at the certain time instant.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first pipeline stage or the second pipeline stage comprises an input interface configured for receiving an input render list, wherein the input render list comprises an input list of render items, meta data for each render item and an audio stream buffer for each render item,<claim-text>wherein at least the first pipeline stage comprises an output interface configured for outputting an output render list, where the output render list comprises an output list of render items, meta data for each render item and an audio stream buffer for each render item, and</claim-text><claim-text>wherein, when the second pipeline stage is connected to the first pipeline stage, the output render list of the first pipeline stage is the input render list for the second pipeline stage.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. Apparatus of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first pipeline stage is configured to write audio samples into a corresponding audio stream buffer indicated by the output list of render items, so that the second pipeline stage succeeding the first pipeline stage is able to retrieve the audio stream samples from the corresponding audio stream buffer at a processing workflow rate.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the central controller is configured to provide the input or output render list to the first or the second pipeline stage, wherein the first or the second configuration of the reconfigurable first or second audio data processors comprises a processing diagram, wherein the first or the second control layer is configured to create the processing diagram for the second configuration from the input or the output render list received from the central controller or from a preceding pipeline stage,<claim-text>wherein the processing diagram comprises audio data processor steps and references to input and output buffers of the corresponding first or second reconfigurable audio data processor.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. Apparatus of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the central controller is configured to provide additional data necessary for creating the processing diagram, to the first or the second pipeline stage, wherein the additional data are not comprised by the input render list or the output render list.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the central controller is configured to receive a sound scene change via a sound scene interface at a sound scene change instant,<claim-text>wherein the central controller is configured to generate a first render list for the first pipeline stage, and a second render list for the second pipeline stage in response to the sound scene change and based on a current sound scene defined by the sound scene change, and wherein the central controller is configured to send the first render list to the first control layer and the second central render list to the second control layer subsequent to the sound scene change time instant.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. Apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>,<claim-text>wherein the first control layer is configured to calculate the second configuration of the first reconfigurable audio data processor from the first render list subsequent to the sound scene change time instant, and</claim-text><claim-text>wherein the second control layer is configured to calculate the second configuration of the second reconfigurable data processor from the second render list, and</claim-text><claim-text>wherein the central controller is configured to trigger the switch control simultaneously for the first and the second pipeline stages.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the central controller is configured to use the switch control without interfering with an audio sample calculation operation as performed by the first and the second reconfigurable audio data processors.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the central controller is configured to receive changes to the audio scene at change time instants having an irregular data rate,</claim-text><claim-text>wherein the central controller is configured to provide control instructions to the first and the second control layers at a regular control rate, and</claim-text><claim-text>wherein the reconfigurable first and second audio data processors operate at an audio block rate calculating output audio samples from input audio samples received from an input buffer of the reconfigurable first or second audio data processor, wherein the output samples are stored in an output buffer of the reconfigurable first or second audio data processor, wherein the control rate is lower than the audio block rate.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the central controller is configured to trigger the switch control at a certain time period subsequent to controlling the first and the second control layers to prepare the second configuration, or in response to a ready signal received from the first and the second pipeline stages indicating that the first and the second pipeline stages are ready for the chance to the corresponding second configuration.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first or the second pipeline stage is configured to create a list of output render items from a list of input render items,</claim-text><claim-text>wherein the creating comprises changing meta data for render items of the input list and writing changed meta data in the output list, or</claim-text><claim-text>comprises calculating output audio data for the render items using input audio data retrieved from an input stream buffer of the input render list and writing the output audio data into an output stream buffer of the output render list.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first or the second control layer is configured to control the first or the second reconfigurable audio data processor to fade in a new render item to be processed subsequent to the switch control or to fade out an old render item not existing anymore subsequent to the switch control, but existing before the switch control.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>where each render item of a list of render items comprises, in an input list or an output list of the first or the second render stage, a state indicator indicating at least one of the following states: rendering is active, rendering is to be activated, rendering is inactive, rendering is to be deactivated.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the central controller is configured to fill, in response to a request from the first or the second rendering stage, input buffers of render items maintained by the central controller with new samples, and</claim-text><claim-text>wherein the central controller is configured to trigger the reconfigurable first and second audio data processors sequentially, so that the configurable first and second audio data processors act on corresponding input buffers of the render items in accordance with the first or the second configuration depending on which configuration is currently active.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the second pipeline stage is a spatializer stage that provides, as an output, a channel representation for a headphone reproduction or a loudspeaker set up.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first and the second pipeline stages comprise at least one of the following groups of stages:</claim-text><claim-text>a transmission stage, an extent stage, an early reflection stage, a clustering stage, a diffraction stage, a propagation stage, a spatializer stage, a limiter stage, and a visualization stage.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first pipeline stage is a directivity stage for one or more render items, and wherein the second pipeline stage is a propagation stage for one or more render items,</claim-text><claim-text>wherein the central controller is configured to receive a change of the audio scene indicating that the one or more render items have one or more new positions,</claim-text><claim-text>wherein the central controller is configured to control the first control layer and the second control layer to adapt filter settings for the first and the second reconfigurable audio data processors to the one or more new positions, and</claim-text><claim-text>wherein the first control layer or the second control layer are configured to change to the second configuration at the certain time instant, wherein, when changing to the second configuration, a crossfade operation from the first configuration to the second configuration is performed in the reconfigurable first or second audio data processor.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first pipeline stage is a directivity stage, and the second pipeline stage is a clustering stage,<claim-text>wherein the central controller is configured to receive a change of the audio scene indicating that a clustering of the render items is to be stopped, and</claim-text><claim-text>wherein the central controller is configured to control the first control layer to deactivate the reconfigurable audio data processor of the clustering stage, and to copy an input list of render items into an output list of render items of the second pipeline stage.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. Apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>,<claim-text>wherein the first pipeline stage is a reverb stage, and wherein the second pipeline stage is an early reflections stage,</claim-text><claim-text>wherein the central controller is configured to receive a change of the audio scene indicating that an additional image source is to be added, and</claim-text><claim-text>wherein the central controller is configured to control the control layer of the second pipeline stage to multiply a render item from the input render list to acquire a multiplied render item, and to add the multiplied render item to an output render list of the second pipeline stage.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. Method of rendering a sound scene using an apparatus comprising a first pipeline stage comprising a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor; a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage comprising a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor, comprising:<claim-text>controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and</claim-text><claim-text>controlling the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. Non-transitory digital storage medium having a computer program stored thereon to perform the method of rendering a sound scene using an apparatus comprising a first pipeline stage comprising a first control layer and a reconfigurable first audio data processor, wherein the reconfigurable first audio data processor is configured to operate in accordance with a first configuration of the reconfigurable first audio data processor; a second pipeline stage located, with respect to a pipeline flow, subsequent to the first pipeline stage, the second pipeline stage comprising a second control layer and a reconfigurable second audio data processor, wherein the reconfigurable second audio data processor is configured to operate in accordance with a first configuration of the reconfigurable second audio data processor, comprising:<claim-text>controlling the first control layer and the second control layer in response to the sound scene, so that the first control layer prepares a second configuration of the reconfigurable first audio data processor during or subsequent to an operation of the reconfigurable first audio data processor in the first configuration of the reconfigurable first audio data processor, or so that the second control layer prepares a second configuration of the reconfigurable second audio data processor during or subsequent to an operation of the reconfigurable second audio data processor in the first configuration of the reconfigurable second audio data processor, and</claim-text><claim-text>controlling the first control layer or the second control layer using a switch control to reconfigure the reconfigurable first audio data processor to the second configuration for the reconfigurable first audio data processor or to reconfigure the reconfigurable second audio data processor to the second configuration for the reconfigurable second audio data processor at a certain time instant,</claim-text><claim-text>when said computer program is run by a computer.</claim-text></claim-text></claim></claims></us-patent-application>