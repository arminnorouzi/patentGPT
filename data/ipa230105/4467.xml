<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004468A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004468</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17930921</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>26</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>23</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>9</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>Q</subclass><main-group>10</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>36</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>26</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2379</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>1425</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>1433</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3495</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>577</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>079</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>0793</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3409</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>63</main-group><subgroup>1416</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>0709</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>076</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>Q</subclass><main-group>10</main-group><subgroup>06393</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3476</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3668</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2221</main-group><subgroup>034</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">IDENTIFYING AND COLLECTING DATA FROM ASSETS OF A SYSTEM UNDER EVALUATION BY A SYSTEM ANALYSIS SYSTEM</invention-title><us-related-documents><continuation-in-part><relation><parent-doc><document-id><country>US</country><doc-number>17360815</doc-number><date>20210628</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17930921</doc-number></document-id></child-doc></relation></continuation-in-part><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17128471</doc-number><date>20201221</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17360815</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>63268634</doc-number><date>20220228</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62992661</doc-number><date>20200320</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UncommonX Inc.</orgname><address><city>Chicago</city><state>IL</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Hicks</last-name><first-name>Raymond</first-name><address><city>Chicago</city><state>IL</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Pisani</last-name><first-name>Ryan Michael</first-name><address><city>Burlington</city><state>WI</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>McNeela</last-name><first-name>Thomas James</first-name><address><city>Streamwood</city><state>IL</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>UncommonX Inc.</orgname><role>02</role><address><city>Chicago</city><state>IL</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An analysis system determines a system aspect of a system, determines an evaluation perspective for use in performing an asset management evaluation on the system aspect relating to a build of the system, an evaluation viewpoint corresponding to discovered information of the system and selects a plurality of data structures identifying data to be collected based thereupon. The analysis system, based upon the system aspect, the evaluation perspective, the evaluation viewpoint, and the plurality of data structures, determining context data. Based upon the plurality of data structures, the analysis system identifies a plurality of physical assets of the system for collection of data, queries the plurality of physical assets of the system to collect data to populate the plurality of data structures. The analysis system evaluates the data structures using the context data to produce an evaluation of at least some of the plurality of physical assets of the system.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="115.99mm" wi="158.75mm" file="US20230004468A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="226.06mm" wi="168.40mm" orientation="landscape" file="US20230004468A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="214.88mm" wi="153.08mm" orientation="landscape" file="US20230004468A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="218.10mm" wi="161.46mm" orientation="landscape" file="US20230004468A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="232.33mm" wi="159.85mm" orientation="landscape" file="US20230004468A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="233.93mm" wi="148.34mm" orientation="landscape" file="US20230004468A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="232.33mm" wi="163.15mm" orientation="landscape" file="US20230004468A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="229.28mm" wi="162.05mm" orientation="landscape" file="US20230004468A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="227.58mm" wi="152.57mm" orientation="landscape" file="US20230004468A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="227.67mm" wi="148.51mm" orientation="landscape" file="US20230004468A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="232.33mm" wi="167.39mm" orientation="landscape" file="US20230004468A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="232.33mm" wi="149.35mm" orientation="landscape" file="US20230004468A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="232.41mm" wi="150.20mm" orientation="landscape" file="US20230004468A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="232.33mm" wi="150.20mm" orientation="landscape" file="US20230004468A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="232.33mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="232.33mm" wi="162.56mm" orientation="landscape" file="US20230004468A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="232.33mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="232.33mm" wi="162.56mm" orientation="landscape" file="US20230004468A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="232.33mm" wi="162.56mm" orientation="landscape" file="US20230004468A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="232.58mm" wi="162.48mm" orientation="landscape" file="US20230004468A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="227.58mm" wi="159.43mm" orientation="landscape" file="US20230004468A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="213.36mm" wi="165.27mm" orientation="landscape" file="US20230004468A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="232.33mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="232.33mm" wi="165.78mm" orientation="landscape" file="US20230004468A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="219.71mm" wi="165.78mm" orientation="landscape" file="US20230004468A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="229.19mm" wi="168.40mm" orientation="landscape" file="US20230004468A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="232.33mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="215.31mm" wi="165.44mm" orientation="landscape" file="US20230004468A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="232.33mm" wi="166.62mm" orientation="landscape" file="US20230004468A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="172.13mm" wi="139.78mm" orientation="landscape" file="US20230004468A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="172.13mm" wi="145.97mm" orientation="landscape" file="US20230004468A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="191.09mm" wi="156.21mm" orientation="landscape" file="US20230004468A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="231.06mm" wi="131.91mm" orientation="landscape" file="US20230004468A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="196.00mm" wi="72.73mm" orientation="landscape" file="US20230004468A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="193.97mm" wi="149.35mm" orientation="landscape" file="US20230004468A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="207.01mm" wi="131.74mm" orientation="landscape" file="US20230004468A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="207.01mm" wi="144.36mm" orientation="landscape" file="US20230004468A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="226.14mm" wi="141.22mm" orientation="landscape" file="US20230004468A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="232.33mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="182.96mm" wi="154.60mm" orientation="landscape" file="US20230004468A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="224.11mm" wi="160.27mm" orientation="landscape" file="US20230004468A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="206.93mm" wi="144.36mm" orientation="landscape" file="US20230004468A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="232.33mm" wi="165.78mm" orientation="landscape" file="US20230004468A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="232.41mm" wi="165.78mm" orientation="landscape" file="US20230004468A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="228.77mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="230.38mm" wi="126.58mm" orientation="landscape" file="US20230004468A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="219.63mm" wi="127.08mm" orientation="landscape" file="US20230004468A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00047" num="00047"><img id="EMI-D00047" he="219.63mm" wi="160.95mm" orientation="landscape" file="US20230004468A1-20230105-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00048" num="00048"><img id="EMI-D00048" he="219.71mm" wi="157.82mm" orientation="landscape" file="US20230004468A1-20230105-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00049" num="00049"><img id="EMI-D00049" he="203.79mm" wi="127.08mm" orientation="landscape" file="US20230004468A1-20230105-D00049.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00050" num="00050"><img id="EMI-D00050" he="203.79mm" wi="125.48mm" orientation="landscape" file="US20230004468A1-20230105-D00050.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00051" num="00051"><img id="EMI-D00051" he="210.23mm" wi="128.69mm" orientation="landscape" file="US20230004468A1-20230105-D00051.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00052" num="00052"><img id="EMI-D00052" he="159.34mm" wi="92.20mm" orientation="landscape" file="US20230004468A1-20230105-D00052.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00053" num="00053"><img id="EMI-D00053" he="232.33mm" wi="125.39mm" orientation="landscape" file="US20230004468A1-20230105-D00053.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00054" num="00054"><img id="EMI-D00054" he="232.33mm" wi="111.08mm" orientation="landscape" file="US20230004468A1-20230105-D00054.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00055" num="00055"><img id="EMI-D00055" he="232.33mm" wi="97.45mm" orientation="landscape" file="US20230004468A1-20230105-D00055.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00056" num="00056"><img id="EMI-D00056" he="232.33mm" wi="97.45mm" orientation="landscape" file="US20230004468A1-20230105-D00056.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00057" num="00057"><img id="EMI-D00057" he="232.33mm" wi="97.54mm" orientation="landscape" file="US20230004468A1-20230105-D00057.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00058" num="00058"><img id="EMI-D00058" he="232.33mm" wi="97.54mm" orientation="landscape" file="US20230004468A1-20230105-D00058.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00059" num="00059"><img id="EMI-D00059" he="233.85mm" wi="158.50mm" orientation="landscape" file="US20230004468A1-20230105-D00059.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00060" num="00060"><img id="EMI-D00060" he="231.31mm" wi="150.20mm" orientation="landscape" file="US20230004468A1-20230105-D00060.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00061" num="00061"><img id="EMI-D00061" he="221.57mm" wi="159.09mm" orientation="landscape" file="US20230004468A1-20230105-D00061.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00062" num="00062"><img id="EMI-D00062" he="229.02mm" wi="172.04mm" orientation="landscape" file="US20230004468A1-20230105-D00062.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00063" num="00063"><img id="EMI-D00063" he="232.41mm" wi="159.17mm" orientation="landscape" file="US20230004468A1-20230105-D00063.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00064" num="00064"><img id="EMI-D00064" he="192.70mm" wi="158.41mm" orientation="landscape" file="US20230004468A1-20230105-D00064.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00065" num="00065"><img id="EMI-D00065" he="203.88mm" wi="156.46mm" orientation="landscape" file="US20230004468A1-20230105-D00065.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00066" num="00066"><img id="EMI-D00066" he="203.88mm" wi="156.46mm" orientation="landscape" file="US20230004468A1-20230105-D00066.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00067" num="00067"><img id="EMI-D00067" he="205.40mm" wi="156.46mm" orientation="landscape" file="US20230004468A1-20230105-D00067.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00068" num="00068"><img id="EMI-D00068" he="205.40mm" wi="156.46mm" orientation="landscape" file="US20230004468A1-20230105-D00068.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00069" num="00069"><img id="EMI-D00069" he="203.88mm" wi="156.46mm" orientation="landscape" file="US20230004468A1-20230105-D00069.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00070" num="00070"><img id="EMI-D00070" he="203.88mm" wi="156.46mm" orientation="landscape" file="US20230004468A1-20230105-D00070.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00071" num="00071"><img id="EMI-D00071" he="157.73mm" wi="117.35mm" orientation="landscape" file="US20230004468A1-20230105-D00071.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00072" num="00072"><img id="EMI-D00072" he="232.33mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00072.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00073" num="00073"><img id="EMI-D00073" he="219.71mm" wi="134.87mm" orientation="landscape" file="US20230004468A1-20230105-D00073.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00074" num="00074"><img id="EMI-D00074" he="181.61mm" wi="163.91mm" orientation="landscape" file="US20230004468A1-20230105-D00074.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00075" num="00075"><img id="EMI-D00075" he="227.58mm" wi="137.16mm" orientation="landscape" file="US20230004468A1-20230105-D00075.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00076" num="00076"><img id="EMI-D00076" he="232.33mm" wi="136.48mm" orientation="landscape" file="US20230004468A1-20230105-D00076.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00077" num="00077"><img id="EMI-D00077" he="232.49mm" wi="146.30mm" orientation="landscape" file="US20230004468A1-20230105-D00077.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00078" num="00078"><img id="EMI-D00078" he="232.49mm" wi="162.22mm" orientation="landscape" file="US20230004468A1-20230105-D00078.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00079" num="00079"><img id="EMI-D00079" he="184.74mm" wi="126.92mm" orientation="landscape" file="US20230004468A1-20230105-D00079.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00080" num="00080"><img id="EMI-D00080" he="229.79mm" wi="158.50mm" orientation="landscape" file="US20230004468A1-20230105-D00080.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00081" num="00081"><img id="EMI-D00081" he="223.52mm" wi="156.72mm" orientation="landscape" file="US20230004468A1-20230105-D00081.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00082" num="00082"><img id="EMI-D00082" he="222.84mm" wi="158.50mm" orientation="landscape" file="US20230004468A1-20230105-D00082.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00083" num="00083"><img id="EMI-D00083" he="232.33mm" wi="162.56mm" orientation="landscape" file="US20230004468A1-20230105-D00083.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00084" num="00084"><img id="EMI-D00084" he="232.33mm" wi="154.43mm" orientation="landscape" file="US20230004468A1-20230105-D00084.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00085" num="00085"><img id="EMI-D00085" he="105.33mm" wi="153.08mm" orientation="landscape" file="US20230004468A1-20230105-D00085.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00086" num="00086"><img id="EMI-D00086" he="222.84mm" wi="161.63mm" orientation="landscape" file="US20230004468A1-20230105-D00086.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00087" num="00087"><img id="EMI-D00087" he="232.33mm" wi="129.03mm" orientation="landscape" file="US20230004468A1-20230105-D00087.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00088" num="00088"><img id="EMI-D00088" he="105.33mm" wi="129.03mm" orientation="landscape" file="US20230004468A1-20230105-D00088.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00089" num="00089"><img id="EMI-D00089" he="222.84mm" wi="161.63mm" orientation="landscape" file="US20230004468A1-20230105-D00089.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00090" num="00090"><img id="EMI-D00090" he="232.33mm" wi="132.84mm" orientation="landscape" file="US20230004468A1-20230105-D00090.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00091" num="00091"><img id="EMI-D00091" he="113.28mm" wi="132.84mm" orientation="landscape" file="US20230004468A1-20230105-D00091.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00092" num="00092"><img id="EMI-D00092" he="232.33mm" wi="161.63mm" orientation="landscape" file="US20230004468A1-20230105-D00092.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00093" num="00093"><img id="EMI-D00093" he="230.80mm" wi="108.12mm" orientation="landscape" file="US20230004468A1-20230105-D00093.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00094" num="00094"><img id="EMI-D00094" he="105.41mm" wi="108.12mm" orientation="landscape" file="US20230004468A1-20230105-D00094.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00095" num="00095"><img id="EMI-D00095" he="184.74mm" wi="96.60mm" orientation="landscape" file="US20230004468A1-20230105-D00095.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00096" num="00096"><img id="EMI-D00096" he="219.71mm" wi="126.07mm" orientation="landscape" file="US20230004468A1-20230105-D00096.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00097" num="00097"><img id="EMI-D00097" he="229.02mm" wi="143.00mm" orientation="landscape" file="US20230004468A1-20230105-D00097.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00098" num="00098"><img id="EMI-D00098" he="230.63mm" wi="160.27mm" orientation="landscape" file="US20230004468A1-20230105-D00098.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00099" num="00099"><img id="EMI-D00099" he="229.19mm" wi="144.19mm" orientation="landscape" file="US20230004468A1-20230105-D00099.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00100" num="00100"><img id="EMI-D00100" he="227.58mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00100.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00101" num="00101"><img id="EMI-D00101" he="226.06mm" wi="165.78mm" orientation="landscape" file="US20230004468A1-20230105-D00101.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00102" num="00102"><img id="EMI-D00102" he="229.28mm" wi="117.86mm" orientation="landscape" file="US20230004468A1-20230105-D00102.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00103" num="00103"><img id="EMI-D00103" he="207.01mm" wi="149.86mm" orientation="landscape" file="US20230004468A1-20230105-D00103.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00104" num="00104"><img id="EMI-D00104" he="216.49mm" wi="157.82mm" orientation="landscape" file="US20230004468A1-20230105-D00104.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00105" num="00105"><img id="EMI-D00105" he="203.79mm" wi="150.71mm" orientation="landscape" file="US20230004468A1-20230105-D00105.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00106" num="00106"><img id="EMI-D00106" he="215.56mm" wi="157.82mm" orientation="landscape" file="US20230004468A1-20230105-D00106.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00107" num="00107"><img id="EMI-D00107" he="217.93mm" wi="168.91mm" orientation="landscape" file="US20230004468A1-20230105-D00107.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00108" num="00108"><img id="EMI-D00108" he="230.80mm" wi="162.48mm" orientation="landscape" file="US20230004468A1-20230105-D00108.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00109" num="00109"><img id="EMI-D00109" he="184.74mm" wi="153.75mm" orientation="landscape" file="US20230004468A1-20230105-D00109.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00110" num="00110"><img id="EMI-D00110" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00110.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00111" num="00111"><img id="EMI-D00111" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00111.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00112" num="00112"><img id="EMI-D00112" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00112.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00113" num="00113"><img id="EMI-D00113" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00113.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00114" num="00114"><img id="EMI-D00114" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00114.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00115" num="00115"><img id="EMI-D00115" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00115.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00116" num="00116"><img id="EMI-D00116" he="187.96mm" wi="153.84mm" orientation="landscape" file="US20230004468A1-20230105-D00116.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00117" num="00117"><img id="EMI-D00117" he="138.51mm" wi="153.92mm" orientation="landscape" file="US20230004468A1-20230105-D00117.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00118" num="00118"><img id="EMI-D00118" he="129.03mm" wi="141.65mm" orientation="landscape" file="US20230004468A1-20230105-D00118.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00119" num="00119"><img id="EMI-D00119" he="179.15mm" wi="167.56mm" orientation="landscape" file="US20230004468A1-20230105-D00119.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00120" num="00120"><img id="EMI-D00120" he="179.24mm" wi="167.56mm" orientation="landscape" file="US20230004468A1-20230105-D00120.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00121" num="00121"><img id="EMI-D00121" he="179.15mm" wi="136.31mm" orientation="landscape" file="US20230004468A1-20230105-D00121.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00122" num="00122"><img id="EMI-D00122" he="159.51mm" wi="120.48mm" orientation="landscape" file="US20230004468A1-20230105-D00122.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00123" num="00123"><img id="EMI-D00123" he="185.84mm" wi="144.10mm" orientation="landscape" file="US20230004468A1-20230105-D00123.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00124" num="00124"><img id="EMI-D00124" he="182.46mm" wi="111.51mm" orientation="landscape" file="US20230004468A1-20230105-D00124.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00125" num="00125"><img id="EMI-D00125" he="182.46mm" wi="113.54mm" orientation="landscape" file="US20230004468A1-20230105-D00125.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00126" num="00126"><img id="EMI-D00126" he="182.46mm" wi="129.54mm" orientation="landscape" file="US20230004468A1-20230105-D00126.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00127" num="00127"><img id="EMI-D00127" he="182.46mm" wi="122.43mm" orientation="landscape" file="US20230004468A1-20230105-D00127.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00128" num="00128"><img id="EMI-D00128" he="182.46mm" wi="116.33mm" orientation="landscape" file="US20230004468A1-20230105-D00128.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00129" num="00129"><img id="EMI-D00129" he="182.46mm" wi="119.46mm" orientation="landscape" file="US20230004468A1-20230105-D00129.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00130" num="00130"><img id="EMI-D00130" he="182.46mm" wi="122.43mm" orientation="landscape" file="US20230004468A1-20230105-D00130.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00131" num="00131"><img id="EMI-D00131" he="128.86mm" wi="155.19mm" orientation="landscape" file="US20230004468A1-20230105-D00131.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00132" num="00132"><img id="EMI-D00132" he="203.03mm" wi="165.02mm" orientation="landscape" file="US20230004468A1-20230105-D00132.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00133" num="00133"><img id="EMI-D00133" he="158.50mm" wi="147.74mm" orientation="landscape" file="US20230004468A1-20230105-D00133.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00134" num="00134"><img id="EMI-D00134" he="209.30mm" wi="160.36mm" orientation="landscape" file="US20230004468A1-20230105-D00134.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00135" num="00135"><img id="EMI-D00135" he="209.21mm" wi="160.27mm" orientation="landscape" file="US20230004468A1-20230105-D00135.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00136" num="00136"><img id="EMI-D00136" he="209.30mm" wi="163.66mm" orientation="landscape" file="US20230004468A1-20230105-D00136.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00137" num="00137"><img id="EMI-D00137" he="209.30mm" wi="163.66mm" orientation="landscape" file="US20230004468A1-20230105-D00137.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00138" num="00138"><img id="EMI-D00138" he="209.30mm" wi="163.66mm" orientation="landscape" file="US20230004468A1-20230105-D00138.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00139" num="00139"><img id="EMI-D00139" he="209.30mm" wi="163.58mm" orientation="landscape" file="US20230004468A1-20230105-D00139.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00140" num="00140"><img id="EMI-D00140" he="199.81mm" wi="142.32mm" orientation="landscape" file="US20230004468A1-20230105-D00140.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present U.S. Utility patent application claims priority pursuant to 35 U.S.C. &#xa7; 119(e) to U.S. Provisional Application No. 63/268,634, entitled &#x201c;IDENTIFYING AND COLLECTING DATA FROM ASSETS OF A SYSTEM UNDER EVALUATION BY A SYSTEM ANALYSIS SYSTEM&#x201d;, filed Feb. 28, 2022, which is incorporated herein by reference in its entirety and made part of the present U.S. Utility patent application for all purposes.</p><p id="p-0003" num="0002">The present U.S. Utility patent application claims priority pursuant to 35 U.S.C. &#xa7; 120 as a continuation-in-part of U.S. Utility application Ser. No. 17/360,815, entitled &#x201c;GENERATION OF AN ASSET MANAGEMENT EVALUATION REGARDING A SYSTEM ASPECT OF A SYSTEM&#x201d;, filed Jun. 28, 2021, which is a continuation of U.S. Utility application Ser. No. 17/128,471, entitled &#x201c;GENERATION OF AN IDENTIFICATION EVALUATION REGARDING A SYSTEM ASPECT OF A SYSTEM&#x201d;, filed Dec. 21, 2020, which claims priority pursuant to 35 U.S.C. &#xa7; 119(e) to U.S. Provisional Application No. 62/992,661, entitled &#x201c;SYSTEM ANALYSIS SYSTEM&#x201d;, filed Mar. 20, 2020, all of which are hereby incorporated herein by reference in their entirety and made part of the present U.S. Utility patent application for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">STATEMENT REGARDING FEDERALLY SPONSORED RESEARCH OR DEVELOPMENT</heading><p id="p-0004" num="0003">Not Applicable.</p><heading id="h-0003" level="1">INCORPORATION-BY-REFERENCE OF MATERIAL SUBMITTED ON A COMPACT DISC</heading><p id="p-0005" num="0004">Not Applicable.</p><heading id="h-0004" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0005" level="1">Technical Field of the Invention</heading><p id="p-0006" num="0005">This disclosure relates to computer systems and more particularly to evaluation of a computer system.</p><heading id="h-0006" level="1">Description of Related Art</heading><p id="p-0007" num="0006">The structure and operation of the Internet and other publicly available networks are well known and support computer systems (systems) of multitudes of companies, organizations, and individuals. A typical system includes networking equipment, end point devices such as computer servers, user computers, storage devices, printing devices, security devices, and point of service devices, among other types of devices. The networking equipment includes routers, switches, edge devices, wireless access points, and other types of communication devices that intercouple in a wired or wireless fashion. The networking equipment facilitates the creation of one or more networks that are tasked to service all or a portion of a company's communication needs, e.g., Wide Area Networks, Local Area Networks, Virtual Private Networks, etc.</p><p id="p-0008" num="0007">Each device within a system includes hardware components and software components. Hardware components degrade over time and eventually are incapable of performing their intended functions. Software components must be updated regularly to ensure their proper functionality. Some software components are simply replaced by newer and better software even though they remain operational within a system.</p><p id="p-0009" num="0008">Many companies and larger organizations have their own Information Technology (IT) departments. Others outsource their IT needs to third party providers. The knowledge requirements for servicing a system typically outstrip the abilities of the IT department or third-party provider. Thus, hardware and software may not be functioning properly and can adversely affect the overall system.</p><p id="p-0010" num="0009">Cyber-attacks are initiated by individuals or entities with the bad intent of stealing sensitive information such as login/password information, stealing proprietary information such as trade secrets or important new technology, interfering with the operation of a system, and/or holding the system hostage until a ransom is paid, among other improper purposes. A single cyber-attack can make a large system inoperable and cost the system owner many millions of dollars to restore and remedy.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0007" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWING(S)</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic block diagram of an embodiment of a networked environment that includes systems coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref> are schematic block diagrams of embodiments of a computing device in accordance with the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>E</figref> are schematic block diagrams of embodiments of a computing entity in accordance with the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system that includes a plurality of system elements in accordance with the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic block diagram of an example of a system section of a system selected for evaluation in accordance with the present disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic block diagram of another example of a system section of a system selected for evaluation in accordance with the present disclosure;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic block diagram of an embodiment of a networked environment having a system that includes a plurality of system assets coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic block diagram of an embodiment of a system that includes a plurality of physical assets coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system that includes a plurality of system assets coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic block diagram of another embodiment of a system that includes a plurality of physical assets coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a schematic block diagram of another embodiment of a system that includes a plurality of physical assets coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a schematic block diagram of another embodiment of a system that includes a plurality of physical assets in accordance with the present disclosure;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic block diagram of an embodiment of a user computing device in accordance with the present disclosure;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a schematic block diagram of an embodiment of a server in accordance with the present disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system that includes a plurality of system functions coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a schematic block diagram of another embodiment of a system that includes divisions, departments, and groups in accordance with the present disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a schematic block diagram of another embodiment of a system that includes divisions and departments, which include system elements in accordance with the present disclosure;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a schematic block diagram of another embodiment of a division of a system having departments, which include system elements in accordance with the present disclosure;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system that includes a plurality of security functions coupled to an analysis system in accordance with the present disclosure;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a schematic block diagram of an embodiment an engineering department of a division that reports to a corporate department of a system in accordance with the present disclosure;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a schematic block diagram of an example of an analysis system evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a schematic block diagram of another example of an analysis system evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a schematic block diagram of another example of an analysis system evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a schematic block diagram of another example of an analysis system evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a schematic block diagram of an example of the functioning of an analysis system evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a schematic block diagram of another example of the functioning of an analysis system evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram of an example of evaluation options of an analysis system for evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a diagram of another example of evaluation options of an analysis system for evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram of another example of evaluation options of an analysis system for evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a diagram of another example of evaluation options of an analysis system for evaluating a system element under test of a system in accordance with the present disclosure;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a schematic block diagram of an embodiment of an analysis system coupled to a system in accordance with the present disclosure;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a schematic block diagram of an embodiment of a portion of an analysis system coupled to a system in accordance with the present disclosure;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a schematic block diagram of another embodiment of a portion of an analysis system coupled to a system in accordance with the present disclosure;</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a schematic block diagram of an embodiment of a data extraction module of an analysis system coupled to a system in accordance with the present disclosure;</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>39</b></figref> is a schematic block diagram of another embodiment of an analysis system coupled to a system in accordance with the present disclosure;</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>40</b></figref> is a schematic block diagram of another embodiment of an analysis system coupled to a system in accordance with the present disclosure;</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>41</b></figref> is a schematic block diagram of an embodiment of a data analysis module of an analysis system in accordance with the present disclosure;</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>42</b></figref> is a schematic block diagram of an embodiment of an analyze and score module of an analysis system in accordance with the present disclosure;</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>43</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>44</b></figref> is a diagram of another example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>45</b></figref> is a diagram of an example of an identification evaluation category, sub-categories, and sub-sub-categories of the evaluation aspects and in accordance with the present disclosure;</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>46</b></figref> is a diagram of an example of a protect evaluation category, sub-categories, and sub-sub-categories of the evaluation aspects and in accordance with the present disclosure;</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>47</b></figref> is a diagram of an example of a detect evaluation category, sub-categories, and sub-sub-categories of the evaluation aspects and in accordance with the present disclosure;</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>48</b></figref> is a diagram of an example of a respond evaluation category, sub-categories, and sub-sub-categories of the evaluation aspects and in accordance with the present disclosure;</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>49</b></figref> is a diagram of an example of a recover evaluation category, sub-categories, and sub-sub-categories of the evaluation aspects and in accordance with the present disclosure;</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>50</b></figref> is a diagram of a specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>51</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>52</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>53</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>54</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>55</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>56</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>57</b></figref> is a diagram of an example of identifying deficiencies and auto-corrections by an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>58</b></figref> is a schematic block diagram of an embodiment of an evaluation processing module of an analysis system in accordance with the present disclosure;</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>59</b></figref> is a state diagram of an example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>60</b></figref> is a logic diagram of an example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>61</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>62</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>63</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>64</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>65</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>66</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>67</b></figref> is a logic diagram of another example of an analysis system analyzing a section of a system in accordance with the present disclosure;</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>68</b></figref> is a logic diagram of an example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>69</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>70</b></figref> is a schematic block diagram of an example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>71</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for generating an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>72</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>73</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>74</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>75</b></figref> is a diagram of an example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>76</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>77</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>78</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>79</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>80</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>81</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>82</b></figref> is a schematic block diagram of an embodiment of a data analysis module of an analysis system in accordance with the present disclosure;</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>83</b></figref> is a schematic block diagram of another embodiment of a data analysis module of an analysis system in accordance with the present disclosure;</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>84</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>85</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>86</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>87</b></figref> is a diagram of an example of identification data for use by an analysis system to generate an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>88</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>89</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>90</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>91</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>92</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>93</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>94</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>95</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>96</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>97</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>98</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>99</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. <b>100</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>101</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>102</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>103</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0114" num="0113"><figref idref="DRAWINGS">FIG. <b>104</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>105</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>106</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>107</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. <b>108</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>109</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. <b>110</b></figref> is a diagram of another specific example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system for generating an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0121" num="0120"><figref idref="DRAWINGS">FIG. <b>111</b></figref> is a diagram of an example of combining one or more individual identification ratings into an identification rating in accordance with the present disclosure;</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. <b>112</b></figref> is a diagram of another example of combining one or more individual identification ratings into an identification rating in accordance with the present disclosure;</p><p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. <b>113</b></figref> is a diagram of another example of combining one or more individual identification ratings into an identification rating in accordance with the present disclosure;</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>114</b></figref> is a diagram of another example of combining one or more individual identification ratings into an identification rating in accordance with the present disclosure;</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>115</b></figref> is a diagram of another example of combining one or more individual identification ratings into an identification rating in accordance with the present disclosure;</p><p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. <b>116</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. <b>117</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. <b>118</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0129" num="0128"><figref idref="DRAWINGS">FIG. <b>119</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0130" num="0129"><figref idref="DRAWINGS">FIG. <b>120</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0131" num="0130"><figref idref="DRAWINGS">FIG. <b>121</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0132" num="0131"><figref idref="DRAWINGS">FIG. <b>122</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0133" num="0132"><figref idref="DRAWINGS">FIG. <b>123</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0134" num="0133"><figref idref="DRAWINGS">FIG. <b>124</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0135" num="0134"><figref idref="DRAWINGS">FIG. <b>125</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. <b>126</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. <b>127</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. <b>128</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>129</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. <b>130</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0141" num="0140"><figref idref="DRAWINGS">FIG. <b>131</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0142" num="0141"><figref idref="DRAWINGS">FIG. <b>132</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0143" num="0142"><figref idref="DRAWINGS">FIG. <b>133</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0144" num="0143"><figref idref="DRAWINGS">FIG. <b>134</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. <b>135</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0146" num="0145"><figref idref="DRAWINGS">FIG. <b>136</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0147" num="0146"><figref idref="DRAWINGS">FIG. <b>137</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0148" num="0147"><figref idref="DRAWINGS">FIG. <b>138</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. <b>139</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. <b>140</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>141</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure;</p><p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. <b>142</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure; and</p><p id="p-0153" num="0152"><figref idref="DRAWINGS">FIG. <b>143</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a section of a system in accordance with the present disclosure.</p><p id="p-0154" num="0153"><figref idref="DRAWINGS">FIG. <b>144</b></figref> is a logic diagram illustrating an example of an analysis system collecting data from physical assets of a system and evaluating collected data to produce an output in accordance with the present disclosure.</p><p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. <b>145</b>A</figref> is a logic diagram illustrating an example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure.</p><p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. <b>145</b>B</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure.</p><p id="p-0157" num="0156"><figref idref="DRAWINGS">FIG. <b>145</b>C</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>145</b>D</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure.</p><p id="p-0159" num="0158"><figref idref="DRAWINGS">FIG. <b>145</b>E</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure.</p><p id="p-0160" num="0159"><figref idref="DRAWINGS">FIG. <b>146</b></figref> is a block diagram illustrating an example of a data structure storing data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. <b>147</b></figref> is a block diagram illustrating an example of a data structure storing hardware data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0162" num="0161"><figref idref="DRAWINGS">FIG. <b>148</b></figref> is a block diagram illustrating an example of a data structure storing software data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0163" num="0162"><figref idref="DRAWINGS">FIG. <b>149</b></figref> is a block diagram illustrating an example of a data structure storing network hardware data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0164" num="0163"><figref idref="DRAWINGS">FIG. <b>150</b></figref> is a block diagram illustrating an example of a data structure storing network software data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. <b>151</b></figref> is a block diagram illustrating an example of a data structure storing permissions data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0166" num="0165"><figref idref="DRAWINGS">FIG. <b>152</b></figref> is a block diagram illustrating an example of a data structure storing security hardware data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. <b>153</b></figref> is a block diagram illustrating an example of a data structure storing security software data corresponding to a physical asset in accordance with the present disclosure.</p><p id="p-0168" num="0167"><figref idref="DRAWINGS">FIG. <b>154</b></figref> is a logic diagram illustrating an example of an analysis system discovering and collecting basic attribute data from physical assets of a system in accordance with the present disclosure.</p><p id="p-0169" num="0168"><figref idref="DRAWINGS">FIG. <b>155</b></figref> is a logic diagram illustrating an example of an analysis system collecting system design data and reference data relevant to physical assets of a system in accordance with the present disclosure.</p><p id="p-0170" num="0169"><figref idref="DRAWINGS">FIG. <b>156</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for physical assets of a system in accordance with the present disclosure.</p><p id="p-0171" num="0170"><figref idref="DRAWINGS">FIG. <b>157</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for hardware assets of a system in accordance with the present disclosure.</p><p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. <b>158</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for software assets of a system in accordance with the present disclosure.</p><p id="p-0173" num="0172"><figref idref="DRAWINGS">FIG. <b>159</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for network assets of a system in accordance with the present disclosure.</p><p id="p-0174" num="0173"><figref idref="DRAWINGS">FIG. <b>160</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for security assets of a system in accordance with the present disclosure.</p><p id="p-0175" num="0174"><figref idref="DRAWINGS">FIG. <b>161</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for permissions of a system in accordance with the present disclosure.</p><p id="p-0176" num="0175"><figref idref="DRAWINGS">FIG. <b>162</b></figref> is a logic diagram illustrating an example of an analysis system comparing discovered data to design data of physical assets of a system in accordance with the present disclosure.</p><p id="p-0177" num="0176"><figref idref="DRAWINGS">FIG. <b>163</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for a particular resource of a system in accordance with the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0008" level="1">DETAILED DESCRIPTION OF THE INVENTION</heading><p id="p-0178" num="0177"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic block diagram of an embodiment of a networked environment that includes one or more networks <b>14</b>, external data feeds sources <b>15</b>, a plurality of systems <b>11</b>-<b>13</b>, and an analysis system <b>10</b>. The external data feed sources <b>15</b> includes one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more non-business associated computing devices <b>24</b> (e.g., publicly available servers <b>27</b> and subscription based servers <b>28</b>), one or more BOT (i.e., internet robot) computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>. The analysis system <b>10</b> includes one or more analysis computing entities <b>16</b>, a plurality of analysis system modules <b>17</b> (one or more in each of the systems <b>11</b>-<b>13</b>), and a plurality of storage systems <b>19</b>-<b>21</b> (e.g., system A private storage <b>19</b>, system B private storage <b>20</b>, through system x private storage <b>21</b>, and other storage). Each of the systems <b>11</b>-<b>13</b> includes one or more network interfaces <b>18</b> and many more elements not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0179" num="0178">A computing device may be implemented in a variety of ways. A few examples are shown in <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>. A computing entity may be implemented in a variety of ways. A few examples are shown in <figref idref="DRAWINGS">FIGS. <b>3</b>A-<b>3</b>E</figref>.</p><p id="p-0180" num="0179">A storage system <b>19</b>-<b>21</b> may be implemented in a variety of ways. For example, each storage system is a standalone database. As another example, the storage systems are implemented in a common database. A database is a centralized database, a distributed database, an operational database, a cloud database, an object-oriented database, and/or a relational database. A storage system <b>19</b>-<b>21</b> is coupled to the analysis system <b>10</b> using a secure data pipeline to limit and control access to the storage systems. The secure data pipeline may be implemented in a variety of ways. For example, the secure data pipeline is implemented on a provide network of the analysis system and/or of a system under test. As another example, the secure data pipeline is implemented via the network <b>14</b> using access control, using network controls, implementing access and control policies, using encryption, using data loss prevention tools, and/or using auditing tools.</p><p id="p-0181" num="0180">The one or more networks <b>14</b> includes one or more wide area networks (WAN), one or more local area networks (LAN), one or more wireless LANs (WLAN), one or more cellular networks, one or more satellite networks, one or more virtual private networks (VPN), one or more campus area networks (CAN), one or more metropolitan area networks (MAN), one or more storage area networks (SAN), one or more enterprise private networks (EPN), and/or one or more other type of networks.</p><p id="p-0182" num="0181">In general, a system proficiency resource <b>22</b> is a source for data regarding best-in-class practices (for system requirements, for system design, for system implementation, and/or for system operation), governmental and/or regulatory requirements, security risk awareness and/or risk remediation information, security risk avoidance, performance optimization information, system development guidelines, software development guideline, hardware requirements, networking requirements, networking guidelines, and/or other system proficiency guidance. &#x201c;Framework for Improving Critical Instructure Cybersecurity&#x201d;, Version 1.1, Apr. 16, 2018 by the National Institute of Standards and Technology (NIST) is an example of a system proficiency in the form of a guideline for cybersecurity.</p><p id="p-0183" num="0182">A business associated computing device <b>23</b> is one that is operated by a business associate of the system owner. Typically, the business associated computing device <b>23</b> has access to at least a limited portion of the system to which the general public does not have access. For example, the business associated computing device <b>23</b> is operated by a vendor of the organization operating the system and is granted limited access for order placement and/or fulfillment. As another example, the business associated computing device <b>23</b> is operated by a customer of the organization operating the system and is granted limited access for placing orders.</p><p id="p-0184" num="0183">A non-business associated computing device <b>24</b> is a computing device operated by a person or entity that does not have a business relationship with the organization operating the system. Such non-business associated computing device <b>24</b> are not granted special access to the system. For example, a non-business associated computing device <b>24</b> is a publicly available server <b>27</b> to which a user computing device of the system may access. As another example, a non-business associated computing device <b>24</b> is a subscription based servers <b>28</b> to which a user computing device of the system may access if it is authorized by a system administrator of the system to have a subscription and has a valid subscription. As yet another example, the non-business associated computing device <b>24</b> is a computing device operated by a person or business that does not have an affiliation with the organization operating the system.</p><p id="p-0185" num="0184">A BOT (i.e., internet robot) computing device <b>25</b> is a computing device that runs, with little to no human interaction, to interact with a system and/or a computing device of a user via the internet or a network. There are a variety of types of BOTS. For example, there are social media BOTS, chatbots, BOT crawlers, transaction BOTS, information BOTS, and entertainment BOTS (e.g., games, art, books, etc.).</p><p id="p-0186" num="0185">A bad actor computing device <b>26</b> is a computing device operated by a person whose use of the computing device is for illegal and/or immoral purposes. The bad actor computing device <b>26</b> may employ a BOT to execute an illegal and/or immoral purpose. In addition or in the alternative, the person may instruct the bad actor computing device to perform the illegal and/or immoral purpose, such as hacking, planting a worm, planting a virus, stealing data, uploading false data, and so on.</p><p id="p-0187" num="0186">The analysis system <b>10</b> is operable to evaluate a system <b>11</b>-<b>13</b>, or portion thereof, in a variety of ways. For example, the analysis system <b>10</b> evaluates system A <b>11</b>, or a portion thereof, by testing the organization's understanding of its system, or portion thereof; by testing the organization's implementation of its system, or portion thereof; and/or by testing the system's, or portion thereof; operation. As a specific example, the analysis system <b>10</b> tests the organization's understanding of its system requirements for the implementation and/or operation of its system, or portion thereof. As another specific example, the analysis system <b>10</b> tests the organization's understanding of its software maintenance policies and/or procedures. As another specific example, the analysis system <b>10</b> tests the organization's understanding of its cybersecurity policies and/or procedures.</p><p id="p-0188" num="0187">There is an almost endless combination of ways in which the analysis system <b>10</b> can evaluate a system <b>11</b>-<b>13</b>, which may be a computer system, a computer network, an enterprise system, and/or other type of system that includes computing devices operating software. For example, the analysis system <b>10</b> evaluates a system aspect (e.g., the system or a portion of it) based on an evaluation aspect (e.g., options for how the system, or portion thereof, can be evaluated) in view of evaluation rating metrics (e.g., how the system, or portion thereof, is evaluated) to produce an analysis system output (e.g., an evaluation rating, deficiency identification, and/or deficiency auto-correction).</p><p id="p-0189" num="0188">The system aspect (e.g., the system or a portion thereof) includes a selection of one or more system elements of the system, a selection of one or more system criteria, and/or a selection of one or more system modes. A system element of the system includes one or more system assets which is a physical asset of the system and/or a conceptual asset of the system. For example, a physical asset is a computing entity, a computing device, a user software application, a system software application (e.g., operating system, etc.), a software tool, a network software application, a security software application, a system monitoring software application, and the like. As another example, a conceptual asset is a hardware architectural layout, or portion thereof, and/or a software architectural layout, or portion thereof.</p><p id="p-0190" num="0189">A system element and/or system asset may be identified in a variety of ways. For example, it is identifiably by its use and/or location within the organization. As a specific example, a system element and/or system asset is identified by an organizational identifier, a division of the organization identifier, a department of a division identifier, a group of a department identifier, and/or a sub-group of a group identifier. In this manner, if the entire system is to be evaluated, the organization identifier is used to select all of the system elements in the system. If a portion of the system is to be test based on business function, then a division, department, group, and/or sub-group identifier is used to select the desired portion of the system.</p><p id="p-0191" num="0190">In addition or in the alternative, a system element and/or system asset is identifiable based on a serial number, an IP (internet protocol) address, a vendor name, a type of system element and/or system asset (e.g., computing entity, a particular user software application, etc.), registered user of the system element and/or system asset, and/or other identifying metric. In this manner, an individual system element and/or system asset can be evaluated and/or a type of system element and/or system asset can be evaluated (e.g., a particular user software application).</p><p id="p-0192" num="0191">A system criteria is regarding a level of the system, or portion thereof, being evaluated. For example, the system criteria includes guidelines, system requirements, system design, system build, and resulting system. As a further example, the guidelines (e.g., business objectives, security objectives, NIST cybersecurity guidelines, system objectives, governmental and/or regulatory requirements, third party requirements, etc.) are used to develop the system requirements, which are used to design the system, which is used to the build the resulting system. As such, the system, or portion thereof, can be evaluated from a guideline level, a system requirements level, a design level, a build level, and/or a resulting system level.</p><p id="p-0193" num="0192">A system mode is regarding a different level of the system, or portion thereof, being evaluated. For example, the system mode includes assets, system functions, and security functions. As such, the system can be evaluated from an assets level, a system function level, and/or a security function level.</p><p id="p-0194" num="0193">The evaluation aspect (e.g., options for how the system, or portion thereof, can be evaluated) includes a selection of one or more evaluation perspectives, a selection of one or more evaluation viewpoints, and/or a selection of one or more evaluation categories, which may further include sub-categories, and sub-categories of the sub-categories). An evaluation perspective is understanding of the system, or portion thereof; implementation (e.g., design and build) of the system, or portion thereof; operational performance of the system, or portion thereof; or self-analysis of the system, or portion thereof.</p><p id="p-0195" num="0194">An evaluation viewpoint is disclosed information from the system, discovered information about the system by the analysis system, or desired information about the system obtained by the analysis system from system proficiency resources. The evaluation viewpoint complements the evaluation perspective to allow for more in-depth and/or detailed evaluations. For example, the analysis system <b>10</b> can evaluate how well the system is understood by comparing disclosed data with discovered data. As another example, the analysis system <b>10</b> can evaluate how well the system is actually implemented in comparison to a desired level of implementation.</p><p id="p-0196" num="0195">The evaluation category includes an identify category, a protect category, a detect category, a respond category, and a recover category. Each evaluation category includes a plurality of sub-categories and, at least some of the sub-categories include their own sub-categories (e.g., a sub-sub category). For example, the identify category includes the sub-categories of asset management, business environment, governance, risk assessment, risk management, access control, awareness &#x26; training, and data security. As a further example, asset management includes the sub-categories of hardware inventory, software inventory, data flow maps, external system cataloged, resource prioritization, and security roles. The analysis system <b>10</b> can evaluate the system, or portion thereof, in light of one more evaluation categories, in light of an evaluation category and one or more sub-categories, or in light of an evaluation category, a sub-category, and one or more sub-sub-categories.</p><p id="p-0197" num="0196">The evaluation rating metrics (e.g., how the system, or portion thereof, is evaluated) include a selection of process, policy, procedure, certification, documentation, and/or automation. This allows the analysis system to quantify its evaluation. For example, the analysis system <b>10</b> can evaluate the processes a system, or portion thereof, has to generate an evaluation rating, to identify deficiencies, and/or to auto-correct deficiencies. As another example, the analysis system <b>10</b> can evaluate how well the system, or portion thereof, uses the process it has to generate an evaluation rating, to identify deficiencies, and/or to auto-correct deficiencies.</p><p id="p-0198" num="0197">In an example, the analysis computing entity <b>16</b> (which includes one or more computing entities) sends a data gathering request to the analysis system module <b>17</b>. The data gathering request is specific to the evaluation to be performed by the analysis system <b>10</b>. For example, if the analysis system <b>10</b> is evaluating the understanding of the policies, processes, documentation, and automation regarding the assets built for the engineering department, then the data gathering request would be specific to policies, processes, documentation, and automation regarding the assets built for the engineering department.</p><p id="p-0199" num="0198">The analysis system module <b>17</b> is loaded on the system <b>11</b>-<b>13</b> and obtained the requested data from the system. The obtaining of the data can be done in a variety of ways. For example, the data is disclosed by one or more system administrators. The disclosed data corresponds to the information the system administrator(s) has regarding the system. In essence, the disclosed data is a reflection of the knowledge the system administrator(s) has regarding the system.</p><p id="p-0200" num="0199">As another example, the analysis system module <b>17</b> communicates with physical assets of the system to discover the data. The communication may be direct with an asset. For example, the analysis system module <b>17</b> sends a request to a particular computing device. Alternatively or in addition, the communication may be through one or more discovery tools of the system. For example, the analysis system module <b>17</b> communicates with one or more tools of the system to obtain data regarding data segregation &#x26; boundary, infrastructure management, exploit &#x26; malware protection, encryption, identity &#x26; access management, system monitoring, vulnerability management, and/or data protection.</p><p id="p-0201" num="0200">A tool is a network monitoring tool, a network strategy and planning tool, a network managing tool, a Simple Network Management Protocol (SNMP) tool, a telephony monitoring tool, a firewall monitoring tool, a bandwidth monitoring tool, an IT asset inventory management tool, a network discovery tool, a network asset discovery tool, a software discovery tool, a security discovery tool, an infrastructure discovery tool, Security Information &#x26; Event Management (SIEM) tool, a data crawler tool, and/or other type of tool to assist in discovery of assets, functions, security issues, implementation of the system, and/or operation of the system. If the system does not have a particular tool, the analysis system module <b>17</b> engages one to discover a particular piece of data.</p><p id="p-0202" num="0201">The analysis system module <b>17</b> provides the gathered data to the analysis computing entity <b>16</b>, which stores the gathered data in a private storage <b>19</b>-<b>21</b> and processes it. The gathered data is processed alone, in combination with stored data (of the system being evaluated and/or another system's data), in combination with desired data (e.g., system proficiencies), in combination with analysis modeling (e.g., risk modeling, data flow modeling, security modeling, etc.), and/or in combination with stored analytic data (e.g., results of other evaluations). As a result of the processing, the analysis computing entity <b>16</b> produces an evaluation rating, to identify deficiencies, and/or to auto-correct deficiencies. The evaluation results are stored in a private storage and/or in another database.</p><p id="p-0203" num="0202">The analysis system <b>10</b> is operable to evaluate a system and/or its eco-system at any level of granularity from the entire system to an individual asset over a wide spectrum of evaluation options. As an example, the evaluation is to test understanding of the system, to test the implementation of the system, and/or to test the operation of the system. As another example, the evaluation is to test the system's self-evaluation capabilities with respect to understanding, implementation, and/or operation. As yet another example, the evaluation is to test policies regarding software tools; to test which software tools are prescribed by policy; to test which software tools are prohibited by policy; to test the use of the software tools in accordance with policy, to test maintenance of software tools in accordance with policy; to test the sufficiency of the policies, to test the effectiveness of the policies; and/or to test compliancy with the policies.</p><p id="p-0204" num="0203">The analysis system <b>10</b> takes an outside perspective to analyze the system. From within the system, it is often difficult to test the entire system, to test different combinations of system elements, to identify areas of vulnerabilities (assets and human operators), to identify areas of strength (assets and human operators), and to be proactive. Further, such evaluations are additional tasks the system has to perform, which means it consumes resources (human, physicals assets, and financial). Further, since system analysis is not the primary function of a system (supporting the organization is the system's primary purpose), the system analysis is not as thoroughly developed, implemented, and/or executed as is possible when its implemented in a stand-alone analysis system, like system <b>10</b>.</p><p id="p-0205" num="0204">The primary purpose of the analysis system is to analyze other systems to determine an evaluation rating, to identify deficiencies in the system, and, where it can, auto-correct the deficiencies. The evaluation rating can be regarding how well the system, or portion thereof, is understood, how well it is implemented, and/or how well it operates. The evaluation rating can be regarding how effective the system, or portion thereof, is believed (disclosed data) to support a business function; actually (discovered data) supports a business function; and/or should (desired data) support the business function.</p><p id="p-0206" num="0205">The evaluation rating can be regarding how effective the system, or portion thereof, is believed (disclosed data) to mitigate security risks; actually (discovered data) supports mitigating security risks; and/or should (desired data) support mitigating security risks. The evaluation rating can be regarding how effective the system, or portion thereof, is believed (disclosed data) to respond to security risks; actually (discovered data) supports responding to security risks; and/or should (desired data) support responding security risks.</p><p id="p-0207" num="0206">The evaluation rating can be regarding how effective the system, or portion thereof, is believed (disclosed data) to be used by people; is actually (discovered data) used by people; and/or should (desired data) be used by people. The evaluation rating can be regarding how effective the system, or portion thereof, is believed (disclosed data) to identify assets of the system; actually (discovered data) identifies assets of the system; and/or should (desired data) identify assets of the system.</p><p id="p-0208" num="0207">There are a significant number of combinations in which the analysis system <b>10</b> can evaluate a system <b>11</b>-<b>13</b>. A primary purpose the analysis system <b>10</b> is help the system <b>11</b>-<b>13</b> become more self-healing, more self-updating, more self-protecting, more self-recovering, more self-evaluating, more self-aware, more secure, more efficient, more adaptive, and/or more self-responding. By discovering the strengths, weaknesses, vulnerabilities, and other system limitations in a way that the system itself cannot do effectively, the analysis system <b>10</b> significantly improves the usefulness, security, and efficiency of systems <b>11</b>-<b>13</b>.</p><p id="p-0209" num="0208"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a schematic block diagram of an embodiment of a computing device <b>40</b> that includes a plurality of computing resources. The computing resource include a core control module <b>41</b>, one or more processing modules <b>43</b>, one or more main memories <b>45</b>, a read only memory (ROM) <b>44</b> for a boot up sequence, cache memory <b>47</b>, a video graphics processing module <b>42</b>, a display <b>48</b> (optional), an Input-Output (I/O) peripheral control module <b>46</b>, an I/O interface module <b>49</b> (which could be omitted), one or more input interface modules <b>50</b>, one or more output interface modules <b>51</b>, one or more network interface modules <b>55</b>, and one or more memory interface modules <b>54</b>. A processing module <b>43</b> is described in greater detail at the end of the detailed description of the invention section and, in an alternative embodiment, has a direction connection to the main memory <b>45</b>. In an alternate embodiment, the core control module <b>41</b> and the I/O and/or peripheral control module <b>46</b> are one module, such as a chipset, a quick path interconnect (QPI), and/or an ultra-path interconnect (UPI).</p><p id="p-0210" num="0209">Each of the main memories <b>45</b> includes one or more Random Access Memory (RAM) integrated circuits, or chips. For example, a main memory <b>45</b> includes four DDR4 (4th generation of double data rate) RAM chips, each running at a rate of 2,400 MHz. In general, the main memory <b>45</b> stores data and operational instructions most relevant for the processing module <b>43</b>. For example, the core control module <b>41</b> coordinates the transfer of data and/or operational instructions between the main memory <b>45</b> and the memory <b>56</b>-<b>57</b>. The data and/or operational instructions retrieve from memory <b>56</b>-<b>57</b> are the data and/or operational instructions requested by the processing module or will most likely be needed by the processing module. When the processing module is done with the data and/or operational instructions in main memory, the core control module <b>41</b> coordinates sending updated data to the memory <b>56</b>-<b>57</b> for storage.</p><p id="p-0211" num="0210">The memory <b>56</b>-<b>57</b> includes one or more hard drives, one or more solid state memory chips, and/or one or more other large capacity storage devices that, in comparison to cache memory and main memory devices, is/are relatively inexpensive with respect to cost per amount of data stored. The memory <b>56</b>-<b>57</b> is coupled to the core control module <b>41</b> via the I/O and/or peripheral control module <b>46</b> and via one or more memory interface modules <b>54</b>. In an embodiment, the I/O and/or peripheral control module <b>46</b> includes one or more Peripheral Component Interface (PCI) buses to which peripheral components connect to the core control module <b>41</b>. A memory interface module <b>54</b> includes a software driver and a hardware connector for coupling a memory device to the I/O and/or peripheral control module <b>46</b>. For example, a memory interface <b>54</b> is in accordance with a Serial Advanced Technology Attachment (SATA) port.</p><p id="p-0212" num="0211">The core control module <b>41</b> coordinates data communications between the processing module(s) <b>43</b> and the network(s) <b>14</b> via the I/O and/or peripheral control module <b>46</b>, the network interface module(s) <b>55</b>, and a network card <b>58</b> or <b>59</b>. A network card <b>58</b> or <b>59</b> includes a wireless communication unit or a wired communication unit. A wireless communication unit includes a wireless local area network (WLAN) communication device, a cellular communication device, a Bluetooth device, and/or a ZigBee communication device. A wired communication unit includes a Gigabit LAN connection, a Firewire connection, and/or a proprietary computer wired connection. A network interface module <b>55</b> includes a software driver and a hardware connector for coupling the network card to the I/O and/or peripheral control module <b>46</b>. For example, the network interface module <b>55</b> is in accordance with one or more versions of IEEE 802.11, cellular telephone protocols, 10/100/1000 Gigabit LAN protocols, etc.</p><p id="p-0213" num="0212">The core control module <b>41</b> coordinates data communications between the processing module(s) <b>43</b> and input device(s) <b>52</b> via the input interface module(s) <b>50</b>, the I/O interface <b>49</b>, and the I/O and/or peripheral control module <b>46</b>. An input device <b>52</b> includes a keypad, a keyboard, control switches, a touchpad, a microphone, a camera, etc. An input interface module <b>50</b> includes a software driver and a hardware connector for coupling an input device to the I/O and/or peripheral control module <b>46</b>. In an embodiment, an input interface module <b>50</b> is in accordance with one or more Universal Serial Bus (USB) protocols.</p><p id="p-0214" num="0213">The core control module <b>41</b> coordinates data communications between the processing module(s) <b>43</b> and output device(s) <b>53</b> via the output interface module(s) <b>51</b> and the I/O and/or peripheral control module <b>46</b>. An output device <b>53</b> includes a speaker, auxiliary memory, headphones, etc. An output interface module <b>51</b> includes a software driver and a hardware connector for coupling an output device to the I/O and/or peripheral control module <b>46</b>. In an embodiment, an output interface module <b>46</b> is in accordance with one or more audio codec protocols.</p><p id="p-0215" num="0214">The processing module <b>43</b> communicates directly with a video graphics processing module <b>42</b> to display data on the display <b>48</b>. The display <b>48</b> includes an LED (light emitting diode) display, an LCD (liquid crystal display), and/or other type of display technology. The display has a resolution, an aspect ratio, and other features that affect the quality of the display. The video graphics processing module <b>42</b> receives data from the processing module <b>43</b>, processes the data to produce rendered data in accordance with the characteristics of the display, and provides the rendered data to the display <b>48</b>.</p><p id="p-0216" num="0215"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a schematic block diagram of an embodiment of a computing device <b>40</b> that includes a plurality of computing resources similar to the computing resources of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> with the addition of one or more cloud memory interface modules <b>60</b>, one or more cloud processing interface modules <b>61</b>, cloud memory <b>62</b>, and one or more cloud processing modules <b>63</b>. The cloud memory <b>62</b> includes one or more tiers of memory (e.g., ROM, volatile (RAM, main, etc.), non-volatile (hard drive, solid-state, etc.) and/or backup (hard drive, tape, etc.)) that is remoted from the core control module and is accessed via a network (WAN and/or LAN). The cloud processing module <b>63</b> is similar to processing module <b>43</b> but is remoted from the core control module and is accessed via a network.</p><p id="p-0217" num="0216"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is a schematic block diagram of an embodiment of a computing device <b>40</b> that includes a plurality of computing resources similar to the computing resources of <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> with a change in how the cloud memory interface module(s) <b>60</b> and the cloud processing interface module(s) <b>61</b> are coupled to the core control module <b>41</b>. In this embodiment, the interface modules <b>60</b> and <b>61</b> are coupled to a cloud peripheral control module <b>63</b> that directly couples to the core control module <b>41</b>.</p><p id="p-0218" num="0217"><figref idref="DRAWINGS">FIG. <b>2</b>D</figref> is a schematic block diagram of an embodiment of a computing device <b>40</b> that includes a plurality of computing resources, which includes include a core control module <b>41</b>, a boot up processing module <b>66</b>, boot up RAM <b>67</b>, a read only memory (ROM) <b>45</b>, a video graphics processing module <b>42</b>, a display <b>48</b> (optional), an Input-Output (I/O) peripheral control module <b>46</b>, one or more input interface modules <b>50</b>, one or more output interface modules <b>51</b>, one or more cloud memory interface modules <b>60</b>, one or more cloud processing interface modules <b>61</b>, cloud memory <b>62</b>, and cloud processing module(s) <b>63</b>.</p><p id="p-0219" num="0218">In this embodiment, the computing device <b>40</b> includes enough processing resources (e.g., module <b>66</b>, ROM <b>44</b>, and RAM <b>67</b>) to boot up. Once booted up, the cloud memory <b>62</b> and the cloud processing module(s) <b>63</b> function as the computing device's memory (e.g., main and hard drive) and processing module.</p><p id="p-0220" num="0219"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> is schematic block diagram of an embodiment of a computing entity <b>16</b> that includes a computing device <b>40</b> (e.g., one of the embodiments of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>). A computing device may function as a user computing device, a server, a system computing device, a data storage device, a data security device, a networking device, a user access device, a cell phone, a tablet, a laptop, a printer, a game console, a satellite control box, a cable box, etc.</p><p id="p-0221" num="0220"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is schematic block diagram of an embodiment of a computing entity <b>16</b> that includes two or more computing devices <b>40</b> (e.g., two or more from any combination of the embodiments of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>). The computing devices <b>40</b> perform the functions of a computing entity in a peer processing manner (e.g., coordinate together to perform the functions), in a master-slave manner (e.g., one computing device coordinates and the other support it), and/or in another manner.</p><p id="p-0222" num="0221"><figref idref="DRAWINGS">FIG. <b>3</b>C</figref> is schematic block diagram of an embodiment of a computing entity <b>16</b> that includes a network of computing devices <b>40</b> (e.g., two or more from any combination of the embodiments of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>). The computing devices are coupled together via one or more network connections (e.g., WAN, LAN, cellular data, WLAN, etc.) and preform the functions of the computing entity.</p><p id="p-0223" num="0222"><figref idref="DRAWINGS">FIG. <b>3</b>D</figref> is schematic block diagram of an embodiment of a computing entity <b>16</b> that includes a primary computing device (e.g., any one of the computing devices of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>), an interface device (e.g., a network connection), and a network of computing devices <b>40</b> (e.g., one or more from any combination of the embodiments of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>). The primary computing device utilizes the other computing devices as co-processors to execute one or more the functions of the computing entity, as storage for data, for other data processing functions, and/or storage purposes.</p><p id="p-0224" num="0223"><figref idref="DRAWINGS">FIG. <b>3</b>E</figref> is schematic block diagram of an embodiment of a computing entity <b>16</b> that includes a primary computing device (e.g., any one of the computing devices of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>), an interface device (e.g., a network connection) <b>70</b>, and a network of computing resources <b>71</b> (e.g., two or more resources from any combination of the embodiments of <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>D</figref>). The primary computing device utilizes the computing resources as co-processors to execute one or more the functions of the computing entity, as storage for data, for other data processing functions, and/or storage purposes.</p><p id="p-0225" num="0224"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more non-business associated computing devices <b>24</b> (e.g., publicly available servers <b>27</b> and subscription based servers <b>28</b>), one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>. This diagram is similar to <figref idref="DRAWINGS">FIG. <b>1</b></figref> with the inclusion of detail within the system proficiency resource(s) <b>22</b>, with inclusion of detail within the system <b>11</b>, and with the inclusion of detail within the analysis system module <b>17</b>.</p><p id="p-0226" num="0225">In addition to the discussion with respect <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a system proficiency resource <b>22</b> is a computing device that provides information regarding best-in-class assets, best-in-class practices, known protocols, leading edge information, and/or established guidelines regarding risk assessment, devices, software, networking, data security, cybersecurity, and/or data communication. A system proficiency resource <b>22</b> is a computing device that may also provide information regarding standards, information regarding compliance requirements, information regarding legal requirements, and/or information regarding regulatory requirements.</p><p id="p-0227" num="0226">The system <b>11</b> is shown to include three inter-dependent modes: system functions <b>82</b>, security functions <b>83</b>, and system assets <b>84</b>. System functions <b>82</b> correspond to the functions the system executes to support the organization's business requirements. Security functions <b>83</b> correspond to the functions the system executes to support the organization's security requirements. The system assets <b>84</b> are the hardware and/or software platforms that support system functions <b>82</b> and/or the security functions <b>83</b>.</p><p id="p-0228" num="0227">The analysis system module <b>17</b> includes one or more data extraction modules <b>80</b> and one or more system user interface modules <b>81</b>. A data extraction module <b>80</b>, which will be described in greater detail with reference to one or more subsequent figures, gathers data from the system for analysis by the analysis system <b>10</b>. A system user interface module <b>81</b> provides a user interface between the system <b>11</b> and the analysis system <b>10</b> and functions to provide user information to the analysis system <b>10</b> and to receive output data from the analysis system. The system user interface module <b>81</b> will be described in greater detail with reference to one or more subsequent figures.</p><p id="p-0229" num="0228"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more non-business associated computing devices <b>24</b> (e.g., publicly available servers <b>27</b> and subscription based servers <b>28</b>), one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>. This diagram is similar to <figref idref="DRAWINGS">FIG. <b>4</b></figref> with the inclusion of additional detail within the system <b>11</b>.</p><p id="p-0230" num="0229">In this embodiment, the system <b>11</b> includes a plurality of sets of system assets to support the system functions <b>82</b> and/or the security functions <b>83</b>. For example, a set of system assets supports the system functions <b>82</b> and/or security functions <b>83</b> for a particular business segment (e.g., a department within the organization). As another example, a second set of system assets supports the security functions <b>83</b> for a different business segment and a third set of system assets supports the system functions <b>82</b> for the different business segment.</p><p id="p-0231" num="0230"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more non-business associated computing devices <b>24</b> (e.g., publicly available servers <b>27</b> and subscription based servers <b>28</b>), one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>. This diagram is similar to <figref idref="DRAWINGS">FIG. <b>5</b></figref> with the inclusion of additional detail within the system <b>11</b>.</p><p id="p-0232" num="0231">In this embodiment, the system <b>11</b> includes a plurality of sets of system assets <b>84</b>, system functions <b>82</b>, and security functions <b>83</b>. For example, a set of system assets <b>84</b>, system functions <b>82</b>, and security functions <b>83</b> supports one department in an organization and a second set of system assets <b>84</b>, system functions <b>82</b>, and security functions <b>83</b> supports another department in the organization.</p><p id="p-0233" num="0232"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic block diagram of another embodiment of a networked environment that includes a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more non-business associated computing devices <b>24</b> (e.g., publicly available servers <b>27</b> and subscription based servers <b>28</b>), one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>. This diagram is similar to <figref idref="DRAWINGS">FIG. <b>4</b></figref> with the inclusion of additional detail within the system <b>11</b>.</p><p id="p-0234" num="0233">In this embodiment, the system <b>11</b> includes system assets <b>84</b>, system functions <b>82</b>, security functions <b>83</b>, and self-evaluation functions <b>85</b>. The self-evaluation functions <b>85</b> are supported by the system assets <b>84</b> and are used by the system to evaluate its assets, is system functions, and its security functions. In general, self-evaluates looks at system's ability to analyze itself for self-determining it's understanding (self-aware) of the system; self-determining the implementation of the system, and/or self-determining operation of the system. In addition, the self-evaluation may further consider the system's ability to self-heal, self-update, self-protect, self-recover, self-evaluate, and/or self-respond. The analysis system <b>10</b> can evaluate the understanding, implementation, and/or operation of the self-evaluation functions.</p><p id="p-0235" num="0234"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks represented by networking infrastructure, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more publicly available servers <b>27</b>, one or more subscription based servers <b>28</b>, one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>.</p><p id="p-0236" num="0235">In this embodiment, the system <b>11</b> is shown to include a plurality of physical assets dispersed throughout a geographic region (e.g., a building, a town, a county, a state, a country). Each of the physical assets includes hardware and software to perform its respective functions within the system. A physical asset is a computing entity (CE), a public or provide networking device (ND), a user access device (UAD), or a business associate access device (BAAD).</p><p id="p-0237" num="0236">A computing entity may be a user device, a system admin device, a server, a printer, a data storage device, etc. A network device may be a local area network device, a network card, a wide area network device, etc. A user access device is a portal that allows authorizes users of the system to remotely access the system. A business associated access device is a portal that allows authorized business associates of the system access the system.</p><p id="p-0238" num="0237">Some of the computing entities are grouped via a common connection to a network device, which provides the group of computing entities access to other parts of the system and/or the internet. For example, the highlighted computing entity may access a publicly available server <b>25</b> via network devices coupled to the network infrastructure. The analysis system <b>10</b> can evaluation whether this is an appropriate access, the understanding of this access, the implementation to enable this access, and/or the operation of the system to support this access.</p><p id="p-0239" num="0238"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic block diagram of an example of a system section of a system selected for evaluation similar to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In this example, only a portion of the system is being tested, i.e., system section under test <b>91</b>. As such, the analysis system <b>10</b> only evaluates assets, system functions, and/or security functions related to assets within the system section under test <b>91</b>.</p><p id="p-0240" num="0239"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic block diagram of another example of a system section of a system selected for evaluation similar to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. In this example, a single computing entity (CE) is being tested, i.e., system section under test <b>91</b>. As such, the analysis system <b>10</b> only evaluates assets, system functions, and/or security functions related to the selected computing entity.</p><p id="p-0241" num="0240"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic block diagram of an embodiment of a networked environment having a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks <b>14</b>, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more publicly available servers <b>27</b>, one or more subscription based servers <b>28</b>, one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>.</p><p id="p-0242" num="0241">In this embodiment, the system <b>11</b> is shown to include a plurality of system assets (SA). A system asset (SA) may include one or more system sub assets (S2A) and a system sub asset (S2A) may include one or more system sub-sub assets (S3A). While being a part of the analysis system <b>10</b>, at least one data extraction module (DEM) <b>80</b> and at least one system user interface module (SUIM) <b>81</b> are installed on the system <b>11</b>.</p><p id="p-0243" num="0242">A system element includes one or more system assets. A system asset (SA) may be a physical asset or a conceptual asset as previously described. As an example, a system element includes a system asset of a computing device. The computing device, which is the SA, includes user applications and an operating system; each of which are sub assets of the computing device (S2A). In addition, the computing device includes a network card, memory devices, etc., which are sub assets of the computing device (S2A). Documents created from a word processing user application are sub assets of the word processing user application (S3A) and sub-sub assets of the computing device.</p><p id="p-0244" num="0243">As another example, the system asset (SA) includes a plurality of computing devices, printers, servers, etc. of a department of the organization operating the system <b>11</b>. In this example, a computing device is a sub asset of the system asset and the software and hardware of the computing devices are sub-sub assets.</p><p id="p-0245" num="0244">The analysis system <b>10</b> may evaluate understanding, implementation, and/or operation of one or more system assets, one or more system sub assets, and/or one or more system sub-sub assets, as an asset, as it supports system functions <b>82</b>, and/or as it supports security functions. The evaluation may be to produce an evaluation rating, to identify deficiencies, and/or to auto-correct deficiencies.</p><p id="p-0246" num="0245"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic block diagram of an embodiment of a system <b>11</b> that includes a plurality of physical assets <b>100</b> coupled to an analysis system <b>100</b>. The physical assets <b>100</b> include an analysis interface device <b>101</b>, one or more networking devices <b>102</b>, one or more security devices <b>103</b>, one or more system admin devices <b>104</b>, one or more user devices <b>105</b>, one or more storage devices <b>106</b>, and/or one or more servers <b>107</b>. Each device may be a computing entity that includes hardware (HW) components and software (SW) applications (user, device, drivers, and/or system). A device may further include a data extraction module (DEM).</p><p id="p-0247" num="0246">The analysis interface device <b>101</b> includes a data extraction module (DEM) <b>80</b> and the system user interface module <b>81</b> to provide connectivity to the analysis system <b>10</b>. With the connectivity, the analysis system <b>10</b> is able to evaluate understanding, implementation, and/or operation of each device, or portion thereof, as an asset, as it supports system functions <b>82</b>, and/or as it supports security functions. For example, the analysis system <b>10</b> evaluates the understanding of networking devices <b>102</b> as an asset. As a more specific example, the analysis system <b>10</b> evaluates how well the networking devices <b>102</b>, its hardware, and its software are understood within the system and/or by the system administrators. The evaluation includes how well are the networking devices <b>102</b>, its hardware, and its software documented; how well are they implemented based on system requirements; how well do they operate based on design and/or system requirements; how well are they maintained per system policies and/or procedures; how well are their deficiencies identified; and/or how well are their deficiencies auto-corrected.</p><p id="p-0248" num="0247"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system <b>11</b> that includes a plurality of system assets coupled to an analysis system <b>10</b>. This embodiment is similar to the embodiment of <figref idref="DRAWINGS">FIG. <b>11</b></figref> with the addition of additional data extraction modules (DEM) <b>80</b>. In this embodiment, each system asset (SA) is affiliated with its own DEM <b>80</b>. This allows the analysis system <b>10</b> to extract data more efficiently than via a single DEM. A further extension of this embodiment is that each system sub asset (S2A) could have its own DEM <b>80</b>. As yet a further extension, each system sub-sub asset (S3A) could have its own DEM <b>80</b>.</p><p id="p-0249" num="0248"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic block diagram of another embodiment of a system <b>11</b> physical assets <b>100</b> coupled to an analysis system <b>100</b>. The physical assets <b>100</b> include one or more networking devices <b>102</b>, one or more security devices <b>103</b>, one or more system admin devices <b>104</b>, one or more user devices <b>105</b>, one or more storage devices <b>106</b>, and/or one or more servers <b>107</b>. Each device may be a computing entity that includes hardware (HW) components and software (SW) applications (user, system, and/or device).</p><p id="p-0250" num="0249">The system admin device <b>104</b> includes one or more analysis system modules <b>17</b>, which includes a data extraction module (DEM) <b>80</b> and the system user interface module <b>81</b> to provide connectivity to the analysis system <b>10</b>. With the connectivity, the analysis system <b>10</b> is able to evaluate understanding, implementation, and/or operation of each device, or portion thereof, as an asset, as it supports system functions <b>82</b>, and/or as it supports security functions. For example, the analysis system <b>10</b> evaluates the implementation of networking devices <b>102</b> to support system functions. As a more specific example, the analysis system <b>10</b> evaluates how well the networking devices <b>102</b>, its hardware, and its software are implemented within the system to support one or more system functions (e.g., managing network traffic, controlling network access per business guidelines, policies, and/or processes, etc.). The evaluation includes how well is the implementation of the networking devices <b>102</b>, its hardware, and its software documented to support the one or more system functions; how well does their implementation support the one or more system functions; how well have their implementation to support the one or more system functions been verified in accordance with policies, processes, etc.; how well are they updated per system policies and/or procedures; how well are their deficiencies in support of the one or more system functions identified; and/or how well are their deficiencies in support of the one or more system functions auto-corrected.</p><p id="p-0251" num="0250"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a schematic block diagram of another embodiment of a system <b>11</b> that includes a plurality of physical assets <b>100</b> coupled to an analysis system <b>100</b>. The physical assets <b>100</b> include an analysis interface device <b>101</b>, one or more networking devices <b>102</b>, one or more security devices <b>103</b>, one or more system admin devices <b>104</b>, one or more user devices <b>105</b>, one or more storage devices <b>106</b>, and/or one or more servers <b>107</b>. Each device may be a computing entity that includes hardware (HW) components and software (SW) applications (user, device, drivers, and/or system). This embodiment is similar to the embodiment of <figref idref="DRAWINGS">FIG. <b>12</b></figref> with a difference being that the devices <b>102</b>-<b>107</b> do not include a data extraction module (DEM) as is shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0252" num="0251"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a schematic block diagram of another embodiment of a system <b>11</b> that includes networking devices <b>102</b>, security devices <b>103</b>, servers <b>107</b>, storage devices <b>106</b>, and user devices <b>105</b>. The system <b>11</b> is coupled to the network <b>14</b>, which provides connectivity to the business associate computing device <b>23</b>. The network <b>14</b> is shown to include one or more wide area networks (WAN) <b>162</b>, one or more wireless LAN (WLAN) and/or LANs <b>164</b>, one or more virtual private networks <b>166</b>.</p><p id="p-0253" num="0252">The networking devices <b>102</b> includes one or more modems <b>120</b>, one or more routers <b>121</b>, one or more switches <b>122</b>, one or more access points <b>124</b>, and/or one or more local area network cards <b>124</b>. The analysis system <b>10</b> can evaluate the network devices <b>102</b> collectively as assets, as they support system functions, and/or as they support security functions. The analysis system <b>10</b> may also evaluate each network device individually as an asset, as it supports system functions, and/or as it supports security functions. The analysis system may further evaluate one or more network devices as part of the physical assets of a system aspect (e.g., the system or a portion thereof being evaluated with respect to one or more system criteria and one or more system modes).</p><p id="p-0254" num="0253">The security devices <b>103</b> includes one or more infrastructure management tools <b>125</b>, one or more encryption software programs <b>126</b>, one or more identity and access management tools <b>127</b>, one or more data protection software programs <b>128</b>, one or more system monitoring tools <b>129</b>, one or more exploit and malware protection tools <b>130</b>, one or more vulnerability management tools <b>131</b>, and/or one or more data segmentation and boundary tools <b>132</b>. Note that a tool is a program that functions to develop, repair, and/or enhance other programs and/or hardware.</p><p id="p-0255" num="0254">The analysis system <b>10</b> can evaluate the security devices <b>103</b> collectively as assets, as they support system functions, and/or as they support security functions. The analysis system <b>10</b> may also evaluate each security device individually as an asset, as it supports system functions, and/or as it supports security functions. The analysis system may further evaluate one or more security devices as part of the physical assets of a system aspect (e.g., the system or a portion thereof being evaluated with respect to one or more system criteria and one or more system modes).</p><p id="p-0256" num="0255">The servers <b>107</b> include one or more telephony servers <b>133</b>, one or more ecommerce servers <b>134</b>, one or more email servers <b>135</b>, one or more web servers <b>136</b>, and/or one or more content servers <b>137</b>. The analysis system <b>10</b> can evaluate the servers <b>103</b> collectively as assets, as they support system functions, and/or as they support security functions. The analysis system <b>10</b> may also evaluate each server individually as an asset, as it supports system functions, and/or as it supports security functions. The analysis system may further evaluate one or more servers as part of the physical assets of a system aspect (e.g., the system or a portion thereof being evaluated with respect to one or more system criteria and one or more system modes).</p><p id="p-0257" num="0256">The storage devices includes one or more cloud storage devices <b>138</b>, one or more storage racks <b>139</b> (e.g., a plurality of storage devices mounted in a rack), and/or one or more databases <b>140</b>. The analysis system <b>10</b> can evaluate the storage devices <b>103</b> collectively as assets, as they support system functions, and/or as they support security functions. The analysis system <b>10</b> may also evaluate each storage device individually as an asset, as it supports system functions, and/or as it supports security functions. The analysis system may further evaluate one or more storage devices as part of the physical assets of a system aspect (e.g., the system or a portion thereof being evaluated with respect to one or more system criteria and one or more system modes).</p><p id="p-0258" num="0257">The user devices <b>105</b> include one or more landline phones <b>141</b>, one or more IP cameras <b>144</b>, one or more cell phones <b>143</b>, one or more user computing devices <b>145</b>, one or more IP phones <b>150</b>, one or more video conferencing equipment <b>148</b>, one or more scanners <b>151</b>, and/or one or more printers <b>142</b>. The analysis system <b>10</b> can evaluate the use devices <b>103</b> collectively as assets, as they support system functions, and/or as they support security functions. The analysis system <b>10</b> may also evaluate each user device individually as an asset, as it supports system functions, and/or as it supports security functions. The analysis system may further evaluate one or more user devices as part of the physical assets of a system aspect (e.g., the system or a portion thereof being evaluated with respect to one or more system criteria and one or more system modes).</p><p id="p-0259" num="0258">The system admin devices <b>104</b> includes one or more system admin computing devices <b>146</b>, one or more system computing devices <b>194</b> (e.g., data management, access control, privileges, etc.), and/or one or more security management computing devices <b>147</b>. The analysis system <b>10</b> can evaluate the system admin devices <b>103</b> collectively as assets, as they support system functions, and/or as they support security functions. The analysis system <b>10</b> may also evaluate each system admin device individually as an asset, as it supports system functions, and/or as it supports security functions. The analysis system may further evaluate one or more system admin devices as part of the physical assets of a system aspect (e.g., the system or a portion thereof being evaluated with respect to one or more system criteria and one or more system modes).</p><p id="p-0260" num="0259"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic block diagram of an embodiment of a user computing device <b>105</b> that includes software <b>160</b>, a user interface <b>161</b>, processing resources <b>163</b>, memory <b>162</b> and one or more networking device <b>164</b>. The processing resources <b>163</b> include one or more processing modules, cache memory, and a video graphics processing module.</p><p id="p-0261" num="0260">The memory <b>162</b> includes non-volatile memory, volatile memory and/or disk memory. The non-volatile memory stores hardware IDs, user credentials, security data, user IDs, passwords, access rights data, device IDs, one or more IP addresses and security software. The volatile memory includes system volatile memory and user volatile memory. The disk memory includes system disk memory and user disk memory. User memory (volatile and/or disk) stores user data and user applications. System memory (volatile and/or disk) stores system applications and system data.</p><p id="p-0262" num="0261">The user interface <b>104</b> includes one or more I/O (input/output) devices such as video displays, keyboards, mice, eye scanners, microphones, speakers, and other devices that interface with one or more users. The user interface <b>161</b> further includes one or more physical (PHY) interface with supporting software such that the user computing device can interface with peripheral devices.</p><p id="p-0263" num="0262">The software <b>160</b> includes one or more I/O software interfaces (e.g., drivers) that enable the processing module to interface with other components. The software <b>160</b> also includes system applications, user applications, disk memory software interfaces (drivers) and network software interfaces (drivers).</p><p id="p-0264" num="0263">The networking device <b>164</b> may be a network card or network interface that intercouples the user computing device <b>105</b> to devices external to the computing device <b>105</b> and includes one or more PHY interfaces. For example, the network card is a WLAN card. As another example, he network card is a cellular data network card. As yet another example, the network card is an ethernet card.</p><p id="p-0265" num="0264">The user computing device may further include a data extraction module <b>80</b>. This would allow the analysis system <b>10</b> to obtain data directly from the user computing device. Regardless of how the analysis system <b>10</b> obtains data regarding the user computing device, the analysis system <b>10</b> can evaluate the user computing device as an asset, as it supports one or more system functions, and/or as it supports one or more security functions. The analysis system <b>10</b> may also evaluate each element of the user computing device (e.g., each software application, each drive, each piece of hardware, etc.) individually as an asset, as it supports one or more system functions, and/or as it supports one or more security functions.</p><p id="p-0266" num="0265"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a schematic block diagram of an embodiment of a server <b>107</b> that includes software <b>170</b>, processing resources <b>171</b>, memory <b>172</b> and one or more networking resources <b>173</b>. The processing resources <b>171</b> include one or more processing modules, cache memory, and a video graphics processing module. The memory <b>172</b> includes non-volatile memory, volatile memory, and/or disk memory. The non-volatile memory stores hardware IDs, user credentials, security data, user IDs, passwords, access rights data, device IDs, one or more IP addresses and security software. The volatile memory includes system volatile memory and shared volatile memory. The disk memory include server disk memory and shared disk memory.</p><p id="p-0267" num="0266">The software <b>170</b> includes one or more I/O software interfaces (e.g., drivers) that enable the software <b>170</b> to interface with other components. The software <b>170</b> includes system applications, server applications, disk memory software interfaces (drivers), and network software interfaces (drivers). The networking resources <b>173</b> may be one or more network cards that provides a physical interface for the server to a network.</p><p id="p-0268" num="0267">The server <b>107</b> may further include a data extraction module <b>80</b>. This would allow the analysis system <b>10</b> to obtain data directly from the server. Regardless of how the analysis system <b>10</b> obtains data regarding the server, the analysis system <b>10</b> can evaluate the server as an asset, as it supports one or more system functions, and/or as it supports one or more security functions. The analysis system <b>10</b> may also evaluate each element of the server (e.g., each software application, each drive, each piece of hardware, etc.) individually as an asset, as it supports one or more system functions, and/or as it supports one or more security functions.</p><p id="p-0269" num="0268"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks <b>14</b>, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more publicly available servers <b>27</b>, one or more subscription based servers <b>28</b>, one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>.</p><p id="p-0270" num="0269">In this embodiment, the system <b>11</b> is shown to include a plurality of system functions (SF). A system function (SF) may include one or more system sub functions (S2F) and a system sub function (S2F) may include one or more system sub-sub functions (S3F). While being a part of the analysis system <b>10</b>, at least one data extraction module (DEM) <b>80</b> and at least one system user interface module (SUIM) <b>81</b> are installed on the system <b>11</b>.</p><p id="p-0271" num="0270">A system function (SF) includes one or more business operations, one or more compliance requirements, one or more data flow objectives, one or more data access control objectives, one or more data integrity objectives, one or more data storage objectives, one or more data use objectives, and/or one or more data dissemination objectives. Business operation system functions are the primary purpose for the system <b>11</b>. The system <b>11</b> is designed and built to support the operations of the business, which vary from business to business.</p><p id="p-0272" num="0271">In general, business operations include operations regarding critical business functions, support functions for core business, product and/or service functions, risk management objectives, business ecosystem objectives, and/or business contingency plans. The business operations may be divided into executive management operations, information technology operations, marketing operations, engineering operations, manufacturing operations, sales operations, accounting operations, human resource operations, legal operations, intellectual property operations, and/or finance operations. Each type of business operation includes sub-business operations, which, in turn may include its own sub-operations.</p><p id="p-0273" num="0272">For example, engineering operations includes a system function of designing new products and/or product features. The design of a new product or feature involves sub-functions of creating design specifications, creating a design based on the design specification, and testing the design through simulation and/or prototyping. Each of these steps includes sub-steps. For example, for the design of a software program, the design process includes the sub-sub system functions of creating a high level design from the design specifications; creating a low level design from the high level design; and the creating code from the low level design.</p><p id="p-0274" num="0273">A compliance requirement may be a regulatory compliance requirement, a standard compliance requirement, a statutory compliance requirement, and/or an organization compliance requirement. For example, there are a regulatory compliance requirements when the organization has governmental agencies as clients. An example of a standard compliance requirement, encryption protocols are often standardized. Data Encryption Standard (DES), Advanced Encryption Standard (AES), RSA (Rivest-Shamir-Adleman) encryption, and public-key infrastructure (PKI) are examples of encryption type standards. HIPAA (health Insurance Portability and Accountability Act) is an example of a statutory compliance requirement. Examples of organization compliance requirements include use of specific vendor hardware, use of specific vendor software, use of encryption, etc.</p><p id="p-0275" num="0274">A data flow objective is regarding where data can flow, at what rate data can and should flow, the manner in which the data flow, and/or the means over which the data flows. As an example of a data flow objective, data for remote storage is to flow via a secure data pipeline using a particular encryption protocol. As another example of a data flow objective, ingesting of data should have the capacity to handle a data rate of 100 giga-bits per second.</p><p id="p-0276" num="0275">A data access control objective established which type of personnel and/or type of assets can access specific types of data. For example, certain members of the corporate department and human resources department have access to employee personnel files, while all other members of the organization do not.</p><p id="p-0277" num="0276">A data integrity objective establishes a reliability that, when data is retrieved, it is the data that was stored, i.e., it was not lost, damaged, or corrupted. An example of a data integrity protocol is Cyclic Redundancy Check (CRC). Another example of a data integrity protocol is a hash function.</p><p id="p-0278" num="0277">A data storage objective establishes the manner in which data is to be stored. For example, a data storage objective is to store data in a RAID system; in particular, a RAID 6 system. As another example, a data storage objective is regarding archiving of data and the type of storage to use for archived data.</p><p id="p-0279" num="0278">A data use objective establishes the manner in which data can be used. For example, if the data is for sale, then the data use objective would establish what type of data is for sale, at what price, and what is the target customer. As another example, a data use objective establishes read only privileges, editing privileges, creation privileges, and/or deleting privileges.</p><p id="p-0280" num="0279">A data dissemination objective establishes how the data can be shared. For example, a data dissemination objective is regarding confidential information and indicates how the confidential information should be marked, who in can be shared with internally, and how it can be shared externally, if at all.</p><p id="p-0281" num="0280">The analysis system <b>10</b> may evaluate understanding, implementation, and/or operation of one or more system functions, one or more system sub functions, and/or one or more system sub-sub functions. The evaluation may be to produce an evaluation rating, to identify deficiencies, and/or to auto-correct deficiencies. For example, the analysis system <b>10</b> evaluates the understanding of the software development policies and/or processes. As another example, the analysis system <b>10</b> evaluates the use of software development policies and/or processes to implement a software program. As yet another example, analysis system <b>10</b> evaluates the operation of the software program with respect to the business operation, the design specifications, and/or the design.</p><p id="p-0282" num="0281"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a schematic block diagram of another embodiment of a system <b>11</b> that includes, from a business operations perspective, divisions <b>181</b>-<b>183</b>, departments, and groups. The business structure of the system <b>11</b>, as in most businesses, is governed by a corporate department <b>180</b>. The corporate department may have its own sub-system with structures and software tailored to the corporate function of the system. Organized under the corporate department <b>180</b> are divisions, division 1 <b>181</b>, division 2 <b>182</b>, through division k <b>183</b>. These divisions may be different business divisions of a multi-national conglomerate, may be different functional divisions of a business, e.g., finance, marketing, sales, legal, engineering, research and development, etc. Under each division <b>1081</b>-<b>183</b> include a plurality of departments. Under each department are a number of groups.</p><p id="p-0283" num="0282">The business structure is generic and can be used to represent the structure of most conventional businesses and/or organizations. The analysis system <b>10</b> is able to use this generic structure to create and categorize the business structure of the system <b>11</b>. The creation and categorization of the business structure is done in a number of ways. Firstly, the analysis system <b>10</b> accesses corporate organization documents for the business and receive feedback from one or more persons in the business and use these documents and data to initially determine at least partially the business structure. Secondly, the analysis system <b>10</b> determines the network structure of the other system, investigate identities of components of the network structure, and construct a sub-division of the other system. Then, based upon software used within the sub-division, data character, and usage character, the analysis system <b>10</b> identifies more specifically the function of the divisions, departments and groups. In doing so, the analysis system <b>10</b> uses information known of third-party systems to assist in the analysis.</p><p id="p-0284" num="0283">With the abstraction of the business structure, differing portions of the business structure may have different levels of abstraction from a component/sub-component/sub-sub-component/system/sub-system/sub-sub-system level based upon characters of differing segments of the business. For example. a more detailed level of abstraction for elements of the corporate and security departments of the business may be taken than for other departments of the business.</p><p id="p-0285" num="0284"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a schematic block diagram of another embodiment of a business structure of the system <b>11</b>. Shown are a corporate department <b>180</b>, an IT department <b>181</b>, division 2 <b>182</b> through division &#x201c;k&#x201d; <b>183</b>, where k is an integer equal to or greater than 3. The corporate department <b>180</b> includes a plurality of hardware devices <b>260</b>, a plurality of software applications <b>262</b>, a plurality of business policies <b>264</b>, a plurality of business procedures <b>266</b>, local networking <b>268</b>, a plurality of security policies <b>270</b>, a plurality of security procedures <b>272</b>, data protection resources <b>272</b>, data access resources <b>276</b>, data storage devices <b>278</b>, a personnel hierarchy <b>280</b>, and external networking <b>282</b>. Based upon an assessment of these assets of the corporate department <b>180</b>, analysis system <b>10</b> may evaluate the understanding, implementation, and/or operation of the assets, system functions, and/or security functions of the corporate department from a number of different perspectives, as will be described further with reference to one or more the subsequent figures.</p><p id="p-0286" num="0285">Likewise, the IT department <b>181</b> includes a plurality of hardware devices <b>290</b>, a plurality of software applications <b>292</b>, a plurality of business policies <b>294</b>, a plurality of business procedures <b>296</b>, local networking <b>298</b>, a plurality of security policies <b>300</b>, a plurality of security procedures <b>302</b>, data protection resources <b>304</b>, data access resources <b>306</b>, data storage devices <b>308</b>, a personnel hierarchy <b>310</b>, and external networking <b>312</b>. Based upon an assessment of these assets of the IT department <b>181</b>, the analysis system <b>10</b> may evaluate the understanding, implementation, and/or operation of the assets, system functions, and/or security functions of the IT department from a number of different perspectives, as will be described further with reference to one or more of the subsequent figures.</p><p id="p-0287" num="0286"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a schematic block diagram of another embodiment of a division <b>182</b> of a system that includes multiple departments. The departments include a marketing department <b>190</b>, an operations department <b>191</b>, an engineering department <b>192</b>, a manufacturing department <b>193</b>, a sales department <b>194</b>, and an accounting department <b>195</b>. Each of the departments includes a plurality of components relevant to support the corresponding business functions and/or security functions of the division and of the department. In particular, the marketing department <b>190</b> includes a plurality of devices, software, security policies, security procedures, business policies, business procedures, data protection resources, data access resources, data storage resources, a personnel hierarchy, local network resources, and external network resources.</p><p id="p-0288" num="0287">Likewise, each of the operations department <b>191</b>, the engineering department <b>192</b>, the manufacturing department <b>193</b>, the sales department <b>194</b>, and the accounting department <b>195</b> includes a plurality of devices, software, security policies, security procedures, business policies, business procedures, data protection resources, data access resources, data storage resources, a personnel hierarchy, local network resources, and external network resources.</p><p id="p-0289" num="0288">Further, within the business structure, a service mesh may be established to more effectively protect important portions of the business from other portions of the business. The service mesh may have more restrictive safety and security mechanisms for one part of the business than another portion of the business, e.g., manufacturing department service mesh is more restrictive than the sales department service mesh.</p><p id="p-0290" num="0289">The analysis system <b>10</b> may evaluate the understanding, implementation, and/or operation of the assets, system functions, and/or security functions of the division <b>182</b>, of each department, of each type of system elements, and/or each system element. For example, the analysis system <b>10</b> evaluates the data access policies and procedures of each department. As another example, the analysis system <b>10</b> evaluates the data storage policies, procedures, design, implementation, and/or operation of data storage within the engineering department <b>192</b>.</p><p id="p-0291" num="0290"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a schematic block diagram of another embodiment of a networked environment having a system <b>11</b> (or system <b>12</b> or system <b>13</b>), the analysis system <b>10</b>, one or more networks <b>14</b>, one or more system proficiency resources <b>22</b>, one or more business associated computing devices <b>23</b>, one or more publicly available servers <b>27</b>, one or more subscription based servers <b>28</b>, one or more BOT computing devices <b>25</b>, and one or more bad actor computing devices <b>26</b>.</p><p id="p-0292" num="0291">In this embodiment, the system <b>11</b> is shown to include a plurality of security functions (SEF). A security function (SEF) may include one or more system sub security functions (SE2F) and a security sub function (SE2F) may include one or more security sub-sub functions (SE3F). While being a part of the analysis system <b>10</b>, at least one data extraction module (DEM) <b>80</b> and at least one system user interface module (SUIM) <b>81</b> are installed on the system <b>11</b>. As used herein, a security function includes a security operation, a security requirement, a security policy, and/or a security objective with respect to data, system access, system design, system operation, and/or system modifications (e.g., updates, expansion, part replacement, maintenance, etc.).</p><p id="p-0293" num="0292">A security function (SF) includes one or more threat detection functions, one or more threat avoidance functions, one or more threat resolution functions, one or more threat recovery functions, one or more threat assessment functions, one or more threat impact functions, one or more threat tolerance functions, one or more business security functions, one or more governance security functions, one or more data at rest protection functions, one or more data in transit protection functions, and/or one or more data loss prevention functions.</p><p id="p-0294" num="0293">A threat detection function includes detecting unauthorized system access; detecting unauthorized data access; detecting unauthorized data changes; detecting uploading of worms, viruses, and the like; and/or detecting bad actor attacks. A threat avoidance function includes avoiding unauthorized system access; avoiding unauthorized data access; avoiding unauthorized data changes; avoiding uploading of worms, viruses, and the like; and/or avoiding bad actor attacks.</p><p id="p-0295" num="0294">A threat resolution function includes resolving unauthorized system access; resolving unauthorized data access; resolving unauthorized data changes; resolving uploading of worms, viruses, and the like; and/or resolving bad actor attacks. A threat recovery function includes recovering from an unauthorized system access; recovering from an unauthorized data access; recovering from an unauthorized data changes; recovering from an uploading of worms, viruses, and the like; and/or recovering from a bad actor attack.</p><p id="p-0296" num="0295">A threat assessment function includes accessing the likelihood of and/or mechanisms for unauthorized system access; accessing the likelihood of and/or mechanisms for unauthorized data access; accessing the likelihood of and/or mechanisms for unauthorized data changes; accessing the likelihood of and/or mechanisms for uploading of worms, viruses, and the like; and/or accessing the likelihood of and/or mechanisms for bad actor attacks.</p><p id="p-0297" num="0296">A threat impact function includes determining an impact on business operations from an unauthorized system access; resolving unauthorized data access; determining an impact on business operations from an unauthorized data changes; determining an impact on business operations from an uploading of worms, viruses, and the like; and/or determining an impact on business operations from a bad actor attacks.</p><p id="p-0298" num="0297">A threat tolerance function includes determining a level of tolerance for an unauthorized system access; determining a level of tolerance for an unauthorized data access; determining a level of tolerance for an unauthorized data changes; determining a level of tolerance for an uploading of worms, viruses, and the like; and/or determining a level of tolerance for a bad actor attacks.</p><p id="p-0299" num="0298">A business security function includes data encryption, handling of third party data, releasing data to the public, and so on. A governance security function includes HIPAA compliance; data creation, data use, data storage, and/or data dissemination for specific types of customers (e.g., governmental agency); and/or the like.</p><p id="p-0300" num="0299">A data at rest protection function includes a data access protocol (e.g., user ID, password, etc.) to store data in and/or retrieve data from system data storage; data storage requirements, which include type of storage, location of storage, and storage capacity; and/or other data storage security functions.</p><p id="p-0301" num="0300">A data in transit protection function includes using a specific data transportation protocol (e.g., TCP/IP); using an encryption function prior to data transmission; using an error encoding function for data transmission; using a specified data communication path for data transmission; and/or other means to protect data in transit. A data loss prevention function includes a storage encoding technique (e.g., single parity encoding, double parity encoding, erasure encoding, etc.); a storage backup technique (e.g., one or two backup copies, erasure encoding, etc.); hardware maintenance and replacement policies and processes; and/or other means to prevent loss of data.</p><p id="p-0302" num="0301">The analysis system <b>10</b> may evaluate understanding, implementation, and/or operation of one or more security functions, one or more security sub functions, and/or one or more security sub-sub functions. The evaluation may be to produce an evaluation rating, to identify deficiencies, and/or to auto-correct deficiencies. For example, the analysis system <b>10</b> evaluates the understanding of the threat detection policies and/or processes. As another example, the analysis system <b>10</b> evaluates the use of threat detection policies and/or processes to implement a security assets. As yet another example, analysis system <b>10</b> evaluates the operation of the security assets with respect to the threat detection operation, the threat detection design specifications, and/or the threat detection design.</p><p id="p-0303" num="0302"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a schematic block diagram of an embodiment of an engineering department <b>200</b> of a division <b>182</b> that reports to a corporate department <b>180</b> of a system <b>11</b>. The engineering department <b>200</b> includes engineering assets, engineering system functions, and engineering security functions. The engineering assets include security HW &#x26; SW, user device HW &#x26; SW, networking HW &#x26; SW, system HW &#x26; SW, system monitoring HW &#x26; SW, and/or other devices that includes HW and/or SW.</p><p id="p-0304" num="0303">In this example, the organization's system functions includes business operations, compliance requirements, data flow objectives, data access objectives, data integrity objectives, data storage objectives, data use objectives, and/or data dissemination objectives. These system functions apply throughout the system including throughout division 2 and for the engineering department <b>200</b> of division 2.</p><p id="p-0305" num="0304">The division <b>182</b>, however, can issues more restrictive, more secure, and/or more detailed system functions. In this example, the division has issued more restrictive, secure, and/or detailed business operations (business operations +) and more restrictive, secure, and/or detailed data access functions (data access +). Similarly, the engineering department <b>200</b> may issue more restrictive, more secure, and/or more detailed system functions than the organization and/or the division. In this example, the engineering department has issued more restrictive, secure, and/or detailed business operations (business operations ++) than the division; has issued more restrictive, secure, and/or detailed data flow functions (data flow ++) than the organization; has issued more restrictive, secure, and/or detailed data integrity functions (data integrity ++) than the organization; and has issued more restrictive, secure, and/or detailed data storage functions (data storage ++) than the organization.</p><p id="p-0306" num="0305">For example, an organization level business operation regarding the design of new products and/or of new product features specifies high-level design and verify guidelines. The division issued more detailed design and verify guidelines. The engineering department issued even more detailed design and verify guidelines.</p><p id="p-0307" num="0306">The analysis system <b>10</b> can evaluate the compliance with the system functions for the various levels. In addition, the analysis system <b>10</b> can evaluate that the division issued system functions are compliant with the organization issued system functions and/or are more restrictive, more secure, and/or more detailed. Similarly, the analysis system <b>10</b> can evaluate that the engineering department issued system functions are compliant with the organization and the division issued system functions and/or are more restrictive, more secure, and/or more detailed.</p><p id="p-0308" num="0307">As is further shown in this example, the organization security functions includes data at rest protection, data loss prevention, data in transit protection, threat management, security governance, and business security. The division has issued more restrictive, more secure, and/or more detailed business security functions (business security +). The engineering department has issued more restrictive, more secure, and/or more detailed data at rest protection (data at rest protection ++), data loss prevention (data loss prevention ++), and data in transit protection (data in transit ++).</p><p id="p-0309" num="0308">The analysis system <b>10</b> can evaluate the compliance with the security functions for the various levels. In addition, the analysis system <b>10</b> can evaluate that the division issued security functions are compliant with the organization issued security functions and/or are more restrictive, more secure, and/or more detailed. Similarly, the analysis system <b>10</b> can evaluate that the engineering department issued security functions are compliant with the organization and the division issued security functions and/or are more restrictive, more secure, and/or more detailed.</p><p id="p-0310" num="0309"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a schematic block diagram of an example of an analysis system <b>10</b> evaluating a system element under test <b>91</b> of a system <b>11</b>. The system element under test <b>91</b> corresponds to a system aspect (or system sector), which includes one or more system elements, one or more system criteria, and one or more system modes.</p><p id="p-0311" num="0310">In this example, the system criteria are shown to includes guidelines, system requirements, system design &#x26; system build (system implementation), and the resulting system. The analysis system <b>10</b> may evaluate the system, or portion thereof, during initial system requirement development, initial design of the system, initial build of the system, operation of the initial system, revisions to the system requirements, revisions to the system design, revisions to the system build, and/or operation of the revised system. A revision to a system includes adding assets, system functions, and/or security functions; deleting assets, system functions, and/or security functions; and/or modifying assets, system functions, and/or security functions.</p><p id="p-0312" num="0311">The guidelines include one or more of business objectives, security objectives, NIST cybersecurity guidelines, system objectives, governmental and/or regulatory requirements, third party requirements, etc. and are used to help create the system requirements. System requirements outline the hardware requirements for the system, the software requirements for the system, the networking requirements for the system, the security requirements for the system, the logical data flow for the system, the hardware architecture for the system, the software architecture for the system, the logical inputs and outputs of the system, the system input requirements, the system output requirements, the system's storage requirements, the processing requirements for the system, system controls, system backup, data access parameters, and/or specification for other system features.</p><p id="p-0313" num="0312">The system requirements are used to help create the system design. The system design includes a high level design (HDL), a low level design (LLD), a detailed level design (DLD), and/or other design levels. High level design is a general design of the system. It includes a description of system architecture; a database design; an outline of platforms, services, and processes the system will require; a description of relationships between the assets, system functions, and security functions; diagrams regarding data flow; flowcharts; data structures; and/or other documentation to enable more detailed design of the system.</p><p id="p-0314" num="0313">Low level design is a component level design that is based on the HLD. It provides the details and definitions for every system component (e.g., HW and SW). In particular, LLD specifies the features of the system components and component specifications. Detailed level design describes the interaction of every component of the system.</p><p id="p-0315" num="0314">The system is built based on the design to produce a resulting system (i.e., the implemented assets). The assets of system operate to perform the system functions and/or security functions.</p><p id="p-0316" num="0315">The analysis system <b>10</b> can evaluate the understanding, implementation, operation and/or self-analysis of the system <b>11</b> at one or more system criteria level (e.g., guidelines, system requirements, system implementation (e.g., design and/or build), and system) in a variety of ways.</p><p id="p-0317" num="0316">The analysis system <b>10</b> evaluates the understanding of the system (or portion thereof) by determining a knowledge level of the system and/or maturity level of system. For example, an understanding evaluation interprets what is known about the system and compares it to what should be known about the system.</p><p id="p-0318" num="0317">As a more specific example, the analysis system evaluates the understanding of the guidelines. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the guidelines to facilitate the understanding of the guidelines. The more incomplete the data regarding the evaluation metrics, the more likely the guidelines are incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the creation and/or use of the guidelines, the more likely the guidelines are not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0319" num="0318">As another more specific example of an understanding evaluation, the analysis system <b>10</b> evaluates the understanding of the system requirements. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the system requirements to facilitate the understanding of the system requirements. The more incomplete the data regarding the evaluation metrics, the more likely the system requirements are incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the creation and/or use of the system requirements, the more likely the system requirements are not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0320" num="0319">As another more specific example of an understanding evaluation, the analysis system <b>10</b> evaluates the understanding of the system design. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the system design to facilitate the understanding of the system design. The more incomplete the data regarding the evaluation metrics, the more likely the system design is incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the creation and/or use of the system design, the more likely the system design is not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0321" num="0320">As another more specific example of an understanding evaluation, the analysis system <b>10</b> evaluates the understanding of the system build. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the system build to facilitate the understanding of the system build. The more incomplete the data regarding the evaluation metrics, the more likely the system build is incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the execution of and/or use of the system build, the more likely the system build is not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0322" num="0321">As another more specific example of an understanding evaluation, the analysis system <b>10</b> evaluates the understanding of the system functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the system build to facilitate the understanding of the system build. The more incomplete the data regarding the evaluation metrics, the more likely the system build is incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the execution of and/or use of the system build, the more likely the system build is not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0323" num="0322">As another more specific example of an understanding evaluation, the analysis system <b>10</b> evaluates the understanding of the security functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the system functions to facilitate the understanding of the system functions. The more incomplete the data regarding the evaluation metrics, the more likely the system functions are incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the execution of and/or use of the system functions, the more likely the system functions are not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0324" num="0323">As another more specific example of an understanding evaluation, the analysis system <b>10</b> evaluates the understanding of the system assets. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the thoroughness of the system assets to facilitate the understanding of the system assets. The more incomplete the data regarding the evaluation metrics, the more likely the system assets are incomplete; which indicates a lack of understanding. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the selection, identification, and/or use of the system assets, the more likely the system assets are not well understood (e.g., lower level of knowledge and/or of system maturity) resulting in a low evaluation rating.</p><p id="p-0325" num="0324">The analysis system <b>10</b> also evaluates the implementation of the system (or portion thereof) by determining how well the system is being, was developed, and/or is being updated. For example, the analysis system <b>10</b> determines how well the assets, system functions, and/or security functions are being developed, have been developed, and/or are being updated based on the guidelines, the system requirements, the system design, and/or the system build.</p><p id="p-0326" num="0325">As a more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the guidelines. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the guidelines. The more incomplete the data regarding the evaluation metrics, the more likely the development of the guidelines is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the guidelines, the more likely the guidelines are not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0327" num="0326">As another more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the system requirements. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the system requirements. The more incomplete the data regarding the evaluation metrics, the more likely the development of the system requirements is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the system requirements, the more likely the system requirements are not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0328" num="0327">As another more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the system design. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the system design. The more incomplete the data regarding the evaluation metrics, the more likely the development of the system design is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the system design, the more likely the system design is not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0329" num="0328">As another more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the system build. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the system build. The more incomplete the data regarding the evaluation metrics, the more likely the development of the system build is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the system build, the more likely the system build is not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0330" num="0329">As another more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the system functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the system functions. The more incomplete the data regarding the evaluation metrics, the more likely the development of the system functions is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the system functions, the more likely the system functions are not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0331" num="0330">As another more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the security functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the security functions. The more incomplete the data regarding the evaluation metrics, the more likely the development of the security functions is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the security functions, the more likely the security functions are not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0332" num="0331">As another more specific example of an implementation evaluation, the analysis system <b>10</b> evaluates the implementation of the system assets. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the development of the system assets. The more incomplete the data regarding the evaluation metrics, the more likely the development of the system assets is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the development of the system assets, the more likely the system assets are not well developed (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0333" num="0332">The analysis system <b>10</b> also evaluates the operation of the system (or portion thereof) by determining how well the system fulfills its objectives. For example, the analysis system <b>10</b> determines how well the assets, system functions, and/or security functions to fulfill the guidelines, the system requirements, the system design, the system build, the objectives of the system, and/or other purpose of the system.</p><p id="p-0334" num="0333">As a more specific example of an operation evaluation, the analysis system <b>10</b> evaluates the operation (i.e., fulfillment) of the guidelines by the system requirements. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the fulfillment of the guidelines by the system requirements. The more incomplete the data regarding the evaluation metrics, the more likely the fulfillment of the guidelines by the system requirements is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the fulfillment of the guidelines by the system requirements, the more likely the system requirements does not adequately fulfill the guidelines (e.g., lower level of system development maturity) resulting in a low evaluation rating.</p><p id="p-0335" num="0334">As another more specific example of an operation evaluation, the analysis system <b>10</b> evaluates the operation (i.e., fulfillment) of the guidelines and/or the system requirements by the system design. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the fulfillment of the guidelines and/or the system requirements by the system design. The more incomplete the data regarding the evaluation metrics, the more likely the fulfillment of the guidelines and/or the system requirements by the system design is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the fulfillment of the guidelines and/or the system requirements by the system design, the more likely the system design does not adequately fulfill the guidelines and/or the system requirements (e.g., lower level of system operation maturity) resulting in a low evaluation rating.</p><p id="p-0336" num="0335">As another more specific example of an operation evaluation, the analysis system <b>10</b> evaluates the operation (i.e., fulfillment) of the guidelines, the system requirements, and/or the system design by the system build. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the fulfillment of the guidelines, the system requirements, and/or the system design by the system build. The more incomplete the data regarding the evaluation metrics, the more likely the fulfillment of the guidelines, the system requirements, and/or the system design by the system build is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the fulfillment of the guidelines, the system requirements, and/or the system design by the system build, the more likely the system build does not adequately fulfill the guidelines, the system requirements, and/or the system design (e.g., lower level of system operation maturity) resulting in a low evaluation rating.</p><p id="p-0337" num="0336">As another more specific example of an operation evaluation, the analysis system <b>10</b> evaluates the operation (i.e., fulfillment) of the guidelines, the system requirements, the system design, the system build, and/or objectives by the operation of the system in performing the system functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the fulfillment of the guidelines, the system requirements, the system design, the system build, and/or objectives regarding the performance of the system functions by the system. The more incomplete the data regarding the evaluation metrics, the more likely the fulfillment of the guidelines, the system requirements, the system design, the system, and/or the objectives regarding the system functions is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the fulfillment of the guidelines, the system requirements, the system design, the system build, and/or the objectives, the more likely the system does not adequately fulfill the guidelines, the system requirements, the system design, the system build, and/or the objectives regarding the system functions (e.g., lower level of system operation maturity) resulting in a low evaluation rating.</p><p id="p-0338" num="0337">As another more specific example of an operation evaluation, the analysis system <b>10</b> evaluates the operation (i.e., fulfillment) of the guidelines, the system requirements, the system design, the system build, and/or objectives by the operation of the system in performing the security functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the fulfillment of the guidelines, the system requirements, the system design, the system build, and/or objectives regarding the performance of the security functions by the system. The more incomplete the data regarding the evaluation metrics, the more likely the fulfillment of the guidelines, the system requirements, the system design, the system, and/or the objectives regarding the security functions is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the fulfillment of the guidelines, the system requirements, the system design, the system build, and/or the objectives, the more likely the system does not adequately fulfill the guidelines, the system requirements, the system design, the system build, and/or the objectives regarding the security functions (e.g., lower level of system operation maturity) resulting in a low evaluation rating.</p><p id="p-0339" num="0338">As another more specific example of an operation evaluation, the analysis system <b>10</b> evaluates the operation (i.e., fulfillment) of the guidelines, the system requirements, the system design, the system build, and/or objectives by the operation of the system functions. For instance, the analysis system <b>10</b> evaluates the policies, processes, procedures, automation, certifications, documentation, and/or other evaluation metric (e.g., evaluation metrics) regarding the fulfillment of the guidelines, the system requirements, the system design, the system build, and/or objectives regarding the performance of the system assets. The more incomplete the data regarding the evaluation metrics, the more likely the fulfillment of the guidelines, the system requirements, the system design, the system, and/or the objectives regarding the system assets is incomplete. The fewer numbers of and/or incompleteness of policies, processes, procedures, automation, documentation, certification, and/or other evaluation metric regarding the fulfillment of the guidelines, the system requirements, the system design, the system build, and/or the objectives, the more likely the system assets do not adequately fulfill the guidelines, the system requirements, the system design, the system build, and/or the objectives (e.g., lower level of system operation maturity) resulting in a low evaluation rating.</p><p id="p-0340" num="0339">The analysis system <b>10</b> also evaluates the self-analysis capabilities of the system (or portion thereof) by determining how well the self-analysis functions are implemented and how they subsequently fulfill the self-analysis objectives. In an example, the self-analysis capabilities of the system are a self-analysis system that overlies the system. Accordingly, the overlaid self-analysis system can be evaluated by the analysis system <b>10</b> in a similar manner as the system under test <b>91</b>. For example, the understanding, implementation, and/or operation of the overlaid self-analysis system can be evaluated with respect to self-analysis guidelines, self-analysis requirements, design of the self-analysis system, build of the self-analysis system, and/or operation of the self-analysis system</p><p id="p-0341" num="0340">As part of the evaluation process, the analysis system <b>10</b> may identify deficiencies and, when appropriate, auto-correct a deficiency. For example, the analysis system <b>10</b> identifies deficiencies in the understanding, implementation, and/or operation of the guidelines, the system requirements, the system design, the system build, the resulting system, and/or the system objectives. For example, the analysis system <b>10</b> obtains addition information from the system via a data gathering process (e.g., producing discovered data) and/or from a system proficiency resource (e.g., producing desired data). The analysis system <b>10</b> uses the discovered data and/or desired data to identify the deficiencies. When possible, the analysis system <b>10</b> auto-corrects the deficiencies. For example, when a software tool that aides in the creation of guidelines and/or system requirements is missing from the system's tool set, the analysis system <b>10</b> can automatically obtain a copy of the missing software tool for the system.</p><p id="p-0342" num="0341"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a schematic block diagram of another example of an analysis system <b>10</b> evaluating a system element under test <b>91</b>. In this example, the analysis system <b>10</b> is evaluating the system element under test <b>91</b> from three evaluation viewpoints: disclosed data, discovered data, and desired data. Disclosed data is the known data of the system at the outset of an analysis, which is typically supplied by a system administrator and/or is obtained from data files of the system. Discovered data is the data discovered about the system by the analysis system <b>10</b> during the analysis. Desired data is the data obtained by the analysis system <b>10</b> from system proficiency resources regarding desired guidelines, system requirements, system design, system build, and/or system operation.</p><p id="p-0343" num="0342">The evaluation from the three evaluation viewpoints may be done serially, in parallel, and/or in a parallel-serial combination to produce three sets of evaluation ratings. One set for disclosed data, one set for discovered data, and one set for desired data.</p><p id="p-0344" num="0343">A set of evaluation ratings includes one or more of: an evaluation rating regarding the understanding of the guidelines; an evaluation rating regarding the understanding of the system requirements; an evaluation rating regarding the understanding of the system design; an evaluation rating regarding the understanding of the system build; an evaluation rating regarding the understanding of the system operation; an evaluation rating regarding the development of the system requirements from the guidelines; an evaluation rating regarding the design from the system requirements; an evaluation rating regarding the system build from the design; an evaluation rating regarding the system operation based on the system design and/or system build; an evaluation rating regarding the guidelines; an evaluation rating regarding the system requirements; an evaluation rating regarding the system design; an evaluation rating regarding the system build; and/or an evaluation rating regarding the system operation.</p><p id="p-0345" num="0344"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a schematic block diagram of another example of an analysis system <b>10</b> evaluating a system element under test <b>91</b>. In this example, the analysis system <b>10</b> is evaluating the system element under test <b>91</b> from three evaluation viewpoints: disclosed data, discovered data, and desired data with regard to security functions. The evaluation from the three evaluation viewpoints for the security functions may be done serially, in parallel, and/or in a parallel-serial combination to produce three sets of evaluation ratings with respect to security functions: one for disclosed data, one for discovered data, and one for desired data.</p><p id="p-0346" num="0345"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a schematic block diagram of another example of an analysis system <b>10</b> evaluating a system element under test <b>91</b>. In this example, the analysis system <b>10</b> is evaluating the system element under test <b>91</b> from three evaluation viewpoints and from three evaluation modes. For example, disclosed data regarding assets, discovered data regarding assets, desired data regarding assets, disclosed data regarding system functions, discovered data regarding system functions, desired data regarding system functions, disclosed data regarding security functions, discovered data regarding security functions, and desired data regarding security functions.</p><p id="p-0347" num="0346">The evaluation from the nine evaluation viewpoints &#x26; evaluation mode combinations may be done serially, in parallel, and/or in a parallel-serial combination to produce nine sets of evaluation ratings one for disclosed data regarding assets, one for discovered data regarding assets, one for desired data regarding assets, one for disclosed data regarding system functions, one for discovered data regarding system functions, one for desired data regarding functions, one for disclosed data regarding security functions, one for discovered data regarding security functions, and one for desired data regarding security functions.</p><p id="p-0348" num="0347"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is a schematic block diagram of an example of the functioning of an analysis system <b>10</b> evaluating a system element under test <b>91</b>. Functionally, the analysis system <b>10</b> includes evaluation criteria <b>211</b>, evaluation mode <b>212</b>, analysis perspective <b>213</b>, analysis viewpoint <b>214</b>, analysis categories <b>215</b>, data gathering <b>216</b>, pre-processing <b>217</b>, and analysis metrics <b>218</b> to produce one or more ratings <b>219</b>. The evaluation criteria <b>211</b> includes guidelines, system requirements, system design, system build, and system operation. The evaluation mode <b>212</b> includes assets, system functions, and security functions. The evaluation criteria <b>211</b> and the evaluation mode <b>212</b> are part of the system aspect, which corresponds to the system, or portion thereof, being evaluated.</p><p id="p-0349" num="0348">The analysis perspective <b>213</b> includes understanding, implementation, operation, and self-analysis. The analysis viewpoint includes disclosed, discovered, and desired. The analysis categories <b>215</b> include identify, protect, detect, respond, and recover. The analysis perspective <b>213</b>, the analysis viewpoint <b>214</b>, and the analysis categories correspond to how the system, or portion thereof, will be evaluated. For example, the system, or portion thereof, is being evaluated regarding the understanding of the system's ability to identify assets, system functions, and/or security functions from discovered data.</p><p id="p-0350" num="0349">The analysis metrics <b>218</b> includes process, policy, procedure, automation, certification, and documentation. The analysis metric <b>218</b> and the pre-processing <b>217</b> corresponds to manner of evaluation. For example, the policies regarding system's ability to identify assets, system functions, and/or security functions from discovered data of the system, or portion thereof, are evaluated to produce an understanding evaluation rating.</p><p id="p-0351" num="0350">In an example of operation, the analysis system <b>10</b> determines what portion of the system is evaluated (i.e., a system aspect). As such, the analysis system <b>10</b> determines one or more system elements (e.g., including one or more system assets which are physical assets and/or conceptual assets), one or more system criteria (e.g., guidelines, system requirements, system design, system build, and/or system operation), and one or more system modes (e.g., assets, system functions, and security functions). The analysis system <b>10</b> may determine the system aspect in a variety of ways. For example, the analysis system <b>10</b> receives an input identifying the system aspect from an authorized operator of the system (e.g., IT personnel, executive personnel, etc.). As another example, the analysis system determines the system aspect in a systematic manner to evaluate various combinations of system aspects as part of an overall system evaluation. The overall system evaluation may be done one time, periodically, or continuously. As yet another example, the analysis system determines the system aspect as part of a systematic analysis of a section of the system, which may be done one time, periodically, or continuously.</p><p id="p-0352" num="0351">The analysis system then determines how the system aspect is to be evaluated by selecting one or more analysis perspectives (understanding, implementation, operation, and self-analysis), one or more analysis viewpoints (disclosed, discovered, and desired), and one or more analysis categories (identify, protect, detect, respond, and recover). The analysis system <b>10</b> may determine how the system aspect is to be evaluated in a variety of ways. For example, the analysis system <b>10</b> receives an input identifying how the system aspect is to be evaluated from an authorized operator of the system (e.g., IT personnel, executive personnel, etc.). As another example, the analysis system determines how the system aspect is to be evaluated in a systematic manner to evaluate the system aspect in various combinations of analysis perspectives, analysis viewpoints, and analysis categories as part of an overall system evaluation. The overall system evaluation may be done one time, periodically, or continuously. As yet another example, the analysis system determines how the system aspect is to be evaluated as part of a systematic analysis of a section of the system, which may be done one time, periodically, or continuously.</p><p id="p-0353" num="0352">The analysis system <b>10</b> also determines one or more analysis metrics (e.g., process, policy, procedure, automation, certification, and documentation) regarding the manner for evaluating the system aspect in accordance with how it's to be evaluated. A policy sets out a strategic direction and includes high-level rules or contracts regarding issues and/or matters. For example, all software shall be a most recent version of the software. A process is a set of actions for generating outputs from inputs and includes one or more directives for generating outputs from inputs. For example, a process regarding the software policy is that software updates are to be performed by the IT department and all software shall be updated within one month of the release of the new version of software.</p><p id="p-0354" num="0353">A procedure is the working instructions to complete an action as may be outlined by a process. For example, the IT department handling software updates includes a procedure that describes the steps for updating the software, verifying that the updated software works, and recording the updating and verification in a software update log. Automation is in regard to the level of automation the system includes for handling actions, issues, and/or matters of policies, processes, and/or procedures. Documentation is in regard to the level of documentation the system has regard guidelines, system requirements, system design, system build, system operation, system assets, system functions, security functions, system understanding, system implementation, operation of the system, policies, processes, procedures, etc. Certification is in regard to certifications of the system, such as maintenance certification, regulatory certifications, etc.</p><p id="p-0355" num="0354">In an example, the analysis system <b>10</b> receives an input identifying manner in which to evaluate the system aspect from an authorized operator of the system (e.g., IT personnel, executive personnel, etc.). As another example, the analysis system determines the manner in which to evaluate the system aspect in a systematic manner to evaluate the system aspect in various combinations of analysis metrics as part of an overall system evaluation. The overall system evaluation may be done one time, periodically, or continuously. As yet another example, the analysis system determines the manner in which to evaluate the system aspect as part of a systematic analysis of a section of the system, which may be done one time, periodically, or continuously.</p><p id="p-0356" num="0355">Once the analysis system has determined the system aspect, how it is to be evaluated, and the manner for evaluation, the data gathering function <b>216</b> gathers data relevant to the system aspect, how it's to be evaluated, and the manner of evaluation from the system <b>11</b>, from resources that store system information <b>210</b> (e.g., from the system, from a private storage of the analysis system, etc.), and/or from one or more system proficiency resources <b>22</b>. For example, a current evaluation is regarding an understanding (analysis perspective) of policies (analysis metric) to identify (analysis category) assets (evaluation mode) of an engineering department (system elements) regarding operations (evaluation criteria) that the assets perform based on discovered data (analysis viewpoint). As such, the data gathering function <b>216</b> gathers data regarding policies to identify assets of the engineering department and the operations they perform using one or more data discovery tools.</p><p id="p-0357" num="0356">The pre-processing function <b>217</b> processes the gathered data by parsing the data, tagging the data, normalizing the data, and/or de-duplicating the data. The analysis system evaluations the processed data in accordance with the selected analysis metric to produce one or more ratings <b>219</b>. For example, the analysis system would produce a rating regarding the understanding of policies to identify assets of an engineering department regarding operations that the assets perform based on discovered data. The rating <b>219</b> is on a scale from low to high. In this example, a low rating indicates issues with the understanding and a high rating indicates no issues with the understanding.</p><p id="p-0358" num="0357"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is a schematic block diagram of another example of the functioning of an analysis system <b>10</b> evaluating a system element under test <b>91</b>. The functioning of the analysis system includes a deficiency perspective function <b>230</b>, a deficiency evaluation viewpoint function <b>31</b>, and an auto-correction function <b>233</b>.</p><p id="p-0359" num="0358">The deficiency perspective function <b>230</b> receives one or more ratings <b>219</b> and may also receive the data used to generate the ratings <b>219</b>. From these inputs, the deficiency perspective function <b>230</b> determines whether there is an understanding issue, an implementation issue, and/or an operation issue. For example, an understanding (analysis perspective) issue relates to a low understanding evaluation rating for a specific evaluation regarding policies (analysis metric) to identify (analysis category) assets (evaluation mode) of an engineering department (system elements) regarding operations (evaluation criteria) that the assets perform based on discovered data (analysis viewpoint).</p><p id="p-0360" num="0359">As another example, an implementation (analysis perspective) issue relates to a low implementation evaluation rating for a specific evaluation regarding implementation and/or use of policies (analysis metric) to identify (analysis category) assets (evaluation mode) of an engineering department (system elements) regarding operations (evaluation criteria) that the assets perform based on discovered data (analysis viewpoint). As yet another example, an operation (analysis perspective) issue relates to a low operation evaluation rating for a specific evaluation regarding consistent, reliable, and/or accurate mechanism(s) to identify (analysis category) assets (evaluation mode) of an engineering department (system elements) regarding operations (evaluation criteria) that the assets perform based on discovered data (analysis viewpoint) and on policies (analysis metric).</p><p id="p-0361" num="0360">When an understanding, implementation, and/or operation issue is identified, the deficiency evaluation viewpoint function <b>231</b> determines whether the issue(s) is based on disclosed data, discovered data, and/or desired data. For example, an understanding issue may be based on a difference between disclosed data and discovered data. As a specific example, the disclosed data includes a policy outline how to identify (analysis category) assets (evaluation mode) of an engineering department (system elements) regarding operations (evaluation criteria) that the assets perform, which is listed as version 1.12 and a last revision date of Oct. 2, 2020. In this specific example, the discovered data includes the same policy, but is has been updated to version 1.14 and the last revision date as Nov. 13, 2020. As such, the deficiency evaluation viewpoint function identifies a deficiency <b>232</b> in the disclosed data as being an outdated policy.</p><p id="p-0362" num="0361">As another specific example, the disclosed data includes a policy outline how to identify (analysis category) assets (evaluation mode) of an engineering department (system elements) regarding operations (evaluation criteria) that the assets perform. The disclosed data also shows an inconsistent use and/or application of the policy resulting one or more assets not being properly identified. In this instance, the deficiency evaluation viewpoint function identifies a deficiency <b>232</b> in the disclosed data as being inconsistent use and/or application of the policy.</p><p id="p-0363" num="0362">The auto-correct function <b>233</b> receives a deficiency <b>232</b> and interprets it to determine a deficiency type, i.e., a nature of the understanding issue, the implementation issue, and/or the operation issues. Continuing with the outdated policy example, the nature of the understanding issue is that there is a newer version of the policy. Since there is a newer version available, the auto-correct function <b>233</b> can update the policy to the newer version for the system (e.g., an auto-correction). In addition to making the auto-correction <b>235</b>, the analysis system creates an accounting <b>236</b> of the auto-correction (e.g., creates a record). The record includes an identity of the deficiency, date information, what auto-correction was done, how it was done, verification that it was done, and/or more or less data as may be desired for recording auto-corrections.</p><p id="p-0364" num="0363">As another specific example, a deficiency <b>232</b> is discovered that an asset exists in the engineering department that was not included in the disclosed data. This deficiency may include one or more related deficiencies. For example, a deficiency of design, a deficiency of build, a deficiency is oversight of asset installation, etc. The deficiencies of design, build, and/or installation oversight can be auto-corrected; the deficiency of an extra asset cannot. With regard to the deficiency of the extra asset, the analysis system generates a report regarding the extra asset and the related deficiencies.</p><p id="p-0365" num="0364"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is a diagram of an example of evaluation options of an analysis system <b>10</b> for evaluating a system element under test <b>91</b>. The evaluation options are shown in a three-dimensional tabular form. The rows include analysis perspective <b>213</b> options (e.g., understanding, implementation, and operation). The columns includes analysis viewpoint <b>214</b> option (e.g., disclosed, discovered, and desired). The third dimension includes analysis output <b>240</b> options (e.g., ratings <b>219</b>, deficiencies in disclosed data, deficiencies in discovered data, deficiencies in disclosed to discovered data, deficiencies in disclosed to desired data, deficiencies in discovered to desired data, and auto-correct.</p><p id="p-0366" num="0365">The analysis system <b>10</b> can evaluate the system element under test <b>91</b> (e.g., system aspect) in one or more combinations of a row selection, a column selection, and/or a third dimension selection. For example, the analysis system performs an evaluation from an understanding perspective, a disclosed data viewpoint, and a ratings output. As another example, the analysis system performs an evaluation from an understanding perspective, all viewpoints, and a ratings output.</p><p id="p-0367" num="0366"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a diagram of another example of evaluation options of an analysis system <b>10</b> for evaluating a system element under test <b>91</b> (e.g., system aspect). The evaluation options are shown in the form of a table. The rows are assets (physical and conceptual) and the columns are system functions. The analysis system <b>10</b> can evaluate the system element under test <b>91</b> (e.g., system aspect) in one or more combinations of a row selection and a column selection.</p><p id="p-0368" num="0367">For example, the analysis system <b>10</b> can evaluate user HW with respect to business operations. As another example, the analysis system <b>10</b> can evaluate physical assets with respect to data flow. As another example, the analysis system <b>10</b> can evaluate user SW with respect to all system functions.</p><p id="p-0369" num="0368"><figref idref="DRAWINGS">FIG. <b>33</b></figref> is a diagram of another example of evaluation options of an analysis system <b>10</b> for evaluating a system element under test <b>91</b> (e.g., system aspect). The evaluation options are shown in the form of a table. The rows are security functions and the columns are system functions. The analysis system <b>10</b> can evaluate the system element under test <b>91</b> (e.g., system aspect) in one or more combinations of a row selection and a column selection.</p><p id="p-0370" num="0369">For example, the analysis system <b>10</b> can evaluate threat detection with respect to business operations. As another example, the analysis system <b>10</b> can evaluate all security functions with respect to data flow. As another example, the analysis system <b>10</b> can evaluate threat avoidance with respect to all system functions.</p><p id="p-0371" num="0370"><figref idref="DRAWINGS">FIG. <b>34</b></figref> is a diagram of another example of evaluation options of an analysis system <b>10</b> for evaluating a system element under test <b>91</b> (e.g., system aspect). The evaluation options are shown in the form of a table. The rows are assets (physical and conceptual) and the columns are security functions. The analysis system <b>10</b> can evaluate the system element under test <b>91</b> (e.g., system aspect) in one or more combinations of a row selection and a column selection.</p><p id="p-0372" num="0371">For example, the analysis system <b>10</b> can evaluate user HW with respect to threat recovery. As another example, the analysis system <b>10</b> can evaluate physical assets with respect to threat resolution. As another example, the analysis system <b>10</b> can evaluate user SW with respect to all security functions.</p><p id="p-0373" num="0372"><figref idref="DRAWINGS">FIG. <b>35</b></figref> is a schematic block diagram of an embodiment of an analysis system <b>10</b> that includes one or more computing entities <b>16</b>, one or more databases <b>275</b>, one or more data extraction modules <b>80</b>, one or more system user interface modules <b>81</b>, and one or more remediation modules <b>257</b>. The computing entity(ies) <b>16</b> is configured to include a data input module <b>250</b>, a pre-processing module <b>251</b>, a data analysis module <b>252</b>, an analytics modeling module <b>253</b>, an evaluation processing module <b>254</b>, a data output module <b>255</b>, and a control module <b>256</b>. The database <b>275</b>, which includes one or more databases, stores the private data for a plurality of systems (e.g., systems A-x) and stores analytical data <b>270</b> of the analysis system <b>10</b>.</p><p id="p-0374" num="0373">In an example, the system <b>11</b> provides input <b>271</b> to the analysis system <b>10</b> via the system user interface module <b>80</b>. The system user interface module <b>80</b> provides a user interface for an administrator of the system <b>11</b> and provides a s secure end-point of a secure data pipeline between the system <b>11</b> and the analysis system <b>10</b>. While the system user interface module <b>81</b> is part of the analysis system, it is loaded on and is executed on the system <b>11</b>.</p><p id="p-0375" num="0374">Via the system user interface module <b>81</b>, the administrator makes selections as to how the system is to be evaluated and the desired output from the evaluation. For example, the administrator selects evaluate system, which instructs the analysis system <b>10</b> to evaluate the system from most every, if not every, combination of system aspect (e.g., system element, system criteria, and system mode), evaluation aspect (e.g., evaluation perspective, evaluation viewpoint, and evaluation category), evaluation metric (e.g., process, policy, procedure, automation, documentation, and certification), and analysis output (e.g., an evaluation rating, deficiencies identified, and auto-correction of deficiencies). As another example, the administrator selects one or more system aspects, one or more evaluation aspects, one or more evaluation metrics, and/or one or more analysis outputs.</p><p id="p-0376" num="0375">The analysis system <b>10</b> receives the evaluation selections as part of the input <b>271</b>. A control module <b>256</b> interprets the input <b>271</b> to determine what part of the system is to be evaluated (e.g., system aspects), how the system is to be evaluated (e.g., evaluation aspects), the manner in which the system is to be evaluated (e.g., evaluation metrics), and/or the resulting evaluation output (e.g., an evaluation rating, a deficiency report, and/or auto-correction). From the interpretation of the input, the control module <b>256</b> generates data gathering parameters <b>263</b>, pre-processing parameters <b>264</b>, data analysis parameters <b>265</b>, and evaluation parameters <b>266</b>.</p><p id="p-0377" num="0376">The control module <b>256</b> provides the data gathering parameters <b>263</b> to the data input module <b>250</b>. The data input module <b>250</b> interprets the data gathering parameters <b>263</b> to determine data to gather. For example, the data gathering parameters <b>263</b> are specific to the evaluation to be performed by the analysis system <b>10</b>. As a more specific example, if the analysis system <b>10</b> is evaluating the understanding of the policies, processes, documentation, and automation regarding the assets built for an engineering department, then the data gathering parameters <b>263</b> would prescribe gathering data related to policies, processes, documentation, and automation regarding the assets built for the engineering department.</p><p id="p-0378" num="0377">The data input module <b>250</b> may gather (e.g., retrieve, request, etc.) from a variety of sources. For example, the data input module <b>250</b> gathers data <b>258</b> from the data extraction module <b>80</b>. In this example, the data input module <b>250</b> provides instructions to the data extraction module <b>80</b> regarding the data being requested. The data extraction module <b>80</b> pulls the requested data from system information <b>210</b>, which may be centralized data of the system, system administration data, and/or data from assets of the system.</p><p id="p-0379" num="0378">As another example, the data input module <b>250</b> gathers data from one or more external data feeds <b>259</b>. A source of an external data feed includes one or more business associate computing devices <b>23</b>, one or more publicly available servers <b>27</b>, and/or one or more subscriber servers <b>28</b>. Other sources of external data feeds <b>259</b> includes BOT computing devices <b>25</b>, and/or bad actor computing devices <b>26</b>. Typically, the data input module <b>250</b> does not seek data inputs from BOT computing devices <b>25</b> and/or bad actor computing devices <b>26</b> except under certain circumstances involving specific types of cybersecurity risks.</p><p id="p-0380" num="0379">As another example, the data input module <b>250</b> gathers system proficiency data <b>260</b> from one or more system proficiency resources <b>22</b>. As a specific example, for a data request that includes desired data, the data input module <b>250</b> addresses one or more system proficiencies resources <b>22</b> to obtain the desired system proficiency data <b>260</b>. For example, system proficiency data <b>260</b> includes information regarding best-in-class practices (for system requirements, for system design, for system implementation, and/or for system operation), governmental and/or regulatory requirements, security risk awareness and/or risk remediation information, security risk avoidance, performance optimization information, system development guidelines, software development guideline, hardware requirements, networking requirements, networking guidelines, and/or other system proficiency guidance.</p><p id="p-0381" num="0380">As another example, the data input module <b>250</b> gathers stored data <b>261</b> from the database <b>275</b>. The stored data <b>261</b> is previously stored data that is unique to the system <b>11</b>, is data from other systems, is previously processed data, is previously stored system proficiency data, and/or is previously stored data that assists in the current evaluation of the system.</p><p id="p-0382" num="0381">The data input module <b>250</b> provides the gathered data to the pre-processing module <b>251</b>. Based on the pre-processing parameters <b>264</b> (e.g., parse, tag, normalize, de-duplication, sort, filter, etc.), the pre-processing module <b>251</b> processes the gathered data to produce pre-processed data <b>267</b>. The pre-processed data <b>267</b> may be stored in the database <b>275</b> and later retrieved as stored data <b>261</b>.</p><p id="p-0383" num="0382">The analysis modeling module <b>253</b> retrieves stored data <b>261</b> and/or stored analytics <b>262</b> from the database <b>275</b>. The analysis modeling module <b>253</b> operates to increase the artificial intelligence of the analysis system <b>10</b>. For example, the analysis modeling module <b>253</b> evaluates stored data from one or more systems in a variety of ways to test the evaluation processes of the analysis system. As a more specific example, the analysis modeling module <b>253</b> models the evaluation of understanding of the policies, processes, documentation, and automation regarding the assets built for an engineering department across multiple systems to identify commonalities and/or deviations. The analysis modeling module <b>253</b> interprets the commonalities and/or deviations to adjust parameters of the evaluation of understanding and models how the adjustments affect the evaluation of understanding. If the adjustments have a positive effect, the analysis modeling module <b>253</b> stores them as analytics <b>262</b> and/or analysis modeling <b>268</b> in the database <b>275</b>.</p><p id="p-0384" num="0383">The data analysis module <b>252</b> receives the pre-processed data <b>267</b>, the data analysis parameters <b>265</b> and may further receive optional analysis modeling data <b>268</b>. The data analysis parameters <b>265</b> includes identify of selected evaluation categories (e.g., identify, protect, detect, respond, and recover), identity of selected evaluation sub-categories, identify of selected evaluation sub-sub categories, identity of selected analysis metrics (e.g., process, policy, procedure, automation, certification, and documentation), grading parameters for the selected analysis metrics (e.g., a scoring scale for each type of analysis metric), identity of selected analysis perspective (e.g., understanding, implementation, operation, and self-analysis), and/or identity of selected analysis viewpoint (e.g., disclosed, discovered, and desired).</p><p id="p-0385" num="0384">The data analysis module <b>252</b> generates one or more ratings <b>219</b> for the pre-processed data <b>267</b> based on the data analysis parameters <b>265</b>. The data analysis module <b>252</b> may adjust the generation of the one or more rating <b>219</b> based on the analysis modeling data <b>268</b>. For example, the data analysis module <b>252</b> evaluates the understanding of the policies, processes, documentation, and automation regarding the assets built for an engineering department based on the pre-processed data <b>267</b> to produce at least one evaluation rating <b>219</b>.</p><p id="p-0386" num="0385">Continuing with this example, the analysis modeling <b>268</b> is regarding the evaluation of understanding of the policies, processes, documentation, and automation regarding the assets built for an engineering department of a plurality of different organizations operating on a plurality of different systems. The modeling indicates that if processes are well understood, the understanding of the policies is less significant in the overall understanding. In this instance, the data analysis module <b>252</b> may adjusts its evaluation rating of the understanding to a more favorably rating if the pre-processed data <b>267</b> correlates with the modeling (e.g., good understanding of processes).</p><p id="p-0387" num="0386">The data analysis module <b>252</b> provides the rating(s) <b>219</b> to the data output module <b>255</b> and to the evaluation processing module <b>254</b>. The data output module <b>255</b> provides the rating(s) <b>219</b> as an output <b>269</b> to the system user interface module <b>81</b>. The system user interface module <b>81</b> provides a graphical rendering of the rating(s) <b>219</b>.</p><p id="p-0388" num="0387">The evaluation processing module <b>254</b> processes the rating(s) <b>219</b> based on the evaluation parameters <b>266</b> to identify deficiencies <b>232</b> and/or to determine auto-corrections <b>235</b>. The evaluation parameters <b>266</b> provide guidance on how to evaluate the rating(s) <b>219</b> and whether to obtain data (e.g., pre-processed data, stored data, etc.) to assist in the evaluation. The evaluation guidance includes how deficiencies are to be identified. For example, identify the deficiencies based on the disclosed data, based on the discovered data, based on a differences between the disclosed and discovered data, based on a differences between the disclosed and desired data, and/or based on a differences between the discovered and desired data. The evaluation guidance further includes whether auto-correction is enabled. The evaluation parameters <b>266</b> may further includes deficiency parameters, which provide a level of tolerance between the disclosed, discovered, and/or desired data when determining deficiencies.</p><p id="p-0389" num="0388">The evaluation processing module <b>254</b> provides deficiencies <b>232</b> and/or the auto-corrections <b>235</b> to the data output module <b>255</b>. The data output module <b>255</b> provides the deficiencies <b>232</b> and/or the auto-corrections <b>235</b> as an output <b>269</b> to the system user interface module <b>81</b> and to the remediation module <b>257</b>. The system user interface module <b>81</b> provides a graphical rendering of the deficiencies <b>232</b> and/or the auto-corrections <b>235</b>.</p><p id="p-0390" num="0389">The remediation module <b>257</b> interprets the deficiencies <b>232</b> and the auto-corrections <b>235</b> to identify auto-corrections to be performed within the system. For example, if a deficiency is a computing device having an outdated user software application, the remediation module <b>257</b> coordinates obtaining a current copy of the user software application, uploading it on the computing device, and updating maintenance logs.</p><p id="p-0391" num="0390"><figref idref="DRAWINGS">FIG. <b>36</b></figref> is a schematic block diagram of an embodiment of a portion of an analysis system <b>10</b> coupled to a portion of the system <b>11</b>. In particular, the data output module <b>255</b> of the analysis system <b>10</b> is coupled to a plurality of remediation modules <b>257</b>-<b>1</b> through <b>257</b>-<i>n</i>. Each remediation module <b>257</b> is coupled to one or more system assets <b>280</b>-<b>1</b> through <b>280</b>-<i>n. </i></p><p id="p-0392" num="0391">A remediation module <b>257</b> receives a corresponding portion of the output <b>269</b>. For example, remediation module <b>257</b>-<b>1</b> receives output <b>269</b>-<b>1</b>, which is regarding an evaluation rating, deficiency, and/or an auto-correction of system asset <b>280</b>-<b>1</b>. Remediation module <b>257</b>-<b>1</b> may auto-correct a deficiency of the system element or a system asset thereof. Alternatively or in addition, the remediation module <b>257</b>-<b>1</b> may quarantine the system asset or system element thereof if the deficiency cannot be auto-corrected and the deficiency exposes the system to undesired risks, undesired liability, and/or undesired performance degradation.</p><p id="p-0393" num="0392"><figref idref="DRAWINGS">FIG. <b>37</b></figref> is a schematic block diagram of another embodiment of a portion of an analysis system <b>10</b> coupled to a portion of the system <b>11</b>. In particular, the data input module <b>250</b> of the analysis system <b>10</b> is coupled to a plurality of data extraction modules <b>80</b>-<b>1</b> through <b>80</b>-<i>n</i>. Each data extraction module <b>80</b> is coupled to a system data source <b>290</b> of the system <b>11</b>. Each of the system data sources produce system information <b>210</b> regarding a corresponding portion of the system. A system data source <b>290</b>-<b>1</b> through <b>290</b>-<i>n </i>may be an Azure EventHub, Cisco Advanced Malware Protection (AMP), Cisco Email Security Appliance (ESA), Cisco Umbrella, NetFlow, and/or Syslog. In addition, a system data source may be a system asset, a system element, and/or a storage device storing system information <b>210</b>.</p><p id="p-0394" num="0393">An extraction data migration module <b>293</b> coordinates the collection of system information <b>210</b> as extracted data <b>291</b>-<b>1</b> through <b>291</b>-<i>n</i>. An extraction data coordination module <b>292</b> coordinates the forwarding of the extracted data <b>291</b> as data <b>258</b> to the data input module <b>250</b>.</p><p id="p-0395" num="0394"><figref idref="DRAWINGS">FIG. <b>38</b></figref> is a schematic block diagram of an embodiment of a data extraction module <b>80</b> of an analysis system <b>10</b> coupled to a system <b>11</b>. The data extraction module <b>80</b> includes a tool one or more interface modules <b>311</b>, one or more processing module <b>312</b>, and one or more network interfaces <b>313</b>. The network interface <b>313</b> provides a network connections that allows the data extraction module <b>80</b> to be coupled to the one or more computing entities <b>16</b> of the analysis system <b>10</b>. The tool interface <b>311</b> allows the data extraction module <b>80</b> to interact with tools of the system <b>11</b> to obtain system information from system data sources <b>290</b>.</p><p id="p-0396" num="0395">The system <b>11</b> includes one or more tools that can be accessed by the data extraction module <b>80</b> to obtain system information from one or more data sources <b>290</b>-<b>1</b> through <b>290</b>-<i>n</i>. The tools include one or more data segmentation tools <b>300</b>, one or more boundary detection tools <b>301</b>, one or more data protection tools <b>302</b>, one or more infrastructure management tools <b>303</b>, one or more encryption tools <b>304</b>, one or more exploit protection tools <b>305</b>, one or more malware protection tools <b>306</b>, one or more identity management tools <b>307</b>, one or more access management tools <b>308</b>, one or more system monitoring tools, and/or one or more vulnerability management tools <b>310</b>.</p><p id="p-0397" num="0396">A system tool may also be an infrastructure management tool, a network monitoring tool, a network strategy and planning tool, a network managing tool, a Simple Network Management Protocol (SNMP) tool, a telephony monitoring tool, a firewall monitoring tool, a bandwidth monitoring tool, an IT asset inventory management tool, a network discovery tool, a network asset discovery tool, a software discovery tool, a security discovery tool, an infrastructure discovery tool, Security Information &#x26; Event Management (SIEM) tool, a data crawler tool, and/or other type of tool to assist in discovery of assets, functions, security issues, implementation of the system, and/or operation of the system.</p><p id="p-0398" num="0397">Depending on the data gathering parameters, the tool interface <b>311</b> engages a system tool to retrieve system information. For example, the tool interface <b>311</b> engages the identity management tool to identify assets in the engineering department. The processing module <b>312</b> coordinates requests from the analysis system <b>10</b> and responses to the analysis system <b>10</b>.</p><p id="p-0399" num="0398"><figref idref="DRAWINGS">FIG. <b>39</b></figref> is a schematic block diagram of another embodiment of an analysis system <b>10</b> that includes one or more computing entities <b>16</b>, one or more databases <b>275</b>, one or more data extraction modules <b>80</b>, and one or more system user interface modules <b>81</b>. The computing entity(ies) <b>16</b> is configured to include a data input module <b>250</b>, a pre-processing module <b>251</b>, a data analysis module <b>252</b>, an analytics modeling module <b>253</b>, a data output module <b>255</b>, and a control module <b>256</b>. The database <b>275</b>, which includes one or more databases, stores the private data for a plurality of systems (e.g., systems A-x) and stores analytical data <b>270</b> of the analysis system <b>10</b>.</p><p id="p-0400" num="0399">This embodiment operates similarly to the embodiment of <figref idref="DRAWINGS">FIG. <b>35</b></figref> with the removal of the evaluation module <b>254</b>, which produces deficiencies <b>232</b> and auto-corrections <b>235</b>, and the removal of the remediation modules <b>257</b>. As such, this analysis system <b>10</b> produces evaluation ratings <b>219</b> as the output <b>269</b>.</p><p id="p-0401" num="0400"><figref idref="DRAWINGS">FIG. <b>40</b></figref> is a schematic block diagram of another embodiment of an analysis system <b>10</b> that is similar to the embodiment of <figref idref="DRAWINGS">FIG. <b>39</b></figref>. This embodiment does not include a pre-processing module <b>251</b>. As such, the data collected by the data input module <b>250</b> is provided directly to the data analysis module <b>252</b>.</p><p id="p-0402" num="0401"><figref idref="DRAWINGS">FIG. <b>41</b></figref> is a schematic block diagram of an embodiment of a data analysis module <b>252</b> of an analysis system <b>10</b>. The data analysis module <b>252</b> includes a data module <b>321</b> and an analysis &#x26; score module <b>336</b>. The data module <b>321</b> includes a data parse module <b>320</b>, one or more data storage modules <b>322</b>-<b>334</b>, and a source data matrix <b>335</b>. A data storage module <b>322</b>-<b>334</b> may be implemented in a variety of ways. For example, a data storage module is a buffer. As another example, a data storage module is a section of memory (<b>45</b>, <b>56</b>, <b>57</b>, and/or <b>62</b> of the <figref idref="DRAWINGS">FIG. <b>2</b></figref> series) of a computing device (e.g., an allocated, or ad hoc, addressable section of memory). As another example, a data storage module is a storage unit (e.g., a computing device used primarily for storage). As yet another example, a data storage module is a section of a database (e.g., an allocated, or ad hoc, addressable section of a database).</p><p id="p-0403" num="0402">The data module <b>321</b> operates to provide the analyze &#x26; score module <b>336</b> with source data <b>337</b> selected from incoming data based on one or more data analysis parameters <b>265</b>. The data analysis parameter(s) <b>265</b> indicate(s) how the incoming data is to be parsed (if at all) and how it is to be stored within the data storage modules <b>322</b>-<b>334</b>. A data analysis parameter <b>265</b> includes system aspect storage parameters <b>345</b>, evaluation aspect storage parameters <b>346</b>, and evaluation metric storage parameters <b>347</b>. A system aspect storage parameter <b>345</b> may be null or includes information to identify one or more system aspects (e.g., system element, system criteria, and system mode), how the data relating to system aspects is to be parsed, and how the system aspect parsed data is to be stored.</p><p id="p-0404" num="0403">An evaluation aspect storage parameter <b>346</b> may be null or includes information to identify one or more evaluation aspects (e.g., evaluation perspective, evaluation viewpoint, and evaluation category), how the data relating to evaluation aspects is to be parsed, and how the evaluation aspect parsed data is to be stored. An evaluation metric storage parameter <b>347</b> may be null or includes information to identify one or more evaluation metrics (e.g., process, policy, procedure, certification, documentation, and automation), how the data relating to evaluation metrics is to be parsed, and how the evaluation metric parsed data is to be stored. Note that the data module <b>321</b> interprets the data analysis parameters <b>265</b> collectively such that parsing and storage are consistent with the parameters.</p><p id="p-0405" num="0404">The data parsing module <b>320</b> parses incoming data in accordance with the system aspect storage parameters <b>345</b>, evaluation aspect storage parameters <b>346</b>, and evaluation metric storage parameters <b>347</b>, which generally correspond to what part of the system is being evaluation, how the system is being evaluated, the manner of evaluation, and/or a desired analysis output. As such, incoming data may be parsed in a variety of ways. The data storage modules <b>322</b>-<b>334</b> are assigned to store parsed data in accordance with the storage parameters <b>345</b>-<b>347</b>. For example, the incoming data, which includes pre-processed data <b>267</b>, other external feed data <b>259</b>, data <b>258</b> received via a data extraction module, stored data <b>261</b>, and/or system proficiency data <b>260</b>, is parsed based on system criteria (of the system aspect) and evaluation viewpoint (of the evaluation aspect). As a more specific example, the incoming data is parsed into, and stored, as follows:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0405">disclosed guideline data that is stored in a disclosed guideline data storage module <b>322</b>;</li>        <li id="ul0002-0002" num="0406">discovered guideline data that is stored in a discovered guideline data storage module <b>323</b>;</li>        <li id="ul0002-0003" num="0407">desired guideline data that is stored in a desired guideline data storage module <b>324</b>;</li>        <li id="ul0002-0004" num="0408">disclosed system requirement (sys. req.) data that is stored in a disclosed system requirement data storage module <b>325</b>;</li>        <li id="ul0002-0005" num="0409">discovered system requirement (sys. req.) data that is stored in a discovered system requirement data storage module <b>326</b>;</li>        <li id="ul0002-0006" num="0410">desired system requirement (sys. req.) data that is stored in a desired system requirement data storage module <b>327</b>;</li>        <li id="ul0002-0007" num="0411">disclosed design and/or build data that is stored in a disclosed design and/or build data storage module <b>328</b>;</li>        <li id="ul0002-0008" num="0412">discovered design and/or build data that is stored in a discovered design and/or build data storage module <b>329</b>;</li>        <li id="ul0002-0009" num="0413">desired design and/or build data that is stored in a desired design and/or build data storage module <b>330</b>;</li>        <li id="ul0002-0010" num="0414">disclosed system operation data that is stored in a disclosed system operation data storage module <b>331</b>;</li>        <li id="ul0002-0011" num="0415">discovered system operation data that is stored in a discovered system operation data storage module <b>332</b>;</li>        <li id="ul0002-0012" num="0416">desired system operation data that is stored in a desired system operation data storage module <b>333</b>; and/or</li>        <li id="ul0002-0013" num="0417">other data that is stored in another data storage module <b>334</b>.</li>    </ul>    </li></ul></p><p id="p-0406" num="0418">As another example of parsing, the incoming data is parsed based on a combination of one or more system aspects (e.g., system elements, system criteria, and system mode) or sub-system aspects thereof, one or more evaluation aspects (e.g., evaluation perspective, evaluation viewpoint, and evaluation category) or sub-evaluation aspects thereof, and/or one or more evaluation rating metrics (e.g., process, policy, procedure, certification, documentation, and automation) or sub-evaluation rating metrics thereof. As a specific example, the incoming data is parsed based on the evaluation rating metrics, creating processed parsed data, policy parsed data, procedure parsed data, certification parsed data, documentation parsed data, and automation parsed data. As another specific example, the incoming data is parsed based on the evaluation category of identify and its sub-categories of asset management, business environment, governance, risk assessment, risk management, access control, awareness &#x26;, training, and/or data security.</p><p id="p-0407" num="0419">As another example of parsing, the incoming data is not parsed, or is minimally parsed. As a specific example, the data is parsed based on timestamps: data from one time period (e.g., a day) is parsed from data of another time period (e.g., a different day).</p><p id="p-0408" num="0420">The source data matrix <b>335</b>, which may be a configured processing module, retrieves source data <b>337</b> from the data storage modules <b>322</b>-<b>334</b>. The selection corresponds to the analysis being performed by the analyze &#x26; score module <b>336</b>. For example, if the analyze &#x26; score module <b>336</b> is evaluating the understanding of the policies, processes, documentation, and automation regarding the assets built for the engineering department, then the source data <b>337</b> would be data specific to policies, processes, documentation, and automation regarding the assets built for the engineering department.</p><p id="p-0409" num="0421">The analyze &#x26; score module <b>336</b> generates one or more ratings <b>219</b> for the source data <b>337</b> in accordance with the data analysis parameters <b>265</b> and analysis modeling <b>268</b>. The data analysis parameters <b>265</b> includes system aspect analysis parameters <b>342</b>, evaluation aspect analysis parameters <b>343</b>, and evaluation metric analysis parameters <b>344</b>. The analyze &#x26; score module <b>336</b> is discussed in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>42</b></figref>.</p><p id="p-0410" num="0422"><figref idref="DRAWINGS">FIG. <b>42</b></figref> is a schematic block diagram of an embodiment of an analyze and score module <b>336</b> includes a matrix module <b>341</b> and a scoring module <b>348</b>. The matrix module <b>341</b> processes an evaluation mode matrix, an evaluation perspective matrix, an evaluation viewpoint matrix, and an evaluation categories matrix to produce a scoring input. The scoring module <b>348</b> includes an evaluation metric matrix to process the scoring input data in accordance with the analysis modeling <b>268</b> to produce the rating(s) <b>219</b>.</p><p id="p-0411" num="0423">For example, the matrix module <b>341</b> configures the matrixes based on the system aspect analysis parameters <b>342</b> and the evaluation aspect analysis parameters <b>343</b> to process the source data <b>337</b> to produce the scoring input data. As a specific example, the system aspect analysis parameters <b>342</b> and the evaluation aspect analysis parameters <b>343</b> indicate assets as the evaluation mode, understanding as the evaluation perspective, discovered as the evaluation viewpoint, and the identify as the evaluation category.</p><p id="p-0412" num="0424">Accordingly, the matrix module <b>341</b> communicates with the source data matrix module <b>335</b> of the data module <b>321</b> to obtain source data <b>337</b> relevant to assets, understanding, discovered, and identify. The matrix module <b>341</b> may organize the source data <b>337</b> using an organization scheme (e.g., by asset type, by evaluation metric type, by evaluation sub-categories, etc.) or keep the source data <b>337</b> as a collection of data. The matrix module <b>341</b> provides the scoring input data <b>344</b> as a collection of data or as organized data to the scoring module <b>348</b>.</p><p id="p-0413" num="0425">Continuing with the example, the scoring module <b>248</b> receives the scoring input data <b>348</b> and evaluates in accordance with the evaluation metric analysis parameters <b>344</b> and the analysis modeling <b>268</b> to produce the rating(s) <b>219</b>. As a specific example, the evaluation metric analysis parameters <b>344</b> indicate analyzing the scoring input data with respect to processes. In this instance, the analysis modeling <b>268</b> provides a scoring mechanism for evaluating the scoring input data with respect to processes to the scoring module <b>248</b>. For instance, the analysis modeling <b>268</b> includes six levels regarding processes and a corresponding numerical rating: none (e.g., <b>0</b>), inconsistent (e.g., <b>10</b>), repeatable (e.g., <b>20</b>), standardized (e.g., <b>30</b>), measured (e.g., <b>40</b>), and optimized (e.g., <b>50</b>).</p><p id="p-0414" num="0426">In addition, the analysis modeling <b>268</b> includes analysis protocols for interpreting the scoring input data to determine its level and corresponding rating. For example, if there are no processes regarding identifying assess of the discovered data, then an understanding level of processes would be none (e.g., <b>0</b>), since there are no processes. As another example, if there are some processes regarding identifying assess of the discovered data, but there are gaps in the processes (e.g., identifies some assets, but not all, do not produce consistent results), then an understanding level of processes would be inconsistent (e.g., <b>10</b>). To determine if there are gaps in the processes, the score module <b>248</b> executes the processes of the discovered data to identify assets. The scoring module <b>248</b> also executes one or more asset discovery tools to identify assets and then compares the two results. If there are inconsistencies in the identified assets, then there are gaps in the processes.</p><p id="p-0415" num="0427">As a further example, the processes regarding identifying assess of the discovered data are repeatable (e.g., produces consistent results, but there are variations in the processes from process to process, and/or the processes are not all regulated) but not standardized (e.g., produces consistent results, but there are no appreciable variations in the processes from process to process, and/or the processes are regulated). If the processes are repeatable but not standardized, the scoring module establishes an understanding level of the processes as repeatable (e.g., <b>20</b>).</p><p id="p-0416" num="0428">If the processes are standardized, the scoring module then determines whether the processes are measured (e.g., precise, exact, and/or calculated to the task of identifying assets). If not, the scoring module establishes an understanding level of the processes as standardized (e.g., <b>30</b>).</p><p id="p-0417" num="0429">If the processes are measured, the scoring module then determines whether the processes are optimized (e.g., up-to-date and improvement assessed on a regular basis as part of system protocols). If not, the scoring module establishes an understanding level of the processes as measured (e.g., <b>40</b>). If so, the scoring module establishes an understanding level of the processes as optimized (e.g., <b>50</b>).</p><p id="p-0418" num="0430"><figref idref="DRAWINGS">FIG. <b>43</b></figref> is a diagram of an example of system aspect, evaluation aspect, evaluation rating metric, and analysis system output options of an analysis system <b>10</b> for analyzing a system <b>11</b>, or portion thereof. The system aspect corresponds to what part of the system is to be evaluated by the analysis system. The evaluation aspect indicates how the system aspect is to be evaluation. The evaluation rating metric indicates the manner of evaluation of the system aspect in accordance with the evaluation aspect. The analysis system output indicates the type of output to be produced by the analysis system based on the evaluation of the system aspect in accordance with the evaluation aspect as per the evaluation rating metric.</p><p id="p-0419" num="0431">The system aspect includes system elements, system criteria, and system modes. A system element includes one or more system assets which is a physical asset and/or a conceptual asset. For example, a physical asset is a computing entity, a computing device, a user software application, a system software application (e.g., operating system, etc.), a software tool, a network software application, a security software application, a system monitoring software application, and the like. As another example, a conception asset is a hardware architecture (e.g., identification of a system's physical components, their capabilities, and their relationship to each other) and/or sub-architectures thereof and a software architecture (e.g., fundamental structures for the system's software, their requirements, and inter-relational operations) and sub-architectures thereof.</p><p id="p-0420" num="0432">A system element and/or system asset is identifiable in a variety of ways. For example, it can be identified by an organization identifier (ID), which would be associated with most, if not all, system elements of a system. As another example, a system element and/or system asset can be identified by a division ID, where the division is one of a plurality of divisions in the organization. As another example, a system element and/or system asset can be identified by a department ID, where the department is one of a plurality of departments in a division. As yet another example, a system element and/or system asset can be identified by a department ID, where the department is one of a plurality of departments in a division. As a further example, a system element and/or system asset can be identified by a group ID, where the department is one of a plurality of groups in a department. As a still further example, a system element and/or system asset can be identified by a sub-group ID, where the department is one of a plurality of sub-groups in a group. With this type of identifier, a collection of system elements and/or system assets can be selected for evaluation by using an organization ID, a division ID, a department ID, a group ID, or a sub-group ID.</p><p id="p-0421" num="0433">A system element and/or system asset may also be identified based on a user ID, a serial number, vendor data, an IP address, etc. For example, a computing device has a serial number and vendor data. As such, the computing device can be identified for evaluation by its serial number and/or the vendor data. As another example, a software application has a serial number and vendor data. As such, the software application can be identified for evaluation by its serial number and/or the vendor data.</p><p id="p-0422" num="0434">In addition, an identifier of one system element and/or system asset may link to one or more other system elements and/or system assets. For example, computing device has a device ID, a user ID, and/or a serial number to identify it. The computing device also includes a plurality of software applications, each with its own serial number. In this example, the software identifiers are linked to the computing device identifier since the software is loaded on the computing device. This type of an identifier allows a single system asset to be identified for evaluation.</p><p id="p-0423" num="0435">The system criteria includes information regarding the development, operation, and/or maintenance of the system <b>11</b>. For example, a system criteria is a guideline, a system requirement, a system design component, a system build component, the system, and system operation. Guidelines, system requirements, system design, system build, and system operation were discussed with reference to <figref idref="DRAWINGS">FIG. <b>25</b></figref>.</p><p id="p-0424" num="0436">The system mode indicates the assets of the system, the system functions of the system, and/or the security functions of the system are to be evaluated. Assets, system functions, and security functions have been previously discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>7</b>-<b>24</b> and <b>32</b>-<b>34</b></figref>.</p><p id="p-0425" num="0437">The evaluation aspect, which indicates how the system aspect is to be evaluated, includes evaluation perspective, evaluation viewpoint, and evaluation category. The evaluation perspective includes understanding (e.g., how well the system is known, should be known, etc.); implementation, which includes design and/or build, (e.g., how well is the system designed, how well should it be designed); system performance, and/or system operation (e.g., how well does the system perform and/or operate, how well should it perform and/or operate); and self-analysis (e.g., how self-aware is the system, how self-healing is the system, how self-updating is the system).</p><p id="p-0426" num="0438">The evaluation viewpoint includes disclosed data, discovered data, and desired data. Disclosed data is the known data of the system at the outset of an analysis, which is typically supplied by a system administrator and/or is obtained from data files of the system. Discovered data is the data discovered about the system from the by the analysis system during the analysis. Desired data is the data obtained by the analysis system from system proficiency resources regarding desired guidelines, system requirements, system design, system build, and/or system operation. Differences in disclosed, discovered, and desired data are evaluated to support generating an evaluation rating, to identify deficiencies, and/or to determine and provide auto-corrections.</p><p id="p-0427" num="0439">The evaluation category includes an identify category, a protect category, a detect category, a respond category, and a recover category. In general, the identify category is regarding identifying assets, system functions, and/or security functions of the system; the protect category is regarding protecting assets, system functions, and/or security functions of the system from issues that may adversely affect; the detect category is regarding detecting issues that may, or have, adversely affect assets, system functions, and/or security functions of the system; the respond category is regarding responding to issues that may, or have, adversely affect assets, system functions, and/or security functions of the system; and the recover category is regarding recovering from issues that have adversely affect assets, system functions, and/or security functions of the system. Each category includes one or more sub-categories and each sub-category may include one or more sub-sub categories as discussed with reference to <figref idref="DRAWINGS">FIGS. <b>44</b>-<b>49</b></figref>.</p><p id="p-0428" num="0440">The evaluation rating metric includes process, policy, procedure, certification, documentation, and automation. The evaluation rating metric may include more or less topics. The analysis system output options include evaluation rating, deficiency identification, and deficiency auto-correction.</p><p id="p-0429" num="0441">With such a significant number of options with the system aspect, the evaluation aspect, the evaluation rating metrics, and analysis system output options, the analysis system can analyze a system in thousands, or more, combinations. For example, the analysis system <b>10</b> could provide an evaluation rating for the entire system with respect to its vulnerability to cyber-attacks. The analysis system <b>10</b> could also identify deficiencies in the system's cybersecurity processes, policies, documentation, implementation, operation, assets, and/or security functions based on the evaluation rating. The analysis system <b>10</b> could further auto-correct at least some of the deficiencies in the system's cybersecurity processes, policies, documentation, implementation, operation, assets, and/or security functions.</p><p id="p-0430" num="0442">As another example, the analysis system <b>10</b> could evaluates the system's requirements for proper use of software (e.g., authorized to use, valid copy, current version) by analyzing every computing device in the system as to the system's software use requirements. From this analysis, the analysis system generates an evaluation rating. The analysis system <b>10</b> could also identify deficiencies in the compliance with the system's software use requirements (e.g., unauthorized use, invalid copy, outdated copy). The analysis system <b>10</b> could further auto-correct at least some of the deficiencies in compliance with the system's software use requirements (e.g., remove invalid copies, update outdated copies).</p><p id="p-0431" num="0443"><figref idref="DRAWINGS">FIG. <b>44</b></figref> is a diagram of another example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b>. This diagram is similar to <figref idref="DRAWINGS">FIG. <b>43</b></figref> with the exception that this figure illustrates sub-categories and sub-sub categories. Each evaluation category includes sub-categories, which, in turn, include their own sub-sub categories. The various categories, sub-categories, and sub-sub categories corresponds to the categories, sub-categories, and sub-sub categories identified in the &#x201c;Framework for Improving Critical Instructure Cybersecurity&#x201d;, Version 1.1, Apr. 16, 2018 by the National Institute of Standards and Technology (NIST).</p><p id="p-0432" num="0444"><figref idref="DRAWINGS">FIG. <b>45</b></figref> is a diagram of an example of an identification evaluation category that includes a plurality of sub-categories and each sub-category includes its own plurality of sub-sub-categories. The identify category includes the sub-categories of asset management, business environment, governance, risk management, access control, awareness &#x26; training, and data security.</p><p id="p-0433" num="0445">The asset management sub-category includes the sub-sub categories of HW inventoried, SW inventoried, data flow mapped out, external systems cataloged, resources have been prioritized, and security roles have been established. The business environment sub-category includes the sub-sub categories of supply chain roles defined, industry critical infrastructure identified, business priorities established, critical services identified, and resiliency requirements identified.</p><p id="p-0434" num="0446">The governance sub-category includes the sub-sub categories of security policies are established, security factors aligned, and legal requirements are identified. The risk assessment sub-category includes the sub-sub categories of vulnerabilities identified, external sources are leveraged, threats are identified, business impacts are identified, risk levels are identified, and risk responses are identified. The risk management sub-category includes the sub-sub categories of risk management processes are established, risk tolerances are established, and risk tolerances are tied to business environment.</p><p id="p-0435" num="0447">The access control sub-category includes the sub-sub categories of remote access control is defined, permissions are defined, and network integrity is defined. The awareness &#x26; training sub-category includes the sub-sub categories of users are trained, user privileges are known, third party responsibilities are known, executive responsibilities are known, and IT and security responsibilities are known. The data security sub-category includes the sub-sub categories of data at rest protocols are established, data in transit protocols are established, formal asset management protocols are established, adequate capacity of the system is established, data leak prevention protocols are established, integrity checking protocols are established, and use and development separation protocols are established.</p><p id="p-0436" num="0448"><figref idref="DRAWINGS">FIG. <b>46</b></figref> is a diagram of an example of a protect evaluation category that includes a plurality of sub-categories and each sub-category includes its own plurality of sub-sub-categories. The protect category includes the sub-categories of information protection processes and procedures, maintenance, and protective technology.</p><p id="p-0437" num="0449">The information protection processes and procedures sub-category includes the sub-sub categories of baseline configuration of IT/industrial controls are established, system life cycle management is established, configuration control processes are established, backups of information are implemented, policy &#x26; regulations for physical operation environment are established, improving protection processes are established, communication regarding effective protection technologies is embraced, response and recovery plans are established, cybersecurity in is including in human resources, and vulnerability management plans are established.</p><p id="p-0438" num="0450">The maintenance sub-category includes the sub-sub categories of system maintenance &#x26; repair of organizational assets programs are established and remote maintenance of organizational assets is established. The protective technology sub-category includes the sub-sub-categories of audit and recording policies are practiced, removable media is protected &#x26; use policies are established, access to systems and assets is controlled, and communications and control networks are protected.</p><p id="p-0439" num="0451"><figref idref="DRAWINGS">FIG. <b>47</b></figref> is a diagram of an example of a detect evaluation category that includes a plurality of sub-categories and each sub-category includes its own plurality of sub-sub-categories. The detect category includes the sub-categories of anomalies and events, security continuous monitoring, and detection processes.</p><p id="p-0440" num="0452">The anomalies and events sub-category includes the sub-sub categories of baseline of network operations and expected data flows are monitored, detected events are analyzed, event data are aggregated and correlated, impact of events is determined, and incident alert thresholds are established. The security continuous monitoring sub-category includes the sub-sub categories of network is monitored to detect potential cybersecurity attacks, physical environment is monitored for cybersecurity events, personnel activity is monitored for cybersecurity events, malicious code is detected, unauthorized mobile codes is detected, external service provider activity is monitored for cybersecurity events, monitoring for unauthorized personnel, connections, devices, and software is performed, and vulnerability scans are performed. The detection processes sub-category includes the sub-sub categories of roles and responsibilities for detection are defined, detection activities comply with applicable requirements, detection processes are tested, event detection information is communicated, and detection processes are routinely improved.</p><p id="p-0441" num="0453"><figref idref="DRAWINGS">FIG. <b>48</b></figref> is a diagram of an example of a respond evaluation category that includes a plurality of sub-categories and each sub-category includes its own plurality of sub-sub-categories. The respond category includes the sub-categories of response planning, communications, analysis, mitigation, and improvements.</p><p id="p-0442" num="0454">The response planning sub-category includes the sub-sub category of response plan is executed during and/or after an event. The communications sub-category includes the sub-sub category of personnel roles and order of operation are established, events are reported consistent with established criteria, information is shared consistently per the response plan, coordination with stakeholders is consistent with the response plan, and voluntary information is shared with external stakeholders.</p><p id="p-0443" num="0455">The analysis sub-category includes the sub-sub categories of notifications form detection systems are investigated, impact of the incident is understood, forensics are performed, and incidents are categorized per response plan. The mitigation sub-category includes the sub-sub categories of incidents are contained, incidents are mitigated, and newly identified vulnerabilities are processed. The improvements sub-categories includes the sub-sub categories of response plans incorporate lessons learned, and response strategies are updated.</p><p id="p-0444" num="0456"><figref idref="DRAWINGS">FIG. <b>49</b></figref> is a diagram of an example of a recover evaluation category that includes a plurality of sub-categories and each sub-category includes its own plurality of sub-sub-categories. The recover category includes the sub-categories of recovery plan, improvements, and communication. The recovery plan sub-category includes the sub-sub category of recovery plan is executed during and/or after an event.</p><p id="p-0445" num="0457">The improvement sub-category includes the sub-sub categories of recovery plans incorporate lessons learned and recovery strategies are updated. The communications sub-category includes the sub-sub categories of public relations are managed, reputations after an event is repaired, and recovery activities are communicated.</p><p id="p-0446" num="0458"><figref idref="DRAWINGS">FIG. <b>50</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the understanding of the guidelines for identifying assets, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets of a department based on disclosed data.</p><p id="p-0447" num="0459">For this specific example, the analysis system <b>10</b> obtains disclosed data from the system regarding the guidelines associated with the assets of the department. From the disclosed data, the analysis system renders an evaluation rating for the understanding of the guidelines for identifying assets. The analysis system renders a second evaluation rating for the understanding of the guidelines regarding protection of the assets from issues. The analysis system renders a third evaluation rating for the understanding of the guidelines regarding detection of issues that may affect or are affecting the assets.</p><p id="p-0448" num="0460">The analysis system renders a fourth evaluation rating for the understanding of the guidelines regarding responds to issues that may affect or are affecting the assets. The analysis system renders a fifth evaluation rating for the understanding of the guidelines regarding recovery from issues that affected the assets of a department based on disclosed data. The analysis system may render an overall evaluation rating for the understanding of the guidelines based on the first through fifth evaluation ratings.</p><p id="p-0449" num="0461">As another example, the analysis system <b>11</b> evaluates the understanding of guidelines used to determine what assets should be included in the department, how the assets should be protected from issues, how issues that may affect or are affecting the assets are detect, how to respond to issues that may affect or are affecting the assets, and how the assets will recover from issues that may affect or are affecting them based on disclosed data. In this example, the analysis system renders an evaluation rating for the understanding of the guidelines regarding what assets should be in the department. The analysis system renders a second evaluation rating for the understanding of the guidelines regarding how the assets should be protected from issues. The analysis system renders a third evaluation rating for the understanding of the guidelines regarding how to detect issues that may affect or are affecting the assets.</p><p id="p-0450" num="0462">The analysis system renders a fourth evaluation rating for the understanding of the guidelines regarding how to respond to issues that may affect or are affecting the assets. The analysis system renders a fifth evaluation rating for the understanding of the guidelines regarding how to recover from issues that affected the assets of a department based on disclosed data. The analysis system may render an overall evaluation rating for the understanding based on the first through fifth evaluation ratings.</p><p id="p-0451" num="0463"><figref idref="DRAWINGS">FIG. <b>51</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the understanding of the system design for identifying assets, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets of a department based on disclosed data.</p><p id="p-0452" num="0464">For this specific example, the analysis system <b>10</b> obtains disclosed data from the system regarding the system design associated with the assets of the department. From the disclosed data, the analysis system renders an evaluation rating for the understanding of the system design for identifying assets. The analysis system renders a second evaluation rating for the understanding of the system design regarding protection of the assets from issues. The analysis system renders a third evaluation rating for the understanding of the system design regarding detection of issues that may affect or are affecting the assets.</p><p id="p-0453" num="0465">The analysis system renders a fourth evaluation rating for the understanding of the system design regarding responds to issues that may affect or are affecting the assets. The analysis system renders a fifth evaluation rating for the understanding of the system design regarding recovery from issues that affected the assets of a department based on disclosed data. The analysis system may render an overall evaluation rating for the understanding based on the first through fifth evaluation ratings.</p><p id="p-0454" num="0466">As another example, the analysis system <b>11</b> evaluates the understanding of system design used to determine what assets should be included in the department, how the assets should be protected from issues, how issues that may affect or are affecting the assets are detect, how to respond to issues that may affect or are affecting the assets, and how the assets will recover from issues that may affect or are affecting them based on disclosed data. In this example, the analysis system renders an evaluation rating for the understanding of the system design regarding what assets should be in the department. The analysis system renders a second evaluation rating for the understanding of the system design regarding how the assets should be protected from issues. The analysis system renders a third evaluation rating for the understanding of the system design regarding how to detect issues that may affect or are affecting the assets.</p><p id="p-0455" num="0467">The analysis system renders a fourth evaluation rating for the understanding of the system design regarding how to respond to issues that may affect or are affecting the assets. The analysis system renders a fifth evaluation rating for the understanding of the system design regarding how to recover from issues that affected the assets of a department based on disclosed data. The analysis system may render an overall evaluation rating for the understanding based on the first through fifth evaluation ratings.</p><p id="p-0456" num="0468"><figref idref="DRAWINGS">FIG. <b>52</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the understanding of the guidelines, system requirements, and system design for identifying assets, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets of a department based on disclosed data and discovered data.</p><p id="p-0457" num="0469">For this specific example, the analysis system <b>10</b> obtains disclosed data and discovered from the system regarding guidelines, system requirements, and system design associated with the assets of the department. From the disclosed data and discovered data, the analysis system renders one or more first evaluation ratings (e.g., one for each of guidelines, system requirements, and system design, or one for all three) for the understanding of the guidelines, system requirements, and system design for identifying assets. The analysis system renders one or more second evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding protection of the assets from issues. The analysis system renders one or more third evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding detection of issues that may affect or are affecting the assets.</p><p id="p-0458" num="0470">The analysis system renders one or more fourth evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding responds to issues that may affect or are affecting the assets. The analysis system renders one or more fifth evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding recovery from issues that affected the assets of a department based on disclosed data. The analysis system may render an overall evaluation rating for the understanding based on the one or more first through one or more fifth evaluation ratings.</p><p id="p-0459" num="0471">The analysis system <b>11</b> may further render an understanding evaluation rating regarding how well the discovered data correlates with the disclosed data. In other words, evaluate the knowledge level of the system. In this example, the analysis system compares the disclosed data with the discovered data. If they substantially match, the understanding of the system would receive a relatively high evaluation rating. The more the disclosed data differs from the discovered data, the lower the understanding evaluation rating will be.</p><p id="p-0460" num="0472">As another example, the analysis system <b>11</b> evaluates the understanding of guidelines, system requirements, and system design used to determine what assets should be included in the department, how the assets should be protected from issues, how issues that may affect or are affecting the assets are detect, how to respond to issues that may affect or are affecting the assets, and how the assets will recover from issues that may affect or are affecting them based on disclosed data and discovered data. In this example, the analysis system renders one or more first evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding what assets should be in the department. The analysis system renders one or more second evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding how the assets should be protected from issues. The analysis system renders one or more third evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding how to detect issues that may affect or are affecting the assets.</p><p id="p-0461" num="0473">The analysis system renders one or more fourth evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding how to respond to issues that may affect or are affecting the assets. The analysis system renders one or more fifth evaluation ratings for the understanding of the guidelines, system requirements, and system design regarding how to recover from issues that affected the assets of a department based on disclosed data. The analysis system may render an overall evaluation rating for the understanding of the guidelines, system requirements, and system design based on the one or more first through the one or more fifth evaluation ratings.</p><p id="p-0462" num="0474"><figref idref="DRAWINGS">FIG. <b>53</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the implementation for and operation of identifying assets of a department, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets per the guidelines, system requirements, system design, system build, and resulting system based on disclosed data and discovered data.</p><p id="p-0463" num="0475">For this specific example, the analysis system <b>10</b> obtains disclosed data and discovered data from the system regarding the guidelines, system requirements, system design, system build, and resulting system associated with the assets of the department. From the disclosed data and discovered data, the analysis system renders one or more first evaluation ratings (e.g., one for each of guidelines, system requirements, system design, system build, resulting system with respect to each of implementation and operation or one for all of them) for the implementation and operation of identifying the assets per the guidelines, system requirements, system design, system build, and resulting system. The analysis system renders one or more second evaluation ratings for the implementation and operation of protecting the assets from issues per the guidelines, system requirements, system design, system build, and resulting system.</p><p id="p-0464" num="0476">The analysis system renders one or more third evaluation ratings for the implementation and operation of detecting issues that may affect or are affecting the assets per the guidelines, system requirements, system design, system build, and resulting system. The analysis system renders one or more fourth evaluation ratings for the implementation and operation of responding to issues that may affect or are affecting the assets per the guidelines, system requirements, system design, system build, and resulting system.</p><p id="p-0465" num="0477">The analysis system renders one or more fifth evaluation ratings for the implementation and operation of recovering from issues that may affect or are affecting the assets per the guidelines, system requirements, system design, system build, and resulting system. The analysis system may render an overall evaluation rating for the implementation and/or performance based on the one or more first through one or more fifth evaluation ratings.</p><p id="p-0466" num="0478"><figref idref="DRAWINGS">FIG. <b>54</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the implementation for and operation of identifying assets of a department, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets per the guidelines, system requirements, system design, system build, and resulting system based on discovered data and desired data.</p><p id="p-0467" num="0479">For this specific example, the analysis system <b>10</b> obtains disclosed data and discovered from the system regarding the guidelines, system requirements, system design, system build, and resulting system associated with the assets of the department. From the discovered data and desired data, the analysis system renders one or more first evaluation ratings (e.g., one for each of guidelines, system requirements, system design, system build, resulting system with respect to each of implementation and operation or one for all of them) for the implementation and operation of identifying the assets per the guidelines, system requirements, system design, system build, and resulting system. The analysis system renders one or more second evaluation ratings for the implementation and operation of protecting the assets from issues per the guidelines, system requirements, system design, system build, and resulting system.</p><p id="p-0468" num="0480">The analysis system renders one or more third evaluation ratings for the implementation and operation of detecting issues that may affect or are affecting the assets per the guidelines, system requirements, system design, system build, and resulting system. The analysis system renders one or more fourth evaluation ratings for the implementation and operation of responding to issues that may affect or are affecting the assets per the guidelines, system requirements, system design, system build, and resulting system.</p><p id="p-0469" num="0481">The analysis system renders one or more fifth evaluation ratings for the implementation and operation of recovering from issues that may affect or are affecting the assets per the guidelines, system requirements, system design, system build, and resulting system. The analysis system may render an overall evaluation rating for the implementation and/or performance based on the one or more first through one or more fifth evaluation ratings.</p><p id="p-0470" num="0482">The analysis system <b>11</b> may further render an implementation and/or operation evaluation rating regarding how well the discovered data correlates with the desired data. In other words, evaluate the level implementation and operation of the system. In this example, the analysis system compares the disclosed data with the desired data. If they substantially match, the implementation and/or operation of the system would receive a relatively high evaluation rating. The more the discovered data differs from the desired data, the lower the implementation and/or operation evaluation rating will be.</p><p id="p-0471" num="0483"><figref idref="DRAWINGS">FIG. <b>55</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the system's self-evaluation for identifying assets, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets of a department based on disclosed data and discovered data per the guidelines, system requirements, and system design.</p><p id="p-0472" num="0484">For this specific example, the analysis system <b>10</b> obtains disclosed data and discovered from the system regarding the guidelines, system requirements, and system design associated with the assets of the department. From the disclosed data and discovered, the analysis system renders one or more first evaluation ratings (e.g., one for each of guidelines, system requirements, and system design, or one for all three) for the self-evaluation of identifying assets per the guidelines, system requirements, and system design. For instance, what resources does the system have with respect to its guidelines, system requirements, and/or system design for self-identifying of assets.</p><p id="p-0473" num="0485">The analysis system renders one or more second evaluation ratings for the self-evaluation of protecting the assets from issues per the guidelines, system requirements, and system design regarding. The analysis system renders one or more third evaluation ratings for the self-evaluation of detecting issues that may affect or are affecting the assets per the guidelines, system requirements, and system design regarding detection.</p><p id="p-0474" num="0486">The analysis system renders one or more fourth evaluation ratings for the self-evaluation of responding to issues that may affect or are affecting the assets per the guidelines, system requirements, and system design. The analysis system renders one or more fifth evaluation ratings for the self-evaluation of recovering from issues that affected the assets per the guidelines, system requirements, and system design. The analysis system may render an overall evaluation rating for the self-evaluation based on the one or more first through one or more fifth evaluation ratings.</p><p id="p-0475" num="0487"><figref idref="DRAWINGS">FIG. <b>56</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating the understanding of the guidelines, system requirements, system design, system build, and resulting system for identifying assets, protecting the assets from issues, detecting issues that may affect or are affecting the assets, responding to issues that may affect or are affecting the assets, and recovering from issues that affected the assets of a department based on disclosed data and discovered data.</p><p id="p-0476" num="0488">For this specific example, the analysis system <b>10</b> obtains disclosed data and discovered data from the system regarding guidelines, system requirements, system design, system build, and resulting system associated with the assets of the department. As a specific example, the disclosed data includes guidelines that certain types of data shall be encrypted; a system requirement that specifies 128-bit Advanced Encryption Standard (AES) for &#x201c;y&#x201d; types of documents; a system design that includes 12 &#x201c;x&#x201d; type computers that are to be loaded with 128-bit AES software by company &#x201c;M&#x201d;, version 2.0 or newer; and a system build and resulting system that includes 12 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d;, version 2.1.</p><p id="p-0477" num="0489">For this specific example, the discovered data includes the same guideline as the disclosed data; a first system requirement that specifies 128-bit Advanced Encryption Standard (AES) for &#x201c;y&#x201d; types of documents and a second system requirement that specifies 256-bit Advanced Encryption Standard (AES) for &#x201c;A&#x201d; types of documents; a system design that includes 12 &#x201c;x&#x201d; type computers that are to be loaded with 128-bit AES software by company &#x201c;M&#x201d;, version 2.0 or newer, and 3 &#x201c;z&#x201d; type computers that are to be loaded with 256-bit AES software by company &#x201c;N&#x201d; version 3.0 or newer; and a system build and resulting system that includes 10 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d; version 2.1, 2 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d; version 1.3, 2 &#x201c;z&#x201d; type computers that have 256-bit AES software by company &#x201c;N&#x201d; version 3.1, and 1 &#x201c;z&#x201d; type computer that has 256-bit AES software by company &#x201c;K&#x201d; version 0.1.</p><p id="p-0478" num="0490">From just the disclosed data, the analysis system would render a relatively high evaluation rating for the understanding of the guidelines, system requirements, system design, system build, and resulting system associated with the assets of the department. The relatively high evaluation rating would be warranted since the system build and resulting system included what was in the system design (e.g., 12 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d;, version 2.1). Further, the system design is consistent with the system reequipments (e.g., 128-bit Advanced Encryption Standard (AES) for &#x201c;y&#x201d; types of documents), which is consistent with the guidelines (e.g., certain types of data shall be encrypted).</p><p id="p-0479" num="0491">From the discovered data, however, the analysis system would render a relatively low evaluation rating for the understanding of the guidelines, system requirements, system design, system build, and resulting system associated with the assets of the department. The relatively low evaluation rating would be warranted since the system build and resulting system is not consistent with the system design (e.g., is missing 2 &#x201c;x&#x201d; type computers with the right encryption software, only has 2 &#x201c;z&#x201d; type computers with the right software, and has a &#x201c;z&#x201d; type computer with the wrong software).</p><p id="p-0480" num="0492">The analysis system would also process the evaluation ratings from the disclosed data and from the discovered data to produce an overall evaluation rating for the understanding of the guidelines, system requirements, system design, system build, and resulting system associated with the assets of the department. In this instance, the disclosed data does not substantially match the discovered data, which indicates a lack of understanding of what's really in the system (i.e., knowledge of the system). Further, since the evaluation rating from the discovered data was low, the analysis system would produce a low overall evaluation rating for the understanding.</p><p id="p-0481" num="0493"><figref idref="DRAWINGS">FIG. <b>57</b></figref> is a diagram of an extension of the example of <figref idref="DRAWINGS">FIG. <b>56</b></figref>. In this example, the analysis system processes the data and/or evaluation ratings to identify deficiencies and/or auto-corrections of at least some of the deficiencies. As shown, the disclosed data includes:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0494">guidelines that certain types of data shall be encrypted;</li>        <li id="ul0004-0002" num="0495">a system requirement that specifies 128-bit Advanced Encryption Standard (AES) for &#x201c;y&#x201d; types of documents;</li>        <li id="ul0004-0003" num="0496">a system design that includes 12 &#x201c;x&#x201d; type computers that are to be loaded with 128-bit AES software by company &#x201c;M&#x201d;, version 2.0 or newer; and</li>        <li id="ul0004-0004" num="0497">a system build and resulting system that includes 12 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d;, version 2.1.</li>    </ul>    </li></ul></p><p id="p-0482" num="0498">As is also shown, the discovered data includes:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0499">the same guideline as the disclosed data;</li>        <li id="ul0006-0002" num="0500">a first system requirement that specifies 128-bit Advanced Encryption Standard (AES) for &#x201c;y&#x201d; types of documents and a second system requirement that specifies 256-bit Advanced Encryption Standard (AES) for &#x201c;A&#x201d; types of documents;</li>        <li id="ul0006-0003" num="0501">a system design that includes 12 &#x201c;x&#x201d; type computers that are to be loaded with 128-bit AES software by company &#x201c;M&#x201d;, version 2.0 or newer, and 3 &#x201c;z&#x201d; type computers that are to be loaded with 256-bit AES software by company &#x201c;N&#x201d;, version 3.0 or newer; and</li>        <li id="ul0006-0004" num="0502">a system build and resulting system that includes:</li>        <li id="ul0006-0005" num="0503">10 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d;, version 2.1;</li>        <li id="ul0006-0006" num="0504">2 &#x201c;x&#x201d; type computers that have 128-bit AES software by company &#x201c;M&#x201d;, version 1.3;</li>        <li id="ul0006-0007" num="0505">2 &#x201c;z&#x201d; type computers that have 256-bit AES software by company &#x201c;N&#x201d;, version 3.1; and</li>        <li id="ul0006-0008" num="0506">1 &#x201c;z&#x201d; type computer that has 256-bit AES software by company &#x201c;K&#x201d;, version 0.1.</li>    </ul>    </li></ul></p><p id="p-0483" num="0507">From this data, the analysis system identifies deficiencies <b>232</b> and, when possible, provides auto-corrections <b>235</b>. For example, the analysis system determines that the system requirements also included a requirement for 256-bit AES for &#x201c;A&#x201d; type documents. The analysis system can auto-correct this deficiency by updating the knowledge of the system to include the missing requirement. This may include updating one or more policies, one or more processes, one or more procedures, and/or updating documentation.</p><p id="p-0484" num="0508">As another example, the analysis system identifies the deficiency of the design further included 3 &#x201c;z&#x201d; type computers that are to be loaded with 256-bit AES software by company &#x201c;N&#x201d;, version 3.0 or newer. The analysis system can auto-correct this deficiency by updating the knowledge of the system to include the <b>3</b> &#x201c;z&#x201d; type computers with the correct software. Again, this may include updating one or more policies, one or more processes, one or more procedures, and/or updating documentation.</p><p id="p-0485" num="0509">As another example, the analysis system identifies the deficiency of 2 &#x201c;x&#x201d; type computers having old versions of the encryption software (e.g., have version 1.3 of company M's 128-bit AES software instead of a version 2.0 or newer). The analysis system can auto-correct this deficiency by updating the version of software for the two computers.</p><p id="p-0486" num="0510">As another example, the analysis system identifies the deficiency of 1 &#x201c;z&#x201d; type computer has the wrong encryption software (e.g., it has version 0.1 from company K and not version 3.0 or newer from company N). The analysis system can auto-correct this deficiency by replacing the wrong encryption software with the correct encryption software.</p><p id="p-0487" num="0511">As another example, the analysis system identifies the deficiency of 1 &#x201c;z&#x201d; type computer is missing from the system. The analysis system cannot auto-correct this deficiency since it is missing hardware. In this instance, the analysis system notifies a system admin of the missing computer.</p><p id="p-0488" num="0512"><figref idref="DRAWINGS">FIG. <b>58</b></figref> is a schematic block diagram of an embodiment of an evaluation processing module <b>254</b> that includes a plurality of comparators <b>360</b>-<b>362</b>, a plurality of analyzers <b>363</b>-<b>365</b>, and a deficiency correction module <b>366</b>. In general, the evaluation processing module <b>254</b> identifies deficiencies <b>232</b> and, when possible, determines auto-corrections <b>235</b> from the ratings <b>219</b> and/or inputted data (e.g., disclosed data, discovered data, and/or desired data) based on evaluation parameters <b>266</b> (e.g., disclosed to discovered deficiency criteria <b>368</b>, discovered to desired deficiency criteria <b>370</b>, disclosed to desired deficiency criteria <b>372</b>, disclosed to discovered compare criteria <b>373</b>, discovered to desired compare criteria <b>374</b>, and disclosed to desired compare criteria <b>375</b>).</p><p id="p-0489" num="0513">In an example, comparator <b>360</b> compares disclosed data and/or ratings <b>338</b> and discovered data and/or ratings <b>339</b> based on the disclosed to discovered compare criteria <b>373</b> to produce, if any, one or more disclosed to discovered differences <b>367</b>. As a more specific example, the analysis system evaluates disclosed, discovered, and/or desired data to produce one or more evaluation ratings regarding the understanding of the guidelines, system requirements, system design, system build, and resulting system associated with identifying the assets of the department.</p><p id="p-0490" num="0514">Each of the disclosed data, discovered data, and desired data includes data regarding the guidelines, system requirements, system design, system build, and/or resulting system associated with identifying the assets of the department and/or the assets of the department. Recall that disclosed data is the known data of the system at the outset of an analysis, which is typically supplied by a system administrator and/or is obtained from data files of the system. The discovered data is the data discovered about the system by the analysis system during the analysis. The desired data is the data obtained by the analysis system from system proficiency resources regarding desired guidelines, system requirements, system design, system build, and/or system operation.</p><p id="p-0491" num="0515">For the understanding of the guidelines, system requirements, system design, system build, and resulting system associated with identifying the assets of the department, the analysis system may produce one or more evaluation ratings. For example, the analysis system produces an evaluation rating for:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0516">understanding of the guidelines with respect to identifying assets of the department from the disclosed data;</li>        <li id="ul0008-0002" num="0517">understanding of the guidelines with respect to identifying assets of the department from the discovered data;</li>        <li id="ul0008-0003" num="0518">understanding of the guidelines with respect to identifying assets of the department from the desired data;</li>        <li id="ul0008-0004" num="0519">understanding of the system requirements with respect to identifying assets of the department from the disclosed data;</li>        <li id="ul0008-0005" num="0520">understanding of the system requirements with respect to identifying assets of the department from the discovered data;</li>        <li id="ul0008-0006" num="0521">understanding of the system requirements with respect to identifying assets of the department from the desired data;</li>        <li id="ul0008-0007" num="0522">understanding of the system design with respect to identifying assets of the department from the disclosed data;</li>        <li id="ul0008-0008" num="0523">understanding of the system design with respect to identifying assets of the department from the discovered data;</li>        <li id="ul0008-0009" num="0524">understanding of the system design with respect to identifying assets of the department from the desired data;</li>        <li id="ul0008-0010" num="0525">understanding of the system build with respect to identifying assets of the department from the disclosed data;</li>        <li id="ul0008-0011" num="0526">understanding of the system build with respect to identifying assets of the department from the discovered data;</li>        <li id="ul0008-0012" num="0527">understanding of the system build with respect to identifying assets of the department from the desired data;</li>        <li id="ul0008-0013" num="0528">understanding of the resulting system with respect to identifying assets of the department from the disclosed data;</li>        <li id="ul0008-0014" num="0529">understanding of the resulting system with respect to identifying assets of the department from the discovered data;</li>        <li id="ul0008-0015" num="0530">understanding of the resulting system with respect to identifying assets of the department from the desired data; and/or</li>        <li id="ul0008-0016" num="0531">an overall understanding of identifying the assets of the department.</li>    </ul>    </li></ul></p><p id="p-0492" num="0532">The disclosed to discovered compare criteria <b>373</b> specifies the evaluation ratings to be compared and/or which data of the disclosed data is to be compared to data of the discovered data. For example, the disclosed to discovered compare criteria <b>373</b> indicates that the &#x201c;understanding of the guidelines with respect to system design of the department from the disclosed data&#x201d; is to be compared to the &#x201c;understanding of the system design with respect to identifying assets of the department from the discovered data&#x201d;. As another example, the disclosed to discovered compare criteria <b>373</b> indicates that data regarding system design of the disclosed data is to be compared with the data regarding the system design of the discovered data.</p><p id="p-0493" num="0533">In accordance with the disclosed to discovered compare criteria <b>373</b> and for this specific example, the comparator <b>360</b> compares the &#x201c;understanding of the guidelines with respect to system design of the department from the disclosed data&#x201d; with the &#x201c;understanding of the system design with respect to identifying assets of the department from the discovered data&#x201d; to produce, if any, one or more understanding differences. The comparator <b>360</b> also compares the data regarding system design of the disclosed data with the data regarding the system design of the discovered data to produce, if any, one or more data differences. The comparator <b>360</b> outputs the one or more understanding differences and/or the one or more data differences as the disclosed to discovered differences <b>367</b>.</p><p id="p-0494" num="0534">The analyzer <b>363</b> analyzes the disclosed to discovered differences <b>267</b> in accordance with the disclosed to discovered deficiency criteria <b>368</b> to determine whether a difference <b>267</b> constitutes a deficiency. If so, the analyzer <b>363</b> includes it in the disclosed to discovered deficiencies <b>232</b>-<b>1</b>. The disclosed to discovered deficiency criteria <b>368</b> correspond to the disclosed to discovered compare criteria <b>373</b> and specify how the differences <b>367</b> are to be analyzed to determine if they constitute deficiencies <b>232</b>-<b>1</b>.</p><p id="p-0495" num="0535">As an example, the disclosed to discovered deficiency criteria <b>368</b> specify a series of comparative thresholds based on the impact the differences have on the system. The range of impact is from none to significant with as many granular levels in between as desired. For differences that have a significant impact on the system, the comparative threshold is set to trigger a deficiency for virtually any difference. For example, if the difference is regarding system security, then then threshold is set that any difference is a deficiency.</p><p id="p-0496" num="0536">As another example, if the difference is regarding is inconsequential information, then the threshold is set to not identify the difference as a deficiency. For example, the discovered data includes a PO date on Nov. 2, 2020 for a specific purchase order and the disclosed data didn't include a PO date, but the rest of the information regarding the PO is the same for the disclosed and discovered data. In this instance, the missing PO date is inconsequential and would not be identified as a deficiency.</p><p id="p-0497" num="0537">The deficiency correction module <b>366</b> receives the disclosed to discovered deficiencies <b>232</b>-<b>1</b>, if any, and determines whether one or more of the deficiencies <b>232</b>-<b>1</b> can be auto-corrected to produce an auto-correction <b>235</b>. In many instances, software deficiencies are auto-correctable (e.g., wrong software, missing software, out-of-date software, etc.) while hardware deficiencies are not auto-correctable (e.g., wrong computing device, missing computing device, missing network connection, etc.).</p><p id="p-0498" num="0538">The comparator <b>361</b> functions similarly to the comparator <b>360</b> to produce discovered to desired differences <b>369</b> based on the discovered data and/or rating <b>339</b> and the desired data and/or rating <b>340</b> in accordance with the discovered to desired compare criteria <b>374</b>. The analyzer <b>364</b> functions similarly to the analyzer <b>363</b> to produce discovered to desired deficiencies <b>232</b>-<b>2</b> from the discovered to desired differences <b>369</b> in accordance with the discovered to desired deficiency criteria <b>370</b>. The deficiency correction module <b>366</b> auto-corrects, when possible, the discovered to desired deficiencies <b>232</b>-<b>2</b> to produce auto-corrections <b>235</b>.</p><p id="p-0499" num="0539">The comparator <b>362</b> functions similarly to the comparator <b>360</b> to produce disclosed to desired differences <b>371</b> based on the disclosed data and/or rating <b>338</b> and the desired data and/or rating <b>340</b> in accordance with the disclosed to desired compare criteria <b>375</b>. The analyzer <b>365</b> functions similarly to the analyzer <b>363</b> to produce disclosed to desired deficiencies <b>232</b>-<b>3</b> from the disclosed to desired differences <b>371</b> in accordance with the disclosed to desired deficiency criteria <b>372</b>. The deficiency correction module <b>366</b> auto-corrects, when possible, the disclosed to desired deficiencies <b>232</b>-<b>3</b> to produce auto-corrections <b>235</b>.</p><p id="p-0500" num="0540">While the examples were for the understanding of the system with respect to identifying assets of the department, the evaluation processing module <b>254</b> processes any combination of system aspects, evaluation aspects, and evaluation metrics in a similar manner. For example, the evaluation processing module <b>254</b> processes the implementation of the system with respect to identifying assets of the department to identify deficiencies <b>232</b> and auto-corrections in the implementation. As another example, the evaluation processing module <b>254</b> processes the operation of the system with respect to identifying assets of the department to identify deficiencies <b>232</b> and auto-corrections in the operation of the system.</p><p id="p-0501" num="0541"><figref idref="DRAWINGS">FIG. <b>59</b></figref> is a state diagram of an example the analysis system analyzing a system. From a start state <b>380</b>, the analysis proceeds to an understanding of the system state <b>38</b>) or to a test operations of the assets system functions, and/or security functions of a system state <b>386</b> based on the desired analysis to be performed. For testing the understanding, the analysis proceeds to state <b>381</b> where the understanding of the assets, system functions, and/or security functions of the system are evaluated. This may be done via documentation of the system, policies of the supported business, based upon a question and answer session with personnel of the owner/operator of the system, and/or as discussed herein.</p><p id="p-0502" num="0542">If the understanding of the system is inadequate, the analysis proceeds to the determine deficiencies in the understanding of the system state <b>382</b>. In this state <b>382</b>, the deficiencies in understanding are determined by processing differences and/or as discussed herein.</p><p id="p-0503" num="0543">From state <b>382</b>, corrections required in understanding the system are identified and operation proceeds to state <b>383</b> in which a report is generated regarding understanding deficiencies and/or corrective measures to be taken. In addition, a report is generated and sent to the owner/operator of the other system. If there are no understanding deficiencies and/or corrective measures, no auto correction is needed, and operations are complete at the done state.</p><p id="p-0504" num="0544">If an autocorrect can be done, operation proceeds to state <b>384</b> where the analysis system updates a determined ability to understand the other system. Corrections are then implemented and operation proceeds back to state <b>381</b>. Note that corrections may be automatically performed for some deficiencies but not others, depending upon the nature of the deficiency.</p><p id="p-0505" num="0545">From state <b>381</b>, if the tested understanding of the system is adequate, operation proceeds to state <b>385</b> where a report is generated regarding an adequate understanding of the system and the report is sent. From state <b>385</b> if operation is complete, operations proceed to the done state. Alternately, from state <b>385</b> operation may proceed to state <b>386</b> where testing of the assets, system functions and/or security functions of the other system is performed. If testing of the assets, system functions, and/or security functions of the system results in an adequate test result, operation proceeds to state <b>390</b> where a report is generated indicating adequate implementation and/or operation of the system and the report is sent.</p><p id="p-0506" num="0546">Alternately, at state <b>386</b> if the testing of the system results in an inadequate result, operations proceed to state <b>387</b> where deficiencies in the assets, system functions, and/or security functions of the system are tested. At state <b>387</b> differences are compared to identify deficiencies in the assets, system functions, and/or security functions. The analysis then proceeds from state <b>387</b> to state <b>388</b> where a report is generated regarding corrective measures to be taken in response to the assets, system functions, and/or security functions deficiencies. The report is then sent to the owner/operator. If there are no deficiencies and/or corrective measures, no auto correction is needed, and operations are complete at the done state. If autocorrect is required, operation proceeds to state <b>389</b> where the analysis system updates assets, system functions, and/or security functions of the system. Corrections are then implemented and the analysis proceeds to state <b>386</b>. Note that corrections may be automatically performed for some deficiencies but not others, depending upon the nature of the deficiency.</p><p id="p-0507" num="0547"><figref idref="DRAWINGS">FIG. <b>60</b></figref> is a logic diagram of an example of an analysis system analyzing a system, or portion thereof. The method includes the analysis system obtaining system proficiency understanding data regarding the assets of the system (step <b>400</b>) and obtaining data regarding the owner/operator's understanding of the assets (step <b>401</b>). System proficiencies of step <b>400</b> include industry best practices and regulatory requirements, for example. The data obtained from the system at step <b>401</b> is based upon data received regarding the system or received by probing the system.</p><p id="p-0508" num="0548">The data collected at steps <b>400</b> and <b>401</b> is then compared (step <b>402</b>) and a determination is made regarding the comparison. If the comparison is favorable, as determined at step <b>403</b>, meaning that the system proficiency understanding compares favorably to the data regarding understanding, operation is complete, a report is generated (step <b>412</b>), and the report is sent (step <b>413</b>). If the comparison is not favorable, as determined at step <b>403</b>, operation continues with identifying deficiencies in the understanding of the system (step <b>404</b>), identifying corrective measures (step <b>405</b>), generating a corresponding report (step <b>412</b>) and sending the report (step <b>413</b>).</p><p id="p-0509" num="0549">The method also includes the analysis system obtaining system proficiency understanding data of the system functions and/or security implementation and/or operation of the system (step <b>406</b>) and obtaining data regarding the owner/operator's understanding of the system functions and/or security functions implementation and/or operation of the system (step <b>407</b>). System proficiencies of step <b>406</b> include industry best practices and regulatory requirements, for example. The data obtained from the system at step <b>407</b> is based upon data received regarding the system or received by probing the system.</p><p id="p-0510" num="0550">The data collected at steps <b>406</b> and <b>407</b> is then compared (step <b>414</b>) and a determination is made regarding the comparison. If the comparison is favorable, as determined at step <b>415</b>, meaning that the system proficiency understanding compares favorably to the data regarding understanding, operation is complete, a report is generated (step <b>412</b>), and the report is sent (step <b>413</b>). If the comparison is not favorable, as determined at step <b>415</b>, operation continues with identifying deficiencies in the understanding of the system (step <b>416</b>), identifying corrective measures (step <b>417</b>), generating a corresponding report (step <b>412</b>) and sending the report (step <b>413</b>).</p><p id="p-0511" num="0551">The method further includes the analysis system comparing the understanding of the physical structure (obtained at step <b>401</b>) with the understanding of the system functions and/or security functions implementation and/or operation (obtained at step <b>406</b>) at step <b>408</b>. Step <b>408</b> essentially determines whether the understanding of the assets corresponds with the understanding of the system functions and/or security functions of the implementation and/or operation of the system. If the comparison is favorable, as determined at step <b>409</b>, a report is generated (step <b>412</b>), and the report is sent (step <b>413</b>). If the comparison is not favorable, as determined at step <b>409</b>, the method continues with identifying imbalances in the understanding (step <b>410</b>), identifying corrective measures (step <b>410</b>), generating a corresponding report (step <b>412</b>), and sending the report (step <b>413</b>).</p><p id="p-0512" num="0552"><figref idref="DRAWINGS">FIG. <b>61</b></figref> is a logic diagram of another example of an analysis system analyzing a system, or portion thereof. The method begins at step <b>420</b> where the analysis system determines a system evaluation mode (e.g., assets, system functions, and/or security functions) for analysis. The method continues at step <b>421</b> where the analysis system determines a system evaluation level (e.g., the system or a portion thereof). For instance, the analysis system identifies one or more system elements for evaluation.</p><p id="p-0513" num="0553">The method continues at step <b>422</b> where the analysis system determines an analysis perspective (e.g., understanding, implementation, operation, and/or self-evaluate). The method continues at step <b>423</b> where the analysis system determines an analysis viewpoint (e.g., disclosed, discovered, and/or desired). The method continues at step <b>424</b> where the analysis system determines a desired output (e.g., evaluation rating, deficiencies, and/or auto-corrections).</p><p id="p-0514" num="0554">The method continues at step <b>425</b> where the analysis system determines what data to gather based on the preceding determinations. The method continues at step <b>426</b> where the analysis system gathers data in accordance with the determination made in step <b>425</b>. The method continues at step <b>427</b> where the analysis system determines whether the gathered data is to be pre-processed.</p><p id="p-0515" num="0555">If yes, the method continues at step <b>428</b> where the analysis system determines data pre-processing functions (e.g., parse, normalize, tag, and/or de-duplicate). The method continues at step <b>429</b> where the analysis system pre-processes the data based on the pre-processing functions to produce pre-processed data. Whether the data is pre-processed or not, the method continues at step <b>430</b> where the analysis system determines one or more evaluation categories (e.g., identify, protect, detect, respond, and/or recover) and/or sub-categories for evaluation. Note that this may be done prior to step <b>425</b> and be part of determining the data to gather.</p><p id="p-0516" num="0556">The method continues at step <b>431</b> where the analysis system analyzes the data in accordance with the determine evaluation categories and in accordance with a selected evaluation metric (e.g., process, policy, procedure, automation, certification, and/or documentation) to produce analysis results. The method continues at step <b>432</b> where the analysis system processes the analysis results to produce the desired output (e.g., evaluation rating, deficiencies, and/or auto-correct). The method continues at step <b>432</b> where the analysis system determines whether to end the method or repeat it for another analysis of the system.</p><p id="p-0517" num="0557"><figref idref="DRAWINGS">FIG. <b>62</b></figref> is a logic diagram of another example of an analysis system analyzing a system or portion thereof. The method begins at step <b>440</b> where the analysis system determines physical assets of the system, or portion thereof, to analyze (e.g., assets in the resulting system). Recall that a physical asset is a computing entity, a computing device, a user software application, a system software application (e.g., operating system, etc.), a software tool, a network software application, a security software application, a system monitoring software application, and the like.</p><p id="p-0518" num="0558">The method continues at step <b>441</b> where the analysis system ascertains implementation of the system, or portion thereof (e.g., assets designed to be, and/or built, in the system). The method continues at step <b>442</b> where the analysis system correlates components of the assets to components of the implementation (e.g., do the assets of the actual system correlate with assets design/built to be in the system).</p><p id="p-0519" num="0559">The method continues at step <b>443</b> where the analysis system scores the components of the physical assets in accordance with the mapped components of the implementation. For example, the analysis system scores how well the assets of the actual system correlate with assets design/built to be in the system. The scoring may be based on one or more evaluation metrics (e.g. process, policy, procedure, automation, certification, and/or documentation). The method continues at step <b>444</b> where the analysis system performs a function on the scores to obtain a result (e.g., an evaluation rating, identified deficiencies, and/or auto-correction of deficiencies).</p><p id="p-0520" num="0560">The method continues at step <b>445</b> where the analysis system determines whether the result is equal or greater than a target result (e.g., the evaluation rating is a certain value). If yes, the method continues at step <b>446</b> where the analysis system indicates that the system, or portion thereof, passes this particular test. If the results are less than the target result, the method continues at step <b>447</b> where the analysis system identifies vulnerabilities in the physical assets and/or in the implementation. For example, the analysis system determines that a security software application is missing from several computing devices in the system, or portion thereof, being analyzed.</p><p id="p-0521" num="0561">The method continues at step <b>448</b> where the analysis system determines, if possible, corrective measures of the identified vulnerabilities. The method continues at step <b>449</b> where the analysis system determines whether the corrective measures can be done automatically. If not, the method continues at step <b>451</b> where the analysis system reports the corrective measures. If yes, the method continues at step <b>450</b> where the analysis system auto-corrects the vulnerabilities.</p><p id="p-0522" num="0562"><figref idref="DRAWINGS">FIG. <b>63</b></figref> is a logic diagram of another example of an analysis system analyzing a system or portion thereof. The method begins at step <b>460</b> where the analysis system determines physical assets of the system, or portion thereof, to analyze (e.g., assets and their intended operation). The method continues at step <b>461</b> where the analysis system ascertains operation of the system, or portion thereof (e.g., the operations actually performed by the assets). The method continues at step <b>462</b> where the analysis system correlates components of the assets to components of operation (e.g., do the identified operations of the assets correlate with the operations actually performed by the assets).</p><p id="p-0523" num="0563">The method continues at step <b>463</b> where the analysis system scores the components of the physical assets in accordance with the mapped components of the operation. For example, the analysis system scores how well the identified operations of the assets correlate with operations actually performed by the assets. The scoring may be based on one or more evaluation metrics (e.g. process, policy, procedure, automation, certification, and/or documentation). The method continues at step <b>464</b> where the analysis system performs a function on the scores to obtain a result (e.g., an evaluation rating, identified deficiencies, and/or auto-correction of deficiencies).</p><p id="p-0524" num="0564">The method continues at step <b>465</b> where the analysis system determines whether the result is equal or greater than a target result (e.g., the evaluation rating is a certain value). If yes, the method continues at step <b>466</b> where the analysis system indicates that the system, or portion thereof, passes this particular test. If the results are less than the target result, the method continues at step <b>467</b> where the analysis system identifies vulnerabilities in the physical assets and/or in the operation.</p><p id="p-0525" num="0565">The method continues at step <b>468</b> where the analysis system determines, if possible, corrective measures of the identified vulnerabilities. The method continues at step <b>469</b> where the analysis system determines whether the corrective measures can be done automatically. If not, the method continues at step <b>471</b> where the analysis system reports the corrective measures. If yes, the method continues at step <b>470</b> where the analysis system auto-corrects the vulnerabilities.</p><p id="p-0526" num="0566"><figref idref="DRAWINGS">FIG. <b>64</b></figref> is a logic diagram of another example of an analysis system analyzing a system or portion thereof. The method begins at step <b>480</b> where the analysis system determines system functions of the system, or portion thereof, to analyze. The method continues at step <b>481</b> where the analysis system ascertains implementation of the system, or portion thereof (e.g., system functions designed to be, and/or built, in the system). The method continues at step <b>482</b> where the analysis system correlates components of the system functions to components of the implementation (e.g., do the system functions of the actual system correlate with system functions design/built to be in the system).</p><p id="p-0527" num="0567">The method continues at step <b>483</b> where the analysis system scores the components of the system functions in accordance with the mapped components of the implementation. For example, the analysis system scores how well the system functions of the actual system correlate with system functions design/built to be in the system. The scoring may be based on one or more evaluation metrics (e.g. process, policy, procedure, automation, certification, and/or documentation). The method continues at step <b>484</b> where the analysis system performs a function on the scores to obtain a result (e.g., an evaluation rating, identified deficiencies, and/or auto-correction of deficiencies).</p><p id="p-0528" num="0568">The method continues at step <b>485</b> where the analysis system determines whether the result is equal or greater than a target result (e.g., the evaluation rating is a certain value). If yes, the method continues at step <b>486</b> where the analysis system indicates that the system, or portion thereof, passes this particular test. If the results are less than the target result, the method continues at step <b>487</b> where the analysis system identifies vulnerabilities in the physical assets and/or in the implementation.</p><p id="p-0529" num="0569">The method continues at step <b>488</b> where the analysis system determines, if possible, corrective measures of the identified vulnerabilities. The method continues at step <b>489</b> where the analysis system determines whether the corrective measures can be done automatically. If not, the method continues at step <b>491</b> where the analysis system reports the corrective measures. If yes, the method continues at step <b>490</b> where the analysis system auto-corrects the vulnerabilities.</p><p id="p-0530" num="0570"><figref idref="DRAWINGS">FIG. <b>65</b></figref> is a logic diagram of another example of an analysis system analyzing a system or portion thereof. The method begins at step <b>500</b> where the analysis system determines system functions of the system, or portion thereof, to analyze. The method continues at step <b>501</b> where the analysis system ascertains operation of the system, or portion thereof (e.g., the operations associated with the system functions). The method continues at step <b>502</b> where the analysis system correlates components of the system functions to components of operation (e.g., do the identified operations of the system functions correlate with the operations actually performed to provide the system functions).</p><p id="p-0531" num="0571">The method continues at step <b>503</b> where the analysis system scores the components of the system functions in accordance with the mapped components of the operation. For example, the analysis system scores how well the identified operations to support the system functions correlate with operations actually performed to support the system functions. The scoring may be based on one or more evaluation metrics (e.g. process, policy, procedure, automation, certification, and/or documentation). The method continues at step <b>504</b> where the analysis system performs a function on the scores to obtain a result (e.g., an evaluation rating, identified deficiencies, and/or auto-correction of deficiencies).</p><p id="p-0532" num="0572">The method continues at step <b>505</b> where the analysis system determines whether the result is equal or greater than a target result (e.g., the evaluation rating is a certain value). If yes, the method continues at step <b>506</b> where the analysis system indicates that the system, or portion thereof, passes this particular test. If the results are less than the target result, the method continues at step <b>507</b> where the analysis system identifies vulnerabilities in the physical assets and/or in the operation.</p><p id="p-0533" num="0573">The method continues at step <b>508</b> where the analysis system determines, if possible, corrective measures of the identified vulnerabilities. The method continues at step <b>509</b> where the analysis system determines whether the corrective measures can be done automatically. If not, the method continues at step <b>511</b> where the analysis system reports the corrective measures. If yes, the method continues at step <b>510</b> where the analysis system auto-corrects the vulnerabilities.</p><p id="p-0534" num="0574"><figref idref="DRAWINGS">FIG. <b>66</b></figref> is a logic diagram of another example of an analysis system analyzing a system or portion thereof. The method begins at step <b>520</b> where the analysis system determines security functions of the system, or portion thereof, to analyze. The method continues at step <b>521</b> where the analysis system ascertains implementation of the system, or portion thereof (e.g., security functions designed to be, and/or built, in the system). The method continues at step <b>522</b> where the analysis system correlates components of the security functions to components of the implementation (e.g., do the security functions of the actual system correlate with security functions design/built to be in the system).</p><p id="p-0535" num="0575">The method continues at step <b>523</b> where the analysis system scores the components of the security functions in accordance with the mapped components of the implementation. For example, the analysis system scores how well the security functions of the actual system correlate with security functions design/built to be in the system. The scoring may be based on one or more evaluation metrics (e.g. process, policy, procedure, automation, certification, and/or documentation). The method continues at step <b>524</b> where the analysis system performs a function on the scores to obtain a result (e.g., an evaluation rating, identified deficiencies, and/or auto-correction of deficiencies).</p><p id="p-0536" num="0576">The method continues at step <b>525</b> where the analysis system determines whether the result is equal or greater than a target result (e.g., the evaluation rating is a certain value). If yes, the method continues at step <b>526</b> where the analysis system indicates that the system, or portion thereof, passes this particular test. If the results are less than the target result, the method continues at step <b>527</b> where the analysis system identifies vulnerabilities in the physical assets and/or in the implementation.</p><p id="p-0537" num="0577">The method continues at step <b>528</b> where the analysis system determines, if possible, corrective measures of the identified vulnerabilities. The method continues at step <b>529</b> where the analysis system determines whether the corrective measures can be done automatically. If not, the method continues at step <b>531</b> where the analysis system reports the corrective measures. If yes, the method continues at step <b>530</b> where the analysis system auto-corrects the vulnerabilities.</p><p id="p-0538" num="0578"><figref idref="DRAWINGS">FIG. <b>67</b></figref> is a logic diagram of another example of an analysis system analyzing a system or portion thereof. The method begins at step <b>540</b> where the analysis system determines security functions of the system, or portion thereof, to analyze. The method continues at step <b>541</b> where the analysis system ascertains operation of the system, or portion thereof (e.g., the operations associated with the security functions). The method continues at step <b>542</b> where the analysis system correlates components of the security functions to components of operation (e.g., do the identified operations of the security functions correlate with the operations actually performed to provide the security functions).</p><p id="p-0539" num="0579">The method continues at step <b>543</b> where the analysis system scores the components of the security functions in accordance with the mapped components of the operation. For example, the analysis system scores how well the identified operations to support the security functions correlate with operations actually performed to support the security functions. The scoring may be based on one or more evaluation metrics (e.g. process, policy, procedure, automation, certification, and/or documentation). The method continues at step <b>544</b> where the analysis system performs a function on the scores to obtain a result (e.g., an evaluation rating, identified deficiencies, and/or auto-correction of deficiencies).</p><p id="p-0540" num="0580">The method continues at step <b>545</b> where the analysis system determines whether the result is equal or greater than a target result (e.g., the evaluation rating is a certain value). If yes, the method continues at step <b>546</b> where the analysis system indicates that the system, or portion thereof, passes this particular test. If the results are less than the target result, the method continues at step <b>547</b> where the analysis system identifies vulnerabilities in the physical assets and/or in the operation.</p><p id="p-0541" num="0581">The method continues at step <b>548</b> where the analysis system determines, if possible, corrective measures of the identified vulnerabilities. The method continues at step <b>549</b> where the analysis system determines whether the corrective measures can be done automatically. If not, the method continues at step <b>551</b> where the analysis system reports the corrective measures. If yes, the method continues at step <b>550</b> where the analysis system auto-corrects the vulnerabilities.</p><p id="p-0542" num="0582"><figref idref="DRAWINGS">FIG. <b>68</b></figref> is a logic diagram of an example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>560</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for an identification evaluation. An identification evaluation includes evaluating the system's asset management, the system's business environment matters, the system's governance matters, the system's risk assessment matters, the system's risk management, the system's access control matters, the system's awareness &#x26; training matters, and/or the system's data security matters.</p><p id="p-0543" num="0583">The method continues at step <b>561</b> where the analysis system determines at least one evaluation perspective for use in performing the identification evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions are understood. An implementation perspective is with regard to how well the assets, system functions, and/or security functions are implemented. An operation perspective is with regard to how well the assets, system functions, and/or security functions operate. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understanding, implementation, and/or operation of assets, system functions, and/or security functions.</p><p id="p-0544" num="0584">The method continues at step <b>562</b> where the analysis system determines at least one evaluation viewpoint for use in performing the identification evaluation on the system aspect. An evaluation viewpoint is a disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0545" num="0585">The method continues at step <b>563</b> where the analysis system obtains identification data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Identification data is data obtained about the system aspect. The obtaining of identification data will be discussed in greater detail with reference to <figref idref="DRAWINGS">FIGS. <b>72</b>-<b>80</b></figref>.</p><p id="p-0546" num="0586">The method continues at step <b>564</b> where the analysis system calculates an identification rating as a measure of system identification maturity for the system aspect based on the identification data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric. The calculating of an identification rating will be discussed in greater detail with reference to <figref idref="DRAWINGS">FIGS. <b>81</b>-<b>109</b></figref>. As used herein, maturity refers to level of development, level of operation reliability, level of operation predictability, level of operation repeatability, level of understanding, level of implementation, level of advanced technologies, level of operation efficiency, level of proficiency, and/or state-of-the-art level.</p><p id="p-0547" num="0587"><figref idref="DRAWINGS">FIG. <b>69</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular, for determining a system aspect. The method begins at step <b>565</b> where the analysis system determines at least one system element of the system. A system element includes one or more system assets which is a physical asset and/or a conceptual asset. For example, a physical asset is a computing entity, a computing device, a user software application, a system software application (e.g., operating system, etc.), a software tool, a network software application, a security software application, a system monitoring software application, and the like. A system element and/or system asset includes an organization identifier, a division identifier, a department identifier, a group identifier, a sub-group identifier, a device identifier, a software identifier, and/or an internet protocol address identifier.</p><p id="p-0548" num="0588">The method continues at step <b>566</b> where the analysis system determines at least one system criteria of the system. A system criteria is system guidelines, system requirements, system design, system build, or resulting system. Evaluation based on system criteria assists with determining where a deficiency originated and/or how it might be corrected. For example, if the system requirements were lacking a requirement for handling a particular type of threat, the lack of system requirements could be identified and corrected.</p><p id="p-0549" num="0589">The method continues at step <b>567</b> where the analysis system determines at least one system mode of the system. A system mode is assets, system functions, or system security. The method continues at step <b>568</b> where the analysis system determines the system aspect based on the at least one system element, the at least one system criteria, and the at least one system mode. As an example, a system aspect is determined to be for assets with respect to system requirements of system elements in a particular division. As another example, a system aspect is determined to be for system functions with respect to system design and/or system build of system elements in a particular division. As yet another example, a system aspect is determined to be for assets, system functions, and security functions with respect to guidelines, system requirements, system design, system build, and resulting system of system elements in the organization (e.g., the entire system).</p><p id="p-0550" num="0590"><figref idref="DRAWINGS">FIG. <b>70</b></figref> is a schematic block diagram of an example of an analysis system <b>10</b> determining an identification rating for a system, or portion thereof. In this example, the control module <b>256</b> receives an input <b>271</b> from the system user interface module <b>81</b> loaded on a system <b>11</b>. The input <b>271</b> identifies the system aspect to be analyzed and how it is to be analyzed.</p><p id="p-0551" num="0591">The control module <b>256</b> determines one or more system elements, one or more system criteria, and one or more system modes based on the system aspect. The control module <b>256</b> also determines one or more evaluation perspectives, one or more evaluation viewpoints, and/or one or more evaluation rating metrics from the input. As an example, the input <b>271</b> could specify the evaluation perspective(s), the evaluation viewpoint(s), the evaluation rating metric(s), and/or analysis output(s). As another example, the input <b>271</b> indicates a desired analysis output (e.g., an evaluation rating, deficiencies identified, and/or deficiencies auto-corrected). From this input, the control module <b>256</b> determines the evaluation perspective(s), the evaluation viewpoint(s), the evaluation rating metric(s) to fulfill the desired analysis output.</p><p id="p-0552" num="0592">In addition, the control module <b>256</b> generates data gathering parameters <b>263</b>, pre-processing parameters <b>264</b>, data analysis parameters <b>265</b>, and/or evaluation parameters <b>266</b> as discussed with reference to <figref idref="DRAWINGS">FIG. <b>35</b></figref>. The data input module <b>250</b> obtains identification data in accordance with the data gathering parameters <b>263</b> from the data extraction module(s) <b>80</b> loaded on the system <b>11</b>, other external feeds <b>258</b>, and/or system proficiency data <b>260</b>.</p><p id="p-0553" num="0593">The pre-processing module <b>251</b> processes the identification data in accordance with the pre-processing parameters <b>264</b> to produce pre-processed data <b>414</b>. The identification data and/or the pre-processed data <b>414</b> may be stored in the database <b>275</b>. The data analysis module <b>252</b> calculates an identification rating <b>219</b> based on the pre-processed data <b>414</b> in accordance with the data analysis parameters <b>265</b> and the analysis modeling <b>268</b>.</p><p id="p-0554" num="0594">If the requested analysis output was for an evaluation rating only, the data output module <b>255</b> outputs the identification rating <b>219</b> as the output <b>269</b>. The system user interface module <b>80</b> renders a graphical representation of the identification rating and the database <b>275</b> stores it.</p><p id="p-0555" num="0595">If the required analysis output included identify deficiencies, then the evaluation processing module <b>254</b> evaluates the identification rating <b>219</b> and may further evaluate the pre-processed data to identify one or more deficiencies <b>232</b>. In addition, the evaluation processing module <b>254</b> determines whether a deficiency can be auto-corrected and, if so, determines the auto-correction <b>235</b>. In this instance, the data output module <b>255</b> outputs the identification rating <b>219</b>, the deficiencies <b>232</b>, and the auto-corrections <b>235</b> as output <b>269</b> to the database <b>275</b>, the system user interface module <b>81</b>, and the remediation module <b>257</b>.</p><p id="p-0556" num="0596">The system user interface module <b>80</b> renders a graphical representation of the identification rating, the deficiencies, and/or the auto-corrections. The database <b>275</b> stores the identification rating, the deficiencies, and/or the auto-corrections. The remediation module <b>257</b> processes the auto-corrections <b>235</b> within the system <b>11</b>, verifies the auto-corrections, and then records the execution of the auto-correction and its verification.</p><p id="p-0557" num="0597"><figref idref="DRAWINGS">FIG. <b>71</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating, with respect to process, policy, procedure, certification, documentation, and/or automation, the understanding of the system build for identifying assets of an engineering department based on disclosed data to produce an evaluation rating.</p><p id="p-0558" num="0598">For this specific example, the analysis system <b>10</b> obtains disclosed data from the system regarding the system build associated with the assets of the engineering department. From the disclosed data, the analysis system renders a first evaluation rating for the understanding of the system build for identifying assets with respect to an evaluation rating metric of process. The analysis system renders a second evaluation rating for the understanding of the system build for identifying assets with respect to evaluation rating metric of policy. The analysis system renders a third evaluation rating for the understanding of the system build for identifying assets with respect to an evaluation rating metric of procedure. The analysis system renders a fourth evaluation rating for the understanding of the system build for identifying assets with respect to an evaluation rating metric of certification. The analysis system renders a fifth evaluation rating for the understanding of the system build for identifying assets with respect to an evaluation rating metric of documentation. The analysis system renders a sixth evaluation rating for the understanding of the system build for identifying assets with respect to an evaluation rating metric of automation.</p><p id="p-0559" num="0599">The analysis system <b>11</b> generates the identification rating for the understanding of the system build for identifying assets based on the six evaluation ratings. As example, each of the six evaluation rating metrics has a maximum potential rating (e.g., 50 for process, 20 for policy, 15 for procedure, 10 for certification, 20 for documentation, and 20 for automation), which has a maximum rating of 135. Continuing with this example, the first evaluation rating based on process is 35; the second evaluation rating based on policy is 10; the third evaluation rating based on procedure is 10; the fourth evaluation rating based on certification is 10; the fifth evaluation rating based on documentation is 15; and the sixth evaluation rating based on automation is 20, resulting in a cumulative score of 100 out of a possible 135. This rating indicates that there is room for improvement and provides a basis for identifying deficiencies.</p><p id="p-0560" num="0600"><figref idref="DRAWINGS">FIG. <b>72</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular, for obtaining identification data, which is a collection of identification information. The method begins at step <b>570</b> where the analysis system determines data gathering parameters regarding the system aspect in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and the least one evaluation rating metric. The generation of data gathering parameters will be discussed in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>74</b></figref>.</p><p id="p-0561" num="0601">The method continues at step <b>571</b> where the analysis system identifies system elements of the system aspect based on the data gathering parameters and obtains identification information from the system elements in accordance with the data gathering parameters. The obtaining of the identification information is discussed in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>73</b></figref>.</p><p id="p-0562" num="0602">The method continues at step <b>572</b> where the analysis system records the identification information from the system elements to produce the identification data. As an example, the analysis system stores the identification information in the database. As another example, the analysis system temporarily stores the identification information in the data input module. As yet another example, the analysis system uses some form of retaining a record of the identification information. Examples of identification information are provided with reference to <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0563" num="0603"><figref idref="DRAWINGS">FIG. <b>73</b></figref> is a logic diagram of an analysis system determining an identification rating for a system, or portion thereof; in particular, for obtaining the identification information. The method begins at step <b>573</b> where the analysis system probes (e.g., push and/or pull information requests) a system element in accordance with the data gathering parameters to obtain a system element data response. The analysis system would do this for most, if not all of the system elements of the system aspect (e.g., the system, or portion of the system, being evaluated).</p><p id="p-0564" num="0604">The method continues at step <b>574</b> where the analysis system identifies vendor information from the system element data response. For example, vendor information includes vendor name, a model name, a product name, a serial number, a purchase date, and/or other information to identify the system element. The method continues at step <b>575</b> where the analysis system tags the system element data response with the vendor information.</p><p id="p-0565" num="0605"><figref idref="DRAWINGS">FIG. <b>74</b></figref> is a logic diagram of a further of an analysis system determining an identification rating for a system, or portion thereof; in particular, for determining the data gathering parameters for the evaluation category of identify. The method begins at step <b>576</b> where the analysis system, for the system aspect, ascertains identity of one or more system elements of the system aspect. For a system element of the system aspect, the method continues at step <b>577</b> where the analysis system determines a first data gathering parameter based on at least one system criteria (e.g., guidelines, system requirements, system design, system build, and/or resulting system) of the system aspect. For example, if the determined selected criteria is system requirements, then the first data gathering parameter would be to search for system requirement information.</p><p id="p-0566" num="0606">The method continues at step <b>578</b> where the analysis system determines a second data gathering parameter based on at least one system mode (e.g., assets, system functions, and/or security functions). For example, if the determined selected mode is system functions, then the second data gathering parameter would be to search for system function information.</p><p id="p-0567" num="0607">The method continues at step <b>579</b> where the analysis system determines a third data gathering parameter based on the at least one evaluation perspective (e.g., understanding, implementation, operation, and/or self-evaluation). For example, if the determined selected evaluation perspective is operation, then the third data gathering parameter would be to search for information regarding operation of the system aspect.</p><p id="p-0568" num="0608">The method continues at step <b>580</b> where the analysis system determines a fourth data gathering parameter based on the at least one evaluation viewpoint (e.g., disclosed data, discovered data, and/or desired data). For example, if the determined selected evaluation viewpoint is disclosed and discovered data, then the fourth data gathering parameter would be to obtain for disclosed data and to obtain discovered data.</p><p id="p-0569" num="0609">The method continues at step <b>581</b> where the analysis system determines a fifth data gathering parameter based on the at least one evaluation rating metric (e.g., process, policy, procedure, certification, documentation, and/or automation). For example, if the determined selected evaluation rating metric is process, policy, procedure, certification, documentation, and automation, then the fifth data gathering parameter would be to search for data regarding process, policy, procedure, certification, documentation, and automation.</p><p id="p-0570" num="0610">The analysis system generates the data gathering parameters from the first through fifth data gathering parameters. For example, the data gathering parameters include search for information regarding processes, policies, procedures, certifications, documentation, and/or automation (fifth parameter) pertaining to identifying (selected evaluation category) system requirements (first parameter) for system operation (third parameter) of system functions (second parameter) from disclosed and discovered data (fourth parameter).</p><p id="p-0571" num="0611"><figref idref="DRAWINGS">FIG. <b>75</b></figref> is a diagram of an example of identification data for use by an analysis system to generate an identification rating for a system, or portion thereof. For the evaluation category of identify, the sub-categories and/or sub-sub categories are cues for determining what data to gather for an identify evaluation. The sub-categories include asset management, business environment, governance, risk management, access control, awareness &#x26; training, and data security.</p><p id="p-0572" num="0612">The asset management sub-category includes the sub-sub categories of HW inventoried, SW inventoried, data flow mapped out, external systems cataloged, resources have been prioritized, and security roles have been established. The business environment sub-category includes the sub-sub categories of supply chain roles defined, industry critical infrastructure identified, business priorities established, critical services identified, and resiliency requirements identified.</p><p id="p-0573" num="0613">The governance sub-category includes the sub-sub categories of security policies are established, security factors aligned, and legal requirements are identified. The risk assessment sub-category includes the sub-sub categories of vulnerabilities identified, external sources are leveraged, threats are identified, business impacts are identified, risk levels are identified, and risk responses are identified. The risk management sub-category includes the sub-sub categories of risk management processes are established, risk tolerances are established, and risk tolerances are tied to business environment.</p><p id="p-0574" num="0614">The access control sub-category includes the sub-sub categories of remote access control is defined, permissions are defined, and network integrity is defined. The awareness &#x26; training sub-category includes the sub-sub categories of users are trained, user privileges are known, third party responsibilities are known, executive responsibilities are known, and IT and security responsibilities are known. The data security sub-category includes the sub-sub categories of data at rest protocols are established, data in transit protocols are established, formal asset management protocols are established, adequate capacity of the system is established, data leak prevention protocols are established, integrity checking protocols are established, and use and development separation protocols are established.</p><p id="p-0575" num="0615"><figref idref="DRAWINGS">FIG. <b>76</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a system, or portion thereof. In this example, the identification data includes one or more of diagrams, one or more design specifications, one or more purchases, one or more installation notes, one or more maintenance records, one or more user information records, one or more device information records, one or more operating manuals, and/or one or more other documents regarding a system aspect.</p><p id="p-0576" num="0616">A diagram is a data flow diagram, an HLD diagram, an LLD diagram, a DLD diagram, an operation flowchart, a software architecture diagram, a hardware architecture diagram, and/or other diagram regarding, the design, build, and/or operation of the system, or a portion thereof. A design specification is a security specification, a hardware specification, a software specification, a data flow specification, a business operation specification, a build specification, and/or other specification regarding the system, or a portion thereof.</p><p id="p-0577" num="0617">A purchase is a purchase order, a purchase fulfillment document, bill of laden, a quote, a receipt, and/or other information regarding purchases of assets of the system, or a portion thereof. An installation note is a record regarding the installation of an asset of the system, or portion thereof. A maintenance record is a record regarding each maintenance service performed on an asset of the system, or portion thereof.</p><p id="p-0578" num="0618">User information includes affiliation of a user with one or more assets of the system, or portion thereof. User information may also include a log of use of the one or more assets by the user or others. User information may also include privileges and/or restrictions imposed on the use of the one or more assets.</p><p id="p-0579" num="0619">Device information includes an identity for an asset of the system, or portion thereof. A device is identified by vendor information (e.g., name, address, contact person information, etc.), a serial number, a device description, a device model number, a version, a generation, a purchase date, an installation date, a service date, and/or other mechanism for identifying a device.</p><p id="p-0580" num="0620"><figref idref="DRAWINGS">FIG. <b>77</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a system, or portion thereof. In particular, this example illustrates assets of the system, or portion thereof, that would be part of the identification data and/or engaged with to obtain further identification information, which may become part of the identification information.</p><p id="p-0581" num="0621">As shown, asset information of the system, or portion thereof, includes a list of network devices (e.g., hardware and/or software), a list of networking tools, a list of security devices (e.g., hardware and/or software), a list of security tools, a list of storage devices (e.g., hardware and/or software), a list of servers (e.g., hardware and/or software), a list of user applications, a list of user devices (e.g., hardware and/or software), a list of design tools, an list of system applications, and/or a list of verification tools. Recall that a tool is a program that functions to develop, repair, and/or enhance other programs and/or hardware of the system, or portion thereof.</p><p id="p-0582" num="0622">Each list of devices includes vendor information (e.g., name, address, contact person information, etc.), a serial number, a device description, a device model number, a version, a generation, a purchase date, an installation date, a service date, and/or other mechanism for identifying a device. Each list of software includes vendor information (e.g., name, address, contact person information, etc.), a serial number, a software description, a software model number, a version, a generation, a purchase date, an installation date, a service date, and/or other mechanism for identifying software. Each list of tools includes vendor information (e.g., name, address, contact person information, etc.), a serial number, a tool description, a tool model number, a version, a generation, a purchase date, an installation date, a service date, and/or other mechanism for identifying a tool.</p><p id="p-0583" num="0623"><figref idref="DRAWINGS">FIG. <b>78</b></figref> is a diagram of another example of identification data for use by an analysis system to generate an identification rating for a system, or portion thereof. In particular, this example illustrates a list of user devices in a tabular form. The list includes a plurality of columns for various pieces of information regarding a user device and a plurality of rows; one row for each user device.</p><p id="p-0584" num="0624">The columns include a user ID, a user level, a user role, hardware (HW) information, an IP address, user application software (SW) information, device application SW information, device use information, and/or device maintenance information. The user ID includes an individual identifier if a user and may further include an organization ID, a division ID, a department ID, a group ID, and/or a sub-group ID. The user level will be described in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>79</b></figref> and the user role will be described in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>80</b></figref>.</p><p id="p-0585" num="0625">The HW information field stores information regarding the hardware of the device. For example, the HW information includes information regarding a computing device such as vendor information, a serial number, a description of the computing device, a computing device model number, a version of the computing device, a generation of the computing device, and/or other mechanism for identifying a computing device. The HW information may further store information regarding the components of the computing device such as the motherboard, the processor, video graphics card, network card, connection ports, and/or memory.</p><p id="p-0586" num="0626">The user application SW information field stores information regarding the user applications installed on the user's computing device. For example, the user application SW information includes information regarding a SW program (e.g., spreadsheet, word processing, database, email, etc.) such as vendor information, a serial number, a description of the program, a program model number, a version of the program, a generation of the program, and/or other mechanism for identifying a program. The device SW information includes similar information, but for device applications (e.g., operating system, drivers, security, etc.).</p><p id="p-0587" num="0627">The device use data field stores data regarding the use of the device (e.g., use of the computing device and software running on it). For example, the device use data includes a log of use of a user application, or program (e.g., time of day, duration of use, date information, etc.). As another example, the device use data includes a log of data communications to and from the device. As yet another example, the device use data includes a log of network accesses. As a further example, the device use data includes a log of server access (e.g., local and/or remote servers). As still further example, the device use data includes a log of storage access (e.g., local and/or remote memory).</p><p id="p-0588" num="0628">The maintenance field stores data regarding the maintenance of the device and/or its components. As an example, the maintenance data includes a purchase date, purchase information, an installation date, installation notes, a service date, services notes, and/or other maintenance data of the device and/or its components.</p><p id="p-0589" num="0629"><figref idref="DRAWINGS">FIG. <b>79</b></figref> is a diagram of another example of user levels of the device information of <figref idref="DRAWINGS">FIG. <b>78</b></figref>. In this illustration there are three user levels (e.g., C-Level, director level, general level). In practice, there may be more or less user levels than three. For each user level there are options for data access privileges, data access restrictions, network access privileges, network access restrictions, server access privileges, server access restrictions, storage access privileges, storage access restrictions, required user applications, required device applications, and/or prohibited user applications.</p><p id="p-0590" num="0630"><figref idref="DRAWINGS">FIG. <b>80</b></figref> is a diagram of another example of user roles of the device information of <figref idref="DRAWINGS">FIG. <b>78</b></figref>. In this illustration there are four user roles (e.g., project manager, engineer, quality control, administration). In practice, there may be more or less user roles than four. For each user role there are options for data access privileges, data access restrictions, network access privileges, network access restrictions, server access privileges, server access restrictions, storage access privileges, storage access restrictions, required user applications, required device applications, and/or prohibited user applications.</p><p id="p-0591" num="0631"><figref idref="DRAWINGS">FIG. <b>81</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular, for calculating the identification rating. The method begins at step <b>590</b> where the analysis system selects and performs at least two of steps <b>591</b>-<b>596</b>. At step <b>591</b>, the analysis system generates a policy rating for the system aspect based on the identification data and policy analysis parameters and in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and policy as the evaluation rating metric. At step <b>592</b>, the analysis system generates a documentation rating for the system aspect based on the identification data and documentation analysis parameters and in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and documentation as the evaluation rating metric.</p><p id="p-0592" num="0632">At step <b>593</b>, the analysis system generates an automation rating for the system aspect based on the identification data and automation analysis parameters and in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and automation as the evaluation rating metric. At step <b>594</b>, the analysis system generates a policy rating for the system aspect based on the identification data and policy analysis parameters and in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and policy as the evaluation rating metric.</p><p id="p-0593" num="0633">At step <b>595</b>, the analysis system generates a certification rating for the system aspect based on the identification data and certification analysis parameters and in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and certification as the evaluation rating metric. At step <b>596</b>, the analysis system generates a procedure rating for the system aspect based on the identification data and procedure analysis parameters and in accordance with the at least one evaluation perspective, the at least one evaluation viewpoint, and procedure as the evaluation rating metric.</p><p id="p-0594" num="0634">The method continues at step <b>597</b> where the analysis system generates the identification rating based on the selected and performed at least two of the process rating, the policy rating, the documentation rating, the automation rating, the procedure rating, and the certification rating. For example, the identification rating is a summation of the at least individual evaluation metric ratings. As another example, the analysis system performs a mathematical and/or logical function (e.g., a weight average, standard deviation, statistical analysis, trending, etc.) on the at least two individual evaluation metric to produce the identification rating.</p><p id="p-0595" num="0635"><figref idref="DRAWINGS">FIG. <b>82</b></figref> is a schematic block diagram of an embodiment of a scoring module of the data analysis module <b>252</b> that includes a process rating module <b>601</b>, a policy rating module <b>602</b>, a procedure rating module <b>603</b>, a certification rating module <b>604</b>, a documentation rating module <b>605</b>, an automation rating module <b>606</b>, and a cumulative rating module <b>607</b>. In general, the data scoring module generates an identification rating <b>608</b> from a collection of data based on data analysis parameters <b>265</b>.</p><p id="p-0596" num="0636">The process rating module <b>601</b> evaluates the collection of data <b>600</b>, or portion thereof, (e.g., pre-processed data of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to produce a process evaluation rating in accordance with process analysis parameters of the data analysis parameters <b>265</b>. The process analysis parameters indicate how the collection of data is to be evaluated with respect to processes of the system, or portion thereof. As an example, the process analysis parameters include:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0637">an instruction to compare processes of the data <b>600</b> with a list of processes the system, or portion thereof, should have;</li>        <li id="ul0010-0002" num="0638">an instruction to count the number of processes of data <b>600</b> and compare it with a quantity of processes the system, or portion thereof, should have;</li>        <li id="ul0010-0003" num="0639">an instruction to determine last revisions of processes of data <b>600</b> and/or to determine an age of last revisions;</li>        <li id="ul0010-0004" num="0640">an instruction to determine frequency of use of processes of data <b>600</b>;</li>        <li id="ul0010-0005" num="0641">an instruction to determine a volume of access of processes of data <b>600</b>;</li>        <li id="ul0010-0006" num="0642">an instruction to evaluate a process of data <b>600</b> with respect to a checklist regarding content of the process (e.g., what should be in the process);</li>        <li id="ul0010-0007" num="0643">a scaling factor based on the size of the system, or portion thereof;</li>        <li id="ul0010-0008" num="0644">a scaling factor based on the size of the organization;</li>        <li id="ul0010-0009" num="0645">an instruction to compare a balance of local processes with respect to system-wide processes;</li>        <li id="ul0010-0010" num="0646">an instruction to compare topics of the processes of data <b>600</b> with desired topics for processes (which may be at least partially derived from the evaluation category and/or sub-categories); and/or</li>        <li id="ul0010-0011" num="0647">an instruction to evaluate language use within processes of data <b>600</b>.</li>    </ul>    </li></ul></p><p id="p-0597" num="0648">The process rating module <b>601</b> can rate the data <b>600</b> at three levels. The first level is that the system has processes, the system has the right number of processes, and/or the system has processes that address the right topics. The second level digs into the processes themselves to determine whether they are adequately covering the requirements of the system. The third level evaluates how well the processes are used and how well they are adhered to.</p><p id="p-0598" num="0649">As an example, the process rating module <b>601</b> generates a process evaluation rating based on a comparison of the processes of the data <b>600</b> with a list of processes the system, or portion thereof, should have. If all of the processes on the list are found in the data <b>600</b>, then the process evaluation rating is high. The fewer processes on the list that found in the data <b>600</b>, the lower the process evaluation rating will be.</p><p id="p-0599" num="0650">As another example, the process rating module <b>601</b> generates a process evaluation rating based on a determination of the last revisions of processes of data <b>600</b> and/or to determine an age of last revisions. As a specific example, if processes are revised at a rate that corresponds to a rate of revision in the industry, then a relatively high process evaluation rate would be produced. As another specific example, if processes are revised at a much lower rate that corresponds to a rate of revision in the industry, then a relatively low process evaluation rate would be produced (implies a lack of attention to the processes). As yet another specific example, if processes are revised at a much higher rate that corresponds to a rate of revision in the industry, then a relatively low process evaluation rate would be produced (implies processes are inaccurate, incomplete, and/or created with a lack of knowledge as to what's needed).</p><p id="p-0600" num="0651">As another example, the process rating module <b>601</b> generates a process evaluation rating based on a determination of frequency of use of processes of data <b>600</b>. As a specific example, if processes are used at a frequency (e.g., x times per week) that corresponds to a frequency of use in the industry, then a relatively high process evaluation rate would be produced. As another specific example, if processes are used at a much lower frequency that corresponds to a frequency of use in the industry, then a relatively low process evaluation rate would be produced (implies a lack of using and adhering to the processes). As yet another specific example, if processes are used at a much higher frequency that corresponds to a frequency of use in the industry, then a relatively low process evaluation rate would be produced (implies processes are inaccuracy, incompleteness, and/or difficult to use).</p><p id="p-0601" num="0652">As another example, the process rating module <b>601</b> generates a process evaluation rating based on an evaluation of a process of data <b>600</b> with respect to a checklist regarding content of the policy (e.g., what should be in the policy, which may be based, at least in part, on an evaluation category, sub-category, and/or sub-sub category). As a specific example, the topics contained in the process of data <b>600</b> is compared to a checklist of desired topics for such a process. If all of the topics on the checklist are found in the process of data <b>600</b>, then the process evaluation rating is high. The fewer topics on the checklist that found in the process of data <b>600</b>, the lower the process evaluation rating will be.</p><p id="p-0602" num="0653">As another example, the process rating module <b>601</b> generates a process evaluation rating based on a comparison of balance between local processes of data <b>600</b> and system-wide processes of data <b>600</b>. As a specific example, most security processes should be system-wide. Thus, if there are a certain percentage (e.g., less than 10%) of security processes that are local, then a relatively high process evaluation rating will be generated. Conversely, the greater the percentage of local security processes, the lower the process evaluation rating will be.</p><p id="p-0603" num="0654">As another example, the process rating module <b>601</b> generates a process evaluation rating based on evaluation of language use within processes of data <b>600</b>. As a specific example, most security requirements are mandatory. Thus, if the policy includes too much use of the word &#x201c;may&#x201d; (which implies optionality) versus the word &#x201c;shall (which implies must), the lower the process evaluation rating will be.</p><p id="p-0604" num="0655">The process rating module <b>601</b> may perform a plurality of the above examples of process evaluation to produce a plurality of process evaluation ratings. The process rating module <b>601</b> may output the plurality of the process evaluation ratings to the cumulative rating module <b>607</b>. Alternatively, the process rating module <b>601</b> may perform a function (e.g., a weight average, standard deviation, statistical analysis, etc.) on the plurality of process evaluation ratings to produce a process evaluation rating that's provided to the cumulative rating module <b>607</b>.</p><p id="p-0605" num="0656">The policy rating module <b>602</b> evaluates the collection of data <b>600</b>, or portion thereof, (e.g., pre-processed data of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to produce a policy evaluation rating in accordance with policy analysis parameters of the data analysis parameters <b>265</b>. The policy analysis parameters indicate how the collection of data is to be evaluated with respect to policies of the system, or portion thereof. As an example, the policy analysis parameters include:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0657">an instruction to compare policies of the data <b>600</b> with a list of policies the system, or portion thereof, should have;</li>        <li id="ul0012-0002" num="0658">an instruction to count the number of policies of data <b>600</b> and compare it with a quantity of policies the system, or portion thereof, should have;</li>        <li id="ul0012-0003" num="0659">an instruction to determine last revisions of policies of data <b>600</b> and/or to determine an age of last revisions;</li>        <li id="ul0012-0004" num="0660">an instruction to determine frequency of use of policies of data <b>600</b>;</li>        <li id="ul0012-0005" num="0661">an instruction to determine a volume of access of policies of data <b>600</b>;</li>        <li id="ul0012-0006" num="0662">an instruction to evaluate a policy of data <b>600</b> with respect to a checklist regarding content of the policy (e.g., what should be in the policy);</li>        <li id="ul0012-0007" num="0663">a scaling factor based on the size of the system, or portion thereof</li>        <li id="ul0012-0008" num="0664">a scaling factor based on the size of the organization;</li>        <li id="ul0012-0009" num="0665">an instruction to compare a balance of local policies with respect to system-wide policies;</li>        <li id="ul0012-0010" num="0666">an instruction to compare topics of the policies of data <b>600</b> with desired topics for policies (which may be at least partially derived from the evaluation category and/or sub-categories); and/or</li>        <li id="ul0012-0011" num="0667">an instruction to evaluate language use within policies of data <b>600</b>.</li>    </ul>    </li></ul></p><p id="p-0606" num="0668">The policy rating module <b>602</b> can rate the data <b>600</b> at three levels. The first level is that the system has policies, the system has the right number of policies, and/or the system has policies that address the right topics. The second level digs into the policies themselves to determine whether they are adequately covering the requirements of the system. The third level evaluates how well the policies are used and how well they are adhered to.</p><p id="p-0607" num="0669">The procedure rating module <b>603</b> evaluates the collection of data <b>600</b>, or portion thereof, (e.g., pre-processed data of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to produce a procedure evaluation rating in accordance with procedure analysis parameters of the data analysis parameters <b>265</b>. The procedure analysis parameters indicate how the collection of data is to be evaluated with respect to procedures of the system, or portion thereof. As an example, the procedure analysis parameters include:<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0670">an instruction to compare procedures of the data <b>600</b> with a list of procedures the system, or portion thereof, should have;</li>        <li id="ul0014-0002" num="0671">an instruction to count the number of procedures of data <b>600</b> and compare it with a quantity of procedures the system, or portion thereof, should have;</li>        <li id="ul0014-0003" num="0672">an instruction to determine last revisions of procedures of data <b>600</b> and/or to determine an age of last revisions;</li>        <li id="ul0014-0004" num="0673">an instruction to determine frequency of use of procedures of data <b>600</b>;</li>        <li id="ul0014-0005" num="0674">an instruction to determine a volume of access of procedures of data <b>600</b>;</li>        <li id="ul0014-0006" num="0675">an instruction to evaluate a procedure of data <b>600</b> with respect to a checklist regarding content of the procedure (e.g., what should be in the procedure);</li>        <li id="ul0014-0007" num="0676">a scaling factor based on the size of the system, or portion thereof;</li>        <li id="ul0014-0008" num="0677">a scaling factor based on the size of the organization;</li>        <li id="ul0014-0009" num="0678">an instruction to compare a balance of local procedures with respect to system-wide procedures;</li>        <li id="ul0014-0010" num="0679">an instruction to compare topics of the procedures of data <b>600</b> with desired topics for procedures (which may be at least partially derived from the evaluation category and/or sub-categories); and/or</li>        <li id="ul0014-0011" num="0680">an instruction to evaluate language use within procedures of data <b>600</b>.</li>    </ul>    </li></ul></p><p id="p-0608" num="0681">The procedure rating module <b>603</b> can rate the data <b>600</b> at three levels. The first level is that the system has procedures, the system has the right number of procedures, and/or the system has procedures that address the right topics. The second level digs into the procedures themselves to determine whether they are adequately covering the requirements of the system. The third level evaluates how well the procedures are used and how well they are adhered to.</p><p id="p-0609" num="0682">The certification rating module <b>604</b> evaluates the collection of data <b>600</b>, or portion thereof, (e.g., pre-processed data of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to produce a certification evaluation rating in accordance with certification analysis parameters of the data analysis parameters <b>265</b>. The certification analysis parameters indicate how the collection of data is to be evaluated with respect to certifications of the system, or portion thereof. As an example, the certification analysis parameters include:<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0683">an instruction to compare certifications of the data <b>600</b> with a list of certifications the system, or portion thereof, should have;</li>        <li id="ul0016-0002" num="0684">an instruction to count the number of certifications of data <b>600</b> and compare it with a quantity of certifications the system, or portion thereof, should have;</li>        <li id="ul0016-0003" num="0685">an instruction to determine last revisions of certifications of data <b>600</b> and/or to determine an age of last revisions;</li>        <li id="ul0016-0004" num="0686">an instruction to evaluate a certification of data <b>600</b> with respect to a checklist regarding content of the certification (e.g., what should be certified and/or how it should be certified);</li>        <li id="ul0016-0005" num="0687">a scaling factor based on the size of the system, or portion thereof;</li>        <li id="ul0016-0006" num="0688">a scaling factor based on the size of the organization; and</li>        <li id="ul0016-0007" num="0689">an instruction to compare a balance of local certifications with respect to system-wide certifications.</li>    </ul>    </li></ul></p><p id="p-0610" num="0690">The certification rating module <b>603</b> can rate the data <b>600</b> at three levels. The first level is that the system has certifications, the system has the right number of certifications, and/or the system has certifications that address the right topics. The second level digs into the certifications themselves to determine whether they are adequately covering the requirements of the system. The third level evaluates how well the certifications are maintained and updated.</p><p id="p-0611" num="0691">The documentation rating module <b>603</b> evaluates the collection of data <b>600</b>, or portion thereof, (e.g., pre-processed data of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to produce a documentation evaluation rating in accordance with documentation analysis parameters of the data analysis parameters <b>265</b>. The documentation analysis parameters indicate how the collection of data is to be evaluated with respect to documentation of the system, or portion thereof. As an example, the documentation analysis parameters include:<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0000">    <ul id="ul0018" list-style="none">        <li id="ul0018-0001" num="0692">an instruction to compare documentation of the data <b>600</b> with a list of documentation the system, or portion thereof, should have;</li>        <li id="ul0018-0002" num="0693">an instruction to count the number of documentation of data <b>600</b> and compare it with a quantity of documentation the system, or portion thereof, should have;</li>        <li id="ul0018-0003" num="0694">an instruction to determine last revisions of documentation of data <b>600</b> and/or to determine an age of last revisions;</li>        <li id="ul0018-0004" num="0695">an instruction to determine frequency of use and/or creation of documentation of data <b>600</b>;</li>        <li id="ul0018-0005" num="0696">an instruction to determine a volume of access of documentation of data <b>600</b>;</li>        <li id="ul0018-0006" num="0697">an instruction to evaluate a document of data <b>600</b> with respect to a checklist regarding content of the document (e.g., what should be in the document);</li>        <li id="ul0018-0007" num="0698">a scaling factor based on the size of the system, or portion thereof;</li>        <li id="ul0018-0008" num="0699">a scaling factor based on the size of the organization;</li>        <li id="ul0018-0009" num="0700">an instruction to compare a balance of local documents with respect to system-wide documents;</li>        <li id="ul0018-0010" num="0701">an instruction to compare topics of the documentation of data <b>600</b> with desired topics for documentation (which may be at least partially derived from the evaluation category and/or sub-categories); and/or</li>        <li id="ul0018-0011" num="0702">an instruction to evaluate language use within documentation of data <b>600</b>.</li>    </ul>    </li></ul></p><p id="p-0612" num="0703">The documentation rating module <b>605</b> can rate the data <b>600</b> at three levels. The first level is that the system has documentation, the system has the right number of documents, and/or the system has documents that address the right topics. The second level digs into the documents themselves to determine whether they are adequately covering the requirements of the system. The third level evaluates how well the documentation is used and how well it is maintained.</p><p id="p-0613" num="0704">The automation rating module <b>606</b> evaluates the collection of data <b>600</b>, or portion thereof, (e.g., pre-processed data of <figref idref="DRAWINGS">FIG. <b>35</b></figref>) to produce an automation evaluation rating in accordance with automation analysis parameters of the data analysis parameters <b>265</b>. The automation analysis parameters indicate how the collection of data is to be evaluated with respect to automation of the system, or portion thereof. As an example, the automation analysis parameters include:<ul id="ul0019" list-style="none">    <li id="ul0019-0001" num="0000">    <ul id="ul0020" list-style="none">        <li id="ul0020-0001" num="0705">an instruction to compare automation of the data <b>600</b> with a list of automation the system, or portion thereof, should have;</li>        <li id="ul0020-0002" num="0706">an instruction to count the number of automation of data <b>600</b> and compare it with a quantity of automation the system, or portion thereof, should have;</li>        <li id="ul0020-0003" num="0707">an instruction to determine last revisions of automation of data <b>600</b> and/or to determine an age of last revisions;</li>        <li id="ul0020-0004" num="0708">an instruction to determine frequency of use of automation of data <b>600</b>;</li>        <li id="ul0020-0005" num="0709">an instruction to determine a volume of access of automation of data <b>600</b>;</li>        <li id="ul0020-0006" num="0710">an instruction to evaluate an automation of data <b>600</b> with respect to a checklist regarding content of the automation (e.g., what the automation should do);</li>        <li id="ul0020-0007" num="0711">a scaling factor based on the size of the system, or portion thereof</li>        <li id="ul0020-0008" num="0712">a scaling factor based on the size of the organization;</li>        <li id="ul0020-0009" num="0713">an instruction to compare a balance of local automation with respect to system-wide automation;</li>        <li id="ul0020-0010" num="0714">an instruction to compare topics of the automation of data <b>600</b> with desired topics for automation (which may be at least partially derived from the evaluation category and/or sub-categories); and/or</li>        <li id="ul0020-0011" num="0715">an instruction to evaluate operation use of automation of data <b>600</b>.</li>    </ul>    </li></ul></p><p id="p-0614" num="0716">The automation rating module <b>606</b> can rate the data <b>600</b> at three levels. The first level is that the system has automation, the system has the right number of automation, and/or the system has automation that address the right topics. The second level digs into the automation themselves to determine whether they are adequately covering the requirements of the system. The third level evaluates how well the automations are used and how well they are adhered to.</p><p id="p-0615" num="0717">The cumulative rating module <b>607</b> receives one or more process evaluation ratings, one or more policy evaluation ratings, one or more procedure evaluation ratings, one or more certification evaluation ratings, one or more documentation evaluation ratings, and/or one or more automation evaluation ratings. The cumulative rating module <b>607</b> may output the evaluation ratings it receives as the identification rating <b>608</b>. Alternatively, the cumulative rating module <b>607</b> performs a function (e.g., a weight average, standard deviation, statistical analysis, etc.) on the evaluation ratings it receives to produce the identification rating <b>608</b>.</p><p id="p-0616" num="0718"><figref idref="DRAWINGS">FIG. <b>83</b></figref> is a schematic block diagram of another embodiment of a data analysis module <b>252</b> that is similar to the data analysis module of <figref idref="DRAWINGS">FIG. <b>82</b></figref>. In this embodiment, the data analysis module <b>252</b> includes a data parsing module <b>609</b>, which parses the data <b>600</b> into process data, policy data, procedure data, certification data, documentation data, and/or automation data.</p><p id="p-0617" num="0719"><figref idref="DRAWINGS">FIG. <b>84</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular generating a process rating. The method begins at step <b>610</b> where the analysis system generates a first process rating based on a first combination of a system criteria (e.g., system requirements), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data).</p><p id="p-0618" num="0720">The method continues at step <b>611</b> where the analysis system generates a second process rating based on a second combination of a system criteria (e.g., system design), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data). The method continues at step <b>612</b> where the analysis system generates the process rating based on the first and second process ratings.</p><p id="p-0619" num="0721"><figref idref="DRAWINGS">FIG. <b>85</b></figref> is a logic diagram of a further example of generating a process rating for understanding of system build for assets in a department. The method begins at step <b>613</b> where the analysis system identifies processes regarding building of assets from the data. The method continues at step <b>614</b> where the analysis system generates a process rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0620" num="0722">The method also continues at step <b>615</b> where the analysis system determines use of the processes to build the assets. The method continues at step <b>616</b> where the analysis system generates a process rating based on use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0621" num="0723">The method also continues at step <b>617</b> where the analysis system determines consistency of applying the processes to build the assets. The method continues at step <b>618</b> where the analysis system generates a process rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>619</b> where the analysis system generates the process rating based on the process rating from the data, the process rating based on use, and the process rating based on consistency of use.</p><p id="p-0622" num="0724"><figref idref="DRAWINGS">FIG. <b>86</b></figref> is a logic diagram of a further example of generating a process rating for understanding of verifying system build for assets in a department. The method begins at step <b>620</b> where the analysis system identifies processes to verify the building of assets from the data. The method continues at step <b>621</b> where the analysis system generates a process rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0623" num="0725">The method also continues at step <b>622</b> where the analysis system determines use of the processes to verify the build the assets. The method continues at step <b>623</b> where the analysis system generates a process rating based on use of the verify processes. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0624" num="0726">The method also continues at step <b>624</b> where the analysis system determines consistency of applying the verifying processes to build the assets. The method continues at step <b>625</b> where the analysis system generates a process rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>626</b> where the analysis system generates the process rating based on the process rating from the data, the process rating based on use, and the process rating based on consistency of use.</p><p id="p-0625" num="0727"><figref idref="DRAWINGS">FIG. <b>87</b></figref> is a diagram of an example of identification data for use by an analysis system to generate an identification rating for a system, or portion thereof. The identification data includes a collection of data <b>600</b>, which includes disclosed data, discovered data, and/or desired data. For example, data <b>600</b> includes one or more design processes, one or more design tools, one or more verification tools, one or more verification processes, hardware, software, data flow design, resource prioritization implementation, implemented security roles, role in company supply chain, critical infrastructure, business priorities, critical services, resiliency requirements, one or more risk management processes, risk tolerance, remote access implementation, one or more remote access processes, permissions implementation, one or more permissions processes, network integrity implementation, one or more network integrity processes, user training materials, assigned user privileges, assigned third party responsibilities, assigned executive responsibilities, assigned IT &#x26; security responsibilities, one or more security policies, security implementation, one or more security processes, data at rest implementation, one or more data storage processes, asset management implemented plan, data leak prevention implementation, one or more data loss processes, data in transit implementation, one or more data transmit processes, adequate capacitary, integrity checking implementation, use &#x26; development separation plan, diagrams, design specs, installation notes, purchases, operating manuals, vulnerability detection implementation, one or more vulnerability detection processes, external source interaction, threat detection implementation, one or more threat detection processes, risk response implementation based on impact and risk level, one or more risk response processes, user information, maintenance records, other documents, risk tolerance tied to business environment (BE), and/or device information.</p><p id="p-0626" num="0728">The data <b>600</b> may further include one or more design policies, one or more verification policies, one or more risk management policies, one or more remote access policies, one or more permissions policies, one or more network integrity policies, one or more security policies, one or more security policies, one or more data storage policies, one or more data loss policies, one or more data transmit policies, one or more vulnerability detection policies, one or more threat detection policies, and/or one or more risk response policies.</p><p id="p-0627" num="0729">The data <b>600</b> may still further include one or more design procedures, one or more verification procedures, one or more risk management procedures, one or more remote access procedures, one or more permissions procedures, one or more network integrity procedures, one or more security procedures, one or more security procedures, one or more data storage procedures, one or more data loss procedures, one or more data transmit procedures, one or more vulnerability detection procedures, one or more threat detection procedures, and/or one or more risk response procedures.</p><p id="p-0628" num="0730">The data <b>600</b> may still further include one or more design documents, one or more verification documents, one or more risk management documents, one or more remote access documents, one or more permissions documents, one or more network integrity documents, one or more security documents, one or more security documents, one or more data storage documents, one or more data loss documents, one or more data transmit documents, one or more vulnerability detection documents, one or more threat detection documents, and/or one or more risk response documents.</p><p id="p-0629" num="0731">The data <b>600</b> may still further include one or more design certifications, one or more verification certifications, one or more risk management certifications, one or more remote access certifications, one or more permissions certifications, one or more network integrity certifications, one or more security certifications, one or more security certifications, one or more data storage certifications, one or more data loss certifications, one or more data transmit certifications, one or more vulnerability detection certifications, one or more threat detection certifications, and/or one or more risk response certifications.</p><p id="p-0630" num="0732">The data <b>600</b> may still further includes one or more design automations, one or more verification automations, one or more risk management automations, one or more remote access automations, one or more permissions automations, one or more network integrity automations, one or more security automations, one or more security automations, one or more data storage automations, one or more data loss automations, one or more data transmit automations, one or more vulnerability detection automations, one or more threat detection automations, and/or one or more risk response automations.</p><p id="p-0631" num="0733">In this example the blue shaded boxes (e.g., design processes, verification processes, etc.) are data that is directly relevant to the process rating module <b>601</b>. The light green shaded boxes (e.g., design tools, verification tools, etc.) are data that may be relevant to the process rating module <b>601</b>.</p><p id="p-0632" num="0734">In one embodiment, the process rating module <b>601</b> rates how well processes are used for identified tasks. For example, the process rating module <b>601</b> rates how well the design tools are used in accordance with the design processes.</p><p id="p-0633" num="0735">In another embodiment, the process rating module <b>601</b> rates the consistency of application of processes. For example, the process rating module rates the consistency of use of the design processes to use design tools to design hardware and software assets.</p><p id="p-0634" num="0736"><figref idref="DRAWINGS">FIG. <b>88</b></figref> is a logic diagram of an example of generating a process rating by the analysis system; in particular the process rating module generating a process rating based on data. The method begins at step <b>620</b> where the analysis system determines whether there is at least one process in the collection of data. Note that the threshold number in this step could be greater than one. If there are no processes, the method continues at step <b>631</b> where the analysis system generates a process rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0635" num="0737">If there is at least one process, the method continues at step <b>632</b> where the analysis system determines whether the processes are repeatable. In this instance, repeatable processes produce consistent results, include variations from process to process, are not routinely reviewed in an organized manner, and/or are not all regulated. For example, when the number of processes is below a desired number of processes, the analysis system determines that the processes are not repeatable (e.g., with too few processes cannot get repeatable outcomes). As another example, when the processes of the data <b>600</b> does not include one or more processes on a list of processes the system should have, the analysis system determines that the processes are not repeatable (e.g., with missing processes cannot get repeatable outcomes).</p><p id="p-0636" num="0738">If the processes are not repeatable, the method continues at step <b>633</b> where the analysis system generates a process rating of 10 (and/or a word rating of &#x201c;inconsistent&#x201d;). If, however, the processes are at least repeatable, the method continues at step <b>634</b> where the analysis system determines whether the processes are standardized. In this instance, standardized includes repeatable plus there are no appreciable variations in the processes from process to process, and/or the processes are regulated.</p><p id="p-0637" num="0739">If the processes are not standardized, the method continues at step <b>635</b> where the analysis system generates a process rating of 20 (and/or a word rating of &#x201c;repeatable&#x201d;). If, however, the processes are at least standardized, the method continues at step <b>636</b> where the analysis system determines whether the processes are measured. In this instance, measured includes standardized plus precise, exact, and/or calculated to specific needs, concerns, and/or functioning of the system.</p><p id="p-0638" num="0740">If the processes are not measured, the method continues at step <b>637</b> where the analysis system generates a process rating of 30 (and/or a word rating of &#x201c;standardized&#x201d;). If, however, the processes are at least measured, the method continues at step <b>638</b> where the analysis system determines whether the processes are optimized. In this instance, optimized includes measured plus processes are up-to-date and/or process improvement assessed on a regular basis as part of system protocols.</p><p id="p-0639" num="0741">If the processes are not optimized, the method continues at step <b>639</b> where the analysis system generates a process rating of 40 (and/or a word rating of &#x201c;measured&#x201d;). If the processes are optimized, the method continues at step <b>640</b> where the analysis system generates a process rating of 50 (and/or a word rating of &#x201c;optimized&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of process rating may be more or less than the six shown.</p><p id="p-0640" num="0742">For this method, distinguishing between repeatable, standardized, measured, and optimized is interpretative based on the manner in which the data <b>600</b> was analyzed. As an example, weighting factors on certain types of analysis affect the level. As a specific example, weighting factors for analysis to determine last revisions of processes, age of last revisions, content verification of processes with respect to a checklist, balance of local processes and system-wide processes, topic verification of the processes with respect to desired topics, and/or process language evaluation will affect the resulting level.</p><p id="p-0641" num="0743"><figref idref="DRAWINGS">FIG. <b>89</b></figref> is a logic diagram of an example of generating a process rating by the analysis system; in particular the process rating module generating a process rating based on use of processes. The method begins at step <b>641</b> where the analysis system determines whether at least one process in the collection of data has been used. Note that the threshold number in this step could be greater than one. If no processes have been used, the method continues at step <b>642</b> where the analysis system generates a process rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0642" num="0744">If at least one process is used, the method continues at step <b>643</b> where the analysis system determines whether the use of the processes is repeatable. In this instance, repeatable use of processes is consistent use, but with variations from process to process, use is not routinely reviewed or verified in an organized manner, and/or use is not regulated.</p><p id="p-0643" num="0745">If the use of processes is not repeatable, the method continues at step <b>644</b> where the analysis system generates a process rating of 10 (and/or a word rating of &#x201c;inconsistent&#x201d;). If, however, the use of processes is at least repeatable, the method continues at step <b>645</b> where the analysis system determines whether the use of processes is standardized. In this instance, standardized includes repeatable plus there are no appreciable variations in the use of processes from process to process, and/or the use of processes is regulated.</p><p id="p-0644" num="0746">If the use of processes is not standardized, the method continues at step <b>646</b> where the analysis system generates a process rating of 20 (and/or a word rating of &#x201c;repeatable&#x201d;). If, however, the use of processes is at least standardized, the method continues at step <b>647</b> where the analysis system determines whether the use of processes is measured. In this instance, measured includes standardized plus use is precise, exact, and/or calculated to specific needs, concerns, and/or functioning of the system.</p><p id="p-0645" num="0747">If the use of processes is not measured, the method continues at step <b>648</b> where the analysis system generates a process rating of 30 (and/or a word rating of &#x201c;standardized&#x201d;). If, however, the use of processes is at least measured, the method continues at step <b>649</b> where the analysis system determines whether the use of processes is optimized. In this instance, optimized includes measured plus use of processes are up-to-date and/or improving use of processes is assessed on a regular basis as part of system protocols.</p><p id="p-0646" num="0748">If the use of processes is not optimized, the method continues at step <b>650</b> where the analysis system generates a process rating of 40 (and/or a word rating of &#x201c;measured&#x201d;). If the use of processes is optimized, the method continues at step <b>651</b> where the analysis system generates a process rating of 50 (and/or a word rating of &#x201c;optimized&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of process rating may be more or less than the six shown.</p><p id="p-0647" num="0749"><figref idref="DRAWINGS">FIG. <b>90</b></figref> is a logic diagram an example of generating a process rating by the analysis system; in particular the process rating module generating a process rating based on consistency of application of processes. The method begins at step <b>652</b> where the analysis system determines whether at least one process in the collection of data has been consistently applied. Note that the threshold number in this step could be greater than one. If there no processes have been consistently applied, the method continues at step <b>653</b> where the analysis system generates a process rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0648" num="0750">If at least one process has been consistently applied, the method continues at step <b>654</b> where the analysis system determines whether the consistent application of processes is repeatable. In this instance, repeatable consistency of application of processes is a process is consistently applied for a given circumstance of the system (e.g., determining software applications for like devices in a department), but with variations from process to process, application of processes is not routinely reviewed or verified in an organized manner, and/or application of processes is not regulated.</p><p id="p-0649" num="0751">If the consistency of application of processes is not repeatable, the method continues at step <b>655</b> where the analysis system generates a process rating of 10 (and/or a word rating of &#x201c;inconsistent&#x201d;). If, however, the consistency of application of processes is at least repeatable, the method continues at step <b>656</b> where the analysis system determines whether the consistency of application of processes is standardized. In this instance, standardized includes repeatable plus there are no appreciable variations in the application of processes from process to process, and/or the application of processes is regulated.</p><p id="p-0650" num="0752">If the consistency of application of processes is not standardized, the method continues at step <b>657</b> where the analysis system generates a process rating of 20 (and/or a word rating of &#x201c;repeatable&#x201d;). If, however, the consistency of application of processes is at least standardized, the method continues at step <b>658</b> where the analysis system determines whether the consistency of application of processes is measured. In this instance, measured includes standardized plus application of processes is precise, exact, and/or calculated to specific needs, concerns, and/or functioning of the system.</p><p id="p-0651" num="0753">If the consistency of application of processes is not measured, the method continues at step <b>659</b> where the analysis system generates a process rating of 30 (and/or a word rating of &#x201c;standardized&#x201d;). If, however, the consistency of application of processes is at least measured, the method continues at step <b>660</b> where the analysis system determines whether the consistency of application of processes is optimized. In this instance, optimized includes measured plus application of processes is up-to-date and/or improving application of processes is assessed on a regular basis as part of system protocols.</p><p id="p-0652" num="0754">If the consistency of application of processes is not optimized, the method continues at step <b>661</b> where the analysis system generates a process rating of 40 (and/or a word rating of &#x201c;measured&#x201d;). If the consistency of application of processes is optimized, the method continues at step <b>662</b> where the analysis system generates a process rating of 50 (and/or a word rating of &#x201c;optimized&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of process rating may be more or less than the six shown.</p><p id="p-0653" num="0755"><figref idref="DRAWINGS">FIG. <b>91</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular generating a policy rating. The method begins at step <b>670</b> where the analysis system generates a first policy rating based on a first combination of a system criteria (e.g., system requirements), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data).</p><p id="p-0654" num="0756">The method continues at step <b>671</b> where the analysis system generates a second policy rating based on a second combination of a system criteria (e.g., system design), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data). The method continues at step <b>672</b> where the analysis system generates the policy rating based on the first and second policy ratings.</p><p id="p-0655" num="0757"><figref idref="DRAWINGS">FIG. <b>92</b></figref> is a logic diagram of a further example of generating a policy rating for understanding of system build for assets in a department. The method begins at step <b>673</b> where the analysis system identifies policies regarding building of assets from the data. The method continues at step <b>674</b> where the analysis system generates a policy rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0656" num="0758">The method also continues at step <b>675</b> where the analysis system determines use of the policies to build the assets. The method continues at step <b>676</b> where the analysis system generates a policy rating based on use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0657" num="0759">The method also continues at step <b>677</b> where the analysis system determines consistency of applying the policies to build the assets. The method continues at step <b>678</b> where the analysis system generates a policy rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>679</b> where the analysis system generates the policy rating based on the policy rating from the data, the policy rating based on use, and the policy rating based on consistency of use.</p><p id="p-0658" num="0760"><figref idref="DRAWINGS">FIG. <b>93</b></figref> is a logic diagram of a further example of generating a policy rating for understanding of verifying system build for assets in a department. The method begins at step <b>680</b> where the analysis system identifies policies to verify the building of assets from the data. The method continues at step <b>681</b> where the analysis system generates a policy rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0659" num="0761">The method also continues at step <b>682</b> where the analysis system determines use of the policies to verify the build the assets. The method continues at step <b>683</b> where the analysis system generates a policy rating based on use of the verify policies. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0660" num="0762">The method also continues at step <b>684</b> where the analysis system determines consistency of applying the verifying policies to build the assets. The method continues at step <b>685</b> where the analysis system generates a policy rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>686</b> where the analysis system generates the policy rating based on the policy rating from the data, the policy rating based on use, and the policy rating based on consistency of use.</p><p id="p-0661" num="0763"><figref idref="DRAWINGS">FIG. <b>94</b></figref> is a logic diagram of an example of generating a policy rating by the analysis system; in particular the policy rating module generating a policy rating based on data <b>600</b>. The method begins at step <b>687</b> where the analysis system determines whether there is at least one policy in the collection of data. Note that the threshold number in this step could be greater than one. If there are no policies, the method continues at step <b>688</b> where the analysis system generates a policy rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0662" num="0764">If there is at least one policy, the method continues at step <b>689</b> where the analysis system determines whether the policies are defined. In this instance, defined policies include sufficient detail to produce consistent results, include variations from policy to policy, are not routinely reviewed in an organized manner, and/or are not all regulated. For example, when the number of policies is below a desired number of policies, the analysis system determines that the processes are not repeatable (e.g., with too few policies cannot get repeatable outcomes). As another example, when the policies of the data <b>600</b> does not include one or more policies on a list of policies the system should have, the analysis system determines that the policies are not repeatable (e.g., with missing policies cannot get repeatable outcomes).</p><p id="p-0663" num="0765">If the policies are not defined, the method continues at step <b>690</b> where the analysis system generates a policy rating of 5 (and/or a word rating of &#x201c;informal&#x201d;). If, however, the policies are at least defined, the method continues at step <b>691</b> where the analysis system determines whether the policies are audited. In this instance, audited includes defined plus the policies are routinely reviewed, and/or the policies are regulated.</p><p id="p-0664" num="0766">If the policies are not audited, the method continues at step <b>692</b> where the analysis system generates a policy rating of 10 (and/or a word rating of &#x201c;defined&#x201d;). If, however, the policies are at least audited, the method continues at step <b>693</b> where the analysis system determines whether the policies are embedded. In this instance, embedded includes audited plus are systematically rooted in most, if not all, aspects of the system.</p><p id="p-0665" num="0767">If the policies are not embedded, the method continues at step <b>694</b> where the analysis system generates a policy rating of 15 (and/or a word rating of &#x201c;audited&#x201d;). If the policies are embedded, the method continues at step <b>695</b> where the analysis system generates a policy rating of 20 (and/or a word rating of &#x201c;embedded&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of policy rating may be more or less than the five shown.</p><p id="p-0666" num="0768">For this method, distinguishing between defined, audited, and embedded is interpretative based on the manner in which the data <b>600</b> was analyzed. As an example, weighting factors on certain types of analysis affect the level. As a specific example, weighting factors for analysis to determine last revisions of policies, age of last revisions, content verification of policies with respect to a checklist, balance of local policies and system-wide policies, topic verification of the policies with respect to desired topics, and/or policy language evaluation will affect the resulting level.</p><p id="p-0667" num="0769"><figref idref="DRAWINGS">FIG. <b>95</b></figref> is a logic diagram of an example of generating a policy rating by the analysis system; in particular the policy rating module generating a policy rating based on use of the polices. The method begins at step <b>696</b> where the analysis system determines whether there is at least one use of a policy. Note that the threshold number in this step could be greater than one. If there are no uses of policies, the method continues at step <b>697</b> where the analysis system generates a policy rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0668" num="0770">If there is at least one use of a policy, the method continues at step <b>698</b> where the analysis system determines whether the use of policies is defined. In this instance, defined use of policies include sufficient detail on how and/or when to use a policy, include variations in use from policy to policy, use of policies is not routinely reviewed in an organized manner, and/or use of policies is not regulated.</p><p id="p-0669" num="0771">If the use of policies is not defined, the method continues at step <b>699</b> where the analysis system generates a policy rating of 5 (and/or a word rating of &#x201c;informal&#x201d;). If, however, the use of policies is at least defined, the method continues at step <b>700</b> where the analysis system determines whether the use of policies is audited. In this instance, audited includes defined plus the use of policies is routinely reviewed, and/or the use of policies is regulated.</p><p id="p-0670" num="0772">If the use of policies is not audited, the method continues at step <b>701</b> where the analysis system generates a policy rating of 10 (and/or a word rating of &#x201c;defined&#x201d;). If, however, the use of policies is at least audited, the method continues at step <b>702</b> where the analysis system determines whether the use of policies is embedded. In this instance, embedded includes audited plus use of policies is systematically rooted in most, if not all, aspects of the system.</p><p id="p-0671" num="0773">If the use of policies is not embedded, the method continues at step <b>703</b> where the analysis system generates a policy rating of 15 (and/or a word rating of &#x201c;audited&#x201d;). If the use of policies is embedded, the method continues at step <b>704</b> where the analysis system generates a policy rating of 20 (and/or a word rating of &#x201c;embedded&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of policy rating may be more or less than the five shown.</p><p id="p-0672" num="0774"><figref idref="DRAWINGS">FIG. <b>96</b></figref> is a logic diagram of an example of generating a policy rating by the analysis system; in particular the policy rating module generating a policy rating based on consistent application of polices. The method begins at step <b>705</b> where the analysis system determines whether there is at least one consistent application of a policy. Note that the threshold number in this step could be greater than one. If there are no consistent application of policies, the method continues at step <b>706</b> where the analysis system generates a policy rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0673" num="0775">If there is at least one consistent application of a policy, the method continues at step <b>707</b> where the analysis system determines whether the consistent application of policies is defined. In this instance, defined application of policies include sufficient detail on when policies apply, includes application variations from policy to policy, application of policies is not routinely reviewed in an organized manner, and/or application of policies is not regulated.</p><p id="p-0674" num="0776">If the application of policies is not defined, the method continues at step <b>708</b> where the analysis system generates a policy rating of 5 (and/or a word rating of &#x201c;informal&#x201d;). If, however, the application of policies is at least defined, the method continues at step <b>707</b> where the analysis system determines whether the application of policies is audited. In this instance, audited includes defined plus the application of policies is routinely reviewed, and/or the application of policies is regulated.</p><p id="p-0675" num="0777">If the application of policies is not audited, the method continues at step <b>710</b> where the analysis system generates a policy rating of 10 (and/or a word rating of &#x201c;defined&#x201d;). If, however, the application of policies is at least audited, the method continues at step <b>711</b> where the analysis system determines whether the application of policies is embedded. In this instance, embedded includes audited plus application of policies is systematically rooted in most, if not all, aspects of the system.</p><p id="p-0676" num="0778">If the application of policies is not embedded, the method continues at step <b>712</b> where the analysis system generates a policy rating of 15 (and/or a word rating of &#x201c;audited&#x201d;). If the application of policies is embedded, the method continues at step <b>713</b> where the analysis system generates a policy rating of 20 (and/or a word rating of &#x201c;embedded&#x201d;). Note that the numerical ratings are example values and could be other values. Further note that the number of level of policies may be more or less than the five shown.</p><p id="p-0677" num="0779"><figref idref="DRAWINGS">FIG. <b>97</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular generating a documentation rating. The method begins at step <b>720</b> where the analysis system generates a first documentation rating based on a first combination of a system criteria (e.g., system requirements), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data).</p><p id="p-0678" num="0780">The method continues at step <b>721</b> where the analysis system generates a second documentation rating based on a second combination of a system criteria (e.g., system design), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data). The method continues at step <b>722</b> where the analysis system generates the documentation rating based on the first and second documentation ratings.</p><p id="p-0679" num="0781"><figref idref="DRAWINGS">FIG. <b>98</b></figref> is a logic diagram of a further example of generating a documentation rating for understanding of system build for assets in a department. The method begins at step <b>723</b> where the analysis system identifies documentation regarding building of assets from the data. The method continues at step <b>724</b> where the analysis system generates a documentation rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0680" num="0782">The method also continues at step <b>725</b> where the analysis system determines use of the documentation to build the assets. The method continues at step <b>726</b> where the analysis system generates a documentation rating based on use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0681" num="0783">The method also continues at step <b>727</b> where the analysis system determines consistency of applying the documentation to build the assets. The method continues at step <b>728</b> where the analysis system generates a documentation rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>729</b> where the analysis system generates the documentation rating based on the documentation rating from the data, the documentation rating based on use, and the documentation rating based on consistency of use.</p><p id="p-0682" num="0784"><figref idref="DRAWINGS">FIG. <b>99</b></figref> is a logic diagram of a further example of generating a documentation rating for understanding of verifying system build for assets in a department. The method begins at step <b>730</b> where the analysis system identifies documentation to verify the building of assets from the data. The method continues at step <b>731</b> where the analysis system generates a documentation rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0683" num="0785">The method also continues at step <b>732</b> where the analysis system determines use of the documentation to verify the build the assets. The method continues at step <b>733</b> where the analysis system generates a documentation rating based on use of the verify documentation. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0684" num="0786">The method also continues at step <b>734</b> where the analysis system determines consistency of applying the verifying documentation to build the assets. The method continues at step <b>735</b> where the analysis system generates a documentation rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>736</b> where the analysis system generates the documentation rating based on the documentation rating from the data, the documentation rating based on use, and the documentation rating based on consistency of use.</p><p id="p-0685" num="0787"><figref idref="DRAWINGS">FIG. <b>100</b></figref> is a logic diagram of an example of generating a documentation rating by the analysis system; in particular the documentation rating module generating a documentation rating based on data <b>600</b>. The method begins at step <b>737</b> where the analysis system determines whether there is at least one document in the collection of data. Note that the threshold number in this step could be greater than one. If there are no documents, the method continues at step <b>738</b> where the analysis system generates a documentation rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0686" num="0788">If there is at least one document, the method continues at step <b>739</b> where the analysis system determines whether the documents are formalized. In this instance, formalized documents include sufficient detail to produce consistent documentation, include form variations from document to document, are not routinely reviewed in an organized manner, and/or formation of documents is not regulated.</p><p id="p-0687" num="0789">If the documents are not formalized, the method continues at step <b>740</b> where the analysis system generates a documentation rating of 5 (and/or a word rating of &#x201c;informal&#x201d;). If, however, the documents are at least formalized, the method continues at step <b>741</b> where the analysis system determines whether the documents are metric &#x26; reporting. In this instance, metric &#x26; reporting includes formal plus the documents are routinely reviewed, and/or the formation of documents is regulated.</p><p id="p-0688" num="0790">If the documents are not metric &#x26; reporting, the method continues at step <b>742</b> where the analysis system generates a documentation rating of 10 (and/or a word rating of &#x201c;formal&#x201d;). If, however, the documents are at least metric &#x26; reporting, the method continues at step <b>743</b> where the analysis system determines whether the documents are improve. In this instance, improve includes audited plus document formation is systematically rooted in most, if not all, aspects of the system.</p><p id="p-0689" num="0791">If the documents are not improve, the method continues at step <b>744</b> where the analysis system generates a documentation rating of 15 (and/or a word rating of &#x201c;metric &#x26; reporting&#x201d;). If the documents are improve, the method continues at step <b>745</b> where the analysis system generates a documentation rating of 20 (and/or a word rating of &#x201c;improvement&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of documentation rating may be more or less than the five shown.</p><p id="p-0690" num="0792">For this method, distinguishing between formalized, metric &#x26; reporting, and improvement is interpretative based on the manner in which the data <b>600</b> was analyzed. As an example, weighting factors on certain types of analysis affect the level. As a specific example, weighting factors for analysis to determine last revisions of documents, age of last revisions, content verification of documents with respect to a checklist, balance of local documents and system-wide documents, topic verification of the documents with respect to desired topics, and/or document language evaluation will affect the resulting level.</p><p id="p-0691" num="0793"><figref idref="DRAWINGS">FIG. <b>101</b></figref> is a logic diagram of an example of generating a documentation rating by the analysis system; in particular the documentation rating module generating a documentation rating based on use of documents. The method begins at step <b>746</b> where the analysis system determines whether there is at least one use of a document. Note that the threshold number in this step could be greater than one. If there are no use of documents, the method continues at step <b>747</b> where the analysis system generates a documentation rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0692" num="0794">If there is at least one use of a document, the method continues at step <b>748</b> where the analysis system determines whether the use of the documents is formalized. In this instance, formalized use of documents include sufficient detail regarding how to use the documentation, include use variations from document to document, use of documents is not routinely reviewed in an organized manner, and/or use of documents is not regulated.</p><p id="p-0693" num="0795">If the use of documents is not formalized, the method continues at step <b>749</b> where the analysis system generates a documentation rating of 5 (and/or a word rating of &#x201c;informal&#x201d;). If, however, the use of documents is at least formalized, the method continues at step <b>750</b> where the analysis system determines whether the use of the documents is metric &#x26; reporting. In this instance, metric &#x26; reporting includes formal plus use of documents is routinely reviewed, and/or the use of documents is regulated.</p><p id="p-0694" num="0796">If the use of documents is not metric &#x26; reporting, the method continues at step <b>751</b> where the analysis system generates a documentation rating of 10 (and/or a word rating of &#x201c;formal&#x201d;). If, however, the use of documents is at least metric &#x26; reporting, the method continues at step <b>752</b> where the analysis system determines whether the use of documents is improve. In this instance, improve includes metric &#x26; reporting plus use of document is systematically rooted in most, if not all, aspects of the system.</p><p id="p-0695" num="0797">If the use of documents is not improve, the method continues at step <b>753</b> where the analysis system generates a documentation rating of 15 (and/or a word rating of &#x201c;metric &#x26; reporting&#x201d;). If the use of documents is improve, the method continues at step <b>754</b> where the analysis system generates a documentation rating of 20 (and/or a word rating of &#x201c;improvement&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of documentation rating may be more or less than the five shown.</p><p id="p-0696" num="0798"><figref idref="DRAWINGS">FIG. <b>102</b></figref> is a logic diagram of an example of generating a documentation rating by the analysis system; in particular the documentation rating module generating a documentation rating based on application of documents. The method begins at step <b>755</b> where the analysis system determines whether there is at least one application of a document. Note that the threshold number in this step could be greater than one. If there are no applications of documents, the method continues at step <b>756</b> where the analysis system generates a documentation rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0697" num="0799">If there is at least one application of a document, the method continues at step <b>757</b> where the analysis system determines whether the application of the documents is formalized. In this instance, formalized application of documents include sufficient detail regarding how to apply the documentation, include application variations from document to document, application of documents is not routinely reviewed in an organized manner, and/or application of documents is not regulated.</p><p id="p-0698" num="0800">If the application of documents is not formalized, the method continues at step <b>758</b> where the analysis system generates a documentation rating of 5 (and/or a word rating of &#x201c;informal&#x201d;). If, however, the application of documents is at least formalized, the method continues at step <b>759</b> where the analysis system determines whether the application of the documents is metric &#x26; reporting. In this instance, metric &#x26; reporting includes formal plus application of documents is routinely reviewed, and/or the application of documents is regulated.</p><p id="p-0699" num="0801">If the application of documents is not metric &#x26; reporting, the method continues at step <b>760</b> where the analysis system generates a documentation rating of 10 (and/or a word rating of &#x201c;formal&#x201d;). If, however, the application of documents is at least metric &#x26; reporting, the method continues at step <b>761</b> where the analysis system determines whether the application of documents is improve. In this instance, improve includes metric &#x26; reporting plus use of document is systematically rooted in most, if not all, aspects of the system.</p><p id="p-0700" num="0802">If the application of documents is not improve, the method continues at step <b>762</b> where the analysis system generates a documentation rating of 15 (and/or a word rating of &#x201c;metric &#x26; reporting&#x201d;). If the application of documents is improve, the method continues at step <b>763</b> where the analysis system generates a documentation rating of 20 (and/or a word rating of &#x201c;improvement&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of documentation may be more or less than the five shown.</p><p id="p-0701" num="0803"><figref idref="DRAWINGS">FIG. <b>103</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular generating an automation rating. The method begins at step <b>764</b> where the analysis system generates a first automation rating based on a first combination of a system criteria (e.g., system requirements), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data).</p><p id="p-0702" num="0804">The method continues at step <b>765</b> where the analysis system generates a second automation rating based on a second combination of a system criteria (e.g., system design), of a system mode (e.g., system functions), of an evaluation perspective (e.g., implementation), and of an evaluation viewpoint (e.g., disclosed data). The method continues at step <b>766</b> where the analysis system generates the automation rating based on the first and second automation ratings.</p><p id="p-0703" num="0805"><figref idref="DRAWINGS">FIG. <b>104</b></figref> is a logic diagram of a further example of generating an automation rating for understanding of system build for assets in a department. The method begins at step <b>767</b> where the analysis system identifies automation regarding building of assets from the data. The method continues at step <b>768</b> where the analysis system generates an automation rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0704" num="0806">The method also continues at step <b>769</b> where the analysis system determines use of the automation to build the assets. The method continues at step <b>770</b> where the analysis system generates an automation rating based on use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0705" num="0807">The method also continues at step <b>771</b> where the analysis system determines consistency of applying the automation to build the assets. The method continues at step <b>772</b> where the analysis system generates an automation rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>773</b> where the analysis system generates the automation rating based on the automation rating from the data, the automation rating based on use, and the automation rating based on application (i.e., consistency of use).</p><p id="p-0706" num="0808"><figref idref="DRAWINGS">FIG. <b>105</b></figref> is a logic diagram of a further example of generating an automation rating for understanding of verifying system build for assets in a department. The method begins at step <b>774</b> where the analysis system identifies automation to verify the building of assets from the data. The method continues at step <b>775</b> where the analysis system generates an automation rating from the data. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0707" num="0809">The method also continues at step <b>776</b> where the analysis system determines use of the automation to verify the build the assets. The method continues at step <b>777</b> where the analysis system generates an automation rating based on use of the verify automation. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>.</p><p id="p-0708" num="0810">The method also continues at step <b>778</b> where the analysis system determines consistency of applying the verifying automation to build the assets. The method continues at step <b>779</b> where the analysis system generates an automation rating based on consistency of use. Examples of this were discussed with reference to <figref idref="DRAWINGS">FIG. <b>82</b></figref>. The method continues at step <b>780</b> where the analysis system generates the automation rating based on the automation rating from the data, the automation rating based on use, and the automation rating based on consistency of use.</p><p id="p-0709" num="0811"><figref idref="DRAWINGS">FIG. <b>106</b></figref> is a logic diagram of an example of generating an automation rating by the analysis system; in particular the automation rating module generating an automation rating based on data <b>600</b>. The method begins at step <b>781</b> where the analysis system determines whether there is available automation for a particular system aspect, system criteria, and/or system mode. If automation is not available, the method continues at step <b>782</b> where the analysis system generates an automation rating of 10 (and/or a word rating of &#x201c;unavailable&#x201d;).</p><p id="p-0710" num="0812">If automation is available, the method continues at step <b>783</b> where the analysis system determines whether there is at least one automation in the data. If not, the method continues at step <b>784</b> where the analysis system generates an automation rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0711" num="0813">If there is at least one automation, the method continues at step <b>785</b> where the analysis system determines whether full automation is found in the data. In this instance, full automation refers to the automation techniques that are available for the system are in the data <b>600</b>.</p><p id="p-0712" num="0814">If the automation is not full, the method continues at step <b>786</b> where the analysis system generates an automation rating of 5 (and/or a word rating of &#x201c;partial&#x201d;). If, however, the automation is full, the method continues at step <b>787</b> where the analysis system generates an automation rating of 10 (and/or a word rating of &#x201c;full&#x201d;). Note that the numerical rating are example values and could be other values. Further note that the number of level of automation may be more or less than the four shown.</p><p id="p-0713" num="0815"><figref idref="DRAWINGS">FIG. <b>107</b></figref> is a logic diagram of an example of generating an automation rating by the analysis system; in particular the automation rating module generating an automation rating based on use. The method begins at step <b>788</b> where the analysis system determines whether there is available automation for a particular system aspect, system criteria, and/or system mode. If automation is not available, the method continues at step <b>789</b> where the analysis system generates an automation rating of 10 (and/or a word rating of &#x201c;unavailable&#x201d;).</p><p id="p-0714" num="0816">If automation is available, the method continues at step <b>790</b> where the analysis system determines whether there is at least one use of automation. If not, the method continues at step <b>791</b> where the analysis system generates an automation rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0715" num="0817">If there is at least one use of automation, the method continues at step <b>792</b> where the analysis system determines whether automation is fully used. In this instance, full use of automation refers to the automation techniques that the system has are fully used.</p><p id="p-0716" num="0818">If the use of automation is not full, the method continues at step <b>793</b> where the analysis system generates an automation rating of 5 (and/or a word rating of &#x201c;partial&#x201d;). If, however, the use of automation is full, the method continues at step <b>794</b> where the analysis system generates an automation rating of 10 (and/or a word rating of &#x201c;full&#x201d;).</p><p id="p-0717" num="0819"><figref idref="DRAWINGS">FIG. <b>108</b></figref> is a logic diagram of an example of generating an automation rating by the analysis system; in particular the automation rating module generating an automation rating based on application of automation. The method begins at step <b>795</b> where the analysis system determines whether there is available automation for a particular system aspect, system criteria, and/or system mode. If automation is not available, the method continues at step <b>796</b> where the analysis system generates an automation rating of 10 (and/or a word rating of &#x201c;unavailable&#x201d;).</p><p id="p-0718" num="0820">If automation is available, the method continues at step <b>790</b> where the analysis system determines whether there is at least one application of automation. If not, the method continues at step <b>798</b> where the analysis system generates an automation rating of 0 (and/or a word rating of &#x201c;none&#x201d;).</p><p id="p-0719" num="0821">If there is at least one application of automation, the method continues at step <b>799</b> where the analysis system determines whether automation is fully applied. In this instance, full application of automation refers to the automation techniques of the system are applied to achieve consistent use.</p><p id="p-0720" num="0822">If the application of automation is not full, the method continues at step <b>800</b> where the analysis system generates an automation rating of 5 (and/or a word rating of &#x201c;partial&#x201d;). If, however, the application of automation is full, the method continues at step <b>801</b> where the analysis system generates an automation rating of 10 (and/or a word rating of &#x201c;full&#x201d;).</p><p id="p-0721" num="0823"><figref idref="DRAWINGS">FIG. <b>109</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof; in particular the analysis system identifying system elements. The method begins at step <b>810</b> where the analysis system activates at least one detection tool (e.g., cybersecurity tool, end point tool, network tool, IP address tool, hardware detection tool, software detection tool, etc.) based on the system aspect.</p><p id="p-0722" num="0824">The method continues at step <b>811</b> where the analysis system determines whether an identified system element has already been identified for the system aspect (e.g., is already in the collection of data <b>600</b> and/or is part of the gathered data). If yes, the method continues at step <b>812</b> where the analysis system determines whether the identifying of system elements is done. If not, the method repeats at step <b>811</b>. If the identifying of system elements is done, the method continues at step <b>813</b> where the analysis system determines whether to end the method or repeat it for another system aspect, or portion thereof.</p><p id="p-0723" num="0825">If, at step <b>811</b>, the identified system is element is not included in the collection of data, the method continues at step <b>814</b> where the analysis system determines whether the potential system element is already identified as being a part of the system aspect, but not included in the collection of data <b>600</b> (e.g., is it cataloged as being part of the system?). If yes, the method continues at step <b>815</b> where the analysis system adds the identified system element to the collection of data <b>600</b>.</p><p id="p-0724" num="0826">If, at step <b>814</b>, the system element is not cataloged as being part of the system, the method continues at step <b>816</b> where the analysis system obtains data regarding the potential system element. For example, the analysis system obtains a device ID, a user ID, a device serial number, a device description, a software ID, a software serial number, a software description, vendor information and/or other data regarding the system element.</p><p id="p-0725" num="0827">The method continues at step <b>816</b> where the analysis system verifies the potential system element based on the data. For example, the analysis system verifies one or more of a device ID, a user ID, a device serial number, a device description, a software ID, a software serial number, a software description, vendor information and/or other data regarding the system element to establish that the system element is a part of the system. When the potential system element is verified, the method continues at step <b>818</b> where the analysis system adds the system element as a part of the system aspect (e.g., catalogs it as part of the system and/or adds it to the collection of data <b>600</b>).</p><p id="p-0726" num="0828"><figref idref="DRAWINGS">FIG. <b>110</b></figref> is a diagram of an example of system aspects, evaluation aspects, evaluation rating metrics, and analysis system output options of an analysis system <b>11</b> for analyzing a system <b>11</b>, or portion thereof. For instance, analysis system <b>11</b> is evaluating, with respect to process, policy, procedure, certification, documentation, and/or automation, the understanding and implementation of the guidelines, system requirements, system design, and/or system build for identifying assets of an engineering department based on disclosed data and discovered data to produce an evaluation rating.</p><p id="p-0727" num="0829">For this example, the analysis system <b>10</b> can generate one or a plurality of identification evaluation ratings for implementation of the guidelines, system requirements, system design, and/or system build for identifying assets of an engineering department based on disclosed data and discovered data in accordance with the evaluation rating metrics of process, policy, procedure, certification, documentation, and/or automation. A few, but far from exhaustive, examples are shown in <figref idref="DRAWINGS">FIGS. <b>111</b>-<b>115</b></figref>.</p><p id="p-0728" num="0830"><figref idref="DRAWINGS">FIG. <b>111</b></figref> is a diagram of an example of producing a plurality of identification ratings and combining them into one rating. In this example, sixteen individual ratings are generated and then, via the cumulative rating module <b>607</b>, are combined into one identification rating <b>608</b>. A first individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and disclosed data. A second individual identification rating is generated from a combination of engineering department, guidelines, assets, implementation, and disclosed data. A third individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and discovered data. A fourth individual identification rating is generated from a combination of engineering department, guidelines, assets, implementation, and discovered data. The remaining twelve individual identification ratings are generated from the combinations shown.</p><p id="p-0729" num="0831"><figref idref="DRAWINGS">FIG. <b>112</b></figref> is a diagram of another example of producing a plurality of identification ratings and combining them into one rating. In this example, two individual ratings are generated and then, via the cumulative rating module <b>607</b>, are combined into one identification rating <b>608</b>. A first individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and disclosed data. A second individual identification rating is generated from a combination of engineering department, guidelines, assets, implementation, and disclosed data. This allows for a comparison between the understanding of the assets of the engineering department of the guidelines from the disclosed data and the implementation of the assets of the engineering department of the guidelines from the disclosed data. This comparison provides a metric for determining how well the guidelines were understood and how well they were used and/or applied.</p><p id="p-0730" num="0832"><figref idref="DRAWINGS">FIG. <b>113</b></figref> is a diagram of another example of producing a plurality of identification ratings and combining them into one rating. In this example, two individual ratings are generated and then, via the cumulative rating module <b>607</b>, are combined into one identification rating <b>608</b>. A first individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and disclosed data. A second individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and discovered data. This allows for a comparison between the understanding of the assets of the engineering department of the guidelines from the disclosed data and the understanding of the assets of the engineering department of the guidelines from the discovered data. This comparison provides a metric for determining how well the guidelines were believed to be understood and how well they were actually understood.</p><p id="p-0731" num="0833"><figref idref="DRAWINGS">FIG. <b>114</b></figref> is a diagram of another example of producing a plurality of identification ratings and combining them into one rating. In this example, two individual ratings are generated and then, via the cumulative rating module <b>607</b>, are combined into one identification rating <b>608</b>. A first individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and disclosed data. A second individual identification rating is generated from a combination of engineering department, system requirements, assets, understanding, and disclosed data. This allows for a comparison between the understanding of the assets of the engineering department of the guidelines from the disclosed data and the understanding of the assets of the engineering department of the system requirements from the disclosed data. This comparison provides a metric for determining how well the guidelines were converted into the system requirements.</p><p id="p-0732" num="0834"><figref idref="DRAWINGS">FIG. <b>115</b></figref> is a diagram of another example of producing a plurality of identification ratings and combining them into one rating. In this example, four individual ratings are generated and then, via the cumulative rating module <b>607</b>, are combined into one identification rating <b>608</b>. A first individual identification rating is generated from a combination of engineering department, guidelines, assets, understanding, and disclosed data. A second individual identification rating is generated from a combination of engineering department, system requirements, assets, understanding, and disclosed data. A third individual identification rating is generated from a combination of engineering department, system design, assets, understanding, and disclosed data. A fourth individual identification rating is generated from a combination of engineering department, system build, assets, understanding, and disclosed data.</p><p id="p-0733" num="0835">This allows for a comparison between the understanding of the assets of the engineering department of the guidelines from the disclosed data, the understanding of the assets of the engineering department of the system requirements from the disclosed data, the understanding of the assets of the engineering department of the system design from the disclosed data, and the understanding of the assets of the engineering department of the system build from the disclosed data. This comparison provides a metric for determining how well the guidelines, system requirements, system design, and/or system build were understood with respect to each and how well they were used and/or applied.</p><p id="p-0734" num="0836"><figref idref="DRAWINGS">FIG. <b>116</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof, and determining deficiencies. The method includes one or more of steps <b>820</b>-<b>823</b>. At step <b>820</b>, the analysis system determines a system criteria deficiency (e.g., guidelines, system requirements, system design, system build, and/or resulting system) of the system aspect based on the identification rating and the identification data. Examples have been discussed with reference to one or more preceding figures.</p><p id="p-0735" num="0837">At step <b>821</b>, the analysis system determines a system mode deficiency (e.g., assets, system functions, and/or security functions) of the system aspect based on the identification rating and the identification data. At step <b>822</b>, the analysis system determines an evaluation perspective deficiency (e.g., understanding, implementation, operation, and/or self-analysis) of the system aspect based on the identification rating and the identification data. At step <b>823</b>, the analysis system determines an evaluation viewpoint deficiency (e.g., disclosed, discovered, and/or desired) of the system aspect based on the identification rating and the identification data. Examples have been discussed with reference to one or more preceding figures.</p><p id="p-0736" num="0838"><figref idref="DRAWINGS">FIG. <b>117</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof, and determining auto-corrections. The method begins at step <b>824</b> where the analysis system determines a deficiency of the system aspect based on the identification rating and/or the identification data as discussed with reference to <figref idref="DRAWINGS">FIG. <b>116</b></figref>. The method continues at step <b>825</b> where the analysis system determines whether the deficiency is auto-correctable. For example, is the deficiency regarding software and if so, can it be auto-corrected.</p><p id="p-0737" num="0839">If the deficiency is not auto-correctable, the method continues at step <b>826</b> where the analysis system includes the identified deficiency in a report. If, however, the deficiency is auto-correctable, the method continues at step <b>827</b> where the analysis system auto-corrects the deficiency. The method continues at step <b>828</b> where the analysis system includes the identified deficiency and auto-correction in a report. Examples of auto-correction have been discussed with reference to one or more preceding Figures.</p><p id="p-0738" num="0840"><figref idref="DRAWINGS">FIG. <b>118</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>830</b> where the analysis system selects a system, or portion thereof, to evaluate organizational awareness of the system, or portion thereof, with respect to assets, system functions, and/or security functions. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective, one or more evaluation viewpoints, and/or identify as the for the evaluation category. The analysis system may further select one or more identify sub-categories and/or one or more sub-sub categories.</p><p id="p-0739" num="0841">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical assets and/or one or more conceptual assets.</p><p id="p-0740" num="0842">The method continues at step <b>831</b> where the analysis system obtains organization awareness information regarding the system, or portion thereof. The organization awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the assets, the system functions, and/or the security functions. In an example, the analysis system obtains the organization awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0741" num="0843">The method continues at step <b>832</b> where the analysis system engages with the system, or portion thereof, to produce system awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the assets, the system functions, and/or the security functions. Engaging the system, or portion thereof, will be discussed in greater detail with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0742" num="0844">The method continues at step <b>833</b> where the analysis system calculates an awareness rating regarding the organizational awareness of the system, or portion thereof, based on the organization awareness information, the system awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the awareness rating indicates how well the organization awareness information reflects an understanding of the assets of the system, or portion thereof. As another example, the awareness rating indicates how well the organization awareness information reflects an understanding of the system functions of system, or portion thereof.</p><p id="p-0743" num="0845">As another example, the awareness rating indicates how well the organization awareness information reflects an understanding of the security functions of the system, or portion thereof. As another example, the awareness rating indicates how well the organization awareness information reflects intended implementation of the assets of the system, or portion thereof. As another example, the awareness rating indicates how well the organization awareness information reflects intended operation of the assets of the system, or portion thereof.</p><p id="p-0744" num="0846">As another example, the awareness rating indicates how well the organization awareness information reflects intended implementation of the system functions of the system, or portion thereof. As another example, the awareness rating indicates how well the organization awareness information reflects intended operation of the system functions of system, or portion thereof.</p><p id="p-0745" num="0847">As another example, the awareness rating indicates how well the organization awareness information reflects intended implementation of the security functions of the system, or portion thereof. As another example, the awareness rating indicates how well the organization awareness information reflects intended operation of the security of the system, or portion thereof.</p><p id="p-0746" num="0848">The method continues at step <b>834</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>835</b> where the analysis system calculates a second awareness rating regarding a desired level of organizational awareness of the system, or portion thereof, based on the organization awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0747" num="0849"><figref idref="DRAWINGS">FIG. <b>119</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for system, or portion thereof; in particular engaging the system, or portion thereof to obtain data. The method begins at step <b>836</b> where the analysis system interprets the organization awareness information to identity components (e.g., computing device, HW, SW, server, etc.) of the system, or portion thereof. The method continues at step <b>837</b> where the analysis system queries a component regarding implementation, function, and/or operation of the component.</p><p id="p-0748" num="0850">The method continues at step <b>838</b> where the analysis system evaluates a response from the component concurrence with a portion of the organization awareness information relevant to the component. The method continues at step <b>839</b> where the analysis system determines whether the response concurs with a portion of the organization awareness information. If the response concurs, the method continues at step <b>840</b> where the analysis system adds a data element (e.g., a record entry, a note, set a flag, etc.) to the system awareness data regarding the substantial concurrence of the response from the component with the portion of the organization awareness information relevant to the component.</p><p id="p-0749" num="0851">If the responds does not concur, the method continues at step <b>841</b> where the analysis system adds a data element (e.g., a record entry in a table, a note, set a flag, etc.) to the system awareness data regarding the response from the component not substantially concurring with the portion of the organization awareness information relevant to the component. The non-concurrence is indicative of a deviation in the implementation, function, and/or operation of the component as identified in the response from disclosed implementation, function, and/or operation of the component as contained in the organizational awareness information. For example, the deviation is different HW, different SW, different network access, different data access, different data flow, coupled to different other components, and/or other differences.</p><p id="p-0750" num="0852">The method continues in <figref idref="DRAWINGS">FIG. <b>120</b></figref> at step <b>842</b> where the analysis system queries the component and/or another component regarding a cause for the deviation. The method continues at step <b>843</b> where the analysis system updates a data element to include an indication of the one or more causes for a deviation, wherein a cause for the deviation is based on responses from the component and/or the other component.</p><p id="p-0751" num="0853">The method continues at step <b>844</b> where the analysis system determines whether the deviation is a communication deviation. If yes, the method continues at step <b>845</b> where the analysis system evaluate a response from the device to ascertain an error of the organization awareness information regarding the device and/or the communication between the device and the component. The method continues at step <b>846</b> where the analysis system determines one or more causes of the error of the communication deviation.</p><p id="p-0752" num="0854">If the deviation is not a communication deviation, the method continues at step <b>847</b> where the analysis system determines whether the deviation is a system function deviation. If yes, the method continues at step <b>848</b> where the analysis system evaluate a response from the device to ascertain an error of the organization awareness information regarding the device and/or the system function of the device. The method continues at step <b>849</b> where the analysis system determines one or more causes of the error of the system function deviation.</p><p id="p-0753" num="0855">If the deviation is not a system function deviation, the method continues at step <b>850</b> where the analysis system determines whether the deviation is a security function deviation. If yes, the method continues at step <b>851</b> where the analysis system evaluate a response from the device to ascertain an error of the organization awareness information regarding the device and/or the security function of the device. The method continues at step <b>852</b> where the analysis system determines one or more causes of the error of the security function deviation.</p><p id="p-0754" num="0856">If the deviation is not a security function deviation, the method continues at step <b>853</b> where the analysis system evaluates a device response from the device to ascertain an error of the organization awareness information regarding the device and/or of the device. The method continues at step <b>854</b> where the analysis system determines one or more causes of the error of the information and/or of the device.</p><p id="p-0755" num="0857"><figref idref="DRAWINGS">FIG. <b>121</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof, and in particular for calculating the awareness rating. The method begins at steps <b>851</b> and <b>856</b>. At step <b>851</b>, the analysis system processes the organization awareness information into policy related organization awareness information, process related organization awareness information, documentation related organization awareness information, and/or automation related organization awareness information. At step <b>856</b>, the analysis system processes the system awareness data into policy related system awareness data, process related system awareness data, documentation related system awareness data, and/or automation related system awareness data;</p><p id="p-0756" num="0858">The method continues at steps <b>857</b>-<b>860</b>. At step <b>857</b>, the analysis system evaluates the process related organization awareness information with respect to the process related system awareness data to produce a process awareness rating. At step <b>858</b>, the analysis system evaluates the policy related organization awareness information with respect to the policy related system awareness data to produce a policy awareness rating. At step <b>859</b>, the analysis system evaluates the documentation related organization awareness information with respect to the documentation related system awareness data to produce a documentation awareness rating. At step <b>860</b>, the analysis system evaluates the automation related organization awareness information with respect to the automation related system awareness data to produce an automation awareness rating.</p><p id="p-0757" num="0859">The method continues at step <b>861</b> where the analysis system generates an awareness rating based on the automation awareness rating, the documentation awareness rating, the process awareness rating, and the policy awareness rating. For example, the analysis system performs a function on the automation awareness rating, the documentation awareness rating, the process awareness rating, and the policy awareness rating to produce the awareness rating. The function is a weight average, standard deviation, statistical analysis, trending, and/or other mathematical function.</p><p id="p-0758" num="0860"><figref idref="DRAWINGS">FIG. <b>122</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof, and in particular engaging the system to obtain awareness data. The method begins at step <b>862</b> where the analysis system determines data gathering criteria and/or parameters. The determining of data gathering parameters has been discussed with reference to one or moor preceding Figures and one or more subsequent Figures.</p><p id="p-0759" num="0861">The method continues at step <b>863</b> where the analysis system identifies a user device and queries it for data in accordance with the data gathering parameters. The method continues at step <b>864</b> where the analysis system catalogs the user device (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the user device responds. The method continues at step <b>865</b> where the analysis system obtains a data response from the user device. The data response includes data regarding the user device. An example of user device data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0760" num="0862">The method continues at step <b>866</b> where the analysis system identifies vendor information regarding the user device. The method continues at step <b>867</b> where the analysis system tags the data regarding the user device with the vendor information. This enables data to be sorted, searched, etc. based on vendor information.</p><p id="p-0761" num="0863">The method continues at step <b>868</b> where the analysis system determines whether data has been received from all relevant user devices. If not, the method repeats at step <b>863</b>. If yes, the method continues at step <b>869</b> where the analysis system identifies a storage device and queries it for data in accordance with the data gathering parameters. The method continues at step <b>870</b> where the analysis system catalogs the storage device (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the storage device responds. The method continues at step <b>871</b> where the analysis system obtains a data response from the storage device. The data response includes data regarding the storage device. An example of storage device data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0762" num="0864">The method continues at step <b>872</b> where the analysis system identifies vendor information regarding the storage device. The method continues at step <b>873</b> where the analysis system tags the data regarding the storage device with the vendor information. The method continues at step <b>874</b> where the analysis system determines whether data has been received from all relevant storage devices. If not, the method repeats at step <b>869</b>.</p><p id="p-0763" num="0865">If yes, the method continues at step <b>875</b> where the analysis system identifies a server device and queries it for data in accordance with the data gathering parameters. The method continues at step <b>876</b> where the analysis system catalogs the server device (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the server device responds. The method continues at step <b>877</b> where the analysis system obtains a data response from the server device. The data response includes data regarding the server device. An example of server device data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0764" num="0866">The method continues at step <b>878</b> where the analysis system identifies vendor information regarding the server device. The method continues at step <b>879</b> where the analysis system tags the data regarding the server device with the vendor information. The method continues at step <b>880</b> of <figref idref="DRAWINGS">FIG. <b>123</b></figref> where the analysis system determines whether data has been received from all relevant server devices. If not, the method repeats at step <b>875</b>.</p><p id="p-0765" num="0867">If yes, the method continues at step <b>881</b> where the analysis system identifies a security device and queries it for data in accordance with the data gathering parameters. The method continues at step <b>882</b> where the analysis system catalogs the security device (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the security device responds. The method continues at step <b>883</b> where the analysis system obtains a data response from the security device. The data response includes data regarding the security device. An example of security device data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0766" num="0868">The method continues at step <b>884</b> where the analysis system identifies vendor information regarding the security device. The method continues at step <b>885</b> where the analysis system tags the data regarding the security device with the vendor information. The method continues at step <b>886</b> where the analysis system determines whether data has been received from all relevant security devices. If not, the method repeats at step <b>881</b>.</p><p id="p-0767" num="0869">If yes, the method continues at step <b>887</b> where the analysis system identifies a security tool and queries it for data in accordance with the data gathering parameters. The method continues at step <b>888</b> where the analysis system catalogs the security tool (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the security tool responds via hardware on which the tool operates. The method continues at step <b>889</b> where the analysis system obtains a data response from the security tool. The data response includes data regarding the security tool. An example of security tool data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0768" num="0870">The method continues at step <b>890</b> where the analysis system identifies vendor information regarding the security tool. The method continues at step <b>891</b> where the analysis system tags the data regarding the security tool with the vendor information. The method continues at step <b>892</b> where the analysis system determines whether data has been received from all relevant security tools. If not, the method repeats at step <b>887</b>.</p><p id="p-0769" num="0871">If yes, the method continues at step <b>893</b> where the analysis system identifies a network device and queries it for data in accordance with the data gathering parameters. The method continues at step <b>894</b> where the analysis system catalogs the network device (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the network device responds. The method continues at step <b>895</b> where the analysis system obtains a data response from the network device. The data response includes data regarding the network device. An example of network device data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0770" num="0872">The method continues at step <b>896</b> where the analysis system identifies vendor information regarding the network device. The method continues at step <b>897</b> where the analysis system tags the data regarding the network device with the vendor information. The method continues at step <b>898</b> of <figref idref="DRAWINGS">FIG. <b>124</b></figref> where the analysis system determines whether data has been received from all relevant network devices. If not, the method repeats at step <b>893</b>.</p><p id="p-0771" num="0873">If yes, the method continues at step <b>899</b> where the analysis system identifies another device (e.g., any other device that is part of the system, interfaces with the system, uses the system, and/or supports the system) and queries it for data in accordance with the data gathering parameters. The method continues at step <b>900</b> where the analysis system catalogs the other device (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the other device responds. The method continues at step <b>901</b> where the analysis system obtains a data response from the other device. The data response includes data regarding the other device. An example of other device data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0772" num="0874">The method continues at step <b>902</b> where the analysis system identifies vendor information regarding the other device. The method continues at step <b>903</b> where the analysis system tags the data regarding the other device with the vendor information. The method continues at step <b>904</b> where the analysis system determines whether data has been received from all relevant other devices. If not, the method repeats at step <b>899</b>.</p><p id="p-0773" num="0875">If yes, the method continues at step <b>905</b> where the analysis system identifies another tool (e.g., any other tool that is part of the system, interprets the system, monitors the system, and/or supports the system) and queries it for data in accordance with the data gathering parameters. The method continues at step <b>906</b> where the analysis system catalogs the other tool (e.g., records it as being part of the system, or portion thereof, if not already cataloged) when the other tool responds via hardware on which the tool operates. The method continues at step <b>907</b> where the analysis system obtains a data response from the other tool. The data response includes data regarding the other tool. An example of other tool data was discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0774" num="0876">The method continues at step <b>908</b> where the analysis system identifies vendor information regarding the other tool. The method continues at step <b>909</b> where the analysis system tags the data regarding the other tool with the vendor information. The method continues at step <b>910</b> where the analysis system determines whether data has been received from all relevant other tools. If not, the method repeats at step <b>905</b>. If yes, the method continues at step <b>911</b> where the analysis system ends the process or repeats it for another part of the system.</p><p id="p-0775" num="0877"><figref idref="DRAWINGS">FIG. <b>125</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof and, in particular, identifying a device or a tool. The method begins at step <b>920</b> where the analysis system determines whether a device (e.g., hardware and/or software) or tool is already included in the organization awareness information (e.g., the disclosed data for a particular analysis of the system, or portion thereof). If yes, the method continues at step <b>921</b> where the analysis module determines whether it's done with identifying devices or tools. If yes, the method is ended. If not, the method repeats at step <b>920</b>.</p><p id="p-0776" num="0878">If the device or tool is not in the organization awareness information, the method continues at step <b>922</b> where the analysis system engages one or more detection (or discovery) tools to detect a device and/or a tool. Examples of detection tools were discussed with reference to one or more preceding figures. The method continues at step <b>923</b> where the analysis system determines whether the detection tool(s) has identified a device (e.g., hardware and/or software). If not, the method continues at step <b>924</b> where the analysis system determines whether the detection tool(s) has identified a tool. If not, the method repeats at step <b>921</b>.</p><p id="p-0777" num="0879">If a tool is identified, the method continues at step <b>925</b> where the analysis system determines whether the tool is cataloged (e.g., is part of the system, but is not included in the organization awareness information for this particular evaluation). If yes, the method continues at step <b>926</b> where the analysis system adds the tool to the organization awareness information and the method continues at step <b>921</b>.</p><p id="p-0778" num="0880">If the tool is not cataloged, the method continues at step <b>927</b> where the analysis system verifies the tool as being part of the system and then catalogs it as part of the system. The method continues at step <b>928</b> where the analysis system obtains a data response from the tool, via hardware on which the tool operates, in regard to a data gathering request. The data response includes data regarding the tool. Examples of the data regarding the tool were discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0779" num="0881">The method continues at step <b>929</b> where the analysis system identifies vendor information regarding the tool. The method continues at step <b>930</b> where the analysis system tags the data regarding the tool with the vendor information. The method repeats at step <b>921</b>.</p><p id="p-0780" num="0882">If, at step <b>923</b>, a device is identified, the method continues at step <b>931</b> where the analysis system determines whether the device (e.g., hardware and/or software) is cataloged (e.g., is part of the system, but is not included in the organization awareness information for this particular evaluation). If yes, the method continues at step <b>932</b> where the analysis system adds the devices to the organization awareness information and the method continues at step <b>921</b>.</p><p id="p-0781" num="0883">If the device is not cataloged, the method continues at step <b>933</b> where the analysis system verifies the device as being part of the system and then catalogs it as part of the system. The method continues at step <b>934</b> where the analysis system obtains a data response from the device in regard to a data gathering request. The data response includes data regarding the device. Examples of the data regarding the device were discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0782" num="0884">The method continues at step <b>935</b> where the analysis system identifies vendor information regarding the device. The method continues at step <b>936</b> where the analysis system tags the data regarding the device with the vendor information. The method repeats at step <b>921</b>.</p><p id="p-0783" num="0885"><figref idref="DRAWINGS">FIG. <b>126</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof and, in particular, identifying a device or a tool. The method begins at step <b>940</b> where the analysis system determines whether a device (e.g., hardware and/or software) or tool is already included in the organization awareness information (e.g., the disclosed data for a particular analysis of the system, or portion thereof). If yes, the method continues at step <b>941</b> where the analysis module determines whether it's done with identifying devices or tools. If yes, the method is ended. If not, the method repeats at step <b>940</b>.</p><p id="p-0784" num="0886">If the device or tool is not in the organization awareness information, the method continues at step <b>942</b> where the analysis system interprets data from an identified device and/or tool (e.g., already in the organization awareness information) with regards to a device or tool. For example, the analysis system looks for data regarding an identified device exchanging data with the device being reviewed. As another example, the analysis system looks for data regarding a tool being used on the device under review to repair a software issue.</p><p id="p-0785" num="0887">The method continues at step <b>943</b> where the analysis system determines whether the data has identified such a device (e.g., hardware and/or software). If not, the method continues at step <b>944</b> where the analysis system determines whether the detection tool(s) has identified such a tool. If not, the method repeats at step <b>941</b>.</p><p id="p-0786" num="0888">If a tool is identified, the method continues at step <b>945</b> where the analysis system determines whether the tool is cataloged (e.g., is part of the system, but is not included in the organization awareness information for this particular evaluation). If yes, the method continues at step <b>946</b> where the analysis system adds the tool to the organization awareness information and the method continues at step <b>941</b>.</p><p id="p-0787" num="0889">If the tool is not cataloged, the method continues at step <b>947</b> where the analysis system verifies the tool as being part of the system and then catalogs it as part of the system. The method continues at step <b>948</b> where the analysis system obtains a data response from the tool, via hardware on which the tool operates, in regard to a data gathering request. The data response includes data regarding the tool. Examples of the data regarding the tool were discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0788" num="0890">The method continues at step <b>949</b> where the analysis system identifies vendor information regarding the tool. The method continues at step <b>950</b> where the analysis system tags the data regarding the tool with the vendor information. The method repeats at step <b>921</b>.</p><p id="p-0789" num="0891">If, at step <b>943</b>, a device is identified, the method continues at step <b>951</b> where the analysis system determines whether the device (e.g., hardware and/or software) is cataloged (e.g., is part of the system, but is not included in the organization awareness information for this particular evaluation). If yes, the method continues at step <b>952</b> where the analysis system adds the devices to the organization awareness information and the method continues at step <b>941</b>.</p><p id="p-0790" num="0892">If the device is not cataloged, the method continues at step <b>953</b> where the analysis system verifies the device as being part of the system and then catalogs it as part of the system. The method continues at step <b>954</b> where the analysis system obtains a data response from the device in regard to a data gathering request. The data response includes data regarding the device. Examples of the data regarding the device were discussed with reference to one or more of <figref idref="DRAWINGS">FIGS. <b>75</b>-<b>80</b></figref>.</p><p id="p-0791" num="0893">The method continues at step <b>955</b> where the analysis system identifies vendor information regarding the device. The method continues at step <b>956</b> where the analysis system tags the data regarding the device with the vendor information. The method repeats at step <b>941</b>.</p><p id="p-0792" num="0894"><figref idref="DRAWINGS">FIG. <b>127</b></figref> is a logic diagram of a further example of an analysis system determining an identification rating for a system, or portion thereof, and in particular to generating data gathering criteria (or parameters). The method begins at step <b>960</b> where the analysis system determines whether the current analysis is for the entire system or a portion thereof. If the analysis is for the entire system, the method continues at step <b>962</b> where the analysis system prepares to analyze the entire system. If the analysis is for a portion of the system, the method continues at step <b>961</b> where the analysis system determines the particular section (e.g., identifies one or more system elements).</p><p id="p-0793" num="0895">The method continues at step <b>963</b> where the analysis system determines whether the current analysis has identified evaluation criteria (e.g., guidelines, system requirements, system design, system build, and/or resulting system). If yes, the method continues at step <b>964</b> where the analysis system determines the specific evaluation criteria. If not, the method continues at step <b>965</b> where the analysis system determines a set of default evaluation criteria (e.g., one or more of the evaluation criteria).</p><p id="p-0794" num="0896">The method continues at step <b>966</b> where the analysis system determines whether the current analysis has identified an evaluation mode (e.g., assets, system functions, and/or security functions). If yes, the method continues at step <b>966</b> where the analysis system determines the specific evaluation mode(s). If not, the method continues at step <b>967</b> where the analysis system determines a set of default evaluation modes (e.g., one or more of the evaluation modes).</p><p id="p-0795" num="0897">The method continues at step <b>968</b> where the analysis system determines whether the current analysis has identified an evaluation perspective (e.g., understanding, implementation, and/or operation). If yes, the method continues at step <b>969</b> where the analysis system determines the specific evaluation perspective(s). If not, the method continues at step <b>970</b> where the analysis system determines a set of default evaluation perspectives (e.g., one or more of the evaluation perspectives).</p><p id="p-0796" num="0898">The method continues at step <b>971</b> where the analysis system determines whether the current analysis has identified an evaluation viewpoint (e.g., disclosed, discovered, desired, and/or self-analysis). If yes, the method continues at step <b>972</b> where the analysis system determines the specific evaluation viewpoint(s). If not, the method continues at step <b>973</b> where the analysis system determines a set of default evaluation viewpoints (e.g., one or more of the evaluation viewpoints).</p><p id="p-0797" num="0899">The method continues at step <b>974</b> where the analysis system determines whether the current analysis has identified an evaluation category, and/or sub-categories (e.g., categories include identify, protect, detect, response, and/or recover). If yes, the method continues at step <b>975</b> where the analysis system determines one or more specific evaluation categories and/or sub-categories. If not, the method continues at step <b>977</b> where the analysis system determines a set of default evaluation categories and/or sub-categories (e.g., one or more of the evaluation categories and/or sub-categories). The method continues at step <b>976</b> where the analysis system determines the data gathering criteria (or parameters) based on the determination made in the previous steps.</p><p id="p-0798" num="0900"><figref idref="DRAWINGS">FIG. <b>128</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1000</b> where the analysis system selects a system, or portion thereof, to evaluate organizational asset awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the asset management sub-category of the identify category.</p><p id="p-0799" num="0901">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical assets and/or one or more conceptual assets.</p><p id="p-0800" num="0902">The method continues at step <b>1001</b> where the analysis system obtains organization asset awareness information regarding the system, or portion thereof. The organization asset awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the assets (e.g., hardware assets, software assets, data flow assets, external system catalog assets, resource prioritization assets, and/or security role assets). In an example, the analysis system obtains the organization asset awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization asset awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0801" num="0903">The method continues at step <b>1002</b> where the analysis system engages with the system, or portion thereof, to produce system asset awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the assets. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0802" num="0904">The method continues at step <b>1003</b> where the analysis system calculates an asset awareness rating regarding the organizational asset awareness of the system, or portion thereof, based on the organization asset awareness information, the system asset awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The asset awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the asset awareness rating indicates how well the organization asset awareness information reflects an understanding of the assets of the system, or portion thereof. As another example, the asset awareness rating indicates how well the organization asset awareness information reflects an understanding of how well the assets support the system functions of system, or portion thereof.</p><p id="p-0803" num="0905">As another example, the asset awareness rating indicates how well the organization asset awareness information reflects an understanding of how well the assets support the security functions of the system, or portion thereof. As another example, the asset awareness rating indicates how well the organization asset awareness information reflects intended implementation of the assets of the system, or portion thereof. As another example, the asset awareness rating indicates how well the organization asset awareness information reflects intended operation of the assets of the system, or portion thereof.</p><p id="p-0804" num="0906">As another example, the asset awareness rating indicates how well the organization asset awareness information reflects intended implementation of the assets with respect to the system functions of the system, or portion thereof. As another example, the asset awareness rating indicates how well the organization asset awareness information reflects intended operation of the assets with respect to the system functions of system, or portion thereof.</p><p id="p-0805" num="0907">As another example, the asset awareness rating indicates how well the organization asset awareness information reflects intended implementation of the assets to support security functions of the system, or portion thereof. As another example, the asset awareness rating indicates how well the organization asset awareness information reflects intended operation of the assets to support the security functions of the system, or portion thereof.</p><p id="p-0806" num="0908">The method continues at step <b>1004</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1005</b> where the analysis system calculates a second asset awareness rating regarding a desired level of organizational asset awareness of the system, or portion thereof, based on the organization asset awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second asset awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0807" num="0909"><figref idref="DRAWINGS">FIG. <b>129</b></figref> is a logic diagram of an example of an analysis system determining an asset management evaluation rating for a system, or portion thereof. The method begins at step <b>1006</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for an asset management evaluation. An asset management evaluation includes evaluating the system's asset management of the system including hardware management, software management, data flow mapping, cataloging of external systems, managing of resource prioritization, and/or managing of security role establishment.</p><p id="p-0808" num="0910">The method continues at step <b>1007</b> where the analysis system determines at least one evaluation perspective for use in performing the asset management evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions are understood. An implementation perspective is with regard to how well the assets, system functions, and/or security functions are implemented. An operation perspective is with regard to how well the assets, system functions, and/or security functions operate. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understanding, implementation, and/or operation of assets, system functions, and/or security functions.</p><p id="p-0809" num="0911">The method continues at step <b>1008</b> where the analysis system determines at least one evaluation viewpoint for use in performing the asset management evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0810" num="0912">The method continues at step <b>1009</b> where the analysis system obtains asset management data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Asset management data is data obtained that is regarding the system aspect.</p><p id="p-0811" num="0913">The method continues at step <b>1010</b> where the analysis system calculates an asset management rating as a measure of system asset management maturity for the system aspect based on the asset management data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0812" num="0914"><figref idref="DRAWINGS">FIG. <b>130</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1020</b> where the analysis system selects a system, or portion thereof, to evaluate organizational business environment awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the business environment sub-category of the identify category.</p><p id="p-0813" num="0915">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical assets and/or one or more conceptual assets that supports business environment.</p><p id="p-0814" num="0916">The method continues at step <b>1021</b> where the analysis system obtains organization business environment awareness information regarding the system, or portion thereof. The organization business environment awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the business environment (e.g., one or more supply chain roles, one or more industry critical infrastructures, one or more business priorities, one or more critical services, and/or one or more resiliency requirements). In an example, the analysis system obtains the organization business environment awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization business environment awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0815" num="0917">The method continues at step <b>1022</b> where the analysis system engages with the system, or portion thereof, to produce system business environment awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the business environment. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0816" num="0918">The method continues at step <b>1023</b> where the analysis system calculates a business environment awareness rating regarding the organizational business environment awareness of the system, or portion thereof, based on the organization business environment awareness information, the system business environment awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The business environment awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the business environment awareness rating indicates how well the organization business environment awareness information reflects an understanding of the business environment of the system, or portion thereof. As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects an understanding of how well the business environment support the system functions of system, or portion thereof.</p><p id="p-0817" num="0919">As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects an understanding of how well the business environment support the security functions of the system, or portion thereof. As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects intended implementation of assets of the system to support the business environment of the system, or portion thereof. As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects intended operation of assets, system functions, and/or security functions with respect to the business environment of the system, or portion thereof.</p><p id="p-0818" num="0920">As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects intended implementation of the system functions of the system, or portion thereof. As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects intended operation of assets supporting the business environment with respect to the system functions of system, or portion thereof.</p><p id="p-0819" num="0921">As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects intended implementation of assets supporting the business environment to support security functions of the system, or portion thereof. As another example, the business environment awareness rating indicates how well the organization business environment awareness information reflects intended operation of assets supporting the business environment with respect to security of the system, or portion thereof.</p><p id="p-0820" num="0922">The method continues at step <b>1024</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1025</b> where the analysis system calculates a second business environment awareness rating regarding a desired level of organizational business environment awareness of the system, or portion thereof, based on the organization business environment awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second business environment awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0821" num="0923"><figref idref="DRAWINGS">FIG. <b>131</b></figref> is a logic diagram of an example of an analysis system determining a business environment evaluation rating for a system, or portion thereof. The method begins at step <b>1026</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for a business environment evaluation. A business environment evaluation includes evaluating the system's business environment of the system including one or more supply chain roles, one or more industry critical infrastructures, one or more business priorities, one or more critical services, and/or one or more resiliency requirements.</p><p id="p-0822" num="0924">The method continues at step <b>1027</b> where the analysis system determines at least one evaluation perspective for use in performing the business environment evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions with respect to business environment are understood. An implementation perspective is with regard to how well the assets, system functions, and/or security functions with respect to business environment are implemented. An operation perspective is with regard to how well the assets, system functions, and/or security functions with respect to business environment operate. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understanding, implementation, and/or operation of assets, system functions, and/or security functions with respect to business environment.</p><p id="p-0823" num="0925">The method continues at step <b>1028</b> where the analysis system determines at least one evaluation viewpoint for use in performing the business environment evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0824" num="0926">The method continues at step <b>1029</b> where the analysis system obtains business environment data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Business environment data is data obtained that is regarding the system aspect.</p><p id="p-0825" num="0927">The method continues at step <b>1030</b> where the analysis system calculates a business environment rating as a measure of system business environment maturity for the system aspect based on the business environment management data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0826" num="0928"><figref idref="DRAWINGS">FIG. <b>132</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1040</b> where the analysis system selects a system, or portion thereof, to evaluate organizational governance awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the governance sub-category of the identify category.</p><p id="p-0827" num="0929">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical asset and/or one or more conceptual asset regarding governance.</p><p id="p-0828" num="0930">The method continues at step <b>1041</b> where the analysis system obtains organization governance awareness information regarding the system, or portion thereof. The organization governance awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the governance (e.g., one or more security policies, one or more alignment of security responsibilities, and/or one or more legal requirements). In an example, the analysis system obtains the organization governance awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization governance awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0829" num="0931">The method continues at step <b>1042</b> where the analysis system engages with the system, or portion thereof, to produce system governance awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the governance. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0830" num="0932">The method continues at step <b>1043</b> where the analysis system calculates a governance awareness rating regarding the organizational governance awareness of the system, or portion thereof, based on the organization governance awareness information, the system governance awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The governance awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the governance awareness rating indicates how well the organization governance awareness information reflects an understanding of the governance with respect to assets of the system, or portion thereof. As another example, the governance awareness rating indicates how well the organization governance awareness information reflects an understanding of the governance with respect to the system functions of system, or portion thereof.</p><p id="p-0831" num="0933">As another example, the governance awareness rating indicates how well the organization governance awareness information reflects an understanding of the governance with respect to the security functions of the system, or portion thereof. As another example, the governance awareness rating indicates how well the organization governance awareness information reflects intended implementation of the governance with respect to assets of the system, or portion thereof. As another example, the governance awareness rating indicates how well the organization governance awareness information reflects intended operation of the governance with respect to assets of the system, or portion thereof.</p><p id="p-0832" num="0934">As another example, the governance awareness rating indicates how well the organization governance awareness information reflects intended implementation of the governance with respect to the system functions of the system, or portion thereof. As another example, the governance awareness rating indicates how well the organization governance awareness information reflects intended operation of the governance with respect to the system functions of system, or portion thereof.</p><p id="p-0833" num="0935">As another example, the governance awareness rating indicates how well the organization governance awareness information reflects intended implementation of the governance with respect to the security functions of the system, or portion thereof. As another example, the governance awareness rating indicates how well the organization governance awareness information reflects intended operation of the governance to with respect to the security functions of the system, or portion thereof.</p><p id="p-0834" num="0936">The method continues at step <b>1044</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1045</b> where the analysis system calculates a second governance awareness rating regarding a desired level of organizational governance awareness of the system, or portion thereof, based on the organization governance awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second governance awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0835" num="0937"><figref idref="DRAWINGS">FIG. <b>133</b></figref> is a logic diagram of an example of an analysis system determining a governance evaluation rating for a system, or portion thereof. The method begins at step <b>1046</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for a governance evaluation. A governance evaluation includes evaluating the system's governance of the system including one or more security policies, one or more alignment of security responsibilities, and/or one or more legal requirements.</p><p id="p-0836" num="0938">The method continues at step <b>1047</b> where the analysis system determines at least one evaluation perspective for use in performing the governance evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions with respect to governance are understood. An implementation perspective is with regard to how well the assets, system functions, and/or security functions with respect to governance are implemented. An operation perspective is with regard to how well the assets, system functions, and/or security functions with respect to governance operate. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understanding, implementation, and/or operation of assets, system functions, and/or security functions with respect to governance.</p><p id="p-0837" num="0939">The method continues at step <b>1048</b> where the analysis system determines at least one evaluation viewpoint for use in performing the governance evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0838" num="0940">The method continues at step <b>1049</b> where the analysis system obtains governance data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Governance data is data obtained that is regarding the system aspect.</p><p id="p-0839" num="0941">The method continues at step <b>1050</b> where the analysis system calculates a governance rating as a measure of system governance maturity for the system aspect based on the governance data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0840" num="0942"><figref idref="DRAWINGS">FIG. <b>134</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1060</b> where the analysis system selects a system, or portion thereof, to evaluate organizational risk assessment awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the risk assessment sub-category of the identify category.</p><p id="p-0841" num="0943">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical asset and/or one or more conceptual asset with respect to risk assessment.</p><p id="p-0842" num="0944">The method continues at step <b>1061</b> where the analysis system obtains organization risk assessment awareness information regarding the system, or portion thereof. The organization risk assessment awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the risk assessment (e.g., one or more vulnerabilities identified, one or more externals sources are leveraged, one or more threats are identified, one or more business impacts are identified, one or more risk levels are established, and/or one or more risk responses are established). In an example, the analysis system obtains the organization risk assessment awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization risk assessment awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0843" num="0945">The method continues at step <b>1062</b> where the analysis system engages with the system, or portion thereof, to produce system risk assessment awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the risk assessment. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0844" num="0946">The method continues at step <b>1063</b> where the analysis system calculates a risk assessment awareness rating regarding the organizational risk assessment awareness of the system, or portion thereof, based on the organization risk assessment awareness information, the system risk assessment awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The risk assessment awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects an understanding of the risk assessment with respect to assets of the system, or portion thereof. As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects an understanding of the risk assessment with respect to the system functions of system, or portion thereof.</p><p id="p-0845" num="0947">As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects an understanding of the risk assessment with respect to the security functions of the system, or portion thereof. As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects intended implementation of the risk assessment with respect to assets of the system, or portion thereof. As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects intended operation of the risk assessment with respect to assets of the system, or portion thereof.</p><p id="p-0846" num="0948">As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects intended implementation of the risk assessment with respect to the system functions of the system, or portion thereof. As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects intended operation of the risk assessment with respect to the system functions of system, or portion thereof.</p><p id="p-0847" num="0949">As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects intended implementation of the risk assessment with respect to the security functions of the system, or portion thereof. As another example, the risk assessment awareness rating indicates how well the organization risk assessment awareness information reflects intended operation of the risk assessment with respect to the security functions of the system, or portion thereof.</p><p id="p-0848" num="0950">The method continues at step <b>1064</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1065</b> where the analysis system calculates a second risk assessment awareness rating regarding a desired level of organizational risk assessment awareness of the system, or portion thereof, based on the organization risk assessment awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second risk assessment awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0849" num="0951"><figref idref="DRAWINGS">FIG. <b>135</b></figref> is a logic diagram of an example of an analysis system determining a risk assessment evaluation rating for a system, or portion thereof. The method begins at step <b>1066</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for a risk assessment evaluation. A risk assessment evaluation includes evaluating the system's risk assessment of the system including one or more vulnerabilities identified, one or more externals sources are leveraged, one or more threats are identified, one or more business impacts are identified, one or more risk levels are established, and/or one or more risk responses are established.</p><p id="p-0850" num="0952">The method continues at step <b>1067</b> where the analysis system determines at least one evaluation perspective for use in performing the risk assessment evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions with respect to risk assessment are understood. An implementation perspective is with regard to how well the assets, system functions, and/or security functions with respect to risk assessment are implemented. An operation perspective is with regard to how well the assets, system functions, and/or security functions with respect to risk assessment operate. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understanding, implementation, and/or operation of assets, system functions, and/or security functions with respect to risk assessment.</p><p id="p-0851" num="0953">The method continues at step <b>1068</b> where the analysis system determines at least one evaluation viewpoint for use in performing the risk assessment evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0852" num="0954">The method continues at step <b>1069</b> where the analysis system obtains risk assessment data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Risk assessment data is data obtained that is regarding the system aspect.</p><p id="p-0853" num="0955">The method continues at step <b>1070</b> where the analysis system calculates a risk assessment rating as a measure of system risk assessment maturity for the system aspect based on the risk assessment data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0854" num="0956"><figref idref="DRAWINGS">FIG. <b>136</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1080</b> where the analysis system selects a system, or portion thereof, to evaluate organizational risk management awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the risk management sub-category of the identify category.</p><p id="p-0855" num="0957">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical asset and/or one or more conceptual asset with respect to risk management.</p><p id="p-0856" num="0958">The method continues at step <b>1081</b> where the analysis system obtains organization risk management awareness information regarding the system, or portion thereof. The organization risk management awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the risk management (e.g., establishment of one or more risk management processes, one or more risk tolerances established, and/or one or more risk tolerances tied to a critical business environment sub-sub-category). In an example, the analysis system obtains the organization risk management awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization risk management awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0857" num="0959">The method continues at step <b>1082</b> where the analysis system engages with the system, or portion thereof, to produce system risk management awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the risk management. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0858" num="0960">The method continues at step <b>1083</b> where the analysis system calculates a risk management awareness rating regarding the organizational risk management awareness of the system, or portion thereof, based on the organization risk management awareness information, the system risk management awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The risk management awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the risk management awareness rating indicates how well the organization risk management awareness information reflects an understanding of the risk management of the system, or portion thereof. As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects an understanding of the risk management with respect to the system functions of system, or portion thereof.</p><p id="p-0859" num="0961">As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects an understanding of the risk management with respect to the security functions of the system, or portion thereof. As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects intended implementation of assets to support the risk management of the system, or portion thereof. As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects intended operation of assets supporting the risk management of the system, or portion thereof.</p><p id="p-0860" num="0962">As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects intended implementation of the risk management with respect to system functions of the system, or portion thereof. As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects intended operation of the risk management with respect to the system functions of system, or portion thereof.</p><p id="p-0861" num="0963">As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects intended implementation of assets supporting the risk management of the security functions of the system, or portion thereof. As another example, the risk management awareness rating indicates how well the organization risk management awareness information reflects intended operation of assets supporting the risk management of the security of the system, or portion thereof.</p><p id="p-0862" num="0964">The method continues at step <b>1084</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1085</b> where the analysis system calculates a second risk management awareness rating regarding a desired level of organizational risk management awareness of the system, or portion thereof, based on the organization risk management awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second risk management awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0863" num="0965"><figref idref="DRAWINGS">FIG. <b>137</b></figref> is a logic diagram of an example of an analysis system determining a risk management evaluation rating for a system, or portion thereof. The method begins at step <b>1086</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for a risk management evaluation. A risk management evaluation includes evaluating the system's risk management of the system including establishment of one or more risk management processes, one or more risk tolerances established, and/or one or more risk tolerances tied to a critical business environment sub-sub-category.</p><p id="p-0864" num="0966">The method continues at step <b>1087</b> where the analysis system determines at least one evaluation perspective for use in performing the risk management evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions are understood with respect to risk management. An implementation perspective is with regard to how well the assets, system functions, and/or security functions are implemented with respect to the risk management. An operation perspective is with regard to how well the assets, system functions, and/or security functions operate with respect to the risk management. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understating, implementation, and/or operation of assets, system functions, and/or security functions with respect to the risk management.</p><p id="p-0865" num="0967">The method continues at step <b>1088</b> where the analysis system determines at least one evaluation viewpoint for use in performing the risk management evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0866" num="0968">The method continues at step <b>1089</b> where the analysis system obtains risk management data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Risk management data is data obtained that is regarding the system aspect.</p><p id="p-0867" num="0969">The method continues at step <b>1090</b> where the analysis system calculates a risk management rating as a measure of system risk management maturity for the system aspect based on the risk management data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0868" num="0970"><figref idref="DRAWINGS">FIG. <b>138</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1100</b> where the analysis system selects a system, or portion thereof, to evaluate organizational access control awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the access control sub-category of the identify category.</p><p id="p-0869" num="0971">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical assets and/or one or more conceptual assets with respect to access control.</p><p id="p-0870" num="0972">The method continues at step <b>1101</b> where the analysis system obtains organization access control awareness information regarding the system, or portion thereof. The organization access control awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the access control (e.g., one or more remote access controls, one or more permissions, and/or one or more network integrity controls). In an example, the analysis system obtains the organization access control awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization access control awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0871" num="0973">The method continues at step <b>1102</b> where the analysis system engages with the system, or portion thereof, to produce system access control awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the access control. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0872" num="0974">The method continues at step <b>1103</b> where the analysis system calculates an access control awareness rating regarding the organizational access control awareness of the system, or portion thereof, based on the organization access control awareness information, the system access control awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The access control awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the access control awareness rating indicates how well the organization access control awareness information reflects an understanding of the access control of the system, or portion thereof. As another example, the access control awareness rating indicates how well the organization access control awareness information reflects an understanding of how well the access control support the system functions of system, or portion thereof.</p><p id="p-0873" num="0975">As another example, the access control awareness rating indicates how well the organization access control awareness information reflects an understanding of how well the access control supports the security functions of the system, or portion thereof. As another example, the access control awareness rating indicates how well the organization access control awareness information reflects intended implementation of the access control of the system, or portion thereof. As another example, the access control awareness rating indicates how well the organization access control awareness information reflects intended operation of the access control of the system, or portion thereof.</p><p id="p-0874" num="0976">As another example, the access control awareness rating indicates how well the organization access control awareness information reflects intended implementation of the system functions of the system, or portion thereof. As another example, the access control awareness rating indicates how well the organization access control awareness information reflects intended operation of the access control with respect to the system functions of system, or portion thereof.</p><p id="p-0875" num="0977">As another example, the access control awareness rating indicates how well the organization access control awareness information reflects intended implementation of the access control to support security functions of the system, or portion thereof. As another example, the access control awareness rating indicates how well the organization access control awareness information reflects intended operation of the access control to support the security of the system, or portion thereof.</p><p id="p-0876" num="0978">The method continues at step <b>1104</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1105</b> where the analysis system calculates a second access control awareness rating regarding a desired level of organizational access control awareness of the system, or portion thereof, based on the organization access control awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second access control awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0877" num="0979"><figref idref="DRAWINGS">FIG. <b>139</b></figref> is a logic diagram of an example of an analysis system determining an access control evaluation rating for a system, or portion thereof. The method begins at step <b>1106</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for an access control evaluation. An access control evaluation includes evaluating the system's access control of the system including establishment of one or more remote access controls, one or more permissions, and/or one or more network integrity controls.</p><p id="p-0878" num="0980">The method continues at step <b>1107</b> where the analysis system determines at least one evaluation perspective for use in performing the access control evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions are understood with respect to access control. An implementation perspective is with regard to how well the assets, system functions, and/or security functions are implemented with respect to access control. An operation perspective is with regard to how well the assets, system functions, and/or security functions operate with respect to access control. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understating, implementation, and/or operation of assets, system functions, and/or security functions with respect to access control.</p><p id="p-0879" num="0981">The method continues at step <b>1108</b> where the analysis system determines at least one evaluation viewpoint for use in performing the access control evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0880" num="0982">The method continues at step <b>1109</b> where the analysis system obtains access control data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Access control data is data obtained that is regarding the system aspect.</p><p id="p-0881" num="0983">The method continues at step <b>1110</b> where the analysis system calculates an access control rating as a measure of system access control maturity for the system aspect based on the access control data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0882" num="0984"><figref idref="DRAWINGS">FIG. <b>140</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1120</b> where the analysis system selects a system, or portion thereof, to evaluate organizational awareness &#x26; training awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the awareness &#x26; training sub-category of the identify category.</p><p id="p-0883" num="0985">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical assets and/or one or more conceptual assets with respect to awareness &#x26; training.</p><p id="p-0884" num="0986">The method continues at step <b>1121</b> where the analysis system obtains organization awareness &#x26; training awareness information regarding the system, or portion thereof. The organization awareness &#x26; training awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the awareness &#x26; training (e.g., one or more user training established and/or executed, one or more user privileges established, one or more third party responsibilities established, one or more executive responsibilities established, and/or one or more IT and/or security responsibilities established). In an example, the analysis system obtains the organization awareness &#x26; training awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization awareness &#x26; training awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0885" num="0987">The method continues at step <b>1122</b> where the analysis system engages with the system, or portion thereof, to produce system awareness &#x26; training awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the awareness &#x26; training. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0886" num="0988">The method continues at step <b>1123</b> where the analysis system calculates an awareness &#x26; training awareness rating regarding the organizational awareness &#x26; training awareness of the system, or portion thereof, based on the organization awareness &#x26; training awareness information, the system awareness &#x26; training awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The awareness &#x26; training awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects an understanding of the awareness &#x26; training of the system, or portion thereof. As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects an understanding of how well the awareness &#x26; training support the system functions of system, or portion thereof.</p><p id="p-0887" num="0989">As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects an understanding of how well the awareness &#x26; training supports the security functions of the system, or portion thereof. As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects intended implementation of the awareness &#x26; training of the system, or portion thereof. As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects intended operation of the awareness &#x26; training of the system, or portion thereof.</p><p id="p-0888" num="0990">As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects intended implementation of the system functions of the system, or portion thereof. As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects intended operation of the awareness &#x26; training with respect to the system functions of system, or portion thereof.</p><p id="p-0889" num="0991">As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects intended implementation of the awareness &#x26; training to support security functions of the system, or portion thereof. As another example, the awareness &#x26; training awareness rating indicates how well the organization awareness &#x26; training awareness information reflects intended operation of the awareness &#x26; training to support the security of the system, or portion thereof.</p><p id="p-0890" num="0992">The method continues at step <b>1124</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1125</b> where the analysis system calculates a second awareness &#x26; training awareness rating regarding a desired level of organizational awareness &#x26; training awareness of the system, or portion thereof, based on the organization awareness &#x26; training awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second awareness &#x26; training awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0891" num="0993"><figref idref="DRAWINGS">FIG. <b>141</b></figref> is a logic diagram of an example of an analysis system determining an awareness &#x26; training evaluation rating for a system, or portion thereof. The method begins at step <b>1126</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for an awareness &#x26; training evaluation. An awareness &#x26; training evaluation includes evaluating the system's awareness &#x26; training of the system including one or more user training established and/or executed, one or more user privileges established, one or more third party responsibilities established, one or more executive responsibilities established, and/or one or more IT and/or security responsibilities established.</p><p id="p-0892" num="0994">The method continues at step <b>1127</b> where the analysis system determines at least one evaluation perspective for use in performing the awareness &#x26; training evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions are understood with respect to awareness &#x26; training. An implementation perspective is with regard to how well the assets, system functions, and/or security functions are implemented with respect to awareness &#x26; training. An operation perspective is with regard to how well the assets, system functions, and/or security functions operate with respect to awareness &#x26; training. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understating, implementation, and/or operation of assets, system functions, and/or security functions with respect to awareness &#x26; training.</p><p id="p-0893" num="0995">The method continues at step <b>1128</b> where the analysis system determines at least one evaluation viewpoint for use in performing the awareness &#x26; training evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0894" num="0996">The method continues at step <b>1129</b> where the analysis system obtains awareness &#x26; training data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Awareness &#x26; training data is data obtained that is regarding the system aspect.</p><p id="p-0895" num="0997">The method continues at step <b>1130</b> where the analysis system calculates an awareness &#x26; training rating as a measure of system awareness &#x26; training maturity for the system aspect based on the awareness &#x26; training data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0896" num="0998"><figref idref="DRAWINGS">FIG. <b>142</b></figref> is a logic diagram of another example of an analysis system determining an identification rating for a system, or portion thereof. The method begins at step <b>1140</b> where the analysis system selects a system, or portion thereof, to evaluate organizational data security awareness of the system, or portion thereof. For example, the analysis system selects one or more system elements, one or more system criteria, and/or one or more system modes for the system, or portion thereof, to be evaluated. As another example, the analysis system selects one more evaluation perspective and/or one or more evaluation viewpoints. The analysis system may further select one or more sub-sub categories of the data security sub-category of the identify category.</p><p id="p-0897" num="0999">As another example of selecting the system or portion thereof, the analysis system selects the entire system, selects a division of an organization operating the system, selects a department of a division, selects a group of a department, or selects a sub-group of a group. As another example selecting the system or portion thereof, the analysis system selects one or more physical assets and/or one or more conceptual assets with respect to data security.</p><p id="p-0898" num="1000">The method continues at step <b>1141</b> where the analysis system obtains organization data security awareness information regarding the system, or portion thereof. The organization data security awareness information includes information representative of an organization's understanding of the system, or portion thereof, with respect to the data security (e.g., security of data at rest, security of data in transit, one or more formal asset management protocols, system has adequate capacity, one or more data leak prevention protocols, one or more integrity checking protocols, and/or separation of use from development). In an example, the analysis system obtains the organization data security awareness information (e.g., disclosed data from the system) by receiving it from a system admin computing entity. In another example, the analysis system obtains the organization data security awareness information by gathering it from one or more computing entities of the system.</p><p id="p-0899" num="1001">The method continues at step <b>1142</b> where the analysis system engages with the system, or portion thereof, to produce system data security awareness data (e.g., discovered data) regarding the system, or portion thereof, with respect to the data security. Engaging the system, or portion thereof, was discussed with reference to <figref idref="DRAWINGS">FIG. <b>119</b></figref>.</p><p id="p-0900" num="1002">The method continues at step <b>1143</b> where the analysis system calculates a data security awareness rating regarding the organizational data security awareness of the system, or portion thereof, based on the organization data security awareness information, the system data security awareness data, and awareness processes, awareness policies, awareness documentation, and/or awareness automation. The data security awareness rating may be indicative of a variety of factors of the system, or portion thereof. For example, the data security awareness rating indicates how well the organization data security awareness information reflects an understanding of the data security of the system, or portion thereof. As another example, the data security awareness rating indicates how well the organization data security awareness information reflects an understanding of how well the data security support the system functions of system, or portion thereof.</p><p id="p-0901" num="1003">As another example, the data security awareness rating indicates how well the organization data security awareness information reflects an understanding of how well the data security supports the security functions of the system, or portion thereof. As another example, the data security awareness rating indicates how well the organization data security awareness information reflects intended implementation of the data security of the system, or portion thereof. As another example, the data security awareness rating indicates how well the organization data security awareness information reflects intended operation of the data security of the system, or portion thereof.</p><p id="p-0902" num="1004">As another example, the data security awareness rating indicates how well the organization data security awareness information reflects intended implementation of the system functions of the system, or portion thereof. As another example, the data security awareness rating indicates how well the organization data security awareness information reflects intended operation of the data security with respect to the system functions of system, or portion thereof.</p><p id="p-0903" num="1005">As another example, the data security awareness rating indicates how well the organization data security awareness information reflects intended implementation of the data security to support security functions of the system, or portion thereof. As another example, the data security awareness rating indicates how well the organization data security awareness information reflects intended operation of the data security to support the security of the system, or portion thereof.</p><p id="p-0904" num="1006">The method continues at step <b>1144</b> where the analysis system gathers desired system awareness data from one or more system proficiency resources. The method continues at step <b>1145</b> where the analysis system calculates a second data security awareness rating regarding a desired level of organizational data security awareness of the system, or portion thereof, based on the organization data security awareness information, the system awareness data, the desired system awareness data, and the awareness processes, the awareness policies, the awareness documentation, and/or the awareness automation. The second data security awareness rating is regarding a comparison of desired data with the disclosed data and/or discovered data.</p><p id="p-0905" num="1007"><figref idref="DRAWINGS">FIG. <b>143</b></figref> is a logic diagram of an example of an analysis system determining a data security evaluation rating for a system, or portion thereof. The method begins at step <b>1146</b> where the analysis system determines a system aspect (see <figref idref="DRAWINGS">FIG. <b>69</b></figref>) of a system for a data security evaluation. A data security evaluation includes evaluating the system's data security of the system including security of data at rest, security of data in transit, one or more formal asset management protocols, system has adequate capacity, one or more data leak prevention protocols, one or more integrity checking protocols, and/or separation of use from development.</p><p id="p-0906" num="1008">The method continues at step <b>1147</b> where the analysis system determines at least one evaluation perspective for use in performing the data security evaluation on the system aspect. An evaluation perspective is an understanding perspective, an implementation perspective, an operation perspective, or a self-analysis perspective. An understanding perspective is with regard to how well the assets, system functions, and/or security functions are understood with respect to data security. An implementation perspective is with regard to how well the assets, system functions, and/or security functions are implemented with respect to data security. An operation perspective is with regard to how well the assets, system functions, and/or security functions operate with respect to data security. A self-analysis (or self-evaluation) perspective is with regard to how well the system self-evaluates the understating, implementation, and/or operation of data security, system functions, and/or security functions.</p><p id="p-0907" num="1009">The method continues at step <b>1148</b> where the analysis system determines at least one evaluation viewpoint for use in performing the data security evaluation on the system aspect. An evaluation viewpoint is disclosed viewpoint, a discovered viewpoint, or a desired viewpoint. A disclosed viewpoint is with regard to analyzing the system aspect based on the disclosed data. A discovered viewpoint is with regard to analyzing the system aspect based on the discovered data. A desired viewpoint is with regard to analyzing the system aspect based on the desired data.</p><p id="p-0908" num="1010">The method continues at step <b>1149</b> where the analysis system obtains data security data regarding the system aspect in accordance with the at least one evaluation perspective and the at least one evaluation viewpoint. Data security data is data obtained that is regarding the system aspect.</p><p id="p-0909" num="1011">The method continues at step <b>1150</b> where the analysis system calculates a data security rating as a measure of system data security maturity for the system aspect based on the data security data, the at least one evaluation perspective, the at least one evaluation viewpoint, and at least one evaluation rating metric. An evaluation rating metric is a process rating metric, a policy rating metric, a procedure rating metric, a certification rating, a documentation rating metric, or an automation rating metric.</p><p id="p-0910" num="1012"><figref idref="DRAWINGS">FIG. <b>144</b></figref> is a logic diagram illustrating an example of an analysis system collecting data from physical assets of a system and evaluating collected data to produce an output in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>144</b></figref> may be performed by the analysis system <b>10</b> upon the system <b>11</b> being evaluated as described previously herein. Further, the particular operations of <figref idref="DRAWINGS">FIG. <b>144</b></figref> may be performed upon the system <b>11</b> being evaluated by the modules described further herein with reference to <figref idref="DRAWINGS">FIG. <b>35</b></figref>, for example. The system <b>11</b> being evaluated may be generally contained in one general physical location or may be dispersed over a number of physical areas. Further, the system <b>11</b> may include discrete physical components and/or may have some components that are distributed within a physical location and/or distributed via cloud computing. In any of these embodiments, the system <b>11</b> includes physical assets as previously described herein. The operations described with reference to <figref idref="DRAWINGS">FIG. <b>144</b></figref> may be performed on all or a portion of the system <b>11</b> under evaluation by the analysis system <b>10</b>.</p><p id="p-0911" num="1013">The operations of <figref idref="DRAWINGS">FIG. <b>144</b></figref> begin with the analysis system determining a system aspect, an evaluation aspect, and an evaluation viewpoint for a system for evaluation (step <b>1200</b>). With one embodiment that will be described in detail herein, operation <b>1200</b> includes determining, by an analysis system that includes one or more computing entities, a system aspect of a system, the system aspect corresponding to evaluation of physical assets of the system, determining, by the analysis system, an evaluation perspective for use in performing the asset management evaluation on the system aspect, the evaluation perspective relating to a build of the system, and determining, by the analysis system, an evaluation viewpoint for use in performing the asset management evaluation on the system aspect, the evaluation viewpoint corresponding to discovered information of the system.</p><p id="p-0912" num="1014">Operations continue with, based upon the system aspect, the evaluation perspective, and the evaluation viewpoint, selecting, by the analysis system, a plurality of data structures identifying data to be collected (step <b>1202</b>). Examples of these data structures are described further herein with reference to <figref idref="DRAWINGS">FIGS. <b>146</b> through <b>153</b></figref> herein. Examples of operations for identifying physical assets from which data to populate the data structures, querying the identified physical assets, and retrieving data from the physical assets to populate the data structures are described with reference to <figref idref="DRAWINGS">FIGS. <b>145</b>A through <b>145</b>E</figref> herein. Next, operations include, based upon the plurality of data structures, the analysis system identifying a plurality of physical assets of the system for collection of data (step <b>1204</b>). Operations continue with the analysis system querying the plurality of physical assets of the system based upon the plurality of data structures (step <b>1206</b>) and retrieving data from the physical assets of the system to populate the plurality of data structures (step <b>1208</b>).</p><p id="p-0913" num="1015">From step <b>1210</b>, the operations of <figref idref="DRAWINGS">FIG. <b>144</b></figref> include, based upon the system aspect, the evaluation perspective, the evaluation viewpoint, and the plurality of data structures, the analysis system determining context data (step <b>1210</b>). Step <b>1210</b> may also include retrieving/creating the context data based upon the identified physical assets determined at step <b>1204</b> and/or the retrieved data used to populate the data structures at step <b>1208</b>. Examples of retrieving and creating the context data have been previously described herein (also referred to as data analysis parameters <b>265</b> and evaluation parameters <b>266</b> referred to in <figref idref="DRAWINGS">FIG. <b>35</b></figref>) and as will be described further herein with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0914" num="1016">The operations of <figref idref="DRAWINGS">FIG. <b>144</b></figref> continue with evaluating, by the analysis system, the data structures containing the collected data using the context data to produce an evaluation of at least some of the plurality of physical assets of the system (step <b>1212</b>). These operations have been previously described herein and will be further described herein with reference to <figref idref="DRAWINGS">FIGS. <b>157</b> through <b>163</b></figref>.</p><p id="p-0915" num="1017">With the operations of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and subsequent Figures, the plurality of physical assets may be categorized as one or more of a computing entity, a computing device, a security device, a network device, a user software application, a system software application, a network software application, an operating system, a software tool, a network software application, a security software application, or a system monitoring software application. Generally, physical assets may be considered as hardware assets and software assets to which the system being evaluated contains and/or has access. These physical assets may be sub-divided based upon their general components and functionality as described herein. These physical assets may be solely devoted to the system being evaluated or may be shared with other systems/entities. These physical assets may be discrete assets for which a physical location may be identified, or these physical assets may be cloud based assets. Generally, these physical assets are addressable within a network corresponding to the system and have respective IP addresses and/or other communication protocol addresses. Further, each of these physical addresses may have multiple addresses for access, each respective address receiving traffic from particular users, user groups, access types, etc., for example.</p><p id="p-0916" num="1018"><figref idref="DRAWINGS">FIG. <b>145</b>A</figref> is a logic diagram illustrating an example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>145</b>A</figref> begin with identifying all physical assets of the system and collecting basic attributes of the physical assets (step <b>1330</b>), such operations being described further herein with reference to <figref idref="DRAWINGS">FIG. <b>154</b></figref>. The operations of step <b>1330</b> include creating at least one database record for each identified physical asset and storing basic attribute data for the respective physical asset. An example of contents stored for a physical asset are illustrated in <figref idref="DRAWINGS">FIG. <b>146</b></figref>. Step <b>1330</b> may be accomplished by IP interrogation of the system and/or via collecting data from a system administrator.</p><p id="p-0917" num="1019">The operations of <figref idref="DRAWINGS">FIG. <b>145</b>A</figref> proceed with selecting a first/next physical asset of the system for consideration (step <b>1332</b>). Then, the basic attributes of the selected physical asset are retrieved from one or more database records (step <b>1334</b>). Next, it is determined whether the physical asset is relevant to at least one selected data structure (step <b>1336</b>). Recall that data structures were selected at Step <b>1202</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref>. Step <b>1336</b> may be accomplished by comparing the basic attributes of the selected physical asset to the content of each of the selected data structures. If the comparison at step <b>1336</b> is favorable the physical asset is selected for data collection (step <b>1338</b>). From step <b>1338</b> and upon a non-favorable comparison at step <b>1336</b>, operations proceed to step <b>1340</b> where it is determined whether all physical assets of the system have been considered. If not, the next physical asset of the system is selected at step <b>1332</b>. If so, operations end.</p><p id="p-0918" num="1020"><figref idref="DRAWINGS">FIG. <b>145</b>B</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure. With the embodiment of <figref idref="DRAWINGS">FIG. <b>145</b>B</figref>, operations proceed with selecting a first/next data structure from all allocated data structures (step <b>1350</b>). Note that data structures were selected for allocation at step <b>1202</b> if they were relevant to the evaluation being performed by the analysis system. These selected data structures are identified as templates to be filled by data collected from the physical assets. Note that multiple copies of the data structures may be selected and allocated as they relate to multiple physical assets with each allocated data structure populated by data collected from a respective physical asset.</p><p id="p-0919" num="1021">From step <b>1350</b> operations of <figref idref="DRAWINGS">FIG. <b>145</b>B</figref> include selecting a first/next physical asset of the system (step <b>1352</b>). Note that in prior operations a sub-set of all physical assets of the system may have been selected. Next, the basic attributes of the selected physical asset are retrieved (step <b>1352</b>). These basic attributes were previously collected from the physical assets or identified based upon documents received from an owner/operator of the system. Next it is considered whether the physical asset is relevant to the selected data structure (step <b>1356</b>). If not, operations return to step <b>1352</b>. If so, operations include allocating a copy of the data structure for the selected physical asset of the system (step <b>1358</b>). Next, the selected physical asset is queried for data relevant to the selected data structure (step <b>1360</b>). The analysis system then receives relevant data from the physical asset (step <b>1362</b>) and populates the allocated data structure with the received data (step <b>1364</b>).</p><p id="p-0920" num="1022">From step <b>1364</b> the analysis system determines whether it is done considering all physical assets of the system (step <b>1366</b>). If not, operations return to step <b>1352</b> where a next physical asset is selected for consideration. If so, operations proceed to step <b>1368</b> where the analysis system determines whether it is done considering all data structures. In not, operations proceed to step <b>1350</b> where a next data structure is selected. If so, operations end.</p><p id="p-0921" num="1023"><figref idref="DRAWINGS">FIG. <b>145</b>C</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>145</b>C</figref> include first selecting a first/next physical asset of the system (step <b>1370</b>). Note that in prior operations a sub-set of all physical assets of the system may have been selected. Next, the basic attributes of the selected physical asset are retrieved (step <b>1372</b>). These basic attributes were previously collected from the physical assets or identified based upon documents received from an owner/operator of the system.</p><p id="p-0922" num="1024">Operations proceed with selecting a first/next data structure from all allocated considered relevant to the evaluation (step <b>1374</b>). Note that data structures were selected at step <b>1202</b> if they were relevant to the evaluation being performed by the analysis system. These selected data structures are identified as templates to be filled by data collected from the physical assets. Note that multiple copies of the data structures may be selected and allocated as they relate to multiple physical assets with each allocated data structure populated by data collected from a respective physical asset.</p><p id="p-0923" num="1025">Next it is considered whether the physical asset is relevant to the selected data structure (step <b>1376</b>). If not, operations return to step <b>1374</b>. If so, operations include allocating a copy of the data structure for the selected physical asset of the system (step <b>1378</b>). Next, the analysis system queries the physical asset for data relevant to the selected data structure (step <b>1380</b>). The analysis system then receives relevant data from the physical asset (step <b>1382</b>) and populates the allocated data structure with the received data (step <b>1384</b>).</p><p id="p-0924" num="1026">From step <b>1384</b> the analysis system determines whether it is done considering all identified data structures (step <b>1386</b>). If not, operations return to step <b>1374</b> where a next data structure is selected for consideration. If so, operations proceed to step <b>1388</b> where the analysis system determines whether it is done considering all physical assets. In not, operations proceed to step <b>1370</b> where a next data structure is selected. If so, operations end.</p><p id="p-0925" num="1027"><figref idref="DRAWINGS">FIG. <b>145</b>D</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>145</b>D</figref> include first selecting a first/next physical asset of the system (step <b>1390</b>). Note that in prior operations a sub-set of all physical assets of the system may have been selected. Next, the basic attributes of the selected physical asset are retrieved (step <b>1392</b>). These basic attributes were previously collected from the physical assets or identified based upon documents received from an owner/operator of the system.</p><p id="p-0926" num="1028">Operations proceed with selecting a first/next data structure from all allocated data structures (step <b>1394</b>). Note that data structures selected at step <b>1202</b> if they were relevant to the evaluation being performed by the analysis system. These selected data structures are identified as templates to be filled by data collected by the physical assets. Note that multiple copies of the data structures may be selected and allocated as they relate to multiple physical assets with each allocated data structure populated by data collected from a respective physical asset.</p><p id="p-0927" num="1029">Next it is considered whether the physical asset is relevant to the selected data structure (step <b>1396</b>). If not, operations return to step <b>1394</b>. If so, operations include allocating a copy of the data structure for the selected physical asset of the system (step <b>1398</b>). From step <b>1384</b> the analysis system determines whether it is done considering all data structures (step <b>1400</b>). If not, operations return to step <b>1394</b> where a next data structure is selected for consideration. If so, operations proceed to step <b>1402</b> where the analysis system determines whether it is done considering all physical assets. If not, operations proceed to step <b>1390</b> where a next data structure is selected. If so, operations end.</p><p id="p-0928" num="1030"><figref idref="DRAWINGS">FIG. <b>145</b>E</figref> is a logic diagram illustrating another example of an analysis system collecting data from physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>145</b>E</figref> include first selecting a first/next physical asset of the system for which at least one data structure is allocated (step <b>1410</b>). Note that in prior operations a sub-set of all physical assets of the system may have been selected. Next, data to be collected for all allocated data structures corresponding to the selected physical asset is accumulated (step <b>1142</b>). Note that step <b>1142</b> considers that data structures were previously allocated to the physical asset in prior operations. Also note that some of the data required to be retrieved from the physical asset may apply to multiple data structures.</p><p id="p-0929" num="1031">Next, the selected physical asset is queried for data relevant to the selected data structure(s) (step <b>1414</b>). The analysis system then receives relevant data from the physical asset (step <b>1416</b>) and populates the allocated data structure(s) with the received data (step <b>1418</b>). Next the analysis system determines whether it is done considering all physical assets (step <b>1420</b>). If not, operations proceed to step <b>1410</b> where a physical asset is selected. If so, operations end.</p><p id="p-0930" num="1032"><figref idref="DRAWINGS">FIGS. <b>146</b> through <b>153</b></figref> illustrate examples of data structures used for storing data of physical assets that are relevant to the evaluation of the system. Structure and content of these data structures illustrated are examples only. Data structures may include much more detail and have differing structures, depending upon particular embodiments in use and the data structures illustrated herein are not intended to limit the scope of the present disclosure in any manner.</p><p id="p-0931" num="1033"><figref idref="DRAWINGS">FIG. <b>146</b></figref> is a block diagram illustrating an example of a data structure storing data corresponding to a physical asset in accordance with the present disclosure. The data structure includes basic attributes of the physical asset which may include one or more IP addresses <b>1500</b>, one or more system IDs <b>1502</b>, one or more device IDs <b>1504</b>, one or more device types <b>1506</b>, one or more device statuses <b>1508</b>, one or more manufacturers <b>1510</b>, one or more models <b>1512</b>, one or more sub-models <b>1514</b>, one or more firmware versions <b>1516</b>, one or more dates of manufacture <b>1518</b>, one or more locations of manufacture <b>1520</b>, and one or more versions <b>1522</b>, for example. Of course, the data structure in which the basic attributes of the physical asset may be stored may include more or less data, may include more or fewer items, and may include differing information regarding the physical asset.</p><p id="p-0932" num="1034">The data structure may further be associated/linked to one or more additional data structures referring to additional aspects of the physical asset, one or more physical assets loaded thereon, linked thereto or otherwise associated with the physical asset. These additional data structures may include general hardware data <b>1524</b>, user device/system device hardware data <b>1526</b>, security device hardware data <b>1528</b>, network device hardware data <b>1530</b>, system software data <b>1532</b>, user software data <b>1534</b>, security device software data <b>1536</b>, network device software data <b>1538</b>, user permissions data <b>1540</b>, departments permission data <b>1542</b>, network routing data <b>1544</b>, security access data <b>1546</b> and edge device access data <b>1548</b>, for example.</p><p id="p-0933" num="1035">Note that each data structure generally refers the characterization of the physical asset. Thus, for example, a network device may include the basic attributes data and the network device hardware data <b>1530</b>. Further, because it is a network device, it may also include network device software data <b>1538</b>, which is running on the network device. As another example, for other physical assets, e.g., system software data running in the cloud, the physical asset would include the basic attributes data as well as system software data <b>1532</b>. The basic attributes may not include any information regarding the hardware upon which the system software is running and only data regarding access of the system software data.</p><p id="p-0934" num="1036"><figref idref="DRAWINGS">FIG. <b>147</b></figref> is a block diagram illustrating an example of a data structure storing hardware data corresponding to a physical asset in accordance with the present disclosure. The data structure illustrated in <figref idref="DRAWINGS">FIG. <b>147</b></figref> is somewhat generic and could refer to any type of hardware asset of the system being evaluated. For example, a server computer includes multiple components, including processors, memory, network ports, etc. Thus, for each of these components, the data structure may include a plurality of included data structures (may be referred to as sub-data structures herein). A data structure relating to hardware component A <b>1600</b>A may have fields corresponding to a component type <b>1602</b>, a component ID, a component manufacturer <b>1606</b>, a processing capacity <b>1608</b>, a clock frequency <b>1610</b>, a date in service <b>1612</b>, storage A <b>1614</b>, storage B <b>1616</b>, storage N <b>1618</b>, a firmware version <b>1620</b>, a firmware update data <b>1622</b>, a time in service <b>1624</b>, a serial number <b>1626</b>, a maximum operating temperature <b>1628</b>, and an average operating temperature <b>1630</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>147</b></figref> may include sub-data structures corresponding to hardware component B <b>1600</b>B, hardware component C <b>1600</b>C and hardware component N <b>1600</b>N.</p><p id="p-0935" num="1037"><figref idref="DRAWINGS">FIG. <b>148</b></figref> is a block diagram illustrating an example of a data structure storing software data corresponding to a physical asset in accordance with the present disclosure. The data structure illustrated in <figref idref="DRAWINGS">FIG. <b>148</b></figref> is somewhat generic and could refer to any type of software that is run on a hardware device, e.g., multiple software components may run on a single hardware device and may be grouped together, group of hardware device, or a cloud computing installation. A data structure relating to software component A <b>1650</b>A may have fields corresponding to a software type <b>1652</b>, a software license number <b>1654</b>, a software manufacturer <b>1656</b>, an original version <b>1658</b>, a latest version update <b>1660</b>, a date in service <b>1662</b>, maximum users <b>1664</b>, current users <b>1666</b>, available users <b>1668</b>, a first usage <b>1670</b>, a last usage <b>1672</b>, a percentage of time in use <b>1674</b>, permissions list <b>1676</b>, a user list <b>1678</b>, and a department list <b>1680</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>148</b></figref> may include sub-data structures corresponding to software component B <b>1650</b>B, software component C <b>1650</b>C and software component N <b>1650</b>N.</p><p id="p-0936" num="1038"><figref idref="DRAWINGS">FIG. <b>149</b></figref> is a block diagram illustrating an example of a data structure storing network hardware data corresponding to a physical asset in accordance with the present disclosure. A data structure relating to hardware component A <b>1700</b>A may have fields corresponding to a component type <b>1702</b>, a component ID <b>1704</b>, a component manufacturer <b>1706</b>, a processing capacity <b>1708</b>, storage capacity <b>1710</b>, a date in service <b>1712</b>, firmware version <b>1714</b>, serial number <b>1716</b>, a time in service <b>1724</b>, percentage of time operational <b>1726</b>, a maximum operating temperature <b>1728</b>, and an average operating temperature <b>1730</b>. The data structure may also include descriptions of differing traffic categories, including traffic category 1 <b>1732</b>, traffic category 2 <b>1734</b> and traffic category N <b>1736</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>149</b></figref> may also include characteristics of multiple ports. For each port, the data structure may include a port address <b>1738</b>, communication standards supported by the port <b>1740</b>, cache capacity the port <b>1742</b>, connectivity of the port <b>1744</b>, permissions of the port <b>1746</b>, and usage of the port <b>1748</b>. The data structure may also include one or more routing tables <b>1750</b>, network management data <b>1752</b>, and remote access data <b>1754</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>149</b></figref> may include sub-data structures corresponding to network hardware component B <b>1700</b>B, network hardware component C <b>1700</b>C and network hardware component N <b>1700</b>N.</p><p id="p-0937" num="1039"><figref idref="DRAWINGS">FIG. <b>150</b></figref> is a block diagram illustrating an example of a data structure storing network software data corresponding to a physical asset in accordance with the present disclosure. A data structure relating to network software component A <b>1760</b>A may have fields corresponding to a software type <b>1762</b>, a license ID <b>1764</b>, a software manufacturer <b>1766</b>, an original version <b>1768</b>, a latest update version <b>1760</b>, a date in service <b>1762</b>, data throughput limits <b>1764</b>, average throughput <b>1766</b>, available throughput <b>1768</b>, data storage limits <b>1770</b>, usage data <b>1772</b>, availability <b>1774</b>, a permissions list <b>1776</b>, a user list <b>1778</b>, department list <b>1780</b>, security types supported <b>1782</b>, security settings <b>1784</b>, and security exception lists <b>1780</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>150</b></figref> may include sub-data structures corresponding to network software component B <b>1760</b>B, network software component C <b>1760</b>C and network software component N <b>1760</b>N. For example, these multiple network software components <b>1760</b>A, <b>1760</b>B, <b>1760</b>C, and <b>1760</b>D may reside on a single network hardware device.</p><p id="p-0938" num="1040"><figref idref="DRAWINGS">FIG. <b>151</b></figref> is a block diagram illustrating an example of a data structure storing permissions data corresponding to a physical asset in accordance with the present disclosure. With the example of <figref idref="DRAWINGS">FIG. <b>151</b></figref>, there may be multiple permissions components with permissions corresponding to a software asset, a hardware asset, or other physical asset. Component A permissions <b>1800</b>A include a permissions owner <b>1802</b>, users having authority to modify the permissions <b>1804</b>, and permissions relating to different departments, different groups of users, and external users. The department 1 and X permissions include department identifiers <b>1806</b> and <b>1812</b>, permissions corresponding to the department identities <b>1808</b> and <b>1814</b>, and expirations of the permissions <b>1810</b> and <b>1816</b>, respectively. Likewise, the group 1 and X users' permissions include group identifiers <b>1818</b> and <b>1824</b>, permissions corresponding to the user group identities <b>1820</b> and <b>1826</b>, and expirations of the permissions <b>1822</b> and <b>1828</b>. Further, the users' permissions may include external users' identifiers <b>1830</b>, permissions corresponding to the external users' identities <b>1832</b>, and expirations of the external users' permissions <b>1834</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>151</b></figref> may include sub-data structures corresponding to component B permissions <b>1800</b>B, component C permissions <b>1800</b>C and component N permissions <b>1760</b>N. Note that any of these permissions sub-data structures may relate to any other physical asset, e.g., hardware asset, software asset, data storage asset, etc.</p><p id="p-0939" num="1041"><figref idref="DRAWINGS">FIG. <b>152</b></figref> is a block diagram illustrating an example of a data structure storing security hardware data corresponding to a physical asset in accordance with the present disclosure. A data structure relating to security hardware component A <b>1850</b>A may have fields corresponding to a component type <b>1852</b>, a component ID <b>1854</b>, a component manufacturer <b>1856</b>, processing capacity <b>1858</b>, storage capacity <b>1860</b>, a date in service <b>1862</b>, firmware version <b>1864</b>, firmware update date <b>1866</b>, time in service <b>1874</b>, percentage of time operational <b>1876</b>, a maximum operating temperature <b>1878</b>, and an average operating temperature <b>1880</b>. The data structure may also include descriptions of differing security protocols serviced, including security protocol 1 <b>1882</b>, security protocol 2 <b>1884</b> and security protocol N <b>1886</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>152</b></figref> may also include user table data <b>1887</b>, department table data <b>1888</b>, and security data <b>1889</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>152</b></figref> may include sub-data structures corresponding to security hardware component B <b>1850</b>B, security hardware component C <b>1850</b>C and security hardware component N <b>1850</b>N.</p><p id="p-0940" num="1042"><figref idref="DRAWINGS">FIG. <b>153</b></figref> is a block diagram illustrating an example of a data structure storing security software data corresponding to a physical asset in accordance with the present disclosure. A data structure relating to security software component A <b>1900</b>A may have fields corresponding to a security software type <b>1902</b>, a security license ID <b>1904</b>, a security software manufacturer <b>1906</b>, an original version <b>1908</b>, a latest update version <b>1900</b>, a date in service <b>1912</b>, supported security protocol 1 <b>1914</b>, supported security protocol 2 <b>1916</b>, supported security protocol N <b>1918</b>, data storage limits <b>1920</b>, usage data <b>1922</b>, availability <b>1924</b>, permissions list <b>1926</b>, a user list <b>1928</b>, department list <b>1930</b>, security types supported <b>1932</b>, security settings <b>1934</b>, and security exception lists <b>1936</b>. The data structure of <figref idref="DRAWINGS">FIG. <b>153</b></figref> may include sub-data structures corresponding to security software component B <b>1900</b>B, security software component C <b>1900</b>C and security software component N <b>1900</b>N. For example, these multiple security software components <b>1900</b>A, <b>1900</b>B, <b>1900</b>C, and <b>1900</b>D may reside on a single security hardware device.</p><p id="p-0941" num="1043"><figref idref="DRAWINGS">FIG. <b>154</b></figref> is a logic diagram illustrating an example of an analysis system discovering and collecting basic attribute data from physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>154</b></figref> begin with retrieving network data regarding the system to be evaluated (step <b>1950</b>). This network data could be one or more IP addresses ranges that may be used to access physical assets to be evaluated. Operations proceed with selecting a first/next IP address (step <b>1952</b>) attempting to address a physical address at the IP address (step <b>1954</b>). Next, it is determined whether a physical asset exists (is addressable) at the selected IP address (step <b>1956</b>). If there is a physical asset addressable at the IP address, operation proceeds to step <b>1958</b> wherein basic attributes of the physical asset at the selected IP address are retrieved (step <b>1958</b>). Then, a data record for the physical asset at the selected IP address is created and populated with the retrieved basic attributes (step <b>1960</b>). From a negative determination at step <b>1956</b> and from step <b>1960</b>, operation proceeds to step <b>1962</b>, which determines whether the address range of the system to be evaluated has been exhausted. If not, operations return to step <b>1952</b>. If so, operation ends.</p><p id="p-0942" num="1044"><figref idref="DRAWINGS">FIG. <b>155</b></figref> is a logic diagram illustrating an example of an analysis system collecting system design data and reference data relevant to physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>155</b></figref> are shown together as a group for brevity in description. These operations may be singly, partially, or fully performed with differing aspects of the present disclosure. The types of data retrieved in the operations of <figref idref="DRAWINGS">FIG. <b>155</b></figref> have been previously considered herein and relate directly to the data described herein with reference to <figref idref="DRAWINGS">FIGS. <b>146</b>-<b>153</b></figref>. The retrieved information relates to the structure and operation of the physical assets being considered, which may be compared to or used to evaluate the data collected for the physical assets.</p><p id="p-0943" num="1045">The operations of <figref idref="DRAWINGS">FIG. <b>155</b></figref> begin with retrieving system design data that is stored within the system being evaluated (step <b>2000</b>). The analysis system may discover the system design data in storage of the system to be evaluated or may be directed thereto by the system operator owner. Alternatively, the system operator or owner may deliver the system design data to the analysis system. Operations proceed with the analysis system retrieving reference information regarding identified hardware assets (step <b>2002</b>). Reference information includes, for example, manufacturer data, industry standard data, governmental requirements, age data, relative age data, firmware update data, etc.</p><p id="p-0944" num="1046">Next, operations include retrieving reference information regarding identified software data (step <b>2004</b>), which may include software version information, patch information, performance information, industry usage information, etc. Operations then include retrieving reference information regarding identified network hardware assets (step <b>2006</b>), which may include industry standards information, data throughput requirements, governmental requirements, etc. Next, operations include retrieving reference information regarding identified network hardware assets (step <b>2008</b>). Operations also include retrieving reference information regarding identified network software assets (step <b>2010</b>), retrieving information regarding identified security hardware assets (step <b>2012</b>), retrieving reference information regarding identified security software assets (step <b>2012</b>), and retrieving information regarding permissions (step <b>2106</b>).</p><p id="p-0945" num="1047"><figref idref="DRAWINGS">FIG. <b>156</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>156</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and are generic to the physical assets of the system being evaluated. Operations begin with the system selecting a first/next data structure of a plurality of data structures allocated for the system being evaluated (step <b>2020</b>). As may be recalled, data structures are allocated based upon the evaluation being performed. Then, multiple instances of this data structure may be allocated, one or more allocations made for a particular physical asset under consideration. Operations proceed to retrieving context data relevant to the selected data structure (step <b>2022</b>). The retrieved data corresponds to the fields of the data structure that is being considered. The collection of the context data was previously described with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0946" num="1048">Operation proceeds with selecting a first/next populated data structure of the selected data structure (step <b>2024</b>). Next, the fields of the populated data structure are evaluated based upon the context data to produce output (step <b>2026</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2026</b> is then recorded for the selected populated data structure (step <b>2028</b>).</p><p id="p-0947" num="1049">It is next determined whether the selected populated data structure is the last of the populated data structures (step <b>2030</b>). If not, operations return to step <b>2024</b> where a next populated data structure of the selected data structure is selected. If so, operation proceeds to step <b>2032</b> where it is determined whether all data structures of the plurality of data structures have been considered. If not, operation returns to step <b>2020</b> where a next data structure of the plurality of data structures is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2034</b>). From step <b>2034</b>, operation ends.</p><p id="p-0948" num="1050"><figref idref="DRAWINGS">FIG. <b>157</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for hardware assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>157</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to hardware assets of the physical assets of the system being evaluated. Operations begin with the system selecting a first/next hardware data structure of the plurality hardware data structures allocated for the system being evaluated (step <b>2020</b>). As may be recalled, hardware data structures are allocated based upon the evaluation being performed. Then, multiple instances of this hardware data structure may be allocated, one or more allocations made for a particular hardware asset under consideration. Operations proceed to retrieving context data relevant to the selected hardware data structure (step <b>2052</b>). The retrieved context data relates to corresponding fields of the hardware data structure that is being considered. The collection of the context data was previously described with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0949" num="1051">Operation proceeds with selecting a first/next populated hardware data structure of the selected hardware data structure (step <b>2054</b>). Next, the first/next field of the populated hardware data structure is selected (step <b>2056</b>) and then evaluated based upon the context data to produce output (step <b>2058</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2058</b> is recorded for the populated hardware data structure (also at step <b>2058</b>). It is next determined whether all fields of the populated hardware data structure have been considered (step <b>2060</b>). If not, operations return to step <b>2056</b>. If so, it is next determined whether the selected populated hardware data structure is the last of the selected populated hardware data structures (step <b>2062</b>). If not, operations return to step <b>2054</b> where a next populated hardware data structure is selected. If so, operation proceeds to step <b>2064</b> where it is determined whether all hardware data structures have been considered. If not, operation returns to step <b>2050</b> where a next hardware data structure of the plurality of hardware data structures is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2066</b>). From step <b>2066</b>, operation ends.</p><p id="p-0950" num="1052"><figref idref="DRAWINGS">FIG. <b>158</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for software assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>158</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to software assets of the physical assets of the system being evaluated. Operations begin with the system selecting a first/next software data structure of the plurality of allocated software data structures for the system being evaluated (step <b>2070</b>). As may be recalled, software data structures are allocated based upon the evaluation being performed. Then, multiple instances of this software data structure may be allocated, one or more allocations made for a particular software asset under consideration. Operations proceed to retrieving context data relevant to the selected software data structure (step <b>2072</b>). The retrieved data corresponds to the fields of the software data structure that is being considered. The collection of the context data was previously described with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0951" num="1053">Operation proceeds with selecting a first/next populated software data structure of the selected software data structure (step <b>2074</b>). Next, the first/next field of the populated software data structure is selected (step <b>2076</b>) and then evaluated based upon the context data to produce output (step <b>2078</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2078</b> is recorded for the populated data structure (also at step <b>2078</b>). It is next determined whether all fields of the populated software data structure have been considered (step <b>2080</b>). If not, operations return to step <b>2076</b>. If so, it is next determined whether the selected populated software data structure is the last of the selected software data structure (step <b>2082</b>). If not, operations return to step <b>2074</b> where a next populated software data structure is selected. If so, operation proceeds to step <b>2084</b> where it is determined whether all software data structures have been considered. If not, operation returns to step <b>2070</b> where a next software data structure of the plurality of software data structures is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2086</b>). From step <b>2086</b>, operation ends.</p><p id="p-0952" num="1054"><figref idref="DRAWINGS">FIG. <b>159</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for network assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>159</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to network assets of the physical assets of the system being evaluated. The network assets may include network hardware assets and/or network software assets. Operations begin with the system selecting a first/next network asset data structure of a plurality of network asset data structures allocated for the system being evaluated (step <b>2090</b>). As may be recalled, network asset data structures are allocated based upon the evaluation being performed. Then, multiple instances of this network data structure may be allocated, one or more allocations made for a particular network asset under consideration. Operations proceed to retrieving context data relevant to the selected network asset data structure (step <b>2092</b>). The retrieved data corresponds to the fields of the network asset data structure that is being considered. The collection of the context data was previously described with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0953" num="1055">Operation proceeds with selecting a first/next populated network asset data structure of the selected network asset data structure (step <b>2094</b>). Next, the first/next field of the populated network asset data structure is selected (step <b>2096</b>) and then evaluated based upon the context data to produce output (step <b>2098</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2098</b> is recorded for the populated data structure (also at step <b>2098</b>). It is next determined whether all fields of the populated network asset data structure have been considered (step <b>2100</b>). If not, operations return to step <b>2096</b>. If so, it is next determined whether the selected populated network asset data structure is the last of the populated network asset data structures (step <b>2102</b>). If not, operations return to step <b>2094</b> where a next populated network asset data structure is selected. If so, operation proceeds to step <b>2104</b> where it is determined whether all network asset data structures have been considered. If not, operation returns to step <b>2090</b> where a next network asset data structure of the plurality of network asset data structures is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2106</b>). From step <b>2106</b>, operation ends.</p><p id="p-0954" num="1056"><figref idref="DRAWINGS">FIG. <b>160</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for security assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>160</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to security assets of the physical assets of the system being evaluated. The security assets may include hardware security assets and/or software security assets. Operations begin with the system selecting a first/next security asset data structure of the plurality security asset data structures allocated for the system being evaluated (step <b>2110</b>). As may be recalled, security asset data structures are allocated based upon the evaluation being performed. Then, multiple instances of this security asset data structure may be allocated, one or more allocations made for a particular security asset under consideration. Operations proceed to retrieving context data relevant to the selected security asset data structure (step <b>2112</b>). The retrieved data corresponds to the fields of the security asset data structure that is being considered. The collection of the context data was previously described with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0955" num="1057">Operation proceeds with selecting a first/next populated security asset data structure of the selected security asset data structure (step <b>2114</b>). Next, the first/next field of the populated security asset data structure is selected (step <b>2116</b>) and then evaluated based upon the context data to produce output (step <b>2118</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2118</b> is recorded for the populated data structure (also at step <b>2118</b>). It is next determined whether all fields of the selected populated security asset data structure have been considered (step <b>2120</b>). If not, operations return to step <b>2116</b>. If so, it is next determined whether the selected populated security asset data structure is the last of the selected security asset data structure (step <b>2122</b>). If not, operations return to step <b>2114</b> where a next populated security data structure is selected. If so, operation proceeds to step <b>2124</b> where it is determined whether all security asset data structures have been considered. If not, operation returns to step <b>2100</b> where a next security asset data structure of the plurality of security asset data structures is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2126</b>). From step <b>2126</b>, operation ends.</p><p id="p-0956" num="1058"><figref idref="DRAWINGS">FIG. <b>161</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for permissions of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>161</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to permissions of the of the system being evaluated. The permissions may include hardware permissions, software permissions, department permissions, user group permissions, or differing types of permissions. Operations begin with the system selecting a first/next permissions data structure of a plurality of permissions data structures allocated for the system being evaluated (step <b>2130</b>). As may be recalled, permissions data structures are allocated based upon the evaluation being performed. Then, multiple instances of this permissions data structure may be allocated, one or more allocations made for a particular category of evaluation under consideration. Operations proceed to retrieving context data relevant to the selected permissions data structure (step <b>2132</b>). The retrieved data corresponds to the fields of the permissions data structure that is being considered. The collection of the context data was previously described with reference to <figref idref="DRAWINGS">FIG. <b>155</b></figref>.</p><p id="p-0957" num="1059">Operation proceeds with selecting a first/next populated permissions data structure of the selected permissions data structure (step <b>2134</b>). Next, the first/next field of the populated permissions data structure is selected (step <b>2136</b>) and then evaluated based upon the context data to produce output (step <b>2138</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2138</b> is recorded for the populated data structure (also at step <b>2138</b>). It is next determined whether all fields of the populated permissions data structure have been considered (step <b>2140</b>). If not, operations return to step <b>2136</b>. If so, it is next determined whether the selected populated permissions data structure is the last of the selected permissions data structure (step <b>2142</b>). If not, operations return to step <b>2134</b> where a next populated permissions data structure is selected. If so, operation proceeds to step <b>2144</b> where it is determined whether all permissions data structures have been considered. If not, operation returns to step <b>2130</b> where a next permissions data structure of the plurality of allocated permissions data structures is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2146</b>). From step <b>2146</b>, operation ends.</p><p id="p-0958" num="1060"><figref idref="DRAWINGS">FIG. <b>162</b></figref> is a logic diagram illustrating an example of an analysis system comparing discovered data to design data of physical assets of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>161</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to the build of the of the system being evaluated and comparing it to the design of the system being evaluated. The operations of <figref idref="DRAWINGS">FIG. <b>162</b></figref> may relate only to hardware assets, only to software assets, only to network assets, only to security assets, or any combination of these assets. Operations begin with the system selecting a first/next physical asset of the plurality of physical assets of the system (step <b>2150</b>). Operations proceed to retrieving populated data structures corresponding to the selected physical asset (step <b>2152</b>).</p><p id="p-0959" num="1061">Operation proceeds with selecting a first/next populated physical asset data structure of the populated data structures corresponding to the selected physical asset (step <b>2154</b>). Next, the first/next field of the selected populated data structure (step <b>2156</b>) and then evaluated based upon the context data to produce output (step <b>2158</b>). The comparison may be favorable, unfavorable, equal to, different than or simply the manner in which the context data relates to the retrieved data. The output data from step <b>2158</b> is recorded for the populated data structure (also at step <b>2158</b>). It is next determined whether all fields of the selected populated data structure have been considered (step <b>2160</b>). If not, operations return to step <b>2156</b>. If so, it is next determined whether the selected populated physical asset data structure is the last of the populated data structures for the selected physical asset (step <b>2162</b>). If not, operations return to step <b>2154</b> where a next populated physical asset data structure of the populated data structures for the selected physical asset is selected. If so, operation proceeds to step <b>2164</b> where it is determined whether all physical assets of the system have been considered. If not, operation returns to step <b>2150</b> where a next physical asset of the plurality of physical assets of the is selected. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2166</b>). From step <b>2166</b>, operation ends.</p><p id="p-0960" num="1062"><figref idref="DRAWINGS">FIG. <b>163</b></figref> is a logic diagram illustrating an example of an analysis system evaluating collected data for a particular resource of a system in accordance with the present disclosure. The operations of <figref idref="DRAWINGS">FIG. <b>163</b></figref> correspond to step <b>1210</b> of <figref idref="DRAWINGS">FIG. <b>144</b></figref> and relate to a particular resource for consideration, in the particular example, Software X. The particular operations of <figref idref="DRAWINGS">FIG. <b>163</b></figref> could apply to any resource or set of resources and may be repeated for each resource being considered. <figref idref="DRAWINGS">FIG. <b>163</b></figref> particularly considers whether Software X resides on the hardware assets within the system being evaluated and whether it resides there appropriately. Thus, with the analysis of <figref idref="DRAWINGS">FIG. <b>163</b></figref> hardware assets of the system that that potentially have Software X operating thereupon are considered.</p><p id="p-0961" num="1063">Operations begin with the system selecting Software X as the resource for consideration (step <b>2170</b>). Operations proceed with the analysis system identifying hardware assets having populated data structures that are relevant to Software X (step <b>2172</b>). For example, if Software X is an operating system, all hardware assets may be identified. However, if Software X is a server application that only runs on server computers, only hardware assets that are server computers would be identified. Likewise, if Software X is security software, only security hardware devices are identified. Moreover, if Software X is network software, only network hardware devices would be identified.</p><p id="p-0962" num="1064">Operation proceeds with selecting a first/next identified hardware asset of the identified relevant hardware assets (step <b>2174</b>). Next, it is determined whether the selected hardware asset data structure has Software X (step <b>2176</b>). The comparison may be favorable or unfavorable. If the comparison is favorable, the analysis system records that the selected hardware asset has Software X and the current version thereof (step <b>2178</b>). It is next determined and recorded whether the selected hardware asset should have Software X running thereupon and/or whether the selected hardware asset should have the determined version (step <b>2180</b>). From a negative determination of step <b>2176</b> and from step <b>2180</b> it is determined whether all identified assets have been considered (step <b>2182</b>) If not, operations return to step <b>2174</b>. If so, the accumulated output is used in producing evaluation output, which includes reports, scores, discrepancies, corrections, etc., which have previously been described herein (step <b>2184</b>). From step <b>2184</b>, operation ends.</p><p id="p-0963" num="1065">It is noted that terminologies as may be used herein such as bit stream, stream, signal sequence, etc. (or their equivalents) have been used interchangeably to describe digital information whose content corresponds to any of a number of desired types (e.g., data, video, speech, text, graphics, audio, etc. any of which may generally be referred to as &#x2018;data&#x2019;).</p><p id="p-0964" num="1066">As may be used herein, the terms &#x201c;substantially&#x201d; and &#x201c;approximately&#x201d; provide an industry-accepted tolerance for its corresponding term and/or relativity between items. For some industries, an industry-accepted tolerance is less than one percent and, for other industries, the industry-accepted tolerance is 10 percent or more. Other examples of industry-accepted tolerance range from less than one percent to fifty percent. Industry-accepted tolerances correspond to, but are not limited to, component values, integrated circuit process variations, temperature variations, rise and fall times, thermal noise, dimensions, signaling errors, dropped packets, temperatures, pressures, material compositions, and/or performance metrics. Within an industry, tolerance variances of accepted tolerances may be more or less than a percentage level (e.g., dimension tolerance of less than +/&#x2212;1%). Some relativity between items may range from a difference of less than a percentage level to a few percent. Other relativity between items may range from a difference of a few percent to magnitude of differences.</p><p id="p-0965" num="1067">As may also be used herein, the term(s) &#x201c;configured to&#x201d;, &#x201c;operably coupled to&#x201d;, &#x201c;coupled to&#x201d;, and/or &#x201c;coupling&#x201d; includes direct coupling between items and/or indirect coupling between items via an intervening item (e.g., an item includes, but is not limited to, a component, an element, a circuit, and/or a module) where, for an example of indirect coupling, the intervening item does not modify the information of a signal but may adjust its current level, voltage level, and/or power level. As may further be used herein, inferred coupling (i.e., where one element is coupled to another element by inference) includes direct and indirect coupling between two items in the same manner as &#x201c;coupled to&#x201d;.</p><p id="p-0966" num="1068">As may even further be used herein, the term &#x201c;configured to&#x201d;, &#x201c;operable to&#x201d;, &#x201c;coupled to&#x201d;, or &#x201c;operably coupled to&#x201d; indicates that an item includes one or more of power connections, input(s), output(s), etc., to perform, when activated, one or more its corresponding functions and may further include inferred coupling to one or more other items. As may still further be used herein, the term &#x201c;associated with&#x201d;, includes direct and/or indirect coupling of separate items and/or one item being embedded within another item.</p><p id="p-0967" num="1069">As may be used herein, the term &#x201c;compares favorably&#x201d;, indicates that a comparison between two or more items, signals, etc., provides a desired relationship. For example, when the desired relationship is that signal <b>1</b> has a greater magnitude than signal <b>2</b>, a favorable comparison may be achieved when the magnitude of signal <b>1</b> is greater than that of signal <b>2</b> or when the magnitude of signal <b>2</b> is less than that of signal <b>1</b>. As may be used herein, the term &#x201c;compares unfavorably&#x201d;, indicates that a comparison between two or more items, signals, etc., fails to provide the desired relationship.</p><p id="p-0968" num="1070">As may be used herein, one or more claims may include, in a specific form of this generic form, the phrase &#x201c;at least one of a, b, and c&#x201d; or of this generic form &#x201c;at least one of a, b, or c&#x201d;, with more or less elements than &#x201c;a&#x201d;, &#x201c;b&#x201d;, and &#x201c;c&#x201d;. In either phrasing, the phrases are to be interpreted identically. In particular, &#x201c;at least one of a, b, and c&#x201d; is equivalent to &#x201c;at least one of a, b, or c&#x201d; and shall mean a, b, and/or c. As an example, it means: &#x201c;a&#x201d; only, &#x201c;b&#x201d; only, &#x201c;c&#x201d; only, &#x201c;a&#x201d; and &#x201c;b&#x201d;, &#x201c;a&#x201d; and &#x201c;c&#x201d;, &#x201c;b&#x201d; and &#x201c;c&#x201d;, and/or &#x201c;a&#x201d;, &#x201c;b&#x201d;, and &#x201c;c&#x201d;.</p><p id="p-0969" num="1071">As may also be used herein, the terms &#x201c;processing module&#x201d;, &#x201c;processing circuit&#x201d;, &#x201c;processor&#x201d;, &#x201c;processing circuitry&#x201d;, and/or &#x201c;processing unit&#x201d; may be a single processing device or a plurality of processing devices. Such a processing device may be a microprocessor, micro-controller, digital signal processor, microcomputer, central processing unit, field programmable gate array, programmable logic device, state machine, logic circuitry, analog circuitry, digital circuitry, and/or any device that manipulates signals (analog and/or digital) based on hard coding of the circuitry and/or operational instructions. The processing module, module, processing circuit, processing circuitry, and/or processing unit may be, or further include memory and/or an integrated memory element, which may be a single memory device, a plurality of memory devices, and/or embedded circuitry of another processing module, module, processing circuit, processing circuitry, and/or processing unit. Such a memory device may be a read-only memory, random access memory, volatile memory, non-volatile memory, static memory, dynamic memory, flash memory, cache memory, and/or any device that stores digital information. Note that if the processing module, module, processing circuit, processing circuitry, and/or processing unit includes more than one processing device, the processing devices may be centrally located (e.g., directly coupled together via a wired and/or wireless bus structure) or may be distributedly located (e.g., cloud computing via indirect coupling via a local area network and/or a wide area network). Further note that if the processing module, module, processing circuit, processing circuitry and/or processing unit implements one or more of its functions via a state machine, analog circuitry, digital circuitry, and/or logic circuitry, the memory and/or memory element storing the corresponding operational instructions may be embedded within, or external to, the circuitry comprising the state machine, analog circuitry, digital circuitry, and/or logic circuitry. Still further note that, the memory element may store, and the processing module, module, processing circuit, processing circuitry and/or processing unit executes, hard coded and/or operational instructions corresponding to at least some of the steps and/or functions illustrated in one or more of the Figures. Such a memory device or memory element can be included in an article of manufacture.</p><p id="p-0970" num="1072">One or more embodiments have been described above with the aid of method steps illustrating the performance of specified functions and relationships thereof. The boundaries and sequence of these functional building blocks and method steps have been arbitrarily defined herein for convenience of description. Alternate boundaries and sequences can be defined so long as the specified functions and relationships are appropriately performed. Any such alternate boundaries or sequences are thus within the scope and spirit of the claims. Further, the boundaries of these functional building blocks have been arbitrarily defined for convenience of description. Alternate boundaries could be defined as long as the certain significant functions are appropriately performed. Similarly, flow diagram blocks may also have been arbitrarily defined herein to illustrate certain significant functionality.</p><p id="p-0971" num="1073">To the extent used, the flow diagram block boundaries and sequence could have been defined otherwise and still perform the certain significant functionality. Such alternate definitions of both functional building blocks and flow diagram blocks and sequences are thus within the scope and spirit of the claims. One of average skill in the art will also recognize that the functional building blocks, and other illustrative blocks, modules and components herein, can be implemented as illustrated or by discrete components, application specific integrated circuits, processors executing appropriate software and the like or any combination thereof.</p><p id="p-0972" num="1074">In addition, a flow diagram may include a &#x201c;start&#x201d; and/or &#x201c;continue&#x201d; indication. The &#x201c;start&#x201d; and &#x201c;continue&#x201d; indications reflect that the steps presented can optionally be incorporated in or otherwise used in conjunction with one or more other routines. In addition, a flow diagram may include an &#x201c;end&#x201d; and/or &#x201c;continue&#x201d; indication. The &#x201c;end&#x201d; and/or &#x201c;continue&#x201d; indications reflect that the steps presented can end as described and shown or optionally be incorporated in or otherwise used in conjunction with one or more other routines. In this context, &#x201c;start&#x201d; indicates the beginning of the first step presented and may be preceded by other activities not specifically shown. Further, the &#x201c;continue&#x201d; indication reflects that the steps presented may be performed multiple times and/or may be succeeded by other activities not specifically shown. Further, while a flow diagram indicates a particular ordering of steps, other orderings are likewise possible provided that the principles of causality are maintained.</p><p id="p-0973" num="1075">The one or more embodiments are used herein to illustrate one or more aspects, one or more features, one or more concepts, and/or one or more examples. A physical embodiment of an apparatus, an article of manufacture, a machine, and/or of a process may include one or more of the aspects, features, concepts, examples, etc. described with reference to one or more of the embodiments discussed herein. Further, from figure to figure, the embodiments may incorporate the same or similarly named functions, steps, modules, etc. that may use the same or different reference numbers and, as such, the functions, steps, modules, etc. may be the same or similar functions, steps, modules, etc. or different ones.</p><p id="p-0974" num="1076">While transistors may be shown in one or more of the above-described figure(s) as field effect transistors (FETs), as one of ordinary skill in the art will appreciate, the transistors may be implemented using any type of transistor structure including, but not limited to, bipolar, metal oxide semiconductor field effect transistors (MOSFET), N-well transistors, P-well transistors, enhancement mode, depletion mode, and zero voltage threshold (VT) transistors.</p><p id="p-0975" num="1077">Unless specifically stated to the contra, signals to, from, and/or between elements in a figure of any of the figures presented herein may be analog or digital, continuous time or discrete time, and single-ended or differential. For instance, if a signal path is shown as a single-ended path, it also represents a differential signal path. Similarly, if a signal path is shown as a differential path, it also represents a single-ended signal path. While one or more particular architectures are described herein, other architectures can likewise be implemented that use one or more data buses not expressly shown, direct connectivity between elements, and/or indirect coupling between other elements as recognized by one of average skill in the art.</p><p id="p-0976" num="1078">The term &#x201c;module&#x201d; is used in the description of one or more of the embodiments. A module implements one or more functions via a device such as a processor or other processing device or other hardware that may include or operate in association with a memory that stores operational instructions. A module may operate independently and/or in conjunction with software and/or firmware. As also used herein, a module may contain one or more sub-modules, each of which may be one or more modules.</p><p id="p-0977" num="1079">As may further be used herein, a computer readable memory includes one or more memory elements. A memory element may be a separate memory device, multiple memory devices, or a set of memory locations within a memory device. Such a memory device may be a read-only memory, random access memory, volatile memory, non-volatile memory, static memory, dynamic memory, flash memory, cache memory, and/or any device that stores digital information. The memory device may be in a form a solid-state memory, a hard drive memory, cloud memory, thumb drive, server memory, computing device memory, and/or other physical medium for storing digital information.</p><p id="p-0978" num="1080">As applicable, one or more functions associated with the methods and/or processes described herein can be implemented via a processing module that operates via the non-human &#x201c;artificial&#x201d; intelligence (AI) of a machine. Examples of such AI include machines that operate via anomaly detection techniques, decision trees, association rules, expert systems and other knowledge-based systems, computer vision models, artificial neural networks, convolutional neural networks, support vector machines (SVMs), Bayesian networks, genetic algorithms, feature learning, sparse dictionary learning, preference learning, deep learning and other machine learning techniques that are trained using training data via unsupervised, semi-supervised, supervised and/or reinforcement learning, and/or other AI. The human mind is not equipped to perform such AI techniques, not only due to the complexity of these techniques, but also due to the fact that artificial intelligence, by its very definition&#x2014;requires &#x201c;artificial&#x201d; intelligence&#x2014;i.e., machine/non-human intelligence.</p><p id="p-0979" num="1081">As applicable, one or more functions associated with the methods and/or processes described herein can be implemented as a large-scale system that is operable to receive, transmit and/or process data on a large-scale. As used herein, a large-scale refers to a large number of data, such as one or more kilobytes, megabytes, gigabytes, terabytes or more of data that are received, transmitted and/or processed. Such receiving, transmitting and/or processing of data cannot practically be performed by the human mind on a large-scale within a reasonable period of time, such as within a second, a millisecond, microsecond, a real-time basis or other high speed required by the machines that generate the data, receive the data, convey the data, store the data and/or use the data.</p><p id="p-0980" num="1082">As applicable, one or more functions associated with the methods and/or processes described herein can require data to be manipulated in different ways within overlapping time spans. The human mind is not equipped to perform such different data manipulations independently, contemporaneously, in parallel, and/or on a coordinated basis within a reasonable period of time, such as within a second, a millisecond, microsecond, a real-time basis or other high speed required by the machines that generate the data, receive the data, convey the data, store the data and/or use the data.</p><p id="p-0981" num="1083">As applicable, one or more functions associated with the methods and/or processes described herein can be implemented in a system that is operable to electronically receive digital data via a wired or wireless communication network and/or to electronically transmit digital data via a wired or wireless communication network. Such receiving and transmitting cannot practically be performed by the human mind because the human mind is not equipped to electronically transmit or receive digital data, let alone to transmit and receive digital data via a wired or wireless communication network.</p><p id="p-0982" num="1084">As applicable, one or more functions associated with the methods and/or processes described herein can be implemented in a system that is operable to electronically store digital data in a memory device. Such storage cannot practically be performed by the human mind because the human mind is not equipped to electronically store digital data.</p><p id="p-0983" num="1085">While particular combinations of various functions and features of the one or more embodiments have been expressly described herein, other combinations of these features and functions are likewise possible. The present disclosure is not limited by the particular examples disclosed herein and expressly incorporates these other combinations.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprises:<claim-text>determining, by an analysis system that includes one or more computing entities, a system aspect of a system, the system aspect corresponding to evaluation of physical assets of the system;</claim-text><claim-text>determining, by the analysis system, an evaluation perspective, the evaluation perspective corresponding to a build of the system;</claim-text><claim-text>determining, by the analysis system, an evaluation viewpoint, the evaluation viewpoint corresponding to discovered information of the system;</claim-text><claim-text>based upon the system aspect, the evaluation perspective, and the evaluation viewpoint, selecting a plurality of data structures identifying data to be collected;</claim-text><claim-text>based upon the system aspect, the evaluation perspective, the evaluation viewpoint, and the plurality of data structures, determining context data;</claim-text><claim-text>based upon the plurality of data structures, identifying a plurality of physical assets of the system for collection of data;</claim-text><claim-text>querying the plurality of physical assets of the system to collect data to populate the plurality of data structures; and</claim-text><claim-text>evaluating, by the analysis system, the data structures containing the collected data using the context data to produce an evaluation of at least some of the plurality of physical assets of the system.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each physical asset of the plurality of physical assets may be categorized as one or more of: a computing entity, a computing device, a security device, a network device, a user software application, a system software application, a network software application, an operating system, a software tool, a network software application, a security software application, or a system monitoring software application.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein data structures are allocated to physical assets of the plurality of physical assets based upon how the physical assets are categorized.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the evaluation of the at least some of the plurality of physical assets of the system produces a comparison between design data of the system and build data of the system as represented by the data structures containing the collected data.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the context data is based upon system design data; and</claim-text><claim-text>the evaluating the data structures containing the collected data using the context data to produce an evaluation of at least some of the plurality of physical assets of the system produces a comparison between system design data and build data of the system.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising allocating a plurality of data structures of a same data structure, each allocated data structure corresponding to a respective physical asset.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein querying the plurality of physical assets of the system to collect data to populate the plurality of data structures includes requesting data to populate a respective allocated data structure.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a plurality of differing data structures may be allocated to a single physical asset.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising identifying all physical assets of the system, wherein the plurality of physical assets of the system for collection of data is a sub-set of all physical assets of the system.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein collected data comprises at least some of hardware asset information that includes at least one of vendor information, a serial number, a device description, a model number, a version, a generation, a purchase date, an installation date, or a service date for a corresponding physical asset.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein collected data comprises at least some of software asset information that includes at least one of vendor information, software type, license number, a version, a purchase date, an installation date, or a service date for a corresponding physical asset.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An analysis system comprises:<claim-text>a control module configured to interface with a system user interface module to determine:<claim-text>a system aspect of a system, the system aspect corresponding to evaluation of physical assets of the system;</claim-text><claim-text>an evaluation perspective, the evaluation perspective corresponding to a build of the system;</claim-text><claim-text>an evaluation viewpoint, the evaluation viewpoint corresponding to discovered information of the system;</claim-text><claim-text>based upon the system aspect, the evaluation perspective, and the evaluation viewpoint, selecting a plurality of data structures identifying data to be collected;</claim-text><claim-text>based upon the system aspect, the evaluation perspective, the evaluation viewpoint, and the plurality of data structures, determining context data; and</claim-text><claim-text>based upon the plurality of data structures, identifying a plurality of physical assets of the system for collection of data;</claim-text></claim-text><claim-text>the control module configured to interface with a data extraction module to query the plurality of physical assets of the system to collect data to populate the plurality of data structures; and</claim-text><claim-text>the control module configured to interface with a data analysis module to evaluate the plurality data structures containing the collected data using the context data to produce an evaluation of at least some of the plurality of physical assets of the system.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein each physical asset of the plurality of physical assets may be categorized as one or more of: a computing entity, a computing device, a security device, a network device, a user software application, a system software application, a network software application, an operating system, a software tool, a network software application, a security software application, or a system monitoring software application.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The analysis system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the control module allocates data structures to physical assets of the plurality of physical assets based upon how the physical assets are categorized.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the evaluation of the at least some of the plurality of physical assets of the system produces a comparison between design data of the system and build data of the system as represented by the plurality of data structures containing the collected data.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the context data is based upon system design data; and</claim-text><claim-text>the evaluating the data structures containing the collected data using the context data to produce an evaluation of at least some of the plurality of physical assets of the system produces a comparison between system design data and build data of the system.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising allocating a plurality of data structures of a same data structure, each allocated data structure corresponding to a respective physical asset.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein a plurality of differing data structures may be allocated to a single physical asset.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein collected data comprises at least some of hardware asset information that includes at least one of vendor information, a serial number, a device description, a model number, a version, a generation, a purchase date, an installation date, or a service date for a corresponding physical asset.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The analysis system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein collected data comprises at least some of software asset information that includes at least one of vendor information, software type, license number, a version, a purchase date, an installation date, or a service date for a corresponding physical asset.</claim-text></claim></claims></us-patent-application>