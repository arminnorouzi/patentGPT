<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004863A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004863</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17682225</doc-number><date>20220228</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-110283</doc-number><date>20210701</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6277</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6298</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">LEARNING APPARATUS, METHOD, COMPUTER READABLE MEDIUM AND INFERENCE APPARATUS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>KABUSHIKI KAISHA TOSHIBA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KANISHIMA</last-name><first-name>Yasuhiro</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SUDO</last-name><first-name>Takashi</first-name><address><city>Fuchu Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>YANAGIHASHI</last-name><first-name>Hiroyuki</first-name><address><city>Kawasaki Kanagawa</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>KABUSHIKI KAISHA TOSHIBA</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">According to one embodiment, a learning apparatus includes a processor. The processor acquires data with a label indicating whether the data is normal data or anomalous data. The processor calculates an anomaly degree indicating a degree to which the data is the anomalous data using an output of a model for the data. The processor calculates a loss value related to the anomaly degree using a loss function based on an adjustment parameter based on a previously calculated loss value and the label. The processor updates a parameter of the model so as to minimize the loss value.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="159.51mm" wi="153.25mm" file="US20230004863A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="146.05mm" wi="103.12mm" file="US20230004863A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="180.42mm" wi="155.28mm" file="US20230004863A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="232.07mm" wi="110.83mm" file="US20230004863A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="111.42mm" wi="108.46mm" file="US20230004863A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="153.25mm" wi="118.96mm" file="US20230004863A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="90.51mm" wi="165.02mm" file="US20230004863A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application is based upon and claims the benefit of priority from Japanese Patent Application No. 2021-110283, filed Jul. 1, 2021, the entire contents of which are incorporated herein by reference.</p><heading id="h-0001" level="1">FIELD</heading><p id="p-0003" num="0002">Embodiments described herein relate to a learning apparatus, method, computer readable medium and an inference apparatus.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Much research has been done on using machine learning for anomaly detection. In such anomaly detection using machine learning, there is a need to improve the performance of anomaly detection by utilizing generated anomalous data at the stage when the anomalous data is generated during operation.</p><p id="p-0005" num="0004">However, when a model is updated each time in situations in which it is available in stages during the operation of anomalous data, there is a problem wherein the consistency of the model is not considered before and after the model update and the continuity of degrees of anomalies output by the models before and after the update is lost.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram showing a learning apparatus according to a first embodiment.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart showing training processing of the learning apparatus according to the first embodiment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a conceptual diagram showing balance adjustment of a loss function by an adjustment parameter according to the first embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram showing an inference apparatus according to a second embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram showing an example of an anomaly degree determination result by the inference apparatus according to the second embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram showing an example of an image output of a reconstruction error which is a processing result of the inference apparatus according to the second embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram showing an example of a hardware configuration of the learning apparatus and the inference apparatus according to the present embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0013" num="0012">In general, according to one embodiment, a learning apparatus includes a processor. The processor acquires data with a label indicating whether the data is normal data or anomalous data. The processor calculates an anomaly degree indicating a degree to which the data is the anomalous data using an output of a model for the data. The processor calculates a loss value related to the anomaly degree using a loss function based on an adjustment parameter based on a previously calculated loss value and the label. The processor updates a parameter of the model so as to minimize the loss value.</p><p id="p-0014" num="0013">Hereinafter, the learning apparatus, method, computer readable medium, and an inference apparatus according to the present embodiments will be described in detail with reference to the drawings. In the following embodiments, the parts with the same reference signs perform the same operation, and redundant descriptions will be omitted as appropriate.</p><heading id="h-0005" level="1">First Embodiment</heading><p id="p-0015" num="0014">A learning apparatus according to a first embodiment will be described with reference to the block diagram of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0016" num="0015">A learning apparatus <b>10</b> according to the first embodiment includes a data acquisition unit <b>101</b>, an anomaly degree calculation unit <b>102</b>, a loss calculation unit <b>103</b>, a loss holding unit <b>104</b>, an update unit <b>105</b>, and a display control unit <b>106</b>.</p><p id="p-0017" num="0016">The data acquisition unit <b>101</b> acquires a data set from the outside. The data set here includes a plurality of pairs of data x used for training and a label indicating which of two classifications (normal data and anomalous data) the data is.</p><p id="p-0018" num="0017">The anomaly degree calculation unit <b>102</b> receives a data set from the data acquisition unit <b>101</b>, and uses an output of a model for the data to calculate an anomaly degree indicating a degree to which the data is anomalous data. The model here is a network model such as an autoencoder whose task is to detect anomalies.</p><p id="p-0019" num="0018">The loss calculation unit <b>103</b> receives the label associated with the data for which the anomaly degree has been calculated from the data acquisition unit <b>101</b>, the anomaly degree from the anomaly degree calculation unit <b>102</b>, and a previously calculated loss value from the loss holding unit <b>104</b> to be described later, respectively. The loss calculation unit <b>103</b> calculates a loss value related to the anomaly degree by using a loss function. A loss function is a function based on an adjustment parameter based on a loss value calculated in previous processing and a label.</p><p id="p-0020" num="0019">The loss holding unit <b>104</b> holds one or more loss values calculated by the loss calculation unit <b>103</b> in past processing.</p><p id="p-0021" num="0020">The update unit <b>105</b> receives a loss value from the loss calculation unit <b>103</b>, and updates a parameter of the model so as to minimize the loss value. When the update unit <b>105</b> terminates the updating of the model parameter based on a predetermined condition, the training of the model is completed and a trained model is generated.</p><p id="p-0022" num="0021">The display control unit <b>106</b> controls, for example, to display information on the anomaly degree calculated by the anomaly degree calculation unit <b>102</b>, the loss function during training of the model, and the loss value on an external display. The learning apparatus <b>10</b> may include a display unit (not shown) and display the information on that display unit.</p><p id="p-0023" num="0022">Next, training processing of the learning apparatus <b>10</b> according to the first embodiment will be described with reference to the flowchart of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0024" num="0023">Note that the present embodiment aims to generate a trained model for performing an anomaly detection task, but the present embodiment is not limited thereto. For example, if it is a machine learning model for a task that makes a binary judgment such as separating two types of products or judging positive/negative, the learning apparatus <b>10</b> according to the present embodiment can be applied by setting a degree to which the classification is one of the two classifications (a degree of deviation from a classification), and a desired trained model can be generated.</p><p id="p-0025" num="0024">Further, in the training processing of the learning apparatus <b>10</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, if there is no anomalous data before operation, a model is generated by unsupervised training with only correct answer data. After that, if anomalous data can be obtained during the operation, the training processing shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is executed by supervised training in which normal data is labeled as normal and anomalous data is labeled as anomalous. If the anomalous data can be obtained even before the operation, the training processing shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be executed in the same manner. The training processing may be executed every time anomalous data is obtained during the operation, or may be executed at a timing at which a predetermined number of anomalous data pieces are obtained. Alternatively, the training processing may be executed at predetermined intervals such as every six months.</p><p id="p-0026" num="0025">In step S<b>201</b>, the data acquisition unit <b>101</b> acquires a data set. Specifically, X={xm, ym} is given as data set X including m (m is a natural number of 2 or more) data pieces. Here, data xn is the nth (n is a natural number of 1 or more, 1&#x2264;n&#x2264;m) piece of data, and each piece of data has a D-dimensional feature vector. That is, xn=[xn1, xn2, . . . , xnD]. For example, when the data xn is a monochrome image of 64&#xd7;64 pixels, it has a feature vector for each pixel, that is, D=64&#xd7;64=4096 feature vectors. A label yn is the nth (n is a natural number of 1 or more, 1&#x2264;n&#x2264;m) label, and yn=0 indicates normal data, and yn=1 indicates anomalous data.</p><p id="p-0027" num="0026">In step S<b>202</b>, the anomaly degree calculation unit <b>102</b> calculates an anomaly degree of the data. For the anomaly degree, for example, when a model is an autoencoder, a reconstruction error may be used. If a model is a variational autoencoder, a negative log-likelihood of probability distribution may be used.</p><p id="p-0028" num="0027">Specifically, it is assumed that the model is an autoencoder. An anomaly degree S(xn), which is a reconstruction error, may be expressed by equation (1), for example, by using a mean square error between data and an output of the autoencoder.</p><p id="p-0029" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>(<i>xn</i>)=&#x2225;<i>xn&#x2212;f</i>(<i>xn</i>,&#x3b8;)&#x2225;<sub>2</sub><sup>2</sup><i>/D</i>&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0030" num="0028">&#x3b8; is a parameter of a model. f(xn, &#x3b8;) is an output when the data xn is input to the autoencoder having the parameter &#x3b8;. That is, if xn is an image, a root mean square of a difference value for each pixel constituting the image is the reconstruction error. The anomaly degree may be expressed as a likelihood function. It suffices that the anomaly degree calculation unit <b>102</b> can calculate, as the anomaly degree, a value which is low when a probability of appearance of the data is high, that is, when the data is normal, and which is high when the probability of appearance of the data is low, that is, when the data is anomalous.</p><p id="p-0031" num="0029">In step S<b>203</b>, the loss calculation unit <b>103</b> calculates a loss value from the anomaly degree calculated in step S<b>202</b> using a loss function. The loss function can be expressed by, for example, equation (2).</p><p id="p-0032" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>l</i>(<i>xn</i>)=(1&#x2212;<i>yn</i>)<i>S</i>(<i>xn</i>)&#x2212;<i>yn </i>log<sub>e</sub>(1&#x2212;<i>e</i><sup>&#x2212;&#x3b1;S(xn)</sup>)/&#x3b1;&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0033" num="0030">l(xn) is a loss value, and a is an adjustment parameter to be described later.</p><p id="p-0034" num="0031">The loss function of equation (2) is designed such that the smaller the loss value l(xn), the lower the anomaly degree for normal data, and as the loss value l(xn) becomes smaller, the anomaly degree for anomalous data becomes higher than that of the normal data.</p><p id="p-0035" num="0032">The loss function is not limited to equation (2), and may be any function that calculates a low value for normal data and a high value for anomalous data by a part of increasing according to the anomaly degree with respect to the normal data and a part of decreasing according to the anomaly degree with respect to the anomalous data.</p><p id="p-0036" num="0033">Here, in equation (2), the first term &#x201c;(1&#x2212;yn)S(xn)&#x201d; on the right side is also referred to as a normal label term, which is related to a loss of a normal label indicating normality. Similarly, the second term &#x201c;&#x2212;yn log<sub>e</sub>(1&#x2212;e<sup>&#x2212;&#x3b1;S(xn)</sup>)/&#x3b1;&#x201d; is also referred to as an anomaly label term, which is related to a loss of an anomaly label indicating anomaly. The adjustment parameter &#x3b1; can be expressed by, for example, equation (3).</p><p id="p-0037" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b1;=log<sub>e</sub>2/(&#x3a3;<i>l</i>prev(<i>xn</i>)/<i>D</i>)&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0038" num="0034">Here, lprev(xn) is a loss value one step previous. &#x201c;One step previous&#x201d; is assumed to be, for example, one epoch or one iteration previous in training of a model. Specifically, when one step previous is one epoch previous, lprev(xn) is an average value of loss values calculated in one epoch. A value based on a loss value is not limited to an average value, but may be a statistic such as a combination of a maximum value or an average value and a standard deviation.</p><p id="p-0039" num="0035">In step S<b>204</b>, the update unit <b>105</b> determines whether or not the training is finished. In the training completion determination, for example, it may be determined that training is finished when the training of a predetermined number of epochs is completed, it may be determined that the training is finished when the loss value l(xn) is equal to or less than a threshold value, or it may be determined that the training is finished when a decrease in the loss value converges. When the training is finished, the parameter update is terminated and the processing ends. Thereby, a trained model is generated. On the other hand, if the training is not finished, the process proceeds to step S<b>205</b>.</p><p id="p-0040" num="0036">In step S<b>205</b>, the update unit <b>105</b> updates the adjustment parameter &#x3b1; using the loss value calculated in step S<b>203</b>. When minimizing the loss value l(xn) calculated from the loss function of the above equation (2), if it is minimized without considering a balance between the first term and the second term on the right side of equation (2), that is, the normal label term and the anomaly label term, there is a case in which either minimization of the loss value for normal data or minimization of the loss value for anomalous data may act predominantly. Thus, in minimizing the loss value l(xn), the update unit <b>105</b> adjusts the adjustment parameter a so that the first term and the second term on the right side of equation (2) can be balanced.</p><p id="p-0041" num="0037">Specifically, for example, it suffices that the adjustment parameter &#x3b1; is updated so that, of the loss function, a loss function related to normal data and a loss function related to anomalous data intersect at a value based on a previously calculated loss value.</p><p id="p-0042" num="0038">In step S<b>206</b>, the update unit <b>105</b> updates the parameter &#x3b8; of the model, specifically, a weight and bias of a neural network, etc. by means of a gradient descent method and an error backpropagation method so as to minimize the loss value l(xn) to be calculated by the loss function. After that, the process returns to step S<b>201</b>, and the processes from step S<b>201</b> to step S<b>206</b> are repeatedly executed for the next data set.</p><p id="p-0043" num="0039">Next, the balance adjustment of the loss function by the adjustment parameter a will be described with reference to the conceptual diagram of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0044" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a graph of the loss function expressed by the above equation (2), in which the ordinate axis indicates the loss value and the abscissa axis indicates the reconstruction error (the anomaly degree in the present embodiment).</p><p id="p-0045" num="0041">Since the smaller the reconstruction error is, the more the normal data can be reproduced, a graph <b>301</b> of the normal label term is designed so that when the reconstruction error is small, the loss value is also small. That is, it is represented by a linear graph in which the loss value increases in proportion to the reconstruction error. On the other hand, a graph <b>302</b> and a graph <b>303</b> of the anomaly label term are loss values related to anomalous data, and it can be said that the larger the reconstruction error is, the farther the anomalous data is from the normal data. Therefore, the graphs are designed so that when the reconstruction error is large, the loss value is small. Further, a difference between the graph <b>302</b> and the graph <b>303</b> occurs because the curve of the anomaly label term is adjusted by a difference in value of the adjustment parameter &#x3b1;. Hereinafter, the anomaly label term will be described using the graph <b>302</b> as an example.</p><p id="p-0046" num="0042">Here, an intersection of the graph <b>301</b> and the graph <b>302</b> indicates that the loss values of the normal label term and the anomaly label term match. That is, the model parameter is updated by the adjustment parameter &#x3b1; such that the graph <b>301</b> and the graph <b>302</b> intersect at a loss value one step previous and the loss value becomes small, so that a parameter that minimizes the loss value can be calculated while maintaining the balance between the loss value due to the normal label term and the loss value due to the anomaly label term.</p><p id="p-0047" num="0043">Since the adjustment parameter a is based on a previously calculated loss value and is incorporated into the loss function, it is automatically calculated in the training process of the model. For example, the display control unit may display a graph related to the loss function as in <figref idref="DRAWINGS">FIG. <b>3</b></figref> and a user may specify a loss value existing on the graph <b>301</b> of the previously calculated normal label term so that a curve in which the anomaly label term intersects the specified point can be calculated.</p><p id="p-0048" num="0044">According to the first embodiment described above, a parameter of a model is trained by using a loss function including an adjustment parameter based on a loss value one step previous. Specifically, a parameter such as a weight of the model that minimizes a loss value calculated by the loss function using the adjustment parameter is determined, thereby determining a parameter in which a balance between a normal label term and an anomaly label term in the loss function is well judged.</p><p id="p-0049" num="0045">As a result, without biased training such as training in which an anomaly label dominates, a training effect by anomalous data can also be obtained while ensuring consistency with the trained model trained by unsupervised training with only normal data when there is no anomalous data. That is, the performance of the model can be improved while ensuring the consistency of the model.</p><heading id="h-0006" level="1">Second Embodiment</heading><p id="p-0050" num="0046">A second embodiment shows an example of executing an inference using the trained model trained by the learning apparatus of the first embodiment.</p><p id="p-0051" num="0047">A block diagram of an inference apparatus according to the second embodiment is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0052" num="0048">An inference apparatus <b>40</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes a data acquisition unit <b>101</b>, a model execution unit <b>401</b>, and a display control unit <b>106</b>.</p><p id="p-0053" num="0049">The data acquisition unit <b>101</b> acquires target data to be processed. For example, the target data is image data of a product for which it is desired to determine whether or not it is anomalous data.</p><p id="p-0054" num="0050">The model execution unit <b>401</b> includes a trained model <b>400</b> generated by the learning apparatus <b>10</b> according to the first embodiment. The model execution unit <b>401</b> acquires target data from the data acquisition unit <b>101</b>, inputs that target data to the trained model <b>400</b> to execute inference, and outputs an anomaly degree. Here, it is assumed that the trained model <b>400</b> is a trained autoencoder.</p><p id="p-0055" num="0051">Specifically, a parameter of the trained model determined by the update unit <b>105</b> is &#x3b8;{circumflex over (&#x2003;)}. The superscript expresses that &#x201c;{circumflex over (&#x2003;)}&#x201d; is added directly above a character. The parameter &#x3b8;{circumflex over (&#x2003;)} and target data x*n for which the anomaly degree is to be calculated are input to the model execution unit <b>401</b>. With the trained model of that parameter &#x3b8;{circumflex over (&#x2003;)}, the anomaly degree for the target data x*n is calculated by, for example, equation (4).</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i>(<i>xn</i>)=&#x2225;<i>x*n&#x2212;f</i>(<i>x*n</i>,{circumflex over (&#x3b8;)})&#x2225;<sub>2</sub><sup>2</sup><i>/D</i>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0052">Further, the model execution unit <b>401</b> may determine whether or not the data is anomalous data based on the anomaly degree and output a determination result. For example, if the anomaly degree is equal to or greater than a threshold value, it can be determined that the target data x*n is anomalous data. In contrast, if the anomaly degree is less than the threshold value, it can be determined that the target data x*n is normal data.</p><p id="p-0058" num="0053">The display control unit <b>106</b> receives the determination result from the model execution unit <b>401</b>, and outputs the determination result to the outside.</p><p id="p-0059" num="0054">Next, the anomaly degree determination result by the inference apparatus <b>40</b> according to the second embodiment will be described with reference to the graph of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0060" num="0055">A graph <b>501</b> is a graph of a calculation result of an anomaly degree by the inference apparatus <b>40</b> according to the second embodiment including the trained model according to the first embodiment. A graph <b>502</b> is a graph of a calculation result before the trained model according to the first embodiment is operated, the calculation result being of an anomaly degree by a trained model which is an autoencoder trained with only normal data before anomalous data is obtained. A graph <b>503</b>, as a graph of a comparative example, is a graph of a calculation result of an anomaly degree by a trained model generated by training without an adjustment parameter in a loss function including a normal label term and an anomaly label term.</p><p id="p-0061" num="0056"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a result of inputting three types of normal data, known anomalous data, and unknown anomalous data each into the trained models by which the results of the graphs <b>501</b> to <b>503</b> were obtained, and executing inference by the trained models. In a case of the trained model according to the first embodiment by which the result of the graph <b>501</b> is obtained and the trained model as the comparative example by which the result of the graph <b>503</b> is obtained, the normal data and the known anomalous data are normal data and anomalous data used for training the model, respectively. The unknown anomalous data is anomalous data that is not involved in training the model.</p><p id="p-0062" num="0057">On the other hand, in a case of the trained model of the autoencoder by which the result of the graph <b>502</b> is obtained, since it is trained with only normal data, the normal data is data used for training the model, and the known anomalous data and the unknown anomalous data are anomalous data that are not involved in training the model.</p><p id="p-0063" num="0058">First, looking at the calculation result of the anomaly degree for the normal data on the left side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the graph <b>501</b> has almost the same anomaly degree as the graph <b>502</b>. Thus, it can be said that the trained model for the graph <b>501</b> and the trained model for the graph <b>502</b> are highly consistent in inferring the normal data. On the other hand, the graph <b>503</b> has a higher anomaly degree than the graph <b>501</b> and the graph <b>502</b>, despite the processing result for the normal data. This is because a reconstruction error of the normal data increases as a reconstruction error of the anomalous data becomes maximized. Thus, it can be said that the autoencoder related to the graph <b>502</b> and the trained model related to the graph <b>503</b> have low consistency.</p><p id="p-0064" num="0059">Furthermore, looking at the calculation results of the anomaly degree for the known anomalous data in the center of <figref idref="DRAWINGS">FIG. <b>5</b></figref> and the unknown anomalous data on the right side of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the graph <b>501</b> has a higher anomaly degree than the graph <b>502</b> in each result. Thus, the trained model according to the present embodiment of the graph <b>501</b> can determine the anomalous data with a higher accuracy than the autoencoder related to the graph <b>502</b>.</p><p id="p-0065" num="0060">Next, an example of an image output of a reconstruction error will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0066" num="0061"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows, in order from the top, (Input) image data of target data input to a trained model, (Output) image data output from the trained model, and (Reconstruction error) image data that is a difference between the image of the target data and the trained model output. It is assumed that the target data is anomalous data including an anomalous region <b>603</b>.</p><p id="p-0067" num="0062">An image group <b>601</b> is image data related to the trained model according to the second embodiment, and an image group <b>602</b> is image data related to a trained model trained without an adjustment parameter of a loss function in the same manner as the graph <b>503</b>. In the trained model according to the second embodiment, the anomalous region <b>603</b> included in the target data does not exist in the output from the trained model. The image data of the reconstruction error, which is the difference between the input image data and the output image data, includes the anomalous region <b>603</b>, and accurate anomaly detection is performed.</p><p id="p-0068" num="0063">On the other hand, in the trained model trained without an adjustment parameter of a loss function, it can be seen that the output cannot reproduce the normal data and the anomalous region <b>603</b> cannot be correctly extracted even in the reconstruction error.</p><p id="p-0069" num="0064">According to the second embodiment described above, inference is executed by the trained model including a parameter generated in the first embodiment, so that an anomaly degree for known anomalous data is increased, while consistency with a trained model trained with only normal data can be ensured. In addition, it becomes easy to determine an anomaly part from the reconstruction error.</p><p id="p-0070" num="0065">Next, <figref idref="DRAWINGS">FIG. <b>7</b></figref> will be referred to for explaining an exemplary hardware configuration of the learning apparatus <b>10</b> and the inference apparatus <b>40</b> according to the foregoing embodiments.</p><p id="p-0071" num="0066">The learning apparatus <b>10</b> and the inference apparatus <b>40</b> include a central processing unit (CPU) <b>71</b>, a random access memory (RAM) <b>72</b>, a read only memory (ROM) <b>73</b>, a storage <b>74</b>, a display device <b>75</b>, an input device <b>76</b>, and a communication device <b>77</b>, which are connected to one another via a bus.</p><p id="p-0072" num="0067">The CPU <b>71</b> is a processor adapted to execute arithmetic operations and control operations according to one or more programs. The CPU <b>71</b> uses a prescribed area in the RAM <b>72</b> as a work area to perform, in cooperation with one or more programs stored in the ROM <b>73</b>, the storage <b>74</b>, etc., operations of the components of the learning apparatus <b>10</b> and the inference apparatus <b>40</b> described above.</p><p id="p-0073" num="0068">The RAM <b>72</b> is a memory which may be a synchronous dynamic random access memory (SDRAM). The RAM <b>72</b>, as its function, provides the work area for the CPU <b>71</b>. Meanwhile, the ROM <b>73</b> is a memory that stores programs and various types of information in such a manner that no rewriting is permitted.</p><p id="p-0074" num="0069">The storage <b>74</b> is one or any combination of storage media including a magnetic storage medium such as a hard disc drive (HDD) and a semiconductor storage medium such as a flash memory. The storage <b>74</b> may be an apparatus adapted to perform data write and read operations with a magnetically recordable storage medium such as an HDD and an optically recordable storage medium. The storage <b>74</b> may conduct data write and read operations with storage media under the control of the CPU <b>71</b>.</p><p id="p-0075" num="0070">The display device <b>75</b> may be a liquid crystal display (LCD), etc. The display device <b>75</b> is adapted to present various types of information based on display signals from the CPU <b>71</b>.</p><p id="p-0076" num="0071">The input device <b>76</b> may be a mouse, a keyboard, etc. The input device <b>76</b> is adapted to receive information from user operations as instruction signals and send the instruction signals to the CPU <b>71</b>.</p><p id="p-0077" num="0072">The communication device <b>77</b> is adapted to communicate with external devices under the control of the CPU <b>71</b>.</p><p id="p-0078" num="0073">Instructions in the processing steps described for the foregoing embodiments may follow a software program. It is also possible for a general-purpose computer system to store such a program in advance and read the program to realize the same effects as provided through the control of the learning apparatus and the inference apparatus described above. The instructions described in relation to the embodiments may be stored as a computer-executable program in a magnetic disc (flexible disc, hard disc, etc.), an optical disc (CD-ROM, CD-R, CD-RW, DVD-ROM, DVD&#xb1;R, DVD&#xb1;RW, Blu-ray (registered trademark) disc, etc.), a semiconductor memory, or a similar storage medium. The storage medium here may utilize any storage technique provided that the storage medium can be read by a computer or by a built-in system. The computer can realize the same behavior as the control of the learning apparatus and the inference apparatus according to the above embodiments by reading the program from the storage medium and, based on this program, causing the CPU to follow the instructions described in the program. Of course, the computer may acquire or read the program via a network.</p><p id="p-0079" num="0074">Note that the processing for realizing each embodiment may be partly assigned to an operating system (OS) running on a computer, database management software, middleware (MW) of a network, etc., according to an instruction of a program installed in the computer or the built-in system from the storage medium.</p><p id="p-0080" num="0075">Further, each storage medium for the embodiments is not limited to a medium independent of the computer and the built-in system. The storage media may include a storage medium that stores or temporarily stores the program downloaded via a LAN, the Internet, etc.</p><p id="p-0081" num="0076">The embodiments do not limit the number of the storage media to one, either. The processes according to the embodiments may also be conducted with multiple media, where the configuration of each medium is discretionarily determined.</p><p id="p-0082" num="0077">The computer or the built-in system in the embodiments is intended for use in executing each process in the embodiments based on one or more programs stored in one or more storage media. The computer or the built-in system may be of any configuration such as an apparatus constituted by a single personal computer or a single microcomputer, etc., or a system in which multiple apparatuses are connected via a network.</p><p id="p-0083" num="0078">Also, the embodiments do not limit the computer to a personal computer. The &#x201c;computer&#x201d; in the context of the embodiments is a collective term for a device, an apparatus, etc., which are capable of realizing the intended functions of the embodiments according to a program and which include an arithmetic processor in an information processing apparatus, a microcomputer, and so on.</p><p id="p-0084" num="0079">While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel embodiments described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the embodiments described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A learning apparatus comprising a processor configured to:<claim-text>acquire data with a label indicating whether the data is normal data or anomalous data;</claim-text><claim-text>calculate an anomaly degree indicating a degree to which the data is the anomalous data using an output of a model for the data;</claim-text><claim-text>calculate a loss value related to the anomaly degree using a loss function based on an adjustment parameter based on a previously calculated loss value and the label; and</claim-text><claim-text>update a parameter of the model so as to minimize the loss value.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>a probability of appearance of the normal data is higher than a probability of appearance of the anomalous data, and</claim-text><claim-text>the processor calculates the anomaly degree to be low for the normal data and to be high for the anomalous data.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor calculates, as the anomaly degree, a reconstruction error when the model is an autoencoder, and a negative log-likelihood of probability distribution when the model is a variational autoencoder.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor calculates a low value for the normal data and a higher value than the low value of the normal data for the anomalous data, using, as the loss function, a function that increases according to the anomaly degree with respect to the normal data with a high probability of appearance and decreases according to the anomaly degree with respect to the anomalous data with a lower probability of appearance than the normal data.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The learning apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the processor updates the adjustment parameter so that a first part of the loss function related to the normal data and a second part of the loss function related to the anomalous data intersect at a value based on the previously calculated loss value.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the processor calculates a value based on the previously calculated loss value based on a statistic of previously calculated loss values.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the previously calculated loss value is a loss value one epoch previous or a loss value one iteration previous in training of the model.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A learning method comprising:<claim-text>acquiring data with a label indicating whether the data is normal data or anomalous data;</claim-text><claim-text>calculating an anomaly degree indicating a degree to which the data is the anomalous data using an output of a model for the data;</claim-text><claim-text>calculating a loss value related to the anomaly degree using a loss function based on an adjustment parameter based on a previously calculated loss value and the label; and</claim-text><claim-text>updating a parameter of the model so as to minimize the loss value.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A non-transitory computer readable medium including computer executable instructions, wherein the instructions, when executed by a processor, cause the processor to perform a method comprising:<claim-text>acquiring data with a label indicating whether the data is normal data or anomalous data;</claim-text><claim-text>calculating an anomaly degree indicating a degree to which the data is the anomalous data using an output of a model for the data;</claim-text><claim-text>calculating a loss value related to the anomaly degree using a loss function based on an adjustment parameter based on a previously calculated loss value and the label; and</claim-text><claim-text>updating a parameter of the model so as to minimize the loss value.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An inference apparatus comprising a processor configured to:<claim-text>acquire target data to be processed; and</claim-text><claim-text>calculate an anomaly degree of the target data using a trained model generated by the learning apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the processor is further configured to control display of the anomaly degree.</claim-text></claim></claims></us-patent-application>