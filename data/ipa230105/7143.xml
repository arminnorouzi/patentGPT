<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007144A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007144</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17836148</doc-number><date>20220609</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2021-109415</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>1</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>15</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>1</main-group><subgroup>40012</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>15</main-group><subgroup>1822</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>15</main-group><subgroup>1849</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE PROCESSING APPARATUS, METHOD OF CONTROLLING THE SAME, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>CANON KABUSHIKI KAISHA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Inoue</last-name><first-name>Takuya</first-name><address><city>Ibaraki</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An image processing apparatus for converting color data into gray data extracts an object included in the color data in a page, and determines, based on color information and position information on a first object and a second object extracted, whether at least one of the first object and the second object is to be a target of conversion of a color value of the color data into a gray value with improved discriminability. The image processing apparatus controls to convert the color value of the object into a gray value by a conversion targeting all color values, or into a gray value with the improved discriminability, based on the determination whether or not to be the target of conversion.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="189.40mm" wi="153.50mm" file="US20230007144A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="206.25mm" wi="155.53mm" file="US20230007144A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="233.00mm" wi="133.94mm" file="US20230007144A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="237.41mm" wi="156.29mm" file="US20230007144A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="164.68mm" wi="143.93mm" file="US20230007144A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="191.01mm" wi="118.03mm" file="US20230007144A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="205.49mm" wi="139.70mm" file="US20230007144A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="179.66mm" wi="152.48mm" file="US20230007144A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="237.24mm" wi="153.92mm" file="US20230007144A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="218.02mm" wi="147.74mm" file="US20230007144A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="233.68mm" wi="154.26mm" file="US20230007144A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0002" level="1">Field of the Invention</heading><p id="p-0002" num="0001">The present invention relates to an image processing apparatus, a method of controlling the same, and a storage medium.</p><heading id="h-0003" level="1">Description of the Related Art</heading><p id="p-0003" num="0002">When a color image, expressed in an RGB format or the like, is printed in a grayscale format using a printing apparatus, it is a common practice to perform the printing with image data in which input RGB values are converted into gray values using a uniform conversion scheme targeting all the color values (a conversion scheme with which all the color values are converted under the same rule, such as NTSC conversion, for example). Such color conversion targeting all the colors (hereinafter, color conversion for all target colors) is effective for printing of a color image requiring tonality. The NTSC conversion scheme is a conversion scheme for converting RGB values into gray values using the following Formula (1).</p><p id="p-0004" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Gray value=0.30&#xd7;<i>R+</i>0.59&#xd7;<i>G+</i>0.11&#xd7;<i>B</i>&#x2003;&#x2003;Formula (1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0005" num="0003">However, with the color conversion for all target colors, color information is lost in a process of converting RGB (three channels) into gray (one channel). Thus, the color conversion for all target colors converting a plurality of colors expressed by different RGB values (colors) into the same (or similar) gray value(s), may cause a difference between the plurality of colors being unidentifiable. In particular, in a case of a pie chart or the like with images rendered to have two colors partially overlapping or being in contact with each other, the boundary between the two colors is lost. When characters and the like on a figure are converted into close gray values, visibility of the characters is compromised. Thus, there has been a problem in that when a monochrome image thus generated is printed, the discriminability making the original color difference identifiable is compromised from that in the case of color printing.</p><p id="p-0006" num="0004">In Japanese Patent Laid-Open No. 2020-49939, Page Description Language (PDL) data is analyzed, and the number of colors and color values are obtained for text and graphics rendering in a page. A density difference is provided to a monochrome image based on the number of colors and the color values. Thus, information can be prevented from being lost even in a case of monochrome output, whereby discriminability can be improved with densities achieving high visibility.</p><p id="p-0007" num="0005">Unfortunately, this processing of reproduction with improved discriminability in reproduction (hereinafter, discriminability reproducing processing) may lead to an undesirable result, if all the text and graphics in the page are targeted. For example, when a large number of adjacent graphics with similar colors are targeted, the number of colors is large at a portion of those graphics, resulting in a failure to provide the portion that actually requires the improvement in discriminability, with an appropriate density difference. For the rendering of gradation expression among the graphics, a density difference is not desirable. Data transmitted for gradation rendering for such gradation expression can be determined to be gradation data. Such data is mainly configured with fine single color graphics, images, and the like. Thus, providing a density difference with the gradation expression configured with the fine graphics being the target leads to a density step provided in an image reproduced, resulting in compromised appearance. Thus, graphics requiring no improvement in discriminability are desirably excluded from the target of the discriminability reproducing processing.</p><p id="p-0008" num="0006">Japanese Patent Laid-Open No. 2010-186298 describes a technique of detecting gradation configured by single color graphics, images, and the like. With this gradation detection technique employed, only the data for gradation expression can be excluded from the target of the discriminability reproducing processing.</p><p id="p-0009" num="0007">Still, image data for the gradation expression is not the only data that is desired to be excluded from the target of the discriminability reproducing processing. For example, application of the discriminability reproducing processing described above may lead to a density step provided in a reproduced image, resulting in compromised appearance, not only in the case of the gradation expression object, but also in a case of single color graphic object rendering.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0010" num="0008">An aspect of the present invention is to eliminate the above-mentioned problem with conventional technology.</p><p id="p-0011" num="0009">A feature of the present invention is to provide a technique of excluding graphics requiring no processing of improving discriminability from the target of the discriminability reproducing processing, so that a reproduced image of such graphics can be prevented from having compromised appearance.</p><p id="p-0012" num="0010">According to a first aspect of the present invention, there is provided an image processing apparatus for converting color data into gray data, the image processing apparatus comprising: one or more processors and one or more memories being configured to: extract an object included in the color data in a page; determine, based on color information and position information on a first object and a second object extracted, whether at least one of the first object and the second object is to be a target of conversion of a color value of the color data into a gray value with improved discriminability; and control to convert, based on the determining, the color value of the object into a gray value by a conversion targeting all color values, or into a gray value with the improved discriminability.</p><p id="p-0013" num="0011">According to a second aspect of the present invention, there is provided an image processing apparatus for converting color data into gray data, the image processing apparatus comprising: one or more processors and one or more memories being configured to: convert a plurality of color values of the color data respectively into gray values different from each other by a predetermined value or more; convert color values of the color data into gray values based on a predetermined formula; identify a first graphic object and a second graphic object that are adjacent to each other in a page with a density difference between the first graphic object and the second graphic object being smaller than a threshold, and have different colors; and, for the first graphic object and the second graphic object identified, perform the conversion into the gray values based on the predetermined formula, without performing the conversion into the gray values different from each other by the predetermined value or more.</p><p id="p-0014" num="0012">Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0015" num="0013">The accompanying drawings, which are incorporated in and constitute a part of the specification, illustrate embodiments of the invention and, together with the description, serve to explain the principles of the invention.</p><p id="p-0016" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for describing a configuration example of a printing system according to a first embodiment of the present invention.</p><p id="p-0017" num="0015"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram for describing a functional configuration of software modules of an image processing apparatus according to the first embodiment.</p><p id="p-0018" num="0016"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for describing an overall flow from analysis on print data on a page to image forming, performed by the image processing apparatus according to the first embodiment.</p><p id="p-0019" num="0017"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are flowcharts for describing processing for determining whether graphics are single color graphic to be a target of discriminability reproducing processing in step S<b>301</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0020" num="0018"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart for describing processing of generating intermediate data in step S<b>302</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0021" num="0019"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a flowchart for describing discriminability reproducing processing in step S<b>505</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0022" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a diagram for describing an example of a color conversion table according to an embodiment.</p><p id="p-0023" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart for describing image forming processing in step S<b>303</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0024" num="0022"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> are flowcharts for describing processing of determining whether an object is to be a discriminability reproduction target in an image processing apparatus according to a second embodiment.</p><p id="p-0025" num="0023"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>9</b>E</figref> are diagrams for describing examples of processing of updating a predetermined range in the processing of determining whether an object is the discriminability reproduction target in the second embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0026" num="0024">Embodiments of the present invention will be described hereinafter in detail, with reference to the accompanying drawings. It is to be understood that the following embodiments are not intended to limit the claims of the present invention, and that not all of the combinations of the aspects that are described according to the following embodiments are necessarily required with respect to the means to solve the problems according to the present invention. Further, in the accompanying drawings, identical or similar components are denoted by identical reference signs, and redundant description will be omitted</p><heading id="h-0007" level="1">First Embodiment</heading><p id="p-0027" num="0025">A first embodiment of the present invention is described below with reference to the drawings. In the first embodiment, discriminability reproducing processing for reproduction with improved discriminability through density adjustment includes determining whether continuous graphics are to be a discriminability reproducing processing target, and switching whether to execute the discriminability reproducing processing on the graphics.</p><p id="p-0028" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for describing a configuration example of a printing system according to the first embodiment of the present invention.</p><p id="p-0029" num="0027">In this printing system, a host computer <b>101</b> and an image processing apparatus <b>110</b> are connected to each other via a LAN <b>102</b>. A user generates, using the host computer <b>101</b>, PDL data indicating information on a page to be printed, and transmits the PDL data from the host computer <b>101</b> to the image processing apparatus <b>110</b> via the LAN <b>102</b>. The image processing apparatus <b>110</b> according to the first embodiment may be a printer that is any of a Multi Function Printer (MFP) and a Single Function Printer (SFP). The image processing apparatus <b>110</b> may be a printer other than the MFP or the SFP, and may be an apparatus that displays and outputs an image.</p><p id="p-0030" num="0028">Next, a hardware configuration example of the image processing apparatus <b>110</b> according to the first embodiment will be described.</p><p id="p-0031" num="0029">The image processing apparatus <b>110</b> includes, for example, a printer unit <b>111</b> that is an image output device. The image processing apparatus <b>110</b> obtains PDL data (print data described in a page description language) from the host computer <b>101</b> via the LAN <b>102</b>.</p><p id="p-0032" num="0030">The printer unit <b>111</b> is connected to a device I/F <b>135</b> and outputs (prints) an image on a sheet (paper) based on image data generated by a CPU <b>120</b>. The CPU <b>120</b> is a central processing unit for controlling (each part of) the entire image processing apparatus <b>110</b>. A RAM <b>122</b> provides a work memory for the CPU <b>120</b> to operate. The RAM <b>122</b> also provides a memory area for temporarily storing the obtained PDL data, intermediate data generated for image forming processing, a work area that is a working area used for performing rendering processing, and input image data. A ROM <b>121</b> is, for example, a boot ROM, and stores a boot program and the like for the apparatus. A storage unit <b>123</b> includes, for example, a hard disk drive, and stores system software for various types of processing, the obtained PDL data, and the like.</p><p id="p-0033" num="0031">A console unit interface(I/F) <b>125</b> is an interface unit for a console unit <b>113</b> including a component such as a display unit for displaying various menus, information on print data instructed to be printed, and the like, and outputs screen data to the console unit <b>113</b>. The console unit I/F <b>125</b> transmits information, input by the user using the console unit <b>113</b>, to the CPU <b>120</b>. A network I/F <b>126</b> exchanges information with an external apparatus (the host computer <b>101</b>, for example) via the LAN <b>102</b>. The CPU <b>120</b>, the ROM <b>121</b>, the RAM <b>122</b>, the storage unit <b>123</b>, the console unit I/F <b>125</b>, and the network I/F <b>126</b> are connected to a system bus <b>127</b>.</p><p id="p-0034" num="0032">An image bus I/F <b>128</b> is an interface for connecting the system bus <b>127</b> with an image bus <b>130</b> for high-speed image data transfer, and functions as a bus bridge for converting a data structure. To the image bus <b>130</b>, a raster image processor (RIP) <b>131</b>, an image editing unit <b>132</b>, an image compression unit <b>133</b>, an image decompression unit <b>134</b>, and a device I/F <b>135</b> are connected. Although these are hardware controlled by the CPU <b>120</b>, a configuration in which these are controlled by a sub processor that can execute a program including a command different from that of the CPU <b>120</b>. The RIP <b>131</b> analyzes the PDL data and the intermediate data (display list, DL) based on an instruction from the CPU <b>120</b>, and rasterizes that data into image data in a raster format. The image editing unit <b>132</b> performs pixel-unit image editing such as color space conversion or contour correction, on the image data rasterized by the RIP <b>131</b>. The image compression unit <b>133</b> compresses the image data obtained by the image rasterizing by the RIP <b>131</b>, and stores the compressed image data in the RAM <b>122</b> through the image bus I/F <b>128</b>. The image decompression unit <b>134</b> decompresses the compressed image data stored in the storage unit <b>123</b> or the RAM <b>122</b>, and transmits the decompressed image data to the RIP <b>131</b> or the device I/F <b>135</b> through the image bus <b>130</b>, in accordance with print processing. When box save is designated by the user, the image compression unit <b>133</b> and the image decompression unit <b>134</b> are used for storing in the storage unit <b>123</b>, the image data in the raster format after the rasterizing by the RIP <b>131</b>, and performing printing in accordance with an instruction from the user. When the rasterizing processing performed by the RIP <b>131</b> is earlier than the print processing by the printer unit <b>111</b>, the image compression unit <b>133</b> and the image decompression unit <b>134</b> may be used for temporarily storing the rasterized image data in the storage unit <b>123</b>. The device I/F <b>135</b> is an interface for transmitting data to the printer unit <b>111</b>.</p><p id="p-0035" num="0033"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram for describing a functional configuration of software modules of the image processing apparatus <b>110</b> according to the first embodiment. Functions of these modules are implemented by the CPU <b>120</b> executing the program deployed onto the RAM <b>122</b> from the ROM <b>121</b> or the storage unit <b>123</b>.</p><p id="p-0036" num="0034">A data input module <b>201</b> receives print data from the host computer <b>101</b> via the network I/F <b>126</b> and the LAN <b>102</b>. A job analyzing module <b>202</b> obtains settings for performing density adjustment and discriminability reproducing processing from the print data and analyzes the settings, or obtains the settings from a setting value set in advance if the settings fail to be obtained from the print data. More specifically, print setting information such as the Page Description Language (PDL) and Job Definition Format (JDF) included in the print data is obtained and analyzed. In the first embodiment, only a case where the discriminability reproducing processing is set will be described. A page analyzing module <b>203</b> analyzes information on each page. Specifically, the page description language included in the print data is obtained, and rendering commands included in each page are analyzed to generate the intermediate data. An image forming module <b>204</b> controls the RIP <b>131</b>, the image editing unit <b>132</b>, the image compression unit <b>133</b>, and the image decompression unit <b>134</b>. A printer control module <b>205</b> controls the device I/F <b>135</b> and the printer unit <b>111</b>.</p><p id="p-0037" num="0035">A color information obtaining module <b>211</b> obtains settings, related to color processing, included in the print data. Thus, the settings include a gamma correction value and designation of an ICC profile used, in addition to the settings for performing the density adjustment and the discriminability reproducing processing, but a description on these will be omitted in the first embodiment. A color information analyzing module <b>212</b> analyzes information on a rendering color and an object (graphic object) from rendering commands for each page analyzed by the page analyzing module <b>203</b>. Finally, a color conversion module <b>213</b> performs conversion to gray data (monochrome data), based on the result of the analysis by the color information analyzing module <b>212</b>.</p><p id="p-0038" num="0036">Although the configuration of the software module in the first embodiment is as described above, a configuration in which each module includes a submodule may also be employed as a matter of course. Processing from data input to output to the printer unit <b>111</b> according to the first embodiment will be described below with reference to the following flowcharts.</p><p id="p-0039" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for describing an overall flow from analysis on print data on a page to image forming, performed by the image processing apparatus <b>110</b> according to the first embodiment. Note that the processing illustrated in this flowchart is implemented by the CPU <b>120</b> executing the program deployed onto the RAM <b>122</b>.</p><p id="p-0040" num="0038">In step S<b>301</b>, the CPU <b>120</b> functions as the page analyzing module <b>203</b> to analyze the print data, and obtain information on an object included therein. The CPU <b>120</b> functions as the color information analyzing module <b>212</b> to determine whether single color graphics to be excluded from the target of the discriminability reproducing processing are included in graphics designated to be filled with single color (hereinafter, referred to as single color graphics). Details of this processing will be described later with reference to the flowcharts of <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref>.</p><p id="p-0041" num="0039">Next, the processing proceeds to step S<b>302</b>, and the CPU <b>120</b> functions as the page analyzing module <b>203</b> and the color information analyzing module <b>212</b> to receive the object that has been determined to be excluded from the target of the discriminability reproduction or not, and generate intermediate data on the object. In the processing of generating the intermediate data, processing (generation of a color conversion table) of reproducing with improved discriminability is performed on the object that is the discriminability reproducing processing target. By performing this discriminability reproducing processing (generation of the color conversion table), when an image of the object that is the discriminability reproducing processing target is formed, a grayscale image can be output with improved discriminability by adjusting the density thereof. Details of processing of generating intermediate data on a page for which the discriminability reproducing processing is performed will be described below with reference to a flowchart in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Note that this discriminability reproducing processing is, for example, processing as described in Japanese Patent Laid-Open No. 2020-49939, but the present invention is not limited to such processing.</p><p id="p-0042" num="0040">Next, the processing proceeds to step S<b>303</b>, where the CPU <b>120</b> functions as the image forming module <b>204</b> to, while using the RIP <b>131</b>, receive the intermediate data and perform image forming processing. Here, grayscale conversion with improved discriminability is performed, with the original color indicated by a density difference, by using the color conversion table for the discriminability reproducing processing target object. On the other hand, color conversion for all target colors is performed on an object excluded from the target of discriminability reproducing processing.</p><p id="p-0043" num="0041">In this manner, an image involving a risk of having discriminability compromised when converted into a grayscale image can be formed as a converted grayscale image provided with the discriminability reproducing processing. Furthermore, a color image involving a risk of a compromised gray image when performed by the discriminability reproducing processing is excluded from the target of the discriminability reproducing processing, whereby the gray image can be prevented from being compromised. Details of the image forming processing will be described later with reference to a flowchart in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0044" num="0042"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are flowcharts for describing processing for determining whether graphics are single color graphics to be the target of the discriminability reproducing processing in step S<b>301</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The processing described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> needs to be performed before intermediate data generation processing (step S<b>302</b>) described with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0045" num="0043">First of all, in step S<b>401</b>, the CPU <b>120</b> functions as the color information analyzing module <b>212</b> to determine whether there is an object to be read. When there is such an object, the processing proceeds to step S<b>402</b>. When there is no such object, the processing proceeds to step S<b>411</b>. In step S<b>402</b>, the CPU <b>120</b> extracts and obtains the object. Next, the processing proceeds to step S<b>403</b>, and the CPU <b>120</b> determines whether the object is a single color graphic having a width and height smaller than a threshold (smaller than a predetermined value), based on position information and color information on the object obtained. When the object is a single color graphic having a width and height smaller than the threshold, the processing proceeds to step S<b>404</b>. Otherwise, the processing proceeds to step S<b>411</b>.</p><p id="p-0046" num="0044">This is because an object having a width and height not smaller than the threshold is the discriminability reproducing processing target object, that is, not to be excluded from the target of discriminability reproducing processing.</p><p id="p-0047" num="0045">In step S<b>404</b>, the CPU <b>120</b> determines whether there is a spooled object. When there is no spooled object, the processing proceeds to step S<b>405</b>. When there is a spooled object, the processing proceeds to step S<b>407</b>. In step S<b>405</b>, the CPU <b>120</b> turns on a same color flag to determine whether objects to be spooled all have the same color, and the processing proceeds to step S<b>406</b>. In step S<b>406</b>, the CPU <b>120</b> spools the currently obtained object. Then, the processing proceeds to step S<b>401</b> to obtain the next object. And thus, a state is achieved where an object satisfying the first condition is spooled, and the same color flag is on.</p><p id="p-0048" num="0046">On the other hand, in step S<b>407</b>, because a single color object satisfying the condition is successfully detected in a state where there are objects spooled though step S<b>401</b> to step S<b>404</b>, the CPU <b>120</b> determines whether an overlap with the immediately preceding object is smaller than a threshold. When the overlap is smaller than the threshold, the processing proceeds to step S<b>408</b>. Otherwise, the processing proceeds to step S<b>412</b>. This is because an object with the overlap with the immediately preceding object not smaller than the threshold is the discriminability reproducing processing target object, that is, not to be excluded from the target of discriminability reproducing processing.</p><p id="p-0049" num="0047">In step S<b>408</b>, the CPU <b>120</b> determines whether a density difference of each color component between a rendering color (a color value before the color conversion) of the immediately preceding object and a rendering color (a color value before the color conversion) of the currently obtained object is smaller than a threshold. When the density difference of each color component between the objects is smaller than the threshold, the processing proceeds to step S<b>409</b>. Otherwise, the processing proceeds to step S<b>412</b>. This is because an object with the density difference from the immediately preceding object not smaller than the threshold is the discriminability reproducing processing target object, that is, not to be excluded from the target of discriminability reproducing processing.</p><p id="p-0050" num="0048">In step S<b>409</b>, the CPU <b>120</b> determines whether the current object with the density difference of each color component smaller than the threshold has the same color as the immediately preceding object (whether all the corresponding color components are of the same value). When the objects are not determined to have the same color, the processing proceeds to step S<b>410</b> where the same color flag is turned off, and then the processing proceeds to step S<b>406</b>. Thus, when the immediately preceding object and the current object are single color graphics that are adjacent to each other and have different but similar colors, the current object is determined to be a candidate of the non-discriminability reproducing processing target. When the objects are determined to have the same color in step S<b>409</b>, the processing proceeds to step S<b>406</b>, without turning off the same color flag, to spool the currently obtained object, and then the processing proceeds to step S<b>401</b>.</p><p id="p-0051" num="0049">Thus, the processing proceeds to step S<b>411</b> where the CPU <b>120</b> determines whether there is a spooled object, when there is no object to be read in step S<b>401</b>, or when the object does not satisfy the condition in step S<b>403</b>, that is, when the object is not a single color graphic having a width and height smaller than the threshold. When it is determined that there is a spooled object in step S<b>411</b>, the processing proceeds to step S<b>412</b>. When there is no spooled object, this processing is terminated. In step S<b>412</b>, the CPU <b>120</b> determines whether the number of spooled objects is not smaller than a threshold. When the number is determined to be smaller than the threshold, such an object is to be the discriminability reproducing processing target. Thus, the processing skips steps S<b>413</b> and S<b>414</b> and proceeds to step S<b>415</b> where the spooled objects are flushed (saved). Then, the processing is terminated.</p><p id="p-0052" num="0050">On the other hand, when the number of spooled objects is not smaller than the threshold in step S<b>412</b>, the processing proceeds to step S<b>413</b>, where whether the same color flag is off is determined. The same color flag being off indicates that the spooled objects form single color graphics that are adjacent to each other and have different but similar colors. A group of such objects is desirably excluded from the target of discriminability reproducing processing. Thus, the processing proceeds to step S<b>414</b>, where the spooled objects are provided with a flag indicating the non-discriminability reproduction target. Then, the processing proceeds to step S<b>415</b> where the spooled objects are flushed, and then the processing is terminated.</p><p id="p-0053" num="0051">The same color flag being on in step S<b>413</b> indicates that the spooled objects all have the same color, and thus are a group of single color graphics. Such a group of objects is to be the discriminability reproducing processing target. Thus, the processing proceeds to step S<b>415</b>, without performing any processing on the spooled objects, where the objects are flushed, and the processing is terminated.</p><p id="p-0054" num="0052">With this processing, when the overlap with the immediately preceding object is not smaller than the threshold, the density difference from the immediately preceding object is not smaller than the threshold, or when the number of objects is smaller than the threshold, the objects can be the discriminability reproducing processing target. Furthermore, when these objects have the same color, the objects can be the discriminability reproducing processing target for improving the discriminability.</p><p id="p-0055" num="0053">On the other hand, when the overlap with the immediately preceding object is smaller than the threshold, when the density difference from the immediately preceding object is smaller than the threshold, the number of objects is not smaller than the threshold, and when the objects do not have the same color, the objects can be excluded from the target of discriminability reproducing processing.</p><p id="p-0056" num="0054"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart for describing processing of generating intermediate data in step S<b>302</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0057" num="0055">First of all, in step S<b>501</b>, the CPU <b>120</b> functions as the page analyzing module <b>203</b> to obtain an object from the rendering commands. Next, the processing proceeds to step S<b>502</b>, and the CPU <b>120</b> determines whether the obtained object is text or graphic. When the object is text or graphic, the processing proceeds to step S<b>503</b>. When the object is something other than text or graphic, for example, the objects has gradation of a photograph and the like for example, the processing proceeds to step S<b>506</b>, without executing the discriminability reproducing processing. When the object is graphic, the CPU <b>120</b> determines whether the object is a single color graphic in step S<b>503</b>. When the object is a single color graphic, the processing proceeds to step S<b>504</b>. When the object is something (such as gradation) other than a single color graphic, the processing proceeds to step S<b>506</b>, without performing the discriminability reproducing processing.</p><p id="p-0058" num="0056">In step S<b>504</b>, the CPU <b>120</b> functions as the page analyzing module <b>203</b> to determine whether the object is provided with the flag indicating the non-discriminability reproduction target provided in step S<b>414</b> in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> as described above. When the flag indicating the non-discriminability reproduction target is not provided, the processing proceeds to step S<b>505</b> where the discriminability reproducing processing is performed. On the other hand, when the flag indicating the non-discriminability reproduction target is provided, the processing skips the discriminability reproducing processing in step S<b>505</b>, and proceeds to step S<b>506</b>. In step S<b>505</b>, the CPU <b>120</b> functions as the color information analyzing module <b>212</b> to execute the discriminability reproducing processing on the target text and graphics. Then, the processing proceeds to step S<b>506</b>. Details of the discriminability reproducing processing will be described later with reference to <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. In step S<b>506</b>, the CPU <b>120</b> functions as the page analyzing module <b>203</b> to convert the rendering commands into intermediate data. Specifically, the rendering commands expressed using relative coordinates and Bezier curve and registered characters (font) are converted into a path point sequence of absolute coordinates conforming to the actual output resolution and paper size.</p><p id="p-0059" num="0057">Thus, no discriminability reproducing processing is performed on an object other than text or graphic, or an object that is not a graphic filled with single color. On the other hand, a single color graphic object that is text or graphic not provided with the flag indicating the non-discriminability reproduction target is the discriminability reproducing processing target.</p><p id="p-0060" num="0058"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a flowchart for describing discriminability reproducing processing in step S<b>505</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a diagram illustrating a specific example of gray conversion.</p><p id="p-0061" num="0059">First of all, in step S<b>601</b>, the CPU <b>120</b> functions as the color information analyzing module <b>212</b> to obtain information on a rendering color from the object read by the page analyzing module <b>203</b>, and count the number of colors used. When the color value at this point corresponds to the color already used for the target object in the page, the number of colors is not incremented. Next, the processing proceeds to step S<b>602</b>, and the CPU <b>120</b> determines whether the number of colors used for the target object in the page is not smaller than a predetermined threshold (not smaller than a predetermined value). More specifically, it is determined whether color data can be converted into gray data with a density difference rendering it visible, that is, whether the total number of colors exceeds the gradation number of the gray data that can be expressed with the printer unit <b>111</b>, or the like. When the number of colors used in the page is determined to be smaller than the predetermined threshold, the processing proceeds to step S<b>603</b>, and the CPU <b>120</b> executes processing of generating a color conversion table for color data to achieve a density difference. Then, this processing is terminated. This color conversion table is a table for performing the color conversion in accordance with a predetermined color conversion scheme, and is here a table storing, in association, a color value of each color used in the page and a color value for converting the color value into a gray value with a visible density difference. As illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, for example, this color conversion table stores, in association, the color value before the conversion and the color value after the conversion. Such a color conversion table is described in, for example, Japanese Patent Laid-Open No. 2020-49939 and Japanese Patent Laid-Open No. 2017-38242 described above, and the like. The gray color value after the conversion corresponding to the color value of the color before the conversion in the color conversion table is determined, in accordance with the number of colors in a page and a possible range of the gray color value after the conversion. For example, when the number of colors in the page is three, and the gray color values after the conversion are in a range from 0 to 255, the gray color values are determined with an interval &#x201c;51&#x201d; as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. On the other hand, when the number of colors used in the page is determined to be not smaller than the predetermined value in step S<b>602</b>, the processing proceeds to step S<b>604</b>, and the CPU <b>120</b> switches the processing to execute color conversion for all target colors on all the objects, without generating the color conversion table. Then, this processing is terminated.</p><p id="p-0062" num="0060">Thus, the color conversion using the color conversion table is executed on a discriminability reproducing processing target object, whereby color data on the object can be converted into gray data with improved discriminability. When the number of colors in the page is not smaller than the threshold, the conversion using the color conversion table cannot be performed, and thus the processing is switched to execute the color conversion for all target colors.</p><p id="p-0063" num="0061"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart for describing image forming processing in step S<b>303</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0064" num="0062">In step S<b>701</b>, the CPU <b>120</b> obtains intermediate data. Next, the processing proceeds to step S<b>702</b>, and the CPU <b>120</b> determines whether the color conversion table exists. When it is determined that the color conversion table exists, the location of the color conversion table is obtained, and the processing proceeds to step S<b>703</b>. On the other hand, when the color conversion table is determined not to exist, the processing proceeds to step S<b>706</b>, and grayscale conversion is performed through color conversion for all target colors on all the objects. Then, the processing proceeds to step S<b>707</b>, and the CPU <b>120</b> functions as the image forming module <b>204</b> to perform image forming processing of performing conversion into a raster format image formed from the intermediate data converting into a grayscale in accordance with the output resolution and the paper size. Then, this processing is terminated.</p><p id="p-0065" num="0063">In step S<b>703</b>, the CPU <b>120</b> obtains the color conversion table based on the location of the color conversion table obtained in step S<b>702</b>. Next, the processing proceeds to step S<b>704</b>, and the CPU <b>120</b> determines whether the object for which the color conversion is performed is a target object of the discriminability reproducing processing using the color conversion table. When the object is determined to be the discriminability reproducing processing target, the processing proceeds to step S<b>705</b>, and color conversion is executed with a certain color as a target, to reproduce the discriminability with a density difference provided to the certain color, using the color conversion table. Then, the processing proceeds to step S<b>707</b>. On the other hand, when the object is determined to be excluded from the target of discriminability reproducing processing in step S<b>704</b>, the processing proceeds to step S<b>706</b>, and grayscale conversion is performed through color conversion for all target colors. Then, the processing proceeds to step S<b>707</b>.</p><p id="p-0066" num="0064">The processing described above is image forming processing performed by the image forming module <b>204</b> notified of the intermediate data for a single page and the color conversion method, and is executed by the RIP <b>131</b> and the image editing unit <b>132</b> under an instruction from the page analyzing module <b>203</b> executed by the CPU <b>120</b>.</p><p id="p-0067" num="0065">Through the processing, reproducing processing for reproduction with improved discriminability through density adjustment can include determining whether continuous graphics are graphics to be a discriminability reproducing processing target, and switching whether to execute the discriminability reproducing processing on the graphics. Thus, continuous graphics requiring no processing of improving discriminability can be excluded from the target of discriminability reproducing processing, whereby the appearance of the graphics can be prevented from being compromised. On the other hand, it is possible to improve the discriminability of the gray image including the graphics for which the discriminability is required to be improved.</p><p id="p-0068" num="0066">Specifically, when the objects are single color graphics that are adjacent to each other and have different but similar colors, the adjacent graphics can be excluded from the target of discriminability reproducing processing, whereby the appearance of the graphics can be prevented from being compromised.</p><heading id="h-0008" level="1">Second Embodiment</heading><p id="p-0069" num="0067">A second embodiment of the present invention is described below with reference to the drawings. In the first embodiment described above, the processing is performed to make a determination in the order of objects in a page. In the second embodiment on the other hand, in reproducing processing with improved discriminability through density adjustment, graphics appearing in a random order can be determined to be graphics that are a discriminability reproducing processing target or not. Then, based on the determination, whether the discriminability reproducing processing is executed on the graphics or not executed is switched. The hardware configurations and the like of the printing system and the image processing apparatus <b>110</b> according to the second embodiment are the same as those in the first embodiment described above, and thus a description thereof will be omitted. Points different from the first embodiment are described below.</p><p id="p-0070" num="0068"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> are flowcharts for describing processing of determining whether an object is to be a discriminability reproduction target in the image processing apparatus <b>110</b> according to the second embodiment.</p><p id="p-0071" num="0069"><figref idref="DRAWINGS">FIGS. <b>9</b>A-<b>9</b>E</figref> are diagrams illustrating examples of processing of updating a predetermined range in the processing of determining whether an object is the discriminability reproduction target in the second embodiment.</p><p id="p-0072" num="0070">In the second embodiment, a method that corresponds to the processing of providing a flag indicating the non-discriminability reproducing processing target, and is different from the method described with reference to <figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> in the first embodiment will be described. In the second embodiment, processing, implemented by the CPU <b>120</b> functioning as the color information obtaining module <b>211</b> and the color information analyzing module <b>212</b>, to determine whether single color graphics appearing in a random order are the single color graphics that are the discriminability reproducing processing target is described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> and <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>9</b>E</figref>.</p><p id="p-0073" num="0071">First of all, in step S<b>801</b>, the CPU <b>120</b> adds and holds all single color graphics having a width and height smaller than a threshold in a list in advance. Processing from step S<b>802</b> is performed in the order of the objects held in the list. In step S<b>802</b>, the CPU <b>120</b> obtains the position coordinates and rendering color of a reference object, and spools the object. The first object in the list is first determined as the reference object. Then, the processing proceeds to step S<b>803</b>, and the CPU <b>120</b> turns on the same color flag to determine whether objects to be spooled all have the same color.</p><p id="p-0074" num="0072">Next, the processing proceeds to step S<b>804</b>, and the CPU <b>120</b> determines whether there is a next object. When there is the next object, the processing proceeds to step S<b>805</b> to obtain the object, and then the processing proceeds to step S<b>806</b>. When there is no next object, the processing proceeds to step S<b>812</b>. In step S<b>806</b>, the CPU <b>120</b> determines whether the next object obtained in step S<b>805</b> exists within a predetermined range from the first reference object. When the object is determined to exist within the predetermined range, the processing proceeds to step S<b>807</b>. When the object is outside the predetermined range, the object is not the target of the processing of determining whether the object is the target of the discriminability reproducing processing, and thus the processing proceeds to step S<b>804</b>. In step S<b>807</b>, the CPU <b>120</b> determines whether a difference of each color component (density difference) between a rendering color (a color value before the color conversion) of the reference object and a rendering color (a color value before the color conversion) of the next object obtained is smaller than a threshold. When the density difference of each color component is smaller than the threshold, the processing proceeds to step S<b>808</b>. Otherwise, the object is not the target of the determination processing, and thus the processing proceeds to step S<b>804</b>. In step S<b>808</b>, the CPU <b>120</b> determines whether the rendering colors of the reference object and the next object are the same (all the color components have the same value). When the colors are not the same, the object is the non-discriminability reproduction target, and thus the same color flag turned off in step S<b>809</b> and the processing proceeds to step S<b>810</b>. When the colors are the same, the processing proceeds to step S<b>810</b>, and the object is spooled. Then, the processing proceeds to step S<b>811</b>. In step S<b>811</b>, the CPU <b>120</b> updates the predetermined range, and the processing proceeds to step S<b>804</b>.</p><p id="p-0075" num="0073">Thus, when the current object and the next object are found to be single color graphics existing within the predetermined range and having different but similar colors, the objects can be candidates of the non-discriminability reproducing processing target.</p><p id="p-0076" num="0074">Here, a specific example of determining the predetermined range in step S<b>811</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>9</b>E</figref>.</p><p id="p-0077" num="0075"><figref idref="DRAWINGS">FIG. <b>9</b>A</figref> depicts a view illustrating a case where only one reference object <b>901</b> is spooled. A minimum rectangle surrounding the reference object <b>901</b> is obtained, and a range of a certain distance from the rectangle is determined as a predetermined range <b>902</b>.</p><p id="p-0078" num="0076"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> depicts a view illustrating a case where a comparative object (next object) <b>903</b> is found that exists in the predetermined range <b>902</b> and satisfies the spool condition of the density difference being smaller than the threshold. In this case, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>, a minimum rectangle surrounding the reference object <b>901</b> and the comparative object <b>903</b> is obtained, and the range is updated to a new predetermined range <b>904</b> at a certain distance from the minimum rectangle. Similarly, <figref idref="DRAWINGS">FIG. <b>9</b>D</figref> depicts a view illustrating a case where a comparative object <b>905</b> is found that exists in the predetermined range <b>904</b> and satisfies the spool condition of the density difference being smaller than the threshold. In this case, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>E</figref>, a minimum rectangle <b>907</b> surrounding the reference object <b>901</b>, the comparative object <b>903</b>, and the comparative object <b>905</b> is obtained, and the range is updated to a new predetermined range <b>906</b> at a certain distance from the minimum rectangle <b>907</b>.</p><p id="p-0079" num="0077">In this manner, when the predetermined range is updated in step S<b>811</b>, the processing proceeds to step S<b>804</b> to process the next object. With the processing in step S<b>805</b> to step S<b>811</b> thus repeatedly executed on the objects in the list, single color graphics having a short distance from each other and similar colors are detected.</p><p id="p-0080" num="0078">When the comparison between the current reference object and the objects in the list is completed, the processing proceeds to step S<b>812</b>, and the CPU <b>120</b> determines whether the number of objects that satisfy the spool condition is not smaller than the threshold. When the number is determined to be smaller than the threshold, the processing proceeds to step S<b>816</b> to perform the discriminability reproducing processing, and the objects spooled are flushed. Then, the processing proceeds to step S<b>817</b>. This is for, when the number of objects is smaller than the threshold, not performing the determination on whether the objects are to be excluded from the target of discriminability reproducing processing, because the image is not largely deteriorated even when the discriminability reproducing processing is executed on the objects.</p><p id="p-0081" num="0079">On the other hand, when the number of objects is not smaller than the threshold, the processing proceeds to step S<b>813</b>, and the CPU <b>120</b> determines whether the same color flag is off. When the same color flag is determined to be off, the objects spooled form single color graphics existing within a certain distance and having different but similar colors. Thus, in this case, such a group of objects is desirably excluded from the target of discriminability reproducing processing, and thus the processing proceeds to step S<b>814</b>. In step S<b>814</b>, the CPU <b>120</b> provides the spooled objects with the flag indicating the non-discriminability reproduction target. Then, the processing proceeds to step S<b>815</b>, and the CPU <b>120</b> removes the objects provided with the flag from the list and the processing proceeds to step S<b>816</b>, to flush the spooled objects. Then, the processing proceeds to step S<b>817</b>.</p><p id="p-0082" num="0080">On the other hand, when the CPU <b>120</b> determines that the same color flag is on in step S<b>813</b>, it indicates that the objects spooled all have the same color, and thus are a group of single color graphics. The group of objects are to be the discriminability reproducing processing target, and thus the processing skips step S<b>814</b> and step S<b>815</b>, to proceed to step S<b>816</b> to flush the spooled objects, without processing the objects spooled.</p><p id="p-0083" num="0081">Then, the processing proceeds to step S<b>817</b>, and the CPU <b>120</b> removes the current reference object from the list if such a current object remains in the list. The processing proceeds to step S<b>818</b> when an object still exists in the list. In step S<b>818</b>, the CPU <b>120</b> updates such an object to be the reference object. When the object is thus updated, the processing proceeds to step S<b>802</b>, and whether the object is formed by single color graphics that are concentrated within the certain distance and have different but similar colors is determined as described above. When the CPU <b>120</b> determines that there is no object to be the new reference object in the list in step S<b>817</b>, this processing is terminated.</p><p id="p-0084" num="0082">As described above, even when adjacent graphics with similar colors are configured as discontinuous commands or even when adjacent graphics with similar colors do not overlap, the graphics are detected as adjacent graphics with similar colors and whether they are the target of the discriminability reproducing processing can be determined, as long as the graphics are within the certain distance.</p><p id="p-0085" num="0083">When the graphics are configured as discontinuous commands, the determination processing may be executed with the graphics sorted based on the position information on each of the graphics. The value of the certain distance may be changed as appropriate.</p><p id="p-0086" num="0084">With the second embodiment as described above, in reproducing processing with improved discriminability through density adjustment, graphics appearing in a random order can be determined to be graphics that are a discriminability reproducing processing target or not. Thus, whether the discriminability reproducing processing is executed or not executed can be switched in accordance with the graphics. With this configuration, graphics requiring no processing of improving the discriminability are excluded from the target of discriminability reproducing processing, whereby the appearance of a gray image including the graphics can be prevented from being compromised. Furthermore, discriminability of graphics requiring the processing of improving the discriminability is improved, whereby reproducibility can be improved.</p><heading id="h-0009" level="1">Other Embodiments</heading><p id="p-0087" num="0085">Embodiments of the present invention can also be realized by a computer of a system or apparatus that reads out and executes computer executable instructions (e.g., one or more programs) recorded on a storage medium (which may also be referred to more fully as a &#x2018;non-transitory computer-readable storage medium&#x2019;) to perform the functions of one or more of the above-described embodiments and/or that includes one or more circuits (e.g., application specific integrated circuit (ASIC)) for performing the functions of one or more of the above-described embodiments, and by a method performed by the computer of the system or apparatus by, for example, reading out and executing the computer executable instructions from the storage medium to perform the functions of one or more of the above-described embodiments and/or controlling the one or more circuits to perform the functions of one or more of the above-described embodiments. The computer may comprise one or more processors (e.g., central processing unit (CPU), micro processing unit (MPU)) and may include a network of separate computers or separate processors to read out and execute the computer executable instructions. The computer executable instructions may be provided to the computer, for example, from a network or the storage medium. The storage medium may include, for example, one or more of a hard disk, a random-access memory (RAM), a read only memory (ROM), a storage of distributed computing systems, an optical disk (such as a compact disc (CD), digital versatile disc (DVD), or Blu-ray Disc (BD)&#x2122;), a flash memory device, a memory card, and the like.</p><p id="p-0088" num="0086">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p><p id="p-0089" num="0087">This application claims the benefit of Japanese Patent Application No. 2021-109415, filed Jun. 30, 2021, which is hereby incorporated by reference herein in its entirety.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing apparatus for converting color data into gray data, the image processing apparatus comprising:<claim-text>one or more processors and one or more memories being configured to:</claim-text><claim-text>extract an object included in the color data in a page;</claim-text><claim-text>determine, based on color information and position information on a first object and a second object extracted, whether at least one of the first object and the second object is to be a target of conversion of a color value of the color data into a gray value with improved discriminability; and</claim-text><claim-text>control to convert, based on the determining, the color value of the object into a gray value by a conversion targeting all color values, or into a gray value with the improved discriminability.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the determining, it is determined to convert the color values of the first object and the second object into the gray value by the conversion targeting all color values, in a case where a distance between the first object and the second object extracted is smaller than a first threshold and a density difference between the first object and the second object is smaller than a second threshold, and in a case where the first object and the second object have different colors.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the determining, it is determined to convert the color values of the first object and the second object into the gray value with the improved discriminability, in a case where a distance between the first object and the second object extracted is not smaller than a first threshold or a density difference between the first object and the second object is not smaller than a second threshold and the first object and the second object have a same color.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the determining, it is determined to exclude an object other than text or graphic from a target of the conversion into a gray value with the improved discriminability.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the determining, it is determined to exclude an object other than a graphic filled with single color from a target of the conversion into a gray value with the improved discriminability.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the determining, it is determined to convert the object extracted into the gray value by the conversion targeting all the color values, in a case where a number of colors included in the color data is not smaller than a predetermined value.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining is performed in an order of objects in the page.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the determining, it is determined that an object, among objects that are each a single color graphic having a width and height in the page being smaller than a threshold, forming single color graphics that exist within a predetermined range of a certain distance and have different but similar colors, is excluded from a target of the conversion into a gray value with the improved discriminability.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image processing apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors and the one or more memories being further configured to:<claim-text>update, in a case where a first object to be a reference and a second object forming single color graphics having different but similar colors exist, the predetermined range to a range at a certain distance from a minimum rectangle surrounding the first object and the second object.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object is a single color graphic having a width and height smaller than a predetermined value.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in the conversion to a gray value with the improved discriminability, the one or more processors and the one or more memories are configured to:<claim-text>generate a color conversion table conforming to a predetermined color conversion scheme in accordance with number of colors used in the page; and</claim-text><claim-text>convert, based on the color conversion table, color values of respective colors of intermediate data of the color data in the page into gray values with a density difference.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image processing apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the conversion targeting all color values is a conversion scheme with which all the color values are converted under a same rule.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. An image processing apparatus for converting color data into gray data, the image processing apparatus comprising:<claim-text>one or more processors and one or more memories being configured to:</claim-text><claim-text>convert a plurality of color values of the color data respectively into gray values different from each other by a predetermined value or more;</claim-text><claim-text>convert color values of the color data into gray values based on a predetermined formula;</claim-text><claim-text>identify a first graphic object and a second graphic object that are adjacent to each other in a page with a density difference between the first graphic object and the second graphic object being smaller than a threshold, and have different colors; and</claim-text><claim-text>for the first graphic object and the second graphic object identified, perform the conversion into the gray values based on the predetermined formula, without performing the conversion into the gray values different from each other by the predetermined value or more.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A method of controlling an image processing apparatus for converting color data into gray data, the method comprising:<claim-text>converting a color value of the color data into a gray value with improved discriminability;</claim-text><claim-text>converting a color value of the color data into a gray value by a conversion targeting all color values;</claim-text><claim-text>extracting an object included in the color data in a page;</claim-text><claim-text>determining, based on color information and position information on a first object and a second object extracted, whether at least one of the first object and the second object is to be a target of conversion into a gray value with improved discriminability; and</claim-text><claim-text>converting, based on the determining, the color value of the object into a gray value by a conversion targeting all color values, or a conversion into a gray value with the improved discriminability.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method of controlling an image processing apparatus for converting color data into gray data, the method comprising:<claim-text>converting a plurality of color values of the color data respectively into gray values different from each other by a predetermined value or more;</claim-text><claim-text>converting color values of the color data into gray value based on a predetermined formula;</claim-text><claim-text>identifying a first graphic object and a second graphic object that are adjacent to each other in a page with a density difference between the first graphic object and the second graphic object being smaller than a threshold, and have different colors; and</claim-text><claim-text>for the first graphic object and the second graphic object identified, performing the conversion into the gray values based on the predetermined formula, without performing the conversion into the gray values different from each other by the predetermined value or more.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory computer-readable storage medium storing a program for causing a processor to execute a method of controlling an image processing apparatus for converting color data into gray data, the method comprising:<claim-text>converting a color value of the color data into a gray value with improved discriminability;</claim-text><claim-text>converting a color value of the color data into a gray value by a conversion targeting all color values;</claim-text><claim-text>extracting an object included in the color data in a page;</claim-text><claim-text>determining, based on color information and position information on a first object and a second object extracted, whether at least one of the first object and the second object is to be a target of conversion into a gray value with improved discriminability; and</claim-text><claim-text>converting, based on the determining, the color value of the object into a gray value by a conversion targeting all color values, or a conversion into a gray value with the improved discriminability.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A non-transitory computer-readable storage medium storing a program for causing a processor to execute a method of controlling an image processing apparatus for converting color data into gray data, the method comprising:<claim-text>converting a plurality of color values of the color data respectively into gray values different from each other by a predetermined value or more;</claim-text><claim-text>converting color values of the color data into gray value based on a predetermined formula;</claim-text><claim-text>identifying a first graphic object and a second graphic object that are adjacent to each other in a page with a density difference between the first graphic object and the second graphic object being smaller than a threshold, and have different colors; and</claim-text><claim-text>for the first graphic object and the second graphic object identified, performing the conversion into the gray values based on the predetermined formula, without performing the conversion into the gray values different from each other by the predetermined value or more.</claim-text></claim-text></claim></claims></us-patent-application>