<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005495A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005495</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364583</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0272</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>93</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0272</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>93</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>51</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR VIRTUAL MEETING SPEAKER SEPARATION</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>RingCentral, Inc</orgname><address><city>Belmont</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kukde</last-name><first-name>Prashant</first-name><address><city>Milpitas</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Hiray</last-name><first-name>Sushant Shivram</first-name><address><city>Nashik, Maharashtra</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer-implemented machine learning method for improving speaker separation is provided. The method comprises processing audio data to generate prepared audio data and determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file. The method further comprises re-segmenting the audio file to generate a speaker segment and causing to display the speaker segment through a client device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="128.27mm" wi="158.75mm" file="US20230005495A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="110.49mm" wi="163.66mm" file="US20230005495A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="121.84mm" wi="151.55mm" file="US20230005495A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="134.96mm" wi="164.93mm" file="US20230005495A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="131.06mm" wi="146.98mm" file="US20230005495A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="115.91mm" wi="185.84mm" file="US20230005495A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="121.16mm" wi="144.86mm" file="US20230005495A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="134.79mm" wi="161.71mm" file="US20230005495A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="78.74mm" wi="139.28mm" file="US20230005495A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="119.63mm" wi="144.19mm" file="US20230005495A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="112.01mm" wi="167.81mm" file="US20230005495A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates generally to the field of virtual meetings. Specifically, the present disclosure relates to systems and methods for separating multiple speakers during a video, audio, virtual reality (VR), and/or augmented reality (AR) conference.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The approaches described in this section are approaches that could be pursued, but not necessarily approaches that have been previously conceived or pursued. Therefore, unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section.</p><p id="p-0004" num="0003">Virtual conferencing has become a standard method of communication for both professional and personal meetings. Multiple people often join a virtual conferencing session from a single conference room for work. For personal use, a single household with multiple individuals often initiates a virtual conferencing session with another household with other members of the family. In such scenarios, separating each individual speaker in a room with multiple speakers becomes both a practical and a technical challenge. Such meetings often contain an unknown number of speakers, variability in speaker environments, overlaps in speech from different speakers, unbalanced talk time of individual speakers, and gender variability, all of which make speaker separation and identification during an active conferencing session difficult. Therefore, there is a need for an improved virtual conferencing system that automatically and intelligently separates multiple speakers.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">The appended claims may serve as a summary of the invention.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a network diagram depicting a networked collaboration system, in an example embodiment.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of a server system, in an example embodiment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a relational node diagram depicting a neural network, in an example embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a speaker separation process, in an example embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of speaker separation, in an example embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart depicting an audio preparation process for separating multiple speakers, in an example embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart depicting an iterative block for separating multiple speakers, in an example embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart depicting a re-segmentation and display of separated speakers, in an example embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart depicting a speaker separation process, in an example embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram of an example conference server, in an example embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0016" num="0015">Before various example embodiments are described in greater detail, it should be understood that the embodiments are not limiting, as elements in such embodiments may vary. It should likewise be understood that a particular embodiment described and/or illustrated herein has elements which may be readily separated from the particular embodiment and optionally combined with any of several other embodiments or substituted for elements in any of several other embodiments described herein.</p><p id="p-0017" num="0016">It should also be understood that the terminology used herein is for the purpose of describing concepts, and the terminology is not intended to be limiting. Unless defined otherwise, all technical and scientific terms used herein have the same meaning as commonly understood by those skilled in the art to which the embodiment pertains.</p><p id="p-0018" num="0017">Unless indicated otherwise, ordinal numbers (e.g., first, second, third, etc.) are used to distinguish or identify different elements or steps in a group of elements or steps, and do not supply a serial or numerical limitation on the elements or steps of the embodiments thereof. For example, &#x201c;first,&#x201d; &#x201c;second,&#x201d; and &#x201c;third&#x201d; elements or steps need not necessarily appear in that order, and the embodiments thereof need not necessarily be limited to three elements or steps. It should also be understood that the singular forms of &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise.</p><p id="p-0019" num="0018">Some portions of the detailed descriptions that follow are presented in terms of procedures, methods, flows, logic blocks, processing, and other symbolic representations of operations performed on a computing device or a server. These descriptions are the means used by those skilled in the arts to most effectively convey the substance of their work to others skilled in the art. In the present application, a procedure, logic block, process, or the like, is conceived to be a self-consistent sequence of operations or steps or instructions leading to a desired result. The operations or steps are those utilizing physical manipulations of physical quantities. Usually, although not necessarily, these quantities take the form of electrical, optical or magnetic signals capable of being stored, transferred, combined, compared, and otherwise manipulated in a computer system or computing device or a processor. These signals are sometimes referred to as transactions, bits, values, elements, symbols, characters, samples, pixels, or the like.</p><p id="p-0020" num="0019">It should be borne in mind, however, that all of these and similar terms are to be associated with the appropriate physical quantities and are merely convenient labels applied to these quantities. Unless specifically stated otherwise as apparent from the following discussions, it is appreciated that throughout the present disclosure, discussions utilizing terms such as &#x201c;storing,&#x201d; &#x201c;determining,&#x201d; &#x201c;sending,&#x201d; &#x201c;receiving,&#x201d; &#x201c;generating,&#x201d; &#x201c;creating,&#x201d; &#x201c;fetching,&#x201d; &#x201c;transmitting,&#x201d; &#x201c;facilitating,&#x201d; &#x201c;providing,&#x201d; &#x201c;forming,&#x201d; &#x201c;detecting,&#x201d; &#x201c;processing,&#x201d; &#x201c;updating,&#x201d; &#x201c;instantiating,&#x201d; &#x201c;identifying&#x201d;, &#x201c;contacting&#x201d;, &#x201c;gathering&#x201d;, &#x201c;accessing&#x201d;, &#x201c;utilizing&#x201d;, &#x201c;resolving&#x201d;, &#x201c;applying&#x201d;, &#x201c;displaying&#x201d;, &#x201c;requesting&#x201d;, &#x201c;monitoring&#x201d;, &#x201c;changing&#x201d;, &#x201c;updating&#x201d;, &#x201c;establishing&#x201d;, &#x201c;initiating&#x201d;, or the like, refer to actions and processes of a computer system or similar electronic computing device or processor. The computer system or similar electronic computing device manipulates and transforms data represented as physical (electronic) quantities within the computer system memories, registers or other such information storage, transmission or display devices.</p><p id="p-0021" num="0020">A &#x201c;computer&#x201d; is one or more physical computers, virtual computers, and/or computing devices. As an example, a computer can be one or more server computers, cloud-based computers, cloud-based cluster of computers, virtual machine instances or virtual machine computing elements such as virtual processors, storage and memory, data centers, storage devices, desktop computers, laptop computers, mobile devices, Internet of Things (IoT) devices such as home appliances, physical devices, vehicles, and industrial equipment, computer network devices such as gateways, modems, routers, access points, switches, hubs, firewalls, and/or any other special-purpose computing devices. Any reference to &#x201c;a computer&#x201d; herein means one or more computers, unless expressly stated otherwise.</p><p id="p-0022" num="0021">The &#x201c;instructions&#x201d; are executable instructions and comprise one or more executable files or programs that have been compiled or otherwise built based upon source code prepared in JAVA, C++, OBJECTIVE-C or any other suitable programming environment.</p><p id="p-0023" num="0022">Communication media can embody computer-executable instructions, data structures, program modules, or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term &#x201c;modulated data signal&#x201d; means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example, and not limitation, communication media can include wired media such as a wired network or direct-wired connection, and wireless media such as acoustic, radio frequency (RF), infrared and other wireless media. Combinations of any of the above can also be included within the scope of computer-readable storage media.</p><p id="p-0024" num="0023">Computer storage media can include volatile and nonvolatile, removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules, or other data. Computer storage media can include, but is not limited to, random access memory (RAM), read only memory (ROM), electrically erasable programmable ROM (EEPROM), flash memory, or other memory technology, compact disk ROM (CD-ROM), digital versatile disks (DVDs) or other optical storage, solid state drives, hard drives, hybrid drive, or any other medium that can be used to store the desired information and that can be accessed to retrieve that information.</p><p id="p-0025" num="0024">It is appreciated that present systems and methods can be implemented in a variety of architectures and configurations. For example, present systems and methods can be implemented as part of a distributed computing environment, a cloud computing environment, a client server environment, hard drive, etc. Example embodiments described herein may be discussed in the general context of computer-executable instructions residing on some form of computer-readable storage medium, such as program modules, executed by one or more computers, computing devices, or other devices. By way of example, and not limitation, computer-readable storage media may comprise computer storage media and communication media. Generally, program modules include routines, programs, objects, components, data structures, etc., that perform particular tasks or implement particular data types. The functionality of the program modules may be combined or distributed as desired in various embodiments.</p><p id="p-0026" num="0025">It should be understood, that terms &#x201c;user&#x201d; and &#x201c;participant&#x201d; have equal meaning in the following description.</p><p id="p-0027" num="0026">Embodiments are described in sections according to the following outline:</p><p id="p-0028" num="0027">1.0 GENERAL OVERVIEW</p><p id="p-0029" num="0028">2.0 STRUCTURAL OVERVIEW</p><p id="p-0030" num="0029">3.0 FUNCTIONAL OVERVIEW<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0030">3.1 Preparation</li>        <li id="ul0002-0002" num="0031">3.2 Iterative Hierarchical Clustering Analysis</li>        <li id="ul0002-0003" num="0032">3.3 Re-segmentation and Display</li>    </ul>    </li></ul></p><p id="p-0031" num="0033">4.0 PROCEDURAL OVERVIEW</p><p id="p-0032" num="0034">1.0 General Overview</p><p id="p-0033" num="0035">Virtual conferencing sessions that feature multiple speakers from a single location present technical challenges for conferencing systems to correctly separate out and identify each individual speaker. In some instances, these virtual meetings have an unknown number of speakers joining from a single source (e.g. a single conference room), variability in speaker environments that create variability in audio quality, overlapping speech, unbalanced talk-times of individual speakers, and variability in gender. All these factors often cause an under-prediction or over-prediction of the number of speakers, the merging of non-homogenous speaker segments, and incorrect speaker turns when using existing methods for separating speakers. This low accuracy and reliability of speaker separation techniques creates a technical challenge that the presently describe approaches seek to address.</p><p id="p-0034" num="0036">The current disclosure provides an artificial intelligence (AI)-based technological solution to the technological problem of separating multiple speakers. Specifically, the technological solution involves using a series of machine learning (ML) algorithms or models to accurately distinguish between and separate any number of speakers. Consequently, these solutions provide the technological benefit of increasing the accuracy and reliability of speaker separation in virtual conferencing systems. Since the conferencing system improved by this method is capable of accurately identifying and displaying speakers, as well as presenting more accurate transcriptions, the current solutions also provide for generating and displaying information that users otherwise would not have had.</p><p id="p-0035" num="0037">In some embodiments, divisive hierarchical clustering and agglomerative hierarchical clustering are both performed in iterative steps in order to separate out each speaker, as further described herein. Using an agglomerative hierarchical clustering analysis alone often results in speakers incorrectly clubbed together due to their gender, environments (e.g. multiple speakers joining from the same noisy conference room), or some other identified features. Consequently, implementing a divisive hierarchical clustering to divide out audio segments based on features such as environment or gender before implementing agglomerative hierarchical clustering has the benefit of more accurate detection and separation of speakers, even those who speak for only a short duration throughout the course of a long meeting.</p><p id="p-0036" num="0038">A computer-implemented machine learning method for improving a collaboration environment is provided. The method comprises processing audio data to generate prepared audio data. The method further comprises determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file. The method further comprises re-segmenting the audio file to generate a speaker segment and causing to display the speaker segment through a client device.</p><p id="p-0037" num="0039">A non-transitory, computer-readable medium storing a set of instructions is also provided. In an example embodiment, when the instructions are executed by a processor the instructions cause processing audio data to generate prepared audio data, determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file, re-segmenting the audio file to generate a speaker segment, and causing to display the speaker segment through a client device.</p><p id="p-0038" num="0040">A machine learning system for improving speaker separation is also provided. The system includes a processor and a memory storing instructions that, when executed by the processor, cause processing audio data to generate prepared audio data, determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file, re-segmenting the audio file to generate a speaker segment, and causing to display the speaker segment through a client device.</p><p id="p-0039" num="0041">2.0 Structural Overview</p><p id="p-0040" num="0042"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example collaboration system <b>100</b> in which various implementations as described herein may be practiced. The collaboration system <b>100</b> enables a plurality of users to collaborate and communicate through various means, including email, instant message, SMS and MMS message, video, audio, VR, AR, transcriptions, closed captioning, or any other means of communication. In some examples, one or more components of the collaboration system <b>100</b>, such as client device(s) <b>112</b>A, <b>112</b>B and server <b>132</b>, can be used to implement computer programs, applications, methods, processes, or other software to perform the described techniques and to realize the structures described herein. In an embodiment, the collaboration system <b>100</b> comprises components that are implemented at least partially by hardware at one or more computing devices, such as one or more hardware processors executing program instructions stored in one or more memories for performing the functions that are described herein.</p><p id="p-0041" num="0043">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the collaboration system <b>100</b> includes one or more client device(s) <b>112</b>A, <b>112</b>B that are accessible by users <b>110</b>A, <b>110</b>B, a network <b>120</b>, a server system <b>130</b>, a server <b>132</b>, and a database <b>136</b>. The client devices <b>112</b>A, <b>112</b>B are configured to execute one or more client application(s) <b>114</b>A, <b>114</b>B, that are configured to enable communication between the client devices <b>112</b>A, <b>112</b>B and the server <b>132</b>. In some embodiments, the client applications <b>114</b>A, <b>114</b>B are web-based applications that enable connectivity through a browser, such as through Web Real-Time Communications (WebRTC). The server <b>132</b> is configured to execute a server application <b>134</b>, such as a server back-end that facilitates communication and collaboration between the server <b>132</b> and the client devices <b>112</b>A, <b>121</b>B. In some embodiments, the server <b>132</b> is a WebRTC server. The server <b>132</b> may use a Web Socket protocol, in some embodiments. The components and arrangements shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> are not intended to limit the disclosed embodiments, as the system components used to implement the disclosed processes and features can vary.</p><p id="p-0042" num="0044">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, users <b>110</b>A, <b>110</b>B may communicate with the server <b>132</b> and each other using various types of client devices <b>112</b>A, <b>112</b>B via network <b>120</b>. As an example, client devices <b>112</b>A, <b>112</b>B may include a display such as a television, tablet, computer monitor, video conferencing console, or laptop computer screen. Client devices <b>112</b>A, <b>112</b>B may also include video/audio input devices such as a microphone, video camera, web camera, or the like. As another example, client device <b>112</b>A, <b>112</b>B may include mobile devices such as a tablet or a smartphone having display and video/audio capture capabilities. In some embodiments, the client device <b>112</b>A, <b>112</b>B may include AR and/or VR devices such as headsets, glasses, etc. Client devices <b>112</b>A, <b>112</b>B may also include one or more software-based client applications that facilitate the user devices to engage in communications, such as instant messaging, text messages, email, Voice over Internet Protocol (VoIP), video conferences, and so forth with one another. In some embodiments, the client application <b>114</b>A, <b>114</b>B may be a web browser configured to enabled browser-based WebRTC conferencing sessions. In some embodiments, the systems and methods further described herein are implemented to separate speakers for WebRTC conferencing sessions and provide the separated speaker information to a client device <b>112</b>A, <b>112</b>B.</p><p id="p-0043" num="0045">The network <b>120</b> facilitates the exchanges of communication and collaboration data between client device(s) <b>112</b>A, <b>112</b>B and the server <b>132</b>. The network <b>120</b> may be any type of networks that provides communications, exchanges information, and/or facilitates the exchange of information between the server <b>132</b> and client device(s) <b>112</b>A, <b>112</b>B. For example, network <b>120</b> broadly represents a one or more local area networks (LANs), wide area networks (WANs), metropolitan area networks (MANs), global interconnected internetworks, such as the public internet, public switched telephone networks (&#x201c;PSTN&#x201d;), or other suitable connection(s) or combination thereof that enables collaboration system <b>100</b> to send and receive information between the components of the collaboration system <b>100</b>. Each such network <b>120</b> uses or executes stored programs that implement internetworking protocols according to standards such as the Open Systems Interconnect (OSI) multi-layer networking model, including but not limited to Transmission Control Protocol (TCP) or User Datagram Protocol (UDP), Internet Protocol (IP), Hypertext Transfer Protocol (HTTP), and so forth. All computers described herein are configured to connect to the network <b>120</b> and the disclosure presumes that all elements of <figref idref="DRAWINGS">FIG. <b>1</b></figref> are communicatively coupled via network <b>120</b>. A network may support a variety of electronic messaging formats and may further support a variety of services and applications for client device(s) <b>112</b>A, <b>112</b>B.</p><p id="p-0044" num="0046">The server system <b>130</b> can be a computer-based system including computer system components, desktop computers, workstations, tablets, hand-held computing devices, memory devices, and/or internal network(s) connecting the components. The server <b>132</b> is configured to provide collaboration services, such as telephony, video conferencing, messaging, email, project management, or any other types of communication between users. The server <b>132</b> is also configured to receive information from client device(s) <b>112</b>A, <b>112</b>B over the network <b>120</b>, process the unstructured information to generate structured information, store the information in a database <b>136</b>, and/or transmit the information to the client devices <b>112</b>A, <b>112</b>B over the network <b>120</b>. For example, the server <b>132</b> may be configured to receive physical inputs, video signals, audio signals, text data, user data, or any other data, analyze the received information, separate out and identify multiple speakers, and/or send identifying information or any other information pertaining to the separate speakers to the client devices <b>112</b>A, <b>112</b>B. In some embodiments, the server <b>132</b> is configured to generate a transcript, closed-captioning, speaker identification, and/or any other content featuring the separated speakers.</p><p id="p-0045" num="0047">In some implementations, the functionality of the server <b>132</b> described in the present disclosure is distributed among one or more of the client devices <b>112</b>A, <b>112</b>B. For example, one or more of the client devices <b>112</b>A, <b>112</b>B may perform functions such as processing audio data for speaker separation. In some embodiments, the client devices <b>112</b>A, <b>112</b>B may share certain tasks with the server <b>132</b>.</p><p id="p-0046" num="0048">Database(s) <b>136</b> include one or more physical or virtual, structured or unstructured storages coupled with the server <b>132</b>. The database <b>136</b> is configured to store a variety of data. For example, the database <b>136</b> stores communications data, such as audio, video, text, or any other form of communication data. The database <b>136</b> is also stores security data, such as access lists, permissions, and so forth. The database <b>136</b> also stores internal user data, such as names, positions, organizational charts, etc., as well as external user data, such as data from as Customer Relation Management (CRM) software, Enterprise Resource Planning (ERP) software, project management software, source code management software, or any other external or third-party sources. In some embodiments, the database <b>136</b> is also configured to store processed audio data, ML training data, or any other data. In some embodiments, the database <b>136</b> is stored in a cloud-based server (not shown) that is accessible by the server <b>132</b> and/or the client devices <b>112</b>A, <b>112</b>B through the network <b>120</b>. While the database <b>136</b> is illustrated as an external device connected to the server <b>132</b>, the database <b>136</b> may also reside within the server <b>132</b> as an internal component of the server <b>132</b>.</p><p id="p-0047" num="0049">3.0 Functional Overview</p><p id="p-0048" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of a server system <b>200</b>, such as server system <b>130</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in an example embodiment. A server application <b>134</b> contains sets of instructions or modules which, when executed by one or more processors, perform various functions related to separating multiple speakers. In the example of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the server system <b>200</b> is configured with a preprocessing module <b>202</b>, an activity detection module <b>204</b>, an overlap detection module <b>206</b>, a feature extraction module <b>208</b>, a speaker clustering module <b>210</b>, a re-segmentation module <b>212</b>, and a display module <b>214</b>, as further described herein. While seven modules are depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref> serves as an example and is not intended to be limiting. For example, fewer modules or more modules serving any number of purposes may be used.</p><p id="p-0049" num="0051">One or more modules use ML algorithms or models. In some embodiments, all the above modules comprise of one or more ML models or implement ML techniques. For instances, any of the modules of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be one or more: Voice Activity Detection (VAD) models, Gaussian Mixture Models (GMM), Deep Neural Networks (DNN), Time Delay Neural Networks (TDNN), Long Short-Term Memory (LSTM) networks, Agglomerative Hierarchical Clustering (AHC), Divisive Hierarchical Clustering (DHC), Hidden Markov Models (HMM), Natural Language Processing (NLP), Convolution Neural Networks (CNN), General Language Understanding Evaluation (GLUE), Word2Vec, Gated Recurrent Unit (GRU) networks, Hierarchical Attention Networks (HAN), or any other type of machine learning model. The models listed herein serve as examples and are not intended to be limiting.</p><p id="p-0050" num="0052">In an embodiment, each of the machine learning models are trained on one or more types of data in order to separate multiple speakers. Using the neural network <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> as an example, a neural network <b>300</b> includes an input layer <b>310</b>, one or more hidden layers <b>320</b>, and an output layer <b>330</b> to train the model to perform various functions in relation to separating multiple speakers. In some embodiments, where the training data is labeled, supervised learning is used such that known input data, a weighted matrix, and known output data is used to gradually adjust the model to accurately compute the already known output. In other embodiments, where the training data is not labeled, unsupervised and/or semi-supervised learning is used such that a model attempts to reconstruct known input data over time in order to learn.</p><p id="p-0051" num="0053">Training of example neural network <b>300</b> using one or more training input matrices, a weight matrix, and one or more known outputs is initiated by one or more computers associated with the ML modules. For example, one, some, or all of the modules of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be trained by one or more training computers, and once trained, used in association with the server <b>132</b> and/or client devices <b>112</b>A, <b>112</b>B, to process live audio data for separating individual speakers. In an embodiment, a computing device may run known input data through a deep neural network in an attempt to compute a particular known output. For example, a server, such as server <b>132</b>, uses a first training input matrix and a default weight matrix to compute an output. If the output of the deep neural network does not match the corresponding known output of the first training input matrix, the server <b>132</b> adjusts the weight matrix, such as by using stochastic gradient descent, to slowly adjust the weight matrix over time. The server <b>132</b> then re-computes another output from the deep neural network with the input training matrix and the adjusted weight matrix. This process continues until the computer output matches the corresponding known output. The server <b>132</b> then repeats this process for each training input dataset until a fully trained model is generated.</p><p id="p-0052" num="0054">In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the input layer <b>310</b> includes a plurality of training datasets that are stored as a plurality of training input matrices in an associated database, such as database <b>136</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In some embodiments, the training datasets may be updated and the ML models retrained using the updated data. In some embodiments, the updated training data may include, for example, user feedback or other user input.</p><p id="p-0053" num="0055">The training input data includes, for example, audio data <b>302</b>, <b>304</b>, <b>306</b>. The audio data <b>302</b>, <b>304</b>, <b>306</b> includes a variety of audio data from different sources. The audio data <b>302</b>, <b>304</b>, <b>306</b> may feature silence, sounds, non-spoken sounds, background noises, white noise, spoken sounds, speakers of different genders with different speech patterns, or any other types of audio. While the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref> uses a single neural network, any number of neural networks may be used to train any number of ML models to separate speakers.</p><p id="p-0054" num="0056">In the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, hidden layers <b>320</b> represent various computational nodes <b>321</b>, <b>322</b>, <b>323</b>, <b>324</b>, <b>325</b>, <b>326</b>, <b>327</b>, <b>328</b>. The lines between each node <b>321</b>, <b>322</b>, <b>323</b>, <b>324</b>, <b>325</b>, <b>326</b>, <b>327</b>, <b>328</b> represent weighted relationships based on the weight matrix. As discussed above, the weight of each line is adjusted overtime as the model is trained. While the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b></figref> features two hidden layers <b>320</b>, the number of hidden layers is not intended to be limiting. For example, one hidden layer, three hidden layers, ten hidden layers, or any other number of hidden layers may be used for a standard or deep neural network. The example of <figref idref="DRAWINGS">FIG. <b>3</b></figref> also features an output layer <b>330</b> with speaker(s) <b>332</b> as the output. The speaker(s) <b>332</b> indicate one or more individual speakers that have been separated from the audio data <b>302</b>, <b>304</b>, <b>306</b>. As discussed above, in this structured model, the speakers <b>332</b> are used as a target output for continuously adjusting the weighted relationships of the model. When the model successfully outputs the speakers <b>332</b>, then the model has been trained and may be used to process live or field data.</p><p id="p-0055" num="0057">Once the neural network <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is trained, the trained model will accept field data at the input layer <b>310</b>, such as audio data from current conferencing sessions. In some embodiments, the field data is live data that is accumulated in real time, such as a live audio-video conferencing session. In other embodiments, the field data may be current data that has been saved in an associated database, such as database <b>136</b>. The trained model is applied to the field data in order to identify the one or more speakers <b>332</b> at the output layer <b>330</b>. For instance, a trained model can separate out individual speakers based on environment, gender, or any other factor.</p><p id="p-0056" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of a speaker separation process <b>400</b>, in an example embodiment. The speaker separation process <b>400</b> may be understood in relation to the data preparation, the iterative hierarchical clustering analysis, and the re-segmentation steps, as further described herein.</p><p id="p-0057" num="0059">3.1 Preparation</p><p id="p-0058" num="0060">In some embodiments, audio data <b>402</b> is fed into a preprocessing module <b>202</b>. The preprocessing module <b>202</b> normalizes the audio data <b>402</b> for subsequent processing by other modules. In some embodiments, normalizing the audio data <b>402</b> includes normalizing the data to standards in which the ML models are trained, thereby ensuring parity between the data used to train the ML models and the live audio data <b>402</b> fed into the server system <b>130</b>. Some nonlimiting examples of preprocessing include remixing audio channels, such as remixing multiple channels into a single channel, down-sampling the audio to a predetermined sampling rate, adding or removing white noise, performing root mean square (RMS) normalization, or performing any other methods for normalization and standardization. Subsequently, the pre-processed audio data it sent to the activity detection module <b>204</b> for processing.</p><p id="p-0059" num="0061">In an embodiment, the activity detection module <b>204</b> is a voice filter that detects and eliminates non-speech segments within the audio data. Non-speech audio segments include, for example, background noise, silence, or any other audio segments that do not include speech. In some embodiments, the activity detection module <b>204</b> extracts features from the pre-processed audio data from the preprocessing module <b>202</b>. The features may be Mel-frequency cepstral coefficient (MFCC) features, which are then passed as input into one or more VAD models, for example. In some embodiments, a GMM model is trained to detect speech, silence, and/or background noise. In other embodiments, a DNN model is trained to enhance speech segments of the audio and/or detect the presence of a noise. In some embodiments, one or both GMM and DNN models are used while in other embodiments, other known ML learning techniques are used.</p><p id="p-0060" num="0062">In some embodiments, the MFCC features extracted by the activity detection module <b>204</b> are passed to an overlap detection module <b>206</b>. In an embodiment, the overlap detection module <b>206</b> is a filter that separates overlapping speech segments. Overlapping speech segments include, for example, more than one speaker in a single frame. In some embodiments, the overlap detection module <b>206</b> is a ML model, such as a TDNN model, that is trained to classify whether a frame has a single speaker or more than one speaker. In some embodiments, the model is also trained to distinguish between overlapping speakers and a new speaker such that overlapping speakers are not erroneously identified as new speakers. In some embodiments, the overlap detection module <b>206</b> is a LSTM model that uses raw audio data as input rather than MFCC features. The LSTM model performs sequence to sequence labeling in order to separate overlapping speakers. Following the overlap detection module <b>206</b>, the audio data proceeds into an iterative block <b>404</b> for hierarchical clustering, which is described further herein.</p><p id="p-0061" num="0063">3.2 Iterative Hierarchical Clustering Analysis</p><p id="p-0062" num="0064">The iterative block <b>404</b> implements a hierarchical clustering analysis. In some embodiments, the iterative block <b>404</b> repeats a series of outer and inner processing loops on the audio data to separate out multiple speakers. In an embodiment, each iteration includes both the outer loop and the inner loop. In some embodiments, the outer loop is a top-down divisive hierarchical clustering (DHC) analysis performed by a trained ML feature extraction module <b>208</b>. In some embodiments, the inner loop is a bottom-up agglomerative hierarchical clustering (AHC) analysis performed by a trained ML speaker clustering module <b>210</b>. The hierarchical clustering techniques are examples only and are not intended to be limiting.</p><p id="p-0063" num="0065">In some instances, during the DHC outer loop, the feature extraction module <b>208</b> takes the audio data as the input and divides the audio data based on patterns or features that the feature extraction module <b>208</b> is trained to detect. In some instances, during the AHC inner loop, the speaker clustering module <b>210</b> identifies the speakers and splits the audio data into audio files <b>406</b> based on those relevant patterns or features determined by the feature extraction module <b>208</b>. In some embodiments, the audio files <b>406</b> are homogenous audio files. The outer loop and inner loop are repeated through the iterative block <b>404</b> to separate and break down the audio files until each individual speaker is identified and separated. In some embodiments, three iterations of the outer and inner loops are implemented to obtain the output of separated speakers. In other embodiments, two iterations, four iterations, five iterations, or any other number of iterations may be implemented. In instances where three iterations are used, the first iteration may first separate out speakers by their environments, the second iteration may take each of those environments and further separate out speakers by genders, and the third iteration may take each of those genders and further separate out each individual speaker. The features or patterns that each speaker is separated by are examples only and are not intended to be limiting; as such, each iteration may separate speakers based on any type of feature and is not limited to environment, gender, etc. This speaker separation process <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be understood in relation to the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0064" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of speaker separation <b>500</b>, in an example embodiment. During the first iteration <b>502</b>, the prepared audio data <b>510</b> that was processed using the preprocessing module <b>202</b>, activity detection module <b>204</b>, and/or overlap detection module <b>206</b> are fed into the feature extraction module <b>208</b> and speaker clustering module <b>210</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> for the first round of DHC and AHC analysis. In some embodiments, this prepared audio data <b>510</b> is a single audio file featuring all speakers. In some embodiments, the prepared audio data <b>510</b> is a near-real-time or real-time audio feed. In some embodiments, the prepared audio data <b>510</b> is a root node that features speaker timestamps that the feature extraction module <b>208</b> uses to compare timestamps from subsequently separated audio data to. The feature extraction module <b>208</b> listens for background signals in order to separate the audio based on the environment. This may include, for example, the background sounds of people talking, dogs barking, music playing, white noise, or any other background signals that may distinguish one speaker's environment from another speaker's environment. The feature extraction module <b>208</b> may also compare the audio data timestamps corresponding to each background signal to the timestamps from the prepared audio data <b>510</b> prior to passing the sending the audio data to the speaker clustering module <b>210</b>. The speaker clustering module <b>210</b> then breaks the audio down into individual audio files <b>406</b> in accordance with the separated environments. In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the separated audio files <b>406</b> belong to four different identified environments: conference room <b>512</b>, laptop <b>514</b>, phone <b>516</b>, and conference room <b>518</b>. This means that speakers can be found within each of these four different environments. While four environments are shown in the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, any number of environments may be identified and separated.</p><p id="p-0065" num="0067">During the second iteration <b>504</b>, each audio file <b>406</b> associated with the conference room <b>512</b>, laptop <b>514</b>, phone <b>516</b>, and conference room <b>518</b>, respectively, are fed into the feature extraction module <b>208</b> and speaker clustering module <b>210</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> for the second round of DHC and AHC analysis. The feature extraction module <b>208</b> extracts features for gender detection from each of these audio files associated with separate environments. The feature extraction module <b>208</b> may also compare the audio data timestamps corresponding to the gender features to the timestamps from the prepared audio data <b>510</b> prior to passing the sending the audio data to the speaker clustering module <b>210</b>. The speaker clustering module <b>210</b> further breaks the audio down into individual audio files <b>406</b> in accordance with the genders detected within each of the separate environments. In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the separated audio files <b>406</b> of the second iteration correspond to six speaker groups that have been separated by gender within each separate environment: male(s) <b>520</b>, female(s) <b>522</b>, female(s) <b>524</b>, male(s) <b>526</b>, male(s) <b>528</b>, and female(s) <b>530</b>. This means that one or more speakers of the identified gender are located within their respective environments. While six gender groups are shown in the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, any number of gender groups may be identified and separated.</p><p id="p-0066" num="0068">During the third iteration <b>506</b>, each audio file <b>406</b> associated with gender groups identified during the second iteration <b>504</b> are again fed into the feature extraction module <b>208</b> and speaker clustering module <b>210</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> for the third round of DHC and AHC analysis. The feature extraction module <b>208</b> extracts DNN speaker embeddings in accordance with known ML techniques and is trained to differentiate one speaker from another. The feature extraction module <b>208</b> may also compare the audio data timestamps corresponding to individual speakers to the timestamps from the prepared audio data <b>510</b> prior to passing the sending the audio data to the speaker clustering module <b>210</b>. The speaker clustering module <b>210</b> further breaks the audio down into audio files <b>406</b> associated with each individual speaker from the gender groups and the separate environments. In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the final output <b>508</b> includes ten separated audio files <b>406</b> corresponding to ten individual speakers: speaker <b>532</b>, <b>534</b>, <b>536</b>, <b>538</b>, <b>540</b>, <b>542</b>, <b>544</b>, <b>546</b>, <b>548</b>, <b>550</b>.</p><p id="p-0067" num="0069">Returning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in some embodiments, the iterative block <b>404</b> stops upon meeting one or more stopping criteria. For the feature extraction module <b>208</b> performing DHC, the stopping criteria may be, for example, a specific number of iterations. While three iterations are used in the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, any number of iterations may be implemented. For example, if the individual speakers can be separated using two iterations, then the stopping criteria for the feature extraction module <b>208</b> may be set at two iterations. For the speaker clustering module <b>210</b> performing AHC, the stopping criteria may be, for example, meeting a score threshold. Since clusters are merged if their distance is less than a certain threshold, the process continues until all clusters are greater than this score threshold. Since the same score threshold is not universally applicable across all scenarios, AHC may be performed using a list of thresholds, where the appropriate threshold is applied for the appropriate scenario. In some embodiments, the most appropriate threshold for any given scenario is chosen based on a variance ratio criterion, which is a metric for determining cluster purity.</p><p id="p-0068" num="0070">3.3 Re-Segmentation and Display</p><p id="p-0069" num="0071">Upon meeting the stopping criteria, the audio files featuring all the separated speakers are fed into the re-segmentation module <b>212</b>, in some embodiments. In an embodiment, the re-segmentation module <b>212</b> refines the speaker separation output from the iterative block <b>404</b> and generates the final speaker diarized segments <b>408</b> as the output. For example, the individual speaker patterns extracted by the speaker clustering modules <b>210</b> are rough and lack refinement with regards to boundaries that separate one speaker from another. To further refine these speaker boundaries, a machine learning model such as Variable Bayes re-segmentation or Viterbi re-segmentation may be used. In some embodiments, the re-segmentation module <b>212</b> also re-maps the timestamps of the speakers from the original audio file to the audio files that were split due to the DHC process. This generates complete diarized speaker segment(s) <b>408</b> featuring a record of which speaker spoke at each specific time during the meeting.</p><p id="p-0070" num="0072">In some embodiments, the re-segmentation module <b>212</b> is configured to generate a transcript featuring the separated speakers, speaker labels or identifications, closed-captioning of live meetings, and/or any other content featuring the separated speakers generated by the re-segmentation module <b>212</b>. In some embodiments, this includes formatting the diarized speaker segments <b>408</b> into these various types of content featuring the separated speakers, storing these segments or the formatted content in database <b>136</b>, and/or sending the segments or formatted content to a display module <b>214</b> for subsequent display, as further described herein.</p><p id="p-0071" num="0073">In an embodiment, the diarized speaker segments <b>408</b> or any other content generated by the diarized speaker segments <b>408</b>, such as transcripts, closed captioning and so forth, are provided to a display module <b>214</b> for displaying to one or more users, such as user <b>110</b>A, <b>110</b>B. In some embodiments, upon receiving a request sent from client device <b>112</b>A, <b>112</b>B, the transcripts or other speaker information is retrieved from database <b>136</b> and sent to the client device <b>112</b>A, <b>112</b>B for subsequent display. In an embodiment, the display module <b>214</b> may display the closed captioning, transcripts, speaker identities, or any other information in real time, such as during a live conferencing session.</p><p id="p-0072" num="0074">4.0 Procedural Overview</p><p id="p-0073" num="0075"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart depicting an audio preparation process <b>600</b> for separating multiple speakers, in an example embodiment. In some embodiments, the server <b>132</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is configured to implement each of the following steps in the audio preparation process <b>600</b>.</p><p id="p-0074" num="0076">At step <b>602</b>, audio data is normalized. In some embodiments, the audio data is normalized to standards that the ML models were trained on. Normalization may include, for example, remixing audio channels, down-sampling, adding or removing white noise, RMS normalization, or any other normalization and standardization methods. In some embodiments, step <b>602</b> is performed by the preprocessing module <b>202</b>, as described herein in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0075" num="0077">In some embodiments, at step <b>604</b>, non-speech audio segments are eliminated following the normalization. In some embodiments, non-speech segments may include background noise, silence, non-human sounds, or any other audio segments that do not include speech. Eliminating non-speech audio segments enables only audio featuring speech to be processed for speaker separation. In some embodiments, step <b>604</b> is performed by the activity detection module <b>204</b>, as described herein in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0076" num="0078">At step <b>606</b>, overlapping speech segments are separated, in some embodiments. In some embodiments, overlapping speech segments occur when more than one speaker is featured in a single frame. Upon detecting more than one speaker in a frame, the speech segments featuring multiple speakers are separated for further processing, in accordance with the steps of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In some embodiments, step <b>606</b> is performed by the overlap detection module <b>206</b>, as described herein in relation to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0077" num="0079"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart <b>700</b> depicting an iterative block <b>404</b> for separating multiple speakers, in an example embodiment. The prepared audio data from audio preparation process <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> is funneled into this iterative block to separate out individual speakers.</p><p id="p-0078" num="0080">At step <b>702</b>, one or more environments are extracted using a first divisive clustering technique. The environment may be, for example, one or more conference rooms that speakers are calling from, a laptop in an indoor environment, a phone in an outdoor environment, a laptop in a coffee shop, a phone in a car, or any other environment that a speaker may be joining the conferencing session from. The divisive clustering technique is a divisive hierarchical clustering analysis which is done using a trained ML model. The trained ML model is trained to divide out these separate environments for subsequent analysis.</p><p id="p-0079" num="0081">At step <b>704</b>, an environment audio file is generated for each of the one or more environments using a first agglomerative clustering technique. The environment audio file may be audio data featuring speakers located in each of the extracted environments. The agglomerative clustering technique is an agglomerative hierarchical clustering analysis which is done using a trained ML model. In some embodiments, the combination of steps <b>702</b> and <b>704</b> may be done in a first iteration of the iterative block <b>404</b>, in an example embodiment. This first iteration separates out speakers into separate audio files based on their environments before the data proceeds through one or more additional iterations for further separation. In some embodiments, the agglomerative clustering is seeded with the output from the divisive clustering to ensure the best possible clustering for the separated file(s).</p><p id="p-0080" num="0082">At step <b>706</b>, one or more genders are extracted from the environment audio file using a second divisive clustering technique. The genders may be, for example, separated into male or female groups. In some embodiments, where multiple speakers have joined from the same environment such as a large conference room, multiple male speakers may be extracted and grouped together as males while multiple female speakers may be extracted and grouped together as females. The divisive clustering technique is a divisive hierarchical clustering analysis which is done using a trained ML model. The trained ML model is trained to divide out these separate gender groups for subsequent analysis. Since this divisive clustering technique is performed after the first iteration that determines different environments, the gender division is dependent upon each environment.</p><p id="p-0081" num="0083">At step <b>708</b>, a gendered audio file for each of the one or more genders is generated using a second agglomerative clustering technique. The gendered audio file may be audio data featuring speakers that are grouped together by gender. The agglomerative clustering technique is an agglomerative hierarchical clustering analysis which is done using a trained ML model. In some embodiments, the combination of steps <b>706</b> and <b>708</b> may be done in a second iteration of the iterative block, in an example embodiment. This second iteration separates out speakers into separate audio files based on their genders before the data proceeds through one or more additional iterations for further separation. In some embodiments, the agglomerative clustering is seeded with the output from the divisive clustering to ensure the best possible clustering for the separated file(s).</p><p id="p-0082" num="0084">At step <b>710</b>, one or more speaker embeddings are extracted from the gendered audio file using a third divisive clustering technique. A speaker embedding may be, for example, a space for capturing similar inputs in an effort to simplify the machine learning process for larger inputs. The divisive clustering technique is a divisive hierarchical clustering analysis which is done using a trained ML model. The trained ML model is trained to divide out these each separate speaker. Since this divisive clustering technique is performed after the second iteration that determines different genders and after the first iteration that determines different environments, this speaker division is dependent upon both the genders and environments determined in the previous steps of the iterative block <b>700</b>.</p><p id="p-0083" num="0085">At step <b>712</b>, a speaker audio file is generated from each of the one or more speaker embeddings using a third agglomerative clustering technique. A speaker audio file may be audio data pertaining to each individual speaker that has been separated stepwise through each of the previous steps in the iterative block <b>700</b>. The agglomerative clustering technique is an agglomerative hierarchical clustering analysis which is done using a trained ML model. In some embodiments, the combination of steps <b>710</b> and <b>712</b> may be done in a third iteration of the iterative block, in an example embodiment. This third iteration separates out each individual speaker into separate audio files before the data is re-segmented, as further described in relation to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0084" num="0086"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart <b>800</b> depicting a re-segmentation and display of separated speakers, in an example embodiment. At step <b>802</b>, the audio files corresponding to each of the separate speakers determined through the process <b>700</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref> are re-segmented to generate a speaker segment. Re-segmentation may include, for example, refining boundaries that separate one speaker from another using known ML techniques. Re-segmentation may also include re-mapping timestamps from the original audio file to these individual speaker files to generate speaker segment(s). The speaker segment may be, for example, a diarized segment that includes timestamps of when each speaker spoke. In some embodiments, a transcript, closed-captioning, speaker identifiers, or other speaker-related information may be generated using the speaker segments. At step <b>804</b>, this content may subsequently be displayed through client devices <b>112</b>A, <b>112</b>B.</p><p id="p-0085" num="0087"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart <b>900</b> depicting a speaker separation process, in an example embodiment. At step <b>902</b>, audio data is processed to generate prepared audio data. In an embodiment, the audio data may be audio data from a live conferencing session featuring one or more speakers. The processing may include, for example, generating normalized audio data, eliminating non-speech audio segments, and/or separating overlapping speech segments.</p><p id="p-0086" num="0088">At step <b>904</b>, feature data and speaker data are determined from the prepared audio data. In some embodiments, the feature data and speaker data are determined through one or more clustering iterations. In some embodiments, a first clustering iteration identifies different environments from the audio data and separates out speakers based on the different environments. In some embodiments, a second clustering iteration identifies different genders from within the different environments and separates out speakers based on the different genders within the different environments. In some embodiments, a third clustering iteration identifies speaker embeddings from those genders and separates out the individual speakers. In some embodiments, the output from these one or more iterations of machine learning hierarchical clustering is one or more audio files featuring separated speakers.</p><p id="p-0087" num="0089">At step <b>906</b>, the one or more audio files from step <b>904</b> are re-segmented to generate a speaker segment. The re-segmentation may include, for example, refining boundaries that separate one speaker from another, re-mapping timestamps from the original audio file to these individual speaker files to generate diarized speaker segment(s), and/or generating transcripts, closed-captioning, speaker identifiers, or other speaker-related information using the diarized speaker segments.</p><p id="p-0088" num="0090">At step <b>908</b>, the server <b>132</b> may cause the speaker segment to be displayed through a client device. The client device may be the client device <b>112</b>A, <b>112</b>B of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for example. In some embodiments, a user <b>110</b>A, <b>110</b>B may initiate a transcription, closed-captioning, speaker identification, or any other request using a user interface. The server <b>132</b> may receive the request and activate the speaker separation process <b>900</b> to properly identify each individual speaker during a live conferencing session and subsequently provide transcription, closed-captioning, or any other information requested by the user <b>110</b>A, <b>110</b>B.</p><p id="p-0089" num="0091"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a diagram of an example conference server <b>132</b>, consistent with the disclosed embodiments. The server <b>132</b> includes a bus <b>1002</b> (or other communication mechanism) which interconnects subsystems and components for transferring information within the server <b>132</b>. As shown, the server <b>132</b> may include one or more processors <b>1010</b>, input/output (&#x201c;I/O&#x201d;) devices <b>1050</b>, network interface <b>1060</b> (e.g., a modem, Ethernet card, or any other interface configured to exchange data with a network), and one or more memories <b>1020</b> storing programs <b>1030</b> including, for example, server app(s) <b>1032</b>, operating system <b>1034</b>, and data <b>1040</b>, and can communicate with an external database <b>136</b> (which, for some embodiments, may be included within the server <b>132</b>). The server <b>132</b> may be a single server or may be configured as a distributed computer system including multiple servers, server farms, clouds, or computers that interoperate to perform one or more of the processes and functionalities associated with the disclosed embodiments.</p><p id="p-0090" num="0092">The processor <b>1010</b> may be one or more processing devices configured to perform functions of the disclosed methods, such as a microprocessor manufactured by Intel&#x2122; or manufactured by AMD&#x2122;. The processor <b>1010</b> may comprise a single core or multiple core processors executing parallel processes simultaneously. For example, the processor <b>1010</b> may be a single core processor configured with virtual processing technologies. In certain embodiments, the processor <b>1010</b> may use logical processors to simultaneously execute and control multiple processes. The processor <b>1010</b> may implement virtual machine technologies, or other technologies to provide the ability to execute, control, run, manipulate, store, etc. multiple software processes, applications, programs, etc. In some embodiments, the processor <b>1010</b> may include a multiple-core processor arrangement (e.g., dual, quad core, etc.) configured to provide parallel processing functionalities to allow the server <b>132</b> to execute multiple processes simultaneously. It is appreciated that other types of processor arrangements could be implemented that provide for the capabilities disclosed herein.</p><p id="p-0091" num="0093">The memory <b>1020</b> may be a volatile or non-volatile, magnetic, semiconductor, tape, optical, removable, non-removable, or other type of storage device or tangible or non-transitory computer-readable medium that stores one or more program(s) <b>1030</b> such as server apps <b>1032</b> and operating system <b>1034</b>, and data <b>1040</b>. Common forms of non-transitory media include, for example, a flash drive a flexible disk, hard disk, solid state drive, magnetic tape, or any other magnetic data storage medium, a CD-ROM, any other optical data storage medium, any physical medium with patterns of holes, a RAM, a PROM, and EPROM, a FLASH-EPROM or any other flash memory, NVRAM, a cache, a register, any other memory chip or cartridge, and networked versions of the same.</p><p id="p-0092" num="0094">The server <b>132</b> may include one or more storage devices configured to store information used by processor <b>1010</b> (or other components) to perform certain functions related to the disclosed embodiments. For example, the server <b>132</b> includes memory <b>1020</b> that includes instructions to enable the processor <b>1010</b> to execute one or more applications, such as server apps <b>1032</b>, operating system <b>1034</b>, and any other type of application or software known to be available on computer systems. Alternatively or additionally, the instructions, application programs, etc. are stored in an external database <b>136</b> (which can also be internal to the server <b>132</b>) or external storage communicatively coupled with the server <b>132</b> (not shown), such as one or more database or memory accessible over the network <b>120</b>.</p><p id="p-0093" num="0095">The database <b>136</b> or other external storage may be a volatile or non-volatile, magnetic, semiconductor, tape, optical, removable, non-removable, or other type of storage device or tangible or non-transitory computer-readable medium. The memory <b>1020</b> and database <b>136</b> may include one or more memory devices that store data and instructions used to perform one or more features of the disclosed embodiments. The memory <b>1020</b> and database <b>136</b> may also include any combination of one or more databases controlled by memory controller devices (e.g., server(s), etc.) or software, such as document management systems, Microsoft SQL databases, SharePoint databases, Oracle&#x2122; databases, Sybase&#x2122; databases, or other relational databases.</p><p id="p-0094" num="0096">In some embodiments, the server <b>132</b> may be communicatively connected to one or more remote memory devices (e.g., remote databases (not shown)) through network <b>120</b> or a different network. The remote memory devices can be configured to store information that the server <b>132</b> can access and/or manage. By way of example, the remote memory devices could be document management systems, Microsoft SQL database, SharePoint databases, Oracle&#x2122; databases, Sybase&#x2122; databases, or other relational databases. Systems and methods consistent with disclosed embodiments, however, are not limited to separate databases or even to the use of a database.</p><p id="p-0095" num="0097">The programs <b>1030</b> include one or more software modules causing processor <b>1010</b> to perform one or more functions of the disclosed embodiments. Moreover, the processor <b>1010</b> may execute one or more programs located remotely from one or more components of the communications system <b>100</b>. For example, the server <b>132</b> may access one or more remote programs that, when executed, perform functions related to disclosed embodiments.</p><p id="p-0096" num="0098">In the presently described embodiment, server app(s) <b>1032</b> causes the processor <b>1010</b> to perform one or more functions of the disclosed methods. For example, the server app(s) <b>1032</b> may cause the processor <b>1010</b> to analyze different types of audio communications to separate multiple speakers from the audio data and send the separated speakers to one or more users in the form of transcripts, closed-captioning, speaker identifiers, or any other type of speaker information. In some embodiments, other components of the communications system <b>100</b> may be configured to perform one or more functions of the disclosed methods. For example, client devices <b>112</b>A, <b>112</b>B may be configured to separate multiple speakers from the audio data and send the separated speakers to one or more users in the form of transcripts, closed-captioning, speaker identifiers, or any other type of speaker information.</p><p id="p-0097" num="0099">In some embodiments, the program(s) <b>1030</b> may include the operating system <b>1034</b> performing operating system functions when executed by one or more processors such as the processor <b>1010</b>. By way of example, the operating system <b>1034</b> may include Microsoft Windows&#x2122;, Unix&#x2122;, Linux&#x2122;, Apple&#x2122; operating systems, Personal Digital Assistant (PDA) type operating systems, such as Apple iOS, Google Android, Blackberry OS, Microsoft CE&#x2122;, or other types of operating systems. Accordingly, disclosed embodiments may operate and function with computer systems running any type of operating system <b>1034</b>. The server <b>132</b> may also include software that, when executed by a processor, provides communications with network <b>120</b> through the network interface <b>1060</b> and/or a direct connection to one or more client devices <b>112</b>A, <b>112</b>B.</p><p id="p-0098" num="0100">In some embodiments, the data <b>1040</b> includes, for example, audio data, which may include silence, sounds, non-speech sounds, speech sounds, or any other type of audio data.</p><p id="p-0099" num="0101">The server <b>132</b> may also include one or more I/O devices <b>1050</b> having one or more interfaces for receiving signals or input from devices and providing signals or output to one or more devices that allow data to be received and/or transmitted by the server <b>132</b>. For example, the server <b>132</b> may include interface components for interfacing with one or more input devices, such as one or more keyboards, mouse devices, and the like, that enable the server <b>132</b> to receive input from an operator or administrator (not shown).</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented machine learning method for improving speaker separation, the method comprising:<claim-text>processing audio data to generate prepared audio data;</claim-text><claim-text>determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file;</claim-text><claim-text>re-segmenting the audio file to generate a speaker segment; and</claim-text><claim-text>causing to display the speaker segment through a client device.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented machine learning method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining feature data comprises determining an environment or a gender using machine learning.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented machine learning method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining speaker data comprises extracting speaker data from a determined environment or a determined gender using machine learning.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented machine learning method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the audio data comprises generating normalized audio data.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented machine learning method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the audio data comprises eliminating a non-speech audio segment.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented machine learning method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the audio data comprises separating an overlapping speech segment.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented machine learning method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the clustering iteration comprises a divisive clustering machine learning technique and an agglomerative clustering machine learning technique.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory, computer-readable medium storing a set of instructions that, when executed by a processor, cause:<claim-text>processing audio data to generate prepared audio data;</claim-text><claim-text>determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file;</claim-text><claim-text>re-segmenting the audio file to generate a speaker segment; and</claim-text><claim-text>causing to display the speaker segment through a client device.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The non-transitory, computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining feature data comprises determining an environment or a gender using machine learning.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The non-transitory, computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein determining speaker data comprises extracting speaker data from a determined environment or a determined gender using machine learning.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The non-transitory, computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein processing the audio data comprises generating normalized audio data.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory, computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein processing the audio data comprises eliminating a non-speech audio segment.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory, computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein processing the audio data comprises separating an overlapping speech segment.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory, computer-readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the clustering iteration comprises a divisive clustering machine learning technique and an agglomerative clustering machine learning technique.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A machine learning system for improving speaker separation, the system comprising:<claim-text>a processor;</claim-text><claim-text>a memory storing instructions that, when executed by the processor, cause:<claim-text>processing audio data to generate prepared audio data;</claim-text><claim-text>determining feature data and speaker data from the prepared audio data through a clustering iteration to generate an audio file;</claim-text><claim-text>re-segmenting the audio file to generate a speaker segment; and</claim-text><claim-text>causing to display the speaker segment through a client device.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The machine learning system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining feature data comprises determining an environment or a gender using machine learning.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The machine learning system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining speaker data comprises extracting speaker data from a determined environment or a determined gender using machine learning.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The machine learning system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein processing the audio data comprises generating normalized audio data.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The machine learning system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein processing the audio data comprises eliminating a non-speech audio segment.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The machine learning system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein processing the audio data comprises separating an overlapping speech segment.</claim-text></claim></claims></us-patent-application>