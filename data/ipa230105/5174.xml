<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005175A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005175</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941172</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-058401</doc-number><date>20200327</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23229</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">POSITION ANALYSIS DEVICE AND METHOD, AND CAMERA SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/042644</doc-number><date>20201116</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17941172</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Panasonic Intellectual Property Management Co., Ltd.</orgname><address><city>Osaka</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ICHIMURA</last-name><first-name>Daijiroh</first-name><address><city>Hyogo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SHIN</last-name><first-name>Hidehiko</first-name><address><city>Osaka</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A position analysis device includes: an input interface that acquires the captured image obtained by the camera; a processor that calculates coordinate transformation from a first coordinate system to a second coordinate system with respect to the target position in the acquired captured image; and a memory that stores a correction model to generate a correction amount of a position in the second coordinate system, wherein the processor corrects a transformed position from the target position, based on the correction model including weight functions corresponding to reference positions, the transformed position being a position obtained by the coordinate transformation on the target position in the first coordinate system to the second coordinate system, the weight functions each producing a larger weighting in the correction amount as the transformed position is closer to a corresponding reference position, the reference positions being different from each other in the second coordinate system.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="155.62mm" wi="140.63mm" file="US20230005175A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="201.34mm" wi="146.39mm" file="US20230005175A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="183.30mm" wi="145.03mm" file="US20230005175A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="148.08mm" wi="106.09mm" file="US20230005175A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.10mm" wi="121.33mm" file="US20230005175A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.69mm" wi="155.02mm" file="US20230005175A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="220.56mm" wi="131.49mm" file="US20230005175A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="182.63mm" wi="103.12mm" file="US20230005175A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="222.84mm" wi="126.75mm" file="US20230005175A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">1. Technical Field</heading><p id="p-0002" num="0001">The present disclosure relates to a position analysis device and method, and a camera system.</p><heading id="h-0003" level="1">2. Related Art</heading><p id="p-0003" num="0002">JP 2003-78811 A discloses a method of associating marker coordinates in a calibration pattern with respect to acquisition of camera parameters. In this method, a quadrangular calibration pattern having a plurality of marker patterns is photographed, a calibration pattern region is extracted from the photographed image, and image coordinates of four corners of the calibration pattern region are obtained. Further, in this method, the projective transformation matrix is obtained from the image coordinates of the corners of the calibration pattern region and coordinates of corners in a planar image of an original calibration pattern. The camera parameter is obtained by obtaining the correspondence between the coordinates of the marker in the original calibration pattern planar image and the coordinates of the marker in the photographed image using the projective transformation matrix.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0004" num="0003">The present disclosure provides a position analysis device and method, and a camera system capable of accurately measuring a position as a target of an analysis in a subject from a captured image by a camera.</p><p id="p-0005" num="0004">A position analysis device in the present disclosure is a device for measuring a target position in a subject, based on a captured image of the subject by a camera. The position analysis device includes an input interface, a processor, and a memory. The input interface acquires the captured image obtained by the camera. The processor calculates coordinate transformation from a first coordinate system to a second coordinate system with respect to the target position in the acquired captured image, the first coordinate system providing measure in the captured image, the second coordinate system providing measure in the subject. The memory stores a correction model to generate a correction amount of a position in the second coordinate system. The processor corrects a transformed position from the target position, based on the correction model, the transformed position being a position obtained by the coordinate transformation on the target position in the first coordinate system to the second coordinate system, the weight functions each producing a larger weighting in the correction amount as the transformed position is closer to a corresponding reference position, the reference positions being different from each other in the second coordinate system.</p><p id="p-0006" num="0005">These general and specific aspects may be realized by a system, a method, and a computer program, and a combination thereof.</p><p id="p-0007" num="0006">According to the position analysis device and method, and the camera system of the present disclosure, it is possible to accurately measure the target position in the subject from the captured image by the camera.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for explaining a camera system according to a first embodiment of the present disclosure;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of a position analysis device according to the first embodiment;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for explaining a basic operation of the position analysis device;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. <b>4</b>A to <b>4</b>C</figref> are diagrams for explaining the basic operation of the position analysis device;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are diagrams for explaining a correction model of the position analysis device;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>C</figref> are diagrams for explaining training data in machine learning of the correction model;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart for explaining a calibration operation of the position analysis device; and</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> are diagrams for explaining a modification of the calibration operation of the position analysis device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0016" num="0015">Embodiments will be described in detail below with reference to the drawings as appropriate. However, more detailed description than necessary may be omitted. Fox example, detailed description of already well-known matters and redundant description of substantially the same configuration may be omitted. This is to avoid the following description from becoming unnecessary redundant and to facilitate understanding by those skilled in the art.</p><p id="p-0017" num="0016">In addition, the applicant(s) provides the accompanying drawings and the following description to enable those skilled in the art to sufficiently understand the present disclosure, which does not intend to limit the claimed subject matter.</p><heading id="h-0007" level="1">First Embodiment</heading><p id="p-0018" num="0017">Hereinafter, a first embodiment of the present disclosure will be described with reference to the drawings.</p><heading id="h-0008" level="2">1. Configuration</heading><p id="p-0019" num="0018">A configuration of a position analysis device and a camera system according to the present embodiment will be described below.</p><heading id="h-0009" level="2">1-1. Camera System</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram for explaining a camera system <b>1</b> according to the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the camera system <b>1</b> of the present embodiment includes an omnidirectional camera <b>10</b> and a position analysis device <b>2</b>, for example. The present system <b>1</b> can be applied to an application of providing various data, which is related to machining work or the like performed by a worker <b>31</b>, to a user <b>11</b> such as a person in charge of data analysis in a workplace <b>3</b> such as a factory, for example.</p><p id="p-0021" num="0020">Hereinafter, the vertical direction in the workplace <b>3</b> is referred to as a Z direction. Two directions perpendicular to each other on a horizontal plane orthogonal to the Z direction are referred to as an X direction and a Y direction, respectively. Further, the +Z direction may be referred to as upward, and the &#x2212;Z direction may be referred to as downward.</p><p id="p-0022" num="0021">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the omnidirectional camera <b>10</b> is arranged on a ceiling or the like of the workplace <b>3</b> so as to overlook the workplace <b>3</b> from above. In this example, in the workplace <b>3</b>, a workpiece <b>32</b> is arranged on a pedestal <b>33</b>. For example, the workpiece <b>32</b> is various members constituting a large product, e.g. a constituent member of a large blower, an automobile, a train, an aircraft, a ship, or the like. The workpiece <b>32</b> is an example of a subject in the present system <b>1</b>.</p><p id="p-0023" num="0022">The example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a state in which the worker <b>31</b> performs surface machining on the workpiece <b>32</b> by using a surface machining tool <b>34</b> that is a tool for local surface machining. The surface machining tool <b>34</b> is provided with a marker <b>34</b><i>a </i>at the end portion, for example. By a position measurement method to the captured image from the omnidirectional camera <b>10</b>, the position analysis device <b>2</b> of the present system <b>1</b> can accurately measure the position of the portion of the workpiece <b>32</b> subjected to the surface machining by the worker <b>31</b>, for example. Hereinafter, the configuration of each device in the present system <b>1</b> will be described.</p><p id="p-0024" num="0023">The omnidirectional camera <b>10</b> is an example of a camera in the present system <b>1</b>. For example, the omnidirectional camera <b>10</b> includes an optical system such as a fisheye lens, and an imaging element such as a CCD or a CMOS image sensor. For example, the omnidirectional camera <b>10</b> performs art imaging operation according to the equidistant projection method, to generate image data indicating a captured image. The omnidirectional camera <b>10</b> is connected to the position analysis device <b>2</b> such that image data is transmitted to the position analysis device <b>2</b>, for example.</p><p id="p-0025" num="0024">In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the omnidirectional camera <b>10</b> is arranged so that the workpiece <b>32</b> is included in a range to be imaged in the workplace <b>3</b>. The omnidirectional camera <b>10</b> is arranged with its optical axis oriented in the Z direction, for example. For example, the half angle of view of the omnidirectional camera <b>10</b> is &#x3b8;&#x2264;90&#xb0; at an angle &#x3b8; with respect to the Z direction. According to the omnidirectional camera <b>10</b>, a relatively wide area can be imaged by one camera.</p><p id="p-0026" num="0025">The position analysis device <b>2</b> includes an information processing device such as a personal computer (PC). The configuration of the position analysis device <b>2</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><heading id="h-0010" level="2">1-2. Configuration of Position Analysis Device</heading><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram illustrating a configuration of the position analysis device <b>2</b>. The position analysis device <b>2</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> includes a processor <b>20</b>, a memory <b>21</b>, a user interface <b>22</b>, a display <b>23</b>, a device interface <b>24</b>, and a network interface <b>25</b>. Hereinafter, the interface may be abbreviated as &#x201c;I/F&#x201d;.</p><p id="p-0028" num="0027">The processor <b>20</b> includes a CPU or an MPU that realizes a predetermined function in cooperation with software, for example. The processor <b>20</b> controls the overall operation of the position analysis device <b>2</b>, for example. The processor <b>20</b> reads data and programs stored in the memory <b>21</b> and performs various arithmetic processing to realize various functions. For example, the processor <b>20</b> includes an image recognition module <b>41</b>, a coordinate transformation module <b>42</b>, a coordinate correction module <b>43</b>, and a model learner <b>44</b> as functional configurations.</p><p id="p-0029" num="0028">The image recognition module <b>41</b> applies various image recognition technologies to the image data to recognize a position of a preset processing target in the image indicated by the image data. The coordinate transformation module <b>42</b> calculates coordinate transformation between predetermined coordinate systems with respect to the position recognized in the image. The coordinate correction module <b>43</b> executes processing of correcting a calculation result of the coordinate transformation module <b>42</b> based on a correction model to be described later. The model learner <b>44</b> executes machine learning of the correction model. The operation of such various functions of the position analysis device <b>2</b> will be described later.</p><p id="p-0030" num="0029">The processor <b>20</b> executes a program including a command group for realizing the function of the position analysis device <b>2</b> as described above, for example. The above program may be provided from a communication network such as the internet, or may be stored in a portable recording medium. Furthermore, the processor <b>20</b> may be a hardware circuit such as a dedicated electronic circuit or a reconfigurable electronic circuit designed to realize each of the above-described functions. The processor <b>20</b> may be configured by various semiconductor integrated circuits such as a CPU, an MPU, a GPU, a GPGPU, a TPU, a microcomputer, a DSP, an FPGA, and an ASIC.</p><p id="p-0031" num="0030">The memory <b>21</b> is a storage medium that stores programs and data necessary for realizing the functions of the position analysis device <b>2</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the memory <b>21</b> includes a storage <b>21</b><i>a </i>and a temporary memory <b>21</b><i>b. </i></p><p id="p-0032" num="0031">The storage <b>21</b><i>a </i>stores parameters, data, control programs, and the like for realizing a predetermined function. The storage <b>21</b><i>a </i>includes an HDD or an SSD, for example. For example, the storage <b>21</b><i>a </i>stores the above-described programs, correction model information D<b>1</b>, training data D<b>2</b>, map information D<b>3</b>, and the like.</p><p id="p-0033" num="0032">The correction model information D<b>1</b> includes various parameters such as a weight parameter indicating a learning result of the correction model. The correction model information D<b>1</b> may include an arithmetic expression of the correction model. The training data D<b>2</b> is data used as a ground truth in the machine learning of the correction model. The map information D<b>3</b> indicates the arrangement of various objects such as the workpiece <b>32</b> and the pedestal <b>33</b> in the workplace <b>3</b> in a predetermined coordinate system, for example. Details of the various information will be described later.</p><p id="p-0034" num="0033">For example, the temporary memory <b>21</b><i>b </i>includes a RAM such as a DRAM or an SRAM, to temporarily store (i.e., hold) data. For example, the temporary memory <b>21</b><i>b </i>holds image data and the like received from the omnidirectional camera <b>10</b>. In addition, the temporary memory <b>21</b><i>b </i>may function as a work area of the processor <b>20</b>, and may be configured as a storage area in an internal memory of the processor <b>20</b>.</p><p id="p-0035" num="0034">The user interface <b>22</b> is a general term for operation members operated by a user. The user interface <b>22</b> may constitute a touch panel together with the display <b>23</b>. The user interface <b>22</b> is not limited to the touch panel, and may be e.g. a keyboard, a touch pad, a button, a switch, or the like. The user interface <b>22</b> is an example of an input interface that acquires various information input by a user operation.</p><p id="p-0036" num="0035">The display <b>23</b> is an example of an output interface including a liquid crystal display or an organic EL display, for example. The display <b>23</b> may display various information such as various icons for operating the user interface <b>22</b> and information input from the user interface <b>22</b>.</p><p id="p-0037" num="0036">The device I/F <b>24</b> is a circuit for connecting an external device such as the omnidirectional camera <b>10</b> to the position analysis device <b>2</b>. The device I/F <b>24</b> performs communication in accordance with a predetermined communication standard. The predetermined standard includes USB, HDMI (registered trademark), IEEE 1395, WiFi, Bluetooth, and the like. The device I/F <b>24</b> is an example of an input interface that receives image data and the like from the omnidirectional camera <b>10</b>. The device I/F <b>24</b> may constitute an output interface that transmits various information to an external device in the position analysis device <b>2</b>.</p><p id="p-0038" num="0037">The network I/F <b>25</b> is a circuit for connecting the position analysis device <b>2</b> to a communication network via a wireless or wired communication line. The network I/F <b>25</b> performs communication conforming to a predetermined communication standard. The predetermined communication standard includes communication standards such as IEEE802.3 and IEEE802.11a/11b/11g/11ac. The network I/F <b>25</b> may constitute an input interface that receives or an output interface that transmits various information via a communication network in the position analysis device <b>2</b>. For example, the network I/F <b>25</b> may be connected to the omnidirectional camera <b>10</b> via a communication network.</p><p id="p-0039" num="0038">The configuration of the position analysis device <b>2</b> as described above is an example, and the configuration of the position analysis device <b>2</b> is not limited thereto. The position analysis device <b>2</b> may be configured by various computers including a server device. The position analysis method of the present embodiment may be executed in distributed computing. In addition, the input interface in the position analysis device <b>2</b> may be realized by cooperation with various software in the processor <b>20</b> and the like. The input interface in the position analysis device <b>2</b> may acquire various information by reading various information stored in various storage media (e.g., the storage <b>21</b><i>a</i>) to a work area (e.g., the temporary memory <b>21</b><i>b</i>) of the processor <b>20</b>.</p><heading id="h-0011" level="2">2. Operation</heading><p id="p-0040" num="0039">Operations of the camera system <b>1</b> and the position analysis device <b>2</b> configured as described above will be described below.</p><heading id="h-0012" level="2">2-1. Basic Operation</heading><p id="p-0041" num="0040">A basic operation of the position analysis device <b>2</b> in the present system <b>1</b> will be described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> to <b>4</b>C</figref>.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart for explaining the basic operation of the position analysis device <b>2</b>. <figref idref="DRAWINGS">FIGS. <b>4</b>A to <b>4</b>C</figref> are diagrams for explaining the basic operation of the position analysis device <b>2</b>. Each processing illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>3</b></figref> is executed by the processor <b>20</b> functioning as the image recognition module <b>41</b>, the coordinate transformation module <b>42</b>, and the coordinate correction module <b>43</b>.</p><p id="p-0043" num="0042">At first, the processor <b>20</b> of the position analysis device <b>2</b> acquires image data from the omnidirectional camera <b>10</b> via the device I/F <b>24</b>, for example (S<b>1</b>). For example, while the worker <b>31</b> is working in the workplace <b>3</b>, the omnidirectional camera <b>10</b> repeats an imaging operation at a predetermined cycle, generates image data indicating a captured image, and transmits the image data to the position analysis device <b>2</b>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates an example of image data <b>50</b> acquired in step S<b>1</b>. The image data <b>50</b> has a two-dimensional coordinate system (H, V) according to arrangement of pixels in a captured image <b>5</b>, for example. Hereinafter, the coordinate system (H, V) of the captured image <b>5</b> by the omnidirectional camera <b>10</b> is referred to as a camera coordinate system. The camera coordinate system includes an H coordinate indicating the position of the captured image <b>5</b> in the horizontal direction and a V coordinate indicating the position of the captured image <b>5</b> in the vertical direction. The camera coordinate system is an example of a first coordinate system in the present embodiment.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates the workpiece <b>32</b> illustrated in the captured image <b>5</b>. For convenience of description, illustration of the worker <b>31</b> and the like in the captured image <b>5</b> is omitted. In the captured image <b>5</b> of the omnidirectional camera <b>10</b>, the scale changes such that the workpiece <b>32</b> is shrunk as the angle &#x3b8; increases from the center position p<b>0</b> at the angle &#x3b8;=0&#xb0;. If there is no lens distortion or the like in the omnidirectional camera <b>10</b>, the change is isotropic from the center position p<b>0</b> by the equidistant projection method and is at equal intervals at the angle &#x3b8;.</p><p id="p-0046" num="0045">Next, the processor <b>20</b>, functioning as the image recognition module <b>41</b> based on the acquired image data <b>50</b>, recognizes the machining position in the captured image <b>5</b> in the camera coordinate system (S<b>2</b>). The machining position is a position at which the machining work by the surface machining tool <b>34</b> is performed on the workpiece <b>32</b>. The machining position is an example of the target position in the present embodiment. <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> illustrates a machining position p in the captured image <b>5</b>.</p><p id="p-0047" num="0046">For example, the processor <b>20</b> performs image recognition processing to recognize the machining position p by detecting the marker <b>34</b><i>a </i>provided on the surface machining tool <b>34</b> in the captured image <b>5</b> (S<b>2</b>). In step S<b>2</b>, as exemplified in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the processor <b>20</b> extracts coordinates (Hp, Vp) of the machining position p in the camera coordinate system from the image data <b>50</b>.</p><p id="p-0048" num="0047">Next, the processor <b>20</b>, functioning as the coordinate transformation module <b>42</b>, calculates the coordinate transformation from the camera coordinate system to the map coordinate system, based on the recognition result in the captured image <b>5</b> (S<b>3</b>). The map coordinate system is a coordinate system for providing measure on a position in the workplace <b>3</b> based on the map information D<b>3</b>. <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates a processing example of step S<b>3</b>.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> illustrates coordinate data <b>51</b> indicating the processing result of step S<b>3</b> based on the image data <b>50</b> of <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. The coordinate data <b>51</b> indicates coordinates of a position in the map coordinate system. For example, the map coordinate system includes an X coordinate for indicating a position in the X direction of the workplace <b>3</b> and a Y coordinate for indicating a position in the Y direction. The map coordinate system is an example of a second coordinate system in the present embodiment.</p><p id="p-0050" num="0049">The coordinate data <b>51</b> illustrated in <figref idref="DRAWINGS">FIG. <b>48</b></figref> indicates coordinates (Xp, Yp) obtained by coordinate-transforming the coordinates (Hp, Vp) of the machining position p in the camera coordinate system into the map coordinate system. In step S<b>3</b>, the processor <b>20</b> calculates the coordinates (Xp, Yp) of the transformed position xp, which is the transformation result in the map coordinate system, from the coordinates (Hp, Vp) of the machining position p in the camera coordinate system, by applying a predetermined arithmetic expression, for example. For example, the predetermined arithmetic expression is a transformation expression including inverse transformation of equidistant projection, and includes e.g. nonlinear transformation using a trigonometric function.</p><p id="p-0051" num="0050">According to the coordinate data <b>51</b> obtained by the coordinate transformation as described above, a situation is conceivable that an accurate position in the map coordinate system cannot be obtained in the vicinity of the end portion of the captured image <b>5</b> having a relatively large angle &#x3b8; due to the influence of lens distortion, for example. Therefore, in the present system <b>1</b>, the above transformation result is corrected.</p><p id="p-0052" num="0051">In the present embodiment, the processor <b>20</b>, functioning as the coordinate correction module <b>43</b>, executes arithmetic processing to correct the coordinate data <b>51</b> of the transformed position xp, based on the correction model F (x, W) (S<b>4</b>). For example, in response to input of the coordinate data <b>51</b> in the map coordinate system, the correction model F (x, W) generates a correction amount &#x394;x of the vector amount. <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> illustrates a processing example of step S<b>3</b>.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> illustrates coordinate data <b>52</b> indicating the processing result of step S<b>4</b> based on the coordinate data <b>51</b> of <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>. <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> illustrates an accurate arrangement <b>32</b><i>a </i>of the workpiece <b>32</b> in the map coordinate system.</p><p id="p-0054" num="0053">The corrected coordinate data <b>52</b> indicates coordinates (Xc, Yc) of a position xc at which the transformed position xp in the map coordinate system is corrected. The correction position xc is expressed by the following expression (1), based on the transformed position xp before correction and the correction amount &#x394;x.</p><p id="p-0055" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>xc=xp=&#x394;x </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x394;<i>x=&#x2212;F</i>(<i>x=xp,W</i>)&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0054">In step S<b>4</b>, the processor <b>20</b> performs the arithmetic processing of the above expression (1) using the learned correction model F (x, W) to generate the corrected coordinate data <b>52</b>. The correction model F (x, W) of the present embodiment is machine-learned so as to acquire weighting of the correction amount &#x394;x that changes according to each distance from a plurality of positions as a calibration reference of the position analysis device <b>2</b> to the transformation position xp, for example.</p><p id="p-0058" num="0055">For example, the processor <b>20</b> stores the corrected coordinate data <b>52</b> (S<b>4</b>) in the storage <b>21</b><i>a</i>, to end the processing shown in this flowchart.</p><p id="p-0059" num="0056">According to the above processing, even if an error occurs in the coordinate transformation (S<b>3</b>) of the machining position p appearing in the captured image <b>5</b> by the omnidirectional camera <b>10</b>, the correction position xc in which the error of the transformation position xp is corrected can be obtained, based on the correction model F (x, W) as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> (S<b>4</b>). In this manner, it is possible to accurately measure the target position, such as the machining position of the workpiece <b>32</b> in the workplace <b>3</b>.</p><heading id="h-0013" level="2">2-2. Correction Model</heading><p id="p-0060" num="0057">Details of the correction model F (x, W) in the position analysis device <b>2</b> of the present embodiment will be described with reference to <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>.</p><p id="p-0061" num="0058">The correction model F (x, W) of the present embodiment is expressed by the following expression (11), based on n reference positions a<b>1</b> to an in the map coordinate system.</p><p id="p-0062" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>F</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>W</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>n</mi>       </msubsup>       <mrow>        <mo>{</mo>        <mrow>         <mrow>          <mo>(</mo>          <mrow>           <msub>            <mi>X</mi>            <mi>i</mi>           </msub>           <mo>-</mo>           <msub>            <mi>a</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#xb7;</mo>         <mrow>          <mi>f</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mrow>            <semantics definitionURL="">             <mo>&#x2758;</mo>             <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>            </semantics>            <mrow>             <mi>x</mi>             <mo>-</mo>             <msub>              <mi>a</mi>              <mi>i</mi>             </msub>            </mrow>            <semantics definitionURL="">             <mo>&#x2758;</mo>             <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>            </semantics>           </mrow>           <mo>,</mo>           <msub>            <mi>w</mi>            <mi>i</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>}</mo>       </mrow>      </mrow>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>n</mi>       </msubsup>       <mrow>        <mi>f</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>          </semantics>          <mrow>           <mi>x</mi>           <mo>-</mo>           <msub>            <mi>a</mi>            <mi>i</mi>           </msub>          </mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>          </semantics>         </mrow>         <mo>,</mo>         <msub>          <mi>w</mi>          <mi>i</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>11</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0063" num="0059">The denominator and numerator on the right side in the above expression (11) take a sum &#x3a3; over numbers i=1 to n for identifying the n reference positions a<b>1</b> to an, respectively. <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> illustrate a case where the number of reference positions a<b>1</b> to an is n=5 (the same applies hereinafter).</p><p id="p-0064" num="0060">In the above expression (11), the vector is indicated in bold (the same applies hereinafter). ai indicates coordinates (Xai, Yai) of an i-th reference position in the map coordinate system as a position vector. Xi indicates, as a position vector, a position corresponding to the i-th reference position ai before correction, that is, coordinates (X<sub>Xi</sub>, Y<sub>Xi</sub>) of the corresponding position in the map coordinate system. In addition, the weight parameter W can be described as an n-dimensional vector including n weight factors w<b>1</b> to wn.</p><p id="p-0065" num="0061"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates a relation between the reference position ai and the corresponding position Xi in the correction model F (x, W). <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> illustrates the accurate arrangement <b>32</b><i>a </i>of the workpiece <b>32</b> in the map coordinate system, and an arrangement <b>32</b><i>b </i>of the workpiece <b>32</b> with coordinate transformation on the captured image of the workplace <b>3</b>.</p><p id="p-0066" num="0062">The first to n-th reference positions a<b>1</b> to an are set in accordance with the accurate arrangement <b>32</b><i>a </i>in the map coordinate system, and are set in association with different positions in the workpiece <b>32</b>, for example. For example, the i-th corresponding position Xi is at a position corresponding to the i-th reference position ai in the arrangement <b>32</b><i>b </i>of the workpiece <b>32</b> with the coordinate transformation by the coordinate transformation module <b>42</b> performed on the whole captured image of the workplace <b>3</b> by the omnidirectional camera <b>10</b>.</p><p id="p-0067" num="0063">In the example of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, each corresponding position Xi is shifted from the corresponding reference position ai by the error of the coordinate transformation by the above-described coordinate transformation module <b>42</b>. Such an error is expressed by a difference vector di indicating a difference between the i-th reference position ai and the corresponding position Xi as in the following expression (12), for example.</p><p id="p-0068" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>di=ai&#x2212;Xi</i>&#x2003;&#x2003;(12)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0069" num="0064">The error such as each difference vector di may be caused by lens distortion in the omnidirectional camera <b>10</b>, for example. The correction model F (x, W) of the present embodiment is constructed by machine learning using the reference position ai as the ground truth of the correction position xc output when x=Xi is input.</p><p id="p-0070" num="0065">For example, the correction model F (x, W) includes n weight functions f<b>1</b> (<i>x</i>), f<b>2</b> (<i>x</i>), . . . , fn (x) constituting weighting of difference vectors dl to dn. The i-th weight function fi (x) is expressed by the following expression (13).</p><p id="p-0071" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>f</mi>       <mi>i</mi>      </msub>      <mo>(</mo>      <mi>x</mi>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <mi>f</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mrow>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>         </semantics>         <mrow>          <mi>x</mi>          <mo>-</mo>          <msub>           <mi>a</mi>           <mi>i</mi>          </msub>         </mrow>         <semantics definitionURL="">          <mo>&#x2758;</mo>          <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>         </semantics>        </mrow>        <mo>,</mo>        <msub>         <mi>w</mi>         <mi>i</mi>        </msub>       </mrow>       <mo>)</mo>      </mrow>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>n</mi>       </msubsup>       <mrow>        <mi>f</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>          </semantics>          <mrow>           <mi>x</mi>           <mo>-</mo>           <msub>            <mi>a</mi>            <mi>i</mi>           </msub>          </mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[RightBracketingBar]"</annotation>          </semantics>         </mrow>         <mo>,</mo>         <msub>          <mi>w</mi>          <mi>i</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>13</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0072" num="0066">In the above expression (13), |x&#x2212;ai| represents a distance from the i-th reference position ai to the position x in the map coordinate system as an absolute value of a vector. The i-th weight function fi (x) includes the i-th weight factor wi. Each of the weight factors w<b>1</b> to wn is set to 0 or more and 1 or less, for example.</p><p id="p-0073" num="0067">The i-th weight function fi (x) gives a secondary weighting that varies in accordance with the distance between the position x and each reference position ai, based on each weight factor wi. The weighting by each weight function fi (x) has a functional form that increases as the distance between the position x and each reference position ai decreases, approaching &#x201c;0&#x201d; as the distance increases.</p><p id="p-0074" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates a result of inputting the position x as a correction target into the correction model F (x, W) of the example of <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. In the example of <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the position x before correction is located relatively closer to three reference positions a<b>1</b> to a<b>3</b> among the five reference positions a<b>1</b> to a<b>5</b>. Thus, the corresponding three weight functions f<b>1</b> (<i>x</i>) to f<b>3</b> (<i>x</i>) give main weighting in the correction amount &#x394;x for the position x. On the other hand, as the remaining two reference positions a<b>4</b> and a<b>5</b> are relatively farther from the position x, the weighting of the corresponding correction functions f<b>4</b> (<i>x</i>) and f<b>5</b> (<i>x</i>) is negligibly small in the correction amount &#x394;x.</p><p id="p-0075" num="0069">In the example of <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, among the main three weight functions f<b>1</b> (<i>x</i>) to f<b>3</b> (<i>x</i>), the second weight function f<b>2</b> (<i>x</i>) has the largest weighting. The weighting distribution in the correction amount &#x394;x also varies in accordance with each weight factor wi, in addition to the distance to each reference position ai. In the present embodiment, machine learning of the correction model F (x, W) is performed so as to optimize such a weight factor wi. According to this, it is possible to realize the correction model F (x, W) that accurately corrects the position x of the coordinate transformation over the whole area in the map coordinate system.</p><p id="p-0076" num="0070">The weight function fi (x) in the correction model F (x, W) as described above can be configured by adopting an exponential functional form such as the following expression (14) on the right side of the expression (13), for example.</p><p id="p-0077" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>f</i>(<i>d,w</i>)=exp(&#x2212;<i>w&#xb7;d</i>)&#x2003;&#x2003;(14)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0078" num="0071">In the above expression (14), as the distance d and the weight factor w, the distance |x&#x2212;ai| to each reference position ai and the corresponding weight factor wi are appropriately used. The weight factor w in this case indicates a decrease rate at which f (d, w) decreases as the distance d increases. Therefore, the change for the weighting of each weight function fi (x) to decrease with the distance |x&#x2212;ai| increasing can be made steeper as each weight factor wi increases.</p><p id="p-0079" num="0072">The functional form of f (d, w) is not particularly limited to the above expression (14), and may be various monotonically decreasing functions in which the value range is 0 or more for a non-negative domain. For example, instead of the above expression (14), a functional form as in the following expression (15) can be adopted.</p><p id="p-0080" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>f</i>(<i>d,w</i>)=<i>w/d</i>&#x2003;&#x2003;(15)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0081" num="0073">in addition, f (d, w) of n weight functions f<b>1</b> (<i>x</i>) to fn (x) is not necessarily a common functional form, and a different functional form may be used according to the reference position ai.</p><heading id="h-0014" level="2">2-3. Calibration Operation</heading><p id="p-0082" num="0074">An operation to calibrate the position analysis device <b>2</b> of the present embodiment by performing machine learning of the correction model F (x, W) as described above will be described with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0083" num="0075"><figref idref="DRAWINGS">FIGS. <b>6</b>A to <b>6</b>C</figref> are diagrams for explaining the training data D<b>2</b> in machine learning of the correction model F (x, W). <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> illustrates training data D<b>2</b><i>a </i>in the map coordinate system. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> illustrates training data D<b>2</b><i>b </i>in the camera coordinate system. <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> illustrates transformed data D<b>2</b><i>c </i>obtained from the training data D<b>2</b><i>b </i>in the camera coordinate system.</p><p id="p-0084" num="0076">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, the training data D<b>2</b><i>a </i>in the map coordinate system includes coordinates (Xa<b>1</b>, Ya<b>1</b>) to (Xan, Yan) of the n reference positions a<b>1</b> to an in the map coordinate system. For example, the training data D<b>2</b><i>a </i>in the map coordinate system can be created by using shape data such as three-dimensional CAD data indicating the three-dimensional shape of the workpiece <b>32</b>.</p><p id="p-0085" num="0077">For example, the training data D<b>2</b><i>a </i>in the map coordinate system is obtained by setting, as the reference positions a<b>1</b> to an n representative points, n representative points in the shape data of the workpiece <b>32</b>, and calculating the coordinates of each representative point in the map coordinate system in consideration of the arrangement of the omnidirectional camera <b>10</b>.</p><p id="p-0086" num="0078"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> illustrates training data D<b>2</b><i>b </i>in the camera coordinate system. For example, the training data D<b>2</b><i>b </i>in the camera coordinate system includes coordinates (Ha<b>1</b>, Va<b>1</b>) to (Han, Van) of corresponding positions P<b>1</b> to Pn in the camera coordinate system corresponding to n reference positions a<b>1</b> to an in a captured image <b>5</b>A which is captured in advance for calibration. For example, the captured image <b>5</b>A for calibration is captured by the omnidirectional camera <b>10</b> in a state where the omnidirectional camera <b>10</b>, the workpiece <b>32</b>, and the like are arranged in the actual workplace <b>3</b>.</p><p id="p-0087" num="0079">For example, the training data D<b>2</b><i>b </i>in the camera coordinate system is obtained by specifying, as a corresponding position Pi in the camera coordinate system, the same place as each reference position ai in the workpiece <b>32</b> in the captured image <b>5</b>A as described above. Means for specifying the corresponding position Pi in the camera coordinate system may be visual observation or image recognition processing.</p><p id="p-0088" num="0080">An example of an operation performed in a state where the training data D<b>2</b><i>a </i>and D<b>2</b><i>b </i>as described above are prepared will be described below. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart for explaining a calibration operation of the position analysis device <b>2</b>. The processing illustrated in this flowchart is executed by the processor <b>20</b> functioning as the model learner <b>44</b>, for example.</p><p id="p-0089" num="0081">In the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the processor <b>20</b> acquires the training data D<b>2</b><i>a </i>in the map coordinate system, which is stored in advance in the storage <b>21</b><i>a</i>, for example (S<b>11</b>). Similarly, the processor <b>20</b> acquires the training data D<b>2</b><i>b </i>in the camera coordinate system from the storage <b>21</b><i>a</i>, for example (S<b>12</b>). Next, the processor <b>20</b> performs coordinate transformation from the camera coordinate system to the map coordinate system on the training data D<b>2</b><i>b </i>in the camera coordinate system (S<b>13</b>). <figref idref="DRAWINGS">FIG. <b>6</b>C</figref> illustrates a processing example of step S<b>13</b>.</p><p id="p-0090" num="0082"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> illustrates the transformed data D<b>2</b><i>c </i>indicating the processing result of step S<b>13</b> based on the training data D<b>2</b><i>b </i>of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>. The transformed data D<b>2</b><i>c </i>includes the coordinates (X<sub>Xi</sub>, Y<sub>Xi</sub>) of each corresponding position Xi in the map coordinate system, to which coordinates (HPi, VPi) of each corresponding position Pi in the camera coordinate system is converted. In step S<b>13</b>, the processor <b>20</b> performs processing similar to step S<b>3</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref> on n corresponding positions P<b>1</b> to Pn in the training data D<b>2</b><i>b </i>in the camera coordinate system, thereby calculating n corresponding positions X<b>1</b> to Xn in the map coordinate system.</p><p id="p-0091" num="0083">Returning to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the processor <b>20</b> executes machine learning of the correction model F (x, W) according to the following expression (20), based on the n reference positions a<b>1</b> to an and the n corresponding positions X<b>1</b> to Xn in the map coordinate system (S<b>14</b>).</p><p id="p-0092" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <munder>      <mrow>       <mi>min</mi>       <mtext> </mtext>      </mrow>      <mrow>       <msub>        <mi>w</mi>        <mn>1</mn>       </msub>       <mo>,</mo>       <mo>&#x2026;</mo>       <mtext>  </mtext>       <mo>,</mo>       <msub>        <mi>w</mi>        <mi>n</mi>       </msub>      </mrow>     </munder>     <mo>&#x2062;</mo>     <mrow>      <munder>       <mover>        <mo>&#x2211;</mo>        <mi>N</mi>       </mover>       <mrow>        <mi>i</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>      </munder>      <msup>       <mrow>        <mo>[</mo>        <mrow>         <msub>          <mi>a</mi>          <mi>i</mi>         </msub>         <mo>-</mo>         <mrow>          <mo>(</mo>          <mrow>           <msub>            <mi>X</mi>            <mi>i</mi>           </msub>           <mo>-</mo>           <mrow>            <mi>F</mi>            <mo>&#x2061;</mo>            <mo>(</mo>            <mrow>             <msub>              <mi>X</mi>              <mi>i</mi>             </msub>             <mo>,</mo>             <mi>W</mi>            </mrow>            <mo>)</mo>           </mrow>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>]</mo>       </mrow>       <mn>2</mn>      </msup>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>20</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0093" num="0084">According to the above expression (20), the weight factors w<b>1</b> to wn of the weight parameter W are adjusted so as to minimize a square error between each reference position ai in the map coordinate system and a position to which the learning correction model F (x=Xi, W) is applied for each corresponding position Xi. The processor <b>20</b> repeats the numerical calculation of the expression (20) as appropriate until convergence is achieved.</p><p id="p-0094" num="0085">Next, the processor <b>20</b> stores the result of the numerical calculation of the expression (20) in the memory <b>21</b> as a learning result (S<b>15</b>). For example, the correction model information D<b>1</b> including the weight parameter W of the numerical calculation result is stored in the storage <b>21</b><i>a</i>. Thereafter, the processing illustrated in this flowchart ends.</p><p id="p-0095" num="0086">According to the above processing, as calibration of the position analysis device <b>2</b>, machine learning of the correction model F (x, W) can be performed using the training data D<b>2</b><i>a </i>in the map coordinate system and the training data D<b>2</b><i>b </i>in the camera coordinate system. This processing provides a method of generating the learned correction model F (x, W).</p><heading id="h-0015" level="2">2-4. Modification</heading><p id="p-0096" num="0087">The calibration operation as described above may be performed multiple times in consideration of various situations in the workplace <b>3</b>, for example. This modification will be described with reference to <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref>.</p><p id="p-0097" num="0088"><figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref> are diagrams for explaining a calibration operation of the position analysis device <b>2</b> in the present modification. For example, it is anticipated that the height of at least a part of the workpiece <b>32</b> in the workplace <b>3</b>, that is, the distance to the omnidirectional camera <b>10</b> in the Z direction changes, as the work progresses. Therefore, in the present modification, the calibration operation as described above is repeatedly performed under the settings in which the height of the workpiece <b>32</b> is changed.</p><p id="p-0098" num="0089"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates training data D<b>21</b><i>a </i>in the map coordinate system in a case where the workpiece <b>32</b> is at a height Z<b>1</b> of an initial value. In this case, the distance to the omnidirectional camera <b>10</b> is relatively short. <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates training data D<b>22</b><i>a </i>in the map coordinate system in the case of a middle distance with a height Z<b>2</b> (&#x3c;Z<b>1</b>). <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> illustrates training data D<b>23</b><i>a </i>in the map coordinate system with a long distance as a height Z<b>3</b> (&#x3c;Z<b>2</b>).</p><p id="p-0099" num="0090">Examples of <figref idref="DRAWINGS">FIGS. <b>8</b>A, <b>8</b>B, and <b>8</b>C</figref> correspond to e.g. an initial stage, an intermediate stage, and an end stage of work, respectively. For example, as illustrated in <figref idref="DRAWINGS">FIGS. <b>8</b>A to <b>8</b>C</figref>, the arrangement <b>32</b><i>a </i>of the workpiece <b>32</b> may be different in map coordinate systems having various heights Z<b>1</b>, Z<b>2</b>, and Z<b>3</b>. Therefore, in the calibration operation of the present modification, the position analysis device <b>2</b> uses the training data D<b>21</b><i>a</i>, D<b>22</b><i>a</i>, and D<b>23</b><i>a </i>different for each of the heights Z<b>1</b>, Z<b>2</b>, and Z<b>3</b>, and executes processing similar to steps S<b>11</b> to S<b>15</b> of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0100" num="0091">According to the calibration operation described above, a first correction model F<b>1</b> (x, W<b>1</b>) is generated in the machine learning (<b>514</b>) based on the training data D<b>21</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>. The first correction model F<b>1</b> (x, W<b>1</b>) includes a plurality of weight functions f<b>1</b> (<i>x</i>) to fn (x) based on the weight parameter W<b>1</b> obtained by machine learning based on reference positions a<b>1</b><sup>(1) </sup>to an<sup>(1) </sup>in the map coordinate system having the height Z<b>1</b>, similarly to the correction model F (x, W) described above.</p><p id="p-0101" num="0092">Furthermore, second and third correction models F<b>2</b> (x, W<b>2</b>) and F<b>3</b> (x, W<b>3</b>) are generated in the same manner as described above based on the training data D<b>22</b><i>a </i>and D<b>23</b><i>a </i>in <figref idref="DRAWINGS">FIGS. <b>8</b>B and <b>8</b>C</figref>. The correction models F<b>2</b> (x, W<b>2</b>) and F<b>3</b> (x, W<b>3</b>) are based on reference positions a<b>1</b><sup>(2) </sup>to an<sup>(2) </sup>and a<b>1</b><sup>(3) </sup>to an<sup>(3) </sup>in the case of the heights Z<b>2</b> and Z<b>3</b>, respectively, instead of the reference positions a<b>1</b><sup>(1) </sup>to an<sup>(1)</sup>, in the case of the height Z<b>1</b>. The learning results such as the weight parameters W<b>1</b>, W<b>2</b>, and W<b>3</b> of the plurality of correction models F<b>1</b> (x, WI) to F<b>3</b> (x, W<b>3</b>) are stored in the memory <b>21</b> as the correction model information D<b>1</b> (S<b>15</b>).</p><p id="p-0102" num="0093">In the present modification, the position analysis device <b>2</b> during the basic operation appropriately detects the height of the workpiece <b>32</b>, and switches, among the plurality of correction models F<b>1</b> (x, W<b>1</b>) to F<b>3</b> (x, W<b>3</b>), to the correction model to be applied to step S<b>4</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, according to the detection result. The height may be detected by providing one or more dedicated marker at a point where the height change of the workpiece <b>32</b> is relatively larger or at some or all of the reference positions. Alternatively, the detection of the height may be estimation based on the lapse of time.</p><p id="p-0103" num="0094">The above description is an example, and the present modification is not limited thereto. For example, the number of the plurality of correction models F<b>1</b> (x, W<b>1</b>) to F<b>3</b> (x, W<b>3</b>) is not limited to three, and may be two or may be four or more. The present modification can be applied to a case where the workpiece <b>32</b> is curved or deformed according to the progress of work. Also in this case, an appropriate correction model can be applied according to the progress of work, by generating a plurality of correction models based on a plurality of sets of reference positions from different training data D<b>2</b> similarly to the calibration operation described above.</p><heading id="h-0016" level="2">3. Summary</heading><p id="p-0104" num="0095">As described above, the position analysis device <b>2</b> in the present embodiment measures the target position in the workpiece <b>32</b> as an example of the subject, based on the captured image <b>5</b> of the subject by the omnidirectional camera <b>10</b>. The position analysis device <b>2</b> includes, the various I/Fs <b>24</b> and <b>25</b> each as an example of an input interface, the processor <b>20</b>, and the memory <b>21</b>. The input interface acquires the image data <b>50</b> of the captured image <b>5</b> obtained by the omnidirectional camera <b>10</b> (S<b>1</b>). The processor <b>20</b> calculates coordinate transformation from the camera coordinate system, which is an example of the first coordinate system providing measure in the captured image <b>5</b>, to the map coordinate system, which is an example of the second coordinate system providing measure in the subject, with respect to the machining position p, which is an example of the target position in the acquired captured image <b>5</b> (S<b>3</b>). The memory <b>21</b> stores the correction model F (x, W) for generating the correction amount &#x394;x of the position in the map coordinate system. The processor <b>20</b> corrects the position xp obtained by transforming the position p as the target in the analysis from the camera coordinate system to the map coordinate system based on the correction model (S<b>4</b>). The correction model F (x, W) includes the plurality of weight functions f<b>1</b> (<i>x</i>) to fn (x) corresponding to a plurality of different reference positions a<b>1</b> to an in the map coordinate system. Each weight function fi (x) produces a larger weighting to the correction amount &#x394;x as the transformed position xp is closer to the corresponding reference positions a<b>1</b> to an.</p><p id="p-0105" num="0096">According to the position analysis device <b>2</b> described above, the correction in which the weighting is changed according to the distance from each of the reference positions a<b>1</b> to an by the correction model F (x, W) is applied to the coordinate transformation result from the camera coordinate system to the map coordinate system by the omnidirectional camera <b>10</b>, and thus the correction position xc is obtained. With such a correction position xc, it is possible to accurately measure the target position in the subject from the captured image <b>5</b> by the omnidirectional camera <b>10</b>.</p><p id="p-0106" num="0097">In the present embodiment, the correction model F (x, W) sums up, for each reference position ai, a product of a difference vector di, which is an example of a vector indicating an error in coordinate transformation based on the reference position ai, and the weight function fi (x) corresponding to the reference position ai, to generate the correction amount &#x394;x (cf. the expressions (11) and (13)). According to this, the correction position xc is obtained so as to offset the error at the reference position ai, resulting in improving the accuracy of the position measurement.</p><p id="p-0107" num="0098">In the present embodiment, the correction model F (x, W) has the weight factor wi, which adjusts weighting according to the distance |x&#x2212;ai| from each reference position ai to the transformed position x, for each weight function fi. (x) (cf. the expression (13)). By the secondary weighting of the weight function fi (x) based on the weight factor wi, the correction position xc can be obtained with high accuracy.</p><p id="p-0108" num="0099">In the present embodiment, the correction model F (x, W) is constructed by machine learning that presets the weight factor wi so as to minimize the error at each reference position ai (S<b>14</b>, the expression (20)). The position can be accurately corrected by the learned correction model F (x, W).</p><p id="p-0109" num="0100">An example of the camera in the present embodiment is the omnidirectional camera <b>10</b>. The camera coordinate system, which is an example of the first coordinate system, is defined by a projection method of the omnidirectional camera <b>10</b>. According to the position analysis device <b>2</b> of the present embodiment, it is possible to correct an error in coordinate transformation due to lens distortion or the like of the omnidirectional camera <b>10</b>.</p><p id="p-0110" num="0101">In the present embodiment, the map coordinate system, which is an example of the second coordinate system, is defined by information indicating the arrangement of the subject such as the workpiece <b>32</b> in a place such as the workplace <b>3</b> in which the omnidirectional camera <b>10</b> is installed. According to the position analysis device <b>2</b> of the present embodiment, it is possible to accurately obtain the target position in the map coordinate system.</p><p id="p-0111" num="0102">In the present embodiment, the target position is a position such as the machining position p at which the machining work is performed on the workpiece <b>32</b> as the subject. According to the position analysis device <b>2</b> of the present embodiment, it is possible to analyze the position at which the machining work has been performed on the workpiece <b>32</b>.</p><p id="p-0112" num="0103">In the present embodiment, the camera system <b>1</b> includes the omnidirectional camera <b>10</b> that captures an image of the subject to generate the captured image <b>5</b>, and the position analysis device <b>2</b> that measures the target position in the subject based on the captured image <b>5</b> by the omnidirectional camera <b>10</b>. According to the present system <b>1</b>, it is possible to accurately measure the target position in the subject from the captured image <b>5</b> by the camera such as the omnidirectional camera <b>10</b>.</p><p id="p-0113" num="0104">In the present embodiment, the memory <b>21</b> stores the plurality of correction models F<b>1</b> (x, WI), F<b>2</b> (x, W<b>2</b>), and F<b>3</b> (x, W<b>3</b>). Each of the correction models F<b>1</b> (x, WI), F<b>2</b> (x, W<b>2</b>), and F<b>3</b> (x, W<b>3</b>) corresponds to each set of reference positions in the plurality of sets of reference positions a<b>1</b><sup>(1) </sup>to an<sup>(1)</sup>, a<b>1</b><sup>(2) </sup>to an<sup>(2)</sup>, and a<b>1</b><sup>(3) </sup>to an<sup>(3)</sup>. The processor <b>20</b> switches, among the plurality of correction models F<b>1</b> (x, W<b>1</b>) to F<b>3</b> (x, W<b>3</b>), to the correction model used to correct the target position. Appropriate correction can be performed according to the state of the subject by the plurality of correction models F<b>1</b> (x, W<b>1</b>) to F<b>3</b> (x, W<b>3</b>).</p><p id="p-0114" num="0105">The position measurement method in the present embodiment is a method of measuring the target position in the subject based on the captured image of the subject by the omnidirectional camera <b>10</b>. The method includes: acquiring (S<b>1</b>) the captured image captured by the omnidirectional camera <b>10</b>; calculating (S<b>3</b>) coordinate transformation from the first coordinate system according to the captured image to the second coordinate system according to the subject, with respect to the target position in the acquired captured image; and correcting (S<b>4</b>) the transformed position from the target position in the first coordinate system to the second coordinate system, based on the correction model F (x, W) to generate the correction amount ax of the position in the second coordinate system. The correction model F (x, W) gives a larger weighting to the correction amount &#x394;x as the transformation position xp is closer to the corresponding reference position ai in each weight function fi (x) in the plurality of weight functions f<b>1</b> (<i>x</i>) to fn (x) corresponding to the plurality of reference positions a<b>1</b> to an different from each other in the second coordinate system.</p><p id="p-0115" num="0106">In the present embodiment, a program for causing a computer to execute the position analysis method as described above is provided. The position analysis method of the present embodiment can accurately measure the target position in the subject from the captured image <b>5</b> by the camera such as the omnidirectional camera <b>10</b>.</p><heading id="h-0017" level="1">Other Embodiments</heading><p id="p-0116" num="0107">As described above, the first embodiment has been described as an example of the technology disclosed in the present application. However, the technology in the present disclosure is not limited to this, and is applicable to embodiments in which changes, replacements, additions, omissions, and the like are appropriately made. Further, each component described in the above embodiments can be combined to make a new embodiment. Therefore, other embodiments are described below.</p><p id="p-0117" num="0108">In the first embodiment described above, in step S<b>1</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an example in which the image data <b>50</b> of the captured image <b>5</b> of the omnidirectional camera <b>10</b> is acquired via the device I/F <b>24</b> has been described, but step S<b>1</b> is not limited thereto. For example, the position analysis device <b>2</b> may acquire the image data <b>50</b> via the network I/F <b>25</b>, or may read the image data <b>50</b> stored in advance in the memory <b>21</b>. For example, the position analysis device <b>2</b> may analyze recorded image data captured in advance by the omnidirectional camera <b>10</b>. Furthermore, in step S<b>1</b>, the image data <b>50</b> may be acquired in units of one frame or in units of a plurality of frames. The processing after step S<b>2</b> can also be executed in units of a plurality of frames.</p><p id="p-0118" num="0109">In the recognition of the machining position p in step S<b>2</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the height of the surface machining tool <b>34</b> may be considered. For example, the processor <b>20</b> as the image recognition module <b>41</b> may detect the contour line of the surface machining tool <b>34</b> together with the position of the marker <b>34</b><i>a </i>in the captured image <b>5</b> of the omnidirectional camera <b>10</b>, and recognize the position closest to the center position p<b>0</b> of the captured image <b>5</b> in the detected contour line as the machining position p. When the height to the marker <b>34</b><i>a </i>in the surface machining tool <b>34</b> is known, the processor <b>20</b> may calculate the machining position p by triangulation from the position of the marker <b>34</b><i>a </i>in the captured image <b>5</b>.</p><p id="p-0119" num="0110">In the above description, an example in which the height of the surface machining tool <b>34</b> is reflected in the processing of step S<b>2</b> has been described, but the height may be reflected at the time of calibration. For example, when the captured image <b>5</b>A for calibration is captured, a jig having a marker having the same height as the height of the marker <b>34</b><i>a </i>of the surface machining tool <b>34</b> is arranged at each reference position. By performing the calibration operation similar to that in <figref idref="DRAWINGS">FIG. <b>7</b></figref> using the training data D<b>2</b><i>b </i>based on the captured image <b>5</b>A, it is possible to obtain the weighting of the correction model F (x, W) in which the height of the marker is considered in advance. In this case, in step S<b>2</b> during the basic operation, even if the position of the marker <b>34</b><i>a </i>is recognized as the machining position p, the correction position xc can be obtained with high accuracy.</p><p id="p-0120" num="0111">In the above embodiments, the machining position at which the surface machining is performed has been described as an example of the target position in the position analysis device <b>2</b>. In the present embodiment, the target position is not limited thereto, and can be set to various positions to be analyzed by the user <b>11</b>, for example. For example, the target position may be a position of a place where machining work different from surface machining is performed on the workpiece <b>32</b>, or may be a position of the worker <b>31</b> in the workplace <b>3</b> as an example of the subject. The workplace <b>3</b> and various objects constituting the workplace <b>3</b> are an example of a subject in the present embodiment. Further, the target position is not limited to one point, and may be a plurality of points, lines, or regions.</p><p id="p-0121" num="0112">In the above embodiments, an example in which the reference positions a<b>1</b> to an are set in the workpiece <b>32</b> has been described, but the reference positions are not limited thereto. In the present embodiment, the reference position may be set in the pedestal <b>33</b> of the workpiece <b>32</b>, for example. For example, the training data D<b>21</b><i>a </i>in the map coordinate system may be created using shape data such as three-dimensional CAD data of the pedestal <b>33</b>.</p><p id="p-0122" num="0113">In the above embodiments, the omnidirectional camera <b>10</b> has been described as an example of the camera in the camera system <b>1</b>. In the present embodiment, the camera system <b>1</b> is not limited to the omnidirectional camera <b>10</b>, and may include various cameras. For example, the camera of the present system <b>1</b> may be various imaging devices that employ various projection methods such as an equisolid angle projection method, a stereoscopic projection method, an orthogonal projection method, a cylindrical projection method, and a central projection method.</p><p id="p-0123" num="0114">Furthermore, in the above embodiments, an example in which the camera system <b>1</b> is applied to the workplace <b>3</b> has been described. In the present embodiment, the site to which the camera system <b>1</b> and the position analysis device <b>2</b> are applied is not particularly limited to the workplace <b>3</b>, and may be various sites such as a shop floor.</p><p id="p-0124" num="0115">As described above, the embodiments have been described as an example of the technology in the present disclosure. For this purpose, the accompanying drawings and the detailed description have been provided.</p><p id="p-0125" num="0116">Accordingly, some of the components described in the accompanying drawings and the detailed description may include not only essential components for solving the problem but also components which are not essential for solving the problem in order to describe the above technology. Therefore, it should not be immediately recognized that these non-essential components are essential based on the fact that these non-essential components are described in the accompanying drawings and the detailed description.</p><p id="p-0126" num="0117">The present disclosure is applicable to an application of measuring various target positions in various subjects using a camera.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005175A1-20230105-M00001.NB"><img id="EMI-M00001" he="9.14mm" wi="76.20mm" file="US20230005175A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005175A1-20230105-M00002.NB"><img id="EMI-M00002" he="7.79mm" wi="76.20mm" file="US20230005175A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005175A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230005175A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A position analysis device for measuring a target position in a subject based on a captured image of the subject by a camera, the position analysis device comprising:<claim-text>an input interface that acquires the captured image obtained by the camera;</claim-text><claim-text>a processor that calculates coordinate transformation from a first coordinate system to a second coordinate system with respect to the target position in the acquired captured image, the first coordinate system providing measure in the captured image, the second coordinate system providing measure in the subject; and</claim-text><claim-text>a memory that stores a correction model to generate a correction amount of a position in the second coordinate system, wherein</claim-text><claim-text>the processor corrects a transformed position from the target position, based on the correction model including a plurality of weight functions corresponding to a plurality of reference positions, the transformed position being a position obtained by the coordinate transformation on the target position in the first coordinate system to the second coordinate system, the weight functions each producing a larger weighting in the correction amount as the transformed position is closer to a corresponding reference position, the reference positions being different from each other in the second coordinate system.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction model generates the correction amount by summing up, for each reference position, a product of a vector and a weight function corresponding to the reference position, the vector indicating an error of the coordinate transformation with respect to the reference position.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the correction model has a weight factor for each weight function, the weight factor adjusting the weighting in accordance with a distance from each reference position to the transformed position.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The position analysis device according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the correction model is constructed by machine learning for presetting the weight factor to minimize an error at each reference position.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the memory stores a plurality of correction models each corresponding to each set of reference positions in a plurality of sets of reference positions, and</claim-text><claim-text>the processor switches, among the plurality of correction models, a correction model to be used for correction of the target position.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the camera is an omnidirectional camera, and</claim-text><claim-text>the first coordinate system is defined by a projection method of the omnidirectional camera.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second coordinate system is defined by information on arrangement of the subject at a place where the camera is installed.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the target position is a position at which a machining work is performed on a workpiece as the subject.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A camera system comprising:<claim-text>a camera that captures an image of the subject to generate the captured image; and</claim-text><claim-text>the position analysis device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, which measures the target position in the subject, based on the captured image by the camera.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A position analysis method for measuring a target position in a subject based on a captured image of the subject by a camera, the position analysis method comprising:<claim-text>acquiring the captured image obtained by the camera;</claim-text><claim-text>calculating coordinate transformation from a first coordinate system to a second coordinate system with respect to the target position in the acquired captured image, the first coordinate system providing measure in the captured image, and the second coordinate system providing measure in the subject; and</claim-text><claim-text>correcting a transformed position from the target position, based on a correction model to generate a correction amount of a position in the second coordinate system, the transformed position being a position obtained by the coordinate transformation on the target position in the first coordinate system to the second coordinate system, wherein the correction model provides a larger weighting in the correction amount for each weight function in a plurality of weight functions corresponding to a plurality of reference positions as the transformed position is closer to a corresponding reference position, the reference positions being different from each other in the second coordinate system.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory computer-readable recording medium storing a program that causes a computer to execute the position analysis method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>.</claim-text></claim></claims></us-patent-application>