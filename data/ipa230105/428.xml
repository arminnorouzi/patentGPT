<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000429A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000429</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17930569</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2020-0054051</doc-number><date>20200506</date></priority-claim><priority-claim sequence="02" kind="national"><country>KR</country><doc-number>10-2020-0102803</doc-number><date>20200814</date></priority-claim><priority-claim sequence="03" kind="national"><country>KR</country><doc-number>10-2020-0127093</doc-number><date>20200929</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4818</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7425</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>743</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7267</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>7289</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e98">DEVICE AND METHOD FOR TESTING RESPIRATORY STATE, AND DEVICE AND METHOD FOR CONTROLLING SLEEP DISORDER</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2021/005672</doc-number><date>20210506</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17930569</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Seoul National University R&#x26;DB Foundation</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SHIN</last-name><first-name>Hyun-Woo</first-name><address><city>Seongnam-si</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A respiratory status examination apparatus and method, and a sleep disorder control device and method are proposed. The apparatus may include at least one image capturing unit that is movably arranged to adjust a distance with respect to a subject and configured to obtain a thermal image by photographing the subject. The apparatus may also include a motion sensor unit configured to detect a motion of the subject to generate motion information, and a temperature information extracting unit configured to specify at least one examination region from the thermal image obtained by the image capturing unit and extract temperature information from the examination region. The apparatus may further include a respiratory status examining unit configured to determine a respiratory status of the subject based on the temperature information extracted by the temperature information extracting unit and the motion information generated by the motion sensor unit.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="92.03mm" wi="111.42mm" file="US20230000429A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="121.75mm" wi="113.45mm" file="US20230000429A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="130.73mm" wi="126.49mm" file="US20230000429A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="208.79mm" wi="143.51mm" orientation="landscape" file="US20230000429A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="117.35mm" wi="124.21mm" file="US20230000429A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.27mm" wi="117.09mm" file="US20230000429A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="168.23mm" wi="152.57mm" file="US20230000429A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="157.65mm" wi="117.52mm" file="US20230000429A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="96.18mm" wi="130.73mm" file="US20230000429A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="194.48mm" wi="138.60mm" file="US20230000429A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="215.82mm" wi="104.31mm" orientation="landscape" file="US20230000429A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="99.65mm" wi="132.67mm" file="US20230000429A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="152.15mm" wi="144.36mm" file="US20230000429A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="217.00mm" wi="153.67mm" file="US20230000429A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="186.44mm" wi="107.61mm" file="US20230000429A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="130.30mm" wi="157.14mm" file="US20230000429A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="167.81mm" wi="152.06mm" orientation="landscape" file="US20230000429A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="120.23mm" wi="132.84mm" file="US20230000429A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="132.16mm" wi="114.55mm" file="US20230000429A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This is a continuation application of International Application No. PCT/KR2021/005672, filed on May 6, 2021, which claims the benefit of Korean Patent Applications Nos. 10-2020-0054051 filed on May 6, 2020, 10-2020-0102803 filed on Aug. 14, 2020, and 10-2020-0127093 filed on Sep. 29, 2020 in the Korean Intellectual Property Office, the entire disclosure of each of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Technical Field</heading><p id="p-0003" num="0002">Embodiments of the present disclosure relate to a respiratory status examination apparatus and method, and a sleep disorder control device and method.</p><heading id="h-0004" level="1">Description of Related Technology</heading><p id="p-0004" num="0003">In general, when the muscles surrounding the airway are relaxed during sleep, the uvula, tonsils, and tongue collapse backwards. This may result in a slightly narrower airway than when awake, but this is not a problem for most people. However, for some people, snoring or obstructive sleep apnea (OSA) occurs because the airways are severely narrowed during sleep by this phenomenon, preventing air from passing through the airways.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0005" num="0004">One aspect is a respiratory status monitoring apparatus and method, whereby a respiratory status of a patient may be easily and precisely examined while alleviating discomfort of the patient.</p><p id="p-0006" num="0005">Embodiments of the present disclosure provide a sleep disorder control device and method for maximizing the mandibular advancement effect.</p><p id="p-0007" num="0006">Embodiments of the present disclosure are intended to provide a polysomnography device which allows efficient learning by using processed images, instead of time-series data of a source signal of examination units as learning data, and an examination method thereof.</p><p id="p-0008" num="0007">A respiratory status monitoring apparatus according to an embodiment of the present disclosure may include: at least one image capturing unit that is movably arranged to adjust a distance with respect to a subject and configured to obtain a thermal image by photographing the subject; a motion sensor unit configured to detect a motion of the subject to generate motion information; a temperature information extracting unit configured to specify at least one examination region from the thermal image obtained by the image capturing unit and extract temperature information from the examination region; and a respiratory status examining unit configured to determine a respiratory status of the subject based on the temperature information extracted by the temperature information extracting unit and the motion information generated by the motion sensor unit.</p><p id="p-0009" num="0008">According to a respiratory status monitoring apparatus and method according to embodiments of the present disclosure, a decrease in the accuracy of examination due to obstruction factors may be prevented by capturing a thermal image by using a near-infrared or infrared camera, and the discomfort of a subject may be reduced through a non-contact type examination method.</p><p id="p-0010" num="0009">According to a sleep disorder control device and an operating method thereof according to embodiments of the present disclosure, a sleep disorder may be detected using biometric information, and when treating the detected sleep disorder by advancing the mandible, arousal due to the movement of the mandible may be minimized by also considering a sleep satisfaction level of a user to thereby improve sleep quality.</p><p id="p-0011" num="0010">According to the sleep disorder control device and the operating method thereof according to the embodiments of the present disclosure, not only sleep satisfaction level data obtained immediately after a sleep but also sleep satisfaction level data of before going to sleep (which data evaluating daytime activity or cognitive ability, etc.) after spending a day may be used as learning data, and thus the learning efficiency may be improved.</p><p id="p-0012" num="0011">According to a polysomnography device and an examination method thereof according to embodiments of the present disclosure, instead of raw data obtained from a plurality of examination units, a graph image generated using the raw data is used as learning data, and thus, accurate reading results may be derived while increasing the efficiency of artificial intelligence- or deep learning-based learning.</p><p id="p-0013" num="0012">The polysomnography device and the examination method thereof according to the embodiments of the present disclosure may realize automated examination through a trained sleep state reading model, thereby shortening the examination time as well as reducing the examination deviation according to readers.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a respiratory status monitoring apparatus according to an embodiment of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a respiratory status monitoring apparatus according to another embodiment of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a respiratory status monitoring apparatus according to another embodiment of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a processor and a motion sensor unit of the respiratory status monitoring apparatus according to the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a method of specifying an examination region and extracting temperature information, performed by the respiratory status monitoring apparatus according to the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a method of adjusting a position of an image capturing unit of the respiratory status monitoring apparatus according to the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating a respiratory status monitoring method according to an embodiment of the present disclosure in order.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram schematically illustrating a mandibular advancement system according to an embodiment of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram schematically illustrating a server according to an embodiment of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref> are diagrams for explaining a process of obtaining and learning sleep satisfaction level data.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart sequentially illustrating a sleep disorder control method according to an embodiment of the present disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart for explaining a control method of the mandibular advancement system.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram schematically illustrating a polysomnography device <b>100</b>&#x2033; according to an embodiment of the present disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a conceptual diagram for explaining a process of obtaining polysomnography data from a plurality of examination units.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a graph image which is learning data of a polysomnography device according to an embodiment of the present disclosure.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows a labeled graph image.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram sequentially illustrating an examination method of a polysomnography device according to an embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0031" num="0030">Snoring or obstructive sleep apnea (OSA) can lower the quality of a person's sleep or cause other complex problems, and thus an examination and treatment are required, and thus apparatuses and methods for examination and treating symptoms are developed. However, the devices and methods developed and used so far cause discomfort or pain of the patient during the examination or treatment, preventing good quality sleep, and furthermore, the precision or accuracy of the examination thereof is low. Therefore, development of a technique enabling precise diagnosis and observation of snoring or OSA of a patient and allowing alleviation of the discomfort of a user in a treatment process is required.</p><p id="p-0032" num="0031">A respiratory status monitoring apparatus according to an embodiment of the present disclosure may include: at least one image capturing unit that is movably arranged to adjust a distance with respect to a subject and configured to obtain a thermal image by photographing the subject; a motion sensor unit configured to detect a motion of the subject to generate motion information; a temperature information extracting unit configured to specify at least one examination region from the thermal image obtained by the image capturing unit and extract temperature information from the examination region; and a respiratory status examining unit configured to determine a respiratory status of the subject based on the temperature information extracted by the temperature information extracting unit and the motion information generated by the motion sensor unit.</p><p id="p-0033" num="0032">In an embodiment of the present disclosure, the image capturing unit may be provided in plurality, and the plurality of the image capturing units may be arranged apart from each other around the subject.</p><p id="p-0034" num="0033">In an embodiment of the present disclosure, the image capturing unit may include a near-infrared camera.</p><p id="p-0035" num="0034">In an embodiment of the present disclosure, the temperature information extracting unit may specify a plurality of examination regions from the thermal image, and the plurality of the examination regions may include: a first examination region specified based on positions of the nose and mouth of the subject; a second examination region specified based on positions of the chest and abdomen of the subject; and a third examination region specified based on positions of the arms and legs of the subject.</p><p id="p-0036" num="0035">In an embodiment of the present disclosure, the respiratory status examining unit may determine the respiratory status of the subject based on the temperature information detected from the first examination region to the third examination region.</p><p id="p-0037" num="0036">In an embodiment of the present disclosure, the respiratory status monitoring apparatus may further include a learning unit that learns, by machine learning, respiratory status determination criteria based on the temperature information and the motion information, wherein the respiratory status examining unit determines the respiratory status of the subject based on the respiratory status determination criteria.</p><p id="p-0038" num="0037">In an embodiment of the present disclosure, the respiratory status monitoring apparatus may further include a position adjuster adjusting a position of the image capturing unit according to a change in a posture of the subject.</p><p id="p-0039" num="0038">In an embodiment of the present disclosure, the respiratory status monitoring apparatus may further include a learning unit that learns, by machine learning, posture determination criteria for determining the posture of the subject, based on the motion information, wherein the position adjuster determines the posture of the subject based on the posture determination criteria, and adjusts the position of the image capturing unit according to the determined posture of the subject.</p><p id="p-0040" num="0039">In an embodiment of the present disclosure, a respiratory status monitoring method may include: capturing a thermal image of a subject by using a near-infrared camera; specifying, by a temperature information extracting unit, an examination region from the thermal image, based on positions of the nose and mouth of the subject; extracting, by the temperature information extracting unit, temperature information from the examination region; generating, by a motion sensor unit, motion information by detecting a motion of the subject; and detecting, by a respiratory status examining unit, a respiratory status of the subject based on the temperature information and the motion information.</p><p id="p-0041" num="0040">In an embodiment of the present disclosure, the near-infrared camera may be provided in plurality, and the plurality of the near-infrared cameras may be arranged apart from each other around the subject.</p><p id="p-0042" num="0041">In an embodiment of the present disclosure, the respiratory status monitoring method may further include specifying, by the temperature information extracting unit, an additional examination region; and detecting, by the temperature information extracting unit, temperature information from the additional examination region, wherein the additional examination region is specified based on at least one of positions of the chest and abdomen of the subject and positions of arms and legs of the subject.</p><p id="p-0043" num="0042">In an embodiment of the present disclosure, the respiratory status monitoring method may further include learning, by a learning unit by machine learning, respiratory status determination criteria based on the temperature information and the motion information.</p><p id="p-0044" num="0043">In an embodiment of the present disclosure, the detecting of the respiratory status of the subject may include determining the respiratory status of the subject based on the respiratory status determination criteria.</p><p id="p-0045" num="0044">In an embodiment of the present disclosure, the respiratory status monitoring method may further include adjusting, by a position adjuster, a position of the near-infrared camera according to a change in a posture of the subject.</p><p id="p-0046" num="0045">In an embodiment of the present disclosure, the respiratory status monitoring method may further include learning, by a learning unit by machine learning, posture determination criteria based on the motion information, wherein the adjusting of the position of the near-infrared camera includes determining, by the position adjuster, the posture of the subject based on the posture determination criteria.</p><p id="p-0047" num="0046">An embodiment of the present disclosure provides a sleep disorder control method including: obtaining sleep satisfaction level data and bio-signal data of a user wearing a sleep disorder treatment device, and usage record data of the sleep disorder treatment device; training a machine learning model based on the sleep satisfaction level data, the bio-signal data, and the usage record data; and controlling an operation of the sleep disorder treatment device while the user is wearing the sleep disorder treatment device, by using the sleep satisfaction level data, the bio-signal data, the usage record data, and the machine learning model.</p><p id="p-0048" num="0047">In an embodiment of the present disclosure, the obtaining the sleep satisfaction level data and the bio-signal data of the user and the usage record data of the sleep disorder treatment device may include obtaining the bio-signal data of the user and the usage record data of the sleep disorder treatment device during a sleep of the user wearing the sleep disorder treatment device and obtaining the sleep satisfaction level data after the user wearing the sleep disorder treatment device completes the sleep.</p><p id="p-0049" num="0048">In an embodiment of the present disclosure, the obtaining of the sleep satisfaction level data may include obtaining first sleep satisfaction level data at a first time point when the user completes sleep and obtaining second sleep satisfaction level data at a second time point different from the first time point.</p><p id="p-0050" num="0049">In an embodiment of the present disclosure, the obtaining of the second sleep satisfaction level data may obtain the second sleep satisfaction level data after a preset period of time from the first time point and before a next sleep of the user.</p><p id="p-0051" num="0050">In an embodiment of the present disclosure, the obtaining of the sleep satisfaction level data may include generating a first notification signal to the user before the first time point and generating a second notification signal to the user before the second time point.</p><p id="p-0052" num="0051">In an embodiment of the present disclosure, the controlling of the operation of the sleep disorder treatment device may include controlling a degree of advancement or the number of advances of the sleep disorder treatment device while the user is wearing the sleep disorder treatment device.</p><p id="p-0053" num="0052">An embodiment of the present disclosure provides a sleep disorder control device including: a data obtaining unit configured to obtain sleep satisfaction level data and bio-signal data of a user wearing a sleep disorder treatment device, and usage record data of the sleep disorder treatment device; a learning unit configured to train, by machine learning, a machine learning model, based on the sleep satisfaction level data, the bio-signal data, and the usage record data; and an operation controller configured to control an operation of the sleep disorder treatment device while the user is wearing the sleep disorder treatment device, by using the sleep satisfaction level data, the bio-signal data, the usage record data, and the machine learning model.</p><p id="p-0054" num="0053">In an embodiment of the present disclosure, the data obtaining unit may include: a bio-signal obtaining unit configured to obtain the bio-signal data by using one or more sensors during a sleep of the user wearing the sleep disorder treatment device; a usage record obtaining unit configured to obtain the usage record data of the sleep disorder treatment device during the sleep of the user wearing the sleep disorder treatment device; and a sleep satisfaction level obtaining unit configured to obtain the sleep satisfaction level data after the user wearing the sleep disorder treatment device completes the sleep.</p><p id="p-0055" num="0054">In an embodiment of the present disclosure, the sleep satisfaction level obtaining unit may obtain first sleep satisfaction level data at a first time point when the user completes the sleep and second sleep satisfaction level data at a second time point different from the first time point.</p><p id="p-0056" num="0055">In an embodiment of the present disclosure, the second sleep satisfaction level data may be obtained at the second time point which is after a preset period of time from the first time point and before a next sleep of the user.</p><p id="p-0057" num="0056">In an embodiment of the present disclosure, the sleep disorder control device may further include a notification signal generator that generates a first notification signal to the user before the first time point and generates a second notification signal to the user before the second time point.</p><p id="p-0058" num="0057">In an embodiment of the present disclosure, the operation controller may control, by using the sleep satisfaction level data, the bio-signal data, the usage record data, and the machine learning model, a degree of advancement or the number of advances of the sleep disorder treatment device while the user is wearing the sleep disorder treatment device.</p><p id="p-0059" num="0058">An embodiment of the present disclosure provides a polysomnography device including: a graph image generator configured to obtain polysomnography raw data that is measured in time series, and convert the polysomnography data into a graph with respect to time to generate a graph image, a learning unit configured to train a sleep state reading model based on the graph image; and a reader configured to read a sleep state of a user based on the graph image and the sleep state reading model.</p><p id="p-0060" num="0059">In an embodiment of the present disclosure, the polysomnography device may further include: a split image generator configured to generate a plurality of images by splitting the graph image in units of a preset time, wherein the learning unit trains the sleep state reading model based on the plurality of images obtained by the splitting of the graph image.</p><p id="p-0061" num="0060">In an embodiment of the present disclosure, the polysomnography data may be a plurality of pieces of biometric data of a user, which are measured using a plurality of examination units, and the graph image generator may convert each piece of the plurality of biometric data into an individual graph with respect to time, and sequentially arrange the converted, plurality of individual graphs on a time axis to generate the graph image.</p><p id="p-0062" num="0061">In an embodiment of the present disclosure, the plurality of pieces of biometric data may include biometric data obtained using at least one of sensing units among an Electroencephalogram (EEG) sensor, an Electrooculography (EOG) sensor, an Electromyogram (EMG) sensor, an Electrokardiogramme (EKG) sensor, a Photoplethysmography (PPG) sensor, a chest belt, an abdomen belt, oxygen saturation, end-tidal CO2 (EtCO2), a respiration detection thermistor, a flow sensor, a pressure sensor (manometer), a microphone, and a positive pressure gauge of a continuous positive pressure device.</p><p id="p-0063" num="0062">In an embodiment of the present disclosure, the graph image generator may generate the graph image by matching times of the plurality of pieces of biometric data.</p><p id="p-0064" num="0063">In an embodiment of the present disclosure, the graph image may include labeled data.</p><p id="p-0065" num="0064">An embodiment of the present disclosure provides an examination method of a polysomnography device, the method including: obtaining time-serially measured polysomnography data; converting the polysomnography data into a graph with respect to time to generate a graph image; training a sleep state reading model based on the graph image; and reading a sleep state of a user based on the graph image and the sleep state reading model.</p><p id="p-0066" num="0065">In an embodiment of the present disclosure, the method may further include generating a plurality of images by splitting the graph image in units of a preset time, and the training of the sleep state reading model may include training the sleep state reading model based on the plurality of images obtained by the splitting.</p><p id="p-0067" num="0066">In an embodiment of the present disclosure, the polysomnography data may include a plurality of pieces of biometric data of a user, which are measured using a plurality of examination units, and the graph image generator may convert each of the plurality of pieces of biometric data into an individual graph with respect to time, and sequentially arrange the converted, plurality of individual graphs on a time axis to generate the graph image.</p><p id="p-0068" num="0067">In an embodiment of the present disclosure, the plurality of pieces of biometric data may include biometric data obtained using at least one of sensing units among an Electroencephalogram (EEG) sensor, an Electrooculography (EOG) sensor, Electromyogram (EMG) sensor, an Electrokardiogramme (EKG) sensor, a Photoplethysmography (PPG) sensor, a chest belt, an abdomen belt, oxygen saturation, end-tidal CO2 (EtCO2), a respiration detection thermistor, a flow sensor, a pressure sensor (manometer), a microphone, and a positive pressure gauge of a continuous positive pressure device.</p><p id="p-0069" num="0068">In an embodiment of the present disclosure, the generating of the graph image may include generating the graph image by matching times of the plurality of pieces of biometric data.</p><p id="p-0070" num="0069">In an embodiment of the present disclosure, the generating of the graph image may include generating the graph image including labeled data.</p><p id="p-0071" num="0070">Other aspects, features and advantages other than those described above will become apparent from the following drawings, claims, and detailed description of the present disclosure.</p><p id="p-0072" num="0071">As the present disclosure allows for various changes and numerous embodiments, particular embodiments will be illustrated in the drawings and described in detail in the written description. However, this is not intended to limit the present disclosure to particular modes of practice, and it is to be appreciated that all changes, equivalents, and substitutes that do not depart from the spirit and technical scope of the present disclosure are encompassed in the present disclosure. In the description of the present disclosure, certain detailed explanations of related art are omitted when it is deemed that they may unnecessarily obscure the essence of the present disclosure.</p><p id="p-0073" num="0072">While such terms as &#x201c;first,&#x201d; &#x201c;second,&#x201d; etc., may be used to describe various components, such components must not be limited to the above terms. The above terms are used only to distinguish one component from another.</p><p id="p-0074" num="0073">The terms used in the present specification are merely used to describe particular embodiments, and are not intended to limit the present disclosure. An expression used in the singular encompasses the expression of the plural, unless it has a clearly different meaning in the context. In the drawings, each constituent element is exaggerated, omitted, or schematically illustrated for convenience of explanation and clarity, and the size of each constituent element does not perfectly reflect an actual size.</p><p id="p-0075" num="0074">In the description of each constituent element, when each constituent element is described as being formed &#x201c;on&#x201d; or &#x201c;under&#x201d; a constituent element, the constituent element may be formed &#x201c;directly&#x201d; or &#x201c;indirectly&#x201d; with any other constituent element interposed therebetween &#x201c;on&#x201d; or &#x201c;under&#x201d; the constituent element. The status of &#x201c;on&#x201d; or &#x201c;under&#x201d; of a constituent element is described based on the drawings.</p><p id="p-0076" num="0075">Hereinafter, embodiments of the present disclosure will be described below in more detail with reference to the accompanying drawings. Those components that are the same or are in correspondence are rendered the same reference numeral regardless of the figure number, and redundant explanations are omitted.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a respiratory status monitoring apparatus according to an embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a respiratory status monitoring apparatus according to another embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a respiratory status monitoring apparatus according to another embodiment of the present disclosure. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a processor and a motion sensor unit according to the present disclosure. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a method of specifying an examination region and extracting temperature information, performed by the respiratory status monitoring apparatus according to the present disclosure. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a method of adjusting a position of an image capturing unit of the respiratory status monitoring apparatus according to the present disclosure.</p><p id="p-0078" num="0077">Referring to <figref idref="DRAWINGS">FIGS. <b>1</b> to <b>6</b></figref>, a respiratory status monitoring apparatus <b>10</b> may include an image capturing unit <b>100</b>, a motion sensor unit <b>200</b>, a temperature information extracting unit <b>310</b>, and a respiratory status examining unit <b>320</b>. In addition, the respiratory status monitoring apparatus <b>10</b> may further include a position adjuster <b>340</b> and a learning unit (or a learning processor) <b>330</b>.</p><p id="p-0079" num="0078">The respiratory status may include a normal respiratory status, a hypopnea state, and an apnea state, and it may be determined, based on a change in body temperature of a subject P, which state a current respiratory status of the subject P corresponds to. For example, during exhalation, the temperature around the nose and mouth of the subject P may rise as air heated by the body temperature of the subject P is discharged to the outside through the nose and mouth. Accordingly, a thermal image of the subject P, captured by a thermal imaging camera, and a temperature signal of the subject P may be changed. Here, compared with the degree of change in a thermal image and a temperature signal in a normal respiratory status, for example, the degree of change in the thermal image and the temperature signal may be lower in a hypopnea state, and in another example, there may be no change in a thermal image of the surroundings in the case of apnea. Accordingly, a respiration-specific pattern may be determined by analyzing thermal images of the vicinity of the nose and mouth.</p><p id="p-0080" num="0079">The image capturing unit <b>100</b> may capture an image of the subject P to obtain a thermal image of the subject P. The image capturing unit <b>100</b> may include a thermal imaging camera capable of photographing a temperature distribution of the body of the subject P. As an embodiment, the thermal imaging camera may include a near-infrared camera, an infrared camera, or other cameras capable of capturing a thermal image of a human body. However, for convenience, description below will focus on an embodiment in which the image capturing unit <b>100</b> includes a near-infrared camera. As the image capturing unit <b>100</b> includes a near-infrared camera, the image capturing unit <b>100</b> may obtain a thermal image of the subject P without being disturbed by obstacles even when there is an interference factor between the image capturing unit <b>100</b> and the subject P (e.g., a blanket covering the subject P, clothes that the subject P is wearing, a curtain arranged between the subject P and the image capturing unit <b>100</b>, etc.). In this case, a thermal image captured by the image capturing unit <b>100</b> may be, for example, a near-infrared multi-spectral image.</p><p id="p-0081" num="0080">The image capturing unit <b>100</b> may be arranged apart from the subject P. In this case, the image capturing unit <b>100</b> may be spaced apart, by a certain distance, from the subject P or an examination bed B on which the subject P is located, and thus may capture an image of the subject P while not contacting the subject P.</p><p id="p-0082" num="0081">The image capturing unit <b>100</b> may be movably arranged. In this case, the image capturing unit <b>100</b> may adjust a distance from the image capturing unit <b>100</b> to the subject P. Accordingly, by adjusting a position of the image capturing unit <b>100</b> according to the body characteristics such as the height of the subject P, a required thermal image of a body region of the subject P may be obtained.</p><p id="p-0083" num="0082">At least one image capturing unit <b>100</b> may be included. In an embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, one image capturing unit <b>100</b> may be included. In this case, the image capturing unit <b>100</b> may be arranged at an optimal position for obtaining a thermal image of the subject P. For example, the image capturing unit <b>100</b> may be located above the subject P or the examination bed B on which the subject P is located, and here, the image capturing unit <b>100</b> may be located above the tiptoe of the subject P or above the head of the subject P. As another example, the image capturing unit <b>100</b> may be located around the examination bed B with respect to the examination bed B, and in this case, the image capturing unit <b>100</b> may be arranged at a side of the subject P or at a side of the examination bed B.</p><p id="p-0084" num="0083">In another embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a plurality of image capturing units <b>100</b> may be included. In this case, the plurality of image capturing units <b>100</b> may be arranged apart from each other. In detail, the plurality of image capturing units <b>100</b> may be arranged apart from each other in a circumferential direction of the subject P or the examination bed B, with respect to the subject P or the examination bed B. In an embodiment, the plurality of image capturing units <b>100</b> may include a first image capturing unit <b>110</b>, a second image capturing unit <b>120</b>, a third image capturing unit <b>130</b>, and a fourth image capturing unit <b>140</b>. Here, the first image capturing unit <b>110</b> to the fourth image capturing unit <b>140</b> may be respectively arranged at different positions from each other, that is, adjacent to one of an upper end (e.g., the head of the subject P, a right side, a left side, and a lower end (e.g., the tiptoe of the subject P of the examination bed B. In this case, the first image capturing unit <b>110</b> to the fourth image capturing unit <b>140</b> may respectively obtain thermal images captured in various directions and at various angles by capturing images of the subject P at different positions and angles. By capturing thermal images at various positions by using the plurality of image capturing units <b>100</b>, noise of the thermal images may be removed and the reliability of thermal imaging results may be improved.</p><p id="p-0085" num="0084">As another embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an image capturing unit <b>100</b>-<b>2</b> may capture a thermal image of the subject P while linearly moving in a longitudinal direction (for example, L direction in <figref idref="DRAWINGS">FIG. <b>3</b></figref>) of the examination bed B on which the subject P is located. Here, the image capturing unit <b>100</b>-<b>2</b> may have a moving hole <b>101</b>-<b>2</b> which is arranged therein and through which the examination bed B passes. The image capturing unit <b>100</b>-<b>2</b> may have a disk shape. However, the present disclosure is not limited thereto, and the image capturing unit <b>100</b>-<b>2</b> may have various shapes such as a square plate, a polygonal plate, and the like.</p><p id="p-0086" num="0085">The image capturing unit <b>100</b>-<b>2</b> may include a camera <b>110</b>-<b>2</b>. The camera <b>110</b>-<b>2</b> may be arranged on an inner surface of the image capturing unit <b>100</b>-<b>2</b>. The camera <b>110</b>-<b>2</b> is rotatable about a connection shaft connected to the inner surface <b>102</b>-<b>2</b> of the image capturing unit <b>100</b>-<b>2</b>, and a tilting angle of the camera <b>110</b>-<b>2</b> may be adjusted. In this case, a position of the camera <b>110</b>-<b>2</b> may be changed while the camera <b>110</b>-<b>2</b> is rotated based on movement of the subject P detected by a motion sensor unit.</p><p id="p-0087" num="0086">As an embodiment, the image capturing unit <b>100</b>-<b>2</b> may include a plurality of cameras. The number of the plurality of cameras is not limited, but for convenience of description, description will focus on an embodiment in which the image capturing unit <b>100</b>-<b>2</b> includes three cameras (that is, a first camera <b>110</b>-<b>2</b>, a second camera <b>120</b>-<b>2</b>, and a third camera <b>130</b>-<b>2</b>).</p><p id="p-0088" num="0087">The first camera <b>110</b>-<b>2</b>, the second camera <b>120</b>-<b>2</b>, and the third camera <b>130</b>-<b>2</b> may be spaced apart from each other along a circumferential direction of the image capturing unit <b>100</b>-<b>2</b>, and arranged on the inner surface <b>102</b>-<b>2</b> of the image capturing unit <b>100</b>-<b>2</b>. For example, the first camera <b>110</b>-<b>2</b> may be arranged on the inner surface <b>102</b>-<b>2</b> of the image capturing unit <b>100</b>-<b>2</b>, in parallel to an arbitrary line that is parallel to the longitudinal direction of the examination bed B and passes through a center of the examination bed B, and the second camera <b>120</b>-<b>2</b> and the third camera <b>130</b>-<b>2</b> may be arranged symmetrically with respect to the first camera <b>110</b>-<b>2</b>. In this case, the subject P on the examination bed B moving through the moving hole <b>101</b>-<b>2</b> of the image capturing unit <b>100</b>-<b>2</b> may be photographed at different angles.</p><p id="p-0089" num="0088">As described above, the first camera <b>110</b>-<b>2</b>, the second camera <b>120</b>-<b>2</b> and the third camera <b>130</b>-<b>2</b> may rotate about the connection shaft connected to the image capturing unit <b>100</b>-<b>2</b>. Here, since each of the first camera <b>110</b>-<b>2</b> to the third camera <b>130</b>-<b>2</b> is rotated independently of each other, a rotation direction and a tilt angle of each camera may be different from each other. For example, the first camera <b>110</b>-<b>2</b> is rotatable in a direction R<b>1</b><i>a </i>or a direction R<b>1</b><i>b</i>, the second camera <b>120</b>-<b>2</b> is rotatable in a direction R<b>2</b><i>a </i>or a direction R<b>2</b><i>b</i>, and the third camera <b>130</b>-<b>2</b> is rotatable in a direction R<b>3</b><i>a </i>or a direction R<b>3</b><i>b</i>. Accordingly, by measuring thermal images of the subject P from various angles, and synthesizing these thermal images to evaluate the respiratory status of the subject P, the accuracy of examination may be improved.</p><p id="p-0090" num="0089">The motion sensor unit <b>200</b> may generate motion information by detecting a motion of the subject P. The motion information may be information including a movement path and movement position of at least one of a body part of the subject P and the entire body of the subject P. In an embodiment, when the subject P moves a body part such as a face, arm, or leg, the motion sensor unit <b>200</b> may detect a motion of the body part, and track the motion to detect a movement path and a movement position of the body part. As another embodiment, the motion sensor unit <b>200</b> may detect a motion of each body part of the subject P, and detect a movement path and a movement position of each body part by tracking the motion, or may detect a movement path and a movement position of the whole body of the subject P based on the detected movement paths and movement positions of the respective body parts. The motion sensor unit <b>200</b> may generate a motion signal showing a movement of the subject P, such as the movement path and the movement position detected as described above.</p><p id="p-0091" num="0090">A plurality of motion sensor units <b>200</b> may be provided. In this case, the plurality of motion sensor units <b>200</b> may be arranged apart from each other. The motion sensor units <b>200</b> may be arranged apart from each other along the circumferential direction of the subject P or the examination bed B, with respect to the subject P or the examination bed B. In an embodiment, the plurality of motion sensor units <b>200</b> may include a first motion sensor unit <b>210</b>, a second motion sensor unit <b>220</b>, a third motion sensor unit <b>230</b>, and a fourth motion sensor unit <b>240</b>. Here, the first motion sensor unit <b>210</b> to the fourth motion sensor unit <b>240</b> may be respectively arranged at different positions from each other, that is, adjacent to one of the upper end (e.g., the head of the subject P, the right side, the left side, and the lower end (e.g., the tiptoe of the subject P of the examination bed B. In this case, each of the first motion sensor unit <b>210</b> to the fourth motion sensor unit <b>240</b> may generate motion information by detecting a motion of the subject P at different positions and angles, thereby precisely determining the motion of the subject P and improving the reliability of the generated motion information.</p><p id="p-0092" num="0091">The motion sensor unit <b>200</b> may transmit the generated motion information to the respiratory status examining unit <b>320</b> or the learning unit <b>330</b>.</p><p id="p-0093" num="0092">The respiratory status monitoring apparatus <b>10</b> according to an embodiment of the present disclosure may include one or more processors <b>300</b>. The respiratory status monitoring apparatus <b>10</b> may be driven in a form included in a hardware device, such as a microprocessor or a general-purpose computer system. Here, the &#x2018;processor&#x2019; may refer to, for example, a data processing device embedded in hardware and having a physically structured circuit to perform a function expressed as code or a command included in a program. Examples of the data processing device embedded in hardware as described above may include a microprocessor, a central processing unit (CPU), a processor core, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA), but the present disclosure is not limited thereto. The processor <b>300</b> may include the temperature information extracting unit <b>310</b> and the respiratory status examining unit <b>320</b>. In addition, the processor <b>300</b> may further include the learning unit <b>330</b> and the position adjuster <b>340</b>.</p><p id="p-0094" num="0093">The temperature information extracting unit <b>310</b> may receive a thermal image captured by the image capturing unit <b>100</b> and specify an examination region A based on the received thermal image. The examination region A may be a portion or a region of the body in which a change in body temperature of the subject P may be checked in order to determine the respiratory status of the subject P.</p><p id="p-0095" num="0094">The temperature information extracting unit <b>310</b> may specify at least one examination region A. As an embodiment, the temperature information extracting unit <b>310</b> may specify one examination region A. In this case, the one examination region A may be specified to include an optimal position for determining the respiratory status of the subject P. For example, the examination region A may be specified based on the positions of the nose and mouth of the subject P; in this case, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the temperature information extracting unit <b>310</b> may set an imaginary circle having, as a diameter, a straight line connecting from the nose to the jaw of the subject P, and specify an inner region of the imaginary circle as the examination region A. As another embodiment, a plurality of examination regions A may be specified. For example, the plurality of examination regions A may include a first examination region A<b>1</b>, a second examination region A<b>2</b>, and a third examination region A<b>3</b>. The first examination region A<b>1</b> may be specified to include the nose and mouth of the subject P, and in this case, the method of specifying the first examination region A<b>1</b> may be the same as described above. The second examination region A<b>2</b> may be specified based on the positions of the chest and abdomen of the subject P. For example, the second examination region A<b>2</b> may be specified as a region extending from just below the clavicle of the subject P, through the chest and abdomen, to the top of the pelvis. The third examination region A<b>3</b> may be specified based on the positions of the arms and legs of the subject P. However, the present disclosure is not limited thereto, and the number and positions of the examination regions A may be changed according to parts to be examined, which require temperature information extraction.</p><p id="p-0096" num="0095">The temperature information extracting unit <b>310</b> may extract temperature information from the specified examination region A. The temperature information may include a body temperature or an amount of change in the body temperature in the examination region A, extracted from the thermal image. When the subject P breathes, the body temperature of the subject P in the examination region A may change. For example, when the subject P breathes in (inhalation), the temperature of the nose, mouth, and the skin surface in the vicinity thereof of the subject P may drop, and when the subject P exhales (exhalation), the temperature of the nose, mouth, and the skin surface in the vicinity thereof of the subject P may rise. As another example, when there is a motion of the subject P, the body temperature of a body part of the subject P may rise.</p><p id="p-0097" num="0096">The temperature information may further include information about the amount of change in carbon dioxide and water vapor in the examination region A in each case of inhalation and exhalation of the subject P. Near-infrared rays have a wavelength of 0.78 &#x3bc;m to 3 &#x3bc;m, and can penetrate to a depth of several millimeters from the skin surface of the subject P, and atmospheric components that absorb infrared rays may vary depending on wavelength bands in the atmosphere. For example, at around 4.3 microns, infrared rays are absorbed by carbon dioxide, and at around 6.5 microns, infrared rays may be absorbed by water vapor, whereby near infrared rays may be selectively transmitted. Due to this selective transmittance of near-infrared rays, the relative amounts of carbon dioxide and water vapor in the examination region A may be significantly vary depending on the wavelength of near-infrared rays during inhalation and exhalation of the subject P. For example, during exhalation of the subject P, the amount of carbon dioxide and water vapor in the examination region A may increase in a certain wavelength band compared to inhalation. The amount of change in carbon dioxide and water vapor in the examination region A according to the wavelength of the near-infrared rays may be analyzed and used to detect the respiratory status of the subject P.</p><p id="p-0098" num="0097">In an embodiment in which a plurality of examination regions A are specified, the temperature information extracting unit <b>310</b> may extract temperature information from each of the plurality of examination regions A. For example, the temperature information extracting unit <b>310</b> may extract a temperature and/or a temperature change amount of the nose, mouth, and the surrounding areas thereof in the first examination region A<b>1</b>, a temperature and/or a temperature change amount of the chest, abdomen, and the surrounding areas thereof in the second examination region A<b>2</b>, and a temperature and/or a temperature change amount of the arms and legs and the surrounding areas thereof in the third examination region A<b>3</b>, respectively. In addition, the temperature information extracting unit <b>310</b> may segment the body of the subject P by increasing the number of specified examination regions A, thereby making it possible to determine a temperature change in each body part and thus allowing to selectively detect temperature information of only certain body parts that needs examination.</p><p id="p-0099" num="0098">The temperature information extracting unit <b>310</b> may extract temperature information from the examination region A and transmit the same to the respiratory status examining unit <b>320</b> or the learning unit <b>330</b>.</p><p id="p-0100" num="0099">The respiratory status examining unit <b>320</b> may determine the respiratory status of the subject P based on temperature information and motion information. In this case, the respiratory status examining unit <b>320</b> may measure a body temperature and motion of the subject P in real time, and monitor the respiration volume, respiratory status, and sleep state or the like of the subject P in real time.</p><p id="p-0101" num="0100">In an embodiment, the learning unit <b>330</b> may learn respiratory status determination criteria of the subject P by machine learning. The learning unit <b>330</b> may learn, by machine-learning, the respiratory status determination criteria on the basis of at least one of temperature information received from the temperature information extracting unit <b>310</b> and motion information received from the motion sensor unit <b>200</b>. In another embodiment, the learning unit <b>330</b> may learn, by machine-learning, posture determination criteria of the subject P.</p><p id="p-0102" num="0101">Here, the learning unit <b>330</b> may learn, by machine learning, posture determination criteria on the basis of the motion information received from the motion sensor unit <b>200</b>. For example, the learning unit <b>330</b> may learn the respiratory status determination criteria or the posture determination criteria by using a machine learning or deep-learning method.</p><p id="p-0103" num="0102">The position adjuster <b>400</b> may adjust a position of the image capturing unit <b>100</b> according to a posture of the subject P. The position adjuster <b>400</b> may determine the posture of the subject P based on the posture determination criteria learned by the learning unit <b>330</b>. Here, the position adjuster <b>400</b> may determine the posture of the subject P by applying information such as the movement path and movement position of the subject P or of body parts of the subject P, measured by the motion sensor unit <b>200</b>, to the posture determination criteria.</p><p id="p-0104" num="0103">After determining the posture of the subject P, the position adjuster <b>400</b> may adjust the position or a photographing angle of the image capturing unit <b>100</b> according to the determined posture.</p><p id="p-0105" num="0104">As an embodiment, the position adjuster <b>400</b> may adjust a tilting angle of the image capturing unit <b>100</b> or rotate the image capturing unit <b>100</b> according to the determined posture. As another embodiment, the position adjuster <b>400</b> may adjust the position of the image capturing unit <b>100</b> by moving the image capturing unit <b>100</b> up, down, left and right around the subject P or the examination bed B according to the determined posture. As another embodiment, the position adjuster <b>340</b> may adjust the tilting angle and the position of the image capturing unit <b>100</b> differently depending on whether the determined posture is a supine, lateral or prone position. In this case, the image capturing unit <b>100</b> may capture an image of the subject P at a position adjusted by the position adjuster <b>400</b>, and the respiratory status examining unit <b>320</b> may determine the respiratory status of the subject P based on the captured image, and thus, even if the posture of the subject P is changed during monitoring, the examination may be continuously performed with uniform accuracy.</p><p id="p-0106" num="0105">When the processor <b>300</b> includes the learning unit <b>330</b>, the respiratory status examining unit <b>320</b> may determine the respiratory status of the subject P based on the respiratory status determination criteria. In this case, the respiratory status examining unit <b>320</b> may monitor the respiration volume, the respiratory status, and sleep state of the subject P in real time by measuring a change in body temperature and motion of the subject P in real time and applying the learned respiratory status determination criteria.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a respiratory status monitoring method according to an embodiment of the present disclosure in order.</p><p id="p-0108" num="0107">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the respiratory status monitoring method according to an embodiment of the present disclosure is as described below, and hereinafter, description will focus on an embodiment in which the processor <b>300</b> includes the learning unit <b>330</b> and the position adjuster <b>340</b>.</p><p id="p-0109" num="0108">In operation S<b>10</b>, the image capturing unit <b>100</b> may capture a thermal image of the subject P. In this case, the image capturing unit <b>100</b> may obtain a thermal image of the subject P by using a near-infrared camera or an infrared camera. Here, the image capturing unit <b>100</b> may include a plurality of near-infrared cameras or infrared cameras, and the plurality of near-infrared or infrared cameras may be spaced apart from each other and arranged at different positions to obtain thermal images in various directions and from various angles.</p><p id="p-0110" num="0109">In operation S<b>20</b>, the motion sensor unit <b>200</b> may generate motion information by detecting a motion of the subject P. The motion sensor unit <b>200</b> may detect a motion of the subject P or a motion of a certain body part of the subject P, and track the motion and generate motion information based on a movement path and movement position of the subject P or the certain body part of the subject P.</p><p id="p-0111" num="0110">In operation S<b>30</b>, the temperature information extracting unit <b>310</b> may specify the examination region A from the thermal image captured by the image capturing unit <b>100</b>, based on the positions of the nose and mouth of the subject P. Next, the temperature information extracting unit <b>310</b> may extract temperature information from the specified examination region A. As an embodiment, the temperature information extracting unit <b>310</b> may specify an additional examination region and extract temperature information from the additional examination. The additional examination region may be specified based on at least one of the positions of the chest and abdomen and the positions of arms and legs of the subject P.</p><p id="p-0112" num="0111">In operation S<b>40</b>, the learning unit <b>330</b> may learn, by machine learning, the respiratory status determination criteria on the basis of the temperature information extracted from the temperature information extracting unit <b>310</b> and the motion information generated by the motion sensor unit <b>200</b>. As another embodiment, the learning unit <b>330</b> may learn, by machine learning, the posture determination criteria of the subject P on the basis of the motion information generated by the motion sensor unit <b>200</b>.</p><p id="p-0113" num="0112">In operation S<b>50</b>, the respiratory status examining unit <b>320</b> may detect the respiratory status of the subject P based on the temperature information extracted from the temperature information extracting unit <b>310</b> and the motion information generated by the motion sensor unit <b>200</b>. Here, the respiratory status examining unit <b>320</b> may determine the respiration volume, the respiratory status, and the lifespan state of the subject P based on the learned respiratory status determination criteria.</p><p id="p-0114" num="0113">The position adjuster <b>400</b> may adjust a position of the image capturing unit <b>100</b> according to a posture of the subject P. A method, by the position adjuster <b>400</b>, of adjusting the position of the image capturing unit <b>100</b> may be as follows.</p><p id="p-0115" num="0114">The position adjuster <b>400</b> may first determine the posture of the subject P based on the posture determination criteria learned by the learning unit <b>330</b>. Here, the position adjuster <b>400</b> may determine the posture of the subject P by applying, to the posture determination criteria, information such as the movement path and movement position of the subject P or of the body part of the subject P, measured by the motion sensor unit <b>200</b>.</p><p id="p-0116" num="0115">Next, the position adjuster <b>400</b> may adjust a position of the image capturing unit <b>100</b> according to the determined posture of the subject P. In an embodiment, the position adjuster <b>400</b> may adjust the tilting angle of the image capturing unit <b>100</b> according to the determined posture, or adjust the image capturing unit <b>100</b> by rotating the image capturing unit <b>100</b>. In another embodiment, the position adjuster <b>400</b> may adjust the position of the image capturing unit <b>100</b> by moving the image capturing unit <b>100</b> up, down, left and right with respect to the subject P or the examination bed B, according to the determined posture.</p><p id="p-0117" num="0116">Next, the respiratory status monitoring apparatus may capture a thermal image of the subject P at the adjusted position of the image capturing unit <b>100</b>, and perform again the examination operation described above.</p><p id="p-0118" num="0117">As described above, according to the respiratory status monitoring apparatus and method according to the embodiments of the present disclosure, a decrease in the accuracy of examination due to obstruction factors may be prevented by taking a thermal image by using a near-infrared or infrared camera, and the discomfort of the subject may be reduced through a non-contact type examination method.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram schematically illustrating a sleep disorder treatment system <b>20</b> according to an embodiment of the present disclosure.</p><p id="p-0120" num="0119">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the sleep disorder treatment system <b>20</b> according to an embodiment of the present disclosure includes a sleep disorder control device <b>100</b>&#x2032;, a sleep disorder treatment device <b>200</b>&#x2032;, a user terminal <b>300</b>&#x2032;, and a network <b>400</b>&#x2032;.</p><p id="p-0121" num="0120">The sleep disorder treatment system <b>20</b> according to an embodiment of the present disclosure may detect a bio-signal of a user while the user is wearing the sleep disorder treatment device <b>200</b>&#x2032; and sleeping, and move the mandible or adjust positive pressure according to a sleep state of the user, determined using the detected bio-signal, thereby alleviating sleep disorders such as snoring or apnea in a customized manner. Here, the sleep disorder treatment system <b>20</b> may obtain not only the bio-signal but also user sleep satisfaction level data, and train a machine learning model based on bio-signal data and sleep satisfaction level data, to thereby minimize arousal of the user during sleep and thus improve sleep quality.</p><p id="p-0122" num="0121">The sleep disorder control device <b>100</b>&#x2032; may include a server implemented using a computer device that communicates with the sleep disorder treatment device <b>200</b>&#x2032; and the user terminal <b>300</b>&#x2032; to provide commands, code, files, contents, services, etc., or a plurality of the computer devices. However, the present disclosure is not limited thereto, and the sleep disorder control device <b>100</b>&#x2032; may be integrally formed with the sleep disorder treatment device <b>200</b>&#x2032;.</p><p id="p-0123" num="0122">For example, the sleep disorder control device <b>100</b>&#x2032; may provide a file for installing an application to the user terminal <b>300</b>&#x2032; accessed through the network <b>400</b>&#x2032;. In this case, the user terminal <b>300</b>&#x2032; may install an application by using the file provided from the sleep disorder control device <b>100</b>&#x2032;. In addition, according to the control by an operating system (OS) and at least one program (e.g., a browser or an installed application) included in the user terminal <b>300</b>&#x2032;, the sleep disorder control device <b>100</b>&#x2032; may be accessed to receive services or contents provided by the sleep disorder control device <b>100</b>&#x2032;. As another example, the sleep disorder control device <b>100</b>&#x2032; may establish a communication session for data transmission or reception, and route data transmission or reception between the user terminals <b>30</b>&#x2032; through the established communication session.</p><p id="p-0124" num="0123">The sleep disorder control device <b>100</b>&#x2032; may include a processor, obtain user sleep satisfaction level data and bio-signal data and train a machine learning model based on deep learning, and perform a function of controlling the sleep order treatment device <b>200</b>&#x2032; by using the machine learning model. However, the present disclosure is not limited thereto, and after training the machine learning model through the sleep disorder control device <b>100</b>&#x2032;, the machine learning model may be provided to the sleep disorder treatment device <b>200</b>&#x2032; for the sleep disorder treatment device <b>200</b>&#x2032; to determine a degree of mandibular advancement or the number of advances. Hereinafter, for convenience of description, an embodiment in which learning and control are performed in the server <b>100</b>&#x2032; will be mainly described.</p><p id="p-0125" num="0124">The sleep disorder treatment device <b>200</b>&#x2032; refers to a treatment unit that a user can wear for treatment of a sleep disorder during sleep. The sleep disorder treatment device <b>200</b>&#x2032; may be, for example, a mandibular advancement device for advancing the mandible, or a positive pressure device for controlling air pressure. Also, although not described, the sleep disorder treatment device <b>200</b>&#x2032; may be applied to any treatment unit that the user may wear while sleeping. Hereinafter, for convenience of description, description will focus on a case in which the sleep disorder treatment device <b>200</b>&#x2032; is a mandibular advancement device.</p><p id="p-0126" num="0125">The sleep disorder treatment device <b>200</b>&#x2032; may include an upper teeth seating portion and a lower teeth seating portion that are arranged in the oral cavity, a driving unit advancing or withdrawing the lower teeth seating portion, relative to the upper teeth seating portion, and a sensing unit for sensing a bio-signal of the user, to move the lower jaw of the user based on a sleep state while the user is wearing the sleep disorder treatment device <b>200</b>&#x2032;. In addition, the sleep disorder treatment device <b>200</b>&#x2032; may include a communicator that transmits bio-signal data sensed through the sensor unit, to the user terminal <b>300</b>&#x2032; or the sleep disorder control device <b>100</b>&#x2032;.</p><p id="p-0127" num="0126">The upper teeth seating portion may be a portion on which the user's upper teeth are seated. The upper teeth seating portion may be formed in a shape into which the user's upper teeth may be inserted. The upper tooth seating portion may be customized according to the user's teeth in order to minimize the foreign body sensation or discomfort when the upper teeth are seated thereon. When the upper teeth seating portion is worn on the upper teeth, the upper teeth seating portion may wrap and be closely adhered to the upper teeth.</p><p id="p-0128" num="0127">The lower teeth seating portion may be a portion on which the user's lower teeth are seated. The lower tooth seating portion may be customized according to the user's teeth in order to minimize the foreign body sensation or discomfort when the lower teeth are seated thereon. When the lower teeth seating portion is worn on the lower teeth, the lower teeth seating portion may wrap and be closely adhered to the lower teeth.</p><p id="p-0129" num="0128">The driving unit may be connected to the upper teeth seating portion and the lower teeth seating portion to change a relative position of the lower teeth seating portion with respect to the upper teeth seating portion. The driving unit may include a power unit providing a driving force and a power transmission unit transmitting the driving force generated by the power unit, to the upper teeth seat portion or the lower teeth seat portion.</p><p id="p-0130" num="0129">The sensing unit may detect biometric information of the user. The sensing unit may include various sensors that detect biometric information for determining whether the user is sleeping, a posture, or a sleep state, such as snoring, or sleep apnea. For example, the sensing unit may include at least one of a respiration sensor, an oxygen saturation sensor, and a posture sensor.</p><p id="p-0131" num="0130">The respiration sensor may be an acoustic sensor capable of detecting a snoring sound, or an airflow sensor detecting respiration of the user, inhaled or exhausted through the nose or mouth. The oxygen saturation sensor may be a sensor for detecting oxygen saturation. Here, the respiration sensor and the oxygen saturation sensor may obtain a bio-signal for determining a sleep state such as snoring or sleep apnea of the user.</p><p id="p-0132" num="0131">The posture sensor may be a sensor that detects a bio-signal for determining a sleeping posture of the user. The posture sensor may consist of a single component, but may also include different types of sensors arranged at different positions to obtain biometric information. For example, the posture sensor may include a three-axis sensor. The three-axis sensor may be a sensor that detects changes in a yaw axis, a pitch axis, and a roll axis. The three-axis sensor may include at least one of a gyro sensor, an acceleration sensor, and a tilt sensor. In addition, the present disclosure is not limited thereto, and a sensor for detecting changes in axes of a number different from three may also be applied.</p><p id="p-0133" num="0132">The communicator may include a communication unit capable of communicating with the sleep disorder control device <b>100</b>&#x2032; or the user terminal <b>300</b>&#x2032;, for example, Bluetooth, ZigBee, Medical Implant Communication Service (MISC), or Near Field Communication (NFC). The communicator may transmit bio-signal data sensed through the sensing unit to the user terminal <b>300</b>&#x2032; or the sleep disorder control device <b>100</b>&#x2032;.</p><p id="p-0134" num="0133">The user terminal <b>300</b>&#x2032; may be a stationary terminal implemented as a computer device or a mobile terminal. The user terminal <b>300</b>&#x2032; may be a terminal of an administrator who controls the sleep disorder control device <b>100</b>&#x2032;. Alternatively, the user terminal <b>300</b>&#x2032; may be an obtaining unit for obtaining sleep satisfaction level data of the user through an interface. The user terminal <b>300</b>&#x2032; may display questionnaire information for obtaining a level of sleep satisfaction provided by the sleep disorder control device <b>100</b>&#x2032;, and generate sleep satisfaction level data by using the questionnaire information selected by the user. The user terminal <b>300</b>&#x2032; may include, for example, a smart phone, a mobile phone, a navigation system, a computer, a laptop computer, a digital broadcasting terminal, a personal digital assistant (PDA), a portable multimedia player (PMP), a tablet PC, and the like. As an example, the user terminal <b>300</b>&#x2032; may communicate with another user terminal <b>300</b>&#x2032;, the sleep disorder treatment device <b>200</b>&#x2032;, or the sleep disorder control device <b>100</b>&#x2032; through the network <b>400</b>&#x2032; by using a wireless or wired communication method.</p><p id="p-0135" num="0134">The communication method is not limited, and not only a communication method using a communication network that the network <b>400</b>&#x2032; may include (e.g., a mobile communication network, a wired Internet, a wireless Internet, a broadcasting network), but also short-range wireless communication between devices may be included as the communication method. For example, the network <b>400</b>&#x2032; may include one or more of a personal area network (PAN), a local area network (LAN), a controller area network (CAN), a metropolitan area network (MAN), a wide area network (WAN), a metropolitan area network (MAN), a wide area network (WAN), a broadband network (BBN), and the Internet. Further, the network <b>400</b>&#x2032; may include one or more of network topologies including a bus network, a star network, a ring network, a mesh network, a star-bus network, a tree or a hierarchical network, and the like, but is not limited thereto.</p><p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram schematically illustrating the sleep disorder control device <b>100</b>&#x2032; according to an embodiment of the present disclosure, and <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref> are diagrams for explaining a process of obtaining and learning sleep satisfaction level data.</p><p id="p-0137" num="0136">Referring to <figref idref="DRAWINGS">FIGS. <b>9</b> to <b>11</b></figref>, the sleep disorder control device <b>100</b>&#x2032; according to an embodiment of the present disclosure may include a communicator <b>110</b>&#x2032;, a processor <b>120</b>&#x2032;, a memory <b>130</b>&#x2032;, and an input/output interface <b>140</b>&#x2032;.</p><p id="p-0138" num="0137">The communicator <b>110</b>&#x2032; may receive bio-signal data and usage record data from the sleep disorder treatment device <b>200</b>&#x2032;, or may receive sleep satisfaction level data from the user terminal <b>300</b>&#x2032;. The communicator <b>110</b>&#x2032; may receive bio-signal data S<b>1</b>&#x2032; and usage record data S<b>2</b>&#x2032; during a sleep period ST of the user wearing the sleep disorder treatment device <b>200</b>&#x2032;. The communicator <b>110</b>&#x2032; may receive sleep satisfaction level data S<b>3</b>&#x2032; during an awake period WT after the user completes sleep.</p><p id="p-0139" num="0138">The processor <b>120</b>&#x2032; may be configured to process a command of a computer program by performing basic arithmetic, logic, and input/output operations. The command may be provided to the processor <b>120</b>&#x2032; by the memory <b>130</b>&#x2032; or the communicator <b>110</b>&#x2032;. For example, the processor <b>120</b>&#x2032; may be configured to execute the received command according to program code stored in a recording device, such as the memory <b>130</b>&#x2032;. Here, the &#x2018;processor&#x2019; may refer to, for example, a data processing device embedded in hardware and having a physically structured circuit to perform a function expressed as code or a command included in a program.</p><p id="p-0140" num="0139">Examples of the data processing device embedded in hardware as described above may include a microprocessor, a central processing unit (CPU), a processor core, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA), but the present disclosure is not limited thereto. The processor <b>120</b>&#x2032; may include a data obtaining unit <b>121</b>&#x2032;, a learning unit <b>122</b>&#x2032;, an operation controller <b>123</b>&#x2032;, and a notification signal generator <b>124</b>&#x2032;.</p><p id="p-0141" num="0140">The data obtaining unit <b>121</b>&#x2032; may obtain the sleep satisfaction level data S<b>3</b>&#x2032; and the bio-signal data S<b>1</b>&#x2032; of the user wearing the sleep disorder treatment device <b>200</b>&#x2032; and the usage record data S<b>2</b>&#x2032; of the sleep disorder treatment device <b>200</b>&#x2032;. The data obtaining unit <b>121</b>&#x2032; may include a biometric data obtaining unit <b>1211</b>&#x2032;, a usage record obtaining unit <b>1212</b>&#x2032;, and a sleep satisfaction level obtaining unit <b>1213</b>&#x2032;.</p><p id="p-0142" num="0141">The biometric data obtaining unit <b>1211</b>&#x2032; may obtain the bio-signal data S<b>1</b>&#x2032; by using one or more sensors during sleep of the user wearing the sleep disorder treatment device <b>200</b>&#x2032;. Here, the bio-signal data S<b>1</b>&#x2032; may be data generated by the sensing unit of the sleep disorder treatment device <b>200</b>&#x2032;. For example, the bio-signal data S<b>1</b>&#x2032; may include information about a respiration volume, snoring sound information, and posture information detected through a respiration sensor, an oxygen saturation sensor, and a posture sensor. The biometric data obtaining unit <b>1211</b>&#x2032; may receive bio-signal data sensed in real time during the user's sleep period ST. However, the present disclosure is not limited thereto, and the biometric data obtaining unit <b>1211</b>&#x2032; may receive the bio-signal data S<b>1</b>&#x2032; when a sleep apnea event occurs, or receive the bio-signal data S<b>1</b>&#x2032; according to a preset cycle.</p><p id="p-0143" num="0142">The usage record obtaining unit <b>1212</b>&#x2032; may obtain the usage record data S<b>2</b>&#x2032; of the sleep disorder treatment device <b>200</b>&#x2032; during a sleep of the user wearing the sleep disorder treatment device <b>200</b>&#x2032;. Here, the usage record data S<b>2</b>&#x2032; may be a history of driving the sleep disorder treatment device <b>200</b>&#x2032; by using the bio-signal data S<b>1</b>&#x2032;, for example, a time at which the mandible was advanced overnight, a total period of time that the mandible was advanced, the number of advances, the degree of advances, and the like.</p><p id="p-0144" num="0143">The usage record obtaining unit <b>1212</b>&#x2032; may obtain the usage record data S<b>2</b>&#x2032; from the sleep disorder treatment device <b>200</b>&#x2032;, but the present disclosure is not limited thereto, and the usage record data S<b>2</b>&#x2032; may also be obtained through a control signal generated by the operation controller <b>123</b>&#x2032; to be described later.</p><p id="p-0145" num="0144">The sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may obtain the sleep satisfaction level data S<b>3</b>&#x2032; after the user who is wearing the sleep disorder treatment device <b>200</b>&#x2032; completes sleep. Here, the sleep satisfaction level data S<b>3</b>&#x2032; may be obtained by providing questionnaire information including a sleep satisfaction-related questionnaire through the interface of the user terminal <b>300</b>&#x2032; and by using the user's response information with respect to the questionnaire information. The sleep satisfaction level data S<b>3</b>&#x2032; may be data obtained by quantifying sleep satisfaction level by using the user's response information. The sleep satisfaction-related questionnaire may be a questionnaire about whether sleep was satisfactory, or the user has a morning headache, emotional changes and depression, concentration, and a dry throat. The sleep satisfaction level data S<b>3</b>&#x2032; may include not only response information on the sleep satisfaction level, but also personal information of the user. The sleep satisfaction level data S<b>3</b>&#x2032; may further include personal information such as the age, gender, height, and weight of the user.</p><p id="p-0146" num="0145">The sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may obtain one or more pieces of sleep satisfaction level data S<b>3</b>&#x2032;. In detail, the sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may obtain first sleep satisfaction level data S<b>31</b>&#x2032; at least at a first time point t<b>1</b> when the user completes sleep. That is, the sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may obtain data about the user's sleep satisfaction level immediately after sleep. Also, the sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may obtain second sleep satisfaction level data S<b>32</b>&#x2032; at a second time point t<b>2</b> different from the first time point t<b>1</b>. The second time point t<b>2</b> may be after a preset period of time from the first time point t<b>1</b> and before a next sleep of the user, and the user may enter state information about daytime sleepiness, concentration, work efficiency, etc. through the user terminal <b>300</b>&#x2032;.</p><p id="p-0147" num="0146">In another embodiment, the sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may obtain sleep satisfaction level data not only at the time point t<b>1</b> immediately after waking up and at the time point t<b>2</b> just before falling asleep, but also at other preset time points. For example, the sleep satisfaction level obtaining unit <b>1213</b>&#x2032; may additionally obtain sleep satisfaction level data after eating lunch.</p><p id="p-0148" num="0147">The learning unit <b>122</b>&#x2032; may train a machine learning model MM based on the obtained sleep satisfaction level data S<b>3</b>&#x2032;, the obtained bio-signal data S<b>1</b>&#x2032;, and the obtained usage record data S<b>2</b>&#x2032;. The machine learning model MM may be an algorithm for learning control criteria for controlling operation of the sleep disorder treatment device based on the sleep satisfaction level data S<b>3</b>&#x2032;, the bio-signal data S<b>1</b>&#x2032;, and the usage record data S<b>2</b>&#x2032;.</p><p id="p-0149" num="0148">The sleep disorder treatment device <b>200</b>&#x2032; may detect a sleep disorder such as sleep apnea, through the bio-signal data S<b>1</b>&#x2032;, and perform a function of treating the disorder by advancing the mandible, and the operation of advancing the mandible may cause inevitable arousal and decrease sleep quality. The sleep disorder treatment system <b>20</b> according to an embodiment of the present disclosure may perform a function of improving sleep quality by not only advancing the mandible simply based on a bio-signal but also by minimizing the number of advances of the mandible in consideration of sleep satisfaction level.</p><p id="p-0150" num="0149">To this end, the learning unit <b>122</b>&#x2032; may learn the control criteria for controlling the operation of the sleep disorder treatment device <b>200</b>&#x2032; by using the sleep satisfaction level data S<b>3</b>&#x2032;, the bio-signal data S<b>1</b>&#x2032;, and the usage record data S<b>2</b>&#x2032;. In detail, the learning unit <b>122</b>&#x2032; may learn control criteria for controlling the degree of advance or the number of advances of the sleep disorder treatment device <b>200</b>&#x2032;.</p><p id="p-0151" num="0150">In addition, the learning unit <b>122</b>&#x2032; may learn the control criteria regarding in which of cases where the mandible is to be selectively advanced, by using the bio-signal data S<b>1</b>&#x2032; of the sleep disorder treatment device <b>200</b>&#x2032;. When it is determined that the user has fallen into light sleep, the learning unit <b>122</b>&#x2032; may train, by using the bio-signal data S<b>1</b>&#x2032;, the machine learning model such that the mandible is not advanced.</p><p id="p-0152" num="0151">The learning unit <b>122</b>&#x2032; trains a machine learning model based on deep learning or artificial intelligence, and deep learning is defined by a machine learning algorithm that tries high-level abstractions (summarizing key contents or functions from large amounts of data or complex data) through a combination of non-linear transformation methods. The learning unit <b>122</b>&#x2032; may use, among deep learning models, for example, one of a deep neural network (DNN), a convolutional neural network (CNN), a recurrent neural network (RNN), and a deep belief neural network (DBN).</p><p id="p-0153" num="0152">The operation controller <b>123</b>&#x2032; may control the operation of the sleep disorder treatment device <b>200</b>&#x2032; by using the bio-signal data S<b>1</b>&#x2032;, the usage record data S<b>2</b>&#x2032;, the sleep satisfaction level data S<b>3</b>&#x2032;, and the machine learning model MM while the user is wearing the sleep disorder treatment device <b>200</b>&#x2032;. The operation controller <b>123</b>&#x2032; may control the number of advances or the degree of advances of the mandibular advancement device by applying new bio-signal data S<b>1</b>&#x2032;, the usage record data S<b>2</b>&#x2032;, and the sleep satisfaction level data S<b>3</b>&#x2032; to the trained machine learning model MM.</p><p id="p-0154" num="0153">The notification signal generator <b>124</b>&#x2032; may provide a first notification signal b<b>1</b> to the user before the first time point t<b>1</b> and provide a second notification signal b<b>2</b> to the user before the second time point t<b>2</b>. When the notification signal generator <b>124</b>&#x2032; provides the first notification signal b<b>1</b> and the second notification signal b<b>2</b> to the user terminal <b>300</b>&#x2032;, the user terminal <b>300</b>&#x2032; may notify, through sound, vibration, a screen or light, the user that it is time to respond to sleep satisfaction level. The notification signal generator <b>124</b>&#x2032; may generate the first notification signal b<b>1</b> within a preset period of time from immediately after the user wakes up, and the second notification signal b<b>2</b> at a preset time point before the average time that user falls asleep.</p><p id="p-0155" num="0154">The memory <b>130</b>&#x2032; is a computer-readable recording medium and may include a random access memory (RAM), a read only memory (ROM), and a permanent mass storage device such as a disk drive. In addition, the memory <b>130</b>&#x2032; may store an operating system and at least one program code (e.g., code for a browser installed and driven in a user terminal or the application described above). These software components may be loaded from a computer-readable recording medium that is readable by an additional computer, separate from the memory <b>130</b>&#x2032;, by using a drive mechanism. The computer-readable recording medium readable by an additional computer may include a computer-readable recording medium such as a floppy drive, a disk, a tape, a DVD/CD-ROM drive, and a memory card. In another embodiment, the software components may be loaded into the memory <b>130</b>&#x2032; through the communicator&#x2032; instead of a computer-readable recording medium. For example, at least one program may be loaded to the memory <b>130</b>&#x2032; based on a program (e.g., the application described above) installed by files provided by, through a network, a file distribution system (e.g., the server described above) for distributing installation files of developers or applications.</p><p id="p-0156" num="0155">The input/output interface <b>140</b>&#x2032; may be used for interfacing with an input/output device. For example, an input device may include a device such as a keyboard or mouse, and an output device may include a device such as a display for displaying a communication session of an application. As another example, the input/output interface <b>140</b>&#x2033; may be used for interfacing with a device in which functions for inputting and outputting are integrated into one, such as a touch screen.</p><p id="p-0157" num="0156"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart sequentially illustrating a sleep disorder control method according to an embodiment of the present disclosure.</p><p id="p-0158" num="0157">Referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the server <b>100</b>&#x2032; may obtain, by using a data obtaining unit, sleep satisfaction level data and bio-signal data of a user who wears the sleep disorder treatment device <b>200</b>&#x2032;, and usage record data of the sleep disorder treatment device <b>200</b>&#x2032; in operation S<b>510</b>&#x2032;.</p><p id="p-0159" num="0158">Next, the server <b>100</b>&#x2032; may train a machine learning model based on the sleep satisfaction level data, the bio-signal data, and the usage record data, by using a learning unit in operation S<b>520</b>&#x2032;.</p><p id="p-0160" num="0159">Next, the server <b>100</b>&#x2032; may control, by using an operation controller, the operation of the sleep disorder treatment device <b>200</b>&#x2032; while the user is wearing the same, by using the sleep satisfaction level data, the bio-signal data, the usage record data, and the machine learning model. The sleep disorder control device <b>100</b>&#x2032; may control whether or not the sleep disorder treatment device <b>200</b>&#x2032; is advanced, an advanced distance, an advancing speed, an advancing force, or the number of advances to thereby minimize unnecessary arousal of the user so as to improve sleep quality.</p><p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart for explaining a control method of a mandibular advancement system.</p><p id="p-0162" num="0161">Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in operation S<b>610</b>&#x2032;, the sleep disorder treatment device <b>200</b>&#x2032; generates bio-signal data during a user's sleep by using a sensing unit. The sleep disorder treatment device <b>200</b>&#x2032; may transmit the bio-signal data detected in real time to the sleep disorder control device <b>100</b>&#x2032;, or transmit bio-signal data detected when a sleep disorder event has occurred, or transmit bio-signal data sensed at each preset cycle in operation S<b>611</b>&#x2032;.</p><p id="p-0163" num="0162">In operation S<b>620</b>&#x2032;, the sleep disorder control device <b>100</b>&#x2032; generates a first notification signal after the user wakes up after completing sleep. The sleep disorder control device <b>100</b>&#x2032; may generate a first notification signal at a preset time point, or may detect a user's waking up by using bio-signal data and generate a first notification signal. The sleep disorder control device <b>100</b>&#x2032; transmits the generated first notification signal to the user terminal <b>300</b>&#x2032; in operation S<b>621</b>&#x2032;.</p><p id="p-0164" num="0163">In operation S<b>630</b>&#x2032;, the user terminal <b>300</b>&#x2032; provides questionnaire information including a sleep satisfaction level-related questionnaire through an interface, and generates first sleep satisfaction level data at a first time point by using response information according to the user's selection. The first sleep satisfaction level data may further include personal information of the user. The user terminal <b>300</b>&#x2032; transmits the first sleep satisfaction level data to the sleep disorder control device <b>100</b>&#x2032; in operation S<b>631</b>&#x2032;.</p><p id="p-0165" num="0164">However, the present disclosure is not limited thereto, and as another embodiment, when the machine learning model trained by the sleep disorder control device <b>100</b>&#x2032; is provided to the sleep disorder treatment device <b>200</b>&#x2032;, the sleep disorder control device <b>100</b>&#x2032; may transmit the first sleep satisfaction level data to the sleep disorder treatment device <b>200</b>&#x2032;. In other words, the sleep disorder control device <b>100</b>&#x2032; may train the machine learning model by using previous first sleep satisfaction level data as learning data, and the sleep disorder treatment device <b>200</b>&#x2032; may control the operation of the sleep disorder treatment device <b>200</b>&#x2032; by using the trained machine learning model and also by additionally using the new first sleep satisfaction level data.</p><p id="p-0166" num="0165">In operation S<b>640</b>&#x2032;, the sleep disorder control device <b>100</b>&#x2032; generates a second notification signal at a time point different from that at which the first notification signal is generated. The second notification signal may be generated before the user spends the day and goes to sleep. The sleep disorder control device <b>100</b>&#x2032; may generate the second notification signal before the average time when the user sleeps, or may generate the second notification signal at a preset time point. The sleep disorder control device <b>100</b>&#x2032; transmits the second notification signal to the user terminal <b>300</b>&#x2032; in operation S<b>631</b>&#x2032;.</p><p id="p-0167" num="0166">In operation S<b>650</b>&#x2032;, the user terminal <b>300</b>&#x2032; provides questionnaire information including a sleep satisfaction level-related questionnaire through an interface, and generates second sleep satisfaction level data at a second time point different from the first time point, by using response information according to the user's selection. The second time point may be after a preset period of time from the first time point and before a next sleep of the user, and the user may enter state information about daytime sleepiness, concentration, work efficiency, etc. through the user terminal <b>300</b>&#x2032;. The user terminal <b>300</b>&#x2032; transmits the second sleep satisfaction level data to the sleep disorder control device <b>100</b>&#x2032; in operation S<b>651</b>&#x2032;.</p><p id="p-0168" num="0167">In operation S<b>660</b>&#x2032;, the sleep disorder control device <b>100</b>&#x2032; may train the machine learning model based on the sleep satisfaction level data, the bio-signal data, and the usage record data. The machine learning model may be an algorithm for learning control criteria for controlling the operation of the sleep disorder treatment device <b>200</b>&#x2032; based on the sleep satisfaction level data, the bio-signal data, and the usage record data.</p><p id="p-0169" num="0168">In operation S<b>670</b>&#x2032;, the sleep disorder control device <b>100</b>&#x2032; generates an operation control signal for controlling the operation of the sleep disorder treatment device <b>200</b>&#x2032; by applying the sleep satisfaction level data, the bio-signal data, and the usage record data to the trained machine learning model. The server <b>100</b>&#x2032; may transmit the generated operation control signal to the sleep disorder treatment device <b>200</b>&#x2032; in operation S<b>661</b>&#x2032; to control the sleep disorder treatment device <b>200</b>&#x2032;.</p><p id="p-0170" num="0169">As described above, according to the sleep disorder control device and method according to the embodiments of the present disclosure, a sleep disorder is detected using biometric information, and when treating a detected sleep disorder by advancing the mandible, sleep quality may be improved by minimizing arousal due to the movement of the mandible by also considering the sleep satisfaction level of the user. According to the sleep disorder control device and method of the embodiments of the present disclosure, the learning efficiency may be improved by using, as learning data, not only sleep satisfaction level data obtained immediately after sleep but also sleep satisfaction level data obtained before going to sleep after spending a day in daily life.</p><p id="p-0171" num="0170"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram schematically illustrating a polysomnography device <b>100</b>&#x2033; according to an embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>15</b></figref> is a conceptual diagram for explaining a process of obtaining polysomnography data from a plurality of examination units.</p><p id="p-0172" num="0171">Referring to <figref idref="DRAWINGS">FIGS. <b>14</b> and <b>15</b></figref>, the polysomnography device <b>100</b>&#x2033; according to an embodiment of the present disclosure may obtain polysomnography data from external examination units <b>1</b>&#x2033; to <b>7</b>&#x2033;, and generate learning data by using the polysomnography data, and then may effectively train a sleep state reading model based on the generated learning data.</p><p id="p-0173" num="0172">A network environment according to the present disclosure may include a plurality of user terminals, a server, and a network. The polysomnography device <b>100</b>&#x2033; may be a server or a user terminal.</p><p id="p-0174" num="0173">The plurality of user terminals may be stationary terminals implemented by a computer device or mobile terminals. When the polysomnography device <b>100</b>&#x2033; is a server, the plurality of user terminals may be terminals of an administrator who controls the server. For example, the plurality of user terminals may include smart phones, smart watches, mobile phones, navigation devices, computers, laptop computers, digital broadcasting terminals, Personal Digital Assistants (PDA), Portable Multimedia Players (PMD), tablet PCs, etc. For example, the user terminals may communicate with other user terminals and/or a server through a network by using a wireless or wired communication method.</p><p id="p-0175" num="0174">The communication method is not limited, and not only a communication method using a communication network that the network may include (e.g., a mobile communication network, a wired Internet, a wireless Internet, a broadcasting network), but also short-range wireless communication between devices may be included as the communication method. For example, the network may include one or more of a personal area network (PAN), a local area network (LAN), a controller area network (CAN), a metropolitan area network (MAN), a wide area network (WAN), a metropolitan area network (MAN), a wide area network (WAN), a broadband network (BBN), and the Internet. Further, the network may include any one or more of network topologies including a bus network, a star network, a ring network, a mesh network, a star-bus network, a tree or a hierarchical network, and the like, but is not limited thereto.</p><p id="p-0176" num="0175">The server may be implemented using a computer device that communicates with a plurality of user terminals through a network to provide commands, codes, files, contents, services, and the like, or a plurality of the computer devices.</p><p id="p-0177" num="0176">For example, the server may provide a file for installing an application to a user terminal accessed through a network. In this case, the user terminal may install the application by using a file provided from the server. In addition, according to the control by an operating system (OS) and at least one program (e.g., a browser or an installed application) included in the user terminal, the user terminal may access the server to receive services or contents provided by the server. As another example, the server may establish a communication session for data transmission or reception, and route data transmission or reception between the plurality of user terminals through the established communication session.</p><p id="p-0178" num="0177">Meanwhile, the polysomnography device <b>100</b>&#x2033; may include a receiver <b>110</b>&#x2033;, a processor <b>120</b>&#x2033;, a memory <b>130</b>&#x2033;, and an input/output interface <b>140</b>&#x2033;.</p><p id="p-0179" num="0178">The receiver <b>110</b>&#x2033; may receive polysomnography data from the external examination units <b>1</b>&#x2033; to <b>7</b>&#x2033;. As an embodiment, the receiver <b>110</b>&#x2033; of the polysomnography device <b>100</b>&#x2033; may be connected to the external examination units <b>1</b>&#x2033; to <b>7</b>&#x2033; by wires as illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref> and obtain polysomnography data measured in time series. In another embodiment, the receiver <b>110</b>&#x2033; may function as a communication module using wireless communication and receive polysomnography data.</p><p id="p-0180" num="0179">The polysomnography data may be a plurality of pieces of biometric data of a user, measured using a plurality of examination units. The plurality of pieces of biometric data may include biometric data obtained using at least one of sensing units among an Electroencephalogram (EEG) sensor, an Electrooculography (EOG) sensor, an Electromyogramme (EMG) sensor, an Electrokardiogramme (EKG) sensor, a Photoplethysmography (PPG) sensor, a chest belt, an abdomen belt, oxygen saturation, end-tidal CO2 (EtCO2), a respiration detection thermistor, a flow sensor, a pressure sensor (manometer), a microphone, and a positive pressure gauge of a continuous positive pressure device.</p><p id="p-0181" num="0180">In detail, the plurality of pieces of biometric data may include at least one of biometric data related to brain waves from the EEG sensor <b>1</b>&#x2033;, biometric data related to eye movement from the EOG sensor <b>2</b>&#x2033;, biometric data related to muscle movement from the EMG sensor <b>3</b>&#x2033;, biometric data related to a heart rate from an EKG sensor (not shown), biometric data related to oxygen saturation and a heart rate from the PPG sensor <b>4</b>&#x2033;, biometric data related to movement of the abdomen and the chest from the chest motion detection belt <b>5</b>&#x2033; and the abdominal motion detection belt <b>6</b>&#x2033;, biometric data related to respiration, from EtCO2, the respiration detection thermistor, and the flow sensor <b>7</b>&#x2033;, and biometric data related to snoring, from a microphone (not shown). In addition, the plurality of pieces of biometric data may include positive pressure level data obtained using the positive pressure gauge of a continuous positive pressure device.</p><p id="p-0182" num="0181">The processor <b>120</b>&#x2033; may be configured to process a command of a computer program by performing basic arithmetic, logic, and input/output operations. The command may be provided to the processor <b>120</b>&#x2033; by the memory <b>130</b>&#x2033; or the receiver <b>110</b>&#x2033;. For example, the processor <b>120</b>&#x2033; may be configured to execute received commands according to program code stored in a recording device, such as the memory <b>130</b>&#x2033;. Here, the &#x2018;processor&#x2019; may refer to, for example, a data processing device embedded in hardware and having a physically structured circuit to perform a function expressed as code or a command included in a program.</p><p id="p-0183" num="0182">Examples of the data processing device embedded in hardware as described above may include a microprocessor, a central processing unit (CPU), a processor core, a multiprocessor, an application-specific integrated circuit (ASIC), and a field programmable gate array (FPGA), but the present disclosure is not limited thereto. The processor <b>120</b>&#x2033; may include a graph image generator <b>121</b>&#x2033;, a learning unit <b>123</b>&#x2033;, and a reader <b>124</b>&#x2033;, and may further include a split image generator <b>122</b>&#x2033;.</p><p id="p-0184" num="0183">The memory <b>130</b>&#x2033; is a computer-readable recording medium and may include a random access memory (RAM), a read only memory (ROM), and a permanent mass storage device such as a disk drive. In addition, the memory <b>130</b>&#x2033; may store an operating system and at least one program code (e.g., code for a browser installed and driven in a user terminal or the application described above). These software components may be loaded from a computer-readable recording medium that is readable by an additional computer, separate from the memory <b>130</b>&#x2033; by using a drive mechanism. The computer-readable recording medium readable by an additional computer may include a computer-readable recording medium such as a floppy drive, a disk, a tape, a DVD/CD-ROM drive, and a memory card. In another embodiment, the software components may be loaded into the memory <b>130</b>&#x2033; through the receiver <b>110</b>&#x2033; instead of a computer-readable recording medium. For example, at least one program may be loaded to the memory <b>130</b>&#x2033; based on a program (e.g., the application described above) installed by files provided by, through a network, a file distribution system (e.g., the server described above) for distributing installation files of developers or applications.</p><p id="p-0185" num="0184">The input/output interface <b>140</b>&#x2033; may be used for interfacing with an input/output device. For example, an input device may include a device such as a keyboard or mouse, and an output device may include a device such as a display for displaying a communication session of an application. As another example, the input/output interface <b>140</b>&#x2033; may be used for interfacing with a device in which functions for inputting and outputting are integrated into one, such as a touch screen.</p><p id="p-0186" num="0185">Hereinafter, the polysomnography device <b>100</b>&#x2033; will be described in detail with further reference to <figref idref="DRAWINGS">FIGS. <b>16</b> and <b>17</b></figref>.</p><p id="p-0187" num="0186"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram illustrating a graph image that is learning data of the polysomnography device <b>100</b>&#x2033; according to an embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>17</b></figref> is a diagram illustrating a labeled graph image.</p><p id="p-0188" num="0187">Referring back to <figref idref="DRAWINGS">FIG. <b>14</b></figref> and <figref idref="DRAWINGS">FIGS. <b>16</b> and <b>17</b></figref>, the polysomnography device <b>100</b>&#x2033; includes the graph image generator <b>121</b>&#x2033;, the learning unit <b>123</b>&#x2033;, and the reader <b>124</b>&#x2033;, and may further include the split image generator <b>122</b>&#x2033;. Here, the polysomnography device <b>100</b>&#x2033; may include a single processor including the components described above, but may include the above-described components by using two or more processors. For example, the learning unit <b>123</b>&#x2033; of the polysomnography device <b>100</b>&#x2033; may be included in a processor of a server, and the reader <b>124</b>&#x2033; may be included in a processor of a user terminal. That is, the polysomnography device <b>100</b>&#x2033; may transmit biometric data of a user to the server in which the learning unit <b>123</b>&#x2033; is arranged, and train a sleep state reading model, and may transmit the trained sleep state reading model to the reader <b>124</b>&#x2033; of the user terminal to perform a function of reading a newly measured sleep state of the user.</p><p id="p-0189" num="0188">The graph image generator <b>121</b>&#x2033; may obtain polysomnography raw data measured in time series, and convert the raw data into a graph with respect to time, to generate a graph image M. As an embodiment, the graph image generator <b>121</b>&#x2033; may convert each of a plurality of pieces of biometric data into individual graphs with respect to time, and sequentially arrange the converted, plurality of individual graphs on a time axis (e.g., x-axis) and generate the graph image M. In other words, the plurality of detection units <b>1</b>&#x2033; to <b>7</b>&#x2033; may obtain biometric data in time series, and a data value of the biometric data may change over time. The graph image generator <b>121</b>&#x2033; may convert each piece of biometric data into a graph represented by a change in the data value over time, and output each graph as a single image. Here, the graph image generator <b>121</b>&#x2033; may generate a graph image by matching times of a plurality of pieces of biometric data. The plurality of pieces of biometric data converted into individual graphs may be sequentially arranged on a time axis. Types of each piece of biometric data may be displayed on a y-axis intersecting with the time axis (x-axis) of the graph image M, but the present disclosure is not limited thereto. In addition, the graph image generator <b>121</b>&#x2033; may obtain a plurality of pieces of biometric data as raw data and convert the same into a certain format, and then generate a graph image. The graph image generator <b>121</b>&#x2033; may generate a graph image in a certain format regardless of the type of detection unit, the combination of detection units, and the configuration by component manufacturing companies. Thereafter, the learning unit <b>123</b>&#x2033; may train a standardized sleep state reading model by using the graph image of the certain format as learning data.</p><p id="p-0190" num="0189">Also, the graph image M may include labeled data. As illustrated, as a labeling method, a labeling method using bounding boxes as illustrated, a labeling method using scribbles, a labeling method using points, an image-level labeling method, etc. may be used. A label L<b>1</b> may be information indicating a sleep state that is read and displayed in advance by a professional examination personnel. The sleep state may include at least one of sleep stages such as W (wake stage), N1 (sleep stage 1), N2 (sleep stage 2), N3 (sleep stage 3), R (REM sleep stage), a sleep apnea state, a snoring state, an oxygen saturation-reduced state.</p><p id="p-0191" num="0190">The split image generator <b>122</b>&#x2033; may generate a plurality of images M1, M2, . . . , Mn (see <figref idref="DRAWINGS">FIG. <b>16</b></figref>) by splitting the graph image M in units of a preset time. In the present disclosure, the graph image M may be used as learning data, but the graph image M may also be split into the images M1, M2, . . . Mn described above and used as learning data. The images M1, M2, . . . Mn may be a set of pieces of biometric data commonly required to interpret a certain stage or certain state of sleep. In this case, the preset time unit may be a unit displayed as one screen on a display device during polysomnography; for example, a graph image may be split in units of 30 seconds. In this case, since the images M1, M2, . . . Mn are biometric data measured time-serially overnight, they may have serial characteristics.</p><p id="p-0192" num="0191">As another embodiment, the images obtained by splitting the graph image M may be generated by extracting a graph area of each detection unit from the graph image M. That is, the polysomnography device <b>100</b>&#x2033; may use, as learning data, one graph image M in which a plurality of pieces of biometric data are displayed, but may also generate a graph image for each piece of biometric data and use the same as learning data.</p><p id="p-0193" num="0192">As another embodiment, the graph image M may be a captured image of a screen displayed on an external display device. That is, the polysomnography device <b>100</b>&#x2033; may not separately obtain biometric data, but may be linked to a display device and capture a graph displayed on the screen for each preset time unit and generate a graph image. In this case, the polysomnography device <b>100</b>&#x2033; may further include a pre-processing unit (not shown). The pre-processing unit (not shown) may convert formats for a scale (size, resolution), contrast, brightness, color balance, and hue/saturation of a graph image in order to maintain the consistency of captured images.</p><p id="p-0194" num="0193">The learning unit <b>123</b>&#x2033; may train a sleep state reading model based on the graph image M described above. When the graph image M includes a plurality of images obtained by splitting the graph image M, the learning unit <b>123</b>&#x2033; may train the sleep state reading model based on the plurality of images.</p><p id="p-0195" num="0194">The sleep state reading model may be a learning model for reading at least one of sleep apnea syndrome, periodic limb movement disorder, narcolepsy, sleep stages, and total sleep time. The learning unit <b>123</b>&#x2033; trains a sleep state reading model based on deep learning or artificial intelligence, and deep learning is defined by a machine learning algorithm that tries high-level abstractions (summarizing key contents or functions from large amounts of data or complex data) through a combination of non-linear transformation methods. The learning unit <b>123</b>&#x2033; may use, among deep learning models, for example, one of a deep neural network (DNN), a convolutional neural network (CNN), a recurrent neural network (RNN), and a deep belief neural network (DBN).</p><p id="p-0196" num="0195">As an embodiment, the learning unit <b>123</b>&#x2033; may train a sleep state reading model by using a convolutional neural network (CNN). Here, a convolutional neural network (CNN) is a type of multilayer perceptrons designed to use minimal preprocessing. A convolutional neural network (CNN) includes a convolutional layer that performs convolution on input data, and may further include a subsampling layer that performs subsampling on an image, and thus extract a feature map from the data. Here, the subsampling layer is a layer that increases the contrast between neighboring data and reduces the amount of data to be processed, and max pooling, average pooling, etc. may be used.</p><p id="p-0197" num="0196">Each of the convolutional layers may include an activation function. The activation function may be applied to each layer to perform a function of making each input have a complex non-linear relationship. As the activation function, a sigmoid function, a tanh function, a Rectified Linear Unit (ReLU), a Leacky ReLU, etc., which are capable of converting an input into a normalized output may be used.</p><p id="p-0198" num="0197">The reader <b>124</b>&#x2033; may read a sleep state of a user who is a subject of examination, based on the graph image of the subject and the trained sleep state reading model. The reader <b>124</b>&#x2033; may directly receive a graph image rather than measured source data from the examination units, and apply the graph image to the sleep state learning model to read the user's sleep state. In addition, the reader <b>124</b>&#x2033; may output and provide the read sleep state of the user as a result.</p><p id="p-0199" num="0198">The polysomnography device <b>100</b>&#x2033; may receive feedback on a reading result derived using the sleep state reading model, generate feedback data therefor, and provide the feedback data to the learning unit <b>123</b>&#x2033;. The learning unit <b>123</b>&#x2033; may re-train the sleep state reading model by using the feedback data, thereby deriving a more accurate reading result.</p><p id="p-0200" num="0199"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram sequentially illustrating an examination method of a polysomnography device according to an embodiment of the present disclosure.</p><p id="p-0201" num="0200">Referring to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the polysomnography device <b>100</b>&#x2033; may obtain, by the receiver <b>110</b>&#x2033;, time-serially measured polysomnography data in operation S<b>51</b>&#x2033;.</p><p id="p-0202" num="0201">In operation S<b>52</b>&#x2033;, the polysomnography device <b>100</b>&#x2033; may generate a graph image by converting the polysomnography data into a graph with respect to time by using the graph image generator <b>121</b>&#x2033;. The graph image may be split in units of a preset time unit and converted into images obtained by splitting the graph image.</p><p id="p-0203" num="0202">In operation S<b>53</b>&#x2033;, the polysomnography device <b>100</b>&#x2033; may train the sleep state reading model based on the graph image by using the learning unit <b>123</b>&#x2033;. When the graph image includes the plurality of images obtained by splitting the graph image, the learning unit <b>123</b>&#x2033; may train the sleep state reading model based on the plurality of images.</p><p id="p-0204" num="0203">In operation S<b>54</b>&#x2033;, the polysomnography device <b>100</b>&#x2033; may read, by using the reader <b>124</b>&#x2033;, the sleep state of the user based on the graph image and the sleep state reading model. The graph image here may be an image processed using a plurality of pieces of biometric data obtained from a plurality of examination units. Alternatively, the graph image may be an image obtained by capturing a graph displayed on the screen of the display device for monitoring polysomnography.</p><p id="p-0205" num="0204">In operation S<b>55</b>&#x2033;, the polysomnography device <b>100</b>&#x2033; may receive feedback on the reading result of the reader <b>124</b>&#x2033;, and generate feedback data thereof. Feedback on the reading result may be performed by a professional polysomnography personnel, and the learning unit <b>123</b>&#x2033; may derive an accurate reading result by re-training the sleep state reading model by using the feedback data.</p><p id="p-0206" num="0205">As described above, according to the polysomnography device and method according to the embodiments of the present disclosure, instead of raw data obtained from a plurality of examination units, a graph image generated using the raw data is used as learning data, thus allowing to derive accurate reading results while increasing the learning efficiency based on artificial intelligence or deep learning. According to the polysomnography device and method according to the embodiments of the present disclosure, automation of the examination may be realized through the trained sleep state reading model, thereby shortening the examination time as well as reducing examination deviation according to a reader. In addition, the polysomnography device and method according to the embodiments of the present disclosure may also be used as a convenient and continuous sleep monitoring apparatus because algorithms are used in various daily IT products such as smart watches.</p><p id="p-0207" num="0206">The embodiments according to the present disclosure described above may be implemented in the form of a computer program that can be executed through various components on a computer, and such a computer program may be recorded in a computer-readable medium. The medium may store a computer-executable program. Examples of the medium include magnetic media such as a hard disk, a floppy disk, and a magnetic tape, optical recording media such as CD-ROM and DVD, magneto-optical media such as a floptical disk, and those configured to store program instructions, including ROM, RAM, flash memory, and the like.</p><p id="p-0208" num="0207">The computer program may be specifically designed and configured for the embodiments of the present disclosure or may be well-known and available to one of ordinary skill in the art Examples of the program instructions include not only machine codes generated by using a compiler but also high-level language codes that can be executed on a computer by using an interpreter or the like.</p><p id="p-0209" num="0208">While the present disclosure is described with reference to the embodiments illustrated in the drawings, this is merely exemplary, and those of ordinary skill in the art will understand that various modifications and equivalent other embodiments may be made therefrom. Therefore, the scope of the present disclosure shall be defined by the appended claims.</p><p id="p-0210" num="0209">According to an embodiment of the present disclosure, a respiratory status examination apparatus and method and a sleep disorder control device and method are provided. In addition, embodiments of the present disclosure may be applied to industrially used examination and treatment of sleep disorders.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A polysomnography device comprising:<claim-text>a graph image generator configured to obtain polysomnography raw data measured in time series, and convert the polysomnography data into a graph with respect to time to generate a graph image;</claim-text><claim-text>a learning processor configured to train a sleep state reading model based on the graph image; and</claim-text><claim-text>a reader configured to read a sleep state of a user based on the graph image and the sleep state reading model.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The polysomnography device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a split image generator configured to generate a plurality of images by splitting the graph image in units of a preset time, wherein the learning processor trains the sleep state reading model based on the plurality of images obtained by the splitting of the graph image.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The polysomnography device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the polysomnography data comprises a plurality of pieces of biometric data of a user, which are measured using a plurality of examination units, and<claim-text>wherein the graph image generator is configured to convert each piece of the plurality of biometric data into an individual graph with respect to time, and sequentially arrange the converted, plurality of individual graphs on a time axis to generate the graph image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The polysomnography device of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the plurality of pieces of biometric data comprise biometric data obtained using at least one of sensing units among an Electroencephalogram (EEG) sensor, an Electrooculography (EOG) sensor, an Electromyogram (EMG) sensor, an Electrokardiogramme (EKG) sensor, a Photoplethysmography (PPG) sensor, a chest belt, an abdomen belt, oxygen saturation, end-tidal CO2 (EtCO2), a respiration detection thermistor, a flow sensor, a pressure sensor (manometer), a microphone, or a positive pressure gauge of a continuous positive pressure device.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The polysomnography device of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the graph image generator is configured to generate the graph image by matching times of the plurality of pieces of biometric data.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The polysomnography device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the graph image comprises labeled data.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A computer-implemented examination method of a polysomnography device, the method comprising:<claim-text>obtaining time-serially measured polysomnography data;</claim-text><claim-text>converting the polysomnography data into a graph with respect to time to generate a graph image;</claim-text><claim-text>training a sleep state reading model based on the graph image; and</claim-text><claim-text>reading a sleep state of a user based on the graph image and the sleep state reading model.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The examination method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising generating a plurality of images by splitting the graph image in units of a preset time, wherein the training of the sleep state reading model comprises training the sleep state reading model based on the plurality of images obtained by the splitting.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The examination method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the polysomnography data comprises a plurality of pieces of biometric data of a user, which are measured using a plurality of examination units, and<claim-text>wherein in the generating of the graph image, each of the plurality of pieces of biometric data is converted into an individual graph with respect to time, and the converted, plurality of individual graphs are sequentially arranged on a time axis to generate the graph image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The examination method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the plurality of pieces of biometric data comprise biometric data obtained using at least one of sensing units among an Electroencephalogram (EEG) sensor, an Electrooculography (EOG) sensor, an Electromyogram (EMG) sensor, an Electrokardiogramme (EKG) sensor, a Photoplethysmography (PPG) sensor, a chest belt, an abdomen belt, oxygen saturation, end-tidal CO2 (EtCO2), a respiration detection thermistor, a flow sensor, a pressure sensor (manometer), a microphone, and a positive pressure gauge of a continuous positive pressure device.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The examination method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein in the generating of the graph image, the times of the plurality of biometric data are matched to generate the graph image.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The examination method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein in the generating of the graph image, the graph image is generated including labeled data.</claim-text></claim></claims></us-patent-application>