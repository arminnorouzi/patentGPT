<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005467A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005467</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779528</doc-number><date>20191126</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>086</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>13</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DIALOGUE APPARATUS, METHOD AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YAMADA</last-name><first-name>Kazunori</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>NAKAMURA</last-name><first-name>Takashi</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NIPPON TELEGRAPH AND TELEPHONE CORPORATION</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/046184</doc-number><date>20191126</date></document-id><us-371c12-date><date>20220524</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A dialogue apparatus includes a speech recognition unit (<b>1</b>) configured to perform speech recognition on utterance input to generate a text corresponding to the utterance, a speech waveform corresponding to the utterance, and information regarding a length of sound of the utterance; a language understanding unit (<b>2</b>) configured to grasp contents of the utterance by using the text corresponding to the utterance; a dialogue management unit (<b>3</b>) configured to determine contents of a response corresponding to the utterance by using the content of the utterance; an utterance state extraction unit (<b>4</b>) configured to extract a state of the utterance by using the text corresponding to the utterance, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance; a response state determination unit (<b>5</b>) configured to determine a state of the response according to the state of the utterance; a response sentence generation unit (<b>6</b>) configured to generate a response sentence by using the content of the response; and a speech synthesis unit (<b>7</b>) configured to synthesize speech corresponding to the response sentence with the state of the response taken into account.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="70.10mm" wi="158.75mm" file="US20230005467A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="235.12mm" wi="114.81mm" orientation="landscape" file="US20230005467A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="164.68mm" wi="155.02mm" orientation="landscape" file="US20230005467A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="232.41mm" wi="74.68mm" orientation="landscape" file="US20230005467A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="234.87mm" wi="90.00mm" orientation="landscape" file="US20230005467A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="220.39mm" wi="153.67mm" orientation="landscape" file="US20230005467A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a technology of generating more natural response utterance in speech dialogue by using synthetic speech.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">General speech synthesis in the related art has been performed in accordance with text information input to a speech synthesis unit (see PTL 1, for example).</p><p id="p-0004" num="0003">In general speech dialogue systems in the related art, utterance responses are made by performing speech recognition for utterance of a dialog partner, converting the utterance into a text for language understanding, and generating a response sentence to perform speech synthesis while managing the state of the dialogue (see PTL 2, for example).</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Literature</heading><p id="p-0005" num="0004">PTL 1: JP 01-284898 A</p><p id="p-0006" num="0005">PTL 2: JP 2018-133070 A</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0007" num="0006">However, how utterance is made by a system in a dialogue system depends on a text input to a speech synthesis unit. Whether a person who is a dialogue partner can naturally have a dialogue with a system depends on a text to be generated and output by a response generation unit.</p><p id="p-0008" num="0007">As described above, because the speech to be uttered for response depends only on text information generated in the response generation unit, a gap may occur between the state of uttered speech itself by the actual dialogue partner and the state of the speech of the response utterance even when response is appropriately performed on the text.</p><p id="p-0009" num="0008">An object of the present invention is to provide a dialogue apparatus, a method, and a program for achieving more natural dialogue.</p><heading id="h-0007" level="1">Means for Solving the Problem</heading><p id="p-0010" num="0009">A dialogue apparatus according to one aspect of the invention includes a speech recognition unit configured to perform speech recognition on utterance input and generate a text corresponding to the utterance, a speech waveform corresponding to the utterance, and information regarding a length of sound of the utterance; a language understanding unit configured to grasp a content of the utterance by using the text corresponding to the utterance; a dialogue management unit configured to determine a content of a response corresponding to the utterance by using the content of the utterance; an utterance state extraction unit configured to extract a state of the utterance by using the text corresponding to the utterance, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance; a response state determination unit configured to determine a state of the response according to the state of the utterance; a response sentence generation unit configured to generate a response sentence by using the content of the response; and a speech synthesis unit configured to synthesize speech corresponding to the response sentence with the state of the response taken into account.</p><heading id="h-0008" level="1">Effects of the Invention</heading><p id="p-0011" num="0010">More natural dialogue can be achieved.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram illustrating an example of a functional configuration of a dialogue apparatus.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a processing procedure of a dialogue method.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram for explaining an example of processing of a response state determination unit <b>5</b>.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram for explaining another example of processing of the response state determination unit <b>5</b>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram illustrating a functional configuration example of a computer.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0017" num="0016">Hereinafter, embodiments of the present invention will be described in detail. The same reference numerals are given to components having the same functions in the drawings, and repeated description will be omitted.</p><heading id="h-0011" level="1">First Embodiment</heading><p id="p-0018" num="0017">As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, as an example, a dialogue apparatus includes a speech recognition unit <b>1</b>, a language understanding unit <b>2</b>, a dialogue management unit <b>3</b>, an utterance state extraction unit <b>4</b>, a response state determination unit <b>5</b>, a response sentence generation unit <b>6</b>, and a speech synthesis unit <b>7</b>.</p><p id="p-0019" num="0018">The dialogue method is achieved, for example, by performing processing of steps S<b>1</b> to S<b>7</b> described below and illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> by components of the dialogue apparatus.</p><p id="p-0020" num="0019">The components of the dialogue apparatus will be described below.</p><p id="p-0021" num="0020">Speech Recognition Unit <b>1</b></p><p id="p-0022" num="0021">Utterance is input to the speech recognition unit <b>1</b>.</p><p id="p-0023" num="0022">The speech recognition unit <b>1</b> performs speech recognition on utterance input and generates a text corresponding to the utterance, a speech waveform corresponding to the utterance, and information regarding a length of sound of the utterance (step S<b>1</b>).</p><p id="p-0024" num="0023">The text corresponding to the utterance is sometimes also referred to as &#x201c;uttered sentence&#x201d;.</p><p id="p-0025" num="0024">The generated text corresponding to the utterance is output to the language understanding unit <b>2</b> and the utterance state extraction unit <b>4</b>.</p><p id="p-0026" num="0025">The speech waveform corresponding to the utterance and the information regarding the length of the sound of the utterance are output to the utterance state extraction unit <b>4</b>.</p><p id="p-0027" num="0026">The information regarding the length of the sound of the utterance may be a length of the utterance itself, or a length of each of phonemes constituting the utterance.</p><p id="p-0028" num="0027">An example of utterance input to the speech recognition unit <b>1</b> is &#x201c;What is the weather tomorrow?&#x201d;</p><p id="p-0029" num="0028">Language Understanding Unit <b>2</b></p><p id="p-0030" num="0029">The text corresponding to utterance generated in the speech recognition unit <b>1</b> is input to the language understanding unit <b>2</b>.</p><p id="p-0031" num="0030">The language understanding unit <b>2</b> uses the text corresponding to the utterance to grasp contents of the utterance (step S<b>2</b>). The grasped contents are output to the dialogue management unit <b>3</b>.</p><p id="p-0032" num="0031">The content of the utterance is, for example, information regarding so-called dialogue action. The dialogue action includes at least information regarding an action type and an attribute (see, for example, Reference Literature 1).<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0032">[Reference Literature 1] Hironsan, &#x201c;Dialogue system made using machine learning&#x201d;, [online], [Searched on Nov. 13, 2019], Internet [URL: https://qiita.com/Hironsan/items/6425787ccbee75dfae36] Examples of dialogue types of utterance include a question, a greeting, and an assertion.</li></ul></p><p id="p-0033" num="0033">An example of contents of utterance when utterance input to the speech recognition unit <b>1</b> is &#x201c;What is the weather tomorrow?&#x201d; is (action type=question, time attribute=tomorrow).</p><p id="p-0034" num="0034">Dialogue Management Unit <b>3</b></p><p id="p-0035" num="0035">The contents of the utterance grasped in the language understanding unit <b>2</b> are input to the dialogue management unit <b>3</b>.</p><p id="p-0036" num="0036">The dialogue management unit <b>3</b> uses the contents of the utterance to determine contents of a response corresponding to the utterance (step S<b>3</b>).</p><p id="p-0037" num="0037">The determined contents of the response are output to the response sentence generation unit <b>6</b>.</p><p id="p-0038" num="0038">The contents of the response are, for example, information regarding a dialogue type. Examples of the dialogue type of response are an answer, an answer (a lie), a question, a greeting, an apology, and a confirmation.</p><p id="p-0039" num="0039">The dialogue management unit <b>3</b> determines the contents of the response according to the method described in Reference Literature 1, for example. That is, the dialogue management unit <b>3</b> updates the internal state on the basis of the contents of the utterance input and determines the dialogue type that is the contents of the utterance on the basis of the updated internal state. At that time, the dialogue management unit <b>3</b> may use an external API to determine the contents of the utterance.</p><p id="p-0040" num="0040">An example of the contents of the response when the contents of the utterance are (action type=question, time attribute=tomorrow) is (action type=answer, weather attribute=sunny).</p><p id="p-0041" num="0041">Utterance State Extraction Unit <b>4</b></p><p id="p-0042" num="0042">The text corresponding to the utterance generated in the speech recognition unit <b>1</b>, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance are input to the utterance state extraction unit <b>4</b>.</p><p id="p-0043" num="0043">The utterance state extraction unit <b>4</b> extracts the state of the utterance by using the text corresponding to the utterance, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance (step S<b>4</b>).</p><p id="p-0044" num="0044">The extracted state of the utterance is output to the response state determination unit <b>5</b>.</p><p id="p-0045" num="0045">The state of the utterance is information related to a state of utterance, such as at least an utterance speed or an emotion of a person who made the utterance. The state of utterance may include the utterance tone by the person who made the utterance.</p><p id="p-0046" num="0046">The utterance speed is information regarding a speed of utterance. The utterance speed is, for example, the number of characters or phonemes included per unit time.</p><p id="p-0047" num="0047">Examples of the emotion of the person who made the utterance include normal, pleasure, sadness, anger, calm, excitement, composure, depression, anxiety, humbleness, cheerful, and gloomy. For example, the utterance state extraction unit <b>4</b> determines the emotion of the person who made the utterance by categorizing the emotion to any of normal, pleasure, sadness, anger, calm, excitement, composure, depression, anxiety, humbleness, cheerful, gloomy, and the like. The utterance state extraction unit <b>4</b> may determine the emotion of the person who made the utterance by categorizing the emotion to any of normal, pleasure, sadness, and anger. The utterance state extraction unit <b>4</b> may determine the emotion of the person who made the utterance by categorizing the emotion to any of calm, excitement, composure, depression, anxiety, and humbleness. The utterance state extraction unit <b>4</b> may determine the emotion of the person who made the utterance by categorizing the emotion to any of cheerful or gloomy.</p><p id="p-0048" num="0048">The utterance state extraction unit <b>4</b> can determine the emotion of the person who made the utterance by, for example, the method described in Reference Literature 2. The emotion of the person who made the utterance is determined, for example, on the basis of the text corresponding to the utterance and the speech waveform corresponding to the utterance.<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0049">[Reference Literature 2] Saori Amanuma, Riki Kurematsu, Jun Hakura, and Hamid Fujita, &#x201c;An idea of Criterion for Cluster Analysis Criteria to Estimate Emotion in Speech&#x201d;, Information Processing Society of Japan, 73rd National Convention, 2011 The utterance tone of the person who made the utterance is, for example, formal or casual. Casual here refers to not formal.</li></ul></p><p id="p-0049" num="0050">The utterance state extraction unit <b>4</b> can determine the utterance tone of the person who made the utterance by, for example, the method described in Reference Literature 3. The utterance tone of the person who made the utterance is determined, for example, on the basis of the text corresponding to the utterance and the speech waveform corresponding to the utterance.<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0051">[Reference Literature 3] Akira Baba, Takehiro Sekine, Shinpei Hibiya, Fumiaki Obayashi, Akira Terasawa, Takashi Nishiyama, Ryoji Nakashima, &#x201c;Application of Tone Identification to Humanoid Agents&#x201d;, Information Processing Society of Japan, 66th National Convention, 2004</li></ul></p><p id="p-0050" num="0052">Response State Determination Unit <b>5</b></p><p id="p-0051" num="0053">The state of the utterance extracted in the utterance state extraction unit <b>4</b> is input to the response state determination unit <b>5</b>.</p><p id="p-0052" num="0054">The response state determination unit <b>5</b> determines the state of the response in accordance with the state of the utterance (step S<b>5</b>).</p><p id="p-0053" num="0055">The determined state of the response is output to the speech synthesis unit <b>7</b>.</p><heading id="h-0012" level="1">Example 1 of Processing of Response State Determination Unit <b>5</b></heading><p id="p-0054" num="0056">The response state determination unit <b>5</b> can determine the state of the response on the basis of a predetermined rule, for example, in response to a state of the utterance input. Examples of the predetermined rule are shown in the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0055" num="0057">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the state of the utterance input is, for example, (utterance speed=normal, emotion of person who made utterance=normal, utterance tone of person who made utterance=formal), the state of the response (utterance speed=normal, emotion of response=normal, utterance tone of response=formal) is determined.</p><p id="p-0056" num="0058">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the state of the utterance input is, for example, (utterance speed=normal, emotion of person who made utterance=pleasure, utterance tone of person who made utterance=casual), the state of the response (utterance speed=normal, emotion of response=pleasure, utterance tone of response=casual) is determined. As described above, when the utterance tone of the person who made the utterance is casual, the utterance tone of the response is made casual so that a frank response to a frank question in consultation can be achieved.</p><p id="p-0057" num="0059">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, when the state of the utterance input is, for example, (utterance speed=fast, emotion of person who made utterance=anger, utterance tone of person who made utterance=casual), the state of the response (utterance speed=slow, emotion of response=normal, utterance tone of response=formal) is determined. As described above, when the emotion of the person who made the utterance is anger, the utterance speed of the response is made slow, the emotion of the response is made normal, and the utterance tone of the response is made formal, so that it is possible to calm down the person who made the utterance.</p><p id="p-0058" num="0060">In the conversion table of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, only the state of the response corresponding to each of the three states of utterance is shown. For example, it is assumed that, in a conversion table that the response state determination unit <b>5</b> actually uses, states of response corresponding to all states of utterance are determined.</p><p id="p-0059" num="0061">The response state determination unit <b>5</b> may determine states of the response by using the conversion table for particular states of utterance described in the conversion table and may determine a predetermined state of the response as the state of the response output by the response state determination unit <b>5</b> for other states of utterance.</p><heading id="h-0013" level="1">Example 2 of Processing of Response State Determination Unit <b>5</b></heading><p id="p-0060" num="0062">The response state determination unit <b>5</b> may determine the state of the response by using a nonlinear transformation that uses a neural network or the like.</p><p id="p-0061" num="0063">For example, the number of dimensions of the input layer of the neural network is the sum of the number of types of utterance speed of an utterance, the number of types of emotions of an utterance, and the number of types of utterance tone of an utterance, and the number of dimensions of the output layer of the neural network is the sum of the number of types of utterance speed of a response, the number of types of emotions of a response, and the number of types of the utterance tone of a response. The number of intermediate layers (hidden layers) of the neural network is optional. The number of dimensions of each intermediate layer (hidden layer) is also optional.</p><p id="p-0062" num="0064">For certain utterance input, 1 is input for the relevant type of utterance speed, emotion, and utterance tone, and 0 is input for non-relevant types. For example, for the utterance in which the utterance speed is normal, the emotion is normal, and the utterance tone is formal, 1 is input for an input node in which the utterance speed is normal (as is the case for emotion and utterance tone), and 0 is input for an input node in which the utterance speed is fast or the like.</p><p id="p-0063" num="0065">Parameters of the neural network are adjusted such that the output values output from the neural network due to the input approach the output of the corresponding response, and thereby, a learned model of the pattern of the conversion of the state of the utterance as an input and the state of the response is generated. In the above example, parameters are adjusted such that the output node in which the utterance speed of the response is normal, the emotion of the response is normal, and the utterance tone of the response is formal outputs 1, and the other output nodes output 0.</p><p id="p-0064" num="0066">Utilizing a neural network may allow for a corresponding response to be made in a form similar to an existing pattern even in a case of utterance of an input pattern that is not in current patterns.</p><p id="p-0065" num="0067">Although the above-described manner of utilizing is limited to input of 0 and 1, when the utilization is extended to allow for a continuous value, it may be possible to respond with subtle nuances for subtle utterance in which utterance speed, emotion, and the like are moderate.</p><p id="p-0066" num="0068">Response Sentence Generation Unit <b>6</b></p><p id="p-0067" num="0069">The contents of the response determined in the dialogue management unit <b>3</b> are input to the response sentence generation unit <b>6</b>.</p><p id="p-0068" num="0070">The response sentence generation unit <b>6</b> generates a response sentence by using the contents of the response (step S<b>6</b>).</p><p id="p-0069" num="0071">The generated response sentence is output to the speech synthesis unit <b>7</b>.</p><p id="p-0070" num="0072">When an example of the contents of the response is (action type=answer, weather attribute=sunny), an example of the response sentence is &#x201c;sunny&#x201d;.</p><p id="p-0071" num="0073">Speech Synthesis Unit <b>7</b></p><p id="p-0072" num="0074">The response sentence generated in the response sentence generation unit <b>6</b> and the state of the response determined in the response state determination unit <b>5</b> are input to the speech synthesis unit <b>7</b>.</p><p id="p-0073" num="0075">The speech synthesis unit <b>7</b> synthesizes the speech corresponding to the response sentence with the state of the response taken into account (step S<b>7</b>).</p><p id="p-0074" num="0076">The synthesized speech is output from the dialogue apparatus.</p><p id="p-0075" num="0077">As described above, not only text but also information on the state of the utterance of the partner of the dialogue obtained from the utterance speech of the partner is also input, and speech synthesis is performed also in consideration of the state. This enables more natural dialogue to be achieved.</p><p id="p-0076" num="0078">First Modification</p><p id="p-0077" num="0079">The state of the response determined by the response state determination unit <b>5</b> may include an utterance tone of the response.</p><p id="p-0078" num="0080">In this case, the response sentence generation unit <b>6</b> may generate the response sentence in consideration of the utterance tone of the response included in the state of the response determined by the response state determination unit <b>5</b>.</p><p id="p-0079" num="0081">By generating a response sentence in consideration of the utterance tone of the person who made the utterance, further natural dialogue can be achieved.</p><p id="p-0080" num="0082">For example, when an example of the contents of the response is (action type=answer, weather attribute=sunny) and the utterance tone of the response=formal, the response sentence generation unit <b>6</b> generates a response sentence of &#x201c;The weather is sunny&#x201d;. When an example of the contents of the response is (action type=answer, weather attribute=sunny) and the utterance tone of the response=casual, the response sentence generation unit <b>6</b> generates a response sentence of &#x201c;It's sunny&#x201d;.</p><heading id="h-0014" level="1">Second Modified Example</heading><p id="p-0081" num="0083">The response state determination unit <b>5</b> may determine the state of the response further according to at least one of the text corresponding to the utterance, the contents of the utterance, the contents of the response, or information obtained up to when the dialogue management unit <b>3</b> determines the contents of the response.</p><p id="p-0082" num="0084">The information obtained up to when the dialogue management unit <b>3</b> determines the contents of the response is internal information in the dialogue management unit <b>3</b>, for example.</p><p id="p-0083" num="0085"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a conversion table that is a predetermined rule used when the response state determination unit <b>5</b> determines the state of the response further on the basis of the dialogue type of utterance that is the contents of the utterance and the dialogue type of response that is the contents of the response.</p><p id="p-0084" num="0086">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=normal, emotion of person who made utterance=normal, utterance tone of person who made utterance=formal, dialogue type of utterance=question, dialogue type of response=answer), the state of the response (utterance speed=normal, emotion of response=normal, utterance tone of response=formal) is determined. As a result, it is possible to correspond to a normal inquiry.</p><p id="p-0085" num="0087">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=slow, emotion of person who made utterance=anxiety, utterance tone of person who made utterance=formal, dialogue type of utterance=question, dialogue type of response=answer), the state of the response (utterance speed=normal, emotion of response=calm, utterance tone of response=formal) is determined. As a result, it is possible to correspond to an inquiry with an anxiety and hesitating emotion.</p><p id="p-0086" num="0088">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=slow, emotion of person who made utterance=anxiety, utterance tone of person who made utterance=formal, dialogue type of utterance=question, dialogue type of response=question), the state of the response (utterance speed=slow, emotion of response=humbleness, utterance tone of response=formal) is determined. As a result, it is possible to ask a question while corresponding to an inquiry with an anxiety and hesitating emotion.</p><p id="p-0087" num="0089">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=normal, emotion of person who made utterance=pleasure, utterance tone of person who made utterance=casual, dialogue type of utterance=greeting, dialogue type of response=greeting), the state of the response (utterance speed=normal, emotion of response=pleasure, utterance tone of response=casual) is determined. As a result, it is possible to achieve exchange of greetings.</p><p id="p-0088" num="0090">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=slow, emotion of person who made utterance=depression, utterance tone of person who made utterance=casual, dialogue type of utterance=greeting, dialogue type of response=question), the state of the response (utterance speed=slow, emotion of response=calm, utterance tone of response=formal) is determined. As a result, it is possible to achieve a formal response (for example, &#x201c;Are you all right?&#x201d;) corresponding to utterance with depressed emotion.</p><p id="p-0089" num="0091">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=normal, emotion of person who made utterance=cheerful, utterance tone of person who made utterance=casual, dialogue type of utterance=question, dialogue type of response=answer), the state of the response (utterance speed=normal, emotion of response=cheerful, utterance tone of response=casual) is determined. As a result, it is possible to provide a normal answer to a frank question in consultation.</p><p id="p-0090" num="0092">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=normal, emotion of person who made utterance=cheerful, utterance tone of person who made utterance=casual, dialogue type of utterance=question, dialogue type of response=answer (lie)), the state of the response (utterance speed=normal, emotion of response=sadness, utterance tone of response=casual) is determined. As a result, it is possible to provide an answer that is not really consistent with the question with respect to a frank question in consultation.</p><p id="p-0091" num="0093">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=fast, emotion of person who made utterance=anger, utterance tone of person who made utterance=casual, dialogue type of utterance=assertion, dialogue type of response=apology), the state of the response (utterance speed=slow, emotion of response=depression, utterance tone of response=formal) is determined. As a result, it is possible to achieve correspondence of complaints in a call center and the like.</p><p id="p-0092" num="0094">With the conversion table illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, when the input for the utterance is, for example, (utterance speed=fast, emotion of person who made utterance=excitement, utterance tone of person who made utterance=formal, dialogue type of utterance=question, dialogue type of response=confirmation), the state of the response (utterance speed=normal, emotion of response=composure, utterance tone of response=formal) is determined. As a result, it is possible to perform repetition or the like for an emergency inquiry.</p><heading id="h-0015" level="1">OTHER MODIFICATIONS</heading><p id="p-0093" num="0095">Although the embodiments and modifications of the present invention have been described above, a specific configuration is not limited to the embodiments, the present invention, of course, also includes configurations appropriately changed in the design without departing from the gist of the present invention.</p><p id="p-0094" num="0096">The various kinds of processing described in the embodiments are not only implemented in the described order in a time-series manner but may also be implemented in parallel or separately as necessary or in accordance with a processing capability of the device which performs the processing.</p><p id="p-0095" num="0097">For example, the exchange of data between the components of the dialogue apparatus may be performed directly or via a storage unit not illustrated.</p><p id="p-0096" num="0098">Program and Recording Medium</p><p id="p-0097" num="0099">When various processing functions in the devices described above are implemented by a computer, processing details of the functions that each of the devices should have are described by a program. In addition, when the program is executed by the computer, the various processing functions of each device described above are implemented on the computer. For example, a variety of processing described above can be performed by causing a recording unit <b>2020</b> of the computer illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> to read a program to be executed and causing a control unit <b>2010</b>, an input unit <b>2030</b>, an output unit <b>2040</b>, and the like to execute the program.</p><p id="p-0098" num="0100">The program in which the processing details are described can be recorded on a computer-readable recording medium. The computer-readable recording medium, for example, may be any type of medium such as a magnetic recording device, an optical disc, a magneto-optical recording medium, or a semiconductor memory.</p><p id="p-0099" num="0101">In addition, the program is distributed, for example, by selling, transferring, or lending a portable recording medium such as a DVD or a CD-ROM with the program recorded on it. Further, the program may be stored in a storage device of a server computer and transmitted from the server computer to another computer via a network, so that the program is distributed.</p><p id="p-0100" num="0102">For example, a computer executing the program first temporarily stores the program recorded on the portable recording medium or the program transmitted from the server computer in its own storage device. When executing the processing, the computer reads the program stored in its own storage device and executes the processing in accordance with the read program. Further, as another execution mode of this program, the computer may directly read the program from the portable recording medium and execute processing in accordance with the program, or, further, may sequentially execute the processing in accordance with the received program each time the program is transferred from the server computer to the computer. In addition, another configuration to execute the processing through a so-called application service provider (ASP) service in which processing functions are implemented just by issuing an instruction to execute the program and obtaining results without transmitting the program from the server computer to the computer may be employed. Further, the program in this mode is assumed to include information which is provided for processing of a computer and is equivalent to a program (data or the like that has characteristics of regulating processing of the computer rather than being a direct instruction to the computer).</p><p id="p-0101" num="0103">In addition, although the device is configured by executing a predetermined program on a computer in this mode, at least a part of the processing details may be implemented by hardware.</p><heading id="h-0016" level="1">REFERENCE SIGNS LIST</heading><p id="p-0102" num="0000"><ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0000">    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0104"><b>1</b> Speech recognition unit</li>        <li id="ul0005-0002" num="0105"><b>2</b> Language understanding unit</li>        <li id="ul0005-0003" num="0106"><b>3</b> Dialogue management unit</li>        <li id="ul0005-0004" num="0107"><b>4</b> Utterance state extraction unit</li>        <li id="ul0005-0005" num="0108"><b>5</b> Response state determination unit</li>        <li id="ul0005-0006" num="0109"><b>6</b> Response sentence generation unit</li>        <li id="ul0005-0007" num="0110"><b>7</b> Speech synthesis unit</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A dialogue apparatus comprising a processor configured to execute a method comprising:<claim-text>performing speech recognition on utterance input to generate a text corresponding to the utterance, a speech waveform corresponding to the utterance, and information regarding a length of sound of the utterance;</claim-text><claim-text>understanding a content of the utterance by using the text corresponding to the utterance;</claim-text><claim-text>determining a content of a response corresponding to the utterance by using the content of the utterance;</claim-text><claim-text>extracting a state of the utterance by using the text corresponding to the utterance, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance;</claim-text><claim-text>determining a state of the response according to the state of the utterance;</claim-text><claim-text>generating a response sentence by using the content of the response; and</claim-text><claim-text>synthesizing speech corresponding to the response sentence with the state of the response taken into account.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The dialogue apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the state of the utterance includes at least an utterance speed, and an emotion of a person who makes the utterance.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The dialogue apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the state of the response includes an utterance tone of the response, and</claim-text><claim-text>the generating generates the response sentence in consideration of the utterance tone of the response included in the state of the response.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The dialogue apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. A dialogue method comprising:<claim-text>performing speech recognition on utterance input to generate a text corresponding to the utterance, a speech waveform corresponding to the utterance, and information regarding a length of sound of the utterance;</claim-text><claim-text>grasping a content of the utterance by using the text corresponding to the utterance;</claim-text><claim-text>determining a content of a response corresponding to the utterance by using the content of the utterance;</claim-text><claim-text>extracting a state of the utterance by using the text corresponding to the utterance, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance;</claim-text><claim-text>determining a state of the response according to the state of the utterance;</claim-text><claim-text>generating a response sentence by using the content of the response; and</claim-text><claim-text>synthesizing speech corresponding to the response sentence with the state of the response taken into account.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. A computer-readable non-transitory recording medium storing computer-executable program instructions that when executed by a processor cause for a computer to execute a method comprising:<claim-text>performing speech recognition on utterance input to generate a text corresponding to the utterance, a speech waveform corresponding to the utterance, and information regarding a length of sound of the utterance;</claim-text><claim-text>understanding content of the utterance by using the text corresponding to the utterance;</claim-text><claim-text>determining content of a response corresponding to the utterance by using the content of the utterance;</claim-text><claim-text>extracting a state of the utterance by using the text corresponding to the utterance, the speech waveform corresponding to the utterance, and the information regarding the length of the sound of the utterance;</claim-text><claim-text>determining a state of the response according to the state of the utterance;</claim-text><claim-text>generating a response sentence by using the content of the response; and</claim-text><claim-text>synthesizing speech corresponding to the response sentence with the state of the response taken into account.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The dialogue apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the state of the response includes an utterance tone of the response, and</claim-text><claim-text>the generating generates the response sentence in consideration of the utterance tone of the response included in the state of the response.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The dialogue apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The dialogue apparatus according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The dialogue method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the state of the utterance includes at least an utterance speed, and an emotion of a person who makes the utterance.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The dialogue method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the state of the response includes an utterance tone of the response, and</claim-text><claim-text>the generating generates the response sentence in consideration of the utterance tone of the response included in the state of the response.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The dialogue method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The dialogue method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the state of the response includes an utterance tone of the response, and</claim-text><claim-text>the generating generates the response sentence in consideration of the utterance tone of the response included in the state of the response.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The dialogue method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The dialogue method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the state of the utterance includes at least an utterance speed, and an emotion of a person who makes the utterance.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the state of the response includes an utterance tone of the response, and</claim-text><claim-text>the generating generates the response sentence in consideration of the utterance tone of the response included in the state of the response.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein<claim-text>the state of the response includes an utterance tone of the response, and</claim-text><claim-text>the generating generates the response sentence in consideration of the utterance tone of the response included in the state of the response.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-readable non-transitory recording medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>,<claim-text>the determining the state of the response determines the state of the response further according to at least one of the text corresponding to the utterance, the content of the utterance, the content of the response, or information obtained until the determining the content of the response determines the content of the response.</claim-text></claim-text></claim></claims></us-patent-application>