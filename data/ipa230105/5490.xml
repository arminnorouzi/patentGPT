<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005491A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005491</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17305270</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>038</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>167</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>0216</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>038</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>19</main-group><subgroup>0204</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>165</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">INFORMATION EXCHANGE ON MOBILE DEVICES USING AUDIO</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Capital One Services, LLC</orgname><address><city>McLean</city><state>VA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>FITZGERALD</last-name><first-name>Ian</first-name><address><city>Arlington Heights</city><state>IL</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">In some implementations, a user device may receive input that triggers transmission of information via sound. The user device may select an audio clip based on a setting associated with the device, and may modify a digital representation of the selected audio clip using an encoding algorithm and based on data associated with a user of the device. The user device may transmit, to a remote server, an indication of the selected audio clip, an indication of the encoding algorithm, and the data associated with the user. The user device may use a speaker to play audio, based on the modified digital representation, for recording by other devices. Accordingly, the user device may receive, from the remote server and based on the speaker playing the audio, a confirmation that users associated with the other devices have performed an action based on the data associated with the user of the device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="217.09mm" wi="158.75mm" file="US20230005491A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="183.81mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="177.29mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="192.45mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="177.04mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.27mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="220.64mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="219.71mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="231.56mm" wi="165.27mm" orientation="landscape" file="US20230005491A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="192.28mm" wi="167.81mm" orientation="landscape" file="US20230005491A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="224.62mm" wi="167.81mm" orientation="landscape" file="US20230005491A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="184.83mm" wi="167.81mm" orientation="landscape" file="US20230005491A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Mobile devices, such as a wireless communication device, a user equipment (UE), a mobile phone, a laptop computer, a tablet computer, or a handheld computer, among other examples, sometimes exchange information with each other. These devices may exchange information with each other in order to facilitate communicate between users of the devices, transactions between users of the devices, and other interactions. In order to exchange information, the devices may use a network (such as a WiFi network, the Internet, and/or another network) or may exchange information directly.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">In some implementations, a system for conveying information to one or more other devices using audio includes one or more memories and one or more processors, communicatively coupled to the one or more memories. The one or more processors are configured to receive at least a first portion of information and a second portion of information for transmission to the one or more other devices, wherein the first portion of information includes an identifier associated with a user of the system; select an audio clip based on a preference associated with the user; modify a digital representation of the audio clip to encode the first portion of information and the second portion of information within the digital representation, wherein the digital representation includes a spectrogram associated with the audio clip; transmit, to a remote server, a copy of the modified digital representation, a copy of the first portion of information and the second portion of information, and an indicator of an association between the modified digital representation and the first portion of information and the second portion of information; use at least one speaker associated with the system to play audio, based on the modified digital representation, for recording by the one or more other devices; and receive, from the remote server and based on the at least one speaker playing the audio, a confirmation that users associated with the one or more other devices have performed an action based on the first portion of information and the second portion of information.</p><p id="p-0004" num="0003">In some implementations, a method of conveying information to one or more other devices using audio includes receiving, at a device, input that triggers transmission of information via sound; selecting an audio clip from a plurality of stored audio clips, based on a setting associated with the device; modifying a digital representation of the selected audio clip using at least one encoding algorithm and based on data associated with a user of the device; transmitting, to a remote server, an indication of the selected audio clip, an indication of the at least one encoding algorithm, and the data associated with the user of the device; using at least one speaker associated with the device to play audio, based on the modified digital representation, for recording by the one or more other devices; and receiving, from the remote server and based on the at least one speaker playing the audio, a confirmation that users associated with the one or more other devices have performed an action based on the data associated with the user of the device.</p><p id="p-0005" num="0004">In some implementations, a non-transitory computer-readable medium stores a set of instructions for conveying information to at least one other device using audio. The set of instructions includes one or more instructions that, when executed by one or more processors of a device, cause the device to receive data associated with a user of the device; modify a digital representation of an audio clip to encode the data within the digital representation; transmit, to a remote server, a copy of the modified digital representation, a copy of the data, and an indicator of an association between the modified digital representation and the data; use at least one speaker associated with the device to play audio, based on the modified digital representation, for recording by the at least one other device; and receive, from the remote server and based on the at least one speaker playing the audio, a confirmation that at least one user associated with the at least one other device has performed an action based on the data.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>E</figref> are diagrams of an example implementation relating to exchanging information on mobile devices using audio.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref> are diagrams of example encoding algorithms relating to exchanging information on mobile devices using audio.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of an example environment in which systems and/or methods described herein may be implemented.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of example components of one or more devices of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an example process relating to exchanging information on mobile devices using audio.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">The following detailed description of example implementations refers to the accompanying drawings. The same reference numbers in different drawings may identify the same or similar elements.</p><p id="p-0012" num="0011">Usually, when mobile devices exchange information with each other, the mobile devices use a network (such as a WiFi network, the Internet, and/or another network). For example, one mobile device may transmit the information to an email server, a cloud service, a cellular base station, or another remote device, such that the information is received by another mobile device from the remote device. However, this process incurs significant latency. Accordingly, some mobile devices are configured to exchange information directly without an intervening network. For example, one mobile device may use near-field communication (NFC), Bluetooth&#xae;, or other short-wave communication to transmit information to another mobile device. However, these short-range radio signals are prone to blockages and interference, and generating and receiving short-range radio signals requires more processing resources and power than communications with cellular networks and wireless local area networks (WLANs). In another example, one mobile device may generate a Quick Response (QR) code or other visual representation of the information such that another mobile device may capture the visual representation (e.g., with a camera or other optical sensor) in order to receive the information. However, optical sensors require even more processing resources and power than generating and receiving short-range radio signals.</p><p id="p-0013" num="0012">Some implementations described herein use audio to facilitate exchange of information across mobile devices. For example, a transmitting mobile device may modify a digital representation of an audio clip (e.g., using one or more encoding algorithms described herein) in order to embed indicators of the information within the digital representation. A speaker associated with the transmitting mobile device may play audio based on the modified digital representation such that a microphone associated with a receiving mobile device may record the audio. Accordingly, the receiving mobile device may decode the indicators of the information within the recorded audio. This exchange of audio is less sensitive to blockages and interferences than short-range radio signals, which increases accuracy and decreases latency in transmitting the information. Further, retransmissions are generally reduced, which conserves processing resources and power at both user devices.</p><p id="p-0014" num="0013">In order to further reduce processing resources and power consumed by the receiving mobile device, the transmitting mobile device may indicate the modified digital representation to a remote server. Accordingly, the receiving mobile device may contact the remote server such that the remote server provides the information to the receiving mobile device based on the recorded audio. This further reduces processing resources and power that the receiving mobile device would otherwise use to decode the indicators of the information within the recorded audio. By using the remote server to map the embedded indicators to the information, the transmitting mobile device also does not have to encode the information directly into the digital representation but instead may encode the indicators, which are smaller in size and thus conserve processing resources and power at the transmitting mobile device.</p><p id="p-0015" num="0014">The remote server may dynamically update mappings between different indicators and different information as different transmitting user devices begin and end audio transmissions to receiving user devices. This allows for transmitting user devices to modify the information to be transmitted, and even modify the audio clip to be used, significantly faster than if the remote server were to use a static or semi-static library of mappings. Additionally, a dynamic library of mappings conserves memory and processing resources at the remote server, as well as speeding up how quickly the remote server can provide information to receiving user devices, as compared with a static or semi-static library, which would be larger and slower to search. The dynamic library also further conserves processing resources and power at the receiving mobile device by allowing smaller indicators to be embedded in the digital representation. Because the dynamic library is smaller than a static or semi-static library, the transmitting user device may use smaller indicators that are still unique within the dynamic library.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>E</figref> are diagrams of an example <b>100</b> associated with exchanging information on mobile devices using audio. As shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>E</figref>, example <b>100</b> includes a remote server, a user device, and one or more other devices (e.g., nearby user devices). These devices are described in more detail in connection with <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>.</p><p id="p-0017" num="0016">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> and by reference number <b>105</b>, the remote server may transmit and the user device may receive, an app (e.g., an executable or another file or files) that includes compiled machine code and/or object code that, when executed by the user device, causes the user device to perform processes as described herein. In some implementations, the user device may request the app from an app store (or other repository implemented on the remote server) based on input from a user of the user device. For example, the user may select (e.g., using a touchscreen, a keyboard, or another input device) a visual indicator associated with the app on a screen associated with the app store such that the user device requests the app from the remote server.</p><p id="p-0018" num="0017">As further shown by reference number <b>105</b>, the user device may additionally receive an indication (e.g., at least one indication) of settings (e.g., one or more settings) that are selectable by the user device. Each setting may include a data structure that can be set to a selected value from a plurality of possible values. In one example, the user device may receive an indication of a plurality of audio clips that are selectable by the user device.</p><p id="p-0019" num="0018">As shown by reference number <b>110</b>, the user device may select from the plurality of audio clips. For example, the user device may display to the user visual indicators associated with the plurality of audio clips, and may receive from the user (e.g., using a touchscreen, a keyboard, or another input device) a selection from the plurality of audio clips. Although described herein as displaying visual indicators and receiving tactile input from the user, the user device may additionally or alternatively play audio indicators and/or receive voice input from the user.</p><p id="p-0020" num="0019">Accordingly, the user device may store (e.g., on a memory) an indication of the selected audio clip as a setting associated with the user device. For example, the user device may store (in a relational database, a graphical database, or other linked data structure) the selection in association with an identifier (e.g., a device ID, a nickname, or another identifier) associated with the user device. Additionally, or alternatively, the user device may store (e.g., on a memory) an indication of the selected audio clip as a preference associated with the user. A preference may include an indicator of a selected value from a plurality of possible values for a corresponding setting. Accordingly, in one example, the user device may store (in a relational database, a graphical database, or other linked data structure) the selection in association with an identifier (e.g., a username, a nickname, or another identifier) associated with the user.</p><p id="p-0021" num="0020">Additionally, or alternatively, the user device may receive, from the user, a file (e.g., at least one file) encoding the selected audio clip. For example, the user may select (e.g., using a touchscreen, a keyboard, or another input device) a visual indicator associated with the file, out of a plurality of files stored on a memory of the user device, such that the user device receives the file as the selected audio clip.</p><p id="p-0022" num="0021">Accordingly, the user device may store (e.g., on a memory) an indicator associated with the file as a setting associated with the user device. For example, the user device may store (in a relational database, a graphical database, or other linked data structure) the indicator (e.g., a file name, a hash value, or another indicator) in association with an identifier associated with the user device. Additionally, or alternatively, the user device may store (e.g., on a memory) an indicator associated with the file as a preference associated with the user. Accordingly, in one example, the user device may store (in a relational database, a graphical database, or other linked data structure) the indicator (e.g., a file name, a hash value, or another indicator) in association with an identifier associated with the user.</p><p id="p-0023" num="0022">As shown by reference number <b>115</b>, the user device may authenticate itself with the remote server. In some implementations, the user device may provide a username, a password, a passcode, a key, a certificate, and/or another portion of information used to identify the user device (and/or the user) to the remote server. For example, the user device may retrieve (e.g., from a memory) information used to authenticate the user device, and/or the user may input (e.g., using a touchscreen, a keyboard, or another input device) information used to authenticate the user. Accordingly, the remote server may transmit, and the user device may receive, confirmation after the remote server completes authentication. Otherwise, the user device may prompt the user for new input in response to the remote server transmitting an indication that authentication did not complete.</p><p id="p-0024" num="0023">Additionally, or alternatively, and as further shown by reference number <b>115</b>, the user device may transmit, and the remote server may receive, an indication (e.g., one or more indications) of a setting (e.g., one or more settings) associated with the user device and/or a preference (e.g., one or more preferences) associated with the user. In some implementations, the user device may transmit, and the remote server may receive, an indication of the selected audio clip. For example, the user device may transmit one or more bits indicating a selection from a plurality of audio clips provided by the remote server (e.g., as described above), an indicator associated with a file including the selected audio clip (e.g., as described above), or a copy of the file including the selected audio clip.</p><p id="p-0025" num="0024">As shown by reference number <b>120</b>, the remote server may generate, in a library (e.g., in a relational database, a graphical database, or another linked data structure) a new entry that associates the indication of the selected audio clip with an identifier associated with the user device and/or an identifier associated with the user. Accordingly, the remote server may use the library to determine a size for an eventual identifier to be encoded within the selected audio clip, as described in further detail below. Alternatively, the remote server may refrain from generating the new entry until the user device requests to transmit information via audio (e.g., as described in further detail below), in order to conserve memory, processing resources, and power.</p><p id="p-0026" num="0025">The user device may receive input that triggers transmission of information via sound. For example, the input may include a request to transmit information via audio. In some implementations, the user may select (e.g., using a touchscreen, a keyboard, or another input device) a button or another visual indicator that triggers transmission of information to nearby devices (e.g., at least one other device or one or more other devices) via audio.</p><p id="p-0027" num="0026">In some implementations, the user device may additionally receive data associated with the user of the device. For example, the user device may retrieve (e.g., from a memory) at least some of the data and/or may receive input from the user (e.g., using a touchscreen, a keyboard, or another input device) indicating at least some of the data. In some implementations, the data associated with the user may be based, at least in part, on the input that triggers transmission. For example, the user may input at least some of the data. The data may include, or at least be associated with, information for transmission to the nearby devices, such as a first portion of information that includes an identifier associated with a user of the system and a second portion of information. For example, the user may be requesting that users associated with the nearby devices perform an action based on the first portion of information and the second portion of information. Additionally, or alternatively, the data may include an identifier associated with the user and an amount associated with a request from the user. For example, the user may be requesting a transaction from the users associated with the nearby devices based on the amount.</p><p id="p-0028" num="0027">In response to the input that triggers transmission of information via sound, the user device may select an audio clip based on a setting associated with the device and/or a preference associated with the user. In one example, the user device may select the audio clip from a plurality of audio clips (e.g., provided by the remote server) based on the setting and/or the preference. In another example, the user device may retrieve (e.g., from a memory) a file including the audio clip based on the setting and/or the preference. Alternatively, the user device may select a default audio clip provided by the remote server.</p><p id="p-0029" num="0028">In any of the examples described above, the user device may transmit, and the remote server may receive, a request for the preference and/or the setting. In some implementations, the request may include a credential (e.g., at least one credential) associated with the user. Alternatively, the user device may authenticate itself (e.g., as described above) during or before transmitting the request. Accordingly, the remote server may transmit, and the user device may receive, based on the request, an indication of the preference and/or the setting. Accordingly, the user device may conserve memory by not storing the preference and/or the setting locally. In some implementations, the remote server may further transmit, and the user device may receive, a file including the audio clip to select such that the user device conserves additional memory.</p><p id="p-0030" num="0029">Accordingly, as shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> and by reference number <b>125</b>, the user device may modify a digital representation of the selected audio clip using an encoding algorithm (e.g., at least one encoding algorithm) and based on the data associated with the user. In some implementations, the user device may modify the digital representation to encode the data within the digital representation. As an alternative, the user device may modify the digital representation to encode a plurality of bits that are used to identify the data associated with the user. In some implementations, the user device may generate the plurality of bits based at least in part on a setting associated with the remote server. For example, the remote server may determine a quantity of bits (e.g., a minimum or a minimum plus additional error bits) sufficient to identify the user, out of a plurality of users registered with the remote server, with a threshold level of certainty. Additionally, or alternatively, the quantity of bits may be sufficient to identify the second portion of information (e.g., the amount associated with a request from the user), out of a plurality of possible values, with a threshold level of certainty. Accordingly, the remote server may indicate the quantity of bits to the user device or may indicate variables (e.g., one or more variables), as described above, that the user device may use to determine the quantity of bits. By encoding a quantity of bits to identify the data associated with the user rather than encoding the data itself, the user device may conserve processing resources and power, as well as reduce noticeable distortion to the selected audio clip. In some implementations, the remote server may use a quantity of entries in the library, as described above, to determine the plurality of users registered with the remote server.</p><p id="p-0031" num="0030">In one example, and as described in further detail with respect to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the user device may perform a discrete wavelet transform (DWT) associated with embedding vectors (e.g., one or more vectors), that encode the data associated with the user (or an indication of that data), within subbands (e.g., one or more subbands) associated with the selected audio clip. Accordingly, in one example, the digital representation may include a spectrogram such that the user device applies a wavelet transform to obtain the spectrogram associated with the audio clip, selects the subbands within the spectrogram that satisfy a threshold (e.g., subbands that are not too strong and/or subbands that are not too weak, in order to reduce noticeable distortion to the selected audio clip), and embeds a vector (e.g., at least one vector) encoding the data associated with the user within the subbands.</p><p id="p-0032" num="0031">Additionally, or alternatively, and as described in further detail with respect to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, the user device may use a phase coding algorithm associated with shifting initial phases (e.g., one or more initial phases) corresponding to segments (e.g., one or more segments) of the selected audio clip such that additional phases (e.g., one or more additional phases) included in each segment are shifted to maintain a relative difference with the initial phase for the segment. For example, the user device may apply a wavelet transform to obtain the segments and then shift the initial phases of those segments to encode the data associated with the user (or an indication of that data).</p><p id="p-0033" num="0032">Additionally, or alternatively, and as described in further detail with respect to <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, the user device may apply a tone insertion algorithm associated with inserting a tone within the selected audio clip such that the tone encodes the data associated with the user of the device (or an indication of that data). In some implementations, to reduce noticeable distortion to the selected audio clip, a frequency range associated with the tone is not within a range of frequencies associated with human hearing. For example, the user device may encode tones that can be played by a speaker (e.g., at least one speaker) associated with the user device but are not within the range associated with human hearing, such as tones between 19 kHz and 21 kHz, tones between 1 Hz and 21 Hz, and/or another similar range.</p><p id="p-0034" num="0033">Additionally, or alternatively, the user device may use other encoding algorithms. For example, the user device may apply a spread spectrum algorithm associated with addition of a spread data sequence, that includes a noise sequence and a carrier sequence encoding the data associated with the user of the device (or an indication of that data), to a frequency spectrum (e.g., a fast Fourier transform (FFT) output) associated with the selected audio clip. In another example, the user device may apply an echo hiding algorithm such that an echo encoding the data associated with the user of the device (or an indication of that data) is inserted into the selected audio clip. In order to reduce noticeable distortion to the selected audio clip, the echo may be inserted with a small (e.g., less than a threshold) volume and/or in a frequency range that is not within a range of frequencies associated with human hearing (e.g., as described above).</p><p id="p-0035" num="0034">As shown by reference number <b>130</b><i>a</i>, the user device may transmit, and the remote server may receive, a copy of the modified digital representation. Additionally with the copy, the user device may transmit, and the remote server may receive, a copy of the data associated with the user along with an indicator of an association between the modified digital representation and the data (e.g., a relational flag, a graphical link, and/or another type of association indicator). Accordingly, the remote server may map the modified digital representation (and thus the data and/or the indication encoded therein) to the data associated with the user (e.g., on behalf of nearby devices, as described in further detail below). This conserves processing resources and power as compared with fully decoding the data and/or the indication within the modified digital representation.</p><p id="p-0036" num="0035">As an alternative, and as shown by reference number <b>130</b><i>b</i>, the user device may transmit, and the remote server may receive, an indication of the selected audio clip and an indication of the encoding algorithm. Additionally with the indications, the user device may transmit, and the remote server may receive, a copy of the data associated with the user along with an indicator of an association between the indications and the data (e.g., a relational flag, a graphical link, and/or another type of association indicator). Accordingly, the remote server may perform the same modification to the digital representation as performed by the user device and then associate the modified digital representation with the data associated with the user. Because the indications are smaller than a copy of the modified digital representation, this conserves network resources and power at the user device and reduces latency in communicating with the remote server as compared with the user device transmitting the copy of the modified digital representation.</p><p id="p-0037" num="0036">In some implementations, the user device transmits the indication of the selected audio clip and at least a portion of the data associated with the user (e.g., the identifier associated with the user) earlier than the indication of the encoding algorithm and a remainder of the data associated with the user (e.g., the amount associated with a request).</p><p id="p-0038" num="0037">As shown by reference number <b>135</b>, the remote server may update, in the library, the entry that associates the indication of the selected audio clip with the identifier associated with the user device and/or the identifier associated with the user. The remote server may update the entry to indicate the modified digital representation and to associate the modified digital representation with at least a portion of the data associated with the user (e.g., the amount associated with a request) along with the identifier associated with the user device and/or the identifier associated with the user. Alternatively, the remote server may generate a new entry that associates the indication of the modified digital representation with the identifier associated with the user device and/or the identifier associated with the user as well as a remaining portion of the data associated with the user. By dynamically updating the entries in the library to associate the modified digital representation with the data associated with the user, the remote server may be used to transmit the data associated with the user from the user device to nearby devices faster and with less overhead than using visual techniques or short-range radio waves. Additionally, unlike if the remote server were using a static library, the remote server may use the dynamic library to transmit new data from the user device to nearby devices with only seconds between receipt of the new data and requests from the nearby devices for the new data. For example, as described above, the data associated with the user may be based, at least in part, on input from the user that is provided at the same time (or within seconds) of a request to transmit that data to the nearby devices via audio.</p><p id="p-0039" num="0038">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> and by reference number <b>140</b>, the user device may use a speaker associated with the user device to play audio, based on the modified digital representation, for recording by the nearby devices. In some implementations, the user device may use the speaker based on a confirmation from the remote server that the library was updated (e.g., as described above). In one example, using the speaker may include outputting, to a driver, the modified digital representation of the selected audio clip such that the modified digital representation is fed to a digital-to-analog converter by the driver. Accordingly, the output from the digital-to-analog converter may drive the speaker to play the audio based on the modified digital representation.</p><p id="p-0040" num="0039">As shown by reference number <b>145</b>, the nearby devices may use microphones (e.g., one or more microphones) associated with the nearby devices to record audio in an environment of the nearby devices. In some implementations, the nearby devices may receive inputs that trigger reception of information via sound. For example, the inputs may include requests to receive information via audio. In some implementations, the users associated with the nearby devices may select (e.g., using touchscreens, keyboards, or other input devices) buttons or other visual indicators that trigger reception of information from a nearby device (e.g., the user device) via audio. In one example, using the microphone may include receiving, from a driver, a digital representation of the recorded audio, where the digital representation is from an analog-to-digital converter controlled by the driver and driven by the microphone. In some implementations, the nearby devices may additionally apply one or more filters (e.g., at a hardware level and/or a software level) to distinguish a portion of the recorded audio transmitted by the user device from ambient noise and/or other background noise also included in the recorded audio. These filters increase an accuracy of mapping the recorded audio to the data (e.g., by the remote server, as described in further detail below) and/or decrease an amount of recorded audio needed by the remote server to map the recorded audio to the data with a threshold level of certainty. Accordingly, the filters conserve processing resources and power at the nearby devices by reducing an amount of data to be transmitted to the remote server and decreasing a latency between recording audio and receiving the corresponding data from the remote server.</p><p id="p-0041" num="0040">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>D</figref> and by reference number <b>150</b>, the nearby devices may transmit, and the remote server may receive, digital representations of the recorded audio. In some implementations, the nearby devices may store segments of the audio as recording occurs, generate digital representations of those segments, and transmit the digital representations to the remote server (e.g., with an application programming interface (API) call and/or another request to identify the data associated with the digital representations). By transmitting the digital representations, the nearby devices conserve network overhead. As an alternative, the nearby devices may transmit the segments to the remote server such that the remote server may generate the digital representations (e.g., spectrograms, phase matrices, discretized signals, and/or other digital repressions) based on the segments. By generating the digital representations, the remote server allows the nearby devices to conserve processing resources and power. In some implementations, the nearby devices may stream the recorded audio to the remote server rather than transmitting segments. By streaming the audio, the nearby devices may receive the data associated with the digital representations faster than if the nearby devices were to transmit segments.</p><p id="p-0042" num="0041">As shown by reference number <b>155</b>, the remote server may use the library to map the received digital representations to the data associated with the user device. For example, the remote server may perform a fuzzy search within the library based on the digital representations to find entries (e.g., one or more entries) within the library. As used herein, a &#x201c;fuzzy search&#x201d; refers to any search within the library that does not rely on a perfect match between the received digital representations and stored digital representations associated with the entries in the library. For example, a fuzzy search may include verifying whether portions of the recorded digital representations (e.g., bits, frequencies, tones, harmonies, and/or other portions) are within threshold measures of corresponding portions of the stored digital representations. In another example, a fuzzy search may include a machine-learning-based engine (e.g., a regression analysis, a decision tree, an artificial neural network, and/or another machine-learning model) to determine similarity scores, between the recorded digital representations and the stored digital representations, that satisfy a similarity threshold. By using a fuzzy search rather than decoding the data (or an indication associated with the data) from the recorded audio, the remote server conserves processing resources and power.</p><p id="p-0043" num="0042">Accordingly, the remote server may identify an entry associated with a stored digital representation that is a closest match to the received digital representations. For example, the stored digital representation may be associated with a highest similarity score and/or other measure of closeness to the received digital representations. In some implementations, when a level of certainty (e.g., an accuracy score and/or another measure of certainty associated with the fuzzy search) does not satisfy a threshold, the remote server may wait for, or may request, additional recorded audio (e.g., additional segments, additional digital representations based on the additional segments, or additional streamed audio) from the nearby devices. Accordingly, the remote server may dynamically update the fuzzy search, and thus the results from and the level of certainty associated with the fuzzy search, as the nearby devices continue to record.</p><p id="p-0044" num="0043">When the remote server identifies the entry that is a closest match, the remote server may transmit, and the nearby devices may receive, the data that is indicated by the entry. In example <b>100</b>, the data indicated by the entry is the data associated with the user device. Accordingly, as shown by reference number <b>160</b>, the remote server may transmit the identifier associated with the user device and/or the identifier associated with the user as well as a remaining portion of the data associated with the user (e.g., the amount associated with the request from the user, as described above). By using the remote server, the nearby devices conserve processing resources and power as compared with determining the data associated with the user locally.</p><p id="p-0045" num="0044">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>E</figref> and by reference number <b>165</b>, the nearby devices may transmit, and the remote server may receive, instructions to perform an action based on the data associated with the user (e.g., based on the identifier associated with the user device and/or the identifier associated with the user as well as the amount associated with the request from the user) received from the remote server. For example, the nearby devices may display, to the users of the nearby devices, visual representations based on the data associated with the user (e.g., textual and/or visual representations of the user along with textual representations of the amount), and the users may select (e.g., using a touchscreen, a keyboard, or another input device) buttons or another visual indicators that trigger the action. Accordingly, the nearby devices may use API calls and/or transmit other requests to perform the action, based on the triggers from the users. In some implementations, the action includes a transaction that is associated with the user of the user device and the users of the nearby devices and that is based at least in part on the amount.</p><p id="p-0046" num="0045">As shown by reference number <b>170</b>, the remote server may perform the action for the nearby devices based on the instructions. For example, the remote server may alter data stored on the remote server and associated with the user of the user device and/or the users associated with the nearby devices in order to complete the action (e.g., altering the data to reflect the transaction). Additionally, or alternatively, the remote server may contact an additional server (e.g., one or more additional servers) via an API call and/or another communication such that the additional server completes the action. In some implementations, the remote server may transmit, and the nearby devices may receive, confirmations that the action was completed.</p><p id="p-0047" num="0046">Additionally, as shown by reference number <b>175</b>, the remote server may transmit, and the user device may receive, a confirmation that users associated with the nearby devices have performed the action based on the data associated with the user of the device. In some implementations, the user device may display visual indicators (e.g., one or more visual indicators) associated with the confirmation. In some implementations, the confirmation may indicate identifiers (e.g., one or more identifiers) associated with the users of the nearby devices (e.g., usernames, nicknames, or other identifiers). Accordingly, the visual indicators may include textual and/or visual representations of the users.</p><p id="p-0048" num="0047">In order to conserve processing resources and power, the user device may halt playing of the audio using the speaker after a finite amount of time. For example, the user device may halt playing after a threshold amount of time (e.g., preconfigured, such as a default; based on a setting associated with the device; and/or based on a preference associated with the user). Additionally, or alternatively, the user device may halt playing based on a dynamic determination. For example, the user device may halt playing when the confirmation indicates a quantity of users, associated with the nearby devices, that satisfy a threshold. The threshold may be based on an expected quantity of users, which may be input by the user of the user device. Additionally, or alternatively, the user device may dynamically determine the threshold based on detection of a quantity of devices that are within a threshold distance (e.g., based on a percentage of the quantity of devices within the threshold distance, where the percentage is preconfigured, such as a default, or based on historical actions performed by the user). In another example, the user device may receive an instruction to halt playing of the audio and halt playing based on the instructions. For example, the user may select (e.g., using a touchscreen, a keyboard, or another input device) a button or another visual indicator that triggers the instruction.</p><p id="p-0049" num="0048">Accordingly, the user device may use the speaker to halt playing of the audio based on a determination or instruction, as described above. In one example, the user device may instruct the driver to stop providing an analog signal to the speaker based on the modified digital representation.</p><p id="p-0050" num="0049">In order to further conserve memory on the remote server, the user device may transmit, and the remote server may receive, an indication to remove the association between the modified digital representation and the data. For example, the user device may transmit the indication based on halting playing of the audio. Accordingly, the server may update the entry, associated with the user and/or the user device, to remove the modified digital representation and to remove at least some of the data associated with the user (e.g., remove the amount associated with a request from the user). Alternatively, the remote server may remove the entry associated with the user and/or the user device. In addition to conserving memory, this allows the remote server to increase accuracy of other fuzzy searches by reducing a size of the library. The reduced size of the library also allows the remote server to perform other fuzzy searches faster and to reduce a size of indications to be encoded in audio clips for use in the other fuzzy searches, which conserves processing resources and power on other user devices performing the encodings.</p><p id="p-0051" num="0050">By using techniques as described in connection with example <b>100</b>, the user device may modify the digital representation of the selected audio clip in order to transmit information (e.g., the data associated with the user) to the nearby devices using the modified digital representation. This exchange of audio is less sensitive to blockages and interferences than short-range radio signals, which increases accuracy and decreases latency in transmitting the information. Further, retransmissions are generally reduced, which conserves processing resources and power at both user devices. Additionally, in order to further reduce processing resources and power consumed by the nearby devices, the user device may indicate the modified digital representation to the remote server such that the nearby devices contact the remote server in order to receive the information. This further reduces processing resources and power that the nearby devices would otherwise use to decode recorded audio. In some implementations, by using the remote server, the user device does not have to encode the information directly into the digital representation but instead may encode an indication of the information, which is smaller in size and thus conserves processing resources and power at the user device. Additionally, the remote server may dynamically update mappings between different indicators and different information as different transmitting user devices begin and end audio transmissions, which allows for transmitting user devices to modify the information to be transmitted, and even modify the audio clip to be used, significantly faster than if the remote server were to use a static or semi-static library of mappings. Additionally, in some implementations, a dynamic library of mappings conserves memory and processing resources at the remote server, and speeds up how quickly the remote server can provide information to receiving user devices, as compared with a static or semi-static library, which would be larger and slower to search.</p><p id="p-0052" num="0051">As indicated above, <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>E</figref> are provided as an example. Other examples may differ from what is described with regard to <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>E</figref>.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIGS. <b>2</b>A, <b>2</b>B, and <b>2</b>C</figref> are diagrams of examples <b>200</b>, <b>220</b>, and <b>240</b>, respectively, of encoding algorithms associated with exchanging information on mobile devices using audio. The algorithms of examples <b>200</b>, <b>220</b>, and <b>240</b> may be implemented by a user device, as shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>E</figref> and as described in more detail in connection with <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref>.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> shows an example <b>200</b> of wavelet transforms associated with embedding vectors in a selected audio clip. For example, the user device may convert an analog signal <b>205</b><i>a </i>(that comprises the selected audio clip) to a digital representation, such as a discrete transform <b>205</b><i>b </i>and/or a continuous transform <b>205</b><i>c</i>. The discrete transform <b>205</b><i>b </i>includes discrete subbands, and the continuous transform <b>205</b><i>c </i>includes continuous subbands. Accordingly, as shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, the transforms may indicate a strength of different frequencies within the selected audio clip (e.g., across time as shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>).</p><p id="p-0055" num="0054">The user device may therefore select subbands for encoding (e.g., as shown by reference numbers <b>210</b><i>a </i>and <b>210</b><i>b</i>) and increase or decrease a frequency strength within the selected subbands to encode information in the digital representation. For example, the user device may increase the frequency strength to encode a &#x201c;1&#x201d; bit and decrease the frequency strength to encode a &#x201c;0&#x201d; bit. In another example, the user device may alter the frequency strength by an amount corresponding to a particular hexadecimal value (or other value). Accordingly, the user device may encode data (or an indication of data) within the selected audio clip.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> shows an example <b>220</b> of phase coding a selected audio clip. For example, the user device may discretize the selected audio clip into segments (e.g., one segment is shown as analog signal <b>225</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>). As shown by reference number <b>230</b>, the user device may thus shift an initial phase of the segment to encode a bit (e.g., &#x3c0;/4 shift to encode a &#x201c;1&#x201d; bit or a &#x2212;&#x3c0;/4 shift to encode a &#x201c;0&#x201d; bit) and shift remaining parts of the segment to align with the shifted initial phase (e.g., resulting in a new segment shown as analog signal <b>225</b><i>b </i>in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>). Accordingly, the user device may encode data (or an indication of data) within the selected audio clip.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> shows an example <b>240</b> of tone insertion within a selected audio clip. For example, the user device may discretize the selected audio clip into segments (e.g., one segment is shown as analog signal <b>245</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>). As shown by reference number <b>250</b>, the user device may insert a tone that represents a bit (e.g., a &#x201c;1&#x201d; or &#x201c;0&#x201d;) or a hexadecimal value (or other value) into the segment (e.g., resulting in a new segment shown as analog signal <b>245</b><i>b </i>in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>). As further shown in <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, the inserted tone may be outside a range of frequencies associated with human hearing. Accordingly, the user device may encode data (or an indication of data) within the selected audio clip.</p><p id="p-0058" num="0057">As indicated above, <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref> are provided as examples. Other examples may differ from what is described with regard to <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref>.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of an example environment <b>300</b> in which systems and/or methods described herein may be implemented. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, environment <b>300</b> may include a remote server <b>301</b>, which may include one or more elements of and/or may execute within a cloud computing system <b>302</b>. The cloud computing system <b>302</b> may include one or more elements <b>303</b>-<b>306</b>, as described in more detail below. As further shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, environment <b>300</b> may include a user device <b>310</b>, a network <b>320</b>, and other devices <b>330</b>. Devices and/or elements of environment <b>300</b> may interconnect via wired connections and/or wireless connections.</p><p id="p-0060" num="0059">The cloud computing system <b>302</b> includes computing hardware <b>303</b>, a resource management component <b>304</b>, a host operating system (OS) <b>305</b>, and/or one or more virtual computing systems <b>306</b>. The resource management component <b>304</b> may perform virtualization (e.g., abstraction) of computing hardware <b>303</b> to create the one or more virtual computing systems <b>306</b>. Using virtualization, the resource management component <b>304</b> enables a single computing device (e.g., a computer, a server, and/or the like) to operate like multiple computing devices, such as by creating multiple isolated virtual computing systems <b>306</b> from computing hardware <b>303</b> of the single computing device. In this way, computing hardware <b>303</b> can operate more efficiently, with lower power consumption, higher reliability, higher availability, higher utilization, greater flexibility, and lower cost than using separate computing devices.</p><p id="p-0061" num="0060">Computing hardware <b>303</b> includes hardware and corresponding resources from one or more computing devices. For example, computing hardware <b>303</b> may include hardware from a single computing device (e.g., a single server) or from multiple computing devices (e.g., multiple servers), such as multiple computing devices in one or more data centers. Computer hardware <b>303</b> may include one or more processors, one or more memories, one or more storage components, and/or one or more networking components, examples of which are described elsewhere herein.</p><p id="p-0062" num="0061">The resource management component <b>304</b> includes a virtualization application (e.g., executing on hardware, such as computing hardware <b>303</b>) capable of virtualizing computing hardware <b>303</b> to start, stop, and/or manage one or more virtual computing systems <b>306</b>. For example, the resource management component <b>304</b> may include a hypervisor (e.g., a bare-metal or Type <b>1</b> hypervisor, a hosted or Type <b>2</b> hypervisor, and/or the like) or a virtual machine monitor, such as when the virtual computing systems <b>306</b> are virtual machines. Additionally, or alternatively, the resource management component <b>304</b> may include a container manager, such as when the virtual computing systems <b>306</b> are containers. In some implementations, the resource management component <b>304</b> executes within and/or in coordination with a host operating system <b>305</b>.</p><p id="p-0063" num="0062">A virtual computing system <b>306</b> includes a virtual environment that enables cloud-based execution of operations and/or processes described herein using computing hardware <b>303</b>. A virtual computing system <b>306</b> may execute one or more applications using a file system that includes binary files, software libraries, and/or other resources required to execute applications on a guest operating system (e.g., within the virtual computing system <b>306</b>) or the host operating system <b>305</b>.</p><p id="p-0064" num="0063">Although the remote server <b>301</b> may include one or more elements <b>303</b>-<b>306</b> of the cloud computing system <b>302</b>, may execute within the cloud computing system <b>302</b>, and/or may be hosted within the cloud computing system <b>302</b>, in some implementations, the remote server <b>301</b> may not be cloud-based (e.g., may be implemented outside of a cloud computing system) or may be partially cloud-based. For example, the remote server <b>301</b> may include one or more devices that are not part of the cloud computing system <b>302</b>, such as device <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, which may include a standalone server or another type of computing device. The remote server <b>301</b> may perform one or more operations and/or processes described in more detail elsewhere herein.</p><p id="p-0065" num="0064">User device <b>310</b> may include one or more devices capable of transmitting information to nearby devices using audio. The user device <b>310</b> may include a communication device. For example, the user device <b>310</b> may include a wireless communication device, a user equipment (UE), a mobile phone (e.g., a smart phone or a cell phone, among other examples), a laptop computer, a tablet computer, a handheld computer, a desktop computer, a gaming device, a wearable communication device (e.g., a smart wristwatch or a pair of smart eyeglasses, among other examples), an Internet of Things (IoT) device, or a similar type of device. The user device <b>310</b> may modify a digital representation of an audio clip and use a speaker to play audio based on the modified digital representation, as described elsewhere herein. Additionally, the user device <b>310</b> may communicate with the remote server <b>301</b> such that the remote server <b>301</b> may map the modified digital representation to data associated with a user of the user device <b>310</b>, as described elsewhere herein. For example, the remote server <b>301</b> may perform the mapping on behalf of other devices <b>330</b>, as described below.</p><p id="p-0066" num="0065">Network <b>320</b> includes one or more wired and/or wireless networks. For example, network <b>320</b> may include a cellular network, a public land mobile network (PLMN), a local area network (LAN), a wide area network (WAN), a private network, the Internet, and/or the like, and/or a combination of these or other types of networks. The network <b>320</b> enables communication among the devices of environment <b>300</b>.</p><p id="p-0067" num="0066">Other devices <b>330</b> may include one or more devices capable of receiving information from another device using audio. The other devices <b>330</b> may include communication devices. For example, the other devices <b>330</b> may include wireless communication devices, UEs, mobile phones (e.g., smart phones or cell phones, among other examples), laptop computers, tablet computers, handheld computers, desktop computers, gaming devices, wearable communication devices (e.g., smart wristwatches or pairs of smart eyeglasses, among other examples), IoT devices, or similar types of devices. The other devices <b>330</b> may communicate with the remote server <b>301</b> such that the remote server <b>301</b> may map audio, recorded by the other devices <b>330</b>, to data associated with a user of another device, as described elsewhere herein. For example, the data may be associated with a user of the user device <b>310</b>, as described above.</p><p id="p-0068" num="0067">The number and arrangement of devices and networks shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are provided as an example. In practice, there may be additional devices and/or networks, fewer devices and/or networks, different devices and/or networks, or differently arranged devices and/or networks than those shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Furthermore, two or more devices shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be implemented within a single device, or a single device shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be implemented as multiple, distributed devices. Additionally, or alternatively, a set of devices (e.g., one or more devices) of environment <b>300</b> may perform one or more functions described as being performed by another set of devices of environment <b>300</b>.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of example components of a device <b>400</b>, which may correspond to a user device and/or other devices. In some implementations, a user device and/or other devices may include one or more devices <b>400</b> and/or one or more components of device <b>400</b>. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, device <b>400</b> may include a bus <b>410</b>, a processor <b>420</b>, a memory <b>430</b>, a storage component <b>440</b>, an input component <b>450</b>, an output component <b>460</b>, and a communication component <b>470</b>.</p><p id="p-0070" num="0069">Bus <b>410</b> includes a component that enables wired and/or wireless communication among the components of device <b>400</b>. Processor <b>420</b> includes a central processing unit, a graphics processing unit, a microprocessor, a controller, a microcontroller, a digital signal processor, a field-programmable gate array, an application-specific integrated circuit, and/or another type of processing component. Processor <b>420</b> is implemented in hardware, firmware, or a combination of hardware and software. In some implementations, processor <b>420</b> includes one or more processors capable of being programmed to perform a function. Memory <b>430</b> includes a RAM, a read-only memory (ROM), and/or another type of memory (e.g., a flash memory, a magnetic memory, and/or an optical memory).</p><p id="p-0071" num="0070">Storage component <b>440</b> stores information and/or software related to the operation of device <b>400</b>. For example, storage component <b>440</b> may include a hard disk drive, a magnetic disk drive, an optical disk drive, a solid state disk drive, a compact disc, a digital versatile disc, and/or another type of non-transitory computer-readable medium. Input component <b>450</b> enables device <b>400</b> to receive input, such as user input and/or sensed inputs. For example, input component <b>450</b> may include a touch screen, a keyboard, a keypad, a mouse, a button, a microphone, a switch, a sensor, a global positioning system component, an accelerometer, a gyroscope, and/or an actuator. Output component <b>460</b> enables device <b>400</b> to provide output, such as via a display, a speaker, and/or one or more light-emitting diodes. Communication component <b>470</b> enables device <b>400</b> to communicate with other devices, such as via a wired connection and/or a wireless connection. For example, communication component <b>470</b> may include a receiver, a transmitter, a transceiver, a modem, a network interface card, and/or an antenna.</p><p id="p-0072" num="0071">Device <b>400</b> may perform one or more processes described herein. For example, a non-transitory computer-readable medium (e.g., memory <b>430</b> and/or storage component <b>440</b>) may store a set of instructions (e.g., one or more instructions, code, software code, and/or program code) for execution by processor <b>420</b>. Processor <b>420</b> may execute the set of instructions to perform one or more processes described herein. In some implementations, execution of the set of instructions, by one or more processors <b>420</b>, causes the one or more processors <b>420</b> and/or the device <b>400</b> to perform one or more processes described herein. In some implementations, hardwired circuitry may be used instead of or in combination with the instructions to perform one or more processes described herein. Thus, implementations described herein are not limited to any specific combination of hardware circuitry and software.</p><p id="p-0073" num="0072">The number and arrangement of components shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are provided as an example. Device <b>400</b> may include additional components, fewer components, different components, or differently arranged components than those shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Additionally, or alternatively, a set of components (e.g., one or more components) of device <b>400</b> may perform one or more functions described as being performed by another set of components of device <b>400</b>.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of an example process <b>500</b> associated with information exchange on mobile devices using audio. In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by a user device (e.g., user device <b>310</b>). In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by another device or a group of devices separate from or including the user device, such as remote server <b>301</b> and/or other devices <b>330</b>. Additionally, or alternatively, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by one or more components of device <b>400</b>, such as processor <b>420</b>, memory <b>430</b>, storage component <b>440</b>, input component <b>450</b>, output component <b>460</b>, and/or communication component <b>470</b>.</p><p id="p-0075" num="0074">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include receiving input that triggers transmission of information via sound (block <b>510</b>). As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include selecting an audio clip (block <b>520</b>). In some implementations, the user device may select the audio clip based on a setting associated with the user device and/or based on a preference associated a user of the device. In some implementations, the audio clip may be selected from a plurality of stored audio clips. As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include modifying a digital representation of the selected audio clip based on data associated with the user of the device (block <b>530</b>). In some implementations, the user device may modify the digital representation using an encoding algorithm (e.g., as described above in connection with <figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref>). As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include transmitting, to a remote server, an indication of the selected audio clip, an indication of the modification, and the data associated with the user (block <b>540</b>). For example, the user device may transmit the modified digital representation or an indication of the encoding algorithm. As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include using a speaker associated with the device to play audio, based on the modified digital representation, for recording by other devices (block <b>550</b>). As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include receiving, from the remote server and based on the speaker playing the audio, a confirmation that users associated with the other devices have performed an action based on the data associated with the user of the device (block <b>560</b>). For example, the action may include a transaction that is associated with the user of the user device and the users of the other devices and that is based at least in part on an amount, associated with a request from the user of the user device, and included in the data associated with the user.</p><p id="p-0076" num="0075">Although <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows example blocks of process <b>500</b>, in some implementations, process <b>500</b> may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Additionally, or alternatively, two or more of the blocks of process <b>500</b> may be performed in parallel.</p><p id="p-0077" num="0076">The foregoing disclosure provides illustration and description, but is not intended to be exhaustive or to limit the implementations to the precise forms disclosed. Modifications may be made in light of the above disclosure or may be acquired from practice of the implementations.</p><p id="p-0078" num="0077">As used herein, the term &#x201c;component&#x201d; is intended to be broadly construed as hardware, firmware, or a combination of hardware and software. It will be apparent that systems and/or methods described herein may be implemented in different forms of hardware, firmware, and/or a combination of hardware and software. The actual specialized control hardware or software code used to implement these systems and/or methods is not limiting of the implementations. Thus, the operation and behavior of the systems and/or methods are described herein without reference to specific software code&#x2014;it being understood that software and hardware can be used to implement the systems and/or methods based on the description herein.</p><p id="p-0079" num="0078">As used herein, satisfying a threshold may, depending on the context, refer to a value being greater than the threshold, greater than or equal to the threshold, less than the threshold, less than or equal to the threshold, equal to the threshold, not equal to the threshold, or the like.</p><p id="p-0080" num="0079">Although particular combinations of features are recited in the claims and/or disclosed in the specification, these combinations are not intended to limit the disclosure of various implementations. In fact, many of these features may be combined in ways not specifically recited in the claims and/or disclosed in the specification. Although each dependent claim listed below may directly depend on only one claim, the disclosure of various implementations includes each dependent claim in combination with every other claim in the claim set. As used herein, a phrase referring to &#x201c;at least one of&#x201d; a list of items refers to any combination of those items, including single members. As an example, &#x201c;at least one of: a, b, or c&#x201d; is intended to cover a, b, c, a-b, a-c, b-c, and a-b-c, as well as any combination with multiple of the same item.</p><p id="p-0081" num="0080">No element, act, or instruction used herein should be construed as critical or essential unless explicitly described as such. Also, as used herein, the articles &#x201c;a&#x201d; and &#x201c;an&#x201d; are intended to include one or more items, and may be used interchangeably with &#x201c;one or more.&#x201d; Further, as used herein, the article &#x201c;the&#x201d; is intended to include one or more items referenced in connection with the article &#x201c;the&#x201d; and may be used interchangeably with &#x201c;the one or more.&#x201d; Furthermore, as used herein, the term &#x201c;set&#x201d; is intended to include one or more items (e.g., related items, unrelated items, or a combination of related and unrelated items), and may be used interchangeably with &#x201c;one or more.&#x201d; Where only one item is intended, the phrase &#x201c;only one&#x201d; or similar language is used. Also, as used herein, the terms &#x201c;has,&#x201d; &#x201c;have,&#x201d; &#x201c;having,&#x201d; or the like are intended to be open-ended terms. Further, the phrase &#x201c;based on&#x201d; is intended to mean &#x201c;based, at least in part, on&#x201d; unless explicitly stated otherwise. Also, as used herein, the term &#x201c;or&#x201d; is intended to be inclusive when used in a series and may be used interchangeably with &#x201c;and/or,&#x201d; unless explicitly stated otherwise (e.g., if used in combination with &#x201c;either&#x201d; or &#x201c;only one of&#x201d;).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system for conveying information to one or more other devices using audio, the system comprising:<claim-text>one or more memories; and</claim-text><claim-text>one or more processors, communicatively coupled to the one or more memories, configured to:<claim-text>receive at least a first portion of information and a second portion of information for transmission to the one or more other devices, wherein the first portion of information includes an identifier associated with a user of the system;</claim-text><claim-text>select an audio clip based on a preference associated with the user;</claim-text><claim-text>modify a digital representation of the audio clip to encode the first portion of information and the second portion of information within the digital representation, wherein the digital representation includes a spectrogram associated with the audio clip;</claim-text><claim-text>transmit, to a remote server, a copy of the modified digital representation, a copy of the first portion of information and the second portion of information, and an indicator of an association between the modified digital representation and the first portion of information and the second portion of information;</claim-text><claim-text>use at least one speaker associated with the system to play audio, based on the modified digital representation, for recording by the one or more other devices; and</claim-text><claim-text>receive, from the remote server and based on the at least one speaker playing the audio, a confirmation that users associated with the one or more other devices have performed an action based on the first portion of information and the second portion of information.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are further configured to:<claim-text>receive, from the remote server, an indication of a plurality of audio clips;</claim-text><claim-text>display, to the user, visual indicators associated with the plurality of audio clips;</claim-text><claim-text>receive, from the user, a selection of the audio clip from the plurality of audio clips;</claim-text><claim-text>store the selection as the preference associated with the user; and</claim-text><claim-text>transmit, to the remote server, an indication of the selected audio clip.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors are further configured to:<claim-text>receive, from the user, at least one file encoding the selected audio clip;</claim-text><claim-text>store, as the preference, an indicator associated with the at least one file; and</claim-text><claim-text>transmit, to the remote server, an indication of the selected audio clip.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors, to select the audio clip, are configured to:<claim-text>transmit, to the remote server, a request for the preference associated with the user, wherein the request includes at least one credential associated with the user; and</claim-text><claim-text>receive, from the remote server and based on the request, an indication of the preference associated with the user.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more processors, to modify the digital representation, are configured to:<claim-text>apply a wavelet transform to obtain the spectrogram associated with the audio clip;</claim-text><claim-text>select one or more subbands within the spectrogram that satisfy a threshold; and</claim-text><claim-text>embed a vector encoding the first portion of information and the second portion of information within the one or more subbands.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the action includes a transaction that is associated with the user of the system and the users of the one or more other devices and that is based at least in part on the second portion of information.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method of conveying information to one or more other devices using audio, comprising:<claim-text>receiving, at a device, input that triggers transmission of information via sound;</claim-text><claim-text>selecting an audio clip from a plurality of stored audio clips, based on a setting associated with the device;</claim-text><claim-text>modifying a digital representation of the selected audio clip using at least one encoding algorithm and based on data associated with a user of the device;</claim-text><claim-text>transmitting, to a remote server, an indication of the selected audio clip, an indication of the at least one encoding algorithm, and the data associated with the user of the device;</claim-text><claim-text>using at least one speaker associated with the device to play audio, based on the modified digital representation, for recording by the one or more other devices; and</claim-text><claim-text>receiving, from the remote server and based on the at least one speaker playing the audio, a confirmation that users associated with the one or more other devices have performed an action based on the data associated with the user of the device.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the data associated with the user of the device is based, at least in part, on the input that triggers transmission.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>displaying visual indicators associated with the plurality of stored audio clips;</claim-text><claim-text>receiving, at the device, a selection of the audio clip from the plurality of stored audio clips; and</claim-text><claim-text>storing, on the device, the selection as the setting associated with the device.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one encoding algorithm comprises:<claim-text>a tone insertion algorithm associated with inserting a tone within the selected audio clip, wherein the tone encodes the data associated with the user of the device, and wherein a frequency range associated with the tone is not within a range of frequencies associated with human hearing;</claim-text><claim-text>a phase coding algorithm associated with shifting one or more initial phases corresponding to one or more segments of the selected audio clip, wherein one or more additional phases included in each segment are shifted to maintain a relative difference with the initial phase for the segment; or</claim-text><claim-text>a discrete wavelet transform associated with embedding one or more vectors, that encode the data associated with the user of the device, within one or more subbands associated with the selected audio clip.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein using the at least one speaker comprises:<claim-text>outputting, to a driver, the modified digital representation of the selected audio clip,</claim-text><claim-text>wherein the modified digital representation is fed to a digital-to-analog converter by the driver.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the data associated with the user of the device includes an identifier associated with the user and an amount associated with a request from the user.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the indication of the selected audio clip and at least a portion of the data associated with the user of the device are transmitted earlier than the indication of the at least one encoding algorithm and a remainder of the data associated with the user of the device.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>displaying one or more visual indicators associated with the confirmation,</claim-text><claim-text>wherein the confirmation indicates one or more identifiers associated with one or more users of the one or more other devices.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable medium storing a set of instructions for conveying information to at least one other device using audio, the set of instructions comprising:<claim-text>one or more instructions that, when executed by one or more processors of a device, cause the device to:<claim-text>receive data associated with a user of the device;</claim-text><claim-text>modify a digital representation of an audio clip to encode the data within the digital representation;</claim-text><claim-text>transmit, to a remote server, a copy of the modified digital representation, a copy of the data, and an indicator of an association between the modified digital representation and the data;</claim-text><claim-text>use at least one speaker associated with the device to play audio, based on the modified digital representation, for recording by the at least one other device; and</claim-text><claim-text>receive, from the remote server and based on the at least one speaker playing the audio, a confirmation that at least one user associated with the at least one other device has performed an action based on the data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the digital representation includes at least one of a spectrogram, a phase matrix, or a discretized signal.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, when executed by the one or more processors, further cause the device to:<claim-text>authenticate the device with the remote server before transmitting to the remote server.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, when executed by the one or more processors, further cause the device to:<claim-text>receive an instruction to halt playing of the audio;</claim-text><claim-text>use the at least one speaker to halt playing of the audio based on the instruction; and</claim-text><claim-text>transmit, to the remote server, an indication to remove the association between the modified digital representation and the data.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, that cause the device to modify the digital representation, cause the device to:<claim-text>generate a plurality of bits, wherein a quantity of the plurality of bits is selected based at least in part on a setting associated with the remote server; and</claim-text><claim-text>modify the digital representation to encode the plurality of bits, wherein the plurality of bits are used to identify the data associated with the user.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the data associated with the user of the device includes an identifier associated with the user and an amount associated with a request from the user.</claim-text></claim></claims></us-patent-application>