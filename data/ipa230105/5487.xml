<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005488A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005488</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756874</doc-number><date>20201210</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2019-227192</doc-number><date>20191217</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>034</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>78</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>17</main-group><subgroup>18</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>034</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>78</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>1</main-group><subgroup>406</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>3</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SIGNAL PROCESSING DEVICE, SIGNAL PROCESSING METHOD, PROGRAM, AND SIGNAL PROCESSING SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SONY GROUP CORPORATION</orgname><address><city>TOKYO</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HIROE</last-name><first-name>ATSUO</first-name><address><city>TOKYO</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2020/046023</doc-number><date>20201210</date></document-id><us-371c12-date><date>20220603</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided is a signal processing device including a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="193.72mm" wi="149.44mm" file="US20230005488A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="195.75mm" wi="151.47mm" file="US20230005488A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="233.34mm" wi="119.80mm" orientation="landscape" file="US20230005488A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="234.95mm" wi="177.21mm" file="US20230005488A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.78mm" wi="158.33mm" file="US20230005488A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.84mm" wi="142.24mm" orientation="landscape" file="US20230005488A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="178.99mm" wi="106.85mm" orientation="landscape" file="US20230005488A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="230.72mm" wi="121.24mm" orientation="landscape" file="US20230005488A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="203.28mm" wi="118.11mm" orientation="landscape" file="US20230005488A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="230.80mm" wi="167.47mm" file="US20230005488A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="149.78mm" wi="151.81mm" file="US20230005488A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="234.19mm" wi="137.75mm" file="US20230005488A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="109.81mm" wi="72.47mm" file="US20230005488A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="229.02mm" wi="133.69mm" orientation="landscape" file="US20230005488A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="124.80mm" wi="71.80mm" file="US20230005488A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="225.81mm" wi="103.63mm" orientation="landscape" file="US20230005488A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="229.11mm" wi="153.92mm" orientation="landscape" file="US20230005488A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="222.50mm" wi="161.63mm" file="US20230005488A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="189.99mm" wi="145.29mm" orientation="landscape" file="US20230005488A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="232.75mm" wi="148.00mm" orientation="landscape" file="US20230005488A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="235.46mm" wi="112.01mm" orientation="landscape" file="US20230005488A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="129.88mm" wi="144.44mm" file="US20230005488A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="219.03mm" wi="152.48mm" file="US20230005488A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="93.64mm" wi="64.52mm" file="US20230005488A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="228.77mm" wi="149.27mm" orientation="landscape" file="US20230005488A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="221.40mm" wi="84.24mm" orientation="landscape" file="US20230005488A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to a signal processing device, a signal processing method, a program, and a signal processing system.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">A technology has been proposed in which a signal collected by a microphone is subjected to voice recognition, and a result of the voice recognition is transcribed into text data (see, for example, Patent Document 1 below).</p><heading id="h-0003" level="1">CITATION LIST</heading><heading id="h-0004" level="1">Patent Document</heading><p id="p-0004" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0003">Patent Document 1: Japanese Patent Application Laid-Open No. 2005-129971</li></ul></p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Problems to be Solved by the Invention</heading><p id="p-0005" num="0004">In such a field, it is desired that a speech of each speaker be correctly recognized.</p><p id="p-0006" num="0005">It is an object of the present disclosure to provide a signal processing device, a signal processing method, a program, and a signal processing system that allow for correct recognition of a speech of each speaker.</p><heading id="h-0007" level="1">Solutions to Problems</heading><p id="p-0007" num="0006">The present disclosure provides, for example,</p><p id="p-0008" num="0007">a signal processing device including:</p><p id="p-0009" num="0008">a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</p><p id="p-0010" num="0009">The present disclosure provides, for example,</p><p id="p-0011" num="0010">a signal processing method including:</p><p id="p-0012" num="0011">detecting, by a main speech detection unit, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and outputting frame information indicating presence or absence of the main speech.</p><p id="p-0013" num="0012">The present disclosure provides, for example,</p><p id="p-0014" num="0013">a program for causing a computer to execute a signal processing method including:</p><p id="p-0015" num="0014">detecting, by a main speech detection unit, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and outputting frame information indicating presence or absence of the main speech.</p><p id="p-0016" num="0015">The present disclosure provides, for example,</p><p id="p-0017" num="0016">a signal processing system including:</p><p id="p-0018" num="0017">a plurality of sound collection devices, each of which is assigned to one of speakers; and</p><p id="p-0019" num="0018">a signal processing device including a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to each sound collection device includes a main speech that is a voice of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams referred to in a description related to a problem to be considered in the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref> are diagrams referred to in a description related to a problem to be considered in the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>3</b>A to <b>3</b>C</figref> are diagrams referred to in a description related to an outline of an embodiment of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIGS. <b>4</b>A and <b>4</b>B</figref> are diagrams referred to in a description related to the outline of the embodiment of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for illustrating a configuration example of a signal processing device according to the embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram for illustrating a configuration example of a main speech detection unit according to the embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> are diagrams for illustrating an overview of processing in a crosstalk reduction unit according to the embodiment.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for illustrating a configuration example of the crosstalk reduction unit according to the embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref> are diagrams referred to in a description related to a neural network unit included in the crosstalk reduction unit according to the embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram referred to in a description related to training data for crosstalk reduction or the like according to the embodiment.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart referred to in a description related to an operation example of the signal processing device according to the embodiment.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flowchart referred to in a description related to a flow of main speech detection processing according to the embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIGS. <b>13</b>A to <b>13</b>C</figref> are diagrams referred to in a description related to processing performed by a short-term Fourier transform unit according to the embodiment.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart referred to in a description related to a flow of crosstalk reduction processing according to the embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram for illustrating a modified example.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram for illustrating the modified example.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIGS. <b>17</b>A and <b>17</b>B</figref> are diagrams for illustrating the modified example.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram for illustrating the modified example.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram for illustrating a modified example.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram for illustrating the modified example.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram for illustrating the modified example.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a flowchart for illustrating the modified example.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram for illustrating a modified example.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIGS. <b>24</b>A and <b>24</b>B</figref> are diagrams for illustrating the modified example.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a diagram for illustrating a modified example.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0045" num="0044">An embodiment and the like of the present disclosure will be described below with reference to the drawings. Note that the description will be given in the order below.</p><p id="p-0046" num="0045">&#x3c;Background of present disclosure&#x3e;</p><p id="p-0047" num="0046">&#x3c;Problems to be considered in present disclosure&#x3e;</p><p id="p-0048" num="0047">&#x3c;Embodiment&#x3e;</p><p id="p-0049" num="0048">&#x3c;Modified examples&#x3e;</p><p id="p-0050" num="0049">The embodiment and the like described below are preferred specific examples of the present disclosure, and contents of the present disclosure are not limited to the embodiment and the like.</p><heading id="h-0010" level="1">Background of Present Disclosure</heading><p id="p-0051" num="0050">The present disclosure provides, for example, a system for automatically generating a transcription text for a voice recorded in a situation in which a microphone is assigned to each one of a plurality of speakers. Generation of a transcription for such a voice has problems to be considered different from those in a case of recording with a single speaker and a single microphone. The present disclosure deals with a technology mainly related to a portion related to speech detection.</p><p id="p-0052" num="0051">First, a background of the present disclosure will be described. Examples of a conceivable situation in which a microphone is assigned to each speaker include the following:<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0000">    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0052">In recording of a television program or the like, each performer, who is a speaker, is wearing a pin microphone near a collar.</li>        <li id="ul0003-0002" num="0053">In recording of a radio program or the like, a unidirectional microphone is prepared for each performer, and is installed close to the performer with directivity (direction in which sensitivity is high) toward the performer.</li>        <li id="ul0003-0003" num="0054">Other assumed modes in which a microphone is installed at a position relatively close to the mouth of each speaker include: a mode in which each speaker is wearing a device (a type of headset), in which an earphone and a microphone are integrated, on an ear; a mode in which each speaker is wearing a microphone shaped like a name badge near the chest; and a mode in which each speaker has a microphone with a neck strap hanging around the neck.</li>    </ul>    </li></ul></p><p id="p-0053" num="0055">In the following description, a microphone used in such a mode is referred to as a &#x201c;wearable distributed microphone&#x201d; as appropriate, and sound recorded by such a microphone is referred to as a &#x201c;sound recorded by a wearable distributed microphone&#x201d; as appropriate.</p><p id="p-0054" num="0056">Sound recorded by a wearable distributed microphone has the following features.</p><p id="p-0055" num="0057">As a first feature, a speech of each speaker is collected most loudly by the microphone assigned to the speaker, but is collected also by other microphones as sound that has crossed over. In the following description, the former is referred to as &#x201c;main speech&#x201d; or &#x201c;main voice&#x201d; as appropriate, and the latter is referred to as &#x201c;crosstalk&#x201d; as appropriate.</p><p id="p-0056" num="0058">As a second feature, since each speaker can speak at a free timing, speeches of a plurality of speakers may exist at the same timing. In the following description, speeches of a plurality of speakers existing at the same timing are referred to as &#x201c;overlapping speeches&#x201d; as appropriate.</p><p id="p-0057" num="0059">When a main speech collected by one microphone is compared with a crosstalk, the crosstalk tends to be lower in volume and, moreover, contain more reverberation components due to the distance from the mouth to the microphone. However, since the loudness of voice varies from person to person, the crosstalk of a person with a loud voice may have a larger amplitude than the main speech of a person with a quiet voice.</p><p id="p-0058" num="0060">As modes in which the above-described overlapping speeches occur, conceivable cases mainly include the following:<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0000">    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0061">One person mainly makes remarks, and the others are back-channeling. The back-channeling constitutes overlapping speeches.</li>        <li id="ul0005-0002" num="0062">Basically, two persons are speaking alternately, but the next speech starts immediately before the end of the previous speech, which constitutes a slightly overlapping portion.</li>        <li id="ul0005-0003" num="0063">A phenomenon commonly called &#x201c;voice in unison&#x201d;, in which two or more speakers make the same speech substantially at the same time.</li>    </ul>    </li></ul></p><p id="p-0059" num="0064">When voice recognition is performed on a recorded sound recorded under such an environment and a transcription text is created on the basis of a result of the voice recognition, the followings are requested regardless of whether the transcription text is generated fully automatically or modified manually.</p><p id="p-0060" num="0065">a) It is desired that a speech of one speaker, which is collected by a plurality of microphones, be recorded as one remark in the transcription text. (It is not desirable that the same speech appear a plurality of times.)</p><p id="p-0061" num="0066">b) It is desired that overlapping speeches be recorded as remarks of the same number as the overlapping speeches. For example, in a case where speeches of two persons overlap, it is desirable that the individual speeches be recorded as two remarks. It is not desirable that one of the speeches be missing, or the speeches be recorded as three or more speeches.</p><p id="p-0062" num="0067">c) It is desired that the individual speeches be correctly transcribed even in a case where speeches overlap.</p><p id="p-0063" num="0068">d) It is desired that remarks be recorded so as to enable distinction between the persons who made the remarks.</p><p id="p-0064" num="0069">In a case of automatically generating a transcription text, it is desirable to satisfy a) to d) described above because the more the requests are satisfied, the more the labor of manually modifying the transcription text later can be reduced.</p><heading id="h-0011" level="1">Problems to be Considered in Present Disclosure</heading><p id="p-0065" num="0070">Next, along with a description of general technologies for responding to the above-described requests, problems to be considered in the present disclosure will be described.</p><p id="p-0066" num="0071">As a system for automatically generating a transcription text for sound recorded by a wearable distributed microphone, there are two configurations illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>. In a system <b>1</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, after mixing processing <b>1</b>A for mixing recorded sounds, each of which has been recorded by one of a plurality of microphones (M<sub>1 </sub>to M<sub>n</sub>), speech frame detection processing <b>1</b>B for detecting a speech frame and voice recognition processing <b>1</b>C are performed, and text generation processing <b>1</b>D is performed on the basis of a result of the voice recognition processing <b>1</b>C.</p><p id="p-0067" num="0072">In a system <b>2</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, after speech frame detection processing <b>2</b>A<sub>1 </sub>to <b>2</b>A<sub>n </sub>and voice recognition processing <b>2</b>B<sub>1 </sub>to <b>2</b>B<sub>n </sub>for each one of a plurality of microphones, connection processing <b>2</b>C for connecting results of the voice recognition is performed, and text generation processing <b>2</b>D is performed on the basis of a result of the connection processing <b>2</b>C. The embodiment of the present disclosure described later is based on the premise that the system <b>2</b> is used. Hereinafter, a description of the system <b>1</b> will be given first, and then a description of the system <b>2</b> in comparison with the system <b>1</b> will be given.</p><p id="p-0068" num="0073">As a technology based on the assumption that the system <b>1</b> is used, there are technologies described in Japanese Patent Application Laid-Open No. 2005-129971, Japanese Patent Application Laid-Open No. 2007-133033, and Japanese Patent Application Laid-Open No. 2008-9693. In the configuration of the system <b>1</b>, a single track is formed at the time of mixing recorded sounds, and this makes it possible to avoid a plurality of recognition results being generated from a single speech, which is included in problems caused by crosstalk described later. On the other hand, it is difficult in principle to generate a plurality of voice recognition results for overlapping speeches. Furthermore, even in a case where overlapping speeches are collected by each microphone with a high signal to noise ratio (SNR) (the voice of the corresponding speaker is collected as a louder sound), the SNR decreases at the time of mixing. Thus, there is a high possibility that a single voice recognition result generated for overlapping speeches is imprecise and does not apply to either of the speeches. That is, the system <b>1</b> generates a highly accurate transcription (text) in a case where there is almost no overlapping speeches, but the transcription requires more manual modification as the rate of overlapping speeches is higher. Furthermore, since it is not possible to specify the speaker of a speech (the speaker to which the microphone, from which a signal is derived, is assigned) once the recorded sounds have been mixed, it is necessary to combine the technology with another technology such as speaker identification and image processing in order to respond to the request d) described above.</p><p id="p-0069" num="0074">On the other hand, as a technology based on the assumption that the system <b>2</b> is used, there are technologies described in Japanese Patent Application Laid-Open No. 2006-39108 and Japanese Patent Application Laid-Open No. 2006-301223. In the configuration of the system <b>2</b>, a speech of each speaker is collected most intensely by the microphone assigned to the speaker, and is collected as a less loud sound by the other microphones. Thus, it is relatively easy to specify from which speaker an input voice is derived. Furthermore, even in a case where speeches overlap, a microphone assigned to each speaker most dominantly collects a speech of the speaker, and it is therefore expected that a relatively precise voice recognition result can be obtained in a case where a frame of the speech is precisely estimated and the microphone corresponding to the speaker is precisely selected.</p><p id="p-0070" num="0075">However, in the configuration of the system <b>2</b>, a speech of one speaker is also collected as crosstalk by a microphone assigned to a speaker who is not speaking at that point of time, and this causes a variety of problems. Hereinafter, problems caused by crosstalk and measures against the problems will be described.</p><p id="p-0071" num="0076">Since crosstalk is voice, in a case where a technology for detecting a frame in which a &#x201c;voice-like&#x201d; signal has been input is applied as the speech frame detection processing <b>2</b>A<sub>1 </sub>to <b>2</b>A<sub>n </sub>of the system <b>2</b>, not only the main speech but also the crosstalk is detected as speech frames. Voice recognition is performed for each frame, and a plurality of recognition results (at most the same number as the microphones) is generated for one speech. This cannot satisfy the request a) described above.</p><p id="p-0072" num="0077">Thus, a technology for discriminating whether or not a speech is crosstalk, or whether or not a speech is a main speech, has been proposed (e.g., the technology described in Japanese Patent Application Laid-Open No. 2006-39108). According to such a technology, correlation coefficients between microphones are calculated on the hypothesis that a speech of each speaker arrives earliest at the assigned microphone and is collected as the loudest sound, and thus the microphone corresponding to the main speech is determined. Furthermore, Document 1 &#x201c;&#x2018;DNN APPROACH TO SPEAKER DIARISATION USING SPEAKER CHANNELS&#x2019;, Rosanna Milner, Thomas Hain, Speech and Hearing Research Group, University of Sheffield, UK, ICASSP 2017&#x201d; describes using deep neural networks (DNNs) to learn a discriminator. Moreover, training is performed for each pair of microphones, and the required number of pairs are combined at the time of inference, so that it is possible to support any number of microphones. A neural network referred to herein is a model like a human cranial nerve circuit, and is a technique for implementing learning ability of a human on a computer. One of features of a neural network is that a neural network has learning ability. In a neural network, artificial neurons (nodes) that form a network by synaptic connection change the strength of synaptic connection through training, so that it is possible to acquire ability to solve a problem. That is, a neural network that has been repeatedly trained can automatically infer a rule for solving a problem.</p><p id="p-0073" num="0078">By taking measures against crosstalk, it is possible to avoid the problem of a plurality of recognition results being generated for one speech, and a single recognition result is generated. However, on the other hand, even in a case where it is desired to generate a plurality of recognition results for overlapping speeches, a trade-off occurs in which only a single recognition result is generated. Thus, there is a technology for detecting overlapping speeches, separately from determination of crosstalk.</p><p id="p-0074" num="0079">As technologies for detecting overlapping speeches, Document 2 &#x201c;&#x2018;Overlapped speech detection for improved speaker diarization in multiparty meetings&#x2019;, K. Boakye, B. Trueba-Hornero, O. Vinyals, G. Friedland, ICASSP 2008&#x201d; and Document 3 &#x201c;&#x2018;Detecting overlapping speech with long short-term memory recurrent neural networks&#x2019;, J. T. Geiger, F. Eyben, B. Schuller, G. Rigoll, INTERSPEECH 2013&#x201d; have been proposed. Note that these technologies are &#x201c;technologies for detecting a portion (timing) where speeches overlap&#x201d;. On the other hand, the embodiment of the present disclosure is a &#x201c;speech detection technology that also supports overlapping speeches&#x201d; as described later, and the two are different technologies.</p><p id="p-0075" num="0080">Next, problems in the system <b>2</b> will be described. In real free speech, the number of overlapping speeches changes from moment to moment. It frequently occurs that only one person is speaking at a certain timing, a plurality of speeches partially overlaps at another timing, and no one is speaking at still another timing. However, it has conventionally been difficult to detect each speech with a single technology in such a situation, and thus, a combination of voice activity detection (e.g., the technology described in Japanese Patent No. 4182444), crosstalk detection (e.g., the technology described in Japanese Patent Application Laid-Open No. 2006-39108 and the technology described in Document 1), and overlapping speech detection (the technologies described in Documents 2 and 3 described above) has been used to cope with the situation.</p><p id="p-0076" num="0081">For example, a situation in which two speakers are speaking is assumed. The two speakers are a speaker SA and a speaker SB. A microphone assigned to the speaker SA is a microphone MA, and a microphone assigned to the speaker SB is a microphone MB. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating a speech of each speaker collected by each microphone in chronological order. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the speech collected by each microphone is indicated by a horizontally long rectangle. The vertical direction of the rectangle indicates the volume of the speech, and the horizontal direction of the rectangle indicates the frame (time) of the speech. It is normally considered that the main speech is collected as a louder sound than the crosstalk, and the main speech is indicated by a rectangle higher than the crosstalk.</p><p id="p-0077" num="0082"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example in which the speaker SA speaks about twice, and the speaker SB starts to speak in the middle of the second speech of the speaker SA, continues to speak after the end of the speech of the speaker SA, and then ends the speech. Since the number of speeches made in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is three, in order to generate a voice recognition result corresponding to each speech, it is desirable to detect only three frames: a frame A<b>1</b> and a frame A<b>2</b> corresponding to the main speeches of the speaker SA; and a frame B<b>2</b> corresponding to the main speech of the speaker SB. However, since it is difficult to detect these frames with a single technology, the frames are estimated with a combination of a plurality of technologies.</p><p id="p-0078" num="0083">For example, first, voice activity detection is applied to each microphone. Since the voice activity detection detects a frame in which a &#x201c;voice-like&#x201d; signal is input, not only the frame A<b>1</b>, which is the main speech, but also a frame B<b>1</b>, which is crosstalk, is detected for the first speech of the speaker SA. The second speech of the speaker SA and the first speech of the speaker SB cannot be distinguished from each other, and are detected as a frame A<b>3</b> and a frame B<b>3</b>, which are long frames in which the two speeches are connected.</p><p id="p-0079" num="0084">Next, crosstalk detection is performed on the detected four frames (frames A<b>1</b>, A<b>3</b>, B<b>1</b>, and B<b>3</b>). Since the crosstalk detection is basically based on the hypothesis that speeches do not overlap, it can be expected that a highly accuracy estimation is obtained for a portion where the hypothesis is established (frames A<b>1</b>, A<b>4</b>, A<b>6</b>, B<b>1</b>, B<b>4</b>, and B<b>6</b>), and it is determined that the frames A<b>1</b>, A<b>4</b>, and B<b>6</b> include a main speech and the frames A<b>6</b>, B<b>1</b>, and B<b>4</b> include crosstalk. On the other hand, the hypothesis described above is not established in a frame where speeches overlap (frames A<b>5</b> and B<b>5</b>), and thus the determination result for the frame is imprecise.</p><p id="p-0080" num="0085">Thus, next, overlapping speech detection is applied to each of the detected four frames (frames A<b>1</b>, A<b>3</b>, B<b>1</b>, and B<b>3</b>), and a determination result that the frame A<b>5</b> and the frame B<b>5</b> include overlapping speeches is obtained. Finally, the determination results described above are integrated according to a rule or the like, and the frames A<b>1</b>, A<b>2</b>, and B<b>2</b>, which are speech frames of the speakers, are obtained.</p><p id="p-0081" num="0086">Since a frame of a main speech has conventionally been estimated with a combination of a plurality of detection technologies as described above, there has been a problem in that an upper limit of the accuracy of finally obtained frames is lowered due to the accuracy of each technology.</p><p id="p-0082" num="0087">Note that, as described in Document 4 &#x201c;&#x2018;MULTICHANNEL SPEAKER ACTIVITY DETECTION FOR MEETINGS&#x2019;, Patrick Meyer, Rolf Jongebloed, Tim Fingscheidt, Institute for Communications Technology, ICASSP2018&#x201d;, there is a speech frame detection technology that is used on a voice recorded by a wearable distributed microphone and supports both single speeches and overlapping speeches. In the technology described in Document 4, a value derived from a power ratio between a microphone that is collecting the loudest sound and each microphone is used as a feature amount for determining whether or not a speech is a main speech. Processing corresponding to normalization or smoothing is performed on the power ratio for each microphone, and then whether or not a speech is a main speech is determined by comparison with a predetermined threshold. However, in order to avoid a frame (background noise frame) in which no one is speaking from being determined as a main speech, the power ratio between the background noise frame and each microphone input signal is also used, and determination is performed only when a sound louder than a certain level is input. According to this scheme, in a case where there is a plurality of microphones in which the feature amount exceeds a threshold, each speech is detected even in a case of overlapping speeches.</p><p id="p-0083" num="0088">However, in the technology described in Document 4, since there is a large number of parameters that need to be adjusted such as a threshold and a smoothing coefficient, there is a problem in that it is necessary to adjust the parameters for each environment in order to achieve an operation of detecting all overlapping speeches while ignoring crosstalk. Furthermore, in the technology described in Document 4, in order to prevent one speech from being detected as a plurality of divided frames, processing corresponding to smoothing in the time direction is performed on the feature amount. However, as a side effect thereof, there is a problem in that, in a case where a speaker ends a speech before others during overlapping speeches, the speaker tends to be determined as still being speaking until everyone ends the speech. For example, describing with reference to the example illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the speaker SA ends the speech first in the overlapping speeches between the speaker SA and the speaker SB, and it is desirable that the frame A<b>2</b> be detected as the frame of the main speech. However, in the method of Document 4, detection of a terminal end is postponed until the speech of the speaker SB ends, and there is a possibility that the frame A<b>3</b> is detected as a frame of a main speech of the speaker SA.</p><p id="p-0084" num="0089">In the conventional technologies, it is difficult to detect only a main speech with high accuracy while ignoring crosstalk from voices recorded in an environment in which a microphone is assigned to each speaker as described above, and the conventional technologies have not been enough. In light of the above points, the embodiment of the present disclosure will be described.</p><heading id="h-0012" level="1">Embodiment</heading><p id="p-0085" num="0090">[Outline]</p><p id="p-0086" num="0091">First, an outline of the embodiment will be described. It is desirable to detect only the main speech for each microphone regardless of whether or not speeches overlap or regardless of the number of overlapping speeches. Thus, in the present embodiment, the following two elements are introduced.</p><p id="p-0087" num="0092">a) An issue of speech detection supporting overlapping speeches is regarded as a multi-label classification issue of giving a label indicating whether or not each speaker is speaking. A multi-label classifier is trained using sound data in which speeches, the number of which is zero or more and equal to or less than the number of microphones, overlap and zero or more labels corresponding thereto.</p><p id="p-0088" num="0093">b) Two or more and n or less microphone-recorded sounds are input to the multi-label classifier. (Where n is the number of microphones.)</p><p id="p-0089" num="0094">Each of them will be described below.</p><p id="p-0090" num="0095">When the number of microphones is n, that is, the number of the corresponding speakers is also n, the issue of speech detection supporting overlapping speeches can be interpreted as an issue of giving a label to a portion (timing) where each speaker is speaking. Since the label is different for every speaker, the number of labels to be given is at least zero (when everyone is silent) and at most n (when everyone is speaking). Such an issue of giving number variable labels is referred to as a multi-label classification issue.</p><p id="p-0091" num="0096">Details of the multi-label classification issue are described in Document 5 &#x201c;&#x2018;Mining Multi-label Data&#x2019;, G. Tsoumakas, I. Katakis, I. Vlahavas, Data Mining and Knowledge Discovery Handbook, Part 6, O. Maimon, L. Rokach (Ed.), Springer, 2nd edition, pp. 667-685, 2010&#x201d; and Document 6 &#x201c;&#x2018;Deep learning for multi-label classification&#x2019;, Read, J., Perez-Cruz, F., CoRR abs/1502.05988 (2014), https://arxiv.org/abs/1502.05988&#x201d;. Document 5 is a general explanation of each approach for resolving the multi-label classification issue, and Document 6 is an explanation for resolving the multi-label issue with a DNN.</p><p id="p-0092" num="0097">Any scheme can be used as the multi-label classifier, and the following description will be given on the premise that a neural network (NN) is used. Then, causing a classifier to memorize a correspondence relationship between input data and a label by using data is referred to as &#x201c;training&#x201d;, and detecting a main speech by using a trained classifier is referred to as &#x201c;inference&#x201d;. Moreover, a description will be given on the premise that a scheme called binary relevance, which is included in various multi-label classification approaches mentioned in Document 5, is used. Binary relevance is a scheme in which binary classifiers, each of which corresponds to one of n types of labels, are prepared so that zero or more and n or less labels are output. This corresponds to preparing binary discriminators of the same number as the microphones, the binary discriminators being for discriminating whether or not an input sound of each microphone is a main speech, in the present disclosure.</p><p id="p-0093" num="0098">Next, the above-described element b) will be described. A main speech and crosstalk differ in volume (power) and degree of reverberation, but neither of them is an absolute standard, and it is therefore difficult to distinguish between (perform correct classification into) the main speech and the crosstalk from an input signal of a single microphone. On the other hand, when input signals of a plurality of microphones are input to a classifier, the volume and reverberation can be perceived in relative terms, and the classification becomes easier.</p><p id="p-0094" num="0099">Two types of schemes are conceivable as to the number of microphones from which signals are to be input to a multi-label classifier and the number of discrimination results to be output. In the first scheme, as schematically illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, in a case where the number of microphones at the time of inference is n (microphones M<b>1</b> to Mn), n microphone-recorded sounds (or signals generated by applying a predetermined transform to the recorded sounds) are input to a multi-label classifier using a neural network, and n main speech discrimination results corresponding to the microphones are output from the multi-label classifier. In the second scheme, as schematically illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, training is performed as a two-input/two-output multi-label classifier at the time of training, and classifiers of the same number as pairs of microphones are prepared at the time of inference so that discrimination is performed for each microphone pair. Then, discrimination results are integrated for each microphone.</p><p id="p-0095" num="0100">In the scheme illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, it is necessary to train another multi-label classifier for each number of microphones. On the other hand, in the scheme illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, once a multi-label classifier has been trained, it is possible to support any number of microphones at the time of inference.</p><p id="p-0096" num="0101">The scheme illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is a two-input/two-output neural network. The input is sounds recorded by the two microphones MA and MB (or signals generated by applying a predetermined transform to the recorded sounds), and the output is frame information indicating whether or not the main speech is included in the sound recorded by each microphone (presence or absence of the main speech), and is, for example, binary discrimination results. The output of the neural network may be a continuous value such as a probability that a main speech is included, and the continuous value may be transformed into a binary value on the basis of a predetermined threshold in the subsequent processing.</p><p id="p-0097" num="0102"><figref idref="DRAWINGS">FIG. <b>3</b>C</figref> illustrates an example in which the neural network illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> is applied to a case where there are four microphones (microphones M<b>1</b> to M<b>4</b>), for example. Since there may be six patterns of pairs in the case of four microphones, six two-input/two-output neural networks illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> are applied. Each neural network may be the same.</p><p id="p-0098" num="0103">In a case where inference is performed for each pair of microphones, (n&#x2212;1) inference results are generated for one microphone. Thus, integration modules <b>3</b>A to <b>3</b>D that integrate inference results, one for each of the microphones, are prepared. The integration modules <b>3</b>A to <b>3</b>D output one discrimination result for each microphone. Specifically, the integration module <b>3</b>A outputs a main speech determination result for the microphone M<b>1</b>, the integration module <b>3</b>B outputs a main speech determination result for the microphone M<b>2</b>, the integration module <b>3</b>C outputs a main speech determination result for the microphone M<b>3</b>, and the integration module <b>3</b>D outputs a main speech determination result for the microphone M<b>4</b>.</p><p id="p-0099" num="0104">The following processing contents are exemplified as the processing performed by each integration module.</p><p id="p-0100" num="0105">(Integration Method 1)</p><p id="p-0101" num="0106">Each neural network outputs a binary value. Each integration module receives a binary value, which is an output of each neural network, and obtains a single discrimination result (binary value) by a logical sum. Alternatively, each integration module may receive a binary value, which is an output of each neural network, and obtain a single discrimination result (binary value) by a logical conjunction. Furthermore, each integration module may obtain a single discrimination result (binary value) in accordance with whether or not the number of true values exceeds a predetermined number.</p><p id="p-0102" num="0107">(Integration Method 2)</p><p id="p-0103" num="0108">Each neural network outputs a continuous value such as a probability that a main speech is included. Each integration module receives the continuous value, obtains a single continuous value by any of methods including maximum, minimum, average, and median, and then transforms the continuous value into a binary value on the basis of a predetermined threshold.</p><p id="p-0104" num="0109">(Integration Method 3)</p><p id="p-0105" num="0110">A two-input/two-output multi-label classifier and each integration module are connected by a weighted connection. An appropriate weight of each connection is calculated on the basis of data different from training data. In other words, a neural network having the same form as illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>C</figref> is constructed, and the weight is learned again. A single discrimination result is obtained on the basis of a result of the training.</p><p id="p-0106" num="0111">Next, a method of creating training data will be described. The training data in the present embodiment is constituted by a set of input data and a teacher label. The input data is a signal in which zero or more and n or less speeches overlap, and the output is a label corresponding thereto. The training data may be created by recording actual speeches in an environment in which each of n speakers is wearing a microphone, and giving a label to a frame of a main speech for each speaker later. Alternatively, the training data may be created by recording a single speech of each speaker with n microphones, and performing mixing on a computing machine later. Alternatively, instead of recording in an actual environment, an impulse response corresponding to a spatial transfer function from each speaker to each microphone may be prepared, and then a convolution operation may be performed with a separately prepared voice of a dry source and the impulse response.</p><p id="p-0107" num="0112">The n-input/n-output neural network illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> can be trained using training data, which has thus been prepared, as it is. On the other hand, in order to learn a two-input/two-output neural network illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, it is necessary to process data. This point will be described with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0108" num="0113"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a diagram illustrating an example of a recording environment. In the present example, the number of speakers and the number of microphones are both four, and each of the speakers is referred to as one of speakers <b>4</b>A to <b>4</b>D, and each of the microphones are referred to as one of microphones MA to MD. It is assumed that the microphone MA is assigned to the speaker <b>4</b>A, the microphone MB is assigned to the speaker <b>4</b>B, the microphone MC is assigned to the speaker <b>4</b>C, and the microphone MD is assigned to the speaker <b>4</b>D. It is assumed that input data and a teacher label for this environment are already prepared.</p><p id="p-0109" num="0114">In order to learn a two-input/two-output neural network, two microphones are selected from the four microphones. When the order is also taken into consideration, there are 12 possibilities. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, the microphone MA and the microphone MB are selected. For the microphones MA and MB, it is sufficient if all the speeches of the speakers other than the speakers <b>4</b>A and <b>4</b>B are treated as crosstalk, and it is not necessary to distinguish the speeches. Thus, the speaker <b>4</b>C and the speaker <b>4</b>D are collectively referred to as &#x201c;other speakers&#x201d;.</p><p id="p-0110" num="0115"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a diagram illustrating an example of each speaker's state (whether or not the speaker is speaking), and input data and a teacher label corresponding to the state. In the Input data column in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, capital letters A, B, and <b>0</b> represent the speaker <b>4</b>A, the speaker <b>4</b>B, and the other speakers, respectively, and small letters d and c represent a main speech and crosstalk, respectively. In Teacher label, &#x201c;1&#x201d; and &#x201c;0&#x201d; indicate whether or not the speaker corresponding to each microphone is speaking.</p><p id="p-0111" num="0116">For example, a case will be considered in which the speaker <b>4</b>A and the other speakers are speaking. Since a signal (Ad+Oc) in which a main speech (Ad) of the speaker <b>4</b>A and crosstalk (Oc) of the other speakers are mixed is input to the microphone MA, the teacher label corresponding to the microphone MA is &#x201c;1&#x201d; representing &#x201c;speaking&#x201d;. On the other hand, a signal (Ac+Oc) in which crosstalk (Ac) of the speaker <b>4</b>A and crosstalk (Oc) of the other speakers are mixed is input to the microphone MB, but the speaker <b>4</b>B is not speaking at that point of time, and thus the teacher label corresponding to the microphone MB is &#x201c;0&#x201d;.</p><p id="p-0112" num="0117">In free speech, the length of a speech and the length of silence between speeches vary, and thus the speaker's state changes frequently. Thus, teacher labels are given at a fine time granularity of, for example, 1/100 (0.01) seconds.</p><p id="p-0113" num="0118">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, two microphones, that is, the microphone MA and the microphone MB, are selected. For the other 11 patterns, training data is prepared in a similar manner. Moreover, training data is prepared for a variety of recording environments in a similar manner.</p><p id="p-0114" num="0119">[Configuration Example of Signal Processing Device]</p><p id="p-0115" num="0120"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a diagram for illustrating a configuration example of a signal processing device (signal processing device <b>100</b>) according to the embodiment. The present embodiment provides a system that automatically generates a transcription text for a voice recorded by a wearable distributed microphone, and provides a system including a plurality of microphones M<sub>1 </sub>to M<sub>n </sub>as a plurality of sound collection devices and the signal processing device <b>100</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Note that, in the following description, in a case where it is not necessary to distinguish the individual microphones, the microphones are collectively referred to as microphones M as appropriate. Each of the microphones M is assigned to one of a plurality of speakers. The plurality of speakers is at least two speakers. Therefore, at least two microphones (microphones M<b>1</b> and M<b>2</b>) are also prepared. Hereinafter, a signal recorded by a microphone is simply referred to as a signal. Note that a signal may be transmitted from each microphone M to the signal processing device <b>100</b> in a wired manner or in a wireless manner.</p><p id="p-0116" num="0121">The microphones M are assigned each to one speaker, and each microphone is either worn by the speaker or installed near the speaker with directivity toward the speaker. Note that, in the subsequent module, a processing result is generated for each of the signals recorded by the microphones. Hereinafter, a unit for each of the speakers corresponding to the microphones will be referred to as a &#x201c;track&#x201d;.</p><p id="p-0117" num="0122">The signal processing device <b>100</b> includes, for example, analog to digital (AD) conversion units <b>10</b>, a main speech detection unit <b>20</b>, a crosstalk reduction unit <b>30</b>, multipliers <b>40</b>, voice recognition units <b>50</b>, a recognition result concatenation unit <b>60</b>, and a text generation unit <b>70</b>. The AD conversion units <b>10</b> include an AD conversion unit <b>10</b><sub>1 </sub>to an AD conversion unit <b>10</b><sub>n </sub>for signals recorded by the corresponding microphones. Similarly, the multipliers <b>40</b> include a multiplier <b>40</b><sub>1 </sub>to a multiplier <b>40</b><sub>n</sub>, and the voice recognition units <b>50</b> include voice recognition units <b>50</b><sub>1 </sub>to <b>50</b><sub>n</sub>. Note that, in a case where it is not necessary to distinguish the individual AD conversion units, the AD conversion units are collectively referred to as the AD conversion units <b>10</b> as appropriate. Furthermore, in a case where it is not necessary to distinguish the individual multipliers, the multipliers are collectively referred to as the multipliers <b>40</b> as appropriate. Furthermore, in a case where it is not necessary to distinguish the individual voice recognition units, the voice recognition units are collectively referred to as the voice recognition units <b>50</b> as appropriate.</p><p id="p-0118" num="0123">The AD conversion units <b>10</b> convert signals in analog form acquired by the corresponding microphones M into signals of digital signals. The signals in digital form are supplied to each of the main speech detection unit <b>20</b> and the crosstalk reduction unit <b>30</b>.</p><p id="p-0119" num="0124">The main speech detection unit <b>20</b> detects whether or not a signal input to a sound collection device (microphone) assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker by a neural network, and outputs frame information indicating presence or absence of the main speech. For example, the main speech detection unit <b>20</b> inputs microphone-recorded sounds for all tracks, and outputs, for each track, &#x201c;1&#x201d; representing true if the main speech is included, or &#x201c;0&#x201d; representing false if the main speech is not included. The determination is made at a predetermined time granularity. Even in a case where the recorded sound includes a voice of a speaker other than the corresponding speaker, &#x201c;0&#x201d; representing false is output if the main speech is not included. Note that the main speech detection unit <b>20</b> may output time information (e.g., the start time and end time of the main speech) of a frame that includes the main speech. Furthermore, the main speech detection unit <b>20</b> may detect whether or not a signal input to a sound collection device assigned to each of three or more speakers includes a main speech that is a voice of the corresponding speaker on the basis of results of detection by a plurality of neural networks. Then, the main speech detection unit <b>20</b> detects whether or not the main speech that is the voice of the speaker corresponding to the sound collection device is included by integrating the results of detection by the plurality of neural networks.</p><p id="p-0120" num="0125">The crosstalk reduction unit <b>30</b> reduces components of voices of speakers other than the corresponding speaker included in a signal in digital form. The crosstalk reduction unit <b>30</b> inputs the microphone-recorded sounds for all tracks, and performs processing of reducing crosstalk while retaining a main speech for each track. Note that, in the present specification, &#x201c;reducing crosstalk&#x201d; includes not only removing the crosstalk but also suppressing the crosstalk (the crosstalk is reduced, if not completely removed). That is, in a case where the recorded sound is a mixture of the main speech and the crosstalk, the crosstalk is reduced and only the main speech is output. In a case where the recorded sound is only the main speech, the main speech is output as it is. Reducing the crosstalk is also referred to as emphasizing the main speech in some cases. Note that the signal processing device <b>100</b> may have a configuration without the crosstalk reduction unit <b>30</b>. In a case of such a configuration, a signal in digital form is supplied to the multiplier <b>40</b>.</p><p id="p-0121" num="0126">The multiplier <b>40</b> multiplies the output of the main speech detection unit <b>20</b>, the output of the crosstalk reduction unit <b>30</b>, and the output of the main speech detection unit <b>20</b>. By such multiplication, only a signal corresponding to a speech frame of each speaker, that is, a signal corresponding to the main speech, is sent to the voice recognition unit <b>50</b> in the subsequent stage.</p><p id="p-0122" num="0127">The voice recognition unit <b>50</b> is prepared for each track, and generates a recognition result from the voice of the main speech. Moreover, in order to facilitate concatenating the recognition results for all tracks in the subsequent processing, a set obtained by connecting, in addition to the voice recognition results, information specifying a track such as a track number (that is, specifying the speaker), speech start time and end time, and the like is generated and then sent to the recognition result concatenation unit <b>60</b>.</p><p id="p-0123" num="0128">The recognition result concatenation unit <b>60</b> concatenates the voice recognition results generated for each track. At the time of concatenation, the recognition result concatenation unit <b>60</b> arranges the recognition results in order of time, and further displays speaker information (name and the like) corresponding to the track number together with the recognition result in order to clarify by whom the remark has been made.</p><p id="p-0124" num="0129">The text generation unit <b>70</b> generates a text on the basis of a result of the concatenation by the recognition result concatenation unit <b>60</b>.</p><p id="p-0125" num="0130">Note that, instead of performing simple multiplication, the multiplier <b>40</b> may concatenate the output of the crosstalk reduction unit <b>30</b> and the output of the main speech detection unit <b>20</b>, and transmit the concatenated data to the voice recognition unit <b>50</b>. Specifically, the output data of the crosstalk reduction unit <b>30</b> is divided into fractions having a predetermined time length, and information as to whether or not a main speech is included, that is, information as to whether it is inside the main speech or outside the main speech is given as an attribute for each of the fractions. In this case, the voice recognition unit <b>50</b> performs voice recognition processing only on a portion included in received data, the portion being given the attribute &#x201c;inside the main speech&#x201d;.</p><p id="p-0126" num="0131">(Main Speech Detection Unit)</p><p id="p-0127" num="0132">Next, details of the main speech detection unit <b>20</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The main speech detection unit <b>20</b> includes, for example, short-term Fourier transform units <b>201</b>, an NN input data generation unit <b>202</b>, a neural network unit <b>203</b>, and a post-processing unit <b>204</b>. The short-term Fourier transform units <b>201</b> include short-term Fourier transform units <b>201</b><sub>1 </sub>to <b>201</b><sub>n </sub>so as to correspond to the AD conversion units <b>10</b><sub>1 </sub>to <b>10</b><sub>n</sub>. Note that, in a case where it is not necessary to distinguish the individual short-term Fourier transform units, the short-term Fourier transform units are collectively referred to as the short-term Fourier transform units <b>201</b> as appropriate.</p><p id="p-0128" num="0133">The short-term Fourier transform units <b>201</b> apply short-term Fourier transform (STFT) for each track to transform a signal into a complex spectrogram.</p><p id="p-0129" num="0134">The NN input data generation unit <b>202</b> transforms a complex spectrogram for each track into data that can be input to a neural network (NN). Specifically, the NN input data generation unit <b>202</b> performs, for example, the following processing.<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0000">    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0135">The NN input data generation unit <b>202</b> concatenates the complex spectrograms, one for each of the tracks, and transforms the concatenated complex spectrograms into a predetermined data format. The predetermined data format varies depending on the type of the neural network, and is, for example, a predetermined two-dimensional structure in a case of a fully connected NN, a predetermined three-dimensional structure in a case of a one-dimensional convolutional NN, and a predetermined four-dimensional structure in a case of a two-dimensional convolutional NN.</li>        <li id="ul0007-0002" num="0136">The NN input data generation unit <b>202</b> transforms a complex spectrogram into a real-valued spectrogram such as an amplitude spectrogram or a logarithmic spectrogram. Note that, in a case where a neural network that can handle complex numbers is used, transform into a real-valued spectrogram can be skipped.</li>        <li id="ul0007-0003" num="0137">The NN input data generation unit <b>202</b> performs normalization such as adjustment so that the root mean square of the spectrogram becomes 1.</li>        <li id="ul0007-0004" num="0138">In a case where the neural network unit <b>203</b> requests spectrograms for a plurality of frames, the NN input data generation unit <b>202</b> accumulates spectrograms until the number of the spectrograms reaches the number of the frames.</li>    </ul>    </li></ul></p><p id="p-0130" num="0139">The neural network unit <b>203</b> is a neural network trained on a correspondence relationship between input data and a teacher label with the use of training data as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>.</p><p id="p-0131" num="0140">The post-processing unit <b>204</b> transforms output data of the neural network unit <b>203</b> into a main speech detection result for each track. For example, the following processing is performed.<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0000">    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="0141">Processing of transforming output data (continuous value) of the neural network into a binary value indicating whether or not a speech is a main speech with the use of a threshold or the like.</li>        <li id="ul0009-0002" num="0142">Integration processing described with reference to <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>.</li>        <li id="ul0009-0003" num="0143">Smoothing processing on the output.</li>    </ul>    </li></ul></p><p id="p-0132" num="0144">A supplementary description of the smoothing processing described above will be given. Since a speech frame has a certain length, in a case where the output of a speech detector changes drastically in a short time, there is a high possibility that a discrimination result is incorrect and does not correspond to the real speech. Thus, for example, the number of continued discrimination results indicating that the main speech is being made in the time direction is counted, and in a case where the discrimination result has changed to &#x201c;outside the main speech&#x201d; before the count reaches a predetermined number of times, processing of ignoring the change (assuming &#x201c;outside the speech&#x201d;) is performed. Similarly, discrimination results &#x201c;outside the main speech&#x201d; are counted in a similar manner, and in a case where the determination result has changed to &#x201c;the main speech is being made&#x201d; before the count reaches the predetermined number of times, processing such as ignoring the change (assuming that the speech is still being made) is performed. Alternatively, instead of such rule-based smoothing, processing may be performed in which a low-pass filter is applied to output data (continuous value) of the neural network so that minute variations in the time direction are removed, and binarization is performed on the output after the application of the filter. Furthermore, instead of smoothing by the post-processing unit <b>204</b>, a layer having an effect of smoothing in the time direction, such as &#x201c;average pooling&#x201d;, may be inserted into the neural network unit <b>203</b>.</p><p id="p-0133" num="0145">Note that the main speech detection unit <b>20</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a configuration example based on the assumption that short-term Fourier transform is used for transform into a spectrogram, the spectrogram is processed, and data thus obtained is input. Furthermore, a result of discrimination on whether or not a speech is a main speech is generated for each frame of the spectrogram. Note that a signal may be directly input to the neural network, or processing in a time domain may be performed.</p><p id="p-0134" num="0146">(Crosstalk Reduction Unit)</p><p id="p-0135" num="0147">Next, details of the crosstalk reduction unit <b>30</b> will be described. <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> are diagrams for schematically illustrating processing in the crosstalk reduction unit <b>30</b>. The crosstalk reduction unit <b>30</b> reduces crosstalk included in a main speech illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>. By such reduction processing, crosstalk is reduced as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. Note that, although the entire crosstalk in a predetermined frame is reduced in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, only crosstalk in a frame overlapping a main speech may be reduced.</p><p id="p-0136" num="0148">Crosstalk reduction processing is processing of removing crosstalk while retaining a main speech in signal components included in a microphone recording signal, and any scheme can be used. For example, various types of beamforming processing may be performed with the microphones worn by the corresponding speakers regarded as a microphone array. Alternatively, a neural network similar to main speech detection may be used. Hereinafter, a description will be given using crosstalk reduction processing using a neural network as an example.</p><p id="p-0137" num="0149"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for illustrating a configuration example of the crosstalk reduction unit <b>30</b> according to the embodiment. The crosstalk reduction unit <b>30</b> includes, for example, short-term Fourier transform units <b>301</b>, an NN input data generation unit <b>302</b>, a neural network unit <b>303</b>, and a post-processing unit <b>304</b>. The short-term Fourier transform units <b>301</b> include short-term Fourier transform units <b>301</b><sub>1 </sub>to <b>301</b><sub>n </sub>so as to correspond to the AD conversion units <b>10</b><sub>1 </sub>to <b>10</b><sub>n</sub>. Note that, in a case where it is not necessary to distinguish the individual short-term Fourier transform units, the short-term Fourier transform units are collectively referred to as the short-term Fourier transform units <b>301</b> as appropriate. Inverse Fourier transform units <b>305</b> include inverse Fourier transform units <b>305</b><sub>1 </sub>to <b>305</b><sub>n </sub>so as to correspond to the AD conversion units <b>10</b><sub>1 </sub>to <b>10</b><sub>n</sub>. Note that, in a case where it is not necessary to distinguish the individual inverse Fourier transform units, the inverse Fourier transform units are collectively referred to as the inverse Fourier transform units <b>305</b> as appropriate.</p><p id="p-0138" num="0150">In a similar manner to the configuration illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the configuration of the crosstalk reduction unit <b>30</b> illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example based on the assumption that, instead of directly inputting a signal to a neural network, short-term Fourier transform is used for transform into a spectrogram, the spectrogram is processed, and data thus obtained is input.</p><p id="p-0139" num="0151">The short-term Fourier transform unit <b>301</b> transforms a signal into a complex spectrogram by applying a short-term Fourier transform (STFT) for each track.</p><p id="p-0140" num="0152">The NN input data generation unit <b>302</b> transforms a complex spectrogram for each track into data that can be input to a neural network. A specific example of the processing performed by the NN input data generation unit <b>302</b> is, for example, the same as that of the NN input data generation unit <b>202</b>. Note that, in a case where the input data for the neural network is the same in the main speech detection processing and the crosstalk reduction processing, the short-term Fourier transform unit and the NN input data generation unit may be commonalized.</p><p id="p-0141" num="0153">The neural network unit <b>303</b> is a neural network trained in advance on the following correspondence relationships. Note that details of a training method and training data will be described later.<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0000">    <ul id="ul0011" list-style="none">        <li id="ul0011-0001" num="0154">In a case where the input data contains a mixture of a main speech and crosstalk, the crosstalk is reduced and only the main speech is output.</li>        <li id="ul0011-0002" num="0155">In a case where the input data contains only a main speech, the main speech is output as it is.</li>        <li id="ul0011-0003" num="0156">In a case where the input data contains only crosstalk, silence is output.</li>        <li id="ul0011-0004" num="0157">In a case where the input data contains silence, silence is output.</li>    </ul>    </li></ul></p><p id="p-0142" num="0158">The output of the neural network unit <b>303</b> may be, instead of a signal, data that can be transformed into a signal, such as a spectrogram. Alternatively, a time-frequency mask having an action of reducing crosstalk and retaining a main speech may be output, and the mask may be applied to a complex spectrogram in the subsequent processing.</p><p id="p-0143" num="0159">The post-processing unit <b>304</b> transforms output data of the neural network unit <b>303</b> into a crosstalk reduction result for each track. The post-processing unit <b>304</b> performs, for example, the following processing.<ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0000">    <ul id="ul0013" list-style="none">        <li id="ul0013-0001" num="0160">In a case where the output data of the neural network unit <b>303</b> is an amplitude spectrogram, a phase of a complex spectrogram generated by the short-term Fourier transform unit <b>301</b> is applied to the amplitude spectrogram so that a complex spectrogram of the crosstalk reduction result is generated.</li>        <li id="ul0013-0002" num="0161">In a case where the output data of the neural network is a time-frequency mask, the mask is applied to a complex spectrogram generated by the short-term Fourier transform unit <b>301</b> so that a complex spectrogram of the crosstalk reduction result is generated.</li>        <li id="ul0013-0003" num="0162">Integration processing illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref> described later is performed.</li>    </ul>    </li></ul></p><p id="p-0144" num="0163">The inverse Fourier transform unit <b>305</b> applies short-time inverse Fourier transform to the complex spectrogram of the crosstalk reduction result for each track generated by the post-processing unit <b>304</b>, and generates a signal of the crosstalk reduction result for each track.</p><p id="p-0145" num="0164">Next, the neural network unit <b>303</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. As a neural network for crosstalk reduction, it is possible to apply a neural network similar to the neural network for main speech detection illustrated in <figref idref="DRAWINGS">FIGS. <b>3</b>A to <b>3</b>C</figref>, except that output data is a signal including only a clean main speech (not including crosstalk of another person).</p><p id="p-0146" num="0165">As for the numbers of inputs and outputs, there are two ways that can be considered in a similar manner to the case of the neural network for main speech detection. An n-input/n-output neural network illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> may be learned and used at the time of inference. On the other hand, a two-input/two-output neural network illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> is learned. Then, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>, at the time of inference, neural networks of the same number as the microphone pairs may be prepared (e.g., six neural networks), and integration modules <b>5</b>A to <b>5</b>D may integrate outputs of the corresponding neural networks for each of the microphones by a predetermined method, thereby generating a crosstalk reduction result for each microphone.</p><p id="p-0147" num="0166">The following processing contents are exemplified as the processing performed by each integration module.</p><p id="p-0148" num="0167">(Integration Method 1)</p><p id="p-0149" num="0168">Each neural network outputs an amplitude spectrogram. Each integration module receives it and obtains one spectrogram by applying any of methods including maximum, minimum, average, and median.</p><p id="p-0150" num="0169">(Integration Method 2)</p><p id="p-0151" num="0170">Each neural network outputs a time-frequency mask. Each integration module receives it and obtains one mask by applying any of methods including maximum, minimum, average, and median. Then, in the subsequent processing, the mask is applied to a complex spectrogram derived from a microphone-recorded sound.</p><p id="p-0152" num="0171">Next, a method of creating training data for reducing crosstalk will be described. The training data for crosstalk reduction includes a set of input data and teacher data. The input data may be the same as that for main speech detection, and is a signal in which zero or more and n or less speeches overlap. On the other hand, the teacher data is, unlike teacher data for detecting the main speech, silence or voice constituted by only a main speech (not including crosstalk of another person).</p><p id="p-0153" num="0172">In a similar manner to the training of the neural network for main speech detection, in order to learn a two-input/two-output neural network, it is necessary to process training data prepared for learning an n-input/n-output neural network. This point will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0154" num="0173">The recording environment assumed in <figref idref="DRAWINGS">FIG. <b>10</b></figref> is the same as the recording environment illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. However, since a clean main speech voice in which crosstalk of another person is not mixed is required as the teacher data, the speeches are mixed on a computing machine.</p><p id="p-0155" num="0174">The number of speakers is four (speakers <b>4</b>A to <b>4</b>D), the number of microphones is also four (microphones MA to MD). Two speakers are the speakers <b>4</b>A and <b>4</b>B, and the remaining speakers <b>4</b>C and <b>4</b>D are other speakers. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the &#x201c;speaker's state&#x201d; and &#x201c;input data&#x201d; are the same as those in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>. On the other hand, the &#x201c;teacher data&#x201d; is a clean main speech or silence.</p><p id="p-0156" num="0175">For example, consideration will be given to a case where the speaker <b>4</b>A and another speaker (one or both of the speaker <b>4</b>C and the speaker <b>4</b>D) are speaking. A signal (Ad+Oc) in which a main speech (Ad) of the speaker <b>4</b>A and crosstalk (Oc) of the other speaker are mixed is input to the microphone MA. A signal (Ac+Oc) in which crosstalk (Ac) of the speaker <b>4</b>A and crosstalk (Oc) of the other speaker are mixed is input to the microphone MB. In this case, (Ad) itself is desirable as a crosstalk reduction result for the speaker <b>4</b>A, and thus (Ad) is designated as teacher data for the speaker <b>4</b>A. On the other hand, since the speaker <b>4</b>B is not speaking at all, silence is designated as teacher data for the speaker <b>4</b>B. Note that, as data corresponding to silence, data having a value of completely zero may be used, or a sound in a quiet environment in which no one is speaking may be recorded so that the recorded signal can be used as silence data.</p><p id="p-0157" num="0176">On the other hand, in a case where the speaker <b>4</b>A and the speaker <b>4</b>B are both speaking, (Ad) and (Bd) are designated as teacher data for the speaker <b>4</b>A and teacher data for the speaker <b>4</b>B, respectively, regardless of whether or not the other speakers are speaking.</p><p id="p-0158" num="0177">[Operation Example of Signal Processing Device]</p><p id="p-0159" num="0178">Next, an operation example of the signal processing device <b>100</b> will be described with reference to the flowchart in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. In the present embodiment, a mode is assumed in which each piece of processing including main speech detection, crosstalk reduction, and voice recognition is performed in synchronization, and the unit of synchronization in this case is, for example, a frame of short-term Fourier transform. Furthermore, the processing of steps ST<b>11</b> to ST<b>22</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a loop of processing executed as long as the system is operating.</p><p id="p-0160" num="0179">When the processing is started, in step ST<b>11</b>, sound is collected by the microphone M worn by each speaker, and a signal is acquired. Then, the processing proceeds to step ST<b>12</b>.</p><p id="p-0161" num="0180">In step ST<b>12</b>, the analog signal acquired by the microphone M is converted into a signal in digital form by the AD conversion unit <b>10</b>. The length of the signal (the number of samples) acquired at one time is equal to a shift width in short-term Fourier transform described later. Then, the processing proceeds to step ST<b>13</b>.</p><p id="p-0162" num="0181">In step ST<b>13</b>, the main speech detection unit <b>20</b> performs main speech detection processing. Then, the processing proceeds to step ST<b>14</b>.</p><p id="p-0163" num="0182">In step ST<b>14</b>, the crosstalk reduction unit <b>30</b> performs crosstalk reduction processing. Note that the main speech detection processing and the crosstalk reduction processing may be performed in the illustrated order, may be performed in reverse order, or may be performed in parallel. Note that details of each piece of processing will be described later.</p><p id="p-0164" num="0183">The processing from step ST<b>15</b> to step ST<b>21</b> is a loop of processing performed for each track. Since the results of the main speech detection processing and the crosstalk reduction processing that are output are the same in number as the microphones, that is, the output corresponds to a plurality of tracks, the processing from step ST<b>16</b> to step ST<b>22</b> is performed for each track. Since the processing of each track is independent, the pieces of processing for the corresponding tracks may be executed in time series, or may be executed in parallel.</p><p id="p-0165" num="0184">In step ST<b>16</b>, the voice recognition unit <b>50</b> discriminates whether or not the speaker corresponding to the track is speaking on the basis of the result of the main speech detection. If the speaker is speaking, the processing proceeds to step ST<b>17</b>.</p><p id="p-0166" num="0185">In step ST<b>17</b>, the voice recognition unit <b>50</b> performs voice recognition decoding processing (voice recognition processing). That is, a signal corresponding to one frame of the crosstalk reduction result for the track is sent to the voice recognition unit <b>50</b>, and the voice recognition unit <b>50</b> updates a hypothesis of the recognition result.</p><p id="p-0167" num="0186">On the other hand, if it is discriminated in the processing of step ST<b>16</b> that the speaker is not speaking, the processing proceeds to step ST<b>18</b>. In step ST<b>18</b>, it is determined whether or not it is immediately after the end of the speech. Immediately after the end of the speech means a case in which the determination result of step ST<b>16</b> has previously branched to step ST<b>17</b>, but the determination result of step ST<b>16</b> branches to step ST<b>18</b> this time. In a case where it is immediately after the end of the speech, the processing proceeds to step ST<b>19</b>. In a case where it is not immediately after the end of the speech, the processing proceeds to step ST<b>21</b>, which is the end of the loop for the track.</p><p id="p-0168" num="0187">In step ST<b>19</b>, the voice recognition unit <b>50</b> confirms a recognition hypothesis. During decoding, there may be a plurality of recognition hypotheses (candidates for the recognition result) of the voice recognition decoding processing by the voice recognition unit <b>50</b>, and the recognition result having the highest score of the voice recognition at this point of time is confirmed to be the voice recognition result corresponding to the speech. Then, the processing proceeds to step ST<b>20</b>.</p><p id="p-0169" num="0188">In step ST<b>20</b>, the voice recognition result concatenation unit <b>60</b> performs, for example, a process such as rearranging the recognition results in the order in which the speeches have been made, using the start times and the end times of the speeches. Then, the text generation unit <b>80</b> generates text data on the basis of the processing result of the voice recognition result concatenation unit <b>60</b>. Since each track and a speaker are associated with each other, a text in which speaker information (name and the like) deduced from the track number is combined with the voice recognition result may be generated.</p><p id="p-0170" num="0189">In a case where the processing from step ST<b>16</b> to step ST<b>20</b> has been completed for all tracks, the processing proceeds to step ST<b>22</b>. In step ST<b>22</b>, it is determined whether to continue operating the system. In a case where the determination result indicates that the operation is to be ended, the operation is ended. In other cases, the processing returns to step ST<b>11</b>. The determination in step ST<b>22</b> is performed by an appropriate functional block of the signal processing device <b>100</b>.</p><p id="p-0171" num="0190">Next, details of the main speech detection processing will be described with reference to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>. In step ST<b>31</b>, the short-term Fourier transform units <b>201</b> perform short-term Fourier transform on the microphone-recorded sound of each track. In the short-term Fourier transform, as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, pieces of a fixed length are cut out from a waveform (see <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>) of the microphone-recorded sound collected by the microphone M and converted into a digital form by the AD conversion unit <b>10</b>, and a window function such as a Hanning window or a Hamming window is applied to those pieces of the fixed length (see <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>). This cut-out unit is referred to as a frame. By applying short-term Fourier transform to data for one frame, X(1, t) to X(K, t) are obtained as observation signals in a time-frequency domain (see <figref idref="DRAWINGS">FIG. <b>13</b>C</figref>). Where t represents a frame number, and K represents the total number of frequency bins. The amount of movement of a frame per one time is referred to as a shift width. There may be an overlap between the cut-out frames, so that a change in the signal in the time-frequency domain becomes smooth between consecutive frames. X(1, t) to X(K, t), which are data for one frame, are referred to as spectra, and a data structure in which a plurality of spectra is arranged in the time direction is referred to as a spectrogram (<figref idref="DRAWINGS">FIG. <b>13</b>C</figref>). In <figref idref="DRAWINGS">FIG. <b>13</b>C</figref>, a horizontal axis and a vertical axis represent the frame number and the frequency bin number, respectively. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b>C</figref>, each of three spectra is generated. After the processing of step ST<b>31</b> has been performed, the processing proceeds to step ST<b>32</b>.</p><p id="p-0172" num="0191">In step ST<b>32</b>, a complex spectrogram (or complex spectra for one frame) generated by short-term Fourier transform is transformed into input data for the neural network by the NN input data generation unit <b>202</b>. Then, the processing proceeds to step ST<b>33</b>.</p><p id="p-0173" num="0192">In step ST<b>33</b>, neural network inference is performed. The neural network inference is processing of generating output data by inputting the input data generated in step ST<b>32</b> to the neural network, and performing layer-by-layer forward propagation. Then, the processing proceeds to step ST<b>34</b>.</p><p id="p-0174" num="0193">In step ST<b>34</b>, the post-processing unit <b>204</b> performs post-processing. For example, the post-processing unit <b>204</b> performs processing of transforming the output data of the neural network into the form of a main speech detection result.</p><p id="p-0175" num="0194">Next, details of the crosstalk reduction processing will be described with reference to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>. In the processing from step ST<b>41</b> to step ST<b>43</b>, for example, processing similar to the processing from step ST<b>31</b> to step ST<b>33</b> described above is performed. Following the processing of step ST<b>43</b>, processing of step ST<b>44</b> is performed.</p><p id="p-0176" num="0195">In step ST<b>44</b>, the post-processing unit <b>304</b> transforms the output data of the neural network into a complex spectrogram corresponding to a crosstalk reduction result. The complex spectrogram is generated for each track. Then, the processing proceeds to step ST<b>45</b>.</p><p id="p-0177" num="0196">In step ST<b>45</b>, the inverse Fourier transform unit <b>305</b> transforms the complex spectrogram of the crosstalk reduction result into a sound waveform. Note that, depending on the type of the voice feature amount used in the voice recognition, it is possible to omit the inverse Fourier transform, and output a complex spectrogram or an amplitude spectrogram to the voice recognition unit <b>50</b>.</p><p id="p-0178" num="0197">[Effects Obtained by Present Embodiment]</p><p id="p-0179" num="0198">According to the present embodiment described above, for example, the following effects can be obtained.</p><p id="p-0180" num="0199">The main speech detection processing, crosstalk reduction processing, voice recognition, and the like are performed on a signal recorded by a microphone assigned to each speaker, and text data can be automatically generated on the basis of the result thereof.</p><p id="p-0181" num="0200">At a timing when speeches of a plurality of persons overlap, voice recognition results of the same number as the overlapping speeches are generated by the function of main speech detection corresponding to the overlapping speeches. Moreover, since crosstalk reduction is applied to each speech and crosstalk components are removed, the voice recognition can be performed with high accuracy also for overlapping speeches. That is, it is possible to generate correct recognition results also for overlapping speeches while satisfying the feature of the system <b>1</b> in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> that a single recognition result is generated for a single speech.</p><p id="p-0182" num="0201">The main speech detection processing according to the present embodiment uses the same technology to deal with both a single speech and overlapping speeches. Moreover, actual sound data is used for training. It is therefore possible to estimate a speech frame with high accuracy as compared with a case where determination results of a plurality of technologies are integrated on a rule basis.</p><p id="p-0183" num="0202">The present embodiment uses actual voice data or the like for training, and can be applied without parameter adjustment in a variety of environments as long as the environment is covered by training data. Since parameter adjustment is not required, the present embodiment is also superior to the technology described in Document 4.</p><p id="p-0184" num="0203">The present embodiment can improve precision of a generated transcription text (text data), and can therefore reduce labor required for manual modification.</p><heading id="h-0013" level="1">Modified Examples</heading><p id="p-0185" num="0204">Although the embodiment of the present disclosure has been specifically described above, contents of the present disclosure are not limited to the above-described embodiment, and various modifications may be made on the basis of the technical idea of the present disclosure. Note that, in the description of modified examples, the same or equivalent configurations in the above description are denoted by the same reference numerals, and redundant description is omitted as appropriate.</p><heading id="h-0014" level="1">First Modified Example</heading><p id="p-0186" num="0205">A first modified example is a technology related to simultaneous inference of main speech detection and crosstalk reduction. The configuration example of the signal processing device <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a mode in which the main speech detection processing and the crosstalk reduction processing are performed by different modules. In the present modified example, both are performed by a single module. Thus, sounds recorded by a plurality of microphones are input, a neural network that performs main speech detection and crosstalk reduction at the same time is learned, and the neural network is used at the time of inference.</p><p id="p-0187" num="0206"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram for illustrating a configuration example of a signal processing device (signal processing device <b>100</b>A) according to the present modified example. The signal processing device <b>100</b>A is different from the signal processing device <b>100</b> in that the main speech detection unit and the crosstalk reduction unit are integrated to form a main speech detection/crosstalk reduction unit <b>80</b>. The output of the main speech detection/crosstalk reduction unit <b>80</b> is the same as the output of the main speech detection unit <b>20</b> and the output of the crosstalk reduction unit <b>30</b>.</p><p id="p-0188" num="0207"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram for illustrating a configuration example of the main speech detection/crosstalk reduction unit <b>80</b>. Schematically, the configuration of the main speech detection/crosstalk reduction unit <b>80</b> is a configuration in which the configuration of the main speech detection unit <b>20</b> illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> and the configuration of the crosstalk reduction unit <b>30</b> are combined.</p><p id="p-0189" num="0208">The short-term Fourier transform unit <b>301</b> and the NN input data generation unit <b>302</b> are as already described above. A neural network unit <b>801</b> has outputs of two systems. One is an output related to a crosstalk reduction result, and this output is sent to a crosstalk reduction post-processing unit <b>802</b>. The other is an output related to main speech detection, and this output is sent to a main speech detection post-processing unit <b>803</b>.</p><p id="p-0190" num="0209">The function of the crosstalk reduction post-processing unit <b>802</b> is the same as that of the post-processing unit <b>304</b>. The output of the crosstalk reduction post-processing unit <b>802</b> is sent to the inverse Fourier transform unit <b>305</b> in the subsequent stage, and is subjected to inverse Fourier transform. The processing performed by the inverse Fourier transform unit <b>305</b> is as already described above.</p><p id="p-0191" num="0210">The function of the main speech detection post-processing unit <b>803</b> is the same as that of the post-processing unit <b>204</b>. A multiplier <b>804</b> integrates a crosstalk reduction result and a main speech detection result, and has the same function as the multiplier <b>40</b>.</p><p id="p-0192" num="0211">Training a single neural network to output a plurality of types of inference results (a main speech detection result and a crosstalk reduction result in the present modified example) is called multi-task learning. Examples of a neural network trained by multi-task learning are illustrated in <figref idref="DRAWINGS">FIG. <b>17</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>17</b>B</figref>. As a neural network, it is possible to use only one neural network of an n-input/2n-output type illustrated in <figref idref="DRAWINGS">FIG. <b>17</b>A</figref>, and it is also possible to combine and use a required number of neural networks of a two-input/four-output type illustrated in <figref idref="DRAWINGS">FIG. <b>17</b>B</figref>.</p><p id="p-0193" num="0212">In the n-input/2n-output type, n of the outputs are related to crosstalk reduction results, and the remaining n outputs are related to main speech detection results. On the other hand, in the two-input/four-output type, two of the outputs are a crosstalk reduction result of the microphone MA and a crosstalk reduction result of the microphone MB, and the remaining two outputs are main speech detection results for signals recorded by the corresponding microphones. At the time of inference, crosstalk reduction results are subjected to integration processing similar to that of the integration module <b>5</b>A and the like, and main speech detection results are subjected to integration processing similar to that of the integration module <b>3</b>A and the like.</p><p id="p-0194" num="0213"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is an example of training data for multi-task learning. This is generated by merging the training data for main speech detection illustrated in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> and the training data for crosstalk reduction illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. A neural network for multi-task learning has a plurality of types (here, two types) of output nodes. One of the output nodes is trained using &#x201c;teacher data for main speech detection&#x201d;, and the other is trained using &#x201c;teacher data for crosstalk reduction&#x201d;, so that a neural network that outputs inference results for both by one inference can be constructed.</p><p id="p-0195" num="0214">An operation example of the signal processing device <b>100</b>A is substantially similar to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The difference is that the main speech detection processing and the crosstalk reduction processing are integrated to form main speech detection/crosstalk reduction processing.</p><heading id="h-0015" level="1">Second Modified Example</heading><p id="p-0196" num="0215">In the signal processing device <b>100</b> according to the embodiment, the main speech detection unit <b>20</b> and the crosstalk reduction unit <b>30</b> operate independently and in parallel. However, considering that the voice recognition performed in the subsequent stage is performed only for a frame detected as a main speech, it is not efficient to reduce crosstalk outside the main speech frame. Thus, the present modified example reduces a processing load by adopting a configuration in which crosstalk reduction is performed only for a signal of a frame detected as a main speech, that is, a frame that includes a main speech.</p><p id="p-0197" num="0216"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a diagram for illustrating a configuration example of a signal processing device (signal processing device <b>100</b>B) according to the present modified example. The signal processing device <b>100</b>B is different from the signal processing device <b>100</b> in terms of the configuration in that an input signal division unit <b>81</b> is included, that the output of the AD conversion unit <b>10</b> and the output of the main speech detection unit <b>20</b> are supplied to the input signal division unit <b>81</b>, and that a crosstalk reduction unit is prepared for each track (that crosstalk reduction units <b>31</b><sub>1 </sub>to <b>31</b><sub>n </sub>are included).</p><p id="p-0198" num="0217">The operation of the input signal division unit <b>81</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>20</b></figref>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, in a similar manner to the example illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the microphone MA is assigned to a speaker S<b>1</b>, who is one of two speakers (speakers S<b>1</b> and S<b>2</b>), and the microphone MB is assigned to the speaker S<b>2</b>. Then, it is assumed that the speaker S<b>1</b> makes a speech first, the speaker S<b>1</b> makes a speech again next, and then the speaker S<b>2</b> starts to speak in the middle of the speech. It is assumed that each speech frame has been correctly detected by main speech detection processing, and the detected speech frames are a frame D<b>1</b>, a frame D<b>2</b>, and a frame E<b>1</b>.</p><p id="p-0199" num="0218">The input signal division unit <b>81</b> divides microphone input signals for all tracks for each speech frame. For example, as for the frame D<b>1</b>, input signals of the same time range are cut out not only for the microphone MA corresponding to the speaker but also for the microphone MB. Then, the input signals for all the microphones and information regarding the frame D<b>1</b> (track number, speech start time and end time, and the like) are included in a set SE<b>1</b>, and the set SE<b>1</b> is sent to the crosstalk reduction unit <b>31</b><sub>1 </sub>corresponding to a track <b>1</b>.</p><p id="p-0200" num="0219">Similarly, also for the frames D<b>2</b> and E<b>1</b>, the input signal division unit <b>81</b> generates sets such as sets SE<b>2</b> and SE<b>3</b>, respectively, and outputs the sets to the crosstalk reduction units of the corresponding tracks.</p><p id="p-0201" num="0220">The crosstalk reduction units <b>31</b> generate a crosstalk reduction result for each frame. For example, while the crosstalk reduction unit <b>30</b> has n inputs and n outputs (n is the number of microphones), the crosstalk reduction units <b>31</b> have n inputs and 1 output. That is, crosstalk reduction processing is performed only on a track in which a main speech has been detected.</p><p id="p-0202" num="0221">In a case where the crosstalk reduction of the present modified example is performed by a neural network, an n-input/1-output neural network may be directly learned. Alternatively, (n&#x2212;1) two-input/two-output neural networks illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>B</figref> may be combined. As an example of the latter, an example in which the number of microphones is four is illustrated in <figref idref="DRAWINGS">FIG. <b>21</b></figref>.</p><p id="p-0203" num="0222">In <figref idref="DRAWINGS">FIG. <b>21</b></figref>, it is assumed that crosstalk reduction is performed on a main speech of the speaker corresponding to the microphone M<b>1</b>. Since there are three ways the microphone M<b>1</b> can be paired with one of the other microphones, three two-input/two-output neural networks are prepared, and input signals of the microphone pairs are input to the respective neural networks. One of the two outputs of each neural network is a crosstalk reduction result for the microphone M<b>1</b>, and the crosstalk reduction results are input to an integration module <b>8</b> so that a single crosstalk reduction result is obtained. The method of integration is similar to the method performed by the integration module <b>5</b>A and the like. The other output is rejected because it is not related to the microphone M<b>1</b>.</p><p id="p-0204" num="0223">An operation example of the signal processing device <b>100</b>B according to the present modified example will be described with reference to the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>. Differences from the flowchart illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> are that the input signal division unit <b>81</b> performs input signal division processing, and the crosstalk reduction processing is performed at a different timing. Hereinafter, an operation example of the signal processing device <b>100</b>B will be described focusing on such differences.</p><p id="p-0205" num="0224">In step ST<b>15</b>, it is discriminated whether or not the speaker corresponding to the track is speaking, from a result of main speech detection. If the speaker is speaking, the processing proceeds to step ST<b>51</b>. If the speaker is not speaking, the processing proceeds to step ST<b>18</b>.</p><p id="p-0206" num="0225">In step ST<b>51</b>, the input signal division unit <b>81</b> performs input signal division processing. Since the contents of the input signal division processing have already been described with reference to <figref idref="DRAWINGS">FIG. <b>20</b></figref>, redundant description will be omitted as appropriate. Schematically, input signals of all the microphones and information regarding a main speech frame are included in a set, and the set is sent to the crosstalk reduction unit <b>31</b> of the corresponding track in the processing. Then, the processing proceeds to step ST<b>52</b>.</p><p id="p-0207" num="0226">In step ST<b>52</b>, each of the crosstalk reduction units <b>31</b> generates a crosstalk reduction result for one track from the input signals of all the microphones. Since the subsequent processing such as voice recognition decoding processing is similar to that of the embodiment, redundant description is omitted.</p><heading id="h-0016" level="1">Third Modified Example</heading><p id="p-0208" num="0227">Next, a third modified example will be described. The present modified example relates to a method for reducing the calculation amount in a case where there are three or more microphones. The processing according to the present modified example can be applied to both main speech detection processing and crosstalk reduction processing. In the following description, the main speech detection processing will be taken as an example.</p><p id="p-0209" num="0228">In the embodiment described above, with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a method has been described in which a two-input/two-output neural network is learned, and then the neural networks of the same number as the microphone pairs are prepared at the time of inference, so that it is possible to support any number of microphones at the time of inference. However, there are about n&#xd7;(n&#x2212;1)/2 microphone pairs, and a calculation cost is substantially proportional to the square (n<sup>2</sup>) of the number of microphones. Thus, as the number of microphones increases, the processing load rapidly increases.</p><p id="p-0210" num="0229">Regarding the crosstalk reduction processing, the calculation cost can be reduced by the above-described second modified example, but such a modified example cannot be applied to the main speech detection processing. Thus, in the present modified example, the calculation cost of the main speech detection processing is reduced to O(n) with the use of a virtual far-field microphone described below.</p><p id="p-0211" num="0230">A virtual far-field microphone will be described with reference to <figref idref="DRAWINGS">FIG. <b>23</b></figref>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, there are three or more speakers (S<b>1</b> to Sn), and three or more microphones M<b>1</b> to Mn are microphones assigned one to each speaker. Here, it is hypothesized that there is another microphone FM at a position away from any of the speakers. Hereinafter, such a microphone is referred to as the far-field microphone FM.</p><p id="p-0212" num="0231">Since the far-field microphone FM is away from any of the speakers, all speeches are recorded as crosstalk. In a case where there is such a far-field microphone FM, it is possible to relatively easily determine whether or not a speech is a main speech. That is, a sound recorded by one of the microphones M<sub>1 </sub>to M<sub>n </sub>assigned to the speaker is compared with a sound recorded by the far-field microphone FM. In a case where both are similar in terms of the volume, the degree of reverberation, and the like, the sound recorded by the microphone is either crosstalk or silence (background noise when no one is speaking), and is not a main speech in any case. On the other hand, in a case where the sound recorded by the microphone assigned to the speaker is clearer (the sound is louder and contains less reverberation) than the sound recorded by the far-field microphone, there is a high possibility that a main speech has been input.</p><p id="p-0213" num="0232">In this determination method, since pairs of microphones are limited pairs in which the far-field microphone FM is paired with one of the other microphones, there are n pairs. Therefore, the calculation cost is O(n), and the calculation cost can be reduced as compared with O(n {circumflex over (&#x2003;)}2), which is the calculation cost in the case of <figref idref="DRAWINGS">FIG. <b>3</b>C</figref>.</p><p id="p-0214" num="0233">In an actual recording environment, the far-field microphone FM generally does not exist. Thus, in the present modified example, it is necessary to generate a signal of a virtual far-field microphone from microphones assigned to speakers. A main speech detector based on this assumption will be described with reference to <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0215" num="0234">A detector <b>88</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>24</b></figref> is an n-input/1-output detector, and the output is a main speech detection result of a specific track (microphone). In an actual system configuration, as will be described later, a total of n main speech detectors, each of which corresponds to a track, are prepared and used as a main speech detection unit with n inputs and n outputs as a whole.</p><p id="p-0216" num="0235">The microphone M<b>1</b> is a microphone to be a target of main speech detection, and the microphones M<b>2</b> to Mn are other microphones. A sound recorded by the microphone M<b>1</b> is sent as it is (note that AD conversion or short-term Fourier transform may be performed) to one of the inputs of the main speech detection neural network <b>88</b>A. On the other hand, all sounds recorded by the microphones are input to a virtual far-field microphone signal generation unit <b>88</b>B, and the virtual far-field microphone signal generation unit <b>88</b>B generates a signal of a virtual far-field microphone. The generated signal of the virtual far-field microphone is sent to another input of the main speech detection neural network <b>88</b>A. The main speech detection neural network <b>88</b>A performs inference by using two types of input data, and obtains a main speech detection result corresponding to the microphone M<b>1</b>.</p><p id="p-0217" num="0236">As a method by which the virtual far-field microphone signal generation unit <b>88</b>B generates a signal of a virtual far-field microphone, a plurality of methods described below can be exemplified.</p><p id="p-0218" num="0237">a) Each microphone-recorded sound is transformed into an amplitude spectrogram, and a minimum value is obtained between microphones at each temporal frequency.</p><p id="p-0219" num="0238">b) Recorded sounds are averaged among all microphones.</p><p id="p-0220" num="0239">c) This is a modification of b) described above, in which recorded sounds are averaged among microphones (microphones M<b>2</b> to Mn in <figref idref="DRAWINGS">FIG. <b>24</b>A</figref>) that are not to be targets of main speech detection.</p><p id="p-0221" num="0240">A supplementary description will be given for the method a) described above. In a case where at least one of n persons is not speaking at a certain timing, the microphone assigned to that person collects only crosstalk, and that microphone can be used as a virtual far-field microphone. In a case where a main speech of one microphone and crosstalk of another microphone are derived from the same speech of the same speaker, it can be hypothesized that the crosstalk has a lower volume than the main speech. There is therefore a high possibility that a spectrogram obtained by adopting a minimum value among the microphones at each temporal frequency of an amplitude spectrogram is constituted only by the components of the crosstalk.</p><p id="p-0222" num="0241">A supplementary description will be given also for the method b) and the method c) described above. In a case where it can be hypothesized that a relatively small number of speakers out of n speakers are speaking at the same time, it is possible to generate a signal relatively close to crosstalk by averaging recorded sounds among the microphones. Moreover, by excluding the microphone to be a target of main speech detection (in the example illustrated in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, the microphone M<b>1</b>) from the average, it is possible to generate a signal close to crosstalk regardless of whether or not the speaker corresponding to the microphone M<b>1</b> is speaking.</p><p id="p-0223" num="0242"><figref idref="DRAWINGS">FIG. <b>24</b>B</figref> illustrates an example in which the main speech detection neural networks <b>88</b>A of the same number as the microphones are prepared to constitute a main speech detector with n inputs and n outputs, and illustrates an example of n=4. Since the virtual far-field microphone signal generation unit <b>88</b>B can be shared, only one virtual far-field microphone signal generation unit is prepared, and the output thereof is sent to each neural network.</p><heading id="h-0017" level="1">Fourth Modified Example</heading><p id="p-0224" num="0243">The present modified example relates to a method of displaying, in an easily viewable manner, a recognition result of a voice recorded in an environment in which there is a possibility that speeches of a plurality of speakers overlap.</p><p id="p-0225" num="0244">In the present disclosure, a system that automatically or semi-automatically generates a transcription text of a conference, a broadcast program, or the like is assumed as one mode. In the transcription text, it is desirable that recognition results corresponding to speeches of the corresponding speakers be presented in the order of the remarks.</p><p id="p-0226" num="0245">When concatenating voice recognition results generated for each track, the recognition result concatenation unit <b>60</b> according to the embodiment rearranges the voice recognition results in order of time by using the start time and the end time of each speech. However, in a case where there is an overlap between speeches, the order of recognition results arranged in accordance with the start time or the end time may be different from the order of the speeches.</p><p id="p-0227" num="0246">Specifically, the following conversation is conceivable. Note that Hanako and Taro are names of speakers.</p><p id="p-0228" num="0247">(Hanako) &#x201c;It was raining heavily earlier.&#x201d;</p><p id="p-0229" num="0248">(Taro) &#x201c;Really?&#x201d;</p><p id="p-0230" num="0249">(Hanako) &#x201c;But the rain stopped while I was looking for an umbrella.&#x201d;</p><p id="p-0231" num="0250">Note that, in the conversation described above, it is assumed that the remark of Taro has been made in response to Hanako's remark &#x201c;It was raining heavily&#x201d;. In this case, the order of description described above is appropriate as a transcription text.</p><p id="p-0232" num="0251">However, for example, in a case where Hanako has made the first and second speeches with almost no interval between them, almost the entire speech of Taro overlaps with the speeches of Hanako. In the main speech detection according to the present disclosure, even in a case where speeches of two persons overlap as described above, each of the speeches can be detected as a different speech frame, but there is a high possibility that Hanako's speeches are detected as one long frame. In a case where crosstalk reduction and voice recognition are performed for each frame, and the recognition results are arranged in order of speech start time, the following transcription text is generated.</p><p id="p-0233" num="0252">(Hanako) &#x201c;It was raining heavily earlier, but the rain stopped while I was looking for an umbrella.&#x201d;</p><p id="p-0234" num="0253">(Taro) &#x201c;Really?&#x201d;</p><p id="p-0235" num="0254">In the transcription text described above, although the voice recognition results themselves are correct, the remark of Taro looks as if the remark has been made in response to &#x201c;the rain stopped&#x201d;, and the nuances of the conversation have changed.</p><p id="p-0236" num="0255">The cause of this problem is that the length of the frame detected by the main speech detection does not necessarily match the unit described as one speech in the transcription text. In a speech detection technology that supports overlapping speeches, there is a stronger tendency for such a problem to occur.</p><p id="p-0237" num="0256">Thus, as a method of coping with such a problem, the present modified example adopts scroll display of speech frames and recognition results, for example.</p><p id="p-0238" num="0257"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates an example of the scroll display. In <figref idref="DRAWINGS">FIG. <b>25</b></figref>, a display unit <b>91</b> having a rectangular shape is illustrated. The display unit <b>91</b> may be integrated with the signal processing device <b>100</b> or the like, or may be a separate body. Alternatively, the display unit <b>91</b> may have a form of a partial area (window or the like) in a display device (display or the like). The display unit <b>91</b> displays names <b>92</b> of speakers. Specifically, &#x201c;Hanako&#x201d;, which is a name <b>92</b>A, and &#x201c;Taro&#x201d;, which is a name <b>92</b>B, are displayed on the display unit <b>91</b>. In a case where the names of the speakers can be estimated from the track numbers, the names <b>92</b> of the speakers are displayed as described above. In a case where the names of the speakers are unknown, &#x201c;speaker A&#x201d;, &#x201c;speaker B&#x201d;, or the like may be displayed.</p><p id="p-0239" num="0258">A dotted line L<b>1</b> is displayed on the right side of &#x201c;Hanako&#x201d;, which is the name <b>92</b>A, and a dotted line L<b>2</b> is displayed on the right side of &#x201c;Taro&#x201d;, which is the name <b>92</b>B. The line L<b>1</b> and the line L<b>2</b> represent the corresponding tracks, and the horizontal direction represents time. Speech frames and recognition results described later scroll along these lines. Note that, instead of the dotted lines, rectangular areas having a predetermined height may be displayed, and speech frames and recognition results may be superimposed and displayed inside the rectangular areas.</p><p id="p-0240" num="0259">A solid line L<b>3</b> displayed so as to be superimposed on the line L<b>1</b> and a solid line L<b>4</b> displayed so as to be superimposed on the line L<b>2</b> are main speech frames of the corresponding tracks. The left end of each of the line L<b>3</b> and the line L<b>4</b> represents the starting end of the speech, and the right end represents the terminal end. In <figref idref="DRAWINGS">FIG. <b>25</b></figref>, it is assumed that the speech of &#x201c;Hanako&#x201d; is detected as one long frame, and the speech of &#x201c;Taro&#x201d; is detected as a frame that entirely overlaps with Hanako.</p><p id="p-0241" num="0260">A voice recognition result <b>93</b>A corresponding to a speech frame is displayed above the line L<b>3</b>, and a voice recognition result <b>93</b>B corresponding to a speech frame is displayed above the line L<b>4</b>. It is desirable that the voice recognition result be displayed in association with the speech frame. Instead of being displayed above the displayed frame as illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, the voice recognition result may be displayed so as to be superimposed on the displayed frame. It is desirable to display the voice recognition result so as to be the same in length as the speech frame by adjusting a font size, character spacing, and the like.</p><p id="p-0242" num="0261">The displayed speech frame and the voice recognition result are scrolled automatically or manually while maintaining a correspondence relationship. Automatic scrolling is, for example, as follows.</p><p id="p-0243" num="0262">a) A live (not recorded) speech is processed in real time, and a frame and a voice recognition result are scrolled in synchronization with the speech.</p><p id="p-0244" num="0263">b) A previously recorded signal is processed, and a frame and a recognition result are scrolled in synchronization with reproduction of the recorded signal.</p><p id="p-0245" num="0264">On the other hand, manual scrolling means that, for example, a horizontally long screen constituted by processing results (e.g., in a case of a one-hour program, speech frames and recognition results for one hour) for the whole recorded signals is created, and a user selectively displays a part of the screen by using a slide bar or the like.</p><p id="p-0246" num="0265">In a case where a frame and a recognition result are displayed together as described above, the timing each speech has occurred in overlapping speeches becomes clearer. In other words, it becomes easier to grasp in reaction to which part of the immediately preceding speech each remark has been made.</p><p id="p-0247" num="0266">For example, in the display example illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, since it can be seen that the speech of &#x201c;Taro&#x201d; starts in the middle of &#x201c;It was raining heavily&#x201d;, which is a speech of &#x201c;Hanako&#x201d;, it can be seen that the remark of &#x201c;Taro&#x201d; has been made in response to &#x201c;It was raining heavily&#x201d;, not &#x201c;the rain stopped&#x201d;.</p><heading id="h-0018" level="1">Other Modified Examples</heading><p id="p-0248" num="0267">The configurations, methods, steps, shapes, materials, numerical values, and the like described in the above-described embodiment and modified examples are merely examples, and configurations, methods, steps, shapes, materials, numerical values, and the like different from those described above may be used as necessary, or may be replaced with known ones. Furthermore, the configurations, methods, steps, shapes, materials, numerical values, and the like in the embodiment and the modified examples can be combined with each other within a range in which no technical contradiction occurs.</p><p id="p-0249" num="0268">Note that the contents of the present disclosure are not to be construed as being limited by the effects exemplified in the present specification.</p><p id="p-0250" num="0269">The present disclosure can also adopt the following configurations.</p><p id="p-0251" num="0270">(1)</p><p id="p-0252" num="0271">A signal processing device including:</p><p id="p-0253" num="0272">a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</p><p id="p-0254" num="0273">(2)</p><p id="p-0255" num="0274">The signal processing device according to (1), in which</p><p id="p-0256" num="0275">the main speech detection unit detects whether or not the signal includes the main speech even in a case where a voice of a speaker other than the corresponding speaker is included.</p><p id="p-0257" num="0276">(3)</p><p id="p-0258" num="0277">The signal processing device according to (1) or (2), in which</p><p id="p-0259" num="0278">the main speech detection unit outputs time information of a frame that includes the main speech.</p><p id="p-0260" num="0279">(4)</p><p id="p-0261" num="0280">The signal processing device according to any one of (1) to (4), in which</p><p id="p-0262" num="0281">the main speech detection unit detects whether or not a signal input to a sound collection device assigned to each of three or more speakers includes a main speech that is a signal of the corresponding speaker on the basis of results of detection by a plurality of neural networks.</p><p id="p-0263" num="0282">(5)</p><p id="p-0264" num="0283">The signal processing device according to (4), in which</p><p id="p-0265" num="0284">the main speech detection unit detects whether or not the signal includes a main speech that is a signal of the corresponding speaker by integrating the results of detection by the plurality of neural networks.</p><p id="p-0266" num="0285">(6)</p><p id="p-0267" num="0286">The signal processing device according to any one of (1) to (5), further including:</p><p id="p-0268" num="0287">a crosstalk reduction unit configured to reduce components of voices of speakers other than the corresponding speaker included in the signal.</p><p id="p-0269" num="0288">(7)</p><p id="p-0270" num="0289">The signal processing device according to (6), in which</p><p id="p-0271" num="0290">the frame information output from the main speech detection unit is applied to a signal output from the crosstalk reduction unit.</p><p id="p-0272" num="0291">(8)</p><p id="p-0273" num="0292">The signal processing device according to (6) or (7), in which</p><p id="p-0274" num="0293">the crosstalk reduction unit performs processing on a signal of a frame that includes the main speech.</p><p id="p-0275" num="0294">(9)</p><p id="p-0276" num="0295">The signal processing device according to any one of (1) to (9), further including:</p><p id="p-0277" num="0296">a voice recognition unit configured to perform voice recognition on a signal to which the frame information output from the main speech detection unit is applied.</p><p id="p-0278" num="0297">(10)</p><p id="p-0279" num="0298">The signal processing device according to (9), further including:</p><p id="p-0280" num="0299">a text information generation unit configured to generate text information based on a result of recognition by the voice recognition unit.</p><p id="p-0281" num="0300">(11)</p><p id="p-0282" num="0301">A signal processing method including:</p><p id="p-0283" num="0302">detecting, by a main speech detection unit, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a signal of the corresponding speaker, and outputting frame information indicating presence or absence of the main speech.</p><p id="p-0284" num="0303">(12)</p><p id="p-0285" num="0304">A program for causing a computer to execute a signal processing method including:</p><p id="p-0286" num="0305">detecting, by a main speech detection unit, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a signal of the corresponding speaker, and outputting frame information indicating presence or absence of the main speech.</p><p id="p-0287" num="0306">(13)</p><p id="p-0288" num="0307">A signal processing system including:</p><p id="p-0289" num="0308">a plurality of sound collection devices, each of which is assigned to one of speakers; and</p><p id="p-0290" num="0309">a signal processing device including a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to each sound collection device includes a main speech that is a voice of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</p><p id="p-0291" num="0310">(14)</p><p id="p-0292" num="0311">The signal processing system according to (13), in which each one of the plurality of sound collection devices is a microphone that is capable of being worn by the corresponding speaker or has directivity.</p><heading id="h-0019" level="1">REFERENCE SIGNS LIST</heading><p id="p-0293" num="0000"><ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0312"><b>20</b> Main speech detection unit</li>    <li id="ul0014-0002" num="0313"><b>30</b> Crosstalk reduction unit</li>    <li id="ul0014-0003" num="0314"><b>50</b> Voice recognition unit</li>    <li id="ul0014-0004" num="0315"><b>70</b> Text generation unit</li>    <li id="ul0014-0005" num="0316"><b>100</b> Signal processing device</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A signal processing device comprising:<claim-text>a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a signal of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the main speech detection unit detects whether or not the signal includes the main speech even in a case where a signal of a speaker other than the corresponding speaker is included.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the main speech detection unit outputs time information of a frame that includes the main speech.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the main speech detection unit detects whether or not a signal input to a sound collection device assigned to each of three or more speakers includes a main speech that is a signal of the corresponding speaker on a basis of results of detection by a plurality of neural networks.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The signal processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the main speech detection unit detects whether or not the signal includes a main speech that is a voice of the corresponding speaker by integrating the results of detection by the plurality of neural networks.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a crosstalk reduction unit configured to reduce components of voices of speakers other than the corresponding speaker included in the signal.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The signal processing device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the frame information output from the main speech detection unit is applied to a signal output from the crosstalk reduction unit.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The signal processing device according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the crosstalk reduction unit performs processing on a signal of a frame that includes the main speech.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The signal processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a voice recognition unit configured to perform voice recognition on a signal to which the frame information output from the main speech detection unit is applied.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The signal processing device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>a text information generation unit configured to generate text information based on a result of recognition by the voice recognition unit.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A signal processing method comprising:<claim-text>detecting, by a main speech detection unit, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and outputting frame information indicating presence or absence of the main speech.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A program for causing a computer to execute a signal processing method comprising:<claim-text>detecting, by a main speech detection unit, by using a neural network, whether or not a signal input to a sound collection device assigned to each of at least two speakers includes a main speech that is a voice of the corresponding speaker, and outputting frame information indicating presence or absence of the main speech.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A signal processing system comprising:<claim-text>a plurality of sound collection devices, each of which is assigned to one of speakers; and</claim-text><claim-text>a signal processing device including a main speech detection unit configured to detect, by using a neural network, whether or not a signal input to each sound collection device includes a main speech that is a voice of the corresponding speaker, and output frame information indicating presence or absence of the main speech.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The signal processing system according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein<claim-text>each one of the plurality of sound collection devices is a microphone that is capable of being worn by the corresponding speaker or has directivity.</claim-text></claim-text></claim></claims></us-patent-application>