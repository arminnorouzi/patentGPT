<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005253A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005253</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17839331</doc-number><date>20220613</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>38</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>38</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">EFFICIENT ARTIFICIAL INTELLIGENCE-BASED BASE CALLING OF INDEX SEQUENCES</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217644</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ILLUMINA, INC.</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>DUTTA</last-name><first-name>Anindita</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>VESSERE</last-name><first-name>Gery</first-name><address><city>Oakland</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>ILLUMINA, INC.</orgname><role>02</role><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques for improving artificial intelligence-based base calling are disclosed. The improved techniques can be used to better train artificial intelligence for base calling by reordering of sequencing images, and training of a neural network-based base caller where the temporal logic is effectively &#x201c;frozen&#x201d; (or bypassed). In addition, the improved techniques include various combinations, including, for example, combining &#x201c;normalization&#x201d; of sequencing images with reordering of sequencing images and/or with effectively &#x201c;freezing&#x201d; the temporal logic.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="194.14mm" wi="151.81mm" file="US20230005253A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="235.29mm" wi="174.50mm" orientation="landscape" file="US20230005253A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="150.28mm" wi="163.83mm" orientation="landscape" file="US20230005253A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="243.84mm" wi="173.48mm" file="US20230005253A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="243.59mm" wi="141.22mm" file="US20230005253A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="243.59mm" wi="141.22mm" file="US20230005253A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="244.18mm" wi="171.96mm" file="US20230005253A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="243.59mm" wi="141.22mm" file="US20230005253A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="191.77mm" wi="147.49mm" file="US20230005253A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="244.43mm" wi="173.57mm" orientation="landscape" file="US20230005253A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="236.64mm" wi="153.84mm" file="US20230005253A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="236.64mm" wi="153.84mm" file="US20230005253A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="236.64mm" wi="153.84mm" file="US20230005253A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="236.64mm" wi="153.84mm" file="US20230005253A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="236.64mm" wi="153.84mm" file="US20230005253A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="224.62mm" wi="149.27mm" orientation="landscape" file="US20230005253A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="237.91mm" wi="167.39mm" file="US20230005253A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="235.37mm" wi="171.20mm" file="US20230005253A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="237.07mm" wi="168.74mm" orientation="landscape" file="US20230005253A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="239.35mm" wi="173.91mm" orientation="landscape" file="US20230005253A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="243.50mm" wi="174.33mm" orientation="landscape" file="US20230005253A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="237.57mm" wi="173.91mm" orientation="landscape" file="US20230005253A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="240.62mm" wi="167.22mm" orientation="landscape" file="US20230005253A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="231.73mm" wi="147.40mm" orientation="landscape" file="US20230005253A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="246.89mm" wi="172.80mm" orientation="landscape" file="US20230005253A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="244.77mm" wi="174.33mm" orientation="landscape" file="US20230005253A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="220.98mm" wi="167.98mm" orientation="landscape" file="US20230005253A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="238.59mm" wi="159.60mm" orientation="landscape" file="US20230005253A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="242.57mm" wi="162.90mm" orientation="landscape" file="US20230005253A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="240.88mm" wi="169.67mm" orientation="landscape" file="US20230005253A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">PRIORITY APPLICATION</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Patent Application No. 63/217,644, titled, &#x201c;EFFICIENT ARTIFICIAL INTELLIGENCE-BASED BASE CALLING OF INDEX SEQUENCES,&#x201d; filed Jul. 1, 2021 (Attorney Docket No. ILLM 1046-1/IP-2135-PRV). The provisional application is hereby incorporated by reference for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE TECHNOLOGY DISCLOSED</heading><p id="p-0003" num="0002">The technology disclosed relates to artificial intelligence type computers and digital data processing systems and corresponding data processing methods and products for emulation of intelligence (i.e., knowledge-based systems, reasoning systems, and knowledge acquisition systems); and including systems for reasoning with uncertainty (e.g., fuzzy logic systems), adaptive systems, machine learning systems, and artificial neural networks. In particular, the technology disclosed relates to using deep neural networks such as deep convolutional neural networks for analyzing data.</p><heading id="h-0003" level="1">INCORPORATIONS BY REFERENCE</heading><p id="p-0004" num="0003">The following are hereby incorporated by reference as if fully set forth herein:</p><p id="p-0005" num="0004">U.S. Patent Application No. 62/979,384, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED BASE CALLING OF INDEX SEQUENCES,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1015-1/IP-1857-PRV);</p><p id="p-0006" num="0005">U.S. Patent Application No. 62/979,414, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED MANY-TO-MANY BASE CALLING,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1016-1/IP-1858-PRV);</p><p id="p-0007" num="0006">U.S. Patent Application No. 62/979,385, titled &#x201c;KNOWLEDGE DISTILLATION-BASED COMPRESSION OF ARTIFICIAL INTELLIGENCE-BASED BASE CALLER,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1017-1/IP-1859-PRV);</p><p id="p-0008" num="0007">U.S. Patent Application No. 63/072,032, titled &#x201c;DETECTING AND FILTERING CLUSTERS BASED ON ARTIFICIAL INTELLIGENCE-PREDICTED BASE CALLS,&#x201d; filed 28 Aug. 2020 (Attorney Docket No. ILLM 1018-1/IP-1860-PRV);</p><p id="p-0009" num="0008">U.S. Patent Application No. 62/979,412, titled &#x201c;MULTI-CYCLE CLUSTER BASED REAL TIME ANALYSIS SYSTEM,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1020-1/IP-1866-PRV);</p><p id="p-0010" num="0009">U.S. patent application Ser. No. 16/825,987, titled &#x201c;TRAINING DATA GENERATION FOR ARTIFICIAL INTELLIGENCE-BASED SEQUENCING,&#x201d; filed 20 Mar. 2020 (Attorney Docket No. ILLM 1008-16/IP-1693-US);</p><p id="p-0011" num="0010">U.S. patent application Ser. No. 16/825,991 titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED GENERATION OF SEQUENCING METADATA,&#x201d; filed 20 Mar. 2020 (Attorney Docket No. ILLM 1008-17/IP-1741-US);</p><p id="p-0012" num="0011">U.S. patent application Ser. No. 16/826,126, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED BASE CALLING,&#x201d; filed 20 Mar. 2020 (Attorney Docket No. ILLM 1008-18/IP-1744-US);</p><p id="p-0013" num="0012">U.S. patent application Ser. No. 16/826,134, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED QUALITY SCORING,&#x201d; filed 20 Mar. 2020 (Attorney Docket No. ILLM 1008-19/IP-1747-US);</p><p id="p-0014" num="0013">U.S. patent application Ser. No. 16/826,168, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED SEQUENCING,&#x201d; filed 21 Mar. 2020 (Attorney Docket No. ILLM 1008-20/IP-1752-PRV-US); and</p><p id="p-0015" num="0014">U.S. patent application Ser. No. 17/179,395, titled &#x201c;DATA COMPRESSION FOR ARTIFICIAL INTELLIGENCE-BASED BASE CALLING,&#x201d; filed 18 Feb. 2021 (Attorney Docket No. ILLM 1029-2/IP-1964-US).</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0016" num="0015">The subject matter discussed in this section cannot and should not be assumed to be prior art merely as a result of its mention in this section because, among other things, it may represent different approaches, which in and of themselves, may also correspond to implementations of the claimed technology. Similarly, a problem mentioned, or identified, in this section or associated with the subject matter provided as background, cannot and should not be assumed to have been previously recognized in the prior art as, among other things, it may also be part of the solution proposed.</p><p id="p-0017" num="0016">Improvements in next-generation sequencing (NGS) technology have greatly increased sequencing speed and data output, resulting in relatively massive sample throughput of current sequencing platforms. Approximately ten years ago, the Illumina Genome Analyzer&#x2122; was capable of generating up to one gigabyte of sequence data per run. Today, the Illumina NovaSeg&#x2122; series are capable of generating up to two terabytes of data in two days, representing a greater than 2000&#xd7; increase in capacity.</p><p id="p-0018" num="0017">One aspect of realizing this increased capacity is multiplexing, which adds unique sequences, called indexes, to each DNA fragment during library preparation. This allows large numbers of libraries to be pooled and sequenced simultaneously during a single sequencing run. Gains in throughput from multiplexing come with an added layer of complexity, as sequencing reads from pooled libraries need to be identified and sorted computationally in a process called demultiplexing before final data analysis. Index misassignment between multiplexed libraries is a known issue that has impacted NGS technologies from the time sample multiplexing was developed.</p><p id="p-0019" num="0018">An opportunity arises to use artificial intelligence and neural networks to base call index sequences. Higher base calling throughput and increased base calling accuracy may result.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0020" num="0019">The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</p><p id="p-0021" num="0020">The color drawings also may be available in PAIR via the Supplemental Content tab. In the drawings, like reference characters generally refer to like parts throughout the different views. Also, the drawings are not necessarily to scale, with an emphasis instead generally being placed upon illustrating the principles of the technology disclosed. In the following description, various implementations of the technology disclosed are described with reference to the following drawings, in which:</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows one implementation of the sequencing of polynucleotides from indexed libraries.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows one implementation of sequencing a target sequence to generate a target read and sequencing an index sequence to generate an index read.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates one implementation of the disclosed neural network-based base caller.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B and <b>4</b>C</figref> depict one implementation of shuffling the index images for processing by the disclosed neural network-based base caller.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B and <b>5</b>C</figref> depict another implementation of shuffling the index images for processing by the disclosed neural network-based base caller.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a subset selector configured to effectively generate a reordering of a set of sequencing images (reordered set of sequencing images) as a reordered subset of sequencing images, based on two parameters in accordance with one implementation.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. <b>7</b>A, <b>7</b>B and <b>7</b>C</figref> depict a reordered subset of sequencing images (also shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) in connection with the neural network-based base caller (also shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> depict additional examples of reordered subsets of sequencing images in connection with a neural network-based base caller in accordance with additional implementations.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts initial (first) training and further (second) training in accordance with two implementations.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a method for training a neural network-based base caller in accordance with one implementation.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts another method for training a neural network-based base caller in accordance with another implementation.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts a method for training a neural network-based base caller for base calling of sequencing images, in accordance with one implementation.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts another method for training a neural network-based base caller in accordance with one implementation.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows a flow chart of a system that executes the disclosed base calling, in accordance with one implementation.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows measured base calling error rates for the previously disclosed &#x201c;augmentation&#x201d; and &#x201c;normalization&#x201d; implementations of the neural network-based base caller <b>300</b>, as applied on 1-Plex sequencing data.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>16</b></figref> compares measured base calling error rates for three different implementations of the neural network-based base caller <b>300</b>, as applied on different complexities (e.g., 96-Plex, 1-Plex, and 3-Plex).</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>17</b></figref> compares measured base calling error rates for three different implementations of the neural network-based base caller <b>300</b>, as applied on different complexities (e.g., 4-Plex, 2-Plex, and 1-Plex).</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>18</b></figref> depicts a plot that compares the &#x201c;demultiplex&#x201d; results for tiles from runs with different Plexities for RTA (blue), a previously disclosed deep learning implementation (orange) (DL 3c Aug), and the currently disclosed joint training and shuffling implementation (green) (DL 5c Norm*).</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIGS. <b>19</b>A and <b>19</b>B</figref> depict one implementation of a sequencing system.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>19</b>C</figref> is a simplified block diagram of a system for the analysis of sensor data from the sequencing system, such as base call sensor outputs.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>20</b>A</figref> is a simplified diagram showing aspects of the base calling operation, including functions of a runtime program executed by a host processor.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>20</b>B</figref> is a simplified diagram of a configuration of a configurable processor such as that of <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>21</b>A</figref> illustrates one implementation of jointly training the neural network-based base caller <b>300</b> on reordered index images and sequencing images.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>21</b>B</figref> illustrates one implementation of how the reordered index images <b>2132</b> of <figref idref="DRAWINGS">FIG. <b>21</b>A</figref> are generated.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a computer system that can be used by the sequencing system of <figref idref="DRAWINGS">FIG. <b>19</b>A</figref> to implement the base calling techniques disclosed herein.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>23</b></figref> depicts an example of 96 indexes.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIGS. <b>24</b> and <b>25</b></figref> show various implementations of the technology disclosed.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0049" num="0048">The following discussion is presented to enable any person skilled in the art to make and use the technology disclosed and is provided in the context of a particular application and its requirements. Various modifications to the disclosed implementations will be readily apparent to those skilled in the art, and the general principles defined herein may be applied to other implementations and applications without departing from the spirit and scope of the technology disclosed. Thus, the technology disclosed is not intended to be limited to the implementations shown but is to be accorded the widest scope consistent with the principles and features disclosed herein.</p><heading id="h-0007" level="2">Multiplexing</heading><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows one implementation of the sequencing of polynucleotides from indexed libraries. When polynucleotides from different libraries are pooled or multiplexed for sequencing, the polynucleotides from each library are modified to include a library-specific index sequence. During sequencing, the index sequences are sequenced along with target polynucleotide sequences from the libraries. An index sequence is associated with a target polynucleotide sequence so that the library from which the target sequence originated can be identified.</p><p id="p-0051" num="0050">Panel A shows indexed libraries <b>102</b>. Here, unique index sequences (&#x201c;indexes&#x201d;) are added to two different libraries during library preparation. The first index sequence (Index A) has a barcode of &#x201c;CATTCG.&#x201d; The second index sequence (Index B) has a barcode of &#x201c;AACTGA.&#x201d;</p><p id="p-0052" num="0051">Panel B shows pooling <b>104</b>. Here, the indexed libraries <b>102</b> are pooled together and loaded into the same flow cell lane.</p><p id="p-0053" num="0052">Panel C shows sequencing <b>106</b> and the sequencing output <b>116</b>. Here, the indexed libraries <b>102</b> are sequenced together during a single instrument run. All sequences are then exported to an output file <b>116</b>. The output file <b>116</b> comprises sequence reads (in green) coupled to corresponding index reads (in blue and magenta).</p><p id="p-0054" num="0053">Panel D shows demultiplexing <b>108</b>. Here, a demultiplexing algorithm sorts the sequence reads into different files according to their indexes.</p><p id="p-0055" num="0054">Panel E shows alignment <b>110</b>. Here, each set of the demultiplexed sequence reads is aligned to the appropriate reference sequence.</p><p id="p-0056" num="0055">Additional details about multiplexing, index sequences, and demultiplexing can be found in Illumina, &#x201c;Indexed Sequencing Overview Guide,&#x201d; Document No. 15057455, v. 5, March 2019 and in Illumina's patent application publications US 2018/0305751, US 2018/0334712, US 2016/0110498, US 2018/0334711, and WO 2019/090251, each of which is incorporated herein by reference.</p><heading id="h-0008" level="2">Target Sequences and Index Sequences</heading><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows one implementation of sequencing a target sequence <b>222</b> to generate a target read <b>202</b> (&#x201c;GTCCGATA&#x201d;) and sequencing an index sequence <b>232</b> to generate an index read <b>204</b> (&#x201c;AACTGA&#x201d;). The index sequence <b>232</b> can be a synthetic sequence of nucleotides that is coupled to the target sequence <b>222</b> during the template preparation step. The target sequence <b>222</b> can be naturally occurring DNA, RNA, or some other biological molecule. The length of the index sequence <b>232</b> can range from two to twenty nucleotides. For example, the index sequence <b>232</b> can be one to ten nucleotides long or four to six nucleotides long. A four-nucleotide index sequence gives the possibility of multiplexing 256 samples on the same array. A six-nucleotide index sequence enables 4096 samples to be processed on the same array.</p><p id="p-0058" num="0057">During the sequencing <b>106</b>, a target primer <b>212</b> traverses the target sequence <b>222</b> and produces the target read <b>202</b> (&#x201c;GTCCGATA&#x201d;) and an index primer <b>224</b> traverses the index sequence <b>232</b> and produces the index read <b>204</b> (&#x201c;AACTGA&#x201d;). In some implementations, the sequencing <b>106</b> is Illumina's single-indexed sequencing. In other implementations, the sequencing <b>106</b> is Illumina's dual-indexed sequencing.</p><p id="p-0059" num="0058">Base calling is the process of determining the nucleotide composition of the target sequence <b>222</b> and the index sequence <b>232</b>, i.e., the process of generating the target read <b>202</b> (&#x201c;GTCCGATA&#x201d;) and the index read <b>204</b> (&#x201c;AACTGA&#x201d;).</p><heading id="h-0009" level="2">Sequencing Images</heading><p id="p-0060" num="0059">Base calling involves analyzing image data, i.e., sequencing images, produced during a sequencing run (or sequencing reaction) carried out by a sequencing instrument such as Illumina's iSeq, HiSeqX, HiSeq 3000, Hi Seq 4000, HiSeq 2500, NovaSeq 6000, NextSeq 550, NextSeq 1000, NextSeq 2000, NextSeqDx, MiSeq, and MiSeqDx.</p><p id="p-0061" num="0060">The following discussion outlines how the sequencing images are generated and what they depict, in accordance with one implementation.</p><p id="p-0062" num="0061">Base calling decodes the intensity data encoded in the sequencing images into nucleotide sequences. In one implementation, the Illumina sequencing platforms employ cyclic reversible termination (CRT) chemistry for base calling. The process relies on growing nascent strands complementary to template strands with fluorescently-labeled nucleotides, while tracking the emitted signal of each newly added nucleotide. The fluorescently-labeled nucleotides each have a 3&#x2032; removable block that anchors a fluorophore signal of the nucleotide type.</p><p id="p-0063" num="0062">Sequencing occurs in repetitive cycles, each comprising three steps: (a) extension of a nascent strand by adding the fluorescently-labeled nucleotide; (b) excitation of the fluorophore using one or more lasers of an optical system of the sequencing instrument and imaging through different filters of the optical system, yielding the sequencing images; and (c) cleavage of the fluorophore and removal of the 3&#x2032; block in preparation for the next sequencing cycle. Incorporation and imaging cycles are repeated up to a designated number of sequencing cycles, defining the read length. Using this approach, each cycle interrogates a new position along the template strands.</p><p id="p-0064" num="0063">The tremendous power of the Illumina sequencers stems from their ability to simultaneously execute and sense millions or even billions of clusters (also called &#x201c;analytes&#x201d;) undergoing CRT reactions. A cluster comprises approximately one thousand identical copies of a template strand, though clusters vary in size and shape. The clusters are grown from the template strand, prior to the sequencing run, by bridge amplification or exclusion amplification of the input library. The purpose of the amplification and cluster growth is to increase the intensity of the emitted signal since the imaging device cannot reliably sense the fluorophore signal of a single strand. However, the physical distance of the strands within a cluster is small, so the imaging device perceives the cluster of strands as a single spot.</p><p id="p-0065" num="0064">Sequencing occurs in a flow cell (or biosensor)&#x2014;a small glass slide that holds the input strands. The flow cell is connected to the optical system, which comprises microscopic imaging, excitation lasers, and fluorescence filters. The flow cell comprises multiple chambers called lanes. The lanes are physically separated from each other and may contain different tagged sequencing libraries, distinguishable without sample cross contamination. In some implementations, the flow cell comprises a patterned surface. A &#x201c;patterned surface&#x201d; refers to an arrangement of different regions in or on an exposed layer of a solid support.</p><p id="p-0066" num="0065">The imaging device of the sequencing instrument (e.g., a solid-state imager such as a charge-coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS) sensor) takes snapshots at multiple locations along the lanes in a series of non-overlapping regions called tiles. For example, there can be sixty-four or ninety-six tiles per lane. A tile holds hundreds of thousands to millions of clusters.</p><p id="p-0067" num="0066">The output of the sequencing run is the sequencing images. Sequencing images depict intensity emissions of the clusters and their surrounding background using a grid (or array) of pixelated units (e.g., pixels, superpixels, subpixels). The intensity emissions are stored as intensity values of the pixelated units. The sequencing images have dimensions w&#xd7;h of the grid of pixelated units, where w (width) and h (height) are any numbers ranging from 1 to 100,000 (e.g., 115&#xd7;115, 200&#xd7;200, 1800&#xd7;2000, 2200&#xd7;25000, 2800&#xd7;3600, 4000&#xd7;400). In some implementations, w and h are the same. In other implementations, w and h are different.</p><p id="p-0068" num="0067">Those sequencing cycles of the sequencing run that sequence the target sequence <b>222</b> are called &#x201c;target sequencing cycles&#x201d; and those sequencing cycles of the sequencing run that sequence the index sequence <b>232</b> are called &#x201c;index sequencing cycles.&#x201d; The sequencing images generated during the target sequencing cycles are called &#x201c;target images&#x201d; and the sequencing images generated during the index sequencing cycles are called &#x201c;index images.&#x201d;</p><heading id="h-0010" level="2">Neural Network-Based Base Calling</heading><p id="p-0069" num="0068">The following discussion focuses on a neural network-based base caller <b>300</b> described herein. First, the input to the neural network-based base caller <b>300</b> is described, in accordance with one implementation. Then, examples of the structure and form of the neural network-based base caller <b>300</b> are provided. Finally, the output of the neural network-based base caller <b>300</b> is described, in accordance with one implementation.</p><p id="p-0070" num="0069">A data flow logic provides the sequencing images to the neural network-based base caller <b>300</b> for base calling. The neural network-based base caller <b>300</b> accesses the sequencing images on a patch-by-patch basis (or a tile-by-tile basis). Each of the patches is a sub-grid (or sub-array) of pixelated units in the grid of pixelated units that forms the sequencing images. The patches have dimensions q&#xd7;r of the sub-grid of pixelated units, where q (width) and r (height) are any numbers ranging from 1 to 10000 (e.g., 3&#xd7;3, 5&#xd7;5, 7&#xd7;7, 10&#xd7;10, 15&#xd7;15, 25&#xd7;25, 64&#xd7;64, 78&#xd7;78, 115&#xd7;115). In some implementations, q and r are the same. In other implementations, q and r are different. In some implementations, the patches extracted from a sequencing image are of the same size. In other implementations, the patches are of different sizes. In some implementations, the patches can have overlapping pixelated units (e.g., on the edges).</p><p id="p-0071" num="0070">Sequencing produces m sequencing images per sequencing cycle for a corresponding m image channels. That is, each of the sequencing images has one or more image (or intensity) channels (analogous to the red, green, blue (RGB) channels of a color image). In one implementation, each image channel corresponds to one of a plurality of filter wavelength bands. In another implementation, each image channel corresponds to one of a plurality of imaging events in a sequencing cycle. In yet another implementation, each image channel corresponds to a combination of illumination with a specific laser and imaging through a specific optical filter. The image patches are tiled (or accessed) from each of the m image channels for a particular sequencing cycle. In different implementations such as 4-, 2-, and 1-channel chemistries, m is 4 or 2. In other implementations, m is 1, 3, or greater than 4.</p><p id="p-0072" num="0071">Consider, for example, that a sequencing run is implemented using two different image channels: a blue channel and a green channel. Then, at each sequencing cycle, the sequencing run produces a blue image and a green image. This way, for a series of k sequencing cycles of the sequencing run, a sequence of k pairs of blue and green images is produced as output and stored as the sequencing images. Accordingly, a sequence of k pairs of blue and green image patches is generated for the patch-level processing by the neural network-based base caller <b>300</b>.</p><p id="p-0073" num="0072">The input image data to the neural network-based base caller <b>300</b> for a single iteration of base calling (or a single instance of forward pass or a single forward traversal) comprises data for a sliding window of multiple sequencing cycles. The sliding window can include, for example, a current sequencing cycle, one or more preceding sequencing cycles, and one or more successive sequencing cycles.</p><p id="p-0074" num="0073">In one implementation, the input image data comprises data for three sequencing cycles, such that data for a current (time t) sequencing cycle to be base called is accompanied with (i) data for a left-flanking/context/previous/preceding/prior (time t&#x2212;1) sequencing cycle and (ii) data for a right-flanking/context/next/successive/subsequent (time t+1) sequencing cycle.</p><p id="p-0075" num="0074">In another implementation, the input image data comprises data for five sequencing cycles, such that data for a current (time t) sequencing cycle to be base called is accompanied with (i) data for a first left-flanking/context/previous/preceding/prior (time t&#x2212;1) sequencing cycle, (ii) data for a second left-flanking/context/previous/preceding/prior (time t&#x2212;2) sequencing cycle, (iii) data for a first right-flanking/context/next/successive/subsequent (time t+1) sequencing cycle, and (iv) data for a second right-flanking/context/next/successive/subsequent (time t+2) sequencing cycle.</p><p id="p-0076" num="0075">In yet another implementation, the input image data comprises data for seven sequencing cycles, such that data for a current (time t) sequencing cycle to be base called is accompanied with (i) data for a first left-flanking/context/previous/preceding/prior (time t&#x2212;1) sequencing cycle, (ii) data for a second left-flanking/context/previous/preceding/prior (time t&#x2212;2) sequencing cycle, (iii) data for a third left-flanking/context/previous/preceding/prior (time t&#x2212;3) sequencing cycle, (iv) data for a first right-flanking/context/next/successive/subsequent (time t+1) sequencing cycle, (v) data for a second right-flanking/context/next/successive/subsequent (time t+2) sequencing cycle, and (vi) data for a third right-flanking/context/next/successive/subsequent (time t+3) sequencing cycle. In other implementations, the input image data comprises data for a single sequencing cycle. In yet other implementations, the input image data comprises data for 10, 15, 20, 30, 58, 75, 92, 130, 168, 175, 209, 225, 230, 275, 318, 325, 330, 525, or 625 sequencing cycles.</p><p id="p-0077" num="0076">The neural network-based base caller <b>300</b> processes the image patches through its convolution layers and produces an alternative representation, according to one implementation. The alternative representation is then used by an output layer (e.g., a softmax layer) to generate a base call for either just the current (time t) sequencing cycle or each of the sequencing cycles, i.e., the current (time t) sequencing cycle, the first and second preceding (time t&#x2212;1, time t&#x2212;2) sequencing cycles, and the first and second succeeding (time t+1, time t+2) sequencing cycles. The resulting base calls form the sequencing reads.</p><p id="p-0078" num="0077">In one implementation, the neural network-based base caller <b>300</b> outputs a base call for a single target cluster for a particular sequencing cycle. In another implementation, the neural network-based base caller <b>300</b> outputs a base call for each target cluster in a plurality of target clusters for the particular sequencing cycle. In yet another implementation, the neural network-based base caller <b>300</b> outputs a base call for each target cluster in a plurality of target clusters for each sequencing cycle in a plurality of sequencing cycles, thereby producing a base call sequence for each target cluster.</p><p id="p-0079" num="0078">In one implementation, the neural network-based base caller <b>300</b> is a multilayer perceptron (MLP). In another implementation, the neural network-based base caller <b>300</b> is a feedforward neural network. In yet another implementation, the neural network-based base caller <b>300</b> is a fully-connected neural network. In a further implementation, the neural network-based base caller <b>300</b> is a fully convolutional neural network. In yet further implementation, the neural network-based base caller <b>300</b> is a semantic segmentation neural network. In yet another further implementation, the neural network-based base caller <b>300</b> is a generative adversarial network (GAN).</p><p id="p-0080" num="0079">In one implementation, the neural network-based base caller <b>300</b> is a convolutional neural network (CNN) with a plurality of convolution layers. In another implementation, the neural network-based base caller <b>300</b> is a recurrent neural network (RNN) such as a long short-term memory network (LSTM), bi-directional LSTM (Bi-LSTM), or a gated recurrent unit (GRU). In yet another implementation, the neural network-based base caller <b>300</b> includes both a CNN and an RNN.</p><p id="p-0081" num="0080">In yet other implementations, the neural network-based base caller <b>300</b> can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. The neural network-based base caller <b>300</b> can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. The neural network-based base caller <b>300</b> can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous stochastic gradient descent (SGD). The neural network-based base caller <b>300</b> can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LSTM or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tanh)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><p id="p-0082" num="0081">The neural network-based base caller <b>300</b> is trained using backpropagation-based gradient update techniques. Example gradient descent techniques that can be used for training the neural network-based base caller <b>300</b> include stochastic gradient descent, batch gradient descent, and mini-batch gradient descent. Some examples of gradient descent optimization algorithms that can be used to train the neural network-based base caller <b>300</b> are Momentum, Nesterov accelerated gradient, Adagrad, Adadelta, RMSprop, Adam, AdaMax, Nadam, and AMSGrad.</p><p id="p-0083" num="0082">In one implementation, the neural network-based base caller <b>300</b> uses a specialized architecture to segregate the processing of data for different sequencing cycles. The motivation for using the specialized architecture is described first. As discussed above, the neural network-based base caller <b>300</b> processes image patches for a current sequencing cycle, one or more preceding sequencing cycles, and one or more successive sequencing cycles. Data for additional sequencing cycles provide sequence-specific context. The neural network-based base caller <b>300</b> learns the sequence-specific context during training and base calls the sequencing cycles. Furthermore, data for pre and post sequencing cycles provide second order contribution of pre-phasing and phasing signals to the current sequencing cycle.</p><p id="p-0084" num="0083">However, images captured at different sequencing cycles and in different image channels are misaligned and have residual registration error with respect to each other. To account for this misalignment, the specialized architecture comprises spatial convolution layers that do not mix information between sequencing cycles and only mix information within a sequencing cycle.</p><p id="p-0085" num="0084">Spatial convolution layers (or spatial logic) use so-called &#x201c;segregated convolutions&#x201d; that operationalize the segregation by independently processing data for each of a plurality of sequencing cycles through a &#x201c;dedicated, non-shared&#x201d; sequence of convolutions. The segregated convolutions convolve over data and resulting feature maps of only a given sequencing cycle, i.e., intra-cycle, without convolving over data and resulting feature maps of any other sequencing cycle.</p><p id="p-0086" num="0085">Consider, for example, that the input image data comprises (i) the current image patch for a current (time t) sequencing cycle to be base called, (ii) the previous image patch for a previous (time t&#x2212;1) sequencing cycle, and (iii) the next image patch for a next (time t+1) sequencing cycle. The specialized architecture then initiates three separate convolution pipelines, namely, a current convolution pipeline, a previous convolution pipeline, and a next convolution pipeline. The current data processing pipeline receives as input the current image patch for the current (time t) sequencing cycle and independently processes it through a plurality of spatial convolution layers to produce a so-called &#x201c;current spatially convolved representation&#x201d; as the output of a final spatial convolution layer. The previous convolution pipeline receives as input the previous image patch for the previous (time t&#x2212;1) sequencing cycle and independently processes it through the plurality of spatial convolution layers to produce a so-called &#x201c;previous spatially convolved representation&#x201d; as the output of the final spatial convolution layer. The next convolution pipeline receives as input the next image patch for the next (time t+1) sequencing cycle and independently processes it through the plurality of spatial convolution layers to produce a so-called &#x201c;next spatially convolved representation&#x201d; as the output of the final spatial convolution layer.</p><p id="p-0087" num="0086">In some implementations, the current, previous, and next convolution pipelines are executed in parallel. In some implementations, the spatial convolution layers are part of a spatial convolution network (or subnetwork) within the specialized architecture.</p><p id="p-0088" num="0087">The neural network-based base caller <b>300</b> further comprises temporal convolution layers (or temporal logic) that mix information between sequencing cycles, i.e., inter-cycles. The temporal convolution layers receive their inputs from the spatial convolution network and operate on the spatially convolved representations produced by the final spatial convolution layer for the respective data processing pipelines.</p><p id="p-0089" num="0088">The inter-cycle operability freedom of the temporal convolution layers emanates from the fact that the misalignment property, which exists in the image data fed as input to the spatial convolution network, is purged out from the spatially convolved representations by the stack, or cascade, of segregated convolutions performed by the sequence of spatial convolution layers.</p><p id="p-0090" num="0089">Temporal convolution layers use so-called &#x201c;combinatory convolutions&#x201d; that groupwise convolve over input channels in successive inputs on a sliding window basis. In one implementation, the successive inputs are successive outputs produced by a previous spatial convolution layer or a previous temporal convolution layer.</p><p id="p-0091" num="0090">In some implementations, the temporal convolution layers are part of a temporal convolution network (or subnetwork) within the specialized architecture. The temporal convolution network receives its inputs from the spatial convolution network. In one implementation, a first temporal convolution layer of the temporal convolution network groupwise combines the spatially convolved representations between the sequencing cycles. In another implementation, subsequent temporal convolution layers of the temporal convolution network combine successive outputs of previous temporal convolution layers. The output of the final temporal convolution layer is fed to an output layer that produces an output. The output is used to base call one or more clusters at one or more sequencing cycles.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a diagram of the neural network-based base caller <b>300</b> that can effectively process sequencing images generated during a sequencing process, in accordance with one implementation. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the neural network-based base caller <b>300</b> can be executed using a system described herein in accordance with one exemplary implementation. It should be noted that the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> can be referred to as a five-cycle input, one-cycle output neural network. As such, the inputs to the multi-cycle neural network model include five (5) spatially aligned patches (e.g., <b>301</b>), for example, patches from the tile data arrays of five sensing cycles of a given tile. In the example, spatially aligned patches can have the same aligned row and column dimensions (x, y) as other patches in the set, so that the information relates to the same clusters of genetic material on the tile in sequence cycles. In addition, a subject patch is a patch from the array of tile data for cycle N, such that, for example, the set of five spatially aligned patches includes a patch from cycle N&#x2212;2 preceding the subject patch (cycle N) by two cycles, a patch from cycle N&#x2212;1 preceding the subject patch (cycle N) by one cycle, a patch from cycle N+1 following the patch from the subject cycle (cycle N) by one cycle, and a patch from cycle N+2 following the patch from the subject cycle (cycle N) by two cycles.</p><p id="p-0093" num="0092">The model (or neural network-based base caller <b>300</b>) depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref> also includes a segregated stack of multiple layers of the neural network for each of the input patches (e.g., segregated stacks <b>302</b>). In other words, the segregated stack <b>302</b> receives as input tile data for the patch from cycle N&#x2212;2 (<b>301</b>), and it is segregated from the stacks <b>304</b>, <b>306</b>, <b>308</b>, and <b>310</b> that receive data from other cycles, so that the segregated stacks do not share their input data, or intermediate data, with each other.</p><p id="p-0094" num="0093">In some implementations, all of the individual segregated stacks can have an identical (or virtually identical) model, and identical (or virtually identical) trained parameters. In other implementations, the models and trained parameters may vary (i.e., can be different in the different stacks). In any case, as a segregated stack, stack <b>302</b> can receive, as input, tile data for the patch from cycle N&#x2212;2. Stack <b>304</b> can receive, as input, tile data for the patch from cycle N&#x2212;1. Stack <b>306</b> can receive, as input, tile data for the patch from cycle N. Stack <b>308</b> can receive, as input, tile data for the patch from cycle N+1. Stack <b>310</b> can receive, as input, tile data for the patch from cycle N+1.</p><p id="p-0095" num="0094">Referring again to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the layers of the segregated stacks (e.g., <b>312</b>, <b>322</b>, <b>332</b>, <b>342</b>, <b>352</b>, <b>362</b>, and <b>372</b>) can each execute one or more convolution operation(s) of one or more kernels, including a plurality of filters over their respective input data for the layer. As an example, a patch associated with Cycle N&#x2212;2 may include three (3) features as it is input to the first layer, namely, the layer <b>312</b>. However, the output of the layer <b>312</b> may include many more features, for example, 10 to 20 features. Likewise, the outputs of each of layers <b>322</b>, <b>332</b>, <b>342</b>, <b>352</b>, <b>362</b>, and <b>372</b> may include virtually any number of features suitable for a particular implementation. The parameters of the filters can be trained parameters for the neural network, for example, as weights and biases. The output feature set (intermediate data) from each of the stacks <b>302</b>, <b>304</b>, <b>306</b>, and <b>310</b> is provided as input to an inverse hierarchy <b>382</b> of temporal combinatorial layers, in which the intermediate data from the multiple cycles are combined. In the example depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the inverse hierarchy <b>382</b> includes a first layer <b>384</b> and a second layer <b>386</b>, such that the first layer includes three combinatorial layers, each receiving intermediate data from three of the segregated stacks, and the second (or final) layer includes one combinatorial layer receiving intermediate data from the three temporal layers.</p><p id="p-0096" num="0095">The output of the second (or final) combinatorial layer <b>382</b> can be an output patch of classification data for clusters located in the corresponding patch of the tile from cycle N. In addition, the output patches can be assembled into an output array of classification data for the tile for cycle N. In some implementations, the output patch may have sizes and dimensions different from the input patches. In some implementations, the output patch may include pixel-by-pixel data that can be filtered by the host to select cluster data.</p><p id="p-0097" num="0096">The output classification data can then be applied to a softmax function <b>394</b> (or other output activation function) optionally executed by the host, or on the configurable processor, depending on a particular implementation. An output function different from the softmax function <b>394</b> can also be used (e.g., used for a making a base call output parameter according to the largest output, then use a learned nonlinear mapping using context/network outputs to give base quality).</p><p id="p-0098" num="0097">Finally, the output of the softmax function <b>394</b> can be provided as base call predictions <b>396</b> for the cycle N and stored in the host memory to be used in subsequent processing. Other systems may use another function for output probability calculation (e.g., another nonlinear model).</p><p id="p-0099" num="0098">The hierarchy <b>382</b> of temporal combinatorial layers <b>384</b>, <b>386</b> can collectively represent a temporal logic of the neural network-based base caller <b>300</b>, and the segregated stacks <b>302</b>, <b>304</b>, <b>306</b>, <b>308</b>, and <b>310</b> can collectively represent a spatial logic of the neural network-based base caller <b>300</b>.</p><p id="p-0100" num="0099">The neural network-based base caller <b>300</b> can, for example, be implemented using a configurable processor with a plurality of execution clusters, such that a complete evaluation of one tile cycle within the duration of the time interval, or close to the duration of the time interval, of one sensing cycle, can effectively provide the output data in real time. Data flow logic can be configured to distribute input units of tile data and trained parameters to the execution clusters, and to distribute output patches for aggregation in memory.</p><p id="p-0101" num="0100">Additional details about the neural network-based base caller <b>300</b> can, for example, be found in U.S. Provisional Patent Application No. 62/821,766, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED SEQUENCING,&#x201d; (Attorney Docket No. ILLM 1008-9/IP-1752-PRV), filed on Mar. 21, 2019, which is incorporated herein by reference.</p><heading id="h-0011" level="2">Index Images</heading><p id="p-0102" num="0101">Index images depict nucleotides with low-complexity patterns in which some of the four bases A, C, T, and G are represented at a frequency of less than 15%, 10%, or 5% of all the nucleotides. This is the case because, for any given index sequencing cycle, an index image depicts intensity emissions of (1) multiple analytes that originate from the same sample and share the same index sequence, and also of (2) analytes that belong to different samples and have different index sequences.</p><p id="p-0103" num="0102">In other words, the first type of analytes has the same index base for every index sequencing cycle. As a result, the index image ends up depicting the same nucleotide for multiple analytes. This reduces the nucleotide diversity of the index image.</p><p id="p-0104" num="0103">The index image's nucleotide diversity is further reduced when the second type of analytes also ends up having the same index base for certain index sequencing cycles. This happens for two reasons. First, the index sequences are short sequences with two to twenty index bases and thus do not have enough positions that can create significant mismatches between different index sequences. Second, often, up to only twenty samples are pooled for simultaneous sequencing. As a result, the number of different index sequences that can be depicted by an index image is not substantial. These factors lead to different index sequences having matching index bases at the same positions (base collision), which in turn causes the analytes with different index sequences to have the same index base for certain index sequencing cycles.</p><p id="p-0105" num="0104">Low nucleotide diversity in the index images creates intensity patterns that lack signal diversity (contrast). On the other hand, the target images depict nucleotides with high-complexity patterns in which each of the four bases A, C, T, and G is represented at a frequency of at least 20%, 25%, or 30% of all the nucleotides. This is the case because the target sequences are often long (e.g., one-fifty bases) and are unique to each analyte regardless of the source sample. Therefore, unlike the index images, the target images have adequate signal diversity.</p><p id="p-0106" num="0105">Convolution kernels and filters of the neural network-based base caller <b>300</b> are trained largely on the target images. So, when, during inference, the trained neural network-based base caller <b>300</b> is presented with index images that have not undergone preprocessing (raw index images), its base calling accuracy for the index reads drops because its convolution kernels and filters are trained to detect intensity patterns based on the contrast.</p><p id="p-0107" num="0106">Bypassing preprocessing by training the neural network-based base caller <b>300</b> on large amounts of raw index images to introduce signal diversity is not feasible because only so many index sequences are published and made publicly available. Furthermore, it is not uncommon for users to design custom index sequences and use them instead of the published index sequences. So, when trained on just the raw index images, the neural network-based base caller <b>300</b> may not be able to &#x201c;generalize&#x201d; well or effectively during inference and it may be prone to a problem that can be referred to as &#x201c;overfitting.&#x201d;</p><heading id="h-0012" level="2">Improved Techniques</heading><p id="p-0108" num="0107">The improved techniques for base calling index images include reordering of sequencing images, and training of the neural network-based base caller <b>300</b> where a &#x201c;temporal&#x201d; logic (or component or layer) can be effectively &#x201c;frozen&#x201d; (or bypassed). In addition, the improved techniques include various combinations, including, for example, combining &#x201c;normalization&#x201d; of sequencing images with reordering of sequencing images and/or with effective &#x201c;freezing&#x201d; of the temporal logic.</p><p id="p-0109" num="0108">In accordance with one aspect, a set of sequencing images can be reordered to generate one or more reordered sets of sequencing images. The set of sequencing images that are reordered (reordered sequencing images) can include index images of one or more indexes provided for one or more libraries used for multiplexing, in addition to target images associated with a target being analyzed (e.g., template strand). In any case, the reordering of sequencing images can provide several additional sequencing images as additional training data for the training of the neural network-based base caller <b>300</b>, as it is possible to generate multiple reordered sets of sequencing images based on a single set of sequencing images, for example, an image in its original consecutive sequencing cycle (e.g., cycle 1, cycle 2 . . . cycle M). For example, for a consecutive sequencing cycle starting from cycle 1, cycle 2 . . . cycle 8, where M=8, it would be possible to generate about (M<sup>N</sup>&#x2212;1) or just about M<sup>N </sup>sets of rearranged sequencing images, where N can, for example, represent a selection window size (e.g., 3, 4, 5, 6, 7) for datasets provided as training data for the training of the neural network-based base caller <b>300</b> (e.g., 8<sup>5</sup>, 8<sup>6</sup>, 8<sup>7</sup>).</p><p id="p-0110" num="0109">As such, the reordering of a set of sequencing images can be especially useful for index images given their relatively low diversity and/or limited availability. It should be noted that the reordering of the sequencing images can be accomplished in various ways. For example, in accordance with one or more implementations, a &#x201c;shuffling&#x201d; technique can be utilized to reorder a set of sequencing images by effectively maintaining one or more images fixed in a central position (e.g., a third image in a sequence of five consecutively numbered images from one to five) while exchanging the position of one or more other images between left (e.g., first image, second images) and right flanks (e.g., fourth image, fifth image) relative to the central position. Other techniques for reordering sequencing images include, for example, permutations with or without repetition, combinations, etc. The techniques can be used to effectively generate the rearrangement of a set of sequencing images based on one or more parameters (e.g., set size, selection window size) in accordance with additional implementations. Again, rearranging of the sequencing images can be especially useful for index images given their relatively lower level of diversity.</p><p id="p-0111" num="0110">In addition to the reordering of sequencing images, in accordance with another aspect, the temporal logic of the neural network-based base caller <b>300</b> can be effectively &#x201c;frozen&#x201d; or bypassed to effectively prevent the temporal logic from learning temporal dependencies (e.g., sequential, chronological, or consecutive patterns). In doing so, the temporal logic can be already trained to at least some extent (e.g., previously trained on a first dataset). As such, &#x201c;freezing&#x201d; or bypassing of the temporal logic of a neural network can take place in the form of further (or additional) training using a second set of training data that trains the spatial logic but does not train the temporal logic at least with respect to backward propagation (e.g., weights update). &#x201c;Freezing&#x201d; or the bypassing of the temporal logic of a neural network can be especially useful for index images given their relatively lower level of diversity.</p><p id="p-0112" num="0111">In accordance with yet another aspect, the neural network-based base caller <b>300</b> can be trained over multiple training iterations, such that in each iteration each one of the respective sequencing images is processed through the neural network-based base caller <b>300</b> to generate respective base call predictions. In doing so, one or more weights of the neural network-based base caller can be effectively improved (e.g., via backpropagation) based on comparisons of the respective base call predictions against respective base call ground truths.</p><p id="p-0113" num="0112">As noted above, in accordance with other aspects, improved techniques also include various combinations including, for example, combining the &#x201c;normalization&#x201d; of sequencing images with the reordering of sequencing images and/or with effective &#x201c;freezing&#x201d; of the temporal logic. These and other aspects and implementations will become even more apparent in view of the description below with accompanying <figref idref="DRAWINGS">FIGS. <b>4</b>A-<b>21</b></figref>. Since freezing is applied during the training of the neural network-based base caller <b>300</b>, it can be combined with the reordering of sequencing images and/or the normalization during training. In other implementations, the trained neural network-based base caller <b>300</b> trained based on the freezing can use the reordering of sequencing images and/or the normalization during inference.</p><p id="p-0114" num="0113">To further elaborate, <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> depicts another multi-cycle neural network model (or neural network-based base caller <b>300</b>) in accordance with another implementation. Referring to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the neural network-based base caller <b>300</b> can effectively process the sequencing images <b>406</b> generated during a sequencing process. It should be noted that the sequencing images <b>406</b> can include index images associated with one or more indexes of one or more libraries (e.g., index A, index B shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) as well as target images associated with a target (e.g., a template strand itself without an index).</p><p id="p-0115" num="0114">In essence, the neural network-based base caller <b>300</b> can be executed to process the sequencing images <b>406</b> as input, for example, similar to or substantially like the processing described above with respect to the neural network-based base caller <b>300</b> (shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Again, a five-cycle input, one-cycle output neural network is shown here as an example. As such, the inputs to the multi-cycle neural network model (or the neural network-based base caller <b>300</b>) depicted in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> can include five spatially aligned patches from the tile data arrays of five sensing cycles of a given tile. However, for simplification, segregated stacks and other components of the neural network-based base caller <b>300</b> are not shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> (depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). Instead, in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the sequencing images <b>406</b> are each depicted as having multiple datasets corresponding to multiple channels used to generate images in sequence.</p><p id="p-0116" num="0115">In any case, referring to <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the neural network-based base caller <b>300</b> is configured to access a set of sequencing images <b>406</b>. It should be noted that the sequencing images <b>406</b> can be a set of sequencing images arranged in a consecutive order, for example, in the order that they were generated as an index or read in a sequencing cycle. In other words, sequencing images in the set of sequencing images <b>406</b> can be arranged in the order of sequencing cycles, N&#x2212;2, N&#x2212;1, N, N+1, N+2 (or cycles 1, 2, 3, 4 and 5) as shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>. As such, the neural network-based base caller <b>300</b> can process the sequencing images <b>406</b> as input, in a similar manner as described above with respect to the neural network-based base caller <b>300</b> (shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>).</p><p id="p-0117" num="0116">In processing the sequencing images <b>406</b> as input, the neural network-based base caller <b>300</b> can effectively perform training and/or can be trained for base calls (i.e., training the neural network-based base caller <b>300</b>) over a plurality of training iterations, such that each one of the respective training iterations processes its respective set of the sequencing images (<b>406</b>) through the neural network-based base caller <b>300</b> (through its stacks and/or its layers) to generate a respective base call prediction, for example, a base call <b>422</b> for cycle N (or cycle 3). In doing so, an error can be determined based on its associated ground truth. For example, an error <b>424</b> can be determined based on a cycle N (or cycle 3) ground truth <b>426</b>. Based on the error <b>424</b>, weights of the neural network-based base caller <b>300</b> can be updated via backward propagation <b>432</b>.</p><p id="p-0118" num="0117">Moreover, the neural network-based base caller <b>300</b> can also be configured to reorder the sequencing images <b>406</b> and/or process one or more reordered sequencing images as will be described in greater detail below (e.g., rearranging sequencing images in an order that is different from the consecutive order of the images in the sequencing cycles used to generate the images). Generally, the sequencing images <b>406</b> can be reordered (or rearranged) from a first order to a second order, different than the first order, as will be described in greater detail below. For ease of illustration here, rather than a more general approach, a more specific approach, namely, &#x201c;shuffling&#x201d; is presented first before presenting more general approaches including, for example, &#x201c;permutation&#x201d; that may yield more reordering options.</p><heading id="h-0013" level="2">Reordering by Shuffling</heading><p id="p-0119" num="0118">To further elaborate, <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> depicts the neural network-based base caller <b>300</b> in connection with a reordered set of the sequencing images <b>446</b> in accordance with one implementation. Although it is not shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, it should be noted that the neural network-based base caller <b>300</b> can also be configured to generate the reordered set of the sequencing images <b>446</b>, by effectively reordering the sequencing images <b>406</b> before processing the reordered set of the sequencing images <b>446</b> as input. Alternatively, the neural network-based base caller <b>300</b> can also be configured to merely process the reordered set of the sequencing images <b>446</b> as input without generating the reordered set of the sequencing images <b>446</b>. For simplicity, the neural network-based base caller <b>300</b> is depicted herein to receive the reordered set of the sequencing images <b>446</b> as input, but it is apparent that it can also be configured to reorder a set of sequencing images in a first order (e.g., sequencing images <b>406</b>) to generate a reordered set of sequencing images in a second order, different than the first order (e.g., sequencing images <b>446</b>).</p><p id="p-0120" num="0119">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the neural network-based base caller <b>300</b> can receive the reordered set of sequencing images <b>446</b> as input. In any case, the reordered set of the sequencing images <b>446</b> is a reordering of the set of the (original) sequencing images <b>406</b> representing a first order, namely: cycle 1, cycle 2, cycle 3, cycle 4 and cycle 5 (e.g., the original sequencing cycles as the images are/were obtained). As such, the sequencing images of the reordered set of the sequencing images <b>446</b> are in a second order which is different than the first order. In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b>B</figref>, the reordered set of the sequencing images <b>446</b> is depicted as having the following reordering: cycle 4, cycle 5, cycle 3, cycle 1, and cycle 2. As such, the reordering depicted in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref> shows an example where an image in a central (cycle) position, namely cycle N, is kept fixed at its central position (cycle 3) while the images from left flanking cycles (N&#x2212;2 and N&#x2212;1) corresponding to cycle 1 and cycle 2 are effectively swapped with the images corresponding to cycle 4 and cycle 5 from the right flanking cycles (N+1 and N+2) relative to the fixed central position (cycle). This kind of reordering, swapping, or exchange can be referred to as &#x201c;shuffling.&#x201d;</p><p id="p-0121" num="0120">More generally, in reordering by &#x201c;shuffling&#x201d; a set of sequencing images (e.g., sequencing images <b>406</b>), a particular one or more image(s) of a set of sequencing images can be kept at its position, and the position of one or more of the other sequencing images can be changed. In other words, at least one of the sequencing images at a right flanking position and/or a left flanking position can be changed between a first order of a first set of sequencing images (e.g., sequencing images <b>406</b>) and a second order (or reordering) of the same set sequencing images (e.g., sequencing images <b>446</b>). As another example, at least some of the sequencing images at one or more right flanking positions and/or one or more left flanking positions can be kept fixed between a first and a second order.</p><p id="p-0122" num="0121">In processing the sequencing images <b>446</b> as input, the neural network-based base caller <b>300</b> can effectively perform training and/or can be trained for base calls (i.e., training the neural network-based base caller <b>300</b>) over a plurality of training iterations, such that each one of the respective training iterations processes its respective set of the sequencing images (<b>446</b>) through the neural network-based base caller <b>300</b> (through its stacks and/or its layers) to generate a respective base call prediction, for example, a base call <b>452</b> for cycle N (or cycle 3). In doing so, an error can be determined based on its associated ground truth. For example, an error <b>454</b> can be determined based on a cycle N (or cycle 3) ground truth <b>426</b>. Based on the error <b>454</b>, weights of the neural network-based base caller <b>300</b> can be updated via backward propagation <b>462</b>.</p><p id="p-0123" num="0122">To provide another example, <figref idref="DRAWINGS">FIG. <b>4</b>C</figref> depicts another exemplary reordered set of the sequencing images <b>476</b> as &#x201c;cycle 1, cycle 4, cycle 3, cycle 2, and cycle 5).&#x201d; In processing the sequencing images <b>476</b> as input, the neural network-based base caller <b>300</b> can effectively perform training and/or can be trained for base calls (i.e., training the neural network-based base caller <b>300</b>) over a plurality of training iterations, such that each one of the respective training iterations processes its respective set of the sequencing images (<b>476</b>) through the neural network-based base caller <b>300</b> (through its stacks and/or its layers) to generate a respective base call prediction, for example, a base call <b>482</b> for cycle N (or cycle 3). In doing so, an error can be determined based on its associated ground truth. For example, an error <b>484</b> can be determined based on a cycle N (or cycle 3) ground truth <b>426</b>. Based on the error <b>484</b>, weights of the neural network-based base caller <b>300</b> can be updated via backward propagation <b>492</b>.</p><p id="p-0124" num="0123">Similarly, <figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B, and <b>5</b>C</figref> depict yet other exemplary reordered sets of the sequencing images <b>506</b>, <b>546</b>, and <b>576</b>, in accordance with yet other exemplary implementations, where an image in a central cycle (N) (or cycle 3) remains fixed while one or more other images of one or more cycles have been reordered. For example, in the reordered set of the sequencing images <b>506</b>, cycle 2 and cycle 3 are both retained in their original positions, but in the reordered sets of the sequencing images <b>546</b> and <b>576</b>, only cycle 3 remains fixed, where one or more other cycles may have been changed and/or swapped between left and right flanks relative to cycle 3 that remains fixed in its central position.</p><p id="p-0125" num="0124">Referring to <figref idref="DRAWINGS">FIGS. <b>5</b>A, <b>5</b>B, and <b>5</b>C</figref>, it should be noted that the neural network-based base caller <b>300</b> can process each one of the reordered (or shuffled) sets of sequencing images <b>506</b>, <b>546</b>, and <b>576</b>, in a similar manner as described above with respect to the neural network-based base caller <b>300</b>. In processing the reordered sets of sequencing images <b>506</b>, <b>546</b>, and <b>576</b>, the neural network-based base caller <b>300</b> can effectively perform training or it can be trained for base calls (i.e., the neural network-based base caller <b>300</b> can be trained) over a plurality of training iterations, such that each one of the respective training iterations processes its respective set of the reordered set of sequencing images <b>506</b>, <b>546</b>, and <b>576</b> through the neural network (not shown) in the neural network-based base caller <b>300</b> through its stacks and/or its layers (not shown) to generate a respective base call prediction, for example, a base call <b>522</b> for cycle N (or cycle 3) as depicted in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. In doing so, an error can be determined based on its associated ground truth. For example, an error <b>524</b> can be determined based on a cycle N (or cycle 3) ground truth <b>526</b>, as also depicted in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. As a result, one or more factors (e.g., weights) can be updated via a process, generally known as backward propagation, as an important part of the training iterations. As an example, backward propagation <b>532</b> is depicted in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. Other examples of errors <b>554</b> and <b>584</b> and corresponding backward propagations <b>562</b> and <b>592</b> are also depicted.</p><p id="p-0126" num="0125">The examples of reordering noted above represent more specific exemplary implementations where, for example, one or more of the sequencing images can be determined to remain fixed in their original position (e.g., a center position N is kept fixed as shown in <figref idref="DRAWINGS">FIGS. <b>4</b>A, <b>4</b>B, and <b>4</b>C</figref>) or one or more sequencing images can be determined to be moved from their original location. However, it will be appreciated that a set of sequencing images can be virtually reordered in any way possible and/or desired, including, for example, by using random selection with or without replacement (or with or without repetition).</p><p id="p-0127" num="0126">Moreover, it will also be appreciated that the reordering of a set of sequencing images into one or more reordered sets of sequencing images can be achieved through even more methodologies, in a manner that may even better address the needs of base calling using the neural network-based base caller <b>300</b>. By way of example, a subset selector (e.g., subset selector component, subset selector logic) can be configured to effectively generate a reordering of a set of sequencing images (reordered set of sequencing images) based on one or more parameters, as will be described below in accordance with yet other implementations.</p><heading id="h-0014" level="2">Random Permutation</heading><p id="p-0128" num="0127">To further elaborate, <figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a subset selector <b>612</b> (e.g., subset selector component, subset selector logic) configured to effectively generate a reordering of a set of sequencing images (reordered set of sequencing images) as a reordered subset of sequencing images, based on two (2) parameters in accordance with one implementation. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a first parameter (M) represents a set of sequencing images (e.g., a complete set of sequencing images that can be generated in a sequencing run, which currently can be eight (8) or ten (10) in some existing implementations). For simplicity, in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the first parameter (M) is depicted to be eight (8) representing eight (8) consecutive cycles (cycle 1, cycle 2 . . . cycle 8) <b>602</b>. In addition, in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a second parameter (N) is depicted to be five (5), six (6) or seven (7). Generally, the second parameter (N) can represent the size of a subset for selection from the set of M images. As such, the second parameter (N) can, for example, represent a &#x201c;window&#x201d; for selection of the input to the neural network-based base caller <b>300</b>. Although not shown, it should be noted that the subset selector <b>612</b> can, for example, be provided by the neural network-based base caller <b>300</b> (shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). Of course, the subset selector <b>612</b> can also be provided as a separate entity configured as it will be readily understood by those skilled in the art. The reordering logic implemented by the subset selector <b>612</b> can be random or driven by sequentially progressing to different permutations and combinations, either until exhaustion or up till early exit due to satisfaction of a terminal condition.</p><p id="p-0129" num="0128">In any case, as depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the subset selector <b>612</b> can effectively obtain (e.g., determine, identify, receive as input) the set of M, or eight consecutive cycles (ordered consecutively from cycle 1-cycle 8) <b>602</b> in the example and generate various subsets of N, for example, five (5) cycles in various reordered subsets as reordered subsets of sequencing images <b>626</b>, <b>636</b>, <b>646</b>, and so on. As the exemplary rearranged subset of sequencing images <b>626</b> shows, there can be repetition of cycles, for example, cycle 8 (or more accurately the images in cycle 8) can be repeated in the reordered subset of sequencing images <b>626</b> as generated by the subset selector <b>612</b>.</p><p id="p-0130" num="0129">As also depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the reordered subsets of sequencing images <b>656</b> and <b>666</b> respectively depict examples of subsets where N is six (6) and seven (7). Generally, for the set of M elements (e.g., eight (8)), about M to the power of N (M<sup>N</sup>) (e.g., <b>8</b><sup>5</sup>) subsets can be determined when there is repetition (i.e., the same cycle can be repeated). The number of possible subsets can be slightly less if no repetition is allowed and/or desired, as it will be readily known to those skilled in the art. As those skilled in the art will also readily appreciate, various &#x201c;permutations&#x201d; and &#x201c;combinations&#x201d; can be selected from an original set of sequencing images (e.g., sequencing images <b>602</b> with cycles 1-cycle 8) based on one or more parameters (e.g., M and N parameters).</p><p id="p-0131" num="0130">Generally, multiple subsets of N sequencing images can be selected from a set of M sequencing images, where M&#x3e;N. In other words, there can be multiple images in a single cycle, where each one of the multiple images can, for example, correspond to a single channel, as for example, two channels are depicted for the sequencing images shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. It should also be noted that respective subsets of the multiple selected subsets can arrange the N sequencing images in respective sequences of the N sequencing images. Also, the respective sequences can have different combinations of N sequencing images (i.e., the combinations of the N sequencing images can vary from sequence-to-sequence). In addition, the combinations can be combinations with or without replacement. Similarly, the permutations can be permutations with or without replacement. The respective sequences can arrange the N sequencing images in accordance with consecutive or non-consecutive sequencing cycles. It should also be noted that N can be determined to be a constant for the selection of respective subsets of the multiple selected subsets, or N can be selected to be a variable, so that N may vary between the respective subsets of the multiple selected subsets. Furthermore, the respective subsets of the multiple selected subsets can be selected to have or not have any overlapping sequencing images.</p><p id="p-0132" num="0131">Training of the neural network-based base caller <b>300</b> can be achieved over multiple training iterations, where each one of the training iterations processes its respective sequences of the N sequencing images through the neural network-based base caller <b>300</b> to generate respective base call predictions, and update the weights (e.g., via backpropagation) of the neural network-based base caller <b>300</b>, based on the comparison of the respective base call predictions against the respective base call ground truths.</p><p id="p-0133" num="0132">As noted above, the subset selector <b>612</b> (shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) can, for example, be provided by the neural network-based base caller <b>300</b> (shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) or as a separate entity. However, for simplicity, additional implementations depict the neural network-based base caller <b>300</b> as receiving already reordered (or rearranged) sequencing images. The depicted reordered sequencing images can, for example, be generated by the subset selector <b>612</b> (shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0134" num="0133">More specifically, in accordance with one implementation, <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> depicts a reordered subset of sequencing images <b>626</b> (also shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) as it can, for example, be generated by the subset selector <b>612</b> (shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) in connection with the neural network-based base caller <b>300</b> (shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). Referring to <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, the neural network-based base caller <b>300</b> can receive, as input, the reordered subset of sequencing images <b>626</b>. The neural network-based base caller <b>300</b> can then process the reordered subset of sequencing images <b>626</b>, in a similar manner as discussed above, to generate one or more base call predictions <b>702</b>, while utilizing the base call ground truths <b>706</b> in a backward propagation <b>712</b>, in consideration of an error <b>704</b>.</p><p id="p-0135" num="0134">Similarly, <figref idref="DRAWINGS">FIGS. <b>7</b>B and <b>7</b>C</figref> depict the neural network-based base caller <b>300</b> in connection with the reordered subsets of sequencing images <b>636</b> and <b>646</b>, respectively, where base call predictions <b>722</b> and <b>742</b> can be made, while utilizing the base call ground truths <b>726</b> and <b>746</b> in backward propagations <b>732</b> and <b>752</b>, in consideration of errors <b>724</b> and <b>744</b>, in a similar manner as disused above. Although not shown, additional reordered subsets of sequencing images can be provided to the neural network-based base caller <b>300</b> and processed by the neural network-based base caller <b>300</b> for the effective training of its neural network for base calling. In this way, the neural network-based base caller <b>300</b> can be trained over a number of training iterations, where each one of the training iterations processes its respective reordered sequences of N (e.g., five) sequencing images through the neural network-based base caller <b>300</b> to generate respective base call predictions, and to update the weights (e.g., via backpropagation) of the neural network-based base caller <b>300</b>, based on the comparison of the respective base call predictions against the respective base call ground truths.</p><p id="p-0136" num="0135">To elaborate even further, <figref idref="DRAWINGS">FIGS. <b>8</b>A and <b>8</b>B</figref> depict additional examples of reordered subsets of sequencing images <b>656</b> and <b>666</b> (also shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) in connection with the neural network-based base caller <b>300</b>, where the reordered subsets of sequencing images <b>656</b> and <b>666</b> can also serve in the training iterations process, alone or in combination with other training iterations, for example, in combination with training iterations using reordered subsets of sequencing images <b>626</b>, <b>636</b>, and <b>646</b> (shown in <figref idref="DRAWINGS">FIGS. <b>7</b>A, <b>7</b>B, and <b>7</b>C</figref>) in accordance with additional implementations. As discussed above, the training will include determining errors <b>804</b> and <b>824</b> based on comparing base calling predictions <b>802</b> and <b>822</b> against base call ground truths <b>806</b> and <b>826</b> and executing back propagation iterations <b>812</b> and <b>832</b> based on the errors <b>804</b> and <b>824</b>, respectively.</p><heading id="h-0015" level="2">Bypassing Retraining of the Trained Temporal Logic</heading><p id="p-0137" num="0136">As noted above and discussed with respect to a few exemplary implementations, reordered sets of sequencing images can be generated in accordance with one implementation of the technology disclosed. In accordance with another aspect, the neural network-based base caller <b>300</b> can be trained (or further trained) by only utilizing the spatial logic of the (or previously trained) neural network-based base caller <b>300</b>. In other words, the temporal logic of the neural network-based base caller <b>300</b> can be effectively &#x201c;frozen&#x201d; or bypassed in a second (or further) training where only the spatial logic (not the temporal logic) is trained at least with respect to backward propagation (e.g., weights update). A different (or second) training dataset can be used in the second (or further) training phase, after the initial (or first) training has been completed based on an initial (or first) training dataset.</p><p id="p-0138" num="0137">A temporal logic can be provided, in addition to the spatial logic in the neural network-based base caller <b>300</b>. Referring back to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the hierarchy <b>382</b> of the temporal combinatorial layers <b>382</b> can represent a temporal logic of the neural network-based base caller <b>300</b>, and the segregated stacks <b>302</b>, <b>304</b>, <b>306</b>, <b>308</b> and <b>310</b> can collectively represent the spatial logic of the neural network-based base caller <b>300</b>.</p><p id="p-0139" num="0138">To elaborate further, <figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an initial (first) training <b>902</b> and a further (second) training <b>932</b> in accordance with two implementations. Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the initial training <b>902</b> can be used to effectively train the neural network-based base caller <b>300</b> (also shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) for making base call predictions. In doing so, the neural network-based base caller <b>300</b> can receive a first training dataset <b>912</b> and process it through two of its components, namely, a spatial logic <b>914</b> and a temporal logic <b>916</b>. As noted above, the temporal logic <b>916</b> can, for example, be provided as the temporal combinatorial layers <b>382</b> (depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref>), and the spatial logic <b>914</b> can, for example, be provided as the segregated stacks <b>302</b>, <b>304</b>, <b>306</b>, <b>308</b> and <b>310</b> (also depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref>) for the neural network-based base caller <b>300</b> depicted in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0140" num="0139">In processing the first training dataset <b>912</b> first through the spatial logic <b>914</b> and then through the temporal logic <b>916</b>, the neural network-based base caller <b>300</b> can utilize backward propagation to update weights <b>926</b> and <b>928</b> associated with the spatial logic <b>914</b> and the temporal logic <b>916</b>, respectively. In other words, both the spatial logic <b>914</b> and the temporal logic <b>916</b> can be utilized to make updates via backward propagation, in a similar manner a discussed above. It should be noted that the first training dataset <b>912</b> can be an ordered set of sequencing images or a set of reordered (reordered set) sequencing images, generated in a similar manner as discussed above. However, it may be desirable for at least some applications to initially use an ordered set of sequencing images in an original consecutive order of sequencing images (cycle 1, cycle 2, . . . cycle M) in the initial training <b>902</b> before a further training <b>932</b> (or second training) takes place.</p><p id="p-0141" num="0140">In any case, after the initial (or first) training <b>902</b> has been completed, a further (or second) training <b>932</b> can be performed. Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the further (or second) training <b>932</b> can be performed by the neural network-based base caller <b>300</b>. In doing so, the neural network-based base caller <b>300</b> can receive and process the second training dataset <b>942</b> through two of its components, namely, a trained spatial logic <b>944</b> and a trained temporal logic <b>946</b>. In processing the second training dataset <b>942</b>, the neural network-based base caller <b>300</b> can utilize backward propagation to update weights <b>956</b> only of the trained spatial logic <b>944</b>. That is, during the further (or second) training <b>932</b>, the backward propagation does not update the weights of the trained temporal logic <b>946</b> to effectively &#x201c;freeze&#x201d; or bypass the trained temporal logic <b>946</b>. In other words, backward propagation of gradient updates through the trained temporal logic <b>946</b> is bypassed during the further (or second) training <b>932</b>. This means that in the further (or second) training <b>932</b>, backward propagation is performed only through the trained spatial logic <b>944</b>, where gradient updates are determined based on comparisons of the base call predictions against the base call ground truths only for the trained spatial logic <b>944</b> (not for the trained temporal logic <b>946</b>).</p><p id="p-0142" num="0141">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the second training dataset <b>942</b> can be an ordered set of sequencing images, as originally obtained during sequencing, or a set of reordered (reordered set) sequencing images generated in a similar manner as discussed above. For example, the sequencing images in the first training dataset <b>912</b> can include read sequencing images generated for read sequencing cycles of a sequencing run. As another example, the second training dataset <b>942</b> can include index sequencing images generated for index sequencing cycles of the sequencing run. In the example, the index sequencing images can be reordered sequencing images or sequencing images obtained in a consecutive order. As yet another example, the sequencing images in the second training dataset <b>942</b> can include a combination of read sequencing images and index sequencing images.</p><p id="p-0143" num="0142">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, it should be noted that the further (or second) training <b>932</b> can effectively retrain the trained spatial logic <b>944</b> to provide it as a retrained spatial logic (not shown). The retrained spatial logic can be different than the trained spatial logic <b>944</b> in its original state prior to the inception of the further (or second) training <b>932</b>, while the trained temporal logic <b>946</b> can be effectively &#x201c;frozen&#x201d; to remain unchanged after the further (or second) training <b>932</b>. After the further (or second) training <b>932</b> has completed, the combination of the retrained spatial logic (not shown) and the trained temporal logic <b>946</b> can be applied to or can be used at inference to generate new base call predictions for a set of (new) sequencing images. It will be appreciated that the bypassing of the backward propagation updating (i.e., retraining) for the trained temporal logic <b>946</b> can prevent the neural network-based base caller <b>300</b> from learning temporal dependencies (e.g., sequential, chronological, or consecutive patterns) between intensity features of the index sequencing images. Learning temporal dependencies can hinder the effectiveness of training for calling index images at inference. As a result of avoiding the learning of temporal dependencies during the further (or second) training <b>932</b>, while allowing the trained spatial logic <b>944</b> to be further trained by using backward proportion to provide the weights update <b>956</b>, even more accurate base calls can be achieved during inference.</p><p id="p-0144" num="0143">To elaborate even further, <figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts a method <b>1000</b> for training a neural network-based base caller in accordance with one implementation. The method <b>1000</b> can, for example, be used by the neural network-based base caller <b>300</b> (shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>) to train its neural network. It should be noted that the neural network-based base caller can include a spatial logic configured to extract intensity features confined to per-cycle sequencing images in an input, and a temporal logic configured to extract intensity features spanning pan-cycle sequencing images in the input. Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, initially, the spatial logic and the temporal logic of the neural network-based base caller are trained (<b>1002</b>) on a first training dataset of sequencing images to generate a trained spatial logic and a trained temporal logic. Next, further (or additional) training is performed (<b>1012</b>) using a second training dataset by further training only the spatial logic of the neural network-based base caller on the second training dataset of sequencing images to generate a retrained spatial logic. It should be noted that the further training (<b>1012</b>) can comprise the forward propagating of sequencing images in the second training dataset through the trained spatial logic and the trained temporal logic and generating further base call predictions. However, in the further training (<b>1012</b>) backward propagating is limited only through the trained spatial logic (not the temporal logic) such that further gradient updates are determined based on the comparison of the further (or additional) base call predictions against the further base call ground truths, thereby bypassing backward propagating the further gradient updates through the trained temporal logic. After the further training (<b>1012</b>), only the trained spatial logic of the neural network-based base caller is retrained (further trained) on a second training dataset of sequencing images to generate a retrained/further/additionally trained spatial logic. The method <b>1000</b> can proceed to apply (<b>1022</b>) the retrained spatial logic and the trained temporal logic of the neural network-based base caller to generate new base call predictions for (or on) new sequencing images at or during inference where, for example, base calls can be made for a target sample or template strand with an added index. The method <b>1000</b> can end after new base call predictions have been generated at or during inference. For example, the sequencing images in the first training dataset can include read sequencing images generated for read sequencing cycles of a sequencing run, as noted above. As another example, the second training dataset can include index sequencing images generated for index sequencing cycles of the sequencing run. As yet another example, the sequencing images in the second training dataset can include a combination of the read sequencing images and index sequencing images.</p><p id="p-0145" num="0144">To elaborate still further, <figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts another method <b>1100</b> for training a neural network-based base caller in accordance with another implementation. The method <b>1100</b> can, for example, be used by the neural network-based base caller <b>300</b> to train its neural network. The method <b>1000</b> can proceed in a similar manner as discussed above with respect to the method <b>1000</b> (shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>) to train (<b>1102</b>) the spatial logic and the temporal logic of the neural network-based base caller on a first training dataset of sequencing images to generate a trained spatial logic and a trained temporal logic. In addition, further (or additional) training can be performed (<b>1112</b>) on a second training dataset by further training only the spatial logic of the neural network-based base caller on the second training dataset of sequencing images to generate a retrained spatial logic, in a similar manner as discussed above. However, as depicted in FIG. <b>11</b>, the method <b>1100</b> can configure (<b>1122</b>) the neural network-based base caller with the retrained spatial logic and the trained temporal logic, thereby allowing future base calls (e.g., during inference) to be executed (or be configured to execute or be configured for the purpose of executing) based on the neural network-based base caller with the retrained spatial logic.</p><p id="p-0146" num="0145">More generally, a neural network-based base caller can be trained for the base calling of sequencing images in accordance with one aspect. To further elaborate, <figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts a method <b>1200</b> for training a neural network-based base caller for the base calling of sequencing images, in accordance with one implementation. The method <b>1200</b> can, for example, be used by the neural network-based base caller <b>300</b> to train its neural network. Referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, initially, multiple sequences of N sequencing images are obtained <b>1202</b> (e.g., accessed, received, determined, identified). Thereafter, a neural network-based base caller is trained (<b>1212</b>) over multiple training iterations, such that in each one of the iterations, respective sequences of the N sequencing images are processed through the neural network-based base caller to generate respective base call predictions. In doing so, one or more weights of the neural network-based base caller can be effectively improved (e.g., via backpropagation) for example, based on comparisons of the respective base call predictions against the respective base call ground truths.</p><p id="p-0147" num="0146">To elaborate further yet, <figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts another method <b>1300</b> for training a neural network-based base caller in accordance with one implementation. The method <b>1300</b> can, for example, be used by the neural network-based base caller <b>300</b> to train its neural network. Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, initially, a set of sequencing images are accessed (<b>1302</b>). It should be noted that the sequencing images in the set of sequencing images are arranged in a first order. Next, the sequencing images are reordered <b>1312</b> (e.g., randomly shuffled, permutated, one or more images are kept fixed in their original location while one or more other images are relocated from their original location in the sequence) in one or more reordered sets of sequencing images, such that the sequencing images in a reordered set are arranged in a second order that is different from the first order. After the reordering <b>1312</b> of the sequencing images from a first order to a second order, the reordered set of the sequencing images is processed (<b>1322</b>) to generate at least one base call prediction. Finally, a neural network-based base caller can be trained (<b>1332</b>) over multiple training iterations, such that each one of the respective training iterations processes a respective reordered set of the sequencing images through the neural network-based base caller to generate a respective base call prediction. During the training (<b>1332</b>) over multiple training iterations, each one of the respective training iterations can also improve the weights (e.g., via backpropagation) of the neural network-based base caller based on one or more comparisons of the respective base call predictions against the respective base call ground truths.</p><p id="p-0148" num="0147">It should be noted that the techniques and implementations described above can be combined with other techniques and implementations including the improved techniques described here, as well as other techniques (e.g., normalization of index images). For example, one or more techniques involving the reordering of sequencing images (e.g., shuffling, permutation) and effectively &#x201c;freezing&#x201d; a temporal logic (or layer) or bypassing the trained temporal logic that have been described above can be combined with one or more normalization techniques, for example, by using one or more of the normalization techniques provided in U.S. Patent Application No. 62/979,384, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED BASE CALLING OF INDEX SEQUENCES,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1015-1/IP-1857-PRV), which has been incorporated by reference herein in its entirety and for all purposes. For example, in accordance with one implementation, an index image from a current index sequencing cycle can be normalized based on one or more of the following: (i) intensity values of index images from one or more preceding index sequencing cycles, (ii) intensity values of index images from one or more succeeding index sequencing cycles, and (iii) intensity values of index images from the current index sequencing cycle.</p><p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows a flow chart <b>1400</b> of a system that executes the disclosed base calling, in accordance with one implementation. An accessing logic is configured to access a set of sequencing images at step <b>1402</b>. Sequencing images in the set of sequencing images are arranged in a first order. A reordering logic is configured to reorder the sequencing images in a reordered set of the sequencing images at step <b>1412</b>. The sequencing images in the reordered set of the sequencing images are arranged in a second order. The first order is different than the second order. A processing logic is configured to process the reordered set of the sequencing images and generate at least one base call prediction <b>1422</b>.</p><heading id="h-0016" level="2">Normalization of Index Images</heading><p id="p-0150" num="0149">Generally, normalization across index sequencing cycles also includes normalization across image channels within the image data of the index sequencing cycles. For example, consider three index sequencing cycles: a first index sequencing cycle, a second index sequencing cycle, and a third index sequencing cycle. Also consider that each of the first, second, and third index sequencing cycles has two index images: a first index image (e.g., red index image) in a first image channel (e.g., red channel) and a second index image (e.g., green index image) in a second image channel (e.g., green channel). A red index image from the second index sequencing cycle is normalized based on (i) the intensity values of red and green images from the first index sequencing cycle, (ii) the intensity values of red and green images from the third index sequencing cycle, and (iii) the intensity values of red and green images from the second index sequencing cycle. A green index image from the second index sequencing cycle is normalized based on (i) the intensity values of red and green images from the first index sequencing cycle, (ii) the intensity values of red and green images from the third index sequencing cycle, and (iii) the intensity values of red and green images from the second index sequencing cycle. In other implementations, the images can be in blue and violet color channels instead of or in addition to the red and green channels. Additional details for &#x201c;normalization&#x201d; techniques are provided, for example, in U.S. Patent Application No. 62/979,384, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED BASE CALLING OF INDEX SEQUENCES,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1015-1/IP-1857-PRV), which has been incorporated by reference herein in its entirety and for all purposes.</p><p id="p-0151" num="0150">Furthermore, normalization can include index images from flanking index sequencing cycles because taken together, nucleotides depicted by the index images from the current, preceding, and succeeding index sequencing cycles are cumulatively more diverse than nucleotides depicted only by the index images from the current index sequencing cycle. Expanding the normalization to index images from the flanking index sequencing cycles also includes at least one index image from the preceding and/or succeeding index sequencing cycles that depicts one or more nucleotides in a detectable signal state.</p><p id="p-0152" num="0151">It should be noted that normalization of sequencing images (e.g., index images) can be performed either prior to reordering of the sequencing images or after they have been reordered. Similarly, sequencing images can be normalized for the first and/or second datasets of sequencing images, selectively, or as a complete set, during the first and/or second training phases of the neural network-based base caller <b>300</b>. For example, one or more index images provided in a second training dataset in the further (or second) training of the neural network can be normalized in combination with effectively &#x201c;freezing&#x201d; the temporal logic of the neural network, in accordance with one exemplary implementation.</p><heading id="h-0017" level="2">Augmentation of Index Images</heading><p id="p-0153" num="0152">To further elaborate, the exemplary implementation noted above and another implementation involving the combination of normalization and reordering of sequencing images are discussed below. In addition, measured comparisons with other techniques for training the neural network-based base caller <b>300</b> (e.g., &#x201c;augmentation&#x201d;) are discussed too. For example, one implementation of preprocessing that uses augmentation can be used, where an image augmenter preprocesses the index images and the target images using an augmentation function. In the example, an image augmenter can multiply the intensity values of the index images and the target images with a scaling factor and add an offset value to the multiplication's result. In another exemplary implementation, the image augmenter changes the contrast of the index images and the target images. In yet another exemplary implementation, the image augmenter changes the focus of the index images and the target images. Additional details for &#x201c;augmentation&#x201d; techniques are provided, for example, in U.S. Patent Application No. 62/979,384, titled &#x201c;ARTIFICIAL INTELLIGENCE-BASED BASE CALLING OF INDEX SEQUENCES,&#x201d; filed 20 Feb. 2020 (Attorney Docket No. ILLM 1015-1/IP-1857-PRV), which has been incorporated by reference herein in its entirety and for all purposes.</p><heading id="h-0018" level="2">Technical Effect and Performance Results as Objective Indicia of Inventiveness</heading><p id="p-0154" num="0153">Illumina Enrichment kits (DNA and RNA based) help to isolate and enrich specific regions of interest in a genome or transcriptome for sequencing. For example, Illumina DNA Prep with Enrichment (formerly known as Nextera Flex for Enrichment) with the TruSight Cancer Panel targets 94 genes and 284 SNPs that are associated with various cancers. The following describes the principles of an enrichment reaction, and the commonly used terminology when describing Illumina enrichment kits. With enrichment kits, &#x2018;Reaction&#x2019; (abbreviated &#x201c;rxn&#x201d;) refers to the number of enrichment reactions that are provided with the kit. &#x2018;Plexity&#x2019; (abbreviated &#x201c;plex&#x201d;) refers to the number of pre-enriched libraries that are pooled together in an enrichment reaction. For example, an 8-rxn&#xd7;12-plex kit has enough reagents to perform eight enrichments. In each enrichment reaction, 12 libraries can be pooled. Therefore, the total number of samples that can be prepared with this kit is 96 samples. The following figure illustrates how three libraries are pooled in one enrichment reaction. <figref idref="DRAWINGS">FIG. <b>23</b></figref> depicts an example of 96 indexes.</p><p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows measured base calling error rates for the previously disclosed &#x201c;augmentation&#x201d; and &#x201c;normalization&#x201d; implementations of the neural network-based base caller <b>300</b>, as applied on 1-Plex sequencing data. The results show that when the neural network-based base caller <b>300</b> is trained with index cycles using single cycle normalization (red), the error rates for certain cycles are really high. Blue and green curves show implementations of the neural network-based base caller <b>300</b> using augmentation and no normalization. They show reasonable performance but would be harder to implement because of additional floating point complexity. This shows the need for a better solution.</p><p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. <b>16</b></figref> compares measured base calling error rates for three different implementations of the neural network-based base caller <b>300</b>, as applied on different complexities (e.g., 96-Plex, 1-Plex, and 3-Plex). The first implementation (in red) is the currently disclosed &#x201c;reordering&#x201d; or &#x201c;shuffling&#x201d; implementation of the neural network-based base caller <b>300</b> using the joint training technique discussed above. The second implementation (in blue) is the currently disclosed &#x201c;freeze&#x201d; implementation of the neural network-based base caller <b>300</b> using the joint training technique discussed above. The third implementation (in green) is the previously disclosed &#x201c;augmentation with no normalization&#x201d; implementation of the neural network-based base caller <b>300</b>. As shown by the performance charts, both the currently disclosed implementations outperform the previously disclosed implementation.</p><p id="p-0157" num="0156"><figref idref="DRAWINGS">FIG. <b>17</b></figref> compares measured base calling error rates for three different implementations of the neural network-based base caller <b>300</b>, as applied on different complexities (e.g., 4-Plex, 2-Plex, and 1-Plex). The first implementation (in red) is the currently disclosed &#x201c;reordering&#x201d; or &#x201c;shuffling&#x201d; implementation of the neural network-based base caller <b>300</b> using the joint training technique discussed above. The second implementation (in green) is the currently disclosed &#x201c;freeze&#x201d; implementation of the neural network-based base caller <b>300</b> using the joint training technique discussed above. The third implementation (in blue) is the currently disclosed &#x201c;random permutation&#x201d; implementation (<figref idref="DRAWINGS">FIG. <b>6</b></figref>) of the neural network-based base caller <b>300</b> using the joint training technique discussed above. As shown by the performance charts, all three of the currently disclosed implementations have comparable performance, with the random permutation showing slightly better results.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>18</b></figref> depicts a plot that compares the &#x201c;demultiplex&#x201d; results for tiles from runs with different Plexities for RTA (blue), a previously disclosed deep learning implementation (orange) (DL 3c Aug), and the currently disclosed joint training and shuffling implementation (green) (DL 5c Norm*). The results from the new implementation are especially better for 1-Plex runs when compared to both the RTA and the previously disclosed deep learning implementation. Demultiplex refers to what percentage of the base called index sequences can be resolved to a unique index with one allowed mismatch.</p><heading id="h-0019" level="2">Sequencing System</heading><p id="p-0159" num="0158"><figref idref="DRAWINGS">FIGS. <b>19</b>A and <b>19</b>B</figref> depict one implementation of a sequencing system <b>1900</b>A. The sequencing system <b>1900</b>A comprises a configurable processor <b>1946</b>. The configurable processor <b>1946</b> implements the base calling techniques disclosed herein. The sequencing system is also referred to as a &#x201c;sequencer.&#x201d;</p><p id="p-0160" num="0159">The sequencing system <b>1900</b>A can operate to obtain any information or data that relates to at least one instance of a biological or chemical substance. In some implementations, the sequencing system <b>1900</b>A is a workstation that may be similar to a bench-top device or desktop computer. For example, a majority (or all) of the systems and components for conducting the desired reactions can be within a common housing <b>1902</b>.</p><p id="p-0161" num="0160">In particular implementations, the sequencing system <b>1900</b>A is a nucleic acid sequencing system configured for various applications, including but not limited to de novo sequencing, resequencing of whole genomes or target genomic regions, and metagenomics. The sequencer may also be used for DNA or RNA analysis. In some implementations, the sequencing system <b>1900</b>A may also be configured to generate reaction sites in a biosensor. For example, the sequencing system <b>1900</b>A may be configured to receive a sample and generate surface attached clusters of clonally amplified nucleic acids derived from the sample. Each cluster may constitute or be part of a reaction site in the biosensor.</p><p id="p-0162" num="0161">The exemplary sequencing system <b>1900</b>A may include a system receptacle or interface <b>1910</b> that is configured to interact with a biosensor <b>1912</b> to perform desired reactions within the biosensor <b>1912</b>. In the following description with respect to <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, the biosensor <b>1912</b> is loaded into the system receptacle <b>1910</b>. However, it is understood that a cartridge that includes the biosensor <b>1912</b> may be inserted into the system receptacle <b>1910</b> and in some states the cartridge can be removed temporarily or permanently. As described above, the cartridge may include, among other things, fluidic control and fluidic storage components.</p><p id="p-0163" num="0162">In particular implementations, the sequencing system <b>1900</b>A is configured to perform a large number of parallel reactions within the biosensor <b>1912</b>. The biosensor <b>1912</b> includes one or more reaction sites where desired reactions can occur. The reaction sites may be, for example, immobilized to a solid surface of the biosensor or immobilized to beads (or other movable substrates) that are located within corresponding reaction chambers of the biosensor. The reaction sites can include, for example, clusters of clonally amplified nucleic acids. The biosensor <b>1912</b> may include a solid-state imaging device (e.g., CCD or CMOS imager) and a flow cell mounted thereto. The flow cell may include one or more flow channels that receive a solution from the sequencing system <b>1900</b>A and direct the solution toward the reaction sites. Optionally, the biosensor <b>1912</b> can be configured to engage a thermal element for transferring thermal energy into or out of the flow channel.</p><p id="p-0164" num="0163">The sequencing system <b>1900</b>A may include various components, assemblies, and systems (or sub-systems) that interact with each other to perform a predetermined method or assay protocol for biological or chemical analysis. For example, the sequencing system <b>1900</b>A includes a system controller <b>1906</b> that may communicate with the various components, assemblies, and sub-systems of the sequencing system <b>1900</b>A and also the biosensor <b>1912</b>. For example, in addition to the system receptacle <b>1910</b>, the sequencing system <b>1900</b>A may also include a fluidic control system <b>1908</b> to control the flow of fluid throughout a fluid network of the sequencing system <b>1900</b>A and the biosensor <b>1912</b>; a fluid storage system <b>1914</b> that is configured to hold all fluids (e.g., gas or liquids) that may be used by the bioassay system; a temperature control system <b>1904</b> that may regulate the temperature of the fluid in the fluid network, the fluid storage system <b>1914</b>, and/or the biosensor <b>1912</b>; and an illumination system <b>1916</b> that is configured to illuminate the biosensor <b>1912</b>. As described above, if a cartridge having the biosensor <b>1912</b> is loaded into the system receptacle <b>1910</b>, the cartridge may also include fluidic control and fluidic storage components.</p><p id="p-0165" num="0164">Also shown, the sequencing system <b>1900</b>A may include a user interface <b>1918</b> that interacts with the user. For example, the user interface <b>1918</b> may include a display <b>1920</b> to display or request information from a user and a user input device <b>1922</b> to receive user inputs. In some implementations, the display <b>1920</b> and the user input device <b>1922</b> are the same device. For example, the user interface <b>1918</b> may include a touch-sensitive display configured to detect the presence of an individual's touch, and it can also be configured to identify a location of the touch on the display. However, other user input devices <b>1922</b> may be used, such as a mouse, touchpad, keyboard, keypad, handheld scanner, voice-recognition system, motion-recognition system, and the like. As will be discussed in greater detail below, the sequencing system <b>1900</b>A may communicate with various components, including the biosensor <b>1912</b> (e.g., in the form of a cartridge), to perform the desired reactions. The sequencing system <b>1900</b>A may also be configured to analyze data obtained from the biosensor to provide a user with desired information.</p><p id="p-0166" num="0165">The system controller <b>1906</b> may include any processor-based or microprocessor-based system, including systems using microcontrollers, reduced instruction set computers (RISC), application specific integrated circuits (ASICs), field programmable gate arrays (FPGAs), coarse-grained reconfigurable architectures (CGRAs), logic circuits, and any other circuit or processor capable of executing functions described herein. The above examples are exemplary only and are thus not intended to limit in any way the definition and/or meaning of the term system controller. In the exemplary implementation, the system controller <b>1906</b> executes a set of instructions that are stored in one or more storage elements, memories, or modules in order to do at least one of the following: obtain and/or analyze detection data. Detection data can include a plurality of sequences of pixel signals, such that a sequence of pixel signals from each of the millions of sensors (or pixels) can be detected over many base calling cycles. Storage elements may be in the form of information sources or physical memory elements within the sequencing system <b>1900</b>A.</p><p id="p-0167" num="0166">The set of instructions may include various commands that instruct the sequencing system <b>1900</b>A or biosensor <b>1912</b> to perform specific operations such as the methods and processes of the various implementations described herein. The set of instructions may be in the form of a software program, which may form part of a tangible, non-transitory computer readable medium or media. As used herein, the terms &#x201c;software&#x201d; and &#x201c;firmware&#x201d; are interchangeable and include any computer program stored in memory for execution by a computer, including RAM memory, ROM memory, EPROM memory, EEPROM memory, and non-volatile RAM (NVRAM) memory. The above memory types are exemplary only and are thus not limiting as to the types of memory usable for storage of a computer program.</p><p id="p-0168" num="0167">The software may be in various forms such as system software or application software. Further, the software may be in the form of a collection of separate programs, or a program module within a larger program or a portion of a program module. The software also may include modular programming in the form of object-oriented programming. After obtaining the detection data, the detection data may be automatically processed by the sequencing system <b>1900</b>A, processed in response to user inputs, or processed in response to a request made by another processing machine (e.g., a remote request through a communication link). In the illustrated implementation, the system controller <b>1906</b> includes an analysis module <b>1944</b>. In other implementations, the system controller <b>1906</b> does not include the analysis module <b>1944</b> and instead has access to the analysis module <b>1944</b> (e.g., the analysis module <b>1944</b> may be separately hosted on the cloud).</p><p id="p-0169" num="0168">The system controller <b>1906</b> may be connected to the biosensor <b>1912</b> and the other components of the sequencing system <b>1900</b>A via communication links. The system controller <b>1906</b> may also be communicatively connected to off-site systems or servers. The communication links may be hardwired, corded, or wireless. The system controller <b>1906</b> may receive user inputs or commands, from the user interface <b>1918</b> and the user input device <b>1922</b>.</p><p id="p-0170" num="0169">The fluidic control system <b>1908</b> includes a fluid network and is configured to direct and regulate the flow of one or more fluids through the fluid network. The fluid network may be in fluid communication with the biosensor <b>1912</b> and the fluid storage system <b>1914</b>. For example, select fluids may be drawn from the fluid storage system <b>1914</b> and directed to the biosensor <b>1912</b> in a controlled manner, or the fluids may be drawn from the biosensor <b>1912</b> and directed toward, for example, a waste reservoir in the fluid storage system <b>1914</b>. Although not shown, the fluidic control system <b>1908</b> may include flow sensors that detect a flow rate or pressure of the fluids within the fluid network. The sensors may communicate with the system controller <b>1906</b>.</p><p id="p-0171" num="0170">The temperature control system <b>1904</b> is configured to regulate the temperature of fluids at different regions of the fluid network, the fluid storage system <b>1914</b>, and/or the biosensor <b>1912</b>. For example, the temperature control system <b>1904</b> may include a thermocycler that interfaces with the biosensor <b>1912</b> and controls the temperature of the fluid that flows along the reaction sites in the biosensor <b>1912</b>. The temperature control system <b>1904</b> may also regulate the temperature of solid elements or components of the sequencing system <b>1900</b>A or the biosensor <b>1912</b>. Although not shown, the temperature control system <b>1904</b> may include sensors to detect the temperature of the fluid or other components. The sensors may communicate with the system controller <b>1906</b>.</p><p id="p-0172" num="0171">The fluid storage system <b>1914</b> is in fluid communication with the biosensor <b>1912</b> and may store various reaction components or reactants that are used to conduct the desired reactions therein. The fluid storage system <b>1914</b> may also store fluids for washing or cleaning the fluid network and biosensor <b>1912</b> and for diluting the reactants. For example, the fluid storage system <b>1914</b> may include various reservoirs to store samples, reagents, enzymes, other biomolecules, buffer solutions, aqueous and non-polar solutions, and the like. Furthermore, the fluid storage system <b>1914</b> may also include waste reservoirs for receiving waste products from the biosensor <b>1912</b>. In implementations that include a cartridge, the cartridge may include one or more of a fluid storage system, fluidic control system or temperature control system. Accordingly, one or more of the components set forth herein as relating to those systems can be contained within a cartridge housing. For example, a cartridge can have various reservoirs to store samples, reagents, enzymes, other biomolecules, buffer solutions, aqueous and non-polar solutions, waste, and the like. As such, one or more of a fluid storage system, fluidic control system or temperature control system can be engaged in a removable way with a bioassay system via a cartridge or other biosensor.</p><p id="p-0173" num="0172">The illumination system <b>1916</b> may include a light source (e.g., one or more LEDs) and a plurality of optical components to illuminate the biosensor. Examples of light sources may include lasers, arc lamps, LEDs, or laser diodes. The optical components may be, for example, reflectors, dichroics, beam splitters, collimators, lenses, filters, wedges, prisms, mirrors, detectors, and the like. In implementations that use an illumination system, the illumination system <b>1916</b> may be configured to direct an excitation light to reaction sites. As one example, fluorophores may be excited by green wavelengths of light; as such the wavelength of the excitation light may be approximately 1932 nm. In one implementation, the illumination system <b>1916</b> is configured to produce illumination that is parallel to a surface normal to a surface of the biosensor <b>1912</b>. In another implementation, the illumination system <b>1916</b> is configured to produce illumination that is off-angle relative to the surface normal to the surface of the biosensor <b>1912</b>. In yet another implementation, the illumination system <b>1916</b> is configured to produce illumination that has plural angles, including some parallel illumination and some off-angle illumination.</p><p id="p-0174" num="0173">The system receptacle or interface <b>1910</b> is configured to engage the biosensor <b>1912</b> in at least one of a mechanical, electrical, or fluidic manner. The system receptacle <b>1910</b> may hold the biosensor <b>1912</b> in a desired orientation to facilitate the flow of fluid through the biosensor <b>1912</b>. The system receptacle <b>1910</b> may also include electrical contacts that are configured to engage the biosensor <b>1912</b> so that the sequencing system <b>1900</b>A may communicate with the biosensor <b>1912</b> and/or provide power to the biosensor <b>1912</b>. Furthermore, the system receptacle <b>1910</b> may include fluidic ports (e.g., nozzles) that are configured to engage the biosensor <b>1912</b>. In some implementations, the biosensor <b>1912</b> is coupled in a removable way to the system receptacle <b>1910</b> in a mechanical manner, in an electrical manner, and also in a fluidic manner.</p><p id="p-0175" num="0174">In addition, the sequencing system <b>1900</b>A may communicate remotely with other systems or networks or with other bioassay systems <b>1900</b>A. Detection data obtained by the bioassay system(s) <b>1900</b>A may be stored in a remote database.</p><p id="p-0176" num="0175"><figref idref="DRAWINGS">FIG. <b>19</b>B</figref> is a block diagram of a system controller <b>1906</b> that can be used in the system of <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>. In one implementation, the system controller <b>1906</b> includes one or more processors or modules that can communicate with one another. Each of the processors or modules may include an algorithm (e.g., instructions stored on a tangible and/or non-transitory computer readable storage medium) or sub-algorithms to perform particular processes. The system controller <b>1906</b> is illustrated conceptually as a collection of modules, but may be implemented utilizing any combination of dedicated hardware boards, DSPs, processors, etc. Alternatively, the system controller <b>1906</b> may be implemented utilizing an off-the-shelf PC with a single processor or multiple processors, with the functional operations distributed between the processors. As a further option, the modules described below may be implemented utilizing a hybrid configuration in which certain modular functions are performed utilizing dedicated hardware, while the remaining modular functions are performed utilizing an off-the-shelf PC and the like. The modules also may be implemented as software modules within a processing unit.</p><p id="p-0177" num="0176">During operation, a communication port <b>1950</b> may transmit information (e.g., commands) to or receive information (e.g., data) from the biosensor <b>1912</b> (<figref idref="DRAWINGS">FIG. <b>19</b>A</figref>) and/or the sub-systems <b>1908</b>, <b>1914</b>, <b>1904</b> (<figref idref="DRAWINGS">FIG. <b>19</b>A</figref>). In implementations, the communication port <b>1950</b> may output a plurality of sequences of pixel signals. A communication link <b>1934</b> may receive user input from the user interface <b>1918</b> (<figref idref="DRAWINGS">FIG. <b>19</b>A</figref>) and transmit data or information to the user interface <b>1918</b>. Data from the biosensor <b>1912</b> or sub-systems <b>1908</b>, <b>1914</b>, <b>1904</b> may be processed by the system controller <b>1906</b> in real-time during a bioassay session. Additionally or alternatively, data may be stored temporarily in a system memory during a bioassay session and processed in slower than real-time or off-line operation.</p><p id="p-0178" num="0177">As shown in <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>, the system controller <b>1906</b> may include a plurality of modules <b>1926</b>-<b>1948</b> that communicate with a main control module <b>1924</b>, along with a central processing unit (CPU) <b>1952</b>. The main control module <b>1924</b> may communicate with the user interface <b>1918</b> (<figref idref="DRAWINGS">FIG. <b>19</b>A</figref>). Although the modules <b>1926</b>-<b>1948</b> are shown as communicating directly with the main control module <b>1924</b>, the modules <b>1926</b>-<b>1948</b> may also communicate directly with each other, the user interface <b>1918</b>, and the biosensor <b>1912</b>. Also, the modules <b>1926</b>-<b>1948</b> may communicate with the main control module <b>1924</b> through the other modules.</p><p id="p-0179" num="0178">The plurality of modules <b>1926</b>-<b>1948</b> includes system modules <b>1928</b>-<b>1932</b>, <b>1926</b> that communicate with the sub-systems <b>1908</b>, <b>1914</b>, <b>1904</b>, and <b>1916</b>. The fluidic control module <b>1928</b> may communicate with the fluidic control system <b>1908</b> to control the valves and flow sensors of the fluid network for controlling the flow of one or more fluids through the fluid network. The fluid storage module <b>1930</b> may notify the user when fluids are low or when the waste reservoir is at or near capacity. The fluid storage module <b>1930</b> may also communicate with the temperature control module <b>1932</b> so that the fluids may be stored at a desired temperature. The illumination module <b>1926</b> may communicate with the illumination system <b>1916</b> to illuminate the reaction sites at designated times during a protocol, such as after the desired reactions (e.g., binding events) have occurred. In some implementations, the illumination module <b>1926</b> may communicate with the illumination system <b>1916</b> to illuminate the reaction sites at designated angles.</p><p id="p-0180" num="0179">The plurality of modules <b>1926</b>-<b>1948</b> may also include a device module <b>1936</b> that communicates with the biosensor <b>1912</b> and an identification module <b>1938</b> that determines identification information relating to the biosensor <b>1912</b>. The device module <b>1936</b> may, for example, communicate with the system receptacle <b>1910</b> to confirm that the biosensor has established an electrical and fluidic connection with the sequencing system <b>1900</b>A. The identification module <b>1938</b> may receive signals that identify the biosensor <b>1912</b>. The identification module <b>1938</b> may use the identity of the biosensor <b>1912</b> to provide other information to the user. For example, the identification module <b>1938</b> may determine and then display a lot number, a date of manufacture, or a protocol that is recommended to be run with the biosensor <b>1912</b>.</p><p id="p-0181" num="0180">The plurality of modules <b>1926</b>-<b>1948</b> also includes an analysis module <b>1944</b> (also called a signal processing module or signal processor) that receives and analyzes the signal data (e.g., image data) from the biosensor <b>1912</b>. The analysis module <b>1944</b> includes memory (e.g., RAM or Flash) to store detection/image data. Detection data can include a plurality of sequences of pixel signals, such that a sequence of pixel signals from each of the millions of sensors (or pixels) can be detected over many base calling cycles. The signal data may be stored for subsequent analysis or may be transmitted to the user interface <b>1918</b> to display desired information to the user. In some implementations, the signal data may be processed by the solid-state imager (e.g., CMOS image sensor) before the analysis module <b>1944</b> receives the signal data.</p><p id="p-0182" num="0181">The analysis module <b>1944</b> is configured to obtain image data from the light detectors at each of a plurality of sequencing cycles. The image data is derived from the emission signals detected by the light detectors. The analysis module <b>1944</b> processes the image data for each of the plurality of sequencing cycles through the neural network-based base caller <b>300</b> and produces a base call for at least some of the analytes at each of the plurality of sequencing cycles. The light detectors can be part of one or more over-head cameras (e.g., Illumina's GAIIx's CCD camera taking images of the clusters on the biosensor <b>1912</b> from the top), or can be part of the biosensor <b>1912</b> itself (e.g., Illumina's iSeq's CMOS image sensors underlying the clusters on the biosensor <b>1912</b> and taking images of the clusters from the bottom).</p><p id="p-0183" num="0182">The output of the light detectors is the sequencing images, each depicting intensity emissions of the clusters and their surrounding background. The sequencing images depict intensity emissions generated as a result of nucleotide incorporation in the sequences during the sequencing. The intensity emissions are from associated analytes and their surrounding background. The sequencing images are stored in memory <b>1948</b>.</p><p id="p-0184" num="0183">Protocol modules <b>1940</b> and <b>1942</b> communicate with the main control module <b>1924</b> to control the operation of the sub-systems <b>1908</b>, <b>1914</b>, and <b>1904</b> when conducting predetermined assay protocols. The protocol modules <b>1940</b> and <b>1942</b> may include sets of instructions for instructing the sequencing system <b>1900</b>A to perform specific operations pursuant to predetermined protocols. As shown, the protocol module may be a sequencing-by-synthesis (SBS) module <b>1940</b> that is configured to issue various commands for performing sequencing-by-synthesis processes. In SBS, the extension of a nucleic acid primer along a nucleic acid template is monitored to determine the sequence of nucleotides in the template. The underlying chemical process can be polymerization (e.g., as catalyzed by a polymerase enzyme) or ligation (e.g., catalyzed by a ligase enzyme). In a particular polymerase-based SBS implementation, fluorescently labeled nucleotides are added to a primer (thereby extending the primer) in a template-dependent fashion such that the detection of the order and type of nucleotides added to the primer can be used to determine the sequence of the template. For example, to initiate a first SBS cycle, commands can be given to deliver one or more labeled nucleotides, DNA polymerase, etc., into/through a flow cell that houses an array of nucleic acid templates. The nucleic acid templates may be located at corresponding reaction sites. Those reaction sites where primer extension causes a labeled nucleotide to be incorporated can be detected through an imaging event. During an imaging event, the illumination system <b>1916</b> may provide an excitation light to the reaction sites. Optionally, the nucleotides can further include a reversible termination property that terminates further primer extension once a nucleotide has been added to a primer. For example, a nucleotide analog having a reversible terminator moiety can be added to a primer such that subsequent extension cannot occur until a deblocking agent is delivered to remove the moiety. Thus, for implementations that use reversible termination, a command can be given to deliver a deblocking reagent to the flow cell (before or after detection occurs). One or more commands can be given to effect wash(es) between the various delivery steps. The cycle can then be repeated n times to extend the primer by n nucleotides, thereby detecting a sequence of length n. Exemplary sequencing techniques are described, for example, in Bentley et al., Nature 4196:193-199 (20019); WO 04/0119497; U.S. Pat. No. 7,0197,026; WO 91/066719; WO 07/123744; U.S. Pat. Nos. 7,329,492; 7,211,414; 7,3119,019; 7,4019,2191, and US 20019/01470190192, each of which is incorporated herein by reference.</p><p id="p-0185" num="0184">For the nucleotide delivery step of an SBS cycle, either a single type of nucleotide can be delivered at a time, or multiple different nucleotide types (e.g., A, C, T and G together) can be delivered. For a nucleotide delivery configuration where only a single type of nucleotide is present at a time, the different nucleotides need not have distinct labels since they can be distinguished based on temporal separation inherent in the individualized delivery. Accordingly, a sequencing method or apparatus can use single color detection. For example, an excitation source need only provide excitation at a single wavelength or in a single range of wavelengths. For a nucleotide delivery configuration where delivery results in multiple different nucleotides being present in the flow cell at one time, sites that incorporate different nucleotide types can be distinguished based on different fluorescent labels that are attached to respective nucleotide types in the mixture. For example, four different nucleotides can be used, each having one of four different fluorophores. In one implementation, the four different fluorophores can be distinguished using excitation in four different regions of the spectrum. For example, four different excitation radiation sources can be used. Alternatively, fewer than four different excitation sources can be used, but optical filtration of the excitation radiation from a single source can be used to produce different ranges of excitation radiation at the flow cell.</p><p id="p-0186" num="0185">In some implementations, fewer than four different colors can be detected in a mixture having four different nucleotides. For example, pairs of nucleotides can be detected at the same wavelength but distinguished based on a difference in intensity for one member of the pair compared to the other, or based on a change to one member of the pair (e.g., via chemical modification, photochemical modification or physical modification) that causes an apparent signal to appear or disappear compared to the signal detected for the other member of the pair. Exemplary apparatus and methods for distinguishing four different nucleotides using detection of fewer than four colors are described for example in US Pat. App. Ser. Nos. 61/19319,294 and 61/619,19719, which are incorporated herein by reference in their entireties. U.S. application Ser. No. 13/624,200, which was filed on Sep. 21, 2012, is also incorporated by reference in its entirety.</p><p id="p-0187" num="0186">The plurality of protocol modules may also include a sample-preparation (or generation) module <b>1942</b> that is configured to issue commands to the fluidic control system <b>1908</b> and the temperature control system <b>1904</b> for amplifying a product within the biosensor <b>1912</b>. For example, the biosensor <b>1912</b> may be engaged to the sequencing system <b>1900</b>A. The amplification module <b>1942</b> may issue instructions to the fluidic control system <b>1908</b> to deliver necessary the amplification components to reaction chambers within the biosensor <b>1912</b>. In other implementations, the reaction sites may already contain some components for amplification, such as the template DNA and/or primers. After delivering the amplification components to the reaction chambers, the amplification module <b>1942</b> may instruct the temperature control system <b>1904</b> to cycle through different temperature stages according to known amplification protocols. In some implementations, the amplification and/or nucleotide incorporation is performed isothermally.</p><p id="p-0188" num="0187">The SBS module <b>1940</b> may issue commands to perform bridge PCR where clusters of clonal amplicons are formed on localized areas within a channel of a flow cell. After generating the amplicons through bridge PCR, the amplicons may be &#x201c;linearized&#x201d; to make single stranded template DNA, or sstDNA, and a sequencing primer may be hybridized to a universal sequence that flanks a region of interest. For example, a reversible terminator-based sequencing by synthesis method can be used as set forth above or as follows.</p><p id="p-0189" num="0188">Each base calling or sequencing cycle can extend an sstDNA by a single base which can be accomplished for example by using a modified DNA polymerase and a mixture of four types of nucleotides. The different types of nucleotides can have unique fluorescent labels, and each nucleotide can further have a reversible terminator that allows only a single-base incorporation to occur in each cycle. After a single base is added to the sstDNA, an excitation light may be incident upon the reaction sites and fluorescent emissions may be detected. After detection, the fluorescent label and the terminator may be chemically cleaved from the sstDNA. Another similar base calling or sequencing cycle may follow. In such a sequencing protocol, the SBS module <b>1940</b> may instruct the fluidic control system <b>1908</b> to direct a flow of reagent and enzyme solutions through the biosensor <b>1912</b>. Exemplary reversible terminator-based SBS methods which can be utilized with the apparatus and methods set forth herein are described in US Patent Application Publication No. 2007/0166705 A1, US Patent Application Publication No. 2006/0188901 A1, U.S. Pat. No. 7,057,026, US Patent Application Publication No. 2006/0240439 A1, US Patent Application Publication No. 2006/0281109 A1, PCT Publication No. WO 05/065814, US Patent Application Publication No. 2005/0100900 A1, PCT Publication No. WO 06/064199 and PCT Publication No. WO 07/010251, each of which is incorporated herein by reference in its entirety. Exemplary reagents for reversible terminator-based SBS are described in U.S. Pat. Nos. 7,541,444; 7,057,026; 7,414,116; 7,427,673; 7,566,537; 7,592,435 and WO 07/135368, each of which is incorporated herein by reference in its entirety.</p><p id="p-0190" num="0189">In some implementations, the amplification and SBS modules may operate in a single assay protocol where, for example, a template nucleic acid is amplified and subsequently sequenced within the same cartridge.</p><p id="p-0191" num="0190">The sequencing system <b>1900</b>A may also allow the user to reconfigure an assay protocol. For example, the sequencing system <b>1900</b>A may offer options to the user through the user interface <b>1918</b> for modifying the determined protocol. For example, if it is determined that the biosensor <b>1912</b> is to be used for amplification, the sequencing system <b>1900</b>A may request a temperature for the annealing cycle. Furthermore, the sequencing system <b>1900</b>A may issue warnings to a user if a user has provided user inputs that are generally not acceptable for the selected assay protocol.</p><p id="p-0192" num="0191">In implementations, the biosensor <b>1912</b> includes millions of sensors (or pixels), each of which generates a plurality of sequences of pixel signals over successive base calling cycles. The analysis module <b>1944</b> detects the plurality of sequences of pixel signals and attributes them to corresponding sensors (or pixels) in accordance with the row-wise and/or column-wise location of the sensors on an array of sensors.</p><p id="p-0193" num="0192"><figref idref="DRAWINGS">FIG. <b>19</b>C</figref> is a simplified block diagram of a system for the analysis of sensor data from the sequencing system <b>1900</b>A, such as base call sensor outputs. In the example of <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>, the system includes the configurable processor <b>1946</b>. The configurable processor <b>1946</b> can execute a base caller (e.g., the neural network-based base caller <b>300</b>) in coordination with a runtime program/logic <b>1980</b> executed by the central processing unit (CPU) <b>1952</b> (i.e., a host processor). The sequencing system <b>1900</b>A comprises the biosensor <b>1912</b> and flow cells. The flow cells can comprise one or more tiles in which clusters of genetic material are exposed to a sequence of analyte flows used to cause reactions in the clusters to identify the bases in the genetic material. The sensors sense the reactions for each cycle of the sequence in each tile of the flow cell to provide tile data. Genetic sequencing is a data intensive operation, which translates base call sensor data into sequences of base calls for each cluster of genetic material sensed during a base call operation.</p><p id="p-0194" num="0193">The system in this example includes the CPU <b>1952</b>, which executes a runtime program/logic <b>1980</b> to coordinate the base call operations, a memory <b>1948</b>B to store sequences of arrays of tile data, base call reads produced by the base calling operation, and other information used in the base call operations. Also, in this illustration the system includes a memory <b>1948</b>A to store a configuration file (or files), such as FPGA bit files, and model parameters for the neural networks used to configure and reconfigure the configurable processor <b>1946</b>, and to execute the neural networks. The sequencing system <b>1900</b>A can include a program for configuring a configurable processor and in some implementations a reconfigurable processor to execute the neural networks.</p><p id="p-0195" num="0194">The sequencing system <b>1900</b>A is coupled by a bus <b>1989</b> to the configurable processor <b>1946</b>. The bus <b>1989</b> can be implemented using a high throughput technology, such as, in one example, bus technology compatible with the PCIe standards (Peripheral Component Interconnect Express) currently maintained and developed by the PCI-SIG (PCI Special Interest Group). Also, in this example, a memory <b>1948</b>A is coupled to the configurable processor <b>1946</b> by the bus <b>1993</b>. The memory <b>1948</b>A can be on-board memory, disposed on a circuit board with the configurable processor <b>1946</b>. The memory <b>1948</b>A is used for high-speed access by the configurable processor <b>1946</b> to working data used in the base call operation. The bus <b>1993</b> can also be implemented using a high throughput technology, such as a bus technology compatible with the PCIe standards.</p><p id="p-0196" num="0195">Configurable processors, including field programmable gate arrays (FPGAs), coarse grained reconfigurable arrays (CGRAs), and other configurable and reconfigurable devices, can be configured to implement a variety of functions more efficiently or faster than might be achieved using a general-purpose processor executing a computer program. The configuration of configurable processors involves compiling a functional description to produce a configuration file, referred to sometimes as a bitstream or bit file, and distributing the configuration file to the configurable elements on the processor. The configuration file defines the logic functions to be executed by the configurable processor, by configuring the circuit to set data flow patterns, use of distributed memory and other on-chip memory resources, lookup table contents, operations of configurable logic blocks and configurable execution units like multiply-and-accumulate units, configurable interconnects and other elements of the configurable array. A configurable processor is reconfigurable if the configuration file may be changed in the field, by changing the loaded configuration file. For example, the configuration file may be stored in volatile SRAM elements, in non-volatile read-write memory elements, and in combinations of the same, distributed among the array of configurable elements on the configurable or reconfigurable processor. A variety of commercially available configurable processors are suitable for use in a base calling operation as described herein. Examples include Google's Tensor Processing Unit (TPU)&#x2122;, rackmount solutions like GX4 Rackmount Series&#x2122;, GX9 Rackmount Series&#x2122;, NVIDIA DGX-1&#x2122;, Microsoft' Stratix V FPGA&#x2122;, Graphcore's Intelligent Processor Unit (IPU)&#x2122;, Qualcomm's Zeroth Platform&#x2122; with Snapdragon Processors&#x2122; NVIDIA's Volta&#x2122;, NVIDIA's DRIVE PX&#x2122;, NVIDIA's JETSON TX1/TX2 MODULE&#x2122; Intel's Nirvana&#x2122;, Movidius VPU&#x2122;, Fujitsu DPI&#x2122;, ARM's DynamiclQ&#x2122;, IBM TrueNorth&#x2122;, Lambda GPU Server with Testa V100s&#x2122;, Xilinx Alveo&#x2122; U200, Xilinx Alveo&#x2122; U2190, Xilinx Alveo&#x2122; U280, Intel/Altera Stratix&#x2122; GX2800, Intel/Altera Stratix&#x2122; GX2800, and Intel Stratix&#x2122; GX10M. In some examples, a host CPU can be implemented on the same integrated circuit as the configurable processor.</p><p id="p-0197" num="0196">Implementations described herein implement the neural network-based base caller <b>300</b> using the configurable processor <b>1946</b>. The configuration file for the configurable processor <b>1946</b> can be implemented by specifying the logic functions to be executed using a high-level description language HDL or a register transfer level RTL language specification. The specification can be compiled using the resources designed for the selected configurable processor to generate the configuration file. The same or similar specification can be compiled for the purposes of generating a design for an application-specific integrated circuit which may not be a configurable processor.</p><p id="p-0198" num="0197">Alternatives for the configurable processor <b>1946</b>, in all implementations described herein, therefore include a configured processor comprising an application specific ASIC or special purpose integrated circuit or set of integrated circuits, or a system-on-a-chip SOC device, or a graphics processing unit (GPU) processor or a coarse-grained reconfigurable architecture (CGRA) processor, configured to execute a neural network based base call operation as described herein.</p><p id="p-0199" num="0198">In general, configurable processors and configured processors described herein, as configured to execute runs of a neural network, are referred to herein as neural network processors.</p><p id="p-0200" num="0199">The configurable processor <b>1946</b> is configured in this example by a configuration file loaded using a program executed by the CPU <b>1952</b>, or by other sources, which configures the array of configurable elements <b>1991</b> (e.g., configuration logic blocks (CLB) such as look up tables (LUTs), flip-flops, compute processing units (PMUs), and compute memory units (CMUs), configurable I/O blocks, programmable interconnects) on the configurable processor to execute the base call function. In this example, the configuration includes a data flow logic <b>1997</b> which is coupled to the buses <b>1989</b> and <b>1993</b> and executes functions for distributing data and control parameters among the elements used in the base call operation.</p><p id="p-0201" num="0200">Also, the configurable processor <b>1946</b> is configured with a data flow logic <b>1997</b> to execute the neural network-based base caller <b>300</b>. The logic <b>1997</b> comprises multi-cycle execution clusters (e.g., <b>1979</b>) which, in this example, include execution cluster <b>1</b> through execution cluster X. The number of multi-cycle execution clusters can be selected according to a trade-off involving the desired throughput of the operation, and the available resources on the configurable processor <b>1946</b>.</p><p id="p-0202" num="0201">The multi-cycle execution clusters are coupled to the data flow logic <b>1997</b> by data flow paths <b>1999</b> implemented using configurable interconnect and memory resources on the configurable processor <b>1946</b>. Also, the multi-cycle execution clusters are coupled to the data flow logic <b>1997</b> by control paths <b>1995</b> implemented using configurable interconnect and memory resources for example on the configurable processor <b>1946</b>, which provide control signals indicating available execution clusters, readiness to provide input units for the execution of a run of the neural network-based base caller <b>300</b> to the available execution clusters, readiness to provide trained parameters for the neural network-based base caller <b>300</b>, readiness to provide output patches of base call classification data, and other control data used for the execution of the neural network-based base caller <b>300</b>.</p><p id="p-0203" num="0202">The configurable processor <b>1946</b> is configured to execute runs of the neural network-based base caller <b>300</b> using trained parameters to produce classification data for the sensing cycles of the base calling operation. A run of the neural network-based base caller <b>300</b> is executed to produce classification data for a subject sensing cycle of the base calling operation. A run of the neural network-based base caller <b>300</b> operates on a sequence including a number N of arrays of tile data from respective sensing cycles of N sensing cycles, where the N sensing cycles provide sensor data for different base call operations for one base position per operation in a time sequence in the examples described herein. Optionally, some of the N sensing cycles can be out of sequence if needed according to a particular neural network model being executed. The number N can be any number greater than one. In some examples described herein, sensing cycles of the N sensing cycles represent a set of sensing cycles for at least one sensing cycle preceding the subject sensing cycle and at least one sensing cycle following the subject cycle in a time sequence. Examples are described herein in which the number N is an integer equal to or greater than five.</p><p id="p-0204" num="0203">The data flow logic <b>1997</b> is configured to move tile data and at least some trained parameters of the model parameters from the memory <b>1948</b>A to the configurable processor <b>1946</b> for runs of the neural network-based base caller <b>300</b>, using input units for a given run including tile data for spatially aligned patches of the N arrays. The input units can be moved by direct memory access operations in one DMA operation, or in smaller units moved during available time slots in coordination with the execution of the neural network deployed.</p><p id="p-0205" num="0204">Tile data for a sensing cycle as described herein can comprise an array of sensor data having one or more features. For example, the sensor data can comprise two images which are analyzed to identify one of four bases at a base position in a genetic sequence of DNA, RNA, or other genetic material. The tile data can also include metadata about the images and the sensors. For example, in implementations of the base calling operation, the tile data can comprise information about the alignment of the images with the clusters such as distance from center information indicating the distance of each pixel in the array of sensor data from the center of a cluster of genetic material on the tile.</p><p id="p-0206" num="0205">During the execution of the neural network-based base caller <b>300</b> as described below, tile data can also include data produced during the execution of the neural network-based base caller <b>300</b>, referred to as intermediate data, which can be reused rather than recomputed during a run of the neural network-based base caller <b>300</b>. For example, during the execution of the neural network-based base caller <b>300</b>, the data flow logic <b>1997</b> can write intermediate data to the memory <b>1948</b>A in place of the sensor data for a given patch of an array of tile data. Implementations like this are described in more detail below.</p><p id="p-0207" num="0206">As illustrated, a system is described for the analysis of base call sensor output, comprising a memory (e.g., <b>1948</b>A) accessible by the runtime program/logic <b>1980</b> storing tile data including sensor data for a tile from sensing cycles of a base calling operation. Also, the system includes a neural network processor, such as the configurable processor <b>1946</b> having access to the memory. The neural network processor is configured to execute runs of a neural network using trained parameters to produce classification data for sensing cycles. As described herein, a run of the neural network is operating on a sequence of N arrays of tile data from respective sensing cycles of N sensing cycles, including a subject cycle, to produce the classification data for the subject cycle. The data flow logic <b>1997</b> is provided to move tile data and the trained parameters from the memory to the neural network processor for runs of the neural network using input units including data for spatially aligned patches of the N arrays from respective sensing cycles of N sensing cycles.</p><p id="p-0208" num="0207">Also, a system is described in which the neural network processor has access to the memory, and includes a plurality of execution clusters, the execution clusters in the plurality of execution clusters configured to execute a neural network. The data flow logic <b>1997</b> has access to the memory and to execution clusters in the plurality of execution clusters, to provide input units of tile data to available execution clusters in the plurality of execution clusters, the input units including a number N of spatially aligned patches of arrays of tile data from respective sensing cycles, including a subject sensing cycle, and to cause the execution clusters to apply the N spatially aligned patches to the neural network to produce output patches of classification data for the spatially aligned patch of the subject sensing cycle, where N is greater than 1.</p><p id="p-0209" num="0208"><figref idref="DRAWINGS">FIG. <b>20</b>A</figref> is a simplified diagram showing aspects of the base calling operation, including functions of a runtime program (e.g., the runtime logic <b>1980</b>) executed by a host processor. In this diagram, the output of image sensors from a flow cell is provided on the lines <b>2000</b> to the image processing threads <b>2001</b>, which can perform processes on images such as alignment and arrangement in an array of sensor data for the individual tiles and resampling of images. The output of image sensors can also be used by processes which calculate a tile cluster mask for each tile in the flow cell, which identifies pixels in the array of sensor data that correspond to clusters of genetic material on the corresponding tile of the flow cell. The outputs of the image processing threads <b>2001</b> are provided on the lines <b>2002</b> to a dispatch logic <b>2003</b> in the CPU which routes the arrays of tile data to a data cache <b>2005</b> (e.g., SSD storage) on a high-speed bus <b>2004</b>, or on high-speed bus <b>2006</b> to the neural network processor hardware <b>2007</b>, such as the configurable processor <b>1946</b> of <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>, according to the state of the base calling operation. The processed and transformed images can be stored on the data cache <b>2005</b> for sensing cycles that were previously used. The hardware <b>2007</b> returns classification data output by the neural network to the dispatch logic <b>2003</b>, which passes the information to the data cache <b>2005</b>, or on lines <b>2008</b> to threads <b>2009</b> that perform base call and quality score computations using the classification data, and can arrange the data in standard formats for base call reads. The outputs of the threads <b>2009</b> that perform base calling and quality score computations are provided on the lines <b>2010</b> to threads <b>2011</b> that aggregate the base call reads, perform other operations such as data compression, and write the resulting base call outputs to specified destinations for utilization by the customers.</p><p id="p-0210" num="0209">In some implementations, the host can include threads (not shown) that perform final processing of the output of the hardware <b>2007</b> in support of the neural network. For example, the hardware <b>2007</b> can provide outputs of classification data from a final layer of the multi-cluster neural network. The host processor can execute an output activation function, such as a softmax function, over the classification data to configure the data for use by the base call and quality score threads <b>2002</b>. Also, the host processor can execute input operations (not shown), such as batch normalization of the tile data prior to input to the hardware <b>2007</b>.</p><p id="p-0211" num="0210"><figref idref="DRAWINGS">FIG. <b>20</b>B</figref> is a simplified diagram of a configuration of a configurable processor <b>1946</b> such as that of <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>. In <figref idref="DRAWINGS">FIG. <b>20</b>B</figref>, the configurable processor <b>1946</b> comprises an FPGA with a plurality of high speed PCIe interfaces. The FPGA is configured with a wrapper <b>2090</b> which comprises the data flow logic <b>1997</b> described with reference to <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>. The wrapper <b>2090</b> manages the interface and coordination with a runtime program in the CPU across the CPU communication link <b>2077</b> and manages communication with the on-board DRAM <b>2099</b> (e.g., a memory <b>1448</b>A) via the DRAM communication link <b>2097</b>. The data flow logic <b>1997</b> in the wrapper <b>2090</b> provides patch data retrieved by traversing the arrays of tile data on the on-board DRAM <b>2099</b> for the number N cycles to a cluster <b>2085</b> and retrieves process data <b>2087</b> from the cluster <b>2085</b> for delivery back to the on-board DRAM <b>2099</b>. The wrapper <b>2090</b> also manages the transfer of data between the on-board DRAM <b>2099</b> and host memory, for both the input arrays of tile data, and for the output patches of classification data. The wrapper transfers patch data on the line <b>2083</b> to the allocated cluster <b>2085</b>. The wrapper provides trained parameters, such as weights and biases on the line <b>2081</b> to the cluster <b>2085</b> retrieved from the on-board DRAM <b>2099</b>. The wrapper provides configuration and control data on the line <b>2079</b> to the cluster <b>2085</b> provided from, or generated in response to, the runtime program on the host via the CPU communication link <b>2077</b>. The cluster can also provide status signals on the line <b>2089</b> to the wrapper <b>2090</b>, which are used in cooperation with control signals from the host to manage traversal of the arrays of tile data to provide spatially aligned patch data, and to execute the multi-cycle neural network over the patch data using the resources of the cluster <b>2085</b>.</p><p id="p-0212" num="0211">As mentioned above, there can be multiple clusters on a single configurable processor managed by the wrapper <b>2090</b> configured for executing on corresponding ones of multiple patches of the tile data. Each cluster can be configured to provide classification data for base calls in a subject sensing cycle using the tile data of multiple sensing cycles described herein.</p><p id="p-0213" num="0212">In examples of the system, model data, including kernel data like filter weights and biases, can be sent from the host CPU to the configurable processor, so that the model can be updated as a function of the cycle number. A base calling operation can comprise, for a representative example, on the order of hundreds of sensing cycles. A base calling operation can include paired end reads in some implementations. For example, the model-trained parameters may be updated once every 20 cycles (or other number of cycles), or according to update patterns implemented for particular systems and neural network models. In some implementations, including paired end reads, in which a sequence for a given string in a genetic cluster on a tile includes a first part extending from a first end down (or up) the string, and a second part extending from a second end up (or down) the string, the trained parameters can be updated on the transition from the first part to the second part.</p><p id="p-0214" num="0213">In some examples, image data for multiple cycles of sensing data for a tile can be sent from the CPU to the wrapper <b>2090</b>. The wrapper <b>2090</b> can optionally do some preprocessing and transformation of the sensing data and write the information to the on-board DRAM <b>2099</b>. The input tile data for each sensing cycle can include arrays of sensor data including on the order of 4000&#xd7;3000 pixels per sensing cycle per tile or more, with two features representing the colors of two images of the tile, and one or two bytes per feature per pixel. For an implementation in which the number N is three sensing cycles to be used in each run of the multi-cycle neural network, the array of tile data for each run of the multi-cycle neural network can consume on the order of hundreds of megabytes per tile. In some implementations of the system, the tile data also includes an array of distance-from-cluster center (DFC) data, stored once per tile, or another type of metadata about the sensor data and the tiles.</p><p id="p-0215" num="0214">In operation, when a multi-cycle cluster is available, the wrapper allocates a patch to the cluster. The wrapper fetches a next patch of tile data in the traversal of the tile and sends it to the allocated cluster along with appropriate control and configuration information. The cluster can be configured with enough memory on the configurable processor to hold a patch of data, including patches from multiple cycles in some systems, that is being worked on in place, and a patch of data that is to be worked on when the current patch of processing is finished using a ping-pong buffer technique or raster scanning technique in various implementations.</p><p id="p-0216" num="0215">When an allocated cluster completes its run of the neural network for the current patch and produces an output patch, it will signal the wrapper. The wrapper will read the output patch from the allocated cluster, or alternatively the allocated cluster will push the data out to the wrapper. Then the wrapper will assemble output patches for the processed tile in the DRAM <b>2099</b>. When the processing of the entire tile has been completed, and the output patches of data have been transferred to the DRAM, the wrapper sends the processed output array for the tile back to the host/CPU in a specified format. In some implementations, the on-board DRAM <b>2099</b> is managed by a memory management logic in the wrapper <b>2090</b>. The runtime program can control the sequencing operations to complete the analysis of all the arrays of tile data for all the cycles in the run in a continuous flow to provide real time analysis.</p><heading id="h-0020" level="2">Joint Training</heading><p id="p-0217" num="0216"><figref idref="DRAWINGS">FIG. <b>21</b>A</figref> illustrates one implementation of jointly training the neural network-based base caller <b>300</b> on reordered index images and sequencing images. Training data <b>2102</b> for training the neural network-based base caller <b>300</b> comprises index images <b>2112</b> and sequencing images <b>2114</b>. To compensate for the low-sequence diversity in the index images <b>2112</b>, a reordering/shuffling logic <b>2122</b> reorders the index images <b>2112</b> into reordered index images <b>2132</b>. The reordered index images <b>2132</b> include the index images <b>2112</b> rearranged/sequenced in N orders.</p><p id="p-0218" num="0217">Then, a join training <b>2126</b> is executed that trains the neural network-based base caller <b>300</b> on both the sequencing images <b>2114</b> and the reordered index images <b>2132</b>. The training is based on, for example, multiple backpropagation iterations, each of which can be in turn use as input a different ordered combination and/or permutation of the index images <b>2112</b>.</p><p id="p-0219" num="0218"><figref idref="DRAWINGS">FIG. <b>21</b>B</figref> illustrates one implementation of how the reordered index images <b>2132</b> of <figref idref="DRAWINGS">FIG. <b>21</b>A</figref> are generated. In one implementation, this includes the following steps&#x2014;(i) accessing a sequence of index images <b>2142</b> generated for index sequencing cycles of a sequencing run, and selecting a plurality of sub-sequences of index images <b>2152</b> from the sequence of index images, (ii) generating respective pluralities of reordered sub-sequences <b>2164</b>, <b>2166</b>, and <b>2168</b> of index images for respective sub-sequences of index images <b>2154</b>, <b>2156</b>, and <b>2158</b> in the plurality of sub-sequences of index images <b>2152</b>, (iii) accessing a sequence of target images generated for target sequencing cycles of the sequencing run, and selecting a plurality of sub-sequences of target images from the sequence of target images, and (iv) training the neural network-based base caller <b>300</b> using the respective pluralities of reordered sub-sequences of index images, and the plurality of sub-sequences of target images.</p><p id="p-0220" num="0219">The target sequencing cycles are early target sequencing cycles of the sequencing run. The respective pluralities of reordered sub-sequences of index images compensate for low-sequence diversity in the sequence of index images. The steps further include, based on the training, generating the trained neural network-based base caller <b>300</b>, and applying the trained neural network-based base caller <b>300</b> for base calling at inference.</p><p id="p-0221" num="0220"><figref idref="DRAWINGS">FIGS. <b>24</b> and <b>25</b></figref> show various implementations of the technology disclosed.</p><heading id="h-0021" level="2">Computer System</heading><p id="p-0222" num="0221"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a computer system <b>2200</b> that can be used by the sequencing system <b>1900</b>A to implement the base calling techniques disclosed herein. Computer system <b>2200</b> includes at least one central processing unit (CPU) <b>2272</b> that communicates with a number of peripheral devices via bus subsystem <b>2255</b>. These peripheral devices can include a storage subsystem <b>2210</b> including, for example, memory devices and a file storage subsystem <b>2236</b>, user interface input devices <b>2238</b>, user interface output devices <b>2276</b>, and a network interface subsystem <b>2274</b>. The input and output devices allow user interaction with computer system <b>2200</b>. Network interface subsystem <b>2274</b> provides an interface to outside networks, including an interface to corresponding interface devices in other computer systems.</p><p id="p-0223" num="0222">In one implementation, the system controller <b>1906</b> is communicably linked to the storage subsystem <b>2210</b> and the user interface input devices <b>2238</b>.</p><p id="p-0224" num="0223">User interface input devices <b>2238</b> can include a keyboard; pointing devices such as a mouse, trackball, touchpad, or graphics tablet; a scanner; a touch screen incorporated into the display; audio input devices such as voice recognition systems and microphones; and other types of input devices. In general, use of the term &#x201c;input device&#x201d; is intended to include all possible types of devices and ways to input information into computer system <b>2200</b>.</p><p id="p-0225" num="0224">User interface output devices <b>2276</b> can include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem can include an LED display, a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem can also provide a non-visual display such as audio output devices. In general, use of the term &#x201c;output device&#x201d; is intended to include all possible types of devices and ways to output information from computer system <b>2200</b> to the user or to another machine or computer system.</p><p id="p-0226" num="0225">Storage subsystem <b>2210</b> stores programming and data constructs that provide the functionality of some or all of the modules and methods described herein. These software modules are generally executed by deep learning processors <b>2278</b>.</p><p id="p-0227" num="0226">Deep learning processors <b>2278</b> can be Graphics Processing Units (GPUs), Field-Programmable Gate Arrays (FPGAs), Application-Specific Integrated Circuits (ASICs), and/or Coarse-Grained Reconfigurable Architectures (CGRAs). Deep learning processors <b>2278</b> can be hosted by a deep learning cloud platform such as Google Cloud Platform&#x2122;, Xilinx&#x2122;, and Cirrascale&#x2122;. Examples of deep learning processors <b>2278</b> include Google's Tensor Processing Unit (TPU)&#x2122;, rackmount solutions like GX4 Rackmount Series&#x2122;, GX22 Rackmount Series&#x2122; NVIDIA DGX-1&#x2122;, Microsoft' Stratix V FPGA&#x2122;, Graphcore's Intelligent Processor Unit (IPU)&#x2122;, Qualcomm's Zeroth Platform&#x2122; with Snapdragon Processors&#x2122;, NVIDIA's Volta&#x2122; NVIDIA's DRIVE PX&#x2122;, NVIDIA's JETSON TX1/TX2 MODULE&#x2122;, Intel's Nirvana&#x2122; Movidius VPU&#x2122;, Fujitsu DPI&#x2122;, ARM's DynamiclQ&#x2122;, IBM TrueNorth&#x2122;, Lambda GPU Server with Testa V100s&#x2122;, and others.</p><p id="p-0228" num="0227">Memory subsystem <b>2222</b> used in the storage subsystem <b>2210</b> can include a number of memories including a main random access memory (RAM) <b>2232</b> for storage of instructions and data during program execution and a read only memory (ROM) <b>2234</b> in which fixed instructions are stored. A file storage subsystem <b>2236</b> can provide persistent storage for program and data files, and can include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations can be stored by file storage subsystem <b>2236</b> in the storage subsystem <b>2210</b>, or in other machines accessible by the processor.</p><p id="p-0229" num="0228">Bus subsystem <b>2255</b> provides a mechanism for letting the various components and subsystems of computer system <b>2200</b> communicate with each other as intended. Although bus subsystem <b>2255</b> is shown schematically as a single bus, alternative implementations of the bus subsystem can use multiple busses.</p><p id="p-0230" num="0229">Computer system <b>2200</b> itself can be of varying types including a personal computer, a portable computer, a workstation, a computer terminal, a network computer, a television, a mainframe, a server farm, a widely-distributed set of loosely networked computers, or any other data processing system or user device. Due to the ever-changing nature of computers and networks, the description of computer system <b>2200</b> depicted in <figref idref="DRAWINGS">FIG. <b>22</b></figref> is intended only as a specific example for purposes of illustrating the preferred implementations of the present invention. Many other configurations of computer system <b>2200</b> are possible having more or less components than the computer system depicted in <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><heading id="h-0022" level="2">Clauses</heading><p id="p-0231" num="0230">The following clauses are part of this disclosure:</p><p id="p-0232" num="0000">1. A computer-implemented method, including:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0231">accessing a set of sequencing images,    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0232">wherein sequencing images in the set of sequencing images are arranged in a first order;</li>    </ul>    </li>    <li id="ul0001-0002" num="0233">reordering the sequencing images in a reordered set of the sequencing images,    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0234">wherein the sequencing images in the reordered set of the sequencing images are arranged in a second order, and</li>        <li id="ul0003-0002" num="0235">wherein the first order is different than the second order; and</li>    </ul>    </li>    <li id="ul0001-0003" num="0236">processing the reordered set of the sequencing images and generating at least one base call prediction.<br/>2. The computer-implemented method of clause 1, further including training a neural network-based base caller over a plurality of training iterations, wherein respective training iterations in the plurality of training iterations process respective reordered sets of the sequencing images through the neural network-based base caller to generate respective base call predictions, and update weights of the neural network-based base caller based on comparison of the respective base call predictions against respective base call ground truths.<br/>3. The computer-implemented method of clause 2, wherein the sequencing images are arranged at a center position, at one or more right flanking positions, and at one or more left flanking positions.<br/>4. The computer-implemented method of clause 3, wherein a particular one of the sequencing images is kept fixed at the center position between the first order and the second order.<br/>5. The computer-implemented method of clause 4, wherein at least some of the sequencing images at the right flanking positions and the left flanking positions are permutated between the first order and the second order.<br/>6. The computer-implemented method of clause 5, wherein at least some of the sequencing images at the right flanking positions and the left flanking positions are kept fixed between the first order and the second order.<br/>7. The computer-implemented method of clause 6, further including training the neural network-based base caller over a first training iteration that processes the set of sequencing images through the neural network-based base caller to generate a first base call prediction, and updates weights of the neural network-based base caller based on comparison of the first base call prediction against a first base call ground truth.<br/>8. The computer-implemented method of clause 7, further including training the neural network-based base caller over a second training iteration that processes the reordered set of sequencing images through the neural network-based base caller to generate a second base call prediction, and updates weights of the neural network-based base caller based on comparison of the second base call prediction against a second base call ground truth.<br/>9. The computer-implemented method of clause 1, wherein respective ones of the sequencing images are generated for respective sequencing cycles of a sequencing run in the first order.<br/>10. The computer-implemented method of clause 9, wherein respective ones of the sequencing images are respective index images generated for respective index sequencing cycles of the sequencing run in the first order.<br/>11. The computer-implemented method of clause 1, further including:</li>    <li id="ul0001-0004" num="0237">selecting a plurality of subsets of N sequencing images from a set of M sequencing images, wherein respective subsets in the plurality of subsets arrange the N sequencing images in respective sequences of the N sequencing images; and wherein M&#x3e;N.<br/>12. The computer-implemented method of clause 11, wherein the respective sequences have different combinations of the N sequencing images.<br/>13. The computer-implemented method of clause 12, wherein the combinations are combinations with replacement.<br/>14. The computer-implemented method of clause 12, wherein the combinations are combinations without replacement.<br/>15. The computer-implemented method of clause 11, wherein the respective sequences have different permutations of the N sequencing images.<br/>16. The computer-implemented method of clause 15, wherein the permutations are permutations with repetition.<br/>17. The computer-implemented method of clause 15, wherein the permutations are permutations without repetition.<br/>18. The computer-implemented method of clause 11, wherein the respective sequences arrange the N sequencing images in accordance with consecutive sequencing cycles.<br/>19. The computer-implemented method of clause 11, wherein the respective sequences arrange the N sequencing images in accordance with non-consecutive sequencing cycles.<br/>20. The computer-implemented method of clause 11, wherein the respective sequences arrange the N sequencing images in accordance with a combination of consecutive sequencing cycles and non-consecutive sequencing cycles.<br/>21. The computer-implemented method of clause 11, wherein the N varies between the respective sequences.<br/>22. The computer-implemented method of clause 11, wherein the N is constant between the respective sequences.<br/>23. The computer-implemented method of clause 11, wherein the respective sequences have overlapping sequencing images.<br/>24. The computer-implemented method of clause 11, wherein the respective sequences have no overlapping sequencing images.<br/>25. The computer-implemented method of clause 11, further including training the neural network-based base caller over respective training iterations that process the respective sequences of the N sequencing images through the neural network-based base caller to generate respective base call predictions, and update weights (e.g., via backpropagation) of the neural network-based base caller based on comparison of the respective base call predictions against respective base call ground truths.<br/>26. The computer-implemented method of clause 11, wherein the M sequencing images are generated for consecutive sequencing cycles.<br/>27. The computer-implemented method of clause 11, wherein the M sequencing images are generated for non-consecutive sequencing cycles.<br/>28. The computer-implemented method of clause 11, wherein the M sequencing images are generated for a combination of consecutive sequencing cycles and non-consecutive sequencing cycles.<br/>29. A computer-implemented method, including:</li>    <li id="ul0001-0005" num="0238">accessing a plurality of sequences of N sequencing images; and</li>    <li id="ul0001-0006" num="0239">training a neural network-based base caller over a plurality of training iterations, wherein respective training iterations in the plurality of training iterations process respective sequences of the N sequencing images in the plurality of sequences in respective orders through the neural network-based base caller to generate respective base call predictions, and update weights (e.g., via backpropagation) of the neural network-based base caller based on comparison of the respective base call predictions against respective base call ground truths.<br/>30. The computer-implemented method of clause 29, wherein the respective sequences arrange the N sequencing images at a center position, at one or more right flanking positions, and at one or more left flanking positions.<br/>31. The computer-implemented method of clause 30, wherein a particular one of the N sequencing images is kept fixed at the center position between the respective sequences.<br/>32. The computer-implemented method of clause 31, wherein at least some of the N sequencing images at the right flanking positions and the left flanking positions vary between the respective sequences.<br/>33. The computer-implemented method of clause 29, wherein the N sequencing images are selected from M sequencing images, wherein M&#x3e;N.<br/>34. The computer-implemented method of clause 33, wherein the respective sequences have different combinations of the N sequencing images.<br/>35. The computer-implemented method of clause 34, wherein the combinations are combinations with replacement.<br/>36. The computer-implemented method of clause 34, wherein the combinations are combinations without replacement.<br/>37. The computer-implemented method of clause 33, wherein the respective sequences have different permutations of the N sequencing images.<br/>38. The computer-implemented method of clause 37, wherein the permutations are permutations with repetition.<br/>39. The computer-implemented method of clause 37, wherein the permutations are permutations without repetition.<br/>40. The computer-implemented method of clause 29, wherein the respective sequences arrange the N sequencing images in accordance with consecutive sequencing cycles.<br/>41. The computer-implemented method of clause 29, wherein the respective sequences arrange the N sequencing images in accordance with non-consecutive sequencing cycles.<br/>42. The computer-implemented method of clause 29, wherein the respective sequences arrange the N sequencing images in accordance with a combination of consecutive sequencing cycles and non-consecutive sequencing cycles.<br/>43. The computer-implemented method of clause 29, wherein the N varies between the respective sequences.<br/>44. The computer-implemented method of clause 29, wherein the N is constant between the respective sequences.<br/>45. The computer-implemented method of clause 29, wherein the respective sequences have overlapping sequencing images.<br/>46. The computer-implemented method of clause 29, wherein the respective sequences have no overlapping sequencing images.<br/>47. The computer-implemented method of clause 33, wherein the M sequencing images are generated for consecutive sequencing cycles.<br/>48. The computer-implemented method of clause 33, wherein the M sequencing images are generated for non-consecutive sequencing cycles.<br/>49. The computer-implemented method of clause 33, wherein the M sequencing images are generated for a combination of consecutive sequencing cycles and non-consecutive sequencing cycles.<br/>50. The computer-implemented method of clause 29, wherein the M sequencing images are index sequencing images generated for index sequencing cycles of a sequencing run.<br/>51. A computer-implemented method of training a neural network-based base caller that comprises a spatial logic configured to extract intensity features confined to per-cycle sequencing images in an input, and a temporal logic configured to extract intensity features spanning pan-cycle sequencing images in the input, including:</li>    <li id="ul0001-0007" num="0240">training the spatial logic and the temporal logic of the neural network-based base caller on a first training dataset of sequencing images to generate a trained spatial logic and a trained temporal logic, wherein the training includes forward propagating sequencing images in the first training dataset through the spatial logic and the temporal logic and generating base call predictions, and backward propagating, through the temporal logic and the spatial logic, gradient updates determined based on comparison of the base call predictions against base call ground truths;</li>    <li id="ul0001-0008" num="0241">further training only the trained spatial logic of the neural network-based base caller on a second training dataset of sequencing images to generate a retrained spatial logic, wherein the further training includes forward propagating sequencing images in the second training dataset through the trained spatial logic and the trained temporal logic and generating further base call predictions, and backward propagating, only through the trained spatial logic, further gradient updates determined based on comparison of the further base call predictions against further base call ground truths, thereby bypassing backward propagating the further gradient updates through the trained temporal logic; and</li>    <li id="ul0001-0009" num="0242">applying the retrained spatial logic and the trained temporal logic of the neural network-based base caller on new sequencing images at inference to generate new base call predictions.<br/>52. The computer-implemented method of clause 51, wherein the sequencing images in the first training dataset of sequencing images are read sequencing images generated for read sequencing cycles of a sequencing run.<br/>53. The computer-implemented method of clause 51, wherein the sequencing images in the second training dataset of sequencing images are index sequencing images generated for index sequencing cycles of the sequencing run.<br/>54. The computer-implemented method of clause 53, wherein the sequencing images in the second training dataset of sequencing images include a combination of the read sequencing images and the index sequencing images.<br/>55. The computer-implemented method of clause 53, wherein the bypassing prevents the trained temporal logic from learning temporal dependencies between intensity features of the index sequencing images.<br/>56. A computer-implemented method of training a neural network-based base caller that comprises a spatial logic configured to extract intensity features confined to per-cycle sequencing images in an input, and a temporal logic configured to extract intensity features spanning pan-cycle sequencing images in the input, including:</li>    <li id="ul0001-0010" num="0243">training the spatial logic and the temporal logic of the neural network-based base caller on a first training dataset of sequencing images to generate a trained spatial logic and a trained temporal logic;</li>    <li id="ul0001-0011" num="0244">further training only the trained spatial logic of the neural network-based base caller on a second training dataset of sequencing images to generate a retrained spatial logic; and</li>    <li id="ul0001-0012" num="0245">configuring the neural network-based base caller with the retrained spatial logic and the trained temporal logic to execute future base calling.<br/>57. A computer-implemented method of training a neural network-based base caller that comprises a spatial logic configured to extract intensity features confined to per-cycle sequencing images in an input, and a temporal logic configured to extract intensity features spanning pan-cycle sequencing images in the input, including:</li>    <li id="ul0001-0013" num="0246">training the spatial logic and the temporal logic of the neural network-based base caller on a first training dataset of sequencing images to generate a trained spatial logic and a trained temporal logic;</li>    <li id="ul0001-0014" num="0247">further training only the trained spatial logic of the neural network-based base caller on a second training dataset of index sequencing images to generate a retrained spatial logic; and</li>    <li id="ul0001-0015" num="0248">configuring the neural network-based base caller with the retrained spatial logic and the trained temporal logic to execute future base calling of index sequencing cycles.<br/>58. A system, comprising:</li>    <li id="ul0001-0016" num="0249">an accessing logic configured to access a set of sequencing images,    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0250">wherein sequencing images in the set of sequencing images are arranged in a first order;</li>    </ul>    </li>    <li id="ul0001-0017" num="0251">a reordering logic configured to reorder the sequencing images in a reordered set of the sequencing images,    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0252">wherein the sequencing images in the reordered set of the sequencing images are arranged in a second order, and</li>        <li id="ul0005-0002" num="0253">wherein the first order is different than the second order; and</li>    </ul>    </li>    <li id="ul0001-0018" num="0254">a processing logic configured to process the reordered set of the sequencing images and generate at least one base call prediction.<br/>59. The system of clause 58, further comprising:</li>    <li id="ul0001-0019" num="0255">a normalization logic configured to normalize one or more of the sequencing images in the reordered set of the sequencing images.<br/>60. The system of clause 58, wherein one or more of the sequencing images in the ordered and/or reordered set of the sequencing images are normalized sequencing images.<br/>61. A computer-implemented method of training a neural network-based base caller to base call index sequences, the method including:</li>    <li id="ul0001-0020" num="0256">accessing a sequence of index images generated for index sequencing cycles of a sequencing run, and selecting a plurality of sub-sequences of index images from the sequence of index images;</li>    <li id="ul0001-0021" num="0257">generating respective pluralities of reordered sub-sequences of index images for respective sub-sequences of index images in the plurality of sub-sequences of index images, wherein reordered sub-sequences of index images in a given plurality of reordered sub-sequences of index images generated for a given sub-sequence of index images each have a different ordering;</li>    <li id="ul0001-0022" num="0258">accessing a sequence of target images generated for target sequencing cycles of the sequencing run, and selecting a plurality of sub-sequences of target images from the sequence of target images;</li>    <li id="ul0001-0023" num="0259">training a neural network-based base caller using the respective pluralities of reordered sub-sequences of index images, and the plurality of sub-sequences of target images.<br/>62. The computer-implemented method of clause 61, wherein the target sequencing cycles are early target sequencing cycles of the sequencing run.<br/>63. The computer-implemented method of clause 61, wherein the respective pluralities of reordered sub-sequences of index images compensate for low-sequence diversity in the sequence of index images.<br/>64. The computer-implemented method of clause 61, further including, based on the training, generating a trained neural network-based base caller, and applying the trained neural network-based base caller for base calling at inference.<br/>65. A non-transitory computer readable storage medium impressed with computer program instructions to train a neural network-based base caller to base call index sequences, the instructions, when executed on a processor, implement a method comprising:</li>    <li id="ul0001-0024" num="0260">accessing a sequence of index images generated for index sequencing cycles of a sequencing run, and selecting a plurality of sub-sequences of index images from the sequence of index images;</li>    <li id="ul0001-0025" num="0261">generating respective pluralities of reordered sub-sequences of index images for respective sub-sequences of index images in the plurality of sub-sequences of index images, wherein reordered sub-sequences of index images in a given plurality of reordered sub-sequences of index images generated for a given sub-sequence of index images each have a different ordering;</li>    <li id="ul0001-0026" num="0262">accessing a sequence of target images generated for target sequencing cycles of the sequencing run, and selecting a plurality of sub-sequences of target images from the sequence of target images;</li>    <li id="ul0001-0027" num="0263">training a neural network-based base caller using the respective pluralities of reordered sub-sequences of index images, and the plurality of sub-sequences of target images.<br/>66. The non-transitory computer readable storage medium of clause 64, wherein the target sequencing cycles are early target sequencing cycles of the sequencing run.<br/>67. The non-transitory computer readable storage medium of clause 64, wherein the respective pluralities of reordered sub-sequences of index images compensate for low-sequence diversity in the sequence of index images.<br/>68. The computer-implemented method of clause 64, implementing the method further comprising, based on the training, generating a trained neural network-based base caller, and applying the trained neural network-based base caller for base calling at inference.<br/>69. A system including one or more processors coupled to memory, the memory loaded with computer instructions to train a neural network-based base caller to base call index sequences, the instructions, when executed on the processors, implement actions comprising:</li>    <li id="ul0001-0028" num="0264">accessing a sequence of index images generated for index sequencing cycles of a sequencing run, and selecting a plurality of sub-sequences of index images from the sequence of index images;</li>    <li id="ul0001-0029" num="0265">generating respective pluralities of reordered sub-sequences of index images for respective sub-sequences of index images in the plurality of sub-sequences of index images, wherein reordered sub-sequences of index images in a given plurality of reordered sub-sequences of index images generated for a given sub-sequence of index images each have a different ordering;</li>    <li id="ul0001-0030" num="0266">accessing a sequence of target images generated for target sequencing cycles of the sequencing run, and selecting a plurality of sub-sequences of target images from the sequence of target images;</li>    <li id="ul0001-0031" num="0267">training a neural network-based base caller using the respective pluralities of reordered sub-sequences of index images, and the plurality of sub-sequences of target images.<br/>70. The system of clause 69, wherein the target sequencing cycles are early target sequencing cycles of the sequencing run.<br/>71. The system of clause 69, wherein the respective pluralities of reordered sub-sequences of index images compensate for low-sequence diversity in the sequence of index images.<br/>72. The system of clause 69, further implementing actions comprising, based on the training, generating a trained neural network-based base caller, and applying the trained neural network-based base caller for base calling at inference.<br/>73. A trained base caller, comprising:</li>    <li id="ul0001-0032" num="0268">neural network weights trained on training data comprising:    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0269">(i) respective pluralities of reordered sub-sequences of index images generated for respective sub-sequences of index images in a plurality of sub-sequences of index images, wherein reordered sub-sequences of index images in a given plurality of reordered sub-sequences of index images generated for a given sub-sequence of index images each have a different ordering, and wherein the plurality of sub-sequences of index images are selected from a sequence of index images generated for index sequencing cycles of a sequencing run; and</li>        <li id="ul0006-0002" num="0270">(ii) a plurality of sub-sequences of target images selected from a sequence of target images generated for target sequencing cycles of the sequencing run.<br/>74. The trained base caller of clause 73, wherein the target sequencing cycles are early target sequencing cycles of the sequencing run.</li>    </ul>    </li></ul></p><p id="p-0233" num="0271">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including a memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-0234" num="0272">While the present invention is disclosed by reference to the preferred implementations and examples detailed above, it is to be understood that these examples are intended in an illustrative rather than in a limiting sense. It is contemplated that modifications and combinations will readily occur to those skilled in the art, which modifications and combinations will be within the spirit of the invention and the scope of the following clauses.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method, including:<claim-text>accessing a set of sequencing images,<claim-text>wherein sequencing images in the set of sequencing images are arranged in a first order;</claim-text></claim-text><claim-text>reordering the sequencing images in a reordered set of the sequencing images,<claim-text>wherein the sequencing images in the reordered set of the sequencing images are arranged in a second order, and</claim-text><claim-text>wherein the first order is different than the second order; and</claim-text></claim-text><claim-text>processing the reordered set of the sequencing images and generating at least one base call prediction.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including training a neural network-based base caller over a plurality of training iterations, wherein respective training iterations in the plurality of training iterations process respective reordered sets of the sequencing images through the neural network-based base caller to generate respective base call predictions, and update weights of the neural network-based base caller based on comparison of the respective base call predictions against respective base call ground truths.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the sequencing images are arranged at a center position, at one or more right flanking positions, and at one or more left flanking positions.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein a particular one of the sequencing images is kept fixed at the center position between the first order and the second order.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein at least some of the sequencing images at the right flanking positions and the left flanking positions are permutated between the first order and the second order.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein at least some of the sequencing images at the right flanking positions and the left flanking positions are kept fixed between the first order and the second order.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further including training the neural network-based base caller over a first training iteration that processes the set of sequencing images through the neural network-based base caller to generate a first base call prediction, and updates weights of the neural network-based base caller based on comparison of the first base call prediction against a first base call ground truth.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further including training the neural network-based base caller over a second training iteration that processes the reordered set of sequencing images through the neural network-based base caller to generate a second base call prediction, and updates weights of the neural network-based base caller based on comparison of the second base call prediction against a second base call ground truth.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein respective ones of the sequencing images are generated for respective sequencing cycles of a sequencing run in the first order.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein respective ones of the sequencing images are respective index images generated for respective index sequencing cycles of the sequencing run in the first order.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further including:<claim-text>selecting a plurality of subsets of N sequencing images from a set of M sequencing images, wherein respective subsets in the plurality of subsets arrange the N sequencing images in respective sequences of the N sequencing images; and wherein M&#x3e;N.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences have different combinations of the N sequencing images.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the combinations are combinations with replacement.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the combinations are combinations without replacement.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences have different permutations of the N sequencing images.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-implemented method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the permutations are permutations with repetition.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-implemented method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the permutations are permutations without repetition.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences arrange the N sequencing images in accordance with consecutive sequencing cycles.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences arrange the N sequencing images in accordance with non-consecutive sequencing cycles.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences arrange the N sequencing images in accordance with a combination of consecutive sequencing cycles and non-consecutive sequencing cycles.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the N varies between the respective sequences.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the N is constant between the respective sequences.</claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences have overlapping sequencing images.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the respective sequences have no overlapping sequencing images.</claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, further including training the neural network-based base caller over respective training iterations that process the respective sequences of the N sequencing images through the neural network-based base caller to generate respective base call predictions, and update weights of the neural network-based base caller based on comparison of the respective base call predictions against respective base call ground truths.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the M sequencing images are generated for consecutive sequencing cycles.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the M sequencing images are generated for non-consecutive sequencing cycles.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The computer-implemented method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the M sequencing images are generated for a combination of consecutive sequencing cycles and non-consecutive sequencing cycles.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. A computer-implemented method, including:<claim-text>accessing a plurality of sequences of N sequencing images; and</claim-text><claim-text>training a neural network-based base caller over a plurality of training iterations, wherein respective training iterations in the plurality of training iterations process respective sequences of the N sequencing images in the plurality of sequences in respective orders through the neural network-based base caller to generate respective base call predictions, and update weights of the neural network-based base caller based on comparison of the respective base call predictions against respective base call ground truths.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The computer-implemented method of <claim-ref idref="CLM-00029">claim 29</claim-ref>, wherein the respective sequences arrange the N sequencing images at a center position, at one or more right flanking positions, and at one or more left flanking positions.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. A system, comprising:<claim-text>accessing logic configured to access a set of sequencing images,<claim-text>wherein sequencing images in the set of sequencing images are arranged in a first order;</claim-text></claim-text><claim-text>reordering logic configured to reorder the sequencing images in a reordered set of the sequencing images,<claim-text>wherein the sequencing images in the reordered set of the sequencing images are arranged in a second order, and</claim-text><claim-text>wherein the first order is different than the second order; and</claim-text></claim-text><claim-text>processing logic configured to process the reordered set of the sequencing images and generate at least one base call prediction.</claim-text></claim-text></claim></claims></us-patent-application>