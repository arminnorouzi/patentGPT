<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005277A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005277</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17896167</doc-number><date>20220826</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010124227.2</doc-number><date>20200227</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>586</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30264</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30204</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">POSE DETERMINING METHOD AND RELATED DEVICE</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/078328</doc-number><date>20210227</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17896167</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HUAWEI TECHNOLOGIES CO., LTD.</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GE</last-name><first-name>Jiange</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>XU</last-name><first-name>Zeying</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WU</last-name><first-name>Zongwu</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>LIU</last-name><first-name>Mu</first-name><address><city>Dongguan</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>WEI</last-name><first-name>Meng</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A pose determining method, which may be applied to the field of photographing and image processing, includes: obtaining a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and determining pose information based on the target parking space mark and the target parking space line. The pose information indicates a corresponding pose of a terminal during photographing of the target image. According to the pose determining method, the pose information may be determined based on the target parking space mark and the target parking space line, to implement positioning.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="206.93mm" wi="155.79mm" file="US20230005277A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="221.49mm" wi="157.82mm" file="US20230005277A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="160.70mm" wi="158.58mm" file="US20230005277A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="192.28mm" wi="118.28mm" file="US20230005277A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="211.33mm" wi="111.25mm" orientation="landscape" file="US20230005277A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="182.29mm" wi="158.83mm" file="US20230005277A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="61.30mm" wi="82.97mm" file="US20230005277A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="215.90mm" wi="128.35mm" orientation="landscape" file="US20230005277A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="180.17mm" wi="123.11mm" file="US20230005277A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="206.33mm" wi="156.13mm" orientation="landscape" file="US20230005277A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="185.84mm" wi="106.93mm" file="US20230005277A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="188.47mm" wi="110.24mm" file="US20230005277A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="187.45mm" wi="114.47mm" file="US20230005277A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="218.02mm" wi="127.42mm" file="US20230005277A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="108.29mm" wi="123.19mm" file="US20230005277A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="204.13mm" wi="156.38mm" orientation="landscape" file="US20230005277A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="205.49mm" wi="120.73mm" file="US20230005277A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="197.61mm" wi="119.30mm" file="US20230005277A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="103.55mm" wi="158.41mm" file="US20230005277A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="154.09mm" wi="136.99mm" file="US20230005277A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="142.41mm" wi="159.17mm" file="US20230005277A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/CN2021/078328, filed on Feb. 27, 2021, which claims priority to Chinese Patent Application No. 202010124227.2, filed on Feb. 27, 2020. The disclosures of the aforementioned applications are hereby incorporated by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates to the image processing field, and in particular, to a pose determining method and a related device.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Navigation belongs to a basic requirement of people's daily life. Map manufacturers have already launched a mature outdoor 2D navigation function that relies on a global positioning system (GPS). However, because there is usually no GPS signal indoors, it is still difficult to provide accurate navigation in an indoor scenario.</p><p id="p-0005" num="0004">With development of computer vision, the academia and the industry are trying to use a visual method for positioning. A real world environment is scanned in advance and a 3D point cloud map is built. For each 2D feature point in an image, distances between a descriptor of the 2D feature point and descriptors of all (or a part of) feature points in a point cloud are calculated to perform 2D-3D feature point matching, and a pose of a terminal device on the 3D point cloud map during image photographing is calculated based on a pose calculation algorithm.</p><p id="p-0006" num="0005">As there are more drivers, navigation in an underground garage also becomes users' important requirement. However, in a scenario of the underground garage in which visual features are repeated and textures are not rich, a success rate of a positioning manner based on a 3D point cloud map is very low.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">This disclosure provides a pose determining method, to determine pose information based on a target parking space mark and a target parking space line when positioning accuracy of a GPS signal is poor, thereby implementing positioning.</p><p id="p-0008" num="0007">According to a first aspect, this disclosure provides a pose determining method, where the method includes:</p><p id="p-0009" num="0008">obtaining a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and</p><p id="p-0010" num="0009">determining pose information based on the target parking space mark and the target parking space line, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0011" num="0010">Optionally, in a design of the first aspect, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0012" num="0011">obtaining a first pixel position of the target parking space line in the target image;</p><p id="p-0013" num="0012">obtaining third position information corresponding to the target parking space line in a digital map, where the third position information indicates a position of the target parking space line in the digital map; and</p><p id="p-0014" num="0013">determining the pose information based on the first pixel position and the third position information.</p><p id="p-0015" num="0014">Optionally, in a design of the first aspect, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0016" num="0015">sending the target image to a server; and</p><p id="p-0017" num="0016">receiving the pose information sent by the server, where the pose information is determined by the server based on a first pixel position of the target parking space line in the target image and third position information corresponding to the target parking space line in a digital map, and the third position information indicates a position of the target parking space line in the digital map.</p><p id="p-0018" num="0017">Optionally, in a design of the first aspect, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0019" num="0018">obtaining a first pixel position of the target parking space line in the target image;</p><p id="p-0020" num="0019">sending the target parking space mark and the first pixel position to a server; and</p><p id="p-0021" num="0020">receiving the pose information sent by the server, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the target parking space line in a digital map, and the third position information indicates a position of the target parking space line in the digital map.</p><p id="p-0022" num="0021">Optionally, in a design of the first aspect, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0023" num="0022">obtaining third position information corresponding to the target parking space line in a digital map, where the third position information indicates position information of the target parking space line in the digital map;</p><p id="p-0024" num="0023">sending the target image and the third position information to a server; and</p><p id="p-0025" num="0024">receiving the pose information sent by the server, where the pose information is determined by the server based on the third position information and a first pixel position of the target parking space line in the target image.</p><p id="p-0026" num="0025">Optionally, in a design of the first aspect, the obtaining a target image includes:</p><p id="p-0027" num="0026">displaying a target photographing interface, where the target photographing interface includes a photographing area, and the photographing area includes a preset guide contour; and</p><p id="p-0028" num="0027">obtaining, through photographing, the target image if the target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range.</p><p id="p-0029" num="0028">Optionally, in a design of the first aspect, before the determining pose information based on the target parking space mark and the target parking space line, the method further includes:</p><p id="p-0030" num="0029">displaying a parking space mark input indication box in the target image; and</p><p id="p-0031" num="0030">obtaining the target parking space mark input in the parking space mark input indication box.</p><p id="p-0032" num="0031">Optionally, in a design of the first aspect, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0033" num="0032">The method further includes:</p><p id="p-0034" num="0033">performing route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position; and</p><p id="p-0035" num="0034">displaying a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0036" num="0035">Optionally, in a design of the first aspect, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image; and the method further includes:</p><p id="p-0037" num="0036">displaying an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0038" num="0037">Optionally, in a design of the first aspect, the obtaining third position information corresponding to the target parking space line in a digital map includes:</p><p id="p-0039" num="0038">obtaining positioning information of the terminal device during photographing of the target image; and</p><p id="p-0040" num="0039">obtaining, from the digital map, the third position information that matches the positioning information and that corresponds to the target parking space line in the digital map.</p><p id="p-0041" num="0040">Optionally, in a design of the first aspect, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line.</p><p id="p-0042" num="0041">The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0043" num="0042">Optionally, in a design of the first aspect, the method further includes:</p><p id="p-0044" num="0043">obtaining a gravity direction of the terminal device during photographing of the target image; and</p><p id="p-0045" num="0044">the determining the pose information based on the first pixel position and the third position information includes:</p><p id="p-0046" num="0045">determining the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0047" num="0046">Optionally, in a design of the first aspect, the determining the pose information based on the first pixel position and the third position information includes:</p><p id="p-0048" num="0047">determining a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and determining the pose information based on the 2D-3D correspondence.</p><p id="p-0049" num="0048">According to a second aspect, this disclosure provides a pose determining method. The method includes:</p><p id="p-0050" num="0049">obtaining a first pixel position of a target parking space line in a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line;</p><p id="p-0051" num="0050">obtaining third position information corresponding to the target parking space line in a digital map, where the third position information indicates a coordinate position of the target parking space line in the digital map; and</p><p id="p-0052" num="0051">determining pose information based on the first pixel position and the third position information, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0053" num="0052">Optionally, in a design of the second aspect, the obtaining a first pixel position of a target parking space line in a target image includes:</p><p id="p-0054" num="0053">receiving the target image sent by the terminal device; and</p><p id="p-0055" num="0054">determining the first pixel position of the target parking space line in the target image.</p><p id="p-0056" num="0055">Optionally, in a design of the second aspect, the obtaining a first pixel position of a target parking space line in a target image includes:</p><p id="p-0057" num="0056">receiving the first pixel position that is of the target parking space line in the target image and that is sent by the terminal device.</p><p id="p-0058" num="0057">Optionally, in a design of the second aspect, the obtaining third position information corresponding to the target parking space line in a digital map includes:</p><p id="p-0059" num="0058">receiving the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0060" num="0059">determining, from the digital map, the third position information corresponding to the target parking space line.</p><p id="p-0061" num="0060">Optionally, in a design of the second aspect, the obtaining third position information corresponding to the target parking space line in a digital map includes:</p><p id="p-0062" num="0061">receiving the third position information that corresponds to the target parking space line in the digital map and that is sent by the terminal device.</p><p id="p-0063" num="0062">Optionally, in a design of the second aspect, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or</p><p id="p-0064" num="0063">the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0065" num="0064">Optionally, in a design of the second aspect, the method further includes:</p><p id="p-0066" num="0065">receiving a gravity direction that is of the terminal device during photographing of the target image and that is sent by the terminal device; and</p><p id="p-0067" num="0066">the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0068" num="0067">determining the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0069" num="0068">Optionally, in a design of the second aspect, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0070" num="0069">determining a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and</p><p id="p-0071" num="0070">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0072" num="0071">According to a third aspect, this disclosure provides a pose determining method. The method includes:</p><p id="p-0073" num="0072">obtaining a target image, where the target image includes a target parking space mark; and</p><p id="p-0074" num="0073">determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0075" num="0074">Optionally, in a design of the third aspect, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed rectangular frame of the target parking space mark and that is in the target image includes:</p><p id="p-0076" num="0075">obtaining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0077" num="0076">obtaining third position information of the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0078" num="0077">determining the pose information based on the first pixel position and the third position information.</p><p id="p-0079" num="0078">Optionally, in a design of the third aspect, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0080" num="0079">sending the target image to a server; and</p><p id="p-0081" num="0080">receiving the pose information sent by the server, where the pose information is determined by the server based on the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image, and third position information corresponding to the target circumscribed pattern in a digital map; and the third position information indicates position information that is of the circumscribed pattern of the target parking space mark and that is in the digital map.</p><p id="p-0082" num="0081">Optionally, in a design of the third aspect, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0083" num="0082">obtaining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0084" num="0083">sending the target parking space mark and the first pixel position to a server; and</p><p id="p-0085" num="0084">receiving the pose information sent by the server, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the circumscribed pattern in a digital map, and the third position information indicates position information that is of the circumscribed pattern of the target parking space mark and that is in the digital map.</p><p id="p-0086" num="0085">Optionally, in a design of the third aspect, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0087" num="0086">obtaining third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates position information that is of the circumscribed pattern of the target parking space mark and that is in the digital map;</p><p id="p-0088" num="0087">sending the target image and the third position information to a server; and</p><p id="p-0089" num="0088">receiving the pose information sent by the server, where the pose information is determined by the server based on the third position information and the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image.</p><p id="p-0090" num="0089">Optionally, in a design of the third aspect, the method further includes:</p><p id="p-0091" num="0090">displaying a parking space mark input indication box; and</p><p id="p-0092" num="0091">obtaining the target parking space mark input in the parking space mark input indication box.</p><p id="p-0093" num="0092">Optionally, in a design of the third aspect, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0094" num="0093">The method further includes:</p><p id="p-0095" num="0094">performing route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position; and</p><p id="p-0096" num="0095">displaying a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0097" num="0096">Optionally, in a design of the third aspect, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image; and the method further includes:</p><p id="p-0098" num="0097">displaying an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0099" num="0098">Optionally, in a design of the third aspect, the method further includes:</p><p id="p-0100" num="0099">obtaining positioning information of the terminal device during photographing of the target image; and the obtaining third position information corresponding to the circumscribed pattern in a digital map includes: obtaining, from the digital map, the third position information that corresponds to the circumscribed pattern matching the positioning information.</p><p id="p-0101" num="0100">Optionally, in a design of the third aspect, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line. The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0102" num="0101">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0103" num="0102">Optionally, in a design of the third aspect, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0104" num="0103">determining a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and</p><p id="p-0105" num="0104">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0106" num="0105">Optionally, in a design of the third aspect, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0107" num="0106">According to a fourth aspect, this disclosure provides a pose determining method. The method includes:</p><p id="p-0108" num="0107">obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image, where the target image includes the target parking space mark;</p><p id="p-0109" num="0108">obtaining third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0110" num="0109">determining pose information based on the first pixel position and the third position information, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0111" num="0110">Optionally, in a design of the fourth aspect, the obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image includes:</p><p id="p-0112" num="0111">receiving the target image sent by the terminal device; and</p><p id="p-0113" num="0112">determining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image.</p><p id="p-0114" num="0113">Optionally, in a design of the fourth aspect, the obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image includes:</p><p id="p-0115" num="0114">receiving the first pixel position that is of the circumscribed pattern of the target parking space mark, that is in the target image, and that is sent by the terminal device.</p><p id="p-0116" num="0115">Optionally, in a design of the fourth aspect, the obtaining third position information corresponding to the circumscribed pattern in a digital map includes:</p><p id="p-0117" num="0116">receiving the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0118" num="0117">determining, from the digital map, third position information corresponding to the circumscribed pattern in the digital map.</p><p id="p-0119" num="0118">Optionally, in a design of the fourth aspect, the obtaining third position information corresponding to the circumscribed pattern in a digital map includes:</p><p id="p-0120" num="0119">receiving the third position information that corresponds to the circumscribed pattern in the digital map and that is sent by the terminal device.</p><p id="p-0121" num="0120">Optionally, in a design of the fourth aspect, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line. The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0122" num="0121">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0123" num="0122">Optionally, in a design of the fourth aspect, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0124" num="0123">determining a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and</p><p id="p-0125" num="0124">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0126" num="0125">Optionally, in a design of the fourth aspect, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0127" num="0126">According to a fifth aspect, this disclosure provides a pose determining apparatus. The apparatus includes:</p><p id="p-0128" num="0127">an obtaining module, configured to obtain a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and</p><p id="p-0129" num="0128">a determining module, configured to determine pose information based on the target parking space mark and the target parking space line, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0130" num="0129">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0131" num="0130">obtain a first pixel position of the target parking space line in the target image;</p><p id="p-0132" num="0131">obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates a position of the target parking space line in the digital map; and</p><p id="p-0133" num="0132">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0134" num="0133">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0135" num="0134">send the target image to a server; and</p><p id="p-0136" num="0135">receive the pose information sent by the server, where the pose information is determined by the server based on a first pixel position of the target parking space line in the target image and third position information corresponding to the target parking space line in a digital map, and the third position information indicates a position of the target parking space line in the digital map.</p><p id="p-0137" num="0136">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0138" num="0137">obtain a first pixel position of the target parking space line in the target image;</p><p id="p-0139" num="0138">send the target parking space mark and the first pixel position to a server; and</p><p id="p-0140" num="0139">receive the pose information sent by the server, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the target parking space line in a digital map, and the third position information indicates a position of the target parking space line in the digital map.</p><p id="p-0141" num="0140">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0142" num="0141">obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates position information of the target parking space line in the digital map;</p><p id="p-0143" num="0142">send the target image and the third position information to a server; and</p><p id="p-0144" num="0143">receive the pose information sent by the server, where the pose information is determined by the server based on the third position information and a first pixel position of the target parking space line in the target image.</p><p id="p-0145" num="0144">Optionally, in a design of the fifth aspect, the obtaining module is specifically configured to:</p><p id="p-0146" num="0145">display a target photographing interface, where the target photographing interface includes a photographing area, and the photographing area includes a preset guide contour; and</p><p id="p-0147" num="0146">obtain, through photographing, the target image if the target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range.</p><p id="p-0148" num="0147">Optionally, in a design of the fifth aspect, the apparatus further includes:</p><p id="p-0149" num="0148">a display module, configured to display a parking space mark input indication box in the target image.</p><p id="p-0150" num="0149">The obtaining module is further configured to obtain the target parking space mark input in the parking space mark input indication box.</p><p id="p-0151" num="0150">Optionally, in a design of the fifth aspect, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0152" num="0151">The apparatus further includes:</p><p id="p-0153" num="0152">a route planning module, configured to perform route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position.</p><p id="p-0154" num="0153">The display module is further configured to display a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0155" num="0154">Optionally, in a design of the fifth aspect, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image.</p><p id="p-0156" num="0155">The display module is further configured to display an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0157" num="0156">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0158" num="0157">obtain positioning information of the terminal device during photographing of the target image; and</p><p id="p-0159" num="0158">obtain, from the digital map, the third position information that matches the positioning information and that corresponds to the target parking space line in the digital map.</p><p id="p-0160" num="0159">Optionally, in a design of the fifth aspect, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or</p><p id="p-0161" num="0160">the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0162" num="0161">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0163" num="0162">obtain a gravity direction of the terminal device during photographing of the target image; and</p><p id="p-0164" num="0163">determine the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0165" num="0164">Optionally, in a design of the fifth aspect, the determining module is specifically configured to:</p><p id="p-0166" num="0165">determine a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and</p><p id="p-0167" num="0166">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0168" num="0167">According to a sixth aspect, this disclosure provides a pose determining apparatus. The apparatus includes:</p><p id="p-0169" num="0168">an obtaining module, configured to: obtain a first pixel position of a target parking space line in a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and</p><p id="p-0170" num="0169">obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates a coordinate position of the target parking space line in the digital map; and</p><p id="p-0171" num="0170">a determining module, configured to determine pose information based on the first pixel position and the third position information, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0172" num="0171">Optionally, in a design of the sixth aspect, the obtaining module is specifically configured to:</p><p id="p-0173" num="0172">receive the target image sent by the terminal device; and</p><p id="p-0174" num="0173">determine the first pixel position of the target parking space line in the target image.</p><p id="p-0175" num="0174">Optionally, in a design of the sixth aspect, the obtaining module is specifically configured to:</p><p id="p-0176" num="0175">receive the first pixel position that is of the target parking space line in the target image and that is sent by the terminal device.</p><p id="p-0177" num="0176">Optionally, in a design of the sixth aspect, the obtaining module is specifically configured to:</p><p id="p-0178" num="0177">receive the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0179" num="0178">determine, from the digital map, the third position information corresponding to the target parking space line.</p><p id="p-0180" num="0179">Optionally, in a design of the sixth aspect, the obtaining module is specifically configured to:</p><p id="p-0181" num="0180">receive the third position information that corresponds to the target parking space line in the digital map and that is sent by the terminal device.</p><p id="p-0182" num="0181">Optionally, in a design of the sixth aspect, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or</p><p id="p-0183" num="0182">the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0184" num="0183">Optionally, in a design of the sixth aspect, the determining module is specifically configured to:</p><p id="p-0185" num="0184">receive a gravity direction that is of the terminal device during photographing of the target image and that is sent by the terminal device; and</p><p id="p-0186" num="0185">determine the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0187" num="0186">Optionally, in a design of the sixth aspect, the determining module is specifically configured to:</p><p id="p-0188" num="0187">determine a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and</p><p id="p-0189" num="0188">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0190" num="0189">According to a seventh aspect, this disclosure provides a pose determining apparatus. The apparatus includes:</p><p id="p-0191" num="0190">an obtaining module, configured to obtain a target image, where the target image includes a target parking space mark; and</p><p id="p-0192" num="0191">a determining module, configured to determine pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0193" num="0192">Optionally, in a design of the seventh aspect, the determining module is specifically configured to:</p><p id="p-0194" num="0193">obtain the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0195" num="0194">obtain third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0196" num="0195">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0197" num="0196">Optionally, in a design of the seventh aspect, the determining module is specifically configured to:</p><p id="p-0198" num="0197">obtain the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0199" num="0198">obtain third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0200" num="0199">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0201" num="0200">Optionally, in a design of the seventh aspect, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0202" num="0201">According to an eighth aspect, this disclosure provides a pose determining apparatus. The apparatus includes:</p><p id="p-0203" num="0202">an obtaining module, configured to: obtain a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image, where the target image includes the target parking space mark; and</p><p id="p-0204" num="0203">obtain third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0205" num="0204">a determining module, configured to determine pose information based on the first pixel position and the third position information, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0206" num="0205">Optionally, in a design of the eighth aspect, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line. The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0207" num="0206">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0208" num="0207">Optionally, in a design of the eighth aspect, the determining module is specifically configured to:</p><p id="p-0209" num="0208">determine a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and</p><p id="p-0210" num="0209">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0211" num="0210">Optionally, in a design of the eighth aspect, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0212" num="0211">According to a ninth aspect, this disclosure provides a pose determining apparatus, including a display, a camera, one or more processors, a memory, a plurality of applications, and one or more computer programs. The one or more computer programs are stored in the memory. The one or more computer programs include instructions. When the instructions are executed by the pose determining apparatus, the pose determining apparatus is enabled to perform the steps according to the first aspect and any one of the possible implementations of the first aspect, or the third aspect and any one of the possible implementations of the third aspect.</p><p id="p-0213" num="0212">According to a tenth aspect, this disclosure provides a server, including one or more processors, a memory, a plurality of applications, and one or more computer programs. The one or more computer programs are stored in the memory. The one or more computer programs include instructions. When the instructions are executed by an electronic device, the electronic device is enabled to perform the steps according to the second aspect and any one of the possible implementations of the second aspect, or the fourth aspect and any one of the possible implementations of the fourth aspect.</p><p id="p-0214" num="0213">According to an eleventh aspect, this disclosure provides a computer storage medium, including computer instructions. When the computer instructions are run on an electronic device or a server, the electronic device is enabled to perform the steps according to the first aspect and any one of the possible implementations of the first aspect, or the third aspect and any one of the possible implementations of the third aspect.</p><p id="p-0215" num="0214">According to a twelfth aspect, this disclosure provides a computer storage medium, including computer instructions. When the computer instructions are run on an electronic device or a server, the electronic device is enabled to perform the steps according to the second aspect and any one of the possible implementations of the second aspect, or the fourth aspect and any one of the possible implementations of the fourth aspect.</p><p id="p-0216" num="0215">According to a thirteenth aspect, this disclosure provides a computer program product. When the computer program product is run on an electronic device or a server, the electronic device is enabled to perform the steps according to the first aspect and any one of the possible implementations of the first aspect, or the third aspect and any one of the possible implementations of the third aspect.</p><p id="p-0217" num="0216">According to a fourteenth aspect, this disclosure provides a computer program product. When the computer program product is run on an electronic device or a server, the electronic device is enabled to perform the steps according to the second aspect and any one of the possible implementations of the second aspect, or the fourth aspect and any one of the possible implementations of the fourth aspect.</p><p id="p-0218" num="0217">Embodiments of this disclosure provide the pose determining method, including: obtaining the target image, where the target image includes the target parking space mark and the target parking space line, and the target parking space corresponding to the target parking space mark includes the target parking space line; and determining the pose information based on the target parking space mark and the target parking space line, where the pose information indicates the corresponding pose of the terminal during photographing of the target image. In the foregoing manner, when positioning precision of a GPS signal is poor, the pose information may be determined based on the target parking space mark and the target parking space line, to implement positioning. In addition, compared with a manner in which positioning is performed based on a 3D point cloud, in embodiments, the digital map includes less data.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0219" num="0218"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a structure of a terminal device according to an embodiment of this disclosure;</p><p id="p-0220" num="0219"><figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>is a block diagram of a software structure of a terminal device according to an embodiment of this disclosure;</p><p id="p-0221" num="0220"><figref idref="DRAWINGS">FIG. <b>2</b><i>b </i></figref>is a block diagram of a structure of a server according to an embodiment of this disclosure;</p><p id="p-0222" num="0221"><figref idref="DRAWINGS">FIG. <b>2</b><i>c </i></figref>is a block diagram of a structure of a pose determining system according to an embodiment of this disclosure;</p><p id="p-0223" num="0222"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a pose determining method according to an embodiment of this disclosure;</p><p id="p-0224" num="0223"><figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>is a schematic diagram of a target image according to an embodiment of this disclosure;</p><p id="p-0225" num="0224"><figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>is a schematic diagram of a target image according to an embodiment of this disclosure;</p><p id="p-0226" num="0225"><figref idref="DRAWINGS">FIG. <b>4</b><i>c </i></figref>is a schematic diagram of a target parking space according to an embodiment of this disclosure;</p><p id="p-0227" num="0226"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a target parking space mark according to an embodiment of this disclosure;</p><p id="p-0228" num="0227"><figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>is a schematic diagram of a map according to an embodiment of this disclosure;</p><p id="p-0229" num="0228"><figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>is a schematic flowchart of a pose determining method according to an embodiment of this disclosure;</p><p id="p-0230" num="0229"><figref idref="DRAWINGS">FIG. <b>7</b><i>a </i></figref>is a schematic flowchart of a pose determining method according to an embodiment of this disclosure;</p><p id="p-0231" num="0230"><figref idref="DRAWINGS">FIG. <b>7</b><i>b </i></figref>is a schematic flowchart of a pose determining method according to an embodiment of this disclosure;</p><p id="p-0232" num="0231"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>to <figref idref="DRAWINGS">FIG. <b>8</b><i>d </i></figref>are schematic diagrams of interfaces of a terminal device according to an embodiment of this disclosure;</p><p id="p-0233" num="0232"><figref idref="DRAWINGS">FIG. <b>8</b><i>e </i></figref>is a schematic flowchart of a pose determining method according to an embodiment of this disclosure;</p><p id="p-0234" num="0233"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic flowchart of a pose determining method according to an embodiment of this disclosure;</p><p id="p-0235" num="0234"><figref idref="DRAWINGS">FIG. <b>10</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>10</b><i>b </i></figref>are schematic diagrams of interfaces of a terminal device according to an embodiment of this disclosure;</p><p id="p-0236" num="0235"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram of a structure of a pose determining apparatus according to an embodiment of this disclosure;</p><p id="p-0237" num="0236"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of a structure of a pose determining apparatus according to an embodiment of this disclosure;</p><p id="p-0238" num="0237"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram of a structure of a terminal device according to an embodiment of this disclosure; and</p><p id="p-0239" num="0238"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic diagram of a structure of a server according to an embodiment of this disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0240" num="0239">The following describes embodiments of the present invention with reference to the accompanying drawings in embodiments of the present invention. Terms used in an implementation part of the present invention are intended only to explain specific embodiments of the present invention, and not intended to limit the present invention.</p><p id="p-0241" num="0240">The following describes embodiments of this disclosure with reference to the accompanying drawings. A person of ordinary skill in the art may learn that the technical solutions provided in embodiments of this disclosure are also applicable to a similar technical problem as a technology develops and a new scenario emerges.</p><p id="p-0242" num="0241">In the specification, claims, and accompanying drawings of this disclosure, terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, and the like are intended to distinguish between similar objects, but do not necessarily indicate a specific order or sequence. It should be understood that the terms used in such a way are interchangeable in appropriate circumstances, which is merely a distinguishing manner that is used when objects having a same attribute are described in embodiments of this disclosure. In addition, terms &#x201c;include&#x201d;, &#x201c;comprise&#x201d; and any other variants thereof mean to cover the non-exclusive inclusion, so that a process, method, system, product, or device that includes a series of units is not necessarily limited to those units, but may include other units not expressly listed or inherent to such a process, method, product, or device.</p><p id="p-0243" num="0242">For ease of understanding, the following describes, by using an example, a structure of a terminal device <b>100</b> provided in an embodiment of this disclosure. Refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a structure of a terminal device according to an embodiment of this disclosure.</p><p id="p-0244" num="0243">As shown in the <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the terminal <b>100</b> may include a processor <b>110</b>, an external memory interface <b>120</b>, an internal memory <b>121</b>, a universal serial bus (USB) interface <b>130</b>, a charging management module <b>140</b>, a power management module <b>141</b>, a battery <b>142</b>, an antenna <b>1</b>, an antenna <b>2</b>, a mobile communication module <b>150</b>, a wireless communication module <b>160</b>, an audio module <b>170</b>, a speaker <b>170</b>A, a receiver <b>170</b>B, a microphone <b>170</b>C, a headset jack <b>170</b>D, a sensor module <b>180</b>, a button <b>190</b>, a motor <b>191</b>, an indicator <b>192</b>, a camera <b>193</b>, a display <b>194</b>, a subscriber identification module (SIM) card interface <b>195</b>, and the like. The sensor module <b>180</b> may include a pressure sensor <b>180</b>A, a gyroscope sensor <b>180</b>B, a barometric pressure sensor <b>180</b>C, a magnetic sensor <b>180</b>D, an acceleration sensor <b>180</b>E, a distance sensor <b>180</b>F, an optical proximity sensor <b>180</b>G, a fingerprint sensor <b>180</b>H, a temperature sensor <b>180</b>J, a touch sensor <b>180</b>K, an ambient light sensor <b>180</b>L, a bone conduction sensor <b>180</b>M, and the like.</p><p id="p-0245" num="0244">It may be understood that the example structure in this embodiment of the present invention does not constitute a specific limitation on the terminal <b>100</b>. In some other embodiments of this disclosure, the terminal <b>100</b> may include more or fewer components than those shown in the figure, or combine some components, or split some components, or have different component arrangements. The components shown in the figure may be implemented by using hardware, software, or a combination of software and hardware.</p><p id="p-0246" num="0245">The processor <b>110</b> may include one or more processing units. For example, the processor <b>110</b> may include an application processor (AP), a modem processor, a graphics processing unit (GPU), an image signal processor (ISP), a controller, a video codec, a digital signal processor (DSP), a baseband processor, a neural network processing unit (NPU), and/or the like. Different processing units may be independent components, or may be integrated into one or more processors.</p><p id="p-0247" num="0246">The controller may generate an operation control signal based on instruction operation code and a time sequence signal, to complete control of instruction reading and instruction execution.</p><p id="p-0248" num="0247">A memory may be further disposed in the processor <b>110</b>, and is configured to store instructions and data. In some embodiments, the memory in the processor <b>110</b> is a cache. The memory may store instructions or data that has been used or cyclically used by the processor <b>110</b>. If the processor <b>110</b> needs to use the instructions or the data again, the processor may directly invoke the instructions or the data from the memory. This avoids repeated access, reduces waiting time of the processor <b>110</b>, and improves system efficiency.</p><p id="p-0249" num="0248">In some embodiments, the processor <b>110</b> may include one or more interfaces. The interface may include an inter-integrated circuit (I2C) interface, an inter-integrated circuit sound (I2S) interface, a pulse code modulation (PCM) interface, a universal asynchronous receiver/transmitter (UART) interface, a mobile industry processor interface (MIPI), a general-purpose input/output (GPIO) interface, a subscriber identity module (SIM) interface, a universal serial bus (USB) interface, and/or the like.</p><p id="p-0250" num="0249">The I2C interface is a two-way synchronization serial bus, and includes one serial data line (SDA) and one serial clock line (SCL). In some embodiments, the processor <b>110</b> may include a plurality of groups of I2C buses. The processor <b>110</b> may be separately coupled to the touch sensor <b>180</b>K, a charger, a flashlight, the camera <b>193</b>, and the like through different I2C bus interfaces. For example, the processor <b>110</b> may be coupled to the touch sensor <b>180</b>K through the I2C interface, so that the processor <b>110</b> communicates with the touch sensor <b>180</b>K through the I2C bus interface, to implement a touch function of the terminal <b>100</b>.</p><p id="p-0251" num="0250">The I2S interface may be configured to perform audio communication. In some embodiments, the processor <b>110</b> may include a plurality of groups of I2S buses. The processor <b>110</b> may be coupled to the audio module <b>170</b> through the I2S bus, to implement communication between the processor <b>110</b> and the audio module <b>170</b>. In some embodiments, the audio module <b>170</b> may transmit an audio signal to the wireless communication module <b>160</b> through the I2S interface, to implement a function of answering a call through a Bluetooth headset.</p><p id="p-0252" num="0251">The PCM interface may also be used to perform audio communication, and sample, quantize, and code an analog signal. In some embodiments, the audio module <b>170</b> may be coupled to the wireless communication module <b>160</b> through a PCM bus interface. In some embodiments, the audio module <b>170</b> may alternatively transmit an audio signal to the wireless communication module <b>160</b> through the PCM interface, to implement a function of answering a call through a Bluetooth headset. Both the I2S interface and the PCM interface may be configured to perform the audio communication.</p><p id="p-0253" num="0252">The UART interface is a universal serial data bus, and is configured to perform asynchronous communication. The bus may be a two-way communication bus. The bus converts to-be-transmitted data between serial communication and parallel communication. In some embodiments, the UART interface is usually used to connect the processor <b>110</b> to the wireless communication module <b>160</b>. For example, the processor <b>110</b> communicates with a Bluetooth module in the wireless communication module <b>160</b> through the UART interface, to implement a Bluetooth function. In some embodiments, the audio module <b>170</b> may transmit an audio signal to the wireless communication module <b>160</b> through the UART interface, to implement a function of playing music through the Bluetooth headset.</p><p id="p-0254" num="0253">The MIPI interface may be used to connect the processor <b>110</b> to a peripheral component such as the display <b>194</b> or the camera <b>193</b>. The MIPI interface includes a camera serial interface (CSI), a display serial interface (DSI), and the like. In some embodiments, the processor <b>110</b> communicates with the camera <b>193</b> through the CSI interface, to implement a photographing function of the terminal <b>100</b>. The processor <b>110</b> communicates with the display <b>194</b> through the DSI interface, to implement a display function of the terminal <b>100</b>.</p><p id="p-0255" num="0254">The GPIO interface may be configured by using software. The GPIO interface may be configured as a control signal or a data signal. In some embodiments, the GPIO interface may be configured to connect the processor <b>110</b> to the camera <b>193</b>, the display <b>194</b>, the wireless communication module <b>160</b>, the audio module <b>170</b>, the sensor module <b>180</b>, or the like. The GPIO interface may alternatively be configured as an I2C interface, an I2S interface, a UART interface, an MIPI interface, or the like.</p><p id="p-0256" num="0255">The USB interface <b>130</b> is an interface that complies with a USB standard specification, and may be specifically a mini USB interface, a micro USB interface, a USB Type-C interface, or the like. The USB interface <b>130</b> may be configured to connect to a charger to charge the terminal <b>100</b>, and may also be configured to transmit data between the terminal <b>100</b> and a peripheral device, or may be configured to connect to a headset, to play audio by using the headset. The interface may be further configured to connect to another electronic device such as an AR device.</p><p id="p-0257" num="0256">It may be understood that an interface connection relationship between the modules that is shown in this embodiment of the present invention is merely an example for description, and does not constitute a limitation on the structure of the terminal <b>100</b>. In some other embodiments of this disclosure, the terminal <b>100</b> may alternatively use an interface connection mode different from that in the foregoing embodiment, or a combination of a plurality of interface connection modes.</p><p id="p-0258" num="0257">The charging management module <b>140</b> is configured to receive a charging input from the charger. The charger may be a wireless charger or a wired charger. In some embodiments of wired charging, the charging management module <b>140</b> may receive a charging input of a wired charger through the USB interface <b>130</b>. In some embodiments of wireless charging, the charging management module <b>140</b> may receive a wireless charging input by using a wireless charging coil of the terminal <b>100</b>. The charging management module <b>140</b> supplies power to the electronic device through the power management module <b>141</b> while charging the battery <b>142</b>.</p><p id="p-0259" num="0258">The power management module <b>141</b> is configured to connect to the battery <b>142</b>, the charging management module <b>140</b>, and the processor <b>110</b>. The power management module <b>141</b> receives an input from the battery <b>142</b> and/or the charging management module <b>140</b>, and supplies power to the processor <b>110</b>, the internal memory <b>121</b>, the display <b>194</b>, the camera <b>193</b>, the wireless communication module <b>160</b>, and the like. The power management module <b>141</b> may be configured to monitor parameters such as a battery capacity, a battery cycle count, and a battery status of health (electric leakage and impedance). In some other embodiments, the power management module <b>141</b> may alternatively be disposed in the processor <b>110</b>. In some other embodiments, the power management module <b>141</b> and the charging management module <b>140</b> may alternatively be disposed in a same device.</p><p id="p-0260" num="0259">A wireless communication function of the terminal <b>100</b> may be implemented by using the antenna <b>1</b>, the antenna <b>2</b>, the mobile communication module <b>150</b>, the wireless communication module <b>160</b>, the modem processor, the baseband processor, and the like.</p><p id="p-0261" num="0260">The antenna <b>1</b> and the antenna <b>2</b> are configured to transmit and receive an electromagnetic wave signal. Each antenna in the terminal <b>100</b> may be configured to cover one or more communication frequency bands. Different antennas may be further reused, to improve antenna utilization. For example, the antenna <b>1</b> may be reused as a diversity antenna of a wireless local area network. In some other embodiments, an antenna may be used in combination with a tuning switch.</p><p id="p-0262" num="0261">The mobile communication module <b>150</b> can provide a solution, applied to the terminal <b>100</b>, to wireless communication including 2G, 3G, 4G, 5G, and the like. The mobile communication module <b>150</b> may include at least one filter, a switch, a power amplifier, a low noise amplifier (LNA), and the like. The mobile communication module <b>150</b> may receive an electromagnetic wave through the antenna <b>1</b>, perform processing such as filtering or amplification on the received electromagnetic wave, and transmit the electromagnetic wave to the modem processor for demodulation. The mobile communication module <b>150</b> may further amplify a signal modulated by the modem processor, and convert the signal into an electromagnetic wave for radiation through the antenna <b>1</b>. In some embodiments, at least a part of functional modules in the mobile communication module <b>150</b> may be disposed in the processor <b>110</b>. In some embodiments, at least a part of functional modules of the mobile communication module <b>150</b> and at least a part of modules of the processor <b>110</b> may be disposed in a same device.</p><p id="p-0263" num="0262">The modem processor may include a modulator and a demodulator. The modulator is configured to modulate a to-be-sent low-frequency baseband signal into a medium-high frequency signal. The demodulator is configured to demodulate a received electromagnetic wave signal into a low-frequency baseband signal. Then, the demodulator transmits the low-frequency baseband signal obtained through demodulation to the baseband processor for processing. The baseband processor processes the low-frequency baseband signal, and then transmits a processed signal to the application processor. The application processor outputs a sound signal through an audio device (which is not limited to the speaker <b>170</b>A, the receiver <b>170</b>B, or the like), or displays an image or a video through the display <b>194</b>. In some embodiments, the modem processor may be an independent component. In some other embodiments, the modem processor may be independent of the processor <b>110</b>, and is disposed in a same device as the mobile communication module <b>150</b> or another functional module.</p><p id="p-0264" num="0263">The wireless communication module <b>160</b> may provide a solution, applied to the terminal <b>100</b>, to wireless communication including a wireless local area network (WLAN) (for example, a wireless fidelity (Wi-Fi) network), Bluetooth (, BT), a global navigation satellite system (GNSS), frequency modulation (FM), near field communication (NFC), an infrared (IR) technology, or the like. The wireless communication module <b>160</b> may be one or more components integrating at least one communication processing module. The wireless communication module <b>160</b> receives an electromagnetic wave by using the antenna <b>2</b>, performs frequency modulation and filtering processing on an electromagnetic wave signal, and sends a processed signal to the processor <b>110</b>. The wireless communication module <b>160</b> may further receive a to-be-sent signal from the processor <b>110</b>, perform frequency modulation and amplification on the signal, and convert a processed signal into an electromagnetic wave for radiation through the antenna <b>2</b>.</p><p id="p-0265" num="0264">In some embodiments, in the terminal <b>100</b>, the antenna <b>1</b> is coupled to the mobile communication module <b>150</b>, and the antenna <b>2</b> is coupled to the wireless communication module <b>160</b>, so that the terminal <b>100</b> can communicate with a network and another device by using a wireless communication technology. The wireless communication technology may include a global system for mobile communications (GSM), a general packet radio service (GPRS), code division multiple access (CDMA), wideband code division multiple access (WCDMA), time-division code division multiple access (TD-SCDMA), long term evolution (LTE), BT, a GNSS, a WLAN, NFC, FM, an IR technology, and/or the like. The GNSS may include a global positioning system (GPS), a global navigation satellite system (GLONASS), a BeiDou navigation satellite system (BDS), a quasi-zenith satellite system (QZSS), and/or a satellite based augmentation system (SBAS).</p><p id="p-0266" num="0265">The terminal <b>100</b> implements the display function through the GPU, the display <b>194</b>, the application processor, and the like. The GPU is a microprocessor for image processing, and is connected to the display <b>194</b> and the application processor. The GPU is configured to: perform mathematical and geometric computation, and render an image. The processor <b>110</b> may include one or more GPUs, which execute program instructions to generate or change display information.</p><p id="p-0267" num="0266">The display <b>194</b> is configured to display an image, a video, and the like. The display <b>194</b> includes a display panel. The display panel may be a liquid crystal display (LCD), an organic light-emitting diode (OLED), an active-matrix organic light-emitting diode (AMOLED), a flexible light-emitting diode (FLED), a mini-LED, a micro-LED, a micro-OLED, a quantum dot light-emitting diode (QLED), or the like. In some embodiments, the terminal <b>100</b> may include one or N displays <b>194</b>, where N is a positive integer greater than 1.</p><p id="p-0268" num="0267">The terminal <b>100</b> can implement a photographing function by using the ISP, the camera <b>193</b>, the video codec, the GPU, the display <b>194</b>, the application processor, and the like.</p><p id="p-0269" num="0268">The ISP may be configured to process data fed back by the camera <b>193</b>. For example, during photographing, a shutter is pressed, and a ray of light is transmitted to a photosensitive element of a camera through a lens. An optical signal is converted into an electrical signal. The photosensitive element of the camera transmits the electrical signal to the ISP for processing, to convert a processed signal into a visible image. The ISP may further perform algorithm optimization on noise, brightness, and complexion of the image. The ISP may further optimize parameters such as exposure and a color temperature of a photographing scenario. In some embodiments, the ISP may be disposed in the camera <b>193</b>.</p><p id="p-0270" num="0269">The camera <b>193</b> may be configured to capture a static image or a video. An optical image of an object is generated by using the lens, and is projected onto the photosensitive element. The photosensitive element may be a charge coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS) phototransistor. The light-sensitive element converts an optical signal into an electrical signal, and then transmits the electrical signal to the ISP for conversion into a digital image signal. The ISP outputs the digital image signal to the DSP for processing. The DSP converts the digital image signal into an image signal in a standard format such as RGB or YUV. In some embodiments, the terminal <b>100</b> may include one or N cameras <b>193</b>, where N is a positive integer greater than 1.</p><p id="p-0271" num="0270">The digital signal processor is configured to process a digital signal, and may process another digital signal in addition to the digital image signal. For example, when the terminal <b>100</b> selects a frequency bin, the digital signal processor is configured to perform Fourier transform and the like on frequency-bin energy.</p><p id="p-0272" num="0271">The video codec is configured to compress or decompress a digital video. The terminal <b>100</b> may support one or more video codecs. In this way, the terminal <b>100</b> can play or record videos in a plurality of coding formats, for example, moving picture experts group (MPEG)-1, MPEG-2, MPEG-3, and MPEG-4.</p><p id="p-0273" num="0272">The NPU is a neural-network (NN) computing processor. The NPU quickly processes input information with reference to a structure of a biological neural network, for example, a transfer mode between human brain neurons, and may further continuously perform self-learning. Applications such as intelligent cognition of the terminal <b>100</b> may be implemented by using the NPU, for example, image recognition, facial recognition, speech recognition, and text understanding.</p><p id="p-0274" num="0273">The external memory interface <b>120</b> may be configured to connect to an external memory card such as a micro SD card, to extend a storage capability of the terminal <b>100</b>. The external memory card communicates with the processor <b>110</b> through the external memory interface <b>120</b>, to implement a data storage function. For example, data such as music and a video is stored in the external memory card.</p><p id="p-0275" num="0274">The internal memory <b>121</b> may be configured to store computer-executable program code. The executable program code includes instructions. The internal memory <b>121</b> may include a program storage area and a data storage area. The program storage area may store an operating system, an application required by at least one function (for example, a sound playing function or an image playing function), and the like. The data storage area may store data (for example, audio data and an address book) created during use of the terminal <b>100</b>, and the like. In addition, the internal memory <b>121</b> may include a high-speed random access memory, and may further include a nonvolatile memory, for example, at least one magnetic disk storage device, a flash memory device, or a universal flash storage (UFS). The processor <b>110</b> runs the instructions stored in the internal memory <b>121</b> and/or the instructions stored in the memory disposed in the processor, to execute various function applications of the terminal <b>100</b> and process data.</p><p id="p-0276" num="0275">The terminal <b>100</b> may implement audio functions such as music playing and recording through the audio module <b>170</b>, the speaker <b>170</b>A, the receiver <b>170</b>B, the microphone <b>170</b>C, the headset jack <b>170</b>D, the application processor, and the like.</p><p id="p-0277" num="0276">The audio module <b>170</b> is configured to convert digital audio information into an analog audio signal for output, and is also configured to convert an analog audio input into a digital audio signal. The audio module <b>170</b> may be further configured to encode and decode the audio signal. In some embodiments, the audio module <b>170</b> may be disposed in the processor <b>110</b>, or some functional modules in the audio module <b>170</b> are disposed in the processor <b>110</b>.</p><p id="p-0278" num="0277">The speaker <b>170</b>A, also referred to as a &#x201c;loudspeaker&#x201d;, is configured to convert an audio electrical signal into a sound signal. The terminal <b>100</b> may listen to music or answer a hands-free call by using the speaker <b>170</b>A.</p><p id="p-0279" num="0278">The receiver <b>170</b>B, also referred to as an &#x201c;earpiece&#x201d;, is configured to convert an audio electrical signal into a sound signal. When the terminal <b>100</b> is used to answer a call or receive a voice message, the receiver <b>170</b>B may be put close to a human ear to listen to a voice.</p><p id="p-0280" num="0279">The microphone <b>170</b>C, also referred to as &#x201c;mike&#x201d; or &#x201c;mic&#x201d;, is configured to convert a sound signal into an electrical signal. When making a call or sending a voice message, a user may make a sound near the microphone <b>170</b>C through the mouth, to input a sound signal to the microphone <b>170</b>C. At least one microphone <b>170</b>C may be disposed in the terminal <b>100</b>. In some other embodiments, two microphones <b>170</b>C may be disposed in the terminal <b>100</b>, to collect a sound signal and further implement a noise reduction function. In some other embodiments, three, four, or more microphones <b>170</b>C may alternatively be disposed in the terminal <b>100</b>, to collect a sound signal, reduce noise, recognize a sound source, implement a directional recording function, and the like.</p><p id="p-0281" num="0280">The headset jack <b>170</b>D is configured to connect to a wired headset. The headset jack <b>170</b>D may be the USB interface <b>130</b>, a 3.5 mm open mobile terminal platform (OMTP) standard interface, or a cellular telecommunications industry association of the USA (CTIA) standard interface.</p><p id="p-0282" num="0281">The pressure sensor <b>180</b>A is configured to sense a pressure signal, and may convert the pressure signal into an electrical signal. In some embodiments, the pressure sensor <b>180</b>A may be disposed on the display <b>194</b>. There are many types of pressure sensors <b>180</b>A, for example, a resistive pressure sensor, an inductive pressure sensor, and a capacitive pressure sensor. The capacitive pressure sensor may include at least two parallel plates using a conductive material. When a force is applied to the pressure sensor <b>180</b>A, capacitance between electrodes changes. The terminal <b>100</b> determines pressure strength based on a capacitance change. When a touch operation is performed on the display <b>194</b>, the terminal <b>100</b> detects intensity of the touch operation by using the pressure sensor <b>180</b>A. The terminal <b>100</b> may also calculate a touch position based on a detection signal of the pressure sensor <b>180</b>A. In some embodiments, touch operations that are performed in a same touch position but have different touch operation intensity may correspond to different operation instructions. For example, when a touch operation whose touch operation intensity is less than a first pressure threshold is performed on an SMS message application icon, an instruction for viewing an SMS message is executed. When a touch operation whose touch operation intensity is greater than or equal to a first pressure threshold is performed on an SMS message application icon, an instruction for creating a new SMS message is executed.</p><p id="p-0283" num="0282">The gyroscope sensor <b>180</b>B may be configured to determine a moving posture of the terminal <b>100</b>. In some embodiments, angular velocities of the terminal <b>100</b> around three axes (namely, x, y, and z axes) may be determined by using the gyroscope sensor <b>180</b>B. The gyroscope sensor <b>180</b>B may be configured to implement image stabilization during photographing. For example, when the shutter is opened, the gyroscope sensor <b>180</b>B detects an angle at which the terminal <b>100</b> jitters, calculates, based on the angle, a distance for which a lens module needs to compensate, and allows the lens to cancel out the jitter of the terminal <b>100</b> through reverse motion, to implement image stabilization. The gyroscope sensor <b>180</b>B may also be used in a navigation scenario and a somatic game scenario.</p><p id="p-0284" num="0283">The barometric pressure sensor <b>180</b>C is configured to measure barometric pressure. In some embodiments, the terminal <b>100</b> calculates an altitude based on a barometric pressure value measured by the barometric pressure sensor <b>180</b>C, to assist in positioning and navigation.</p><p id="p-0285" num="0284">The magnetic sensor <b>180</b>D includes a Hall sensor. The terminal <b>100</b> may detect opening and closing of a flip leather case by using the magnetic sensor <b>180</b>D. In some embodiments, when the terminal <b>100</b> is a clamshell phone, the terminal <b>100</b> may detect opening and closing of a flip cover by using the magnetic sensor <b>180</b>D. Further, a feature such as automatic unlocking of the flip cover is set based on a detected opening or closing state of the leather case or a detected opening or closing state of the flip cover.</p><p id="p-0286" num="0285">The acceleration sensor <b>180</b>E may detect values of accelerations of the terminal <b>100</b> in various directions (usually on three axes). When the terminal <b>100</b> is still, a value and a direction of gravity may be detected. The acceleration sensor <b>180</b>E may be further configured to recognize a posture of the electronic device, and is used in application such as switching between a landscape mode and a portrait mode or a pedometer.</p><p id="p-0287" num="0286">The distance sensor <b>180</b>F is configured to measure a distance. The terminal <b>100</b> may measure a distance through infrared light or a laser. In some embodiments, the terminal <b>100</b> may use the distance sensor <b>180</b>F to measure a distance, to implement fast focusing in a photographing scenario.</p><p id="p-0288" num="0287">The optical proximity sensor <b>180</b>G may include, for example, a light-emitting diode (LED) and an optical detector such as a photodiode. The light-emitting diode may be an infrared light-emitting diode. The terminal <b>100</b> emits infrared light by using the light-emitting diode. The terminal <b>100</b> detects infrared reflected light from a nearby object by using the photodiode. When sufficient reflected light is detected, it may be determined that there is an object near the terminal <b>100</b>. When insufficient reflected light is detected, the terminal <b>100</b> may determine that there is no object near the terminal <b>100</b>. The terminal <b>100</b> may detect, by using the optical proximity sensor <b>180</b>G, that the terminal <b>100</b> held by the user is close to an ear for a call, to automatically turn off a screen to save power. The optical proximity sensor <b>180</b>G may also be used in a smart cover mode or a pocket mode to automatically perform screen unlocking or locking.</p><p id="p-0289" num="0288">The ambient light sensor <b>180</b>L is configured to sense ambient light brightness. The terminal <b>100</b> may adaptively adjust brightness of the display <b>194</b> based on the sensed ambient light brightness. The ambient light sensor <b>180</b>L may also be configured to automatically adjust white balance during photographing. The ambient light sensor <b>180</b>L may also cooperate with the optical proximity sensor <b>180</b>G to detect whether the terminal <b>100</b> is in a pocket to prevent a false touch.</p><p id="p-0290" num="0289">The fingerprint sensor <b>180</b>H is configured to collect a fingerprint. The terminal <b>100</b> may use a feature of the collected fingerprint to implement fingerprint-based unlocking, application lock access, fingerprint-based photographing, fingerprint-based call answering, and the like.</p><p id="p-0291" num="0290">The temperature sensor <b>180</b>J is configured to detect a temperature. In some embodiments, the terminal <b>100</b> executes a temperature processing policy by using the temperature detected by the temperature sensor <b>180</b>J. For example, when the temperature reported by the temperature sensor <b>180</b>J exceeds a threshold, the terminal <b>100</b> lowers performance of a processor located near the temperature sensor <b>180</b>J, to reduce power consumption to implement thermal protection. In some other embodiments, when the temperature is lower than another threshold, the terminal <b>100</b> heats the battery <b>142</b> to prevent the terminal <b>100</b> from being shut down abnormally because of a low temperature. In some other embodiments, when the temperature is lower than still another threshold, the terminal <b>100</b> boosts an output voltage of the battery <b>142</b> to avoid abnormal shutdown caused by a low temperature.</p><p id="p-0292" num="0291">The touch sensor <b>180</b>K is also referred to as a &#x201c;touch component&#x201d;. The touch sensor <b>180</b>K may be disposed on the display <b>194</b>, and the touch sensor <b>180</b>K and the display <b>194</b> constitute a touchscreen, which is also referred to as a &#x201c;touch screen&#x201d;. The touch sensor <b>180</b>K is configured to detect a touch operation performed on or near the touch sensor <b>180</b>K. The touch sensor may transfer the detected touch operation to the application processor to determine a touch event type. A visual output related to the touch operation may be provided through the display <b>194</b>. In some other embodiments, the touch sensor <b>180</b>K may also be disposed on a surface of the terminal <b>100</b> in a position different from a position of the display <b>194</b>.</p><p id="p-0293" num="0292">The bone conduction sensor <b>180</b>M may obtain a vibration signal. In some embodiments, the bone conduction sensor <b>180</b>M may obtain a vibration signal of a vibration bone of a human vocal-cord part. The bone conduction sensor <b>180</b>M may also be in contact with a body pulse to receive a blood pressure beating signal. In some embodiments, the bone conduction sensor <b>180</b>M may also be disposed in the headset, to obtain a bone conduction headset. The audio module <b>170</b> may obtain a speech signal through parsing based on the vibration signal that is of the vibration bone of the vocal-cord part and that is obtained by the bone conduction sensor <b>180</b>M, to implement a speech function. The application processor may parse heart rate information based on the blood pressure beating signal obtained by the bone conduction sensor <b>180</b>M, to implement a heart rate detection function.</p><p id="p-0294" num="0293">The button <b>190</b> includes a power button, a volume button, and the like. The button <b>190</b> may be a mechanical button, or may be a touch button. The terminal <b>100</b> may receive a button input, and generate a button signal input related to a user setting and function control of the terminal <b>100</b>.</p><p id="p-0295" num="0294">The motor <b>191</b> may generate a vibration prompt. The motor <b>191</b> may be configured to provide an incoming call vibration prompt and a touch vibration feedback. For example, touch operations performed on different applications (for example, a photographing application and an audio playing application) may correspond to different vibration feedback effects. The motor <b>191</b> may also correspond to different vibration feedback effects for touch operations performed on different areas of the display <b>194</b>. Different application scenarios (for example, time reminding, information receiving, an alarm clock, and a game) may also correspond to different vibration feedback effects. A touch vibration feedback effect may be further customized.</p><p id="p-0296" num="0295">The indicator <b>192</b> may be an indicator light, and may be configured to indicate a charging status and a power change, or may be configured to indicate a message, a missed call, a notification, and the like.</p><p id="p-0297" num="0296">The SIM card interface <b>195</b> is configured to connect to a SIM card. The SIM card may be inserted into the SIM card interface <b>195</b> or plugged from the SIM card interface <b>195</b>, to implement contact with or separation from the terminal <b>100</b>. The terminal <b>100</b> may support one or N SIM card interfaces, where N is a positive integer greater than 1. The SIM card interface <b>195</b> may support a nano-SIM card, a micro-SIM card, a SIM card, and the like. A plurality of cards may be inserted into a same SIM card interface <b>195</b> at the same time. The plurality of cards may be of a same type or different types. The SIM card interface <b>195</b> may be compatible with different types of SIM cards. The SIM card interface <b>195</b> may also be compatible with the external storage card. The terminal <b>100</b> interacts with a network through a SIM card, to implement functions such as calling and data communication. In some embodiments, the terminal <b>100</b> uses an eSIM, namely, an embedded SIM card. The eSIM card may be embedded into the terminal <b>100</b>, and cannot be separated from the terminal <b>100</b>.</p><p id="p-0298" num="0297">A software system of the terminal <b>100</b> may use a layered architecture, an event-driven architecture, a microkernel architecture, a microservice architecture, or a cloud architecture. In this embodiment of the present invention, an Android system with the layered architecture is used as an example to illustrate a software structure of the terminal <b>100</b>.</p><p id="p-0299" num="0298"><figref idref="DRAWINGS">FIG. <b>2</b><i>a </i></figref>is a block diagram of the software structure of the terminal <b>100</b> according to this embodiment of the present invention.</p><p id="p-0300" num="0299">In the layered architecture, software is divided into several layers, and each layer has a clear role and task. The layers communicate with each other through a software interface. In some embodiments, the Android system is divided into four layers: an application layer, an application framework layer, an Android runtime and system library, and a kernel layer from top to bottom.</p><p id="p-0301" num="0300">The application layer may include a series of application packages.</p><p id="p-0302" num="0301">As shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>a</i></figref>, the application packages may include applications such as Camera, Gallery, Calendar, Phone, Map, Navigation, WLAN, Bluetooth, Music, Videos, and SMS Messages.</p><p id="p-0303" num="0302">The application framework layer provides an application programming interface (API) and a programming framework for an application at the application layer. The application framework layer includes some predefined functions.</p><p id="p-0304" num="0303">As shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>a</i></figref>, the application framework layer may include a window manager, a content provider, a view system, a phone manager, a resource manager, a notification manager, and the like.</p><p id="p-0305" num="0304">The window manager is configured to manage a window program. The window manager may obtain a size of the display, determine whether there is a status bar, lock a screen, take a screenshot, and the like.</p><p id="p-0306" num="0305">The content provider is configured to store and obtain data, and enable the data to be accessed by an application. The data may include a video, an image, an audio, calls that are made and answered, a browsing history and bookmarks, an address book, and the like.</p><p id="p-0307" num="0306">The view system includes visual controls such as a control for displaying text and a control for displaying a picture. The view system may be configured to construct an application. A display interface may include one or more views. For example, a display interface including a notification icon of SMS Messages may include a text display view and a picture display view.</p><p id="p-0308" num="0307">The phone manager is configured to provide a communication function of the terminal <b>100</b>, for example, management of call statuses (including answering, declining, and the like).</p><p id="p-0309" num="0308">The resource manager provides various resources such as a localized character string, an icon, an image, a layout file, and a video file for an application.</p><p id="p-0310" num="0309">The notification manager enables an application to display notification information in a status bar, and may be configured to convey a notification type message. The displayed notification information may automatically disappear after a short pause without user interaction. For example, the notification manager is configured to notify download completion, provide a message notification, and the like. The notification manager may alternatively be a notification that appears in a top status bar of the system in a form of a graph or a scroll bar text, for example, a notification of an application that is run on a background, or may be a notification that appears on a screen in a form of a dialog window. For example, text information is displayed in the status bar, an announcement is given, the electronic device vibrates, or the indicator light blinks.</p><p id="p-0311" num="0310">The Android runtime includes a kernel library and a virtual machine. The Android runtime is responsible for scheduling and management of the Android system.</p><p id="p-0312" num="0311">The kernel library includes two parts: a function that needs to be invoked in java language and a kernel library of Android.</p><p id="p-0313" num="0312">The application layer and the application framework layer run on the virtual machine. The virtual machine executes Java files at the application layer and the application framework layer as binary files. The virtual machine is configured to implement functions such as object lifecycle management, stack management, thread management, security and exception management, and garbage collection.</p><p id="p-0314" num="0313">The system library may include a plurality of functional modules, for example, a surface manager, a media library, a three-dimensional graphics processing library (for example, OpenGL ES), and a 2D graphics engine (for example, SGL).</p><p id="p-0315" num="0314">The surface manager is configured to manage a display subsystem and provide fusion of 2D and 3D layers for a plurality of applications.</p><p id="p-0316" num="0315">The media library supports playing and recording of a plurality of commonly used audio and video formats, static image files, and the like. The media library may support a plurality of audio and video coding formats, for example, MPEG-4, H.264, MP3, AAC, AMR, JPG, and PNG.</p><p id="p-0317" num="0316">The three-dimensional graphics processing library is configured to implement three-dimensional graphics drawing, image rendering, composition, layer processing, and the like.</p><p id="p-0318" num="0317">The 2D graphics engine is a drawing engine for 2D drawing.</p><p id="p-0319" num="0318">The kernel layer is a layer between hardware and software. The kernel layer includes at least a display driver, a camera driver, an audio driver, and a sensor driver.</p><p id="p-0320" num="0319">The following describes a working process of software and hardware of the terminal <b>100</b> by using an example with reference to a photographing capture scenario.</p><p id="p-0321" num="0320">When the touch sensor <b>180</b>K receives a touch operation, a corresponding hardware interrupt is sent to the kernel layer. The kernel layer processes the touch operation into an original input event (including information such as touch coordinates and a timestamp of the touch operation). The original input event is stored at the kernel layer. The application framework layer obtains the original input event from the kernel layer, and identifies a control corresponding to the input event. In an example in which the touch operation is a touch tap operation, and a control corresponding to the tap operation is a control of a camera application icon, the camera application invokes an interface of the application framework layer to start the camera application, then starts the camera driver by invoking the kernel layer, and captures a static image or a video through the camera <b>193</b>.</p><p id="p-0322" num="0321">The pose determining method provided in embodiments of this disclosure may be implemented with reference to the components disposed in the terminal <b>100</b>. For example, communication with the server may be implemented by using the components such as the antenna <b>1</b>, the antenna <b>2</b>, the mobile communication module <b>150</b>, and the wireless communication module <b>160</b>, for example, transmitting a to-be-queried image and N text fields, and receiving an initial pose returned by the server. The audio module <b>170</b>, the loudspeaker <b>170</b>A, and the headset jack <b>170</b>D may be used to broadcast some prompt information to the user through voice. Some prompt information for the user may be displayed by using the display <b>194</b>. A to-be-queried image, an environmental image, an initial image, and the like may be photographed by using the camera <b>193</b>. The gyroscope sensor <b>180</b>B may be used to assist in determining of the motion posture and the like of the terminal. A function of determining the initial pose by the terminal <b>100</b> may be implemented by using the components disposed in the terminal <b>100</b> and the method provided in embodiments of this disclosure. The foregoing is merely an example, and does not constitute a limitation.</p><p id="p-0323" num="0322">Another example embodiment of this disclosure provides a server <b>1300</b>.</p><p id="p-0324" num="0323">The server <b>1300</b> may include a processor <b>1310</b> and a transceiver <b>1320</b>. The transceiver <b>1320</b> may be connected to the processor <b>1310</b>, as shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>b</i></figref>. The transceiver <b>1320</b> may include a receiver and a transmitter, and may be configured to receive or send a message or data. The transceiver <b>1320</b> may be a network interface card. The server <b>1300</b> may further include an acceleration component (which may be referred to as an accelerator). When the acceleration component is a network acceleration component, the acceleration component may be a network interface card. The processor <b>1310</b> may be a control center of the server <b>1300</b>, and is connected to all parts of the entire server <b>1300</b>, such as the transceiver <b>1320</b>, through various interfaces and lines. In the present invention, the processor <b>1310</b> may be a central processing unit (CPU). Optionally, the processor <b>1310</b> may include one or more processing units. The processor <b>1310</b> may alternatively be a digital signal processor, an application-specific integrated circuit, a field programmable gate array, a GPU, another programmable logic device, or the like. The server <b>1300</b> may further include a memory <b>1330</b>. The memory <b>1330</b> may be configured to store a software program and a module. The processor <b>1310</b> reads software code and the module that are stored in the memory <b>1330</b>, to perform various function applications of the server <b>1300</b> and process data.</p><p id="p-0325" num="0324">An example embodiment of this disclosure provides a pose determining system. As shown in <figref idref="DRAWINGS">FIG. <b>2</b><i>c</i></figref>, the system may include a terminal device and a server. The terminal device may be a mobile terminal, a human-computer interaction device, or an in-vehicle visual perception device, for example, a mobile phone, a floor scanner, an intelligent robot, an unmanned vehicle, an intelligent monitor, or an augmented reality (AR) wearing device. Correspondingly, the method provided in embodiments of this disclosure may be applied to an application field such as human-computer interaction, in-vehicle visual perception, augmented reality, intelligent monitoring, unmanned driving, garage car seeking, or exit seeking.</p><p id="p-0326" num="0325">For ease of understanding, a pose determining method provided in embodiments of this disclosure is specifically described with reference to the accompanying drawings and an application scenario.</p><p id="p-0327" num="0326">Refer to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a pose determining method according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the pose determining method provided in this disclosure includes the following steps.</p><p id="p-0328" num="0327"><b>301</b>: Obtain a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line.</p><p id="p-0329" num="0328">In this embodiment of this disclosure, a terminal device may obtain the target image. The target image may include the target parking space in which a currently parked vehicle of a user is located.</p><p id="p-0330" num="0329">In a scenario, after completing parking, the user needs to search for an exit of a garage, an elevator entrance or a staircase entrance for entering a ground floor, or the like. Generally, the user may look at an exit sign indication in the garage. However, no obvious exit sign indication may be found near the parked vehicle. In this embodiment of this disclosure, the terminal device may guide the user to photograph the target image including the target parking space in which the currently parked vehicle is located, or guide the user to select, from an album, the target image including the target parking space in which the currently parked vehicle is located. Accordingly, the terminal device may obtain the target image.</p><p id="p-0331" num="0330">In a scenario, after entering a garage again, the user needs to search for the previously parked vehicle of the user. In this embodiment of this disclosure, the terminal device may guide the user to photograph a target image including any target parking space nearby, or guide the user to select, from an album, the target image including the target parking space in which the currently parked vehicle is located. Accordingly, the terminal device may obtain the target image.</p><p id="p-0332" num="0331">In an embodiment, the terminal device may display a target photographing interface, where the target photographing interface includes a photographing area, and the photographing area includes a preset guide contour. If a target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range, the target image is obtained through photographing.</p><p id="p-0333" num="0332">The target photographing interface further comprises a guide prompt. The guide prompt is used to indicate to move the target parking space in the photographing area to be within the preset guide contour or the position difference between the target parking space and the preset guide contour falls within the preset range.</p><p id="p-0334" num="0333">The target photographing interface further comprises a second guide prompt. The second guide prompt is used to indicate that an image in the photographing area does not meet a image condition until the image in the photographing area meets the image condition.</p><p id="p-0335" num="0334">The preferred image condition includes at least: A definition of the image in the photographing area is greater than or equal to a definition threshold.</p><p id="p-0336" num="0335">It should be noted that step <b>301</b> may be performed by the terminal device.</p><p id="p-0337" num="0336"><b>302</b>: Determine pose information based on the target parking space mark and the target parking space line, where the pose information indicates a corresponding pose of a terminal during photographing of the target image.</p><p id="p-0338" num="0337">In this embodiment of this disclosure, the terminal device may obtain a first pixel position of the target parking space line in the target image; obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates a position of the target parking space line in the digital map; and determine the pose information based on the first pixel position and the third position information.</p><p id="p-0339" num="0338">In this embodiment of this disclosure, the first pixel position may be independently determined by the terminal device, or may be determined through interaction between the terminal device and a server, that is, the server determines the first pixel position, and sends the first pixel position to the terminal device. The following separately describes the processes.</p><p id="p-0340" num="0339">1. The terminal device determines the first pixel position.</p><p id="p-0341" num="0340">In this embodiment of this disclosure, the terminal device may determine the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image.</p><p id="p-0342" num="0341">In this embodiment of this disclosure, after obtaining the target image, the terminal device may determine the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image.</p><p id="p-0343" num="0342">Optionally, the terminal device may recognize the target parking space mark included in the target image. For example, the terminal device may recognize the target parking space mark in the target image by using an optical character recognition (OCR) technology. In other words, the terminal device may determine, based on an image recognition algorithm, the target parking space mark included in the target image. Refer to <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>. <figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>is a schematic diagram of a target image according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>, the target image includes a target parking space mark <b>401</b>. The target parking space mark shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>is specifically &#x201c;551&#x201d;. A terminal device may learn, through recognition based on an image recognition algorithm, that the target parking space mark included in the current target image is &#x201c;551&#x201d;.</p><p id="p-0344" num="0343">It should be noted that the target parking space mark in this embodiment of this disclosure may be a string of digits that uniquely indicate a position of the target parking space, a character string including a text, or the like. This is not limited herein.</p><p id="p-0345" num="0344">Optionally, the terminal device may display a parking space mark input indication box on a display interface, and the user may input a parking space mark in the parking space mark input indication box. Optionally, the terminal device may further display indication information on the display interface, to indicate the user to input, in the parking space mark input indication box, the target parking space mark corresponding to the target parking space in the target image. The target parking space mark may be a target parking space mark of the currently parked vehicle of the user, or a target parking space mark of any nearby vehicle. Accordingly, the terminal device may obtain the target parking space mark input in the parking space mark input indication box.</p><p id="p-0346" num="0345">In this embodiment of this disclosure, the target parking space may include target parking space lines, a first corner point, and a target parking space mark. The first corner point is an intersection point between the target parking space lines of the target parking space. The terminal device may determine the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image.</p><p id="p-0347" num="0346">The first pixel position may include direction information of the target parking space line in the target image. Generally, a parking space line may be a boundary line of a printed line having a specific width on the ground. Parking space lines of a parking space are usually in two different directions. For details, refer to <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>, the target parking space usually includes a plurality of parking space lines, for example, a plurality of parking space lines <b>403</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>. Parking space lines <b>403</b> on left and right sides relative to a vehicle front direction are in a same direction in the target image, and parking space lines <b>403</b> on front and rear sides relative to the vehicle front direction are in a same direction in the target image. The target parking space line in this embodiment of this disclosure may be one or more of the plurality of parking space lines.</p><p id="p-0348" num="0347">It should be noted that the target parking space line in this embodiment may be an edge line of a parking space printed line or a center line of a parking space printed line. This is not limited herein. The following uses an example in which the target parking space line includes a first parking space line.</p><p id="p-0349" num="0348">In this embodiment of this disclosure, the terminal device may learn, through recognition, the first parking space line in the target image based on an image detection algorithm, and determine direction information of the first parking space line in the target image. The direction information may indicate a direction of the first parking space line in the target image. Optionally, the direction information of the first parking space line in the target image may be represented by using an included angle between the first parking space line and a direction of a lateral axis or a longitudinal axis in the target image, or the direction information of the first parking space line in the target image may be represented by using any two points on the first parking space line (the two points may determine a straight line). The any two points may be two points on the first parking space line, for example, endpoints or points in a line segment. It should be noted that because parking space lines of a parking space usually correspond to two directions in the target image, for details about determining direction information that includes one of the directions or determining direction information that includes the two directions, refer to description in the following embodiments. Details are not described herein.</p><p id="p-0350" num="0349">It should be noted that the foregoing method for representing the direction information of the first parking space line in the target image is merely an example. In actual application, another manner of representing the direction information of the first parking space line in the target image may be selected. This is not limited herein.</p><p id="p-0351" num="0350">It should be noted that the terminal device may determine direction information of at least one of the plurality of parking space lines in the target image. For specific description of the direction information of each of the at least one parking space line in the target image, refer to the description in the foregoing embodiment. Details are not described herein again.</p><p id="p-0352" num="0351">In this embodiment of this disclosure, the first pixel position may include a pixel position of the first corner point in the target image. A parking space line may usually be a boundary line of a printed line having a specific width on the ground, and a first corner point may be one of a plurality of intersection points between parking space lines. For details, refer to <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>, the target parking space usually includes a plurality of corner points, for example, a plurality of corner points <b>402</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>. The first corner point may be one of the plurality of corner points <b>402</b>.</p><p id="p-0353" num="0352">It should be noted that a corner point in this embodiment may be an intersection point of edge lines of a printed line of a parking space, or a center point of an intersection area of a printed line of a parking space (for example, a corner point <b>402</b> at the lower left corner in <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>). This is not limited herein.</p><p id="p-0354" num="0353">In this embodiment of this disclosure, in a scenario, a parking space is independent, that is, no other parking space shares a parking space line with this parking space. In this case, corner points may include inner corner points and outer corner points. The inner corner point may be understood as a corner point that is located on an inner side of the printed line of the parking space and that is close to the vehicle. The outer corner point may be understood as a corner point that is located on an outer side of the printed line of the parking space and that is far away from the vehicle. In a scenario, a parking space is non-independent, that is, shares a part of parking space lines with another parking space. In this case, a quantity of outer corner points that may be included in corner points is less than a quantity of outer corner points included in an independent parking space. Refer to <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>. <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>is a schematic diagram of a target image according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>, the target image includes a plurality of corner points. Different from <figref idref="DRAWINGS">FIG. <b>4</b><i>a</i></figref>, in <figref idref="DRAWINGS">FIG. <b>4</b><i>b</i></figref>, a quantity of outer corner points is less than that in <figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>because a part of outer corner points share a parking space printed line with another parking space.</p><p id="p-0355" num="0354">In this embodiment of this disclosure, the terminal device may learn, through recognition, a first corner point in the target image based on an image detection algorithm, and determine a pixel position of the first corner point in the target image. Optionally, the pixel position of the first corner point in the target image may be represented by using pixel coordinates, or the pixel position of the first corner point in the target image may be represented by using direction information of two parking space lines corresponding to the first corner point (an intersection point of the two parking space lines corresponding to the first corner point is the first corner point).</p><p id="p-0356" num="0355">It should be noted that the foregoing method for representing the pixel position of the first corner point in the target image is merely an example. In actual application, another manner of representing the pixel position of the first corner point in the target image may be selected. This is not limited herein.</p><p id="p-0357" num="0356">It should be noted that the terminal device may determine a pixel position of at least one of the plurality of corner points in the target image. For specific description of a pixel position of each of the at least one corner point in the target image, refer to the description in the foregoing embodiment. Details are not described herein again.</p><p id="p-0358" num="0357">It should be noted that the target parking spaces shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>4</b><i>b </i></figref>are merely examples. In actual application, a shape of the target parking space may be, but not limited to, as shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>c</i></figref>. Refer to <figref idref="DRAWINGS">FIG. <b>4</b><i>c</i></figref>. <figref idref="DRAWINGS">FIG. <b>4</b><i>c </i></figref>is a schematic diagram of a target parking space according to an embodiment of this disclosure. <figref idref="DRAWINGS">FIG. <b>4</b><i>c </i></figref>is a schematic diagram of shapes of four target parking spaces. The target parking space may be one of the plurality of parking spaces shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref><i>c. </i></p><p id="p-0359" num="0358">2. The server determines the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image.</p><p id="p-0360" num="0359">In this embodiment of this disclosure, after obtaining the target image, the terminal may send the target image to the server. Accordingly, the server may determine the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image. For details about how the server determines the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, refer to the description in the foregoing embodiment. Details are not described herein again.</p><p id="p-0361" num="0360">In this embodiment of this disclosure, the third position information may be independently determined by the terminal device, or may be implemented through interaction between the terminal device and the server, that is, the server determines third position information, and sends the third position information to the terminal device. The following separately describes the processes.</p><p id="p-0362" num="0361">1. The terminal device obtains the third position information corresponding to the target parking space line in a digital map.</p><p id="p-0363" num="0362">In an embodiment, the terminal device may obtain the target image, determine the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, and obtain the third position information corresponding to the target parking space line in the digital map. The third position information indicates a preset position of the target parking space line in the digital map.</p><p id="p-0364" num="0363">The terminal device may obtain, from the digital map, second position information corresponding to the target parking space mark; and obtain, from the second position information, the third position information corresponding to the target parking space line.</p><p id="p-0365" num="0364">In an embodiment, the terminal device may obtain the target image, and send the obtained target image to the server. The server may determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image; and send, to the terminal device, the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image. The terminal device may obtain the third position information corresponding to the target parking space line, where the third position information indicates preset position information of the target parking space in the digital map.</p><p id="p-0366" num="0365">In this embodiment of this disclosure, the terminal device may store a preset map. The map includes each parking space mark and third position information corresponding to each parking space mark. The third position information may represent position information of a target parking space line in the digital map. Optionally, the third position information represents a three-dimensional coordinate position of the target parking space in the digital map.</p><p id="p-0367" num="0366">The following describes how the terminal device obtains, based on the preset digital map, the third position information corresponding to the target parking space line.</p><p id="p-0368" num="0367">In this embodiment of this disclosure, the map may include preset position information that represents a parking space and that corresponds to each parking space mark. Specifically, the map may include a position of at least one corner point of a parking space, a preset direction of at least one parking space line, or a preset position corresponding to a circumscribed rectangular frame of a target parking space mark. The preset position corresponding to the circumscribed rectangular frame of the target parking space mark may include a preset position of at least one corner point of the circumscribed rectangular frame, or a preset direction of at least one edge line. It should be noted that the preset position and the preset direction may be represented by using coordinates of the Universal Transverse Mercator (UTM). This is not limited in this disclosure.</p><p id="p-0369" num="0368">In this embodiment of this disclosure, the map may include all position information corresponding to the target parking space mark (for example, including preset directions of a plurality of parking space lines and preset positions of a plurality of corner points). The first pixel position determined by the terminal device from the target image may include only direction information of a part of parking space lines of the target parking space, pixel positions of a part of corner points, and the like. Correspondingly, the third position information is position information that one-to-one correspond to parts of the first pixel position.</p><p id="p-0370" num="0369">In this embodiment of this disclosure, after determining the target parking space mark, the terminal device may determine the third position information based on the digital map. Specifically, when the first pixel position includes the direction information of the first parking space line in the target image, the third position information may include a preset direction corresponding to the first parking space line. Alternatively, when the first pixel position includes the pixel position of the first corner point in the target image, the third position information may include a preset position corresponding to the first corner point. Alternatively, when the first pixel position includes a pixel position of the circumscribed rectangular frame that is of the target parking space mark and that is in the target image, the third position information may include a preset position corresponding to the circumscribed rectangular frame of the target parking space mark. Refer to <figref idref="DRAWINGS">FIG. <b>6</b><i>a</i></figref>. <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>is a schematic diagram of a map according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>6</b><i>a</i></figref>, the map includes a plurality of corner points <b>601</b>, parking space lines <b>602</b>, corner points <b>603</b> of a circumscribed rectangular frame, and the like.</p><p id="p-0371" num="0370">Optionally, in this embodiment of this disclosure, the first pixel position may include at least direction information of a feature line <b>1</b> in the target image, direction information of a feature line <b>2</b> in the target image, and direction information of a feature line <b>3</b> in the target image. The third position information may include a preset direction of the feature line <b>1</b>, a preset direction of the feature line <b>2</b>, and a preset direction of the feature line <b>3</b>. The feature line <b>1</b>, the feature line <b>2</b>, or the feature line <b>3</b> may be a parking space line or an edge line of a circumscribed rectangular frame of the target parking space mark, and directions of at least two of the feature line <b>1</b>, the feature line <b>2</b>, or the feature line <b>3</b> in the target image are different.</p><p id="p-0372" num="0371">Optionally, the terminal device may further obtain a gravity direction of the terminal device when the target image is photographed. In this embodiment of this disclosure, the first pixel position may include pixel coordinates of a corner point <b>1</b> in the target image, pixel coordinates of a corner point <b>2</b> in the target image, and the gravity direction. The third position information may include a preset position of the corner point <b>1</b> and a preset position of the corner point <b>2</b>. The corner point <b>1</b> or the corner point <b>2</b> may be an intersection point of parking space lines or a corner point of a circumscribed rectangular frame of the target parking space mark.</p><p id="p-0373" num="0372">Optionally, in this embodiment of this disclosure, the first pixel position may include pixel coordinates of a corner point <b>1</b> in the target image, pixel coordinates of a corner point <b>2</b> in the target image, pixel coordinates of a corner point <b>3</b> in the target image, and pixel coordinates of a corner point <b>4</b> in the target image. The third position information may include a preset position of the corner point <b>1</b>, a preset position of the corner point <b>2</b>, a preset position of the corner point <b>3</b>, and a preset position of the corner point <b>4</b>. The corner point <b>1</b>, the corner point <b>2</b>, the corner point <b>3</b>, or the corner point <b>4</b> may be an intersection point of parking space lines or a corner point of a circumscribed rectangular frame of the target parking space mark.</p><p id="p-0374" num="0373">The following describes a method for constructing the foregoing preset digital map. A map construction process may be performed in, but not limited to, one of the following three manners:</p><p id="p-0375" num="0374">1. Manually measure position information of each parking space by using a distance and angle measurement instrument such as a total station.</p><p id="p-0376" num="0375">2. Because a building is constructed according to a pre-designed CAD construction drawing of the building, if a parking space has been planned in the CAD drawing, a parking space drawing expected in the present invention is directly obtained from the CAD construction drawing.</p><p id="p-0377" num="0376">3. A process of creating an image in a visual manner may be as follows: scanning a garage by using a camera (which may be used together with a laser lidar), to ensure that photos captured by the camera cover all parking spaces, and generating a 3D point cloud and poses of the photos in a point cloud coordinate system, where in this step, if there is a laser lidar, a result is directly generated; or if there is no laser lidar, a result is generated according to an SFM algorithm, and a depth map of each photo is generated according to an MVS (multiple-view stereo) algorithm; for each parking space, selecting an image that can clearly show the parking space, extracting a parking space mark of the parking space through OCR; manually marking, on the image, positions of inner corner points on left and right sides of parking space lines; and calculating coordinates, in the point cloud coordinate system, of inner focuses on the left and right sides of the parking space lines of the parking space based on the poses and the depth map of the images and the mark. A width of a parking space line is directly manually measured with a ruler. A length and a height of a parking space mark of this parking space are measured. Because the parking space mark is located in the middle of the parking space, coordinates of a parking space mark box can be calculated based on coordinates of the parking space line. In this way, a parking space mark of each parking space, coordinates of parking space lines, and coordinates of corner points of the parking space marks are obtained. After a parking space line map in the point cloud coordinate system is generated, the coordinate system is converted to a UTM coordinate system by using a differential GPS and marking control points.</p><p id="p-0378" num="0377">2. The server determines the third position information corresponding to the target parking space line in the digital map.</p><p id="p-0379" num="0378">In an embodiment, the terminal device may obtain the target image, determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, and send, to the server, the target parking space mark included in the target image and the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image. The server may obtain third position information corresponding to the target parking space line. The third position information indicates preset position information of the target parking space line in the digital map.</p><p id="p-0380" num="0379">In an embodiment, the terminal device may obtain the target image, and send the obtained target image to the server. The server may determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, and obtain the third position information corresponding to the target parking space line. The third position information indicates preset position information of the target parking space line in the digital map.</p><p id="p-0381" num="0380">For details about how the server obtains the third position information corresponding to the target parking space line, where the third position information indicates the preset position information of the target parking space line in the digital map, refer to the description in the foregoing embodiment. Details are not described herein again.</p><p id="p-0382" num="0381">In this embodiment of this disclosure, the step of determining the pose information based on the first pixel position and the third position information may be independently performed by the terminal device, or may be implemented through interaction between the terminal device and the server, that is, the server determines the pose information, and sends the pose information to the terminal device. The following separately describes the processes.</p><p id="p-0383" num="0382">1. The terminal device determines the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0384" num="0383">In an embodiment, the terminal device obtains the target image; obtains the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image; obtains the third position information corresponding to the target parking space line; and determines the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0385" num="0384">In an embodiment, the terminal device obtains the target image, obtains the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, and sends the target parking space mark included in the target image to the server. The server may obtain the third position information corresponding to the target parking space line, where the third position information indicates preset position information of the target parking space line in a digital map; and send the third position information corresponding to the target parking space line to the terminal device. The terminal device may determine the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0386" num="0385">In an embodiment, the terminal device obtains the target image, and sends the obtained target image to the server. The server may determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image; obtain the third position information corresponding to the target parking space line, where the third position information indicates preset position information of the target parking space line in a digital map; and send the first pixel position information and the third position information to the terminal device. The terminal device may determine the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0387" num="0386">In an embodiment, the terminal device obtains the target image, and sends the obtained target image to the server. The server may determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image; and send, to the terminal device, the first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image. The terminal device may obtain the third position information corresponding to the target parking space line, where the third position information indicates preset position information of the target parking space line in a digital map; and determine the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0388" num="0387">In this embodiment of this disclosure, after obtaining the first pixel position and the third position information, the terminal device may determine the pose information based on the first pixel position and the third position information. The target image is photographed by the terminal, and the pose information indicates the corresponding pose of the terminal during photographing of the target image.</p><p id="p-0389" num="0388">In an embodiment, the terminal device may determine a pose based on the first pixel position and the third position information according to a 2D-3D coordinate matching algorithm.</p><p id="p-0390" num="0389">In this embodiment of this disclosure, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map. Alternatively, the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0391" num="0390">Specifically, the first pixel position may include at least direction information of a feature line <b>1</b> in the target image, direction information of a feature line <b>2</b> in the target image, and direction information of a feature line <b>3</b> in the target image. The third position information may include a preset direction of the feature line <b>1</b>, a preset direction of the feature line <b>2</b>, and a preset direction of the feature line <b>3</b>. The terminal device may calculate the pose through 2D-3D matching of at least three detected lines (including both a horizontal line and a vertical line) according to a perspective-n-line pose calculation algorithm (PnL).</p><p id="p-0392" num="0391">Specifically, the first pixel position may include pixel coordinates of a corner point <b>1</b> in the target image, pixel coordinates of a corner point <b>2</b> in the target image, and a gravity direction. The third position information may include a preset position of the corner point <b>1</b> and a preset position of the corner point <b>2</b>. The terminal device may calculate the pose according to a perspective-2-point pose calculation algorithm (p2p) in combination with the gravity direction.</p><p id="p-0393" num="0392">Specifically, the first pixel position may include pixel coordinates of a corner point <b>1</b> in the target image, pixel coordinates of a corner point <b>2</b> in the target image, pixel coordinates of a corner point <b>3</b> in the target image, and pixel coordinates of a corner point <b>4</b> in the target image. The third position information may include a preset position of the corner point <b>1</b>, a preset position of the corner point <b>2</b>, a preset position of the corner point <b>3</b>, and a preset position of the corner point <b>4</b>. The terminal device may calculate the pose according to a perspective-n-point pose calculation algorithm (pnp).</p><p id="p-0394" num="0393">In this embodiment of this disclosure, the pose information may include a coordinate position in which the terminal device is located during photographing of the target image. The terminal device may perform route planning based on the coordinate position, to obtain a planned route. A start point or an end point of the planned route is the coordinate position.</p><p id="p-0395" num="0394">In this embodiment of this disclosure, the terminal device may display a two-dimensional navigation interface. The two-dimensional navigation interface includes the planned route.</p><p id="p-0396" num="0395">In this embodiment of this disclosure, the pose information may include a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image. The terminal device may display an AR navigation interface. The AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device. The navigation guidance is determined based on a coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0397" num="0396">2. The server determines the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0398" num="0397">In an embodiment, the terminal device obtains the target image; obtains the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image; obtains the third position information corresponding to the target parking space line, where the third position information indicates preset position information of the target parking space line in a digital map; and sends the first pixel position and the third position information to a server. The server may determine the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates the corresponding pose of the terminal during photographing of the target image.</p><p id="p-0399" num="0398">In an embodiment, the terminal device obtains the target image, obtains the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, and sends the target parking space mark included in the target image to the server. The server may obtain the third position information corresponding to the target parking space line, where the third position information indicates preset position information of the target parking space line in a digital map; and determine the pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal, and the pose information indicates the corresponding pose of the terminal during photographing of the target image.</p><p id="p-0400" num="0399">In an embodiment, the terminal device obtains the target image, and sends the obtained target image to the server. The server may determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image; obtain the third position information corresponding to the target parking space line; and determine the pose information based on the first pixel position and the third position information. The third position information indicates preset position information of the target parking space line in a digital map. The target image is photographed by the terminal, and the pose information indicates the corresponding pose of the terminal during photographing of the target image.</p><p id="p-0401" num="0400">In an embodiment, the terminal device obtains the target image, and sends the obtained target image to the server. The server may determine the target parking space mark included in the target image and a first pixel position that is of the target parking space line corresponding to the target parking space mark and that is in the target image, and send the target parking space mark included in the target image to the terminal device. The terminal device may obtain the third position information corresponding to the target parking space line, and send the third position information to a server. The third position information indicates preset position information of the target parking space line in the digital map. The server may determine the pose information based on the first pixel position and the third position information. The target image is photographed by the terminal, and the pose information indicates the corresponding pose of the terminal during photographing of the target image.</p><p id="p-0402" num="0401">For details about how the server determines the pose information based on the first pixel position and the third position information, refer to the description in the foregoing embodiment. Details are not described herein again.</p><p id="p-0403" num="0402">In this embodiment of this disclosure, after the terminal device obtains the pose information, or the server obtains the pose information and sends the pose information to the terminal device, the terminal device may further obtain a pose change of the terminal device, and determine a real-time pose based on the pose information and the obtained pose change of the terminal device.</p><p id="p-0404" num="0403">In this embodiment of this disclosure, the terminal device may use the obtained pose information as an initial pose, determine the pose change of the terminal device by using a simultaneous localization and mapping (slam) tracking technology, and determine the real-time pose based on the initial pose and the pose change of the terminal. The terminal device may perform processing such as navigation, route planning, and obstacle avoidance based on the real-time pose. For example, the terminal device performs route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position; and displays a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route; or displays an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, where the navigation guidance is determined based on a yaw angle, a pitch angle, and a roll angle of the terminal device.</p><p id="p-0405" num="0404">Optionally, in addition to performing processing such as navigation, route planning, and obstacle avoidance based on the real-time pose, after obtaining the pose information, the terminal device may further obtain a preview stream of a current scenario; determine, based on the pose information, preset media content included in a digital map corresponding to a scenario in the preview stream; and render the media content in the preview stream.</p><p id="p-0406" num="0405">In this implementation of this disclosure, if the terminal device is a mobile phone, an AR wearable device, or the like, a virtual scenario may be constructed based on the pose information. First, the terminal device may obtain the preview stream of the current scenario. For example, the user may photograph a preview stream of a current environment in a shopping mall. Then, the terminal device may determine the pose information as the initial pose according to the method mentioned above. Then, the terminal device may obtain a digital map. The digital map records three-dimensional coordinates of each position in the world coordinate system. Corresponding preset media content exists in a preset three-dimensional coordinate position. The terminal may determine, in the digital map, target three-dimensional coordinates corresponding to a real-time pose. If corresponding preset media content exists at the target three-dimensional coordinates, the terminal obtains the preset media content. For example, the user photographs a target shop, and the terminal recognizes a real-time pose and determines that a camera is currently photographing the target shop. In this case, preset media content corresponding to the target shop may be obtained. The preset media content corresponding to the target shop may be description information of the target shop, for example, specific commodities in the target shop that are worth purchasing. Based on this, the terminal may render the media content in the preview stream. In this case, the user may view, within a preset area near the image corresponding to the target shop in the mobile phone, the preset media content corresponding to the target shop. After viewing the preset media content corresponding to the target shop, the user may have a general understanding of the target shop.</p><p id="p-0407" num="0406">Different digital maps may be set for different places. In this way, when the user moves to another place, preset media content corresponding to a real-time pose may also be obtained based on a media content rendering manner provided in this embodiment of the present disclosure, and the media content is rendered in the preview stream.</p><p id="p-0408" num="0407">Embodiments of this disclosure provide the pose determining method, including: obtaining the target image, where the target image includes the target parking space mark and the target parking space line, and the target parking space corresponding to the target parking space mark includes the target parking space line; and determining the pose information based on the target parking space mark and the target parking space line, where the pose information indicates the corresponding pose of the terminal during photographing of the target image. In the foregoing manner, when positioning precision of a GPS signal is poor, the terminal device may determine the pose information based on the target parking space mark and the target parking space line, to implement positioning. In addition, compared with a manner in which positioning is performed based on a 3D point cloud, in embodiments, the map includes less data.</p><p id="p-0409" num="0408">Refer to <figref idref="DRAWINGS">FIG. <b>6</b><i>b</i></figref>. <figref idref="DRAWINGS">FIG. <b>6</b><i>b </i></figref>is a schematic diagram of a pose determining method according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>6</b><i>b</i></figref>, the pose determining method provided in this disclosure includes the following steps.</p><p id="p-0410" num="0409"><b>604</b>: Obtain a first pixel position of a target parking space line in a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line.</p><p id="p-0411" num="0410"><b>605</b>: Obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates a coordinate position of the target parking space line in the digital map, the digital map includes second position information corresponding to the target parking space mark, and the second position information includes the third position information.</p><p id="p-0412" num="0411"><b>606</b>: Determine pose information based on the first pixel position and the third position information, where the target image is photographed by the terminal device, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0413" num="0412">For description of step <b>604</b> to step <b>606</b>, refer to the description of the server side in step <b>301</b> and step <b>302</b> in the foregoing embodiment. Details are not described herein again.</p><p id="p-0414" num="0413">Optionally, a server may receive the target image sent by the terminal device, and determine the first pixel position of the target parking space line in the target image.</p><p id="p-0415" num="0414">Optionally, the server may receive the first pixel position that is of the target parking space line in the target image and that is sent by the terminal device.</p><p id="p-0416" num="0415">Optionally, the server may receive the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0417" num="0416">determine, from the digital map, the third position information corresponding to the target parking space line.</p><p id="p-0418" num="0417">Optionally, the server may receive the third position information that corresponds to the target parking space line in the digital map and that is sent by the terminal device.</p><p id="p-0419" num="0418">Optionally, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The server may determine the pose information based on the first pixel position and the third position information.</p><p id="p-0420" num="0419">The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or</p><p id="p-0421" num="0420">the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0422" num="0421">Optionally, the server may receive a gravity direction that is of the terminal device during photographing of the target image and that is sent by the terminal device;</p><p id="p-0423" num="0422">determine the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0424" num="0423">Optionally, the server may determine a 2D-3D correspondence between the first pixel position and the third position information, where the 2D-3D correspondence indicates a correspondence between two-dimensional coordinates of the target parking space line in the target image and three-dimensional coordinates of the target parking space line in actual space; and determine the pose information based on the 2D-3D correspondence.</p><p id="p-0425" num="0424">Refer to <figref idref="DRAWINGS">FIG. <b>7</b><i>a</i></figref>. <figref idref="DRAWINGS">FIG. <b>7</b><i>a </i></figref>is a schematic diagram of a pose determining method according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>7</b><i>a</i></figref>, the pose determining method provided in this disclosure includes the following steps.</p><p id="p-0426" num="0425"><b>801</b>: Obtain a target image, where the target image includes a target parking space mark.</p><p id="p-0427" num="0426">For description of step <b>801</b>, refer to the description of step <b>301</b> in the foregoing embodiment. Details are not described herein again.</p><p id="p-0428" num="0427"><b>802</b>: Determine pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image, where the target image is photographed by a terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0429" num="0428">In this embodiment of this disclosure, the circumscribed pattern may be a circumscribed rectangular frame, a circumscribed polygonal frame, or the like. For example, the circumscribed pattern is a circumscribed rectangular frame. The terminal device may obtain a first pixel position that is of the circumscribed rectangular frame of the target parking space mark and that is in the target image, obtain third position information corresponding to the circumscribed rectangular frame in a digital map, and determine the pose information based on the first pixel position and the third position information.</p><p id="p-0430" num="0429">In this embodiment of this disclosure, the first pixel position may include a pixel position of the circumscribed rectangular frame that is of the target parking space mark and that is in the target image. The target parking space mark is usually printed at the front end of the parking space, for example, the target parking space mark <b>401</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>4</b></figref><i>b. </i></p><p id="p-0431" num="0430">In this embodiment of this disclosure, the first pixel position may include a pixel position of a second corner point of a circumscribed rectangular frame that is of the target parking space mark and that is in the target image, or the first pixel position includes direction information of an edge line of a circumscribed rectangular frame that is of the target parking space mark and that is in the target image.</p><p id="p-0432" num="0431">In this embodiment of this disclosure, the terminal device may recognize a position of the target parking space mark in the target image, and obtain a circumscribed rectangular frame of the target parking number. The circumscribed rectangular frame is circumscribed at the periphery of the target parking space mark. Refer to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of the target parking space mark according to this embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a circumscribed rectangle of an area in which the target parking space mark is located may include four corner points <b>404</b>. The second corner point in this embodiment is one of the four corner points <b>404</b>. The circumscribed rectangle of the area in which the target parking space mark is located may include four edge lines <b>405</b>. The first edge line in this embodiment is one of the four edge lines <b>405</b>.</p><p id="p-0433" num="0432">In this embodiment of this disclosure, the third position information corresponding to the circumscribed rectangular frame may be obtained, where the third position information indicates preset position information of the circumscribed rectangular frame of the target parking space mark.</p><p id="p-0434" num="0433">In this embodiment of this disclosure, the third position information may include a preset position corresponding to the circumscribed rectangular frame of the target parking space mark. Refer to <figref idref="DRAWINGS">FIG. <b>6</b><i>a</i></figref>. <figref idref="DRAWINGS">FIG. <b>6</b><i>a </i></figref>is a schematic diagram of a map according to this embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>6</b><i>a</i></figref>, the map includes corner points <b>603</b> of a circumscribed rectangular frame, edge lines of the circumscribed rectangular frame, and the like.</p><p id="p-0435" num="0434">The pose information is determined based on the first pixel position and the third position information. The target image is photographed by the terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0436" num="0435">In this embodiment of this disclosure, a pose may be determined according to a 2D-3D coordinate matching algorithm based on the first pixel position and the third position information.</p><p id="p-0437" num="0436">Specifically, the first pixel position may include at least direction information of a feature line <b>1</b> in the target image, direction information of a feature line <b>2</b> in the target image, and direction information of a feature line <b>3</b> in the target image. The third position information may include a preset direction of the feature line <b>1</b>, a preset direction of the feature line <b>2</b>, and a preset direction of the feature line <b>3</b>. The terminal device may calculate the pose through 2D-3D matching of at least three detected lines (including both a horizontal line and a vertical line) according to a perspective-n-line pose calculation algorithm (PnL).</p><p id="p-0438" num="0437">Specifically, the first pixel position may include pixel coordinates of a corner point <b>1</b> in the target image, pixel coordinates of a corner point <b>2</b> in the target image, and a gravity direction. The third position information may include a preset position of the corner point <b>1</b> and a preset position of the corner point <b>2</b>. The terminal device may calculate the pose according to a perspective-2-point pose calculation algorithm (p2p) and a gravity direction algorithm.</p><p id="p-0439" num="0438">Specifically, the first pixel position may include pixel coordinates of a corner point <b>1</b> in the target image, pixel coordinates of a corner point <b>2</b> in the target image, pixel coordinates of a corner point <b>3</b> in the target image, and pixel coordinates of a corner point <b>4</b> in the target image. The third position information may include a preset position of the corner point <b>1</b>, a preset position of the corner point <b>2</b>, a preset position of the corner point <b>3</b>, and a preset position of the corner point <b>4</b>. The terminal device may calculate the pose according to a perspective-n-point pose calculation algorithm (pnp).</p><p id="p-0440" num="0439">It should be noted that the foregoing corner points are intersection points of circumscribed rectangular frames of the target parking space mark, and the feature lines are edge lines of the circumscribed rectangular frames of the target parking space mark.</p><p id="p-0441" num="0440">Optionally, the terminal device may display a parking space mark input indication box, and obtain the target parking space mark input in the parking space mark input indication box.</p><p id="p-0442" num="0441">Optionally, the pose information includes a coordinate position of the terminal device during photographing of the target image. The terminal device may perform route planning based on the coordinate position, to obtain a planned route, where a start point or an end point of the planned route is the coordinate position; and display a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0443" num="0442">Optionally, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image. The terminal device may display an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0444" num="0443">Optionally, the terminal device may obtain positioning information used when the terminal device photographs the target image. The obtaining third position information corresponding to the target parking space line in a digital map includes: obtaining, from the digital map, the third position information that corresponds to the circumscribed rectangular frame and that matches the positioning information.</p><p id="p-0445" num="0444">Optionally, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line.</p><p id="p-0446" num="0445">The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0447" num="0446">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0448" num="0447">Optionally, the terminal device may obtain a gravity direction of the terminal device during photographing of the target image, and determine the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0449" num="0448">Optionally, the terminal device may determine a 2D-3D correspondence between the first pixel position and the third position information, and determine the pose information based on the 2D-3D correspondence.</p><p id="p-0450" num="0449">In this embodiment of this disclosure, step <b>802</b> may be performed by the terminal device or step <b>802</b> may be performed through interaction between the terminal device and the server.</p><p id="p-0451" num="0450">In this embodiment of this disclosure, the terminal device may obtain the first pixel position, in the target image, of the circumscribed rectangular frame of the target parking space mark; obtain the third position information corresponding to the target parking space line in the digital map, where the third position information indicates the position information that is of the circumscribed rectangular frame of the target parking space mark and that is in the digital map; and determine the pose information based on the first pixel position and the third position information.</p><p id="p-0452" num="0451">In this embodiment of this disclosure, the terminal device may send the target image to the server, and receive the pose information determined by the server based on the target image. The pose information is determined by the server based on the first pixel position that is of the circumscribed rectangular frame of the target parking space mark and that is in the target image, and the third position information corresponding to the target parking space line in the digital map, and the third position information indicates position information that is of the circumscribed rectangular frame of the target parking space mark and that is in the digital map.</p><p id="p-0453" num="0452">In this embodiment of this disclosure, the terminal device may obtain the first pixel position that is of the circumscribed rectangular frame of the target parking space mark and that is in the target image, send the target parking space mark and the first pixel position to a server, and receive the pose information determined by the server based on the target parking space mark and the first pixel position. The pose information is determined by the server based on the first pixel position and third position information corresponding to the target parking space line in the digital map, and the third position information indicates position information that is of the circumscribed rectangular frame of the target parking space mark and that is in the digital map.</p><p id="p-0454" num="0453">In this embodiment of this disclosure, the terminal device may obtain the third position information corresponding to the target parking space line in the digital map, where the third position information indicates position information that is of the circumscribed rectangular frame of the target parking space mark and that is in the digital map; send the target image and the third position information to a server; and receive the pose information determined by the server based on the target image and the third position information, where the pose information is determined by the server based on the third position information and the first pixel position that is of the circumscribed rectangular frame of the target parking space mark and that is in the target image.</p><p id="p-0455" num="0454">In this embodiment of this disclosure, the terminal device may obtain the direction information of the first edge line and a pixel position, in the target image, of a second corner point of a circumscribed rectangular frame of the area in which the target parking space mark is located. For a specific manner of representing the pixel position of the second corner point and the direction information of the first edge line, refer to the description in the foregoing embodiment. Details are not described herein again.</p><p id="p-0456" num="0455">In a scenario, after completing parking, the user needs to search for an exit of a garage, an elevator entrance or a staircase entrance for entering a ground floor, or the like. Generally, the user may look at an exit sign indication in the garage. However, no obvious exit sign indication may be found near the parked vehicle. To resolve the foregoing problem, the following describes a pose determining method. Refer to <figref idref="DRAWINGS">FIG. <b>7</b><i>b</i></figref>. <figref idref="DRAWINGS">FIG. <b>7</b><i>b </i></figref>is a schematic flowchart of a pose determining method according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>7</b><i>b</i></figref>, the pose determining method provided in this embodiment includes the following steps.</p><p id="p-0457" num="0456"><b>701</b>: Receive a first navigation instruction, where the first navigation instruction is used to instruct to perform navigation to a preset coordinate position.</p><p id="p-0458" num="0457">In this embodiment of this disclosure, a map prestored in a terminal device may include a preset coordinate position of a garage exit. The preset coordinate position in this embodiment of this disclosure may be position coordinates of the exit. Refer to <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>to <figref idref="DRAWINGS">FIG. <b>8</b><i>d</i></figref>. <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>to <figref idref="DRAWINGS">FIG. <b>8</b><i>d </i></figref>are schematic diagrams of interfaces of a terminal device according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>a</i></figref>, the terminal device may display a control for instructing exit navigation. A user may tap the control. Accordingly, this is equivalent to that the terminal device receives the first navigation instruction, where the first navigation instruction is used to instruct to perform navigation to the preset coordinate position.</p><p id="p-0459" num="0458"><b>702</b>: Display a target photographing interface.</p><p id="p-0460" num="0459">In this embodiment of this disclosure, as shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>b</i></figref>, the target photographing interface may include a photographing area. As shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>c</i></figref>, the photographing area may further include a preset guide contour and a guide prompt (which may be but not limited to a prompt &#x201c;Please align with parking space lines&#x201d; shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>c</i></figref>). The guide prompt is used to indicate to move a target parking space in the photographing area to be within the preset guide contour or a position difference between the target parking space and the preset guide contour falls within a preset range. If a target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range, the target image is obtained through photographing.</p><p id="p-0461" num="0460"><b>703</b>: Perform a preset step on the target image to obtain pose information.</p><p id="p-0462" num="0461">The pose information includes a coordinate position of the terminal device during photographing of the target image, and a yaw angle, a pitch angle, a roll angle, and the like of the terminal device during photographing of the target image.</p><p id="p-0463" num="0462">For specific description of step <b>703</b>, refer to the description of step <b>301</b> to step <b>304</b> in the foregoing embodiment. Details are not described herein again.</p><p id="p-0464" num="0463"><b>704</b>: Perform route planning based on the pose information to obtain a planned route, where a start point of the planned route is the coordinate position, and an end point of the planned route is the preset coordinate position.</p><p id="p-0465" num="0464">In this embodiment of this disclosure, after coordinates of a current position of the terminal device are obtained, route planning may be performed based on the coordinates of the current position of the terminal device and the coordinate position of the exit position of the garage, to obtain a planned route.</p><p id="p-0466" num="0465"><b>705</b>: Display an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0467" num="0466">In this embodiment of this disclosure, the terminal device may display the AR navigation interface. The AR navigation interface includes the navigation guidance and the image of the environment in which the terminal device is currently located. Refer to the upper half interface in <figref idref="DRAWINGS">FIG. <b>8</b><i>d</i></figref>. The AR navigation interface shown on the upper half interface in <figref idref="DRAWINGS">FIG. <b>8</b><i>d </i></figref>includes the navigation guidance and the image of the environment in which the terminal device is currently located.</p><p id="p-0468" num="0467"><b>706</b>: Display a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0469" num="0468">In this embodiment of this disclosure, the terminal device may display the two-dimensional navigation interface. The two-dimensional navigation interface includes the planned route. Refer to the lower half interface in <figref idref="DRAWINGS">FIG. <b>8</b><i>d</i></figref>. The two-dimensional navigation interface shown on the lower half interface in <figref idref="DRAWINGS">FIG. <b>8</b><i>d </i></figref>includes the planned route.</p><p id="p-0470" num="0469">It should be noted that step <b>705</b> and step <b>706</b> may be simultaneously performed to obtain the interface including both the AR navigation interface and the two-dimensional navigation interface in <figref idref="DRAWINGS">FIG. <b>8</b><i>d</i></figref>, or a similar interface including both an AR navigation interface and a two-dimensional navigation interface. Only one of step <b>705</b> and step <b>706</b> may be performed. When step <b>705</b> is performed, the terminal device may display the AR navigation interface. When step <b>706</b> is performed, the terminal device may display the two-dimensional navigation interface.</p><p id="p-0471" num="0470">It should be noted that content included in the interfaces in <figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>to <figref idref="DRAWINGS">FIG. <b>8</b><i>d </i></figref>is merely an example, and is not limited in this disclosure.</p><p id="p-0472" num="0471">Refer to <figref idref="DRAWINGS">FIG. <b>8</b><i>e</i></figref>. <figref idref="DRAWINGS">FIG. <b>8</b><i>e </i></figref>is a schematic diagram of a pose determining method according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>8</b><i>e</i></figref>, the pose determining method provided in this disclosure includes the following steps.</p><p id="p-0473" num="0472"><b>803</b>: Obtain a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image, where the target image includes the target parking space mark.</p><p id="p-0474" num="0473"><b>804</b>: Obtain third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map, the digital map includes second position information corresponding to the target parking space mark, and the second position information includes the third position information.</p><p id="p-0475" num="0474"><b>805</b>: Determine pose information based on the first pixel position and the third position information.</p><p id="p-0476" num="0475">Optionally, the server is further configured to: receive the target image sent by the terminal device; and determine a first pixel position that is of a circumscribed rectangular frame of the target parking space mark and that is in the target image.</p><p id="p-0477" num="0476">Optionally, the server is further configured to receive the first pixel position that is of the circumscribed rectangular frame of the target parking space mark and is in the target image, and that is sent by the terminal device.</p><p id="p-0478" num="0477">Optionally, the server is further configured to receive the target image or the target parking space mark sent by the terminal device; and determine, from the digital map, the second position information corresponding to the target parking space mark.</p><p id="p-0479" num="0478">Optionally, the server is further configured to receive the third position information that corresponds to the target parking space line in the digital map and that is sent by the terminal device.</p><p id="p-0480" num="0479">Optionally, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line. The server is further configured to:</p><p id="p-0481" num="0480">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0482" num="0481">The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0483" num="0482">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0484" num="0483">Optionally, the server is further configured to: obtain a gravity direction of the terminal device during photographing of the target image, and determine the pose information based on the first pixel position, the second position information, and the gravity direction.</p><p id="p-0485" num="0484">Optionally, the server is further configured to determine a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0486" num="0485">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0487" num="0486">In a scenario, after entering a garage again, the user needs to search for a previously parked vehicle. To resolve the foregoing problem, the following describes a pose determining method. Refer to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic flowchart of a pose determining method according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the pose determining method provided in this embodiment includes the following steps.</p><p id="p-0488" num="0487"><b>901</b>: Receive a first navigation instruction, where the first navigation instruction is used to instruct to perform navigation to a preset coordinate position.</p><p id="p-0489" num="0488">In this embodiment of this disclosure, the preset coordinate position may be a parking position of a user. Refer to <figref idref="DRAWINGS">FIG. <b>10</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>10</b><i>b</i></figref>. <figref idref="DRAWINGS">FIG. <b>10</b><i>a </i></figref>and <figref idref="DRAWINGS">FIG. <b>10</b><i>b </i></figref>are schematic diagrams of interfaces of a terminal device according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>10</b><i>a</i></figref>, the terminal device may display a control for instructing parking space navigation. A user may tap the control. Accordingly, this is equivalent to that the terminal device receives the first navigation instruction, where the first navigation instruction is used to instruct to perform navigation to the preset coordinate position.</p><p id="p-0490" num="0489"><b>902</b>: Display a target photographing interface.</p><p id="p-0491" num="0490">In this embodiment of this disclosure, as shown in <figref idref="DRAWINGS">FIG. <b>10</b><i>b</i></figref>, the target photographing interface may include a photographing area. For specific description of step <b>902</b>, refer to the description of step <b>702</b> in the foregoing embodiment. Details are not described herein again.</p><p id="p-0492" num="0491"><b>903</b>: Perform a preset step on the target image to obtain first pose information.</p><p id="p-0493" num="0492">The first pose information includes a coordinate position of the terminal device during photographing of the target image, and a yaw angle, a pitch angle, a roll angle, and the like of the terminal device during photographing of the target image.</p><p id="p-0494" num="0493">For specific description of step <b>903</b>, refer to the description of step <b>301</b> to step <b>304</b> in the foregoing embodiment. Details are not described herein again.</p><p id="p-0495" num="0494"><b>904</b>: Perform route planning based on the pose information to obtain a planned route, where a start point of the planned route is the coordinate position, and an end point of the planned route is the preset coordinate position.</p><p id="p-0496" num="0495">In this embodiment of this disclosure, after a vehicle parks, the terminal device may obtain, but not limited to, pose information of the terminal device during parking based on step <b>301</b> to step <b>304</b>. After coordinates of a current position of the terminal device are obtained, route planning may be performed based on the coordinates of the current position of the terminal device and the pose information of the terminal device during parking, to obtain a planned route.</p><p id="p-0497" num="0496"><b>905</b>: Display an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0498" num="0497"><b>906</b>: Display a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0499" num="0498">For specific description of step <b>905</b> and step <b>906</b>, refer to the description of step <b>705</b> and step <b>706</b> in the foregoing embodiment. Details are not described herein again.</p><p id="p-0500" num="0499">This disclosure further provides a pose determining apparatus. The pose determining apparatus may be a terminal device. Refer to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram of a structure of a pose determining apparatus according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the pose determining apparatus includes:</p><p id="p-0501" num="0500">an obtaining module <b>1101</b>, configured to obtain a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and</p><p id="p-0502" num="0501">a determining module <b>1102</b>, configured to determine pose information based on the target parking space mark and the target parking space line, where the target image is photographed by a terminal device, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0503" num="0502">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0504" num="0503">obtain a first pixel position of the target parking space line in the target image;</p><p id="p-0505" num="0504">obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates a coordinate position of the target parking space line in the digital map, the digital map includes second position information corresponding to the target parking space mark, and the second position information includes the third position information; and</p><p id="p-0506" num="0505">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0507" num="0506">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0508" num="0507">send the target image to a server; and</p><p id="p-0509" num="0508">receive the pose information sent by the server, where the pose information is determined by the server based on a first pixel position of the target parking space line in the target image and third position information corresponding to the target parking space line in a digital map.</p><p id="p-0510" num="0509">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0511" num="0510">obtain a first pixel position of the target parking space line in the target image;</p><p id="p-0512" num="0511">send the target parking space mark and the first pixel position to a server; and</p><p id="p-0513" num="0512">receive the pose information sent by the server, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the target parking space line in a digital map.</p><p id="p-0514" num="0513">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0515" num="0514">obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates position information of the target parking space line in the digital map;</p><p id="p-0516" num="0515">send the target image and the third position information to a server; and</p><p id="p-0517" num="0516">receive the pose information sent by the server, where the pose information is determined by the server based on the third position information and a first pixel position of the target parking space line in the target image.</p><p id="p-0518" num="0517">Optionally, the obtaining module <b>1101</b> is specifically configured to:</p><p id="p-0519" num="0518">display a target photographing interface, where the target photographing interface includes a photographing area, and the photographing area includes a preset guide contour; and</p><p id="p-0520" num="0519">obtain, through photographing, the target image if the target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range.</p><p id="p-0521" num="0520">The determining module is specifically configured to:</p><p id="p-0522" num="0521">obtain a pixel position of the preset guide contour in the target image.</p><p id="p-0523" num="0522">Optionally, the apparatus further includes:</p><p id="p-0524" num="0523">a display module <b>1103</b>, configured to display a parking space mark input indication box.</p><p id="p-0525" num="0524">The obtaining module <b>1101</b> is further configured to obtain the target parking space mark input in the parking space mark input indication box.</p><p id="p-0526" num="0525">Optionally, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0527" num="0526">The apparatus further includes:</p><p id="p-0528" num="0527">a route planning module, configured to perform route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position.</p><p id="p-0529" num="0528">The display module <b>1103</b> is further configured to display a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0530" num="0529">Optionally, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image.</p><p id="p-0531" num="0530">The display module <b>1103</b> is further configured to display an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0532" num="0531">Optionally, the determining module is specifically configured to:</p><p id="p-0533" num="0532">obtain positioning information of the terminal device during photographing of the target image; and</p><p id="p-0534" num="0533">obtain, from the digital map, the third position information that matches the positioning information and that corresponds to the target parking space line in the digital map.</p><p id="p-0535" num="0534">Optionally, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0536" num="0535">Optionally, the determining module is specifically configured to:</p><p id="p-0537" num="0536">obtain a gravity direction of the terminal device during photographing of the target image; and</p><p id="p-0538" num="0537">determine the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0539" num="0538">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0540" num="0539">determine a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0541" num="0540">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0542" num="0541">This disclosure further provides a pose determining apparatus. The pose determining apparatus may be a terminal device. Refer to <figref idref="DRAWINGS">FIG. <b>11</b></figref>. <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram of a structure of a pose determining apparatus according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the pose determining apparatus includes:</p><p id="p-0543" num="0542">an obtaining module <b>1101</b>, configured to obtain a target image, where the target image includes a target parking space mark; and</p><p id="p-0544" num="0543">a determining module <b>1102</b>, configured to determine pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image, where the target image is photographed by a terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0545" num="0544">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0546" num="0545">obtain the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0547" num="0546">obtain third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0548" num="0547">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0549" num="0548">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0550" num="0549">send the target image to a server; and</p><p id="p-0551" num="0550">receive the pose information determined by the server based on the target image, where the pose information is determined by the server based on a first pixel position that is of a circumscribed rectangular frame of the target parking space mark and that is in the target image, and third position information corresponding to the target parking space line in the digital map; and the third position information indicates position information that is of the circumscribed rectangular frame of the target parking space mark and that is in the digital map.</p><p id="p-0552" num="0551">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0553" num="0552">obtain the first pixel position that is of the circumscribed rectangular frame of the target parking space mark and that is in the target image;</p><p id="p-0554" num="0553">send the target parking space mark and the first pixel position to a server; and</p><p id="p-0555" num="0554">receive the pose information that is determined by the server based on the target parking space mark and the first pixel position, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the target parking space line in a digital map.</p><p id="p-0556" num="0555">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0557" num="0556">obtain third position information corresponding to the target parking space line in a digital map;</p><p id="p-0558" num="0557">send the target image and the third position information to a server; and</p><p id="p-0559" num="0558">receive the pose information determined by the server based on the target image and the third position information, where the pose information is determined by the server based on the third position information and the first pixel position that is of the circumscribed rectangular frame of the target parking space mark and that is in the target image.</p><p id="p-0560" num="0559">Optionally, the apparatus further includes:</p><p id="p-0561" num="0560">a display module <b>1103</b>, configured to display a parking space mark input indication box.</p><p id="p-0562" num="0561">The obtaining module <b>1101</b> is further configured to obtain the target parking space mark input in the parking space mark input indication box.</p><p id="p-0563" num="0562">Optionally, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0564" num="0563">The apparatus further includes:</p><p id="p-0565" num="0564">a route planning module <b>1104</b>, configured to perform route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position;</p><p id="p-0566" num="0565">The display module <b>1103</b> is further configured to display a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0567" num="0566">Optionally, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image.</p><p id="p-0568" num="0567">The display module <b>1103</b> is further configured to display an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0569" num="0568">Optionally, the obtaining module <b>1101</b> is further configured to:</p><p id="p-0570" num="0569">obtain positioning information of the terminal device during photographing of the target image; and the obtaining third position information corresponding to the target parking space line in a digital map includes: obtaining, from the digital map, third position information that corresponds to the target parking space mark and that matches the positioning information.</p><p id="p-0571" num="0570">Optionally, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in a digital map, and a second corner point is an intersection point of the first edge line and the second edge line. The determining module is specifically configured to:</p><p id="p-0572" num="0571">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0573" num="0572">The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0574" num="0573">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0575" num="0574">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0576" num="0575">obtain a gravity direction of the terminal device during photographing of the target image; and determine the pose information based on the first pixel position, the second position information, and the gravity direction.</p><p id="p-0577" num="0576">Optionally, the determining module <b>1102</b> is specifically configured to:</p><p id="p-0578" num="0577">determine a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0579" num="0578">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0580" num="0579">This disclosure further provides a pose determining apparatus. The pose determining apparatus may be a server. Refer to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of a structure of a pose determining apparatus according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the pose determining apparatus includes:</p><p id="p-0581" num="0580">an obtaining module <b>1201</b>, configured to obtain a first pixel position of a target parking space line in a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and</p><p id="p-0582" num="0581">obtain third position information corresponding to the target parking space line in a digital map, where the third position information indicates a coordinate position of the target parking space line in the digital map; and</p><p id="p-0583" num="0582">a determining module <b>1202</b>, configured to determine pose information based on the first pixel position and the third position information, where the target image is photographed by a terminal device, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0584" num="0583">Optionally, the obtaining module <b>1201</b> is specifically configured to:</p><p id="p-0585" num="0584">receive the target image sent by the terminal device; and</p><p id="p-0586" num="0585">determine the first pixel position of the target parking space line in the target image.</p><p id="p-0587" num="0586">Optionally, the obtaining module <b>1201</b> is specifically configured to:</p><p id="p-0588" num="0587">receive the first pixel position that is of the target parking space line in the target image and that is sent by the terminal device.</p><p id="p-0589" num="0588">Optionally, the obtaining module <b>1201</b> is specifically configured to:</p><p id="p-0590" num="0589">receive the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0591" num="0590">determine, from the digital map, the third position information corresponding to the target parking space line.</p><p id="p-0592" num="0591">Optionally, the obtaining module <b>1201</b> is specifically configured to:</p><p id="p-0593" num="0592">receive the third position information that corresponds to the target parking space line in the digital map and that is sent by the terminal device.</p><p id="p-0594" num="0593">Optionally, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line. The determining module is specifically configured to:</p><p id="p-0595" num="0594">determine the pose information based on the first pixel position and the third position information.</p><p id="p-0596" num="0595">The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0597" num="0596">Optionally, the obtaining module <b>1201</b> is further configured to receive a gravity direction that is of the terminal device during photographing of the target image and that is sent by the terminal device. The determining module <b>1202</b> is specifically configured to determine the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0598" num="0597">Optionally, the determining module <b>1202</b> is specifically configured to:</p><p id="p-0599" num="0598">determine a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0600" num="0599">determine the pose information based on the 2D-3D correspondence.</p><p id="p-0601" num="0600">This disclosure further provides a pose determining apparatus. The pose determining apparatus may be a server. Refer to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of a structure of a pose determining apparatus according to an embodiment of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the pose determining apparatus includes:</p><p id="p-0602" num="0601">an obtaining module <b>1201</b>, configured to: obtain a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image, where the target image includes the target parking space mark; and obtain third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0603" num="0602">a determining module <b>1202</b>, configured to determine pose information based on the first pixel position and the third position information.</p><p id="p-0604" num="0603">Optionally, the obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image includes:</p><p id="p-0605" num="0604">receiving the target image sent by a terminal device; and</p><p id="p-0606" num="0605">determining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image.</p><p id="p-0607" num="0606">Optionally, the obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image includes:</p><p id="p-0608" num="0607">receiving the first pixel position that is of the circumscribed pattern of the target parking space mark, that is in the target image, and that is sent by the terminal device.</p><p id="p-0609" num="0608">Optionally, the obtaining third position information corresponding to the circumscribed pattern in a digital map includes:</p><p id="p-0610" num="0609">receiving the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0611" num="0610">determining, from the digital map, third position information corresponding to the circumscribed pattern in the digital map.</p><p id="p-0612" num="0611">Optionally, the obtaining third position information corresponding to the circumscribed pattern in a digital map includes:</p><p id="p-0613" num="0612">receiving the third position information that corresponds to the circumscribed pattern in the digital map and that is sent by the terminal device.</p><p id="p-0614" num="0613">Optionally, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line.</p><p id="p-0615" num="0614">The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0616" num="0615">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0617" num="0616">Optionally, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0618" num="0617">determining a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0619" num="0618">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0620" num="0619">Optionally, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0621" num="0620">The following describes a terminal device provided in an embodiment of this disclosure. The terminal device may be the pose determining apparatus in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. Refer to <figref idref="DRAWINGS">FIG. <b>13</b></figref>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is a schematic diagram of a structure of a terminal device according to an embodiment of this disclosure. The terminal device <b>1300</b> may be specifically a virtual reality VR device, a mobile phone, a tablet computer, a notebook computer, an intelligent wearable device, or the like. This is not limited herein. Specifically, the terminal device <b>1300</b> includes a receiver <b>1301</b>, a transmitter <b>1302</b>, a processor <b>1303</b>, and a memory <b>1304</b> (there may be one or more processors <b>1303</b> in the terminal device <b>1300</b>, and one processor is used as an example in <figref idref="DRAWINGS">FIG. <b>13</b></figref>.) The processor <b>1303</b> may include an application processor <b>13031</b> and a communication processor <b>13032</b>. In some embodiments of this disclosure, the receiver <b>1301</b>, the transmitter <b>1302</b>, the processor <b>1303</b>, and the memory <b>1304</b> may be connected through a bus or in another manner.</p><p id="p-0622" num="0621">The memory <b>1304</b> may include a read-only memory and a random access memory, and provide instructions and data for the processor <b>1303</b>. A part of the memory <b>1304</b> may further include a non-volatile random access memory (NVRAM). The memory <b>1304</b> stores a processor and operation instructions, an executable module or a data structure, a subnet thereof, or an extended set thereof. The operation instructions may include various operation instructions to implement various operations.</p><p id="p-0623" num="0622">The processor <b>1303</b> controls an operation of the terminal device. In a specific application, components of the terminal device are coupled together by using a bus system. In addition to a data bus, the bus system may further include a power bus, a control bus, a status signal bus, and the like. However, for clear description, various types of buses in the figure are marked as the bus system.</p><p id="p-0624" num="0623">The method disclosed in the foregoing embodiments of this disclosure may be applied to the processor <b>1303</b>, or may be implemented by the processor <b>1303</b>. The processor <b>1303</b> may be an integrated circuit chip, and has a signal processing capability. In an implementation process, the steps in the foregoing methods may be implemented by using a hardware integrated logical circuit in the processor <b>1303</b>, or by using instructions in a form of software. The processor <b>1303</b> may be a general-purpose processor, a digital signal processor (DSP), a microprocessor, or a microcontroller; or may include an application-specific integrated circuit (ASIC), a field-programmable gate array (FPGA) or another programmable logic device, a discrete gate or transistor logic device, or a discrete hardware component. The processor <b>1303</b> may implement or perform the methods, steps, and logic block diagrams disclosed in embodiments of this disclosure. The general-purpose processor may be a microprocessor, or the processor may be any conventional processor or the like. The steps in the methods disclosed with reference to embodiments of this disclosure may be directly executed and accomplished by using a hardware decoding processor, or may be executed and accomplished by using a combination of hardware and software modules in a decoding processor. The software module may be located in a storage medium mature in the art, such as a random access memory, a flash memory, a read-only memory, a programmable read-only memory, an electrically erasable programmable memory, or a register. The storage medium is located in the memory <b>1304</b>, and the processor <b>1303</b> reads information in the memory <b>1304</b> and completes the steps in the foregoing methods in combination with hardware in the processor <b>1303</b>.</p><p id="p-0625" num="0624">The receiver <b>1301</b> may be configured to receive input digital or character information, and generate signal input related to a related setting and function control of the terminal device. The transmitter <b>1302</b> may be configured to output digital or character information through a first interface. The transmitter <b>1302</b> may be further configured to send instructions to a disk group through the first interface, to modify data in the disk group. The transmitter <b>1302</b> may further include a display device such as a display.</p><p id="p-0626" num="0625">In this embodiment of this disclosure, in one case, the processor <b>1303</b> is configured to:</p><p id="p-0627" num="0626">obtain a target image, where the target image includes a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark includes the target parking space line; and</p><p id="p-0628" num="0627">determine pose information based on the target parking space mark and the target parking space line, where the target image is photographed by the terminal device, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0629" num="0628">Optionally, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0630" num="0629">obtaining a first pixel position of the target parking space line in the target image;</p><p id="p-0631" num="0630">obtaining third position information corresponding to the target parking space line in a digital map, where the third position information indicates a coordinate position of the target parking space line in the digital map; and</p><p id="p-0632" num="0631">determining the pose information based on the first pixel position and the third position information.</p><p id="p-0633" num="0632">Optionally, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0634" num="0633">sending the target image to a server; and</p><p id="p-0635" num="0634">receiving the pose information sent by the server, where the pose information is determined by the server based on a first pixel position of the target parking space line in the target image and third position information corresponding to the target parking space line in a digital map.</p><p id="p-0636" num="0635">Optionally, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0637" num="0636">obtaining a first pixel position of the target parking space line in the target image;</p><p id="p-0638" num="0637">sending the target parking space mark and the first pixel position to a server; and</p><p id="p-0639" num="0638">receiving the pose information sent by the server, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the target parking space line in a digital map.</p><p id="p-0640" num="0639">Optionally, the determining pose information based on the target parking space mark and the target parking space line includes:</p><p id="p-0641" num="0640">obtaining third position information corresponding to the target parking space line in a digital map, where the third position information indicates position information of the target parking space line in the digital map;</p><p id="p-0642" num="0641">sending the target image and the third position information to a server; and</p><p id="p-0643" num="0642">receiving the pose information sent by the server, where the pose information is determined by the server based on the third position information and a first pixel position of the target parking space line in the target image.</p><p id="p-0644" num="0643">Optionally, the obtaining a target image includes:</p><p id="p-0645" num="0644">displaying a target photographing interface, where the target photographing interface includes a photographing area, and the photographing area includes a preset guide contour; and</p><p id="p-0646" num="0645">obtaining, through photographing, the target image if the target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range.</p><p id="p-0647" num="0646">The obtaining a first pixel position of the target parking space line in the target image includes: obtaining a pixel position of the preset guide contour in the target image.</p><p id="p-0648" num="0647">Optionally, before the determining pose information based on the target parking space mark and the target parking space line, the method further includes:</p><p id="p-0649" num="0648">displaying a parking space mark input indication box; and</p><p id="p-0650" num="0649">obtaining the target parking space mark input in the parking space mark input indication box.</p><p id="p-0651" num="0650">Optionally, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0652" num="0651">The method further includes:</p><p id="p-0653" num="0652">performing route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position; and</p><p id="p-0654" num="0653">displaying a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0655" num="0654">Optionally, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image. The method further includes:</p><p id="p-0656" num="0655">displaying an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0657" num="0656">Optionally, the obtaining third position information corresponding to the target parking space line in a digital map includes:</p><p id="p-0658" num="0657">obtaining positioning information of the terminal device during photographing of the target image; and</p><p id="p-0659" num="0658">obtaining, from the digital map, the third position information that matches the positioning information and that corresponds to the target parking space line in the digital map.</p><p id="p-0660" num="0659">Optionally, the target parking space line includes a first parking space line, a second parking space line, and a third parking space line. At least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line.</p><p id="p-0661" num="0660">The first pixel position includes direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the third position information includes preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or</p><p id="p-0662" num="0661">the first pixel position includes direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the third position information includes preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</p><p id="p-0663" num="0662">Optionally, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0664" num="0663">obtaining a gravity direction of the terminal device during photographing of the target image; and</p><p id="p-0665" num="0664">determining the pose information based on the first pixel position, the third position information, and the gravity direction.</p><p id="p-0666" num="0665">Optionally, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0667" num="0666">determining a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0668" num="0667">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0669" num="0668">The terminal device may alternatively perform the following steps:</p><p id="p-0670" num="0669">obtaining a target image, where the target image includes a target parking space mark; and</p><p id="p-0671" num="0670">determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image, where the target image is photographed by a terminal, and the pose information indicates a corresponding pose of the terminal during photographing of the target image.</p><p id="p-0672" num="0671">Optionally, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0673" num="0672">obtaining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0674" num="0673">obtaining third position information corresponding to the target parking space line in a digital map, where the third position information indicates position information that is of the circumscribed pattern of the target parking space mark and that is in the digital map; and</p><p id="p-0675" num="0674">determining the pose information based on the first pixel position and the third position information.</p><p id="p-0676" num="0675">Optionally, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0677" num="0676">sending the target image to a server; and</p><p id="p-0678" num="0677">receiving the pose information sent by the server, where the pose information is determined by the server based on the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image, and third position information corresponding to the target circumscribed pattern in a digital map; and the third position information indicates position information that is of the circumscribed pattern of the target parking space mark and that is in the digital map.</p><p id="p-0679" num="0678">Optionally, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0680" num="0679">obtaining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image;</p><p id="p-0681" num="0680">sending the target parking space mark and the first pixel position to a server; and</p><p id="p-0682" num="0681">receiving the pose information sent by the server, where the pose information is determined by the server based on the first pixel position and third position information corresponding to the circumscribed pattern in the digital map.</p><p id="p-0683" num="0682">Optionally, the determining pose information based on the target parking space mark and a first pixel position that is of a circumscribed pattern of the target parking space mark and that is in the target image includes:</p><p id="p-0684" num="0683">obtaining third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates position information that is of the circumscribed pattern of the target parking space mark and that is in the digital map;</p><p id="p-0685" num="0684">sending the target image and the third position information to a server; and</p><p id="p-0686" num="0685">receiving the pose information sent by the server, where the pose information is determined by the server based on the second position information and the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image.</p><p id="p-0687" num="0686">Optionally, the method further includes:</p><p id="p-0688" num="0687">displaying a parking space mark input indication box; and</p><p id="p-0689" num="0688">obtaining the target parking space mark input in the parking space mark input indication box.</p><p id="p-0690" num="0689">Optionally, the pose information includes a coordinate position of the terminal device during photographing of the target image.</p><p id="p-0691" num="0690">The method further includes:</p><p id="p-0692" num="0691">performing route planning based on the coordinate position to obtain a planned route, where a start point or an end point of the planned route is the coordinate position; and</p><p id="p-0693" num="0692">displaying a two-dimensional navigation interface, where the two-dimensional navigation interface includes the planned route.</p><p id="p-0694" num="0693">Optionally, the pose information includes a coordinate position, a yaw angle, a pitch angle, and a roll angle of the terminal device during photographing of the target image. The method further includes:</p><p id="p-0695" num="0694">displaying an AR navigation interface, where the AR navigation interface includes a navigation guidance and an image of an environment including the current terminal device, and the navigation guidance is determined based on the coordinate position, the yaw angle, the pitch angle, and the roll angle of the terminal device.</p><p id="p-0696" num="0695">Optionally, the method further includes:</p><p id="p-0697" num="0696">obtaining positioning information of the terminal device during photographing of the target image; and the obtaining third position information corresponding to the circumscribed pattern in a digital map includes: obtaining, from the digital map, the third position information that corresponds to the circumscribed pattern matching the positioning information.</p><p id="p-0698" num="0697">Optionally, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line.</p><p id="p-0699" num="0698">The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0700" num="0699">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0701" num="0700">Optionally, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0702" num="0701">determining a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0703" num="0702">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0704" num="0703">Optionally, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0705" num="0704">An embodiment of this disclosure further provides a server. Refer to <figref idref="DRAWINGS">FIG. <b>14</b></figref>. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is a schematic diagram of a structure of a server according to an embodiment of this disclosure. Specifically, the server <b>1400</b> may vary greatly due to different configurations or performance, and may include one or more central processing units (CPU) <b>1414</b> (for example, one or more processors) and a memory <b>1432</b>, and one or more storage media <b>1430</b> (for example, one or more mass storage devices) that stores an application <b>1442</b> or data <b>1444</b>. The memory <b>1432</b> and the storage medium <b>1430</b> may be transient storage or persistent storage. A program stored in the storage medium <b>1430</b> may include one or more modules (not shown in the figure), and each module may operate a series of instructions of the server. Further, the central processing unit <b>1414</b> may be set to communicate with the storage medium <b>1430</b>, and executes, on the server <b>1400</b>, a series of instruction operations in the storage medium <b>1430</b>.</p><p id="p-0706" num="0705">The server <b>1400</b> may further include one or more power supplies <b>1426</b>, one or more wired or wireless network interfaces <b>1450</b>, one or more input/output interfaces <b>1458</b>; and/or one or more operating systems <b>1441</b>, for example, Windows Server&#x2122;, Mac OS X&#x2122;, Unix&#x2122;, Linux&#x2122;, and FreeBSD&#x2122;.</p><p id="p-0707" num="0706">In this embodiment of this disclosure, the central processing unit <b>1414</b> is configured to obtain a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image, where the target image includes the target parking space mark; and</p><p id="p-0708" num="0707">obtaining third position information corresponding to the circumscribed pattern in a digital map, where the third position information indicates a coordinate position of the circumscribed pattern in the digital map; and</p><p id="p-0709" num="0708">determining the pose information based on the first pixel position and the third position information.</p><p id="p-0710" num="0709">Optionally, the obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image includes:</p><p id="p-0711" num="0710">receiving the target image sent by a terminal device; and</p><p id="p-0712" num="0711">determining the first pixel position that is of the circumscribed pattern of the target parking space mark and that is in the target image.</p><p id="p-0713" num="0712">Optionally, the obtaining a first pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image includes:</p><p id="p-0714" num="0713">receiving the first pixel position that is of the circumscribed pattern of the target parking space mark, that is in the target image, and that is sent by the terminal device.</p><p id="p-0715" num="0714">Optionally, the obtaining third position information corresponding to the circumscribed pattern in a digital map includes:</p><p id="p-0716" num="0715">receiving the target image or the target parking space mark sent by the terminal device; and</p><p id="p-0717" num="0716">determining, from the digital map, third position information corresponding to the circumscribed pattern in the digital map.</p><p id="p-0718" num="0717">Optionally, the obtaining third position information corresponding to the circumscribed pattern in a digital map includes:</p><p id="p-0719" num="0718">receiving the third position information that corresponds to the circumscribed pattern in the digital map and that is sent by the terminal device.</p><p id="p-0720" num="0719">Optionally, the circumscribed pattern includes a first edge line, a second edge line, and a third edge line. At least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line.</p><p id="p-0721" num="0720">The first pixel position includes direction information of the first edge line, the second edge line, and the third edge line in the target image, and the third position information includes preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or</p><p id="p-0722" num="0721">the first pixel position includes direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the third position information includes preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</p><p id="p-0723" num="0722">Optionally, the determining pose information based on the first pixel position and the third position information includes:</p><p id="p-0724" num="0723">determining a 2D-3D correspondence between the first pixel position and the third position information; and</p><p id="p-0725" num="0724">determining the pose information based on the 2D-3D correspondence.</p><p id="p-0726" num="0725">Optionally, the circumscribed pattern includes a circumscribed rectangular frame.</p><p id="p-0727" num="0726">An embodiment of this disclosure further provides a computer program product. When the computer program product is run on a computer, the computer is enabled to perform the steps in the pose determining method.</p><p id="p-0728" num="0727">An embodiment of this disclosure further provides a computer-readable storage medium. The computer-readable storage medium stores a program for signal processing. When the program is run on a computer, the computer is enabled to perform the steps in the pose determining method in the foregoing method embodiments.</p><p id="p-0729" num="0728">In addition, it should be noted that the described apparatus embodiments are merely examples. The units described as separate parts may or may not be physically separate, and parts displayed as units may or may not be physical units, may be located in one position, or may be distributed on a plurality of network units. A part or all of the modules may be selected according to an actual requirement to achieve the objectives of the solutions of embodiments. In addition, in the accompanying drawings of the apparatus embodiments provided in this disclosure, connection relationships between modules indicate that the modules have communication connections with each other, which may be specifically implemented as one or more communications buses or signal cables.</p><p id="p-0730" num="0729">Based on the description of the foregoing implementations, a person skilled in the art may clearly understand that this disclosure may be implemented by software in addition to necessary universal hardware, or by dedicated hardware, including a dedicated integrated circuit, a dedicated CPU, a dedicated memory, a dedicated component, and the like. Generally, any functions that can be performed by a computer program can be easily implemented by using corresponding hardware. Moreover, a specific hardware structure used to achieve a same function may be in various forms, for example, in a form of an analog circuit, a digital circuit, or a dedicated circuit. However, as for this disclosure, software program implementation is a better implementation in most cases. Based on such an understanding, the technical solutions in this disclosure essentially or the part contributing to the prior art may be implemented in a form of a software product. The computer software product is stored in a readable storage medium, such as a floppy disk, a USB flash drive, a removable hard disk, a ROM, a RAM, a magnetic disk, or an optical disc of a computer, and includes several instructions for instructing a computer device (which may be a personal computer, a server, a network device, or the like) to perform the method in embodiments of this disclosure.</p><p id="p-0731" num="0730">All or a part of the foregoing embodiments may be implemented by software, hardware, firmware, or any combination thereof. When software is used to implement embodiments, all or a part of embodiments may be implemented in a form of a computer program product.</p><p id="p-0732" num="0731">The computer program product includes one or more computer instructions. When the computer program instructions are loaded and executed on a computer, all or a part of the procedures or functions according to embodiments of this disclosure are generated. The computer may be a general-purpose computer, a dedicated computer, a computer network, or another programmable apparatus. The computer instructions may be stored in a computer-readable storage medium or may be transmitted from a computer-readable storage medium to another computer-readable storage medium. For example, the computer instructions may be transmitted from a website, computer, server, or data center to another website, computer, server, or data center in a wired (for example, a coaxial cable, an optical fiber, or a digital subscriber line (DSL)) or wireless (for example, infrared, radio, or and microwave) manner. The computer-readable storage medium may be any usable medium accessible by a computer, or a data storage device, such as a server or a data center, integrating one or more usable media. The usable medium may be a magnetic medium (for example, a floppy disk, a hard disk, or a magnetic tape), an optical medium (for example, a DVD), a semiconductor medium (for example, a solid state disk (SSD)), or the like.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A pose determining method comprising:<claim-text>obtaining a target image, wherein the target image comprises a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark comprises the target parking space line; and</claim-text><claim-text>determining pose information based on the target parking space mark and the target parking space line, wherein the pose information indicates a corresponding pose of a terminal during photographing of the target image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the pose information based on the target parking space mark and the target parking space line comprises:<claim-text>obtaining a pixel position of the target parking space line in the target image;</claim-text><claim-text>obtaining position information corresponding to the target parking space line in a digital map, wherein the position information indicates a position of the target parking space line in the digital map; and</claim-text><claim-text>determining the pose information based on the pixel position and the position information.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the pose information based on the target parking space mark and the target parking space line comprises:<claim-text>sending the target image to a server; and</claim-text><claim-text>receiving the pose information sent by the server, wherein the pose information is determined by the server based on a pixel position of the target parking space line in the target image and position information corresponding to the target parking space line in a digital map, and the position information indicates a position of the target parking space line in the digital map.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the pose information based on the target parking space mark and the target parking space line comprises:<claim-text>obtaining a pixel position of the target parking space line in the target image;</claim-text><claim-text>sending the target parking space mark and the pixel position to a server; and</claim-text><claim-text>receiving the pose information sent by the server, wherein the pose information is determined by the server based on the pixel position and position information corresponding to the target parking space line in a digital map, and the position information indicates a position of the target parking space line in the digital map.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the pose information based on the target parking space mark and the target parking space line comprises:<claim-text>obtaining position information corresponding to the target parking space line in a digital map, wherein the of the position information indicates position information of the target parking space line in the digital map;</claim-text><claim-text>sending the target image and the position information to a server; and</claim-text><claim-text>receiving the pose information sent by the server, wherein the pose information is determined by the server based on the position information and a pixel position of the target parking space line in the target image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the obtaining of the target image comprises:<claim-text>displaying a target photographing interface, wherein the target photographing interface comprises a photographing area, and the photographing area comprises a preset guide contour; and</claim-text><claim-text>obtaining, through photographing, the target image if the target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein before the determining of the pose information based on the target parking space mark and the target parking space line, the method further comprises:<claim-text>displaying a parking space mark input indication box in the target image; and</claim-text><claim-text>obtaining the target parking space mark input in the parking space mark input indication box.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the obtaining of the position information corresponding to the target parking space line in the digital map comprises:<claim-text>obtaining positioning information of the terminal device during photographing of the target image; and</claim-text><claim-text>obtaining, from the digital map, the position information that matches the positioning information and that corresponds to the target parking space line in the digital map.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the target parking space line comprises a first parking space line, a second parking space line, and a third parking space line, wherein at least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line, wherein<claim-text>the pixel position comprises direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the position information comprises preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or</claim-text><claim-text>the pixel position comprises direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the position information comprises preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A pose determining method comprising:<claim-text>obtaining a pixel position that is of a circumscribed pattern of a target parking space mark and that is in a target image, wherein the target image comprises the target parking space mark;</claim-text><claim-text>obtaining position information corresponding to the circumscribed pattern in a digital map, wherein the position information indicates a coordinate position of the circumscribed pattern in the digital map; and</claim-text><claim-text>determining pose information based on the pixel position and the position information, wherein the pose information indicates a corresponding pose of a terminal during photographing of the target image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the circumscribed pattern comprises a first edge line, a second edge line, and a third edge line, wherein at least two of the first edge line, the second edge line, and the third edge line are not parallel in the digital map, and a second corner point is an intersection point of the first edge line and the second edge line, wherein the pixel position comprises direction information of the first edge line, the second edge line, and the third edge line in the target image, and the position information comprises preset directions corresponding to the first edge line, the second edge line, and the third edge line in the digital map; or<claim-text>the pixel position comprises direction information of the first edge line and the second edge line in the target image, and a pixel position of the second corner point in the target image; and the position information comprises preset directions corresponding to the first edge line and the second edge line in the digital map, and a preset position corresponding to the second corner point in the digital map.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the circumscribed pattern comprises a circumscribed rectangular frame.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A pose determining apparatus comprising:<claim-text>a processing system, including at least one computer processor, the processing system being at least configured to:<claim-text>obtain a target image, wherein the target image comprises a target parking space mark and a target parking space line, and a target parking space corresponding to the target parking space mark comprises the target parking space line; and</claim-text><claim-text>determine pose information based on the target parking space mark and the target parking space line, wherein the pose information indicates a corresponding pose of a terminal during photographing of the target image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in order to determine the pose information, the processing system is further configured to:<claim-text>obtain a pixel position of the target parking space line in the target image;</claim-text><claim-text>obtain position information corresponding to the target parking space line in a digital map, wherein the position information indicates a position of the target parking space line in the digital map; and</claim-text><claim-text>determine the pose information based on the pixel position and the position information.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in order to determine the pose information, the processing system is further configured to:<claim-text>send the target image to a server; and</claim-text><claim-text>receive the pose information sent by the server, wherein the pose information is determined by the server based on a pixel position of the target parking space line in the target image and position information corresponding to the target parking space line in a digital map, and the position information indicates a position of the target parking space line in the digital map.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in order to determine the pose information, the processing system is further configured to:<claim-text>obtain a pixel position of the target parking space line in the target image;</claim-text><claim-text>send the target parking space mark and the pixel position to a server; and</claim-text><claim-text>receive the pose information sent by the server, wherein the pose information is determined by the server based on the pixel position and position information corresponding to the target parking space line in a digital map, and the position information indicates a position of the target parking space line in the digital map.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in order to determine the pose information, the processing system is further configured to:<claim-text>obtain position information corresponding to the target parking space line in a digital map, wherein the position information indicates position information of the target parking space line in the digital map;</claim-text><claim-text>send the target image and the position information to a server; and</claim-text><claim-text>receive the pose information sent by the server, wherein the pose information is determined by the server based on the position information and a pixel position of the target parking space line in the target image.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein in order to obtain the target image, the processing system is further configured to:<claim-text>display a target photographing interface, wherein the target photographing interface comprises a photographing area, and the photographing area comprises a preset guide contour; and</claim-text><claim-text>obtain, through photographing, the target image if the target parking space in the photographing area is located within the preset guide contour or a position difference between a target parking space in the photographing area and the preset guide contour falls within a preset range.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the processing system is further configured to:<claim-text>display a parking space mark input indication box in the target image; and</claim-text><claim-text>obtain the target parking space mark input in the parking space mark input indication box.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The apparatus according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the target parking space line comprises a first parking space line, a second parking space line, and a third parking space line, wherein at least two of the first parking space line, the second parking space line, and the third parking space line are not parallel in the digital map, and a first corner point is an intersection point of the first parking space line and the second parking space line, wherein the pixel position comprises direction information of the first parking space line, the second parking space line, and the third parking space line in the target image, and the position information comprises preset directions corresponding to the first parking space line, the second parking space line, and the third parking space line in the digital map; or<claim-text>the pixel position comprises direction information of the first parking space line and the second parking space line in the target image, and a pixel position of the first corner point in the target image; and the position information comprises preset directions corresponding to the first parking space line and the second parking space line in the digital map, and a preset position corresponding to the first corner point in the digital map.</claim-text></claim-text></claim></claims></us-patent-application>