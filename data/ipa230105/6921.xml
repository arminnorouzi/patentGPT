<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006922A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006922</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17367347</doc-number><date>20210703</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>717</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>751</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>927</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>45</main-group><subgroup>42</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>45</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>4633</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>47</main-group><subgroup>806</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SCALABLE OVERLAY MULTICAST ROUTING IN MULTI-TIER EDGE GATEWAYS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>VMware, Inc.</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Karunakaran</last-name><first-name>Senthilkumar</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Mathew</last-name><first-name>Subin Cyriac</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Tan</last-name><first-name>Stephen</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Selvaraj</last-name><first-name>Meenakshi Sundaram</first-name><address><city>Pleasanton</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Raman</last-name><first-name>Chidambareswaran</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for offloading multicast replication from multiple tiers of edge nodes implemented by multiple host machines to a physical switch is provided. Each of the multiple host machines implements a provider edge node and a tenant edge node. One host machine among the multiple host machines receives a packet having an overlay multicast group identifier. The host machine maps the overlay multicast group identifier to an underlay multicast group identifier. The host machine encapsulates the packet with an encapsulation header that includes the underlay multicast group identifier to create an encapsulated packet. The host machine forwards the encapsulated packet to a physical switch of the network segment. The physical switch forwards copies of the encapsulated packet to tenant edge nodes at one or more ports that are determined to be interested in the underlay multicast group identifier.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="91.10mm" wi="158.75mm" file="US20230006922A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="219.20mm" wi="156.13mm" orientation="landscape" file="US20230006922A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="219.20mm" wi="160.02mm" orientation="landscape" file="US20230006922A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="215.14mm" wi="151.30mm" orientation="landscape" file="US20230006922A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="214.55mm" wi="151.30mm" orientation="landscape" file="US20230006922A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="208.20mm" wi="175.85mm" orientation="landscape" file="US20230006922A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="234.10mm" wi="155.36mm" orientation="landscape" file="US20230006922A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="216.49mm" wi="148.25mm" file="US20230006922A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="220.64mm" wi="157.14mm" file="US20230006922A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="188.81mm" wi="122.68mm" orientation="landscape" file="US20230006922A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Multicast is group communication in computer networking where data transmission is addressed to a group of destination computers simultaneously. Multicast can be one-to-many or many-to-many distribution. Group communication makes it possible for the source to efficiently send to the group in a single transmission. Copies are automatically created in network elements such as routers, switches, and cellular network base stations, but only to network segments that currently contain members of the group. Multicast protocols such as Internet Group Management Protocol (IGMP) and Protocol Independent Multicast (PIM) are used to setup the forwarding state in routers based on the information exchanged about the senders and the receivers of multicast traffic.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">Some embodiments of the invention provide a method for offloading multicast replication from multiple tiers of edge nodes implemented by multiple host machines to a physical switch. Each of the multiple host machines implements a provider edge node and a tenant edge node. One host machine among the multiple host machines receives a packet having an overlay multicast group identifier. The host machine maps the overlay multicast group identifier to an underlay multicast group identifier. The host machine encapsulates the packet with an encapsulation header that includes the underlay multicast group identifier to create an encapsulated packet. The host machine forwards the encapsulated packet to a physical switch of the network segment. The physical switch forwards copies of the encapsulated packet to tenant edge nodes at one or more ports that are determined to be interested in the underlay multicast group identifier.</p><p id="p-0004" num="0003">The packet may be received from a tenant network and the host machine hosts a tenant edge node that serves data traffic to and from the tenant network. Each tenant edge node is serving data traffic, including multicast traffic to and from a tenant network by performing gateway functions. The packet may also be received from an external network, and the particular provider edge node serves data traffic, including multicast traffic, to and from the external network by performing gateway functions. The particular provider edge node is actively serving data traffic to and from the external network, and other provider edge nodes implemented by other host machines are standing by and not actively serving data traffic to and from the external network.</p><p id="p-0005" num="0004">In some embodiments, a network controller (e.g., SDN controller) sends multicast grouping information associating an overlay multicast group identifier with (i) a corresponding underlay multicast group identifier and (ii) a list of VTEPs that are interested in the multicast group to each tenant edge and each provider edge. In some embodiments, the network controller generates the multicast grouping information based on the multicast reports (e.g., IGMP inquiry reports) that associates VTEPs with overlay multicast group identifiers. In some embodiments, the list of VTEPs that are interested in a multicast group sent to a particular tenant edge node distinguishes (i) VTEPs connected to a same network segment as the particular tenant edge node from (ii) VTEPs connected to a different network segment as the particular tenant edge node.</p><p id="p-0006" num="0005">A tenant edge node receiving a copy of the encapsulated packet may decapsulate the packet to remove the underlay multicast group identifier and forward the decapsulated packet to a tenant network by multicast based on the overlay multicast group identifier. The particular provider edge node may also receive a copy of the encapsulated packet from the physical switch and forward a decapsulated copy of the packet to the external network without the underlay multicast group identifier. In some embodiments, when the packet is received from the external network, the host machine that implements the particular provider edge node receives the packet, maps the overlay multicast group identifier to the underlay multicast group identifier, encapsulates the packet with an encapsulation header that includes the underlay multicast group identifier, and forwards the encapsulated packet to the physical switch of the first network segment.</p><p id="p-0007" num="0006">In some embodiments, the host machine may use the multicast grouping information sent to the host machine to identify any VTEPs in other segments that are also interested in the multicast group. If there is a VTEP in another network segment interested in the multicast group, the host machine identifies a VTEP at a second network segment having a tenant edge node that is interested in the underlay multicast group identifier. The host machine forwards the packet to the identified VTEP by unicast.</p><p id="p-0008" num="0007">The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly, to understand all the embodiments described by this document, a full review of the Summary, Detailed Description, the Drawings and the Claims is needed. Moreover, the claimed subject matters are not to be limited by the illustrative details in the Summary, Detailed Description and the Drawing.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">The novel features of the invention are set forth in the appended claims. However, for purposes of explanation, several embodiments of the invention are set forth in the following figures.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>1</b>A-B</figref> illustrate a software defined network (SDN) environment in which a first tier of edge routers performs gateway functions for traffic to and from a physical network and a second tier of edge routers performs gateway functions for traffic to and from tenant networks.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. <b>2</b>A-B</figref> conceptually illustrate using underlay group IP to offload multicast replication to a physical switch.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> conceptually illustrates multicast group information that are sent by a SDN controller to each host machine implementing tenant edges and provider edges.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates multicast replication to a host machine in a different network segment by unicast.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> conceptually illustrates a process for offloading multicast replication from tiered edge routers to physical switches.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a computing device that serves as a host machine that runs virtualization software.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> conceptually illustrates a computer system with which some embodiments of the invention are implemented.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">In the following detailed description of the invention, numerous details, examples, and embodiments of the invention are set forth and described. However, it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.</p><p id="p-0018" num="0017">In a software defined network (SDN) environment, a provider level (Tier-0 or T0) edge logical router device acts as the gateway between physical network (e.g., wide area network or WAN) and virtual or overlay networks. In a multi-tenant topology, a tenant level (Tier-1 or T1) dedicated edge device can be configured to be the gateway for a given tenant. For traffic originated in an overlay network, a T1 edge routes data packets to a T0 edge to connect to the physical network. Similarly, WAN traffic from the physical network reaches T0 edge gateway and then gets routed to T1 edge gateways. An edge transport node (TN) can host one or more T0 and/or T1 routers and there can be multiple such edge TNs in a cluster. (An edge can be referred to as an edge node, an edge router, an edge device, an edge gateway, an edge TN, etc.)</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIGS. <b>1</b>A-B</figref> illustrate a software defined network (SDN) environment in which a first tier of edge routers performs gateway functions for traffic to and from a physical network and a second tier of edge routers performs gateway functions for traffic to and from tenant networks.</p><p id="p-0020" num="0019">As illustrated, a SDN environment is a network <b>100</b> that provide connectivity to several tenant networks <b>101</b>-<b>107</b> (tenants A through G). The network <b>100</b> also provide connectivity to an external physical network that is a wide area network (WAN) <b>109</b>. The network <b>100</b> may include physical network components provided by one or more datacenters as underlay.</p><p id="p-0021" num="0020">The network <b>100</b> includes a cluster of host machines <b>111</b>-<b>117</b> that implement a first tier of edge routers <b>121</b>-<b>127</b> and a second tier of edge routers <b>131</b>-<b>137</b>. The first tier of edge routers <b>121</b>-<b>127</b> are provider edge routers (also referred to T0 edges) shared by different tenants of a datacenter. The first tier provider edge routers also perform gateway function for traffic to and from a WAN <b>109</b> in active/standby mode. The second tier of routers <b>131</b>-<b>137</b> are tenant edge routers (or T1 edges or T1 TNs) for tenant networks <b>101</b>-<b>107</b>, each T1 edge performing gateway function for traffic to and from a tenant network. The T0 provider edges and the T1 tenant edges together enable traffic between the WAN <b>109</b> and the tenant networks <b>101</b>-<b>107</b> (North-South traffic), as well as traffic among the different tenant networks (East-West traffic).</p><p id="p-0022" num="0021">Each host machine is addressable by a virtual tunnel endpoint (VTEP) address, as traffic to and from the different host machines are conducted by tunnels. In the example, the host machines <b>111</b>-<b>114</b> are interconnected by a physical L2 switch <b>141</b> (Top of Rack or ToR switch), while the host machines <b>115</b>-<b>117</b> are interconnected by a different L2 switch <b>142</b>. In other words, in the physical underlay, the host machines <b>111</b>-<b>114</b> belong to one network segment and the host machines <b>115</b>-<b>117</b> belongs to a different network segment.</p><p id="p-0023" num="0022">When edge devices are configured as multicast-routers, IP-multicast traffic will be routed from the physical network (e.g., the WAN <b>109</b>) to the virtual network (e.g., tenant networks <b>101</b>-<b>107</b>) and vice-versa. While edges are running in a multi-tiered architecture, inter-tier multicast traffic is routed by one centralized T0 router. This is because only one router is allowed to be the multicast querier for each network segment (L2 segment or IP subnet) according to multicast protocols such as Protocol-Independent Multicast (PIM) or Internet Group Management Protocol (IGMP). Thus, one edge gateway (e.g., T0 edge <b>121</b>) that supports IP multicast routing encapsulates and replicates the routed multicast packets to all edge virtual tunnel endpoints (VTEPs) that have receivers for the corresponding multicast group in an overlay domain and another copy towards PIM core for receivers in the physical network (e.g., WAN <b>109</b>).</p><p id="p-0024" num="0023">In the example of <figref idref="DRAWINGS">FIGS. <b>1</b>A-B</figref>, the T0 edge node <b>121</b> is the centralized T0 (provider) router for multicast, and it is also the active edge in active/standby configuration for handling multicast traffic to and from the WAN <b>109</b>. Specifically, the provider edge node <b>121</b> is actively serving data traffic to and from the external network WAN <b>109</b> and other provider edge nodes <b>122</b>-<b>127</b> implemented by other host machines <b>112</b>-<b>117</b> are standing by and not actively serving data traffic to and from the WAN <b>109</b>. The T0 edge node <b>121</b> is therefore also referred to as the active T0 edge.</p><p id="p-0025" num="0024">The active T0 edge node <b>121</b> receives multicast traffic for a multicast group having an identifier of 237.1.1.1, and by multicast inquiry the T0 edge <b>121</b> knows that tenant networks C, D, E have receivers that are interested in the multicast group 237.1.1.1. <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates the active T0 edge node <b>121</b> receiving N-S multicast traffic from the WAN <b>109</b> and replicating the traffic to T1 TNs <b>133</b>, <b>134</b>, and <b>135</b> (corresponding to tenant networks C, D, and E.) <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates the active T0 edge node <b>121</b> receiving E-W multicast traffic from T1 TN <b>132</b> (originates from a source <b>192</b> in tenant network B) and replicating the traffic to T1 TNs <b>133</b>, <b>134</b>, and <b>135</b> (corresponding to tenant networks C, D, and E) as well as the WAN <b>109</b>.</p><p id="p-0026" num="0025">Having the one T0 edge <b>121</b> to centrally replicate multicast traffic for multiple different T1 TNs degrades the multicast routing throughput and latency, because the throughput of multicast traffic flow is limited by total number of replications that the one centralized T0 edge performs. The more T1 edge TNs there are to receive multicast traffic, the more replications that the one centralized T0 edge <b>121</b> has to perform, and more likely to saturate the downlink bandwidth of the centralized T0 edge. Relying on the one T0 edge <b>121</b> to handle traffic for all T1 TNs also makes the multicast replication scheme difficult to scale for additional tenant networks. For example, if 2 Gbps of multicast source traffic are to be replicated to 5 different T1 edges, then the 10 G uplinks of the T0 edge will be saturated by this one multicast flow and traffic from other sources cannot be processed. In that instance, the one centralized T0 edge can only accommodate up to 5 T1 edge TNs at one time.</p><p id="p-0027" num="0026">Some embodiments of the invention provide a method for scaling the multicast replications to a larger number of T1 edge TNs using Top-of-Rack (ToR) L2-multicast, without reducing routing throughput or worsening forwarding latency. Specifically, the number of multicast replications at the active centralized T0 edge (e.g., <b>121</b>) is reduced by offloading multicast replication to L2 ToR switches (e.g., ToRs <b>141</b> and <b>142</b>) using underlay multicast group IPs. Routing throughput is further improved by leveraging underlay multicast replication using underlay multicast group IPs and VTEP list that are synchronized by a SDN controller. Doing so allows larger number of parallel flows which results in higher throughput and enables larger number of tenants to participate in multicast routing. Reducing number of replications at source edge TNs also improves or reduces routing latency. In some embodiments, multicast routing protocols are not used at T1 edges, which keeps T1 edges to be light-weight forwarding planes. Multicast routing throughput can be scaled out (horizontal scaling) by deploying more TNs with T1 edges.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. <b>2</b>A-B</figref> conceptually illustrate using underlay group IP to offload multicast replication to a ToR switch. <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates offloading multicast replication for N-S traffic that originates in the WAN <b>109</b>. The figure illustrates operations that are labeled (<b>1</b>) through (<b>4</b>). At operations labeled (<b>1</b>), the host machine <b>111</b> (in the cluster of host machines <b>111</b>-<b>117</b>) receives a packet <b>210</b> having the overlay multicast group identifier 237.1.1.1 from the WAN <b>109</b>. Since this is traffic from an external network, the traffic is processed by the active T0 edge <b>121</b> implemented by the host machine <b>111</b>.</p><p id="p-0029" num="0028">At operations labeled (<b>2</b>), the host machine <b>111</b> maps the overlay multicast group identifier 237.1.1.1 to an underlay multicast group identifier 240.2.2.2. The host machine <b>111</b> encapsulates the packet <b>210</b> with an encapsulation header that includes the underlay multicast group identifier 240.2.2.2 to create an encapsulated packet <b>212</b>. The host machine <b>111</b> forwards the encapsulated packet to the physical switch (L2-ToR) <b>141</b> (which is the L2 switch of the network segment that includes host machines <b>111</b>-<b>114</b>).</p><p id="p-0030" num="0029">At operations labeled (<b>3</b>), the physical switch <b>141</b> forwards copies of the encapsulated packet <b>212</b> to host machines <b>113</b> and <b>114</b> (having tenant edge nodes <b>133</b> and <b>134</b>) at one or more ports of the switch that are determined (by IGMP snooping) to be interested in (or has receivers for) the underlay multicast group identifier 240.2.2.2. (The multicast traffic is replicated to ports that correspond to tenant C and tenant D).</p><p id="p-0031" num="0030">At operations labeled (<b>4</b>), the T1 tenant edge node <b>133</b> decapsulates the packet <b>212</b> into the packet <b>210</b> and forwards the decapsulated packet <b>210</b> to tenant network <b>103</b> (for tenant C). Likewise, the T1 tenant edge node <b>134</b> decapsulates the packet <b>212</b> into the packet <b>210</b> and forwards the decapsulated packet <b>210</b> to tenant network <b>104</b> (for tenant D).</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates offloading multicast replication for E-W traffic that originates in one of the tenant networks. The figure illustrates operations that are labeled (<b>1</b>) through (<b>4</b>). At operations labeled (<b>1</b>), the host machine <b>112</b> receives a packet <b>220</b> having the overlay multicast group identifier 237.1.1.1 from the tenant network <b>102</b> (tenant B).</p><p id="p-0033" num="0032">At operations labeled (<b>2</b>), the host machine <b>112</b> maps the overlay multicast group identifier 237.1.1.1 to the underlay multicast group identifier 240.2.2.2. The host machine <b>112</b> encapsulates the packet <b>220</b> with an encapsulation header that includes the underlay multicast group identifier 240.2.2.2 to create an encapsulated packet <b>222</b>. The host machine <b>112</b> forwards the encapsulated packet <b>222</b> to the physical switch (L2-ToR) <b>141</b>.</p><p id="p-0034" num="0033">At operations labeled (<b>3</b>), the physical switch <b>141</b> forwards copies of the encapsulated packet <b>222</b> to host machines <b>113</b> and <b>114</b> (having tenant edge nodes <b>133</b> and <b>134</b>) at one or more ports of the switch that are determined (by IGMP snooping) to be interested in (or has receivers for) the underlay multicast group identifier 240.2.2.2. (The multicast traffic is replicated to ports that corresponds to tenant C and tenant D). The physical switch <b>141</b> also forwards a copy of the encapsulated packet <b>222</b> to the host machine <b>111</b> having the active T0 edge <b>121</b> to be forwarded to the WAN <b>109</b>.</p><p id="p-0035" num="0034">At operations labeled (<b>4</b>), the T1 tenant edge node <b>133</b> decapsulates the packet <b>222</b> into the packet <b>220</b> (to remove the underlay multicast group identifier) and forwards the decapsulated packet <b>220</b> to tenant network <b>103</b> (for tenant C). Likewise, the T1 tenant edge node <b>134</b> decapsulates the packet <b>222</b> into the packet <b>220</b> and then forwards the decapsulated packet <b>220</b> to tenant network <b>104</b> (for tenant D). Also, the T0 active edge <b>121</b> decapsulates the packet <b>222</b> and forwards the decapsulated packet <b>220</b> to the WAN <b>109</b>.</p><p id="p-0036" num="0035">In some embodiments, a controller of the SDN network (or SDN controller) associates or maps each overlay multicast group identifier with a corresponding underlay multicast group identifier. The underlay multicast group identifier is one that is predetermined to be available in the underlay domain. In the example of <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, the SDN controller maps the overlay multicast group identifier 237.1.1.1 to the underlay multicast group identifier 240.2.2.2.</p><p id="p-0037" num="0036">The SDN controller also sends multicast group information to each T1 (tenant) edge and each T0 (provider) edge. The multicast group information may include mapping for associating each overlay multicast group identifier with its corresponding underlay multicast group identifier. For each multicast group, the multicast group information may also include a list of VTEPs that are interested in the multicast group. In some embodiments, the list of VTEPs interested in the multicast group is identified based on multicast reports (e.g., IGMP reports) associating VTEPs with overlay multicast group identifiers.</p><p id="p-0038" num="0037">In some embodiments, the list of VTEPs in the multicast information sent to a particular tenant edge node distinguishes (i) VTEPs connected to a same network segment as the particular tenant edge node from (ii) VTEPs connected to a different network segment as the particular tenant edge node. Based on the multicast group information, the T1 tenant edge may send the packet to receivers in the same segment by encapsulating the underlay multigroup identifier (to rely on the ToR switch to perform multicast replication); or directly send the packet to receivers in a different segment without encapsulating the underlay multigroup identifier.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>3</b></figref> conceptually illustrates multicast group information that are sent by a SDN controller to each host machine implementing tenant edges and provider edges. A controller <b>300</b> of the network <b>100</b> receives multicast (e.g., IGMP) reports <b>310</b> associating VTEPs with overlay multicast group identifiers. As illustrated, the reports <b>310</b> include identified VTEPs of host machines that are interested in the each of several multicast groups. For example, for the multicast group 237.1.1.1, the reports <b>310</b> identify VTEPs C, D, and E (of host machines <b>113</b>, <b>114</b>, <b>115</b>) as having interested receivers; for the multicast group 238.3.3.3, the reports <b>310</b> identify VTEPs A, B, D, F, G (of host machines <b>111</b>, <b>112</b>, <b>114</b>, <b>116</b>, <b>117</b>) as having interested receivers, etc.</p><p id="p-0040" num="0039">The controller <b>300</b> then maps each overlay multicast group identifier to underlay multicast group identifier. Thus, the overlay multicast group identifier or address 237.1.1.1 is mapped to an underlay multicast group identifier 240.2.2.2, the overlay multicast group identifier or address 238.3.3.3 is mapped to an underlay multicast group identifier 241.4.4.4, etc. The underlay multicast group identifiers are chosen from IP address ranges that are available for use in the underlay domain. Based on the received multicast reports and the multicast group identifier mapping, the controller <b>300</b> generates and sends multicast group information to VTEPs of host machines that hosts tenant edges and each provider edge. Each host machine and the T0/T1 edges hosted by the host machines in turn uses the multicast group information to map multicast group identifiers and to identify which VTEPs are interested in which multicast group.</p><p id="p-0041" num="0040">In the figure, multicast group information <b>321</b>-<b>327</b> are sent to host machines <b>111</b>-<b>117</b> (VTEPs A through G), respectively. The multicast group information sent to each host machine associates each overlay multicast group identifier with (i) its corresponding underlay multicast group identifier and (ii) a list of VTEPs that are interested in the multicast group to each tenant edge and each provider edge. Thus, in the multicast group information <b>321</b>-<b>327</b>, the overlay multicast group identifier 237.1.1.1 is associated with the underlay multicast group identifier 240.2.2.2 and a list that includes VTEPs C, D, and E, and the overlay multicast group identifier 238.3.3.3 is associated with the underlay multicast group identifier 241.4.4.4 and a list that includes VTEPs A, B, D, F, G, etc.</p><p id="p-0042" num="0041">In some embodiments, the multicast information sent to a host machine does not list the VTEP of the host machine as one of the VTEPs interested in any of the multicast groups. As illustrated in the figure, the multicast group information <b>321</b> sent to VTEP-A does not list VTEP-A as one of the VTEPs interested in the multicast group 238.3.3.3 (so &#x201c;A&#x201d; in information <b>321</b> appear darkened), while the multicast group information <b>322</b> sent to VTEP-B does not list VTEP-B as one of the VTEPs interested in the multicast group 238.3.3.3 (so &#x201c;B&#x201d; in information <b>322</b> appear darkened). Thus, multicast traffic from tenant A network will not be replicated by the tenant edge in VTEP A for multicast group 238.3.3.3, even if tenant A has a receiver for the multicast group 238.3.3.3.</p><p id="p-0043" num="0042">In some embodiments, the list of VTEPs of multicast groups sent to a particular host machine or tenant edge node distinguishes VTEPs connected to a same network segment as the particular tenant edge node or host machine from VTEPs connected to a different network segment as the particular tenant edge node or host machine. In the example of <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>2</b></figref>, VTEPs A-D (host machines <b>111</b>-<b>114</b>) are in a same network segment interconnected by ToR switch <b>141</b>, and VTEPs E-G (host machines <b>115</b>-<b>117</b>) are in a different network segment interconnected by the ToR switch <b>142</b>. For example, for the multicast group 238.3.3.3 (241.4.4.4), in the multicast group information <b>321</b>-<b>324</b> sent to VTEPs A-D, VTEPs A, B, and D are identified as being in the same network segment (segment <b>0</b> with &#x2018;*&#x2019; mark) as the receiving T1 TNs in host machines <b>111</b>-<b>114</b>; VTEPs F and G are identified as being in a different segment. In the multicast information <b>325</b>-<b>327</b> sent to VTEPs E-G, VTEPs F and G are identified as being in the same network segment (segment <b>1</b> with &#x2018;*&#x2019; mark) as the receiving T1 TNs in host machines <b>115</b>-<b>117</b>; VTEP A, B, and D are identified as being in a different segment.</p><p id="p-0044" num="0043">In some embodiments, a T1 (tenant) edge node receiving the multicast group information would identify a VTEP at a different network segment having a tenant edge node that is interested in the underlay multicast group identifier. The T1 edge node then forwards the packet to the identified VTEP by direct unicast rather than by using the ToR switch. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates multicast replication to a host machine in a different network segment by unicast. The example illustrates the replication of a packet belonging to the multicast group 238.3.3.3 across two different network segments in the network <b>100</b>. The figure illustrates operations that are labeled (<b>1</b>) through (<b>6</b>). At operations labeled (<b>1</b>), the host machine <b>113</b> receives a packet <b>410</b> having the overlay multicast group identifier 238.3.3.3 from the tenant network <b>103</b> (tenant C).</p><p id="p-0045" num="0044">At operations labeled (<b>2</b>), the host machine <b>113</b> maps the overlay multicast group identifier 238.3.3.3 to the underlay multicast group identifier 241.4.4.4 (by using the multicast information <b>323</b>.) The host machine <b>113</b> (at the T1 edge <b>133</b>) encapsulates the packet <b>410</b> with an encapsulation header that includes the underlay multicast group identifier 241.4.4.4 to create an encapsulated packet <b>412</b>. The host machine <b>113</b> forwards the encapsulated packet <b>412</b> to the physical switch (L2-ToR) <b>141</b>.</p><p id="p-0046" num="0045">At operations labeled (<b>3</b>), the physical switch <b>141</b> forwards copies of the encapsulated packet <b>412</b> to host machines <b>111</b>, <b>112</b> and <b>114</b> (having T1 edges <b>131</b>, <b>132</b>, <b>134</b>) at one or more ports of the switch <b>141</b> that are determined to be interested in the underlay multicast group identifier 241.4.4.4. (The multicast traffic is replicated to ports that corresponds to VTEPs A, B and D). The physical switch <b>141</b> also forwards a copy of the encapsulated packet <b>412</b> to the host machine <b>111</b> having the active T0 edge <b>121</b> to be forwarded to the WAN <b>109</b>. The T1 tenant edge nodes <b>131</b>, <b>132</b>, <b>134</b>, and the active T0 edge in turn decapsulate the packets to remove the underlay multicast group identifier and forwards the decapsulated packet to their respective tenant networks (including the WAN <b>109</b>).</p><p id="p-0047" num="0046">(In some embodiments, the physical switch <b>141</b> learns which ports or which VTEPs are interested in the underlay multicast group 241.4.4.4 by IGMP snooping. Specifically, the VTEPs may initiate IGMP join on the underlay multicast group 241.4.4.4 toward the physical switch <b>141</b>, when a TN has an overlay receiver learned from its attached tenant network and the SDN controller <b>300</b> sends the underlay multicast mapping for the corresponding overlay multicast group. The physical switch <b>141</b> learns VTEP port based on the IGMP report message on the VLANs (aka Transport VLAN) that the VTEPs belong to. If there is any TN that does not have any tenant that is interested in 241.4.4.4 then TOR will not forward the packets to that particular TN.)</p><p id="p-0048" num="0047">At operations labeled (<b>4</b>), the T1 edge <b>133</b> (at VTEP C) forwards a copy of the multicast packet <b>410</b> to the T1 edge <b>136</b> (at VTEP F) by unicast. The T1 edge <b>133</b> performs this operation because the multicast group information <b>323</b> sent to host machine <b>113</b> indicates that VTEPs F and G are also interested in the multicast group 238.3.3.3 (241.4.4.4), and that VTEPs F and G are in a different network segment than VTEP C. The host machine <b>113</b> therefore sends the packet <b>410</b> to the T1 edge <b>136</b>. In some embodiments, the packet <b>410</b> is sent using an overlay tunnel from the VTEP-C to VTEP-F, though physically through the L2 switches <b>141</b> and <b>142</b> and L3 router <b>150</b>.</p><p id="p-0049" num="0048">At operations labeled (<b>5</b>), the T1 edge <b>136</b> (at VTEP-F or the host machine <b>116</b>) maps the overlay multicast group identifier 238.3.3.3 to the underlay multicast group identifier 241.4.4.4 (by using the multicast information <b>326</b>.) The host machine <b>116</b> (at the T1 edge <b>136</b>) encapsulates the packet <b>410</b> with an encapsulation header that includes the underlay multicast group identifier 241.4.4.4 to create an encapsulated packet <b>412</b>. The host machine <b>113</b> forwards the encapsulated packet <b>412</b> to the physical switch (L2-ToR) <b>142</b>.</p><p id="p-0050" num="0049">At operations labeled (<b>6</b>), the physical switch <b>142</b> forwards copies of the encapsulated packet <b>412</b> to the host machine <b>117</b> (having T1 edge <b>137</b>) at ports of the switch that are determined to be interested in the underlay multicast group identifier 241.4.4.4. (The multicast traffic is replicated to ports that correspond to VTEP-G). The T1 tenant edge <b>137</b> in turn decapsulates the packet to remove the underlay multicast group identifier and forwards the decapsulated packet <b>410</b> to tenant network G <b>107</b> with overlay multicast group identifier 238.3.3.3.</p><p id="p-0051" num="0050">For some embodiments, <figref idref="DRAWINGS">FIG. <b>5</b></figref> conceptually illustrates a process <b>500</b> for offloading multicast replication from tiered edge routers to ToR physical switches. In some embodiments, one or more processing units (e.g., processor) of a computing device implementing one of the host machines <b>111</b>-<b>117</b> performs the process <b>500</b> by executing instructions stored in a computer readable medium. Each host machine implements a provider edge node (T0 edge) and a tenant edge node (T1 edge), each host machine addressable by a unique virtual tunnel endpoint (VTEP). A particular provider edge node implemented by one of the host machines is a multicast querier (e.g., under IGMP) of a first network segment.</p><p id="p-0052" num="0051">In some embodiments, the process <b>500</b> starts when the host machine receives (at <b>510</b>) a packet having an overlay multicast group identifier. The packet may be received from a tenant network and the host machine hosts a tenant edge node that serves data traffic to and from the tenant network. Each tenant edge node is serving data traffic, including multicast traffic to and from a tenant network by performing gateway functions. The packet may also be received from an external network (e.g., WAN), and the particular provider edge node serves data traffic, including multicast traffic, to and from the external network by performing gateway functions. The particular provider edge node is actively serving data traffic to and from the external network, and other provider edge nodes implemented by other host machines are standing by and not actively serving data traffic to and from the external network.</p><p id="p-0053" num="0052">The host machine maps (at <b>520</b>) the overlay multicast group identifier to an underlay multicast group identifier according to mapping information provided by a network controller. In some embodiments, a network controller (e.g., SDN controller) sends multicast grouping information associating an overlay multicast group identifier with (i) a corresponding underlay multicast group identifier and (ii) a list of VTEPs that are interested in the multicast group to each tenant edge and each provider edge. In some embodiments, the network controller generates the multicast grouping information based on the multicast reports (e.g., IGMP inquiry reports) that associates VTEPs with overlay multicast group identifiers. In some embodiments, the list of VTEPs that are interested in a multicast group sent to a particular tenant edge node distinguishes (i) VTEPs connected to a same network segment as the particular tenant edge node from (ii) VTEPs connected to a different network segment as the particular tenant edge node.</p><p id="p-0054" num="0053">The host machine encapsulates (at <b>530</b>) the packet with an encapsulation header that includes the underlay multicast group identifier to create an encapsulated packet.</p><p id="p-0055" num="0054">The host machine forwards (at <b>540</b>) the encapsulated packet to a physical switch of the first network segment. The physical switch then forwards copies of the encapsulated packet to tenant edge nodes at one or more ports that are determined (by e.g., IGMP snooping) to be interested in (or has receivers for) the underlay multicast group identifier. A tenant edge node receiving a copy of the encapsulated packet may decapsulate the packet to remove the underlay multicast group identifier and forward the decapsulated packet to a tenant network by multicast based on the overlay multicast group identifier. The particular provider edge node may also receive a copy of the encapsulated packet from the physical switch and forward a decapsulated copy of the packet to the external network without the underlay multicast group identifier. In some embodiments, when the packet is received from the external network, the host machine that implements the particular provider edge node receives the packet, maps the overlay multicast group identifier to the underlay multicast group identifier, encapsulates the packet with an encapsulation header that includes the underlay multicast group identifier, and forwards the encapsulated packet to the physical switch of the first network segment.</p><p id="p-0056" num="0055">The host machine then determines (at <b>550</b>) whether VTEP(s) in another network segment is interested in the multicast group. The host machine may use the multicast grouping information sent to the host machine to identify any VTEPs in other segments that are also interested in the multicast group. If there is a VTEP in another network segment interested in the multicast group, the process proceeds to <b>560</b>. If no other network segment has a VTEP that is interested in the multicast group, the process <b>500</b> ends.</p><p id="p-0057" num="0056">At <b>560</b>, the host machine identifies a VTEP at a second network segment having a tenant edge node that is interested in the underlay multicast group identifier. The host machine forwards (at <b>570</b>) the packet to the identified VTEP by unicast. The process <b>500</b> then ends.</p><p id="p-0058" num="0057">As mentioned, provider edges (T0 edge nodes) and tenant edges (T1 edge nodes) may be implemented by host machines that are running virtualization software, serving as virtual network forwarding engines. Such a virtual network forwarding engine is also known as managed forwarding element (MFE), or hypervisors. Virtualization software allows a computing device to host a set of virtual machines (VMs) or data compute nodes (DCNs) as well as to perform packet-forwarding operations (including L2 switching and L3 routing operations). These computing devices are therefore also referred to as host machines. The packet forwarding operations of the virtualization software are managed and controlled by a set of central controllers, and therefore the virtualization software is also referred to as a managed software forwarding element (MSFE) in some embodiments. In some embodiments, the MSFE performs its packet forwarding operations for one or more logical forwarding elements as the virtualization software of the host machine operates local instantiations of the logical forwarding elements as physical forwarding elements. Some of these physical forwarding elements are managed physical routing elements (MPREs) for performing L3 routing operations for a logical routing element (LRE), some of these physical forwarding elements are managed physical switching elements (MPSEs) for performing L2 switching operations for a logical switching element (LSE). <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a computing device <b>600</b> that serves as a host machine that runs virtualization software for some embodiments of the invention.</p><p id="p-0059" num="0058">As illustrated, the computing device <b>600</b> has access to a physical network <b>690</b> through a physical NIC (PNIC) <b>695</b>. The host machine <b>600</b> also runs the virtualization software <b>605</b> and hosts VMs <b>611</b>-<b>614</b>. The virtualization software <b>605</b> serves as the interface between the hosted VMs <b>611</b>-<b>614</b> and the physical NIC <b>695</b> (as well as other physical resources, such as processors and memory). Each of the VMs <b>611</b>-<b>614</b> includes a virtual NIC (VNIC) for accessing the network through the virtualization software <b>605</b>. Each VNIC in a VM <b>611</b>-<b>614</b> is responsible for exchanging packets between the VM <b>611</b>-<b>614</b> and the virtualization software <b>605</b>. In some embodiments, the VNICs are software abstractions of physical NICs implemented by virtual NIC emulators.</p><p id="p-0060" num="0059">The virtualization software <b>605</b> manages the operations of the VMs <b>611</b>-<b>614</b>, and includes several components for managing the access of the VMs <b>611</b>-<b>614</b> to the physical network <b>690</b> (by implementing the logical networks to which the VMs connect, in some embodiments). As illustrated, the virtualization software <b>605</b> includes several components, including a MPSE <b>620</b>, a set of MPREs <b>630</b>, a controller agent <b>640</b>, a network data storage <b>645</b>, a VTEP <b>650</b>, and a set of uplink pipelines <b>670</b>.</p><p id="p-0061" num="0060">The VTEP (virtual tunnel endpoint) <b>650</b> allows the host machine <b>600</b> to serve as a tunnel endpoint for logical network traffic (e.g., VXLAN traffic). VXLAN is an overlay network encapsulation protocol. An overlay network created by VXLAN encapsulation is sometimes referred to as a VXLAN network, or simply VXLAN. When a VM <b>611</b>-<b>614</b> on the host machine <b>600</b> sends a data packet (e.g., an Ethernet frame) to another VM in the same VXLAN network but on a different host (e.g., other machines <b>680</b>,) the VTEP <b>650</b> will encapsulate the data packet using the VXLAN network's VNI and network addresses of the VTEP <b>650</b>, before sending the packet to the physical network <b>690</b>. The packet is tunneled through the physical network (i.e., the encapsulation renders the underlying packet transparent to the intervening network elements) to the destination host. The VTEP at the destination host decapsulates the packet and forwards only the original inner data packet to the destination VM. In some embodiments, the VTEP module serves only as a controller interface for VXLAN encapsulation, while the encapsulation and decapsulation of VXLAN packets is accomplished at the uplink module <b>670</b>.</p><p id="p-0062" num="0061">The controller agent <b>640</b> receives control plane messages from a controller <b>660</b> (e.g., a CCP node) or a cluster of controllers. In some embodiments, these control plane messages include configuration data for configuring the various components of the virtualization software <b>605</b> (such as the MPSE <b>620</b> and the MPREs <b>630</b>) and/or the virtual machines <b>611</b>-<b>614</b>. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the controller agent <b>640</b> receives control plane messages from the controller cluster <b>660</b> from the physical network <b>690</b> and in turn provides the received configuration data to the MPREs <b>630</b> through a control channel without going through the MPSE <b>620</b>. However, in some embodiments, the controller agent <b>640</b> receives control plane messages from a direct data conduit (not illustrated) independent of the physical network <b>690</b>. In some other embodiments, the controller agent <b>640</b> receives control plane messages from the MPSE <b>620</b> and forwards configuration data to the router <b>630</b> through the MPSE <b>620</b>.</p><p id="p-0063" num="0062">In some embodiments, the controller agent <b>640</b> receives the multicast group information from the SDN controller and uses the multicast group information to map multicast group identifiers from overlay to underlay, and to identify which VTEPs are interested in which multicast group. The controller agent <b>640</b> also uses the multicast group information to distinguish VTEPs that are in the same network segment as the current host machine from VTEPs that are not in the same network segment. Based on the received multicast group information, the host machine <b>600</b> encapsulates multicast packets with underlay multicast group identifier, and sends multicast packet to VTEPs in different network segments by unicast.</p><p id="p-0064" num="0063">The network data storage <b>645</b> in some embodiments stores some of the data that are used and produced by the logical forwarding elements of the host machine <b>600</b> (logical forwarding elements such as the MPSE <b>620</b> and the MPRE <b>630</b>). Such stored data in some embodiments include forwarding tables and routing tables, connection mappings, as well as packet traffic statistics. These stored data are accessible by the controller agent <b>640</b> in some embodiments and delivered to another computing device (e.g., SDN controller <b>300</b>.)</p><p id="p-0065" num="0064">The MPSE <b>620</b> delivers network data to and from the physical NIC <b>695</b>, which interfaces the physical network <b>690</b>. The MPSE <b>620</b> also includes a number of virtual ports (vPorts) that communicatively interconnect the physical NIC <b>695</b> with the VMs <b>611</b>-<b>614</b>, the MPREs <b>630</b>, and the controller agent <b>640</b>. Each virtual port is associated with a unique L2 MAC address, in some embodiments. The MPSE <b>620</b> performs L2 link layer packet forwarding between any two network elements that are connected to its virtual ports. The MPSE <b>620</b> also performs L2 link layer packet forwarding between any network element connected to any one of its virtual ports and a reachable L2 network element on the physical network <b>690</b> (e.g., another VM running on another host). In some embodiments, a MPSE is a local instantiation of a logical switching element (LSE) that operates across the different host machines and can perform L2 packet switching between VMs on a same host machine or on different host machines. In some embodiments, the MPSE performs the switching function of several LSEs according to the configuration of those logical switches.</p><p id="p-0066" num="0065">The MPREs <b>630</b> perform L3 routing on data packets received from a virtual port on the MPSE <b>620</b>. In some embodiments, this routing operation entails resolving a L3 IP address to a next-hop L2 MAC address and a next-hop VNI (i.e., the VNI of the next-hop's L2 segment). Each routed data packet is then sent back to the MPSE <b>620</b> to be forwarded to its destination according to the resolved L2 MAC address. This destination can be another VM connected to a virtual port on the MPSE <b>620</b>, or a reachable L2 network element on the physical network <b>690</b> (e.g., another VM running on another host, a physical non-virtualized machine, etc.).</p><p id="p-0067" num="0066">As mentioned, in some embodiments, MPRE is a local instantiation of a logical routing element (LRE) that operates across the different host machines and can perform L3 packet forwarding between VMs on a same host machine or on different host machines. In some embodiments, a host machine may have multiple MPREs connected to a single MPSE, where each MPRE in the host machine implements a different LRE. MPREs and MPSEs are referred to as &#x201c;physical&#x201d; routing/switching elements in order to distinguish from &#x201c;logical&#x201d; routing/switching elements, even though MPREs and MPSEs are implemented in software in some embodiments. In some embodiments, a MPRE is referred to as a &#x201c;software router&#x201d; and a MPSE is referred to as a &#x201c;software switch&#x201d;. In some embodiments, LREs and LSEs are collectively referred to as logical forwarding elements (LFEs), while MPREs and MPSEs are collectively referred to as managed physical forwarding elements (MPFEs). Some of the logical resources (LRs) mentioned throughout this document are LREs or LSEs that have corresponding local MPREs or a local MPSE running in each host machine.</p><p id="p-0068" num="0067">In some embodiments, the MPRE <b>630</b> includes one or more logical interfaces (LIFs) that each serve as an interface to a particular segment (L2 segment or VXLAN) of the network. In some embodiments, each LIF is addressable by its own IP address and serves as a default gateway or ARP proxy for network nodes (e.g., VMs) of its particular segment of the network. In some embodiments, all of the MPREs in the different host machines are addressable by a same &#x201c;virtual&#x201d; MAC address (or vMAC), while each MPRE is also assigned a &#x201c;physical&#x201d; MAC address (or pMAC) in order to indicate in which host machine the MPRE operates.</p><p id="p-0069" num="0068">The uplink module <b>670</b> relays data between the MPSE <b>620</b> and the physical NIC <b>695</b>. The uplink module <b>670</b> includes an egress chain and an ingress chain that each perform a number of operations. Some of these operations are pre-processing and/or post-processing operations for the MPRE <b>630</b>.</p><p id="p-0070" num="0069">As illustrated by <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the virtualization software <b>605</b> has multiple MPREs <b>630</b> for multiple, different LREs. In a multi-tenancy environment, a host machine can operate virtual machines from multiple different users or tenants (i.e., connected to different logical networks). In some embodiments, each user or tenant has a corresponding MPRE instantiation of its LRE in the host for handling its L3 routing. In some embodiments, though the different MPREs belong to different tenants, they all share a same vPort on the MPSE, and hence a same L2 MAC address (vMAC or pMAC). In some other embodiments, each different MPRE belonging to a different tenant has its own port to the MPSE.</p><p id="p-0071" num="0070">The MPSE <b>620</b> and the MPRE <b>630</b> make it possible for data packets to be forwarded amongst VMs <b>611</b>-<b>614</b> without being sent through the external physical network <b>690</b> (so long as the VMs connect to the same logical network, as different tenants' VMs will be isolated from each other). Specifically, the MPSE <b>620</b> performs the functions of the local logical switches by using the VNIs of the various L2 segments (i.e., their corresponding L2 logical switches) of the various logical networks. Likewise, the MPREs <b>630</b> perform the function of the logical routers by using the VNIs of those various L2 segments. Since each L2 segment/L2 switch has its own a unique VNI, the host machine <b>600</b> (and its virtualization software <b>605</b>) is able to direct packets of different logical networks to their correct destinations and effectively segregate traffic of different logical networks from each other.</p><p id="p-0072" num="0071">Many of the above-described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer-readable storage medium (also referred to as computer-readable medium). When these instructions are executed by one or more processing unit(s) (e.g., one or more processors, cores of processors, or other processing units), they cause the processing unit(s) to perform the actions indicated in the instructions. Examples of computer-readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc. The computer-readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.</p><p id="p-0073" num="0072">In this specification, the term &#x201c;software&#x201d; is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor. Also, in some embodiments, multiple software inventions can be implemented as sub-parts of a larger program while remaining distinct software inventions. In some embodiments, multiple software inventions can also be implemented as separate programs. Finally, any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments, the software programs, when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>7</b></figref> conceptually illustrates a computer system <b>700</b> with which some embodiments of the invention are implemented. The computer system <b>700</b> can be used to implement any of the above-described hosts, controllers, and managers. As such, it can be used to execute any of the above-described processes. This computer system <b>700</b> includes various types of non-transitory machine-readable media and interfaces for various other types of machine-readable media. Computer system <b>700</b> includes a bus <b>705</b>, processing unit(s) <b>710</b>, a system memory <b>720</b>, a read-only memory <b>730</b>, a permanent storage device <b>735</b>, input devices <b>740</b>, and output devices <b>745</b>.</p><p id="p-0075" num="0074">The bus <b>705</b> collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of the computer system <b>700</b>. For instance, the bus <b>705</b> communicatively connects the processing unit(s) <b>710</b> with the read-only memory <b>730</b>, the system memory <b>720</b>, and the permanent storage device <b>735</b>.</p><p id="p-0076" num="0075">From these various memory units, the processing unit(s) <b>710</b> retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit(s) <b>710</b> may be a single processor or a multi-core processor in different embodiments. The read-only-memory (ROM) <b>730</b> stores static data and instructions that are needed by the processing unit(s) <b>710</b> and other modules of the computer system <b>700</b>. The permanent storage device <b>735</b>, on the other hand, is a read-and-write memory device. This device <b>735</b> is a non-volatile memory unit that stores instructions and data even when the computer system <b>700</b> is off. Some embodiments of the invention use a mass-storage device (such as a magnetic or optical disk and its corresponding disk drive) as the permanent storage device <b>735</b>.</p><p id="p-0077" num="0076">Other embodiments use a removable storage device (such as a floppy disk, flash drive, etc.) as the permanent storage device <b>735</b>. Like the permanent storage device <b>735</b>, the system memory <b>720</b> is a read-and-write memory device. However, unlike storage device <b>735</b>, the system memory <b>720</b> is a volatile read-and-write memory, such a random access memory. The system memory <b>720</b> stores some of the instructions and data that the processor needs at runtime. In some embodiments, the invention's processes are stored in the system memory <b>720</b>, the permanent storage device <b>735</b>, and/or the read-only memory <b>730</b>. From these various memory units, the processing unit(s) <b>710</b> retrieve instructions to execute and data to process in order to execute the processes of some embodiments.</p><p id="p-0078" num="0077">The bus <b>705</b> also connects to the input and output devices <b>740</b> and <b>745</b>. The input devices <b>740</b> enable the user to communicate information and select commands to the computer system <b>700</b>. The input devices <b>740</b> include alphanumeric keyboards and pointing devices (also called &#x201c;cursor control devices&#x201d;). The output devices <b>745</b> display images generated by the computer system <b>700</b>. The output devices <b>745</b> include printers and display devices, such as cathode ray tubes (CRT) or liquid crystal displays (LCD). Some embodiments include devices such as a touchscreen that function as both input and output devices <b>740</b> and <b>745</b>.</p><p id="p-0079" num="0078">Finally, as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, bus <b>705</b> also couples computer system <b>700</b> to a network <b>725</b> through a network adapter (not shown). In this manner, the computer <b>700</b> can be a part of a network of computers (such as a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), or an Intranet, or a network of networks, such as the Internet. Any or all components of computer system <b>700</b> may be used in conjunction with the invention.</p><p id="p-0080" num="0079">Some embodiments include electronic components, such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media). Some examples of such computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic and/or solid state hard drives, read-only and recordable Blu-Ray&#xae; discs, ultra-density optical discs, any other optical or magnetic media, and floppy disks. The computer-readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code, such as is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.</p><p id="p-0081" num="0080">While the above discussion primarily refers to microprocessor or multi-core processors that execute software, some embodiments are performed by one or more integrated circuits, such as application-specific integrated circuits (ASICs) or field-programmable gate arrays (FPGAs). In some embodiments, such integrated circuits execute instructions that are stored on the circuit itself.</p><p id="p-0082" num="0081">As used in this specification, the terms &#x201c;computer&#x201d;, &#x201c;server&#x201d;, &#x201c;processor&#x201d;, and &#x201c;memory&#x201d; all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification, the terms display or displaying means displaying on an electronic device. As used in this specification, the terms &#x201c;computer-readable medium,&#x201d; &#x201c;computer-readable media,&#x201d; and &#x201c;machine-readable medium&#x201d; are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral or transitory signals.</p><p id="p-0083" num="0082">While the invention has been described with reference to numerous specific details, one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. Several embodiments described above include various pieces of data in the overlay encapsulation headers. One of ordinary skill will realize that other embodiments might not use the encapsulation headers to relay all of this data.</p><p id="p-0084" num="0083">Also, several figures conceptually illustrate processes of some embodiments of the invention. In other embodiments, the specific operations of these processes may not be performed in the exact order shown and described in these figures. The specific operations may not be performed in one continuous series of operations, and different specific operations may be performed in different embodiments. Furthermore, the process could be implemented using several sub-processes, or as part of a larger macro process. Thus, one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details, but rather is to be defined by the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>at a first host machine of a plurality of host machines:<claim-text>receiving a multicast packet having an overlay multicast group identifier, wherein each of the plurality of host machines implements first-tier and second-tier edge routers through which packets pass through successively to enter an overlay network and to exit the overlay network, each host machine addressable by a unique virtual tunnel endpoint (VTEP), wherein a particular first-tier edge router implemented on the first host machine is a multicast querier of a first network segment associated with the multicast packet;</claim-text><claim-text>mapping the overlay multicast group identifier to an underlay multicast group identifier;</claim-text><claim-text>encapsulating the multicast packet with an encapsulation header that includes the underlay multicast group identifier to create an encapsulated multicast packet; and</claim-text><claim-text>forwarding the encapsulated multicast packet to a physical switch of the first network segment, wherein the physical switch sends copies of the encapsulated multicast packet to a set of second-tier edge routers that are implemented at a set of other host machines and that are determined to be interested in the underlay multicast group identifier.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the multicast packet is received from an overlay network and the first host machine hosts a second-tier edge router that serves data traffic to and from the overlay network.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the particular first-tier edge router serves data traffic to and from an external network,</claim-text><claim-text>the packet is received from the external network and the first host machine hosts the particular first-tier edge router.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the particular first-tier edge router is actively serving data traffic to and from the external network and other first-tier edge routers implemented by other host machines are standing by and not actively serving data traffic to and from the external network.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the particular first-tier edge router<claim-text>(i) receives a multicast packet having an overlay multicast group identifier from the external network;</claim-text><claim-text>(ii) maps the overlay multicast group identifier to the underlay multicast group identifier;</claim-text><claim-text>(iii) encapsulates the multicast packet with an encapsulation header that includes the underlay multicast group identifier; and</claim-text><claim-text>(iv) forwards the encapsulated multicast packet to the physical switch of the first network segment, wherein the physical switch sends copies of the encapsulated multicast packet at one or more ports to host machines having second-tier edge routers that are determined to be interested in the underlay multicast group identifier.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the particular first-tier edge router receives a copy of the encapsulated multicast packet from the physical switch and forwards a decapsulated copy of the multicast packet to the external network without the underlay multi cast group identifier.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a second-tier edge router receiving a copy of the encapsulated multicast packet decapsulates the packet to remove the underlay multicast group identifier and forwards the decapsulated packet to an overlay network by multicast based on the overlay multicast group identifier.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a network controller sends multicast grouping information associating an overlay multicast group identifier with (i) a corresponding underlay multicast group identifier and (ii) a list of VTEPs that are interested in the multicast group to each first-tier and second-tier edge routers.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the network controller receives multicast reports associating VTEPs with overlay multicast group identifiers and generates the multicast grouping information based on the received multicast reports.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the list of VTEPs that are interested in a multicast group sent to a particular second-tier edge router distinguishes (i) VTEPs connected to a same network segment as the particular second-tier edge router from (ii) VTEPs connected to a different network segment as the particular second-tier edge router.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising identifying a VTEP at a second network segment having a second-tier edge router that is interested in the underlay multicast group identifier and forwarding the packet to the identified VTEP by unicast.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the particular first-tier edge router implemented by one of the plurality of host machines is a multicast querier of a network segment.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A first host machine that is part of a plurality of host machines, the first host machine comprising:<claim-text>one or more processors; and</claim-text><claim-text>a computer-readable storage medium storing a plurality of computer-executable components that are executable by the one or more processors to perform a plurality of actions, the plurality of actions comprising:</claim-text><claim-text>receiving a multicast packet having an overlay multicast group identifier, wherein each of the plurality of host machines implements first-tier and second-tier edge routers through which packets pass through successively to enter an overlay network and to exit the overlay network, each host machine addressable by a unique virtual tunnel endpoint (VTEP), wherein a particular first-tier edge router implemented on the first host machine is a multicast querier of a first network segment associated with the multicast packet;</claim-text><claim-text>mapping the overlay multicast group identifier to an underlay multicast group identifier;</claim-text><claim-text>encapsulating the multicast packet with an encapsulation header that includes the underlay multicast group identifier to create an encapsulated multicast packet; and</claim-text><claim-text>forwarding the encapsulated multicast packet to a physical switch of the first network segment, wherein the physical switch sends copies of the encapsulated multicast packet to a set of second-tier edge routers that are implemented at a set of other host machines and that are determined to be interested in the underlay multicast group identifier.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The first host machine of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the particular first-tier edge router<claim-text>(i) receives a multicast packet having an overlay multicast group identifier from an external network;</claim-text><claim-text>(ii) maps the overlay multicast group identifier to the underlay multicast group identifier;</claim-text><claim-text>(iii) encapsulates the multicast packet with an encapsulation header that includes the underlay multicast group identifier; and</claim-text><claim-text>(iv) forwards the encapsulated multicast packet to the physical switch of the first network segment, wherein the physical switch sends copies of the encapsulated multicast packet at one or more ports to host machines having second-tier edge routers that are determined to be interested in the underlay multicast group identifier.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The first host machine of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a second-tier edge router receiving a copy of the encapsulated multicast packet decapsulates the packet to remove the underlay multicast group identifier and forwards the decapsulated packet to an overlay network by multicast based on the overlay multicast group identifier.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The first host machine of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a network controller sends multicast grouping information associating an overlay multicast group identifier with (i) a corresponding underlay multicast group identifier and (ii) a list of VTEPs that are interested in the multicast group to each first-tier and second-tier edge routers.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The first host machine of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the network controller receives multicast reports associating VTEPs with overlay multicast group identifiers and generates the multicast grouping information based on the received multicast reports.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The first host machine of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the list of VTEPs that are interested in a multicast group sent to a particular second-tier edge router distinguishes (i) VTEPs connected to a same network segment as the particular second-tier edge router from (ii) VTEPs connected to a different network segment as the particular second-tier edge router.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The first host machine of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the plurality of actions further comprises identifying a VTEP at a second network segment having a second-tier edge router that is interested in the underlay multicast group identifier and forwarding the packet to the identified VTEP by unicast.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The first host machine of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the particular first-tier edge router implemented by one of the plurality of host machines is a multicast querier of a network segment.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first-tier edge routers are provider edge routers and the second-tier edge routers are tenant edge routers.</claim-text></claim></claims></us-patent-application>