<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005296A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005296</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782116</doc-number><date>20200514</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>201911211382.1</doc-number><date>20191202</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>172</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>45</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>168</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>443</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">OBJECT RECOGNITION METHOD AND APPARATUS, ELECTRONIC DEVICE AND READABLE STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ZHEJIANG UNIVIEW TECHNOLOGIES CO., LTD.</orgname><address><city>Zhejiang</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Hui</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/090131</doc-number><date>20200514</date></document-id><us-371c12-date><date>20220602</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided is an object recognition method which includes obtaining a first visible-light image acquired by the first camera device and a second visible-light image acquired by the second camera device; performing exposure processing on the first visible-light image according to the luminance information of the bright area image of the first visible-light image and performing exposure processing on the second visible-light image according to the luminance information of the dark area images of the first visible-light image and/or the second visible-light image, where the dark area image is an area image having a luminance value less than or equal to the preset value; and performing target object detection on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing and recognizing and verifying a target object according to the detection result.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="92.79mm" wi="157.73mm" file="US20230005296A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="187.11mm" wi="159.34mm" file="US20230005296A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="145.54mm" wi="168.06mm" file="US20230005296A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="185.84mm" wi="161.46mm" file="US20230005296A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="186.44mm" wi="163.91mm" file="US20230005296A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="78.91mm" wi="68.66mm" file="US20230005296A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application claims priority to Chinese Patent Application No. 201911211382.1 filed with the China National Intellectual Property Administration (CNIPA) on Dec. 2, 2019, the disclosure of which is incorporated herein by reference in its entirety.</p><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present application relates to the field of image recognition technology, for example, an object recognition method and apparatus, an electronic device and a readable storage medium.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In a wide dynamic range scenario, that is, a scenario in which an acquired image includes a bright area image and a dark area image, a recognized object such as a face area in the acquired image is prone to overexposure, overdarkness or ghosting due to the complexity of an environment when a conventional exposure strategy is used, resulting in the failure to effectively detect and recognize faces. There are two processing methods commonly used in the related art. One is the use of digital wide dynamic range technology, and the other is the use of optical wide dynamic range technology, that is, the fusion processing of multiple frames of images.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">The present application provides an object recognition method and apparatus, an electronic device and a readable storage medium to improve the adaptability and the recognition accuracy of object recognition.</p><p id="p-0006" num="0005">An embodiment of the present application provides an object recognition method. The method is applied to an electronic device including a first camera device and a second camera device. The method includes the steps below.</p><p id="p-0007" num="0006">A first visible-light image acquired by the first camera device and a second visible-light image acquired by the second camera device are obtained.</p><p id="p-0008" num="0007">Exposure processing is performed on the first visible-light image according to the luminance information of the bright area image of the first visible-light image. Exposure processing is performed on the second visible-light image according to the luminance information of the dark area images of the first visible-light image and/or the second visible-light image. The bright area image is an area image having a luminance value greater than a preset value. The dark area image is an area image having a luminance value less than or equal to the preset value.</p><p id="p-0009" num="0008">Target object detection is performed on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing. A target object is recognized and verified according to the detection result.</p><p id="p-0010" num="0009">An embodiment of the present application provides an object recognition apparatus. The apparatus is applied to the electronic device including the first camera device and the second camera device. The apparatus includes an image obtaining module, an exposure processing module and a recognition module.</p><p id="p-0011" num="0010">The image obtaining module is configured to obtain the first visible-light image acquired by the first camera device and the second visible-light image acquired by the second camera device.</p><p id="p-0012" num="0011">The exposure processing module is configured to perform exposure processing on the first visible-light image according to the luminance information of the bright area image of the first visible-light image and perform exposure processing on the second visible-light image according to the luminance information of the dark area images of first visible-light image and/or the second visible-light image. The bright area image is the area image having the luminance value greater than the preset value. The dark area image is the area image having the luminance value less than or equal to the preset value.</p><p id="p-0013" num="0012">The recognition module is configured to perform target object detection on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing and recognize and verify the target object according to the detection result.</p><p id="p-0014" num="0013">An embodiment of the present application provides an electronic device. The device includes a memory, a processor and a computer program stored on the memory and executable on the processor. When executing the computer program, the processor performs the preceding object recognition method.</p><p id="p-0015" num="0014">An embodiment of the present application provides a readable storage medium. The readable storage medium stores a computer program. When executing the computer program, a processor performs the preceding object recognition method.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an electronic device according to an embodiment of the present application.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of an object recognition method according to an embodiment of the present application.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view illustrating image acquisition performed by an electronic device according to an embodiment of the present application.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of an exposure processing method for a second visible-light image according to an embodiment of the present application.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a recognition and verification method for a target object according to an embodiment of the present application.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a liveness detection method for a target object according to an embodiment of the present application.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a liveness detection method for a target object according to an embodiment of the present application.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram of function modules of an object recognition apparatus according to an embodiment of the present application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">REFERENCE LIST</heading><p id="p-0024" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0023"><b>10</b> electronic device</li>        <li id="ul0002-0002" num="0024"><b>100</b> processor</li>        <li id="ul0002-0003" num="0025"><b>200</b> memory</li>        <li id="ul0002-0004" num="0026"><b>300</b> object recognition apparatus</li>        <li id="ul0002-0005" num="0027"><b>310</b> image obtaining module</li>        <li id="ul0002-0006" num="0028"><b>320</b> exposure processing module</li>        <li id="ul0002-0007" num="0029"><b>330</b> recognition module</li>    </ul>    </li></ul></p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0030">Technical solutions in the embodiments of the present application will be described in conjunction with drawings in the embodiments of the present application. The embodiments described herein are part, not all, of the embodiments of the present application. Generally, the components of this embodiment of the present application described and illustrated in the drawings herein may be arranged and designed through multiple configurations.</p><p id="p-0026" num="0031">The description of the embodiments of the present application shown in the drawings herein is not intended to limit the scope of the present application, but merely illustrates the selected embodiments of the present application.</p><p id="p-0027" num="0032">Similar reference numerals and letters indicate similar items in the drawings, and therefore, once an item is defined in one drawing, the item needs no definition and explanation in subsequent drawings.</p><p id="p-0028" num="0033">In the description of the present application, if the orientational or positional relationships indicated by terms &#x201c;above&#x201d;, &#x201c;below&#x201d;, &#x201c;inside&#x201d;, &#x201c;outside&#x201d; and the like are based on the orientational or positional relationships illustrated in the drawings or the orientational or positional relationship that products of the present application are usually used in, which are for the mere purpose of facilitating and simplifying the description of the present application and do not indicate or imply that the apparatus or element referred to has a specific orientation and is constructed and operated in a specific orientation, and thus it is not to be construed as limiting the present application.</p><p id="p-0029" num="0034">Terms &#x201c;first&#x201d; and &#x201c;second&#x201d; are merely for distinguishing the description and are not to be construed as indicating or implying relative importance.</p><p id="p-0030" num="0035">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an embodiment of the present application provides an electronic device <b>10</b>. The electronic device <b>10</b> may be an access control device, such as an access control device mounted at a gate of a neighborhood or an access control device at an entrance of an office building. The electronic device <b>10</b> may include a memory <b>200</b>, a processor <b>100</b> and a computer program stored on the memory <b>200</b> and executable on the processor <b>100</b>. When the processor <b>100</b> executes the program, the electronic device <b>10</b> performs the object recognition method of the present application.</p><p id="p-0031" num="0036">The memory <b>200</b> is electrically connected to the processor <b>100</b> in a direct manner or an indirect manner to implement data transmission or interaction. For example, these components may electrically connected through one or more communication buses or signal lines. The memory <b>200</b> stores a software function module that is stored in the memory <b>200</b> in the form of software or firmware. The processor <b>100</b> runs the software programs and modules stored in the memory <b>200</b> to perform multiple function applications and data processing, that is, to perform the object recognition method provided by the embodiments of the present application.</p><p id="p-0032" num="0037">The electronic device <b>10</b> may further include camera devices. In this embodiment, the electronic device <b>10</b> may include a first camera device and a second camera device. The first camera device and the second camera device may be disposed apart. For example, the first camera device and the second camera device are disposed on the left side and the right side of a surveillance scenario separately to perform image acquisition from different angles. The first camera device and the second camera device may be integrated in the electronic device <b>10</b> to be electrically connected to other components included in the electronic device <b>10</b>. The first camera device and the second camera device may also be connected to other components in the electronic device <b>10</b> through wires or wireless communication to implement data transmission and information transmission.</p><p id="p-0033" num="0038">The structure shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is merely illustrative. The electronic device <b>10</b> may further include more or fewer components than the components shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> or may have a configuration different from the configuration shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Multiple components shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be implemented by hardware, software or a combination thereof.</p><p id="p-0034" num="0039">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of an object recognition method applied to the electronic device <b>10</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Multiple steps included in the method are described below.</p><p id="p-0035" num="0040">In step S<b>110</b>, a first visible-light image acquired by the first camera device and a second visible-light image acquired by the second camera device are obtained.</p><p id="p-0036" num="0041">In step S<b>120</b>, exposure processing is performed on the first visible-light image according to the luminance information of the bright area image of the first visible-light image, and exposure processing is performed on the second visible-light image according to the luminance information of the dark area images of the first visible-light image and/or the second visible-light image.</p><p id="p-0037" num="0042">In step S<b>130</b>, target object detection is performed on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing, and a target object is recognized and verified according to the detection result.</p><p id="p-0038" num="0043">In a wide dynamic range scenario, there is often a scenario in which, for example, the electronic device <b>10</b> is mounted at an entrance, and the outside of the entrance is a bright area scenario. When a person stands in an area outside the entrance, and the face is in a bright area environment, exposure processing is performed based on the information of the bright area environment to obtain a clear face image. When the person moves from the area outside the entrance to the entrance, and the face is in a dark area environment, exposure processing is performed based on the information of the dark area environment to obtain a clear face image.</p><p id="p-0039" num="0044">In this embodiment, the first camera device and the second camera device are provided with infrared filters. Infrared light in an environment can be filtered out by the infrared filters to take and obtain a visible-light image. During the implementation, the first camera device is controlled to turn on a configured infrared filter to acquire the first visible-light image, and the second camera device is controlled to turn on a configured infrared filter to acquire the second visible-light image.</p><p id="p-0040" num="0045">In this embodiment, the first camera device may acquire an image from a left viewing angle, and the second camera device may acquire an image from a right viewing angle. Processing such as calibration alignment and partial clipping may be performed on the image taken by the first camera device and the image taken by the second camera device to clip a first detection area in the image taken by the first camera device and clip a second detection area in the image taken by the second camera device as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The image included in the first detection area is the same as the image included in the second detection area.</p><p id="p-0041" num="0046">The first visible-light image obtained by the first camera device may be the image in the first detection area. The second visible-light image obtained by the second camera device may be the image in the second detection area.</p><p id="p-0042" num="0047">The bright area image of the first visible-light image may be obtained. The bright area image may be an area image having a luminance value greater than a preset value. For example, the first visible-light image may be divided into multiple area units, for example, multiple area units of multiple rows and columns, or may be divided in another manner. This is not limited in this embodiment. Then the luminance value of the image in each area unit is computed, and an area unit having a luminance value greater than the preset value is used as a bright area image.</p><p id="p-0043" num="0048">The dark area image of the second visible-light image may be obtained in the preceding manner. The dark area image is an area image having a luminance value less than or equal to the preset value.</p><p id="p-0044" num="0049">Exposure processing is performed on the first visible-light image according to the luminance information of the bright area image of the first visible-light image to obtain a clear image of the bright area by exposure. At the same time, exposure processing is performed on the second visible-light image according to the luminance information of the dark area images of the first visible-light image and/or the second visible-light image to obtain a clear image of the dark area in the second visible-light image by exposure. For example, exposure processing may be performed on the second visible-light image according to the luminance information of the dark area image of the first visible-light image. Exposure processing may also be performed on the second visible-light image according to the luminance information of the dark area image of the second visible-light image. Alternatively, exposure processing may also be performed on the second visible-light image according to the combination of the luminance information of the dark area images of the first visible-light image and the second visible-light image.</p><p id="p-0045" num="0050">As can be seen from the preceding description, the target object may be located in a bright area or a dark area. If the target object is located in a bright area, a relatively clear image of the target object can be obtained by performing exposure processing on the first visible-light image. If the target object is located in a dark area, a relatively clear image of the target object can be obtained by performing exposure processing on the second visible-light image.</p><p id="p-0046" num="0051">Therefore, in this embodiment, target object detection is performed based on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing, and the target object is recognized and verified according to the detection result.</p><p id="p-0047" num="0052">Through the preceding manners, the electronic device <b>10</b> having a dual camera can be fully utilized to implement object recognition applicable to a wide dynamic range scenario, thereby improving the adaptability and accuracy of the recognition.</p><p id="p-0048" num="0053">In this embodiment, during exposure processing, a first photometric value is obtained according to the luminance information of the bright area image of the first visible-light image, and exposure processing is performed on the first visible-light image based on the first photometric value. Similarly, a second photometric value is obtained according to the luminance information of the dark area images of the first visible-light image and/or the second visible-light image, and exposure processing is performed on the second visible-light image based on the second photometric value.</p><p id="p-0049" num="0054">During the implementation, when the second photometric value of the dark area image of the second visible-light image is computed, a manner is to directly perform the computation based on the luminance information of the dark area images of the first visible-light image and/or the second visible-light image. Additionally, in view that the most of an image is generally a bright area, the proportion of a dark area is very small, and the luminance information of the dark area is generally difficult to compute, as a possible implementation, the computation may be performed based on preobtained target luminance information. The target luminance information may be obtained in the following manner: Multiple different wide dynamic range scenarios are prebuilt to obtain multiple images having different wide dynamic range values; and dark area images of the multiple images are analyzed to obtain the luminance information of the dark area images of the multiple images. The luminance information of a dark area image that appears most frequently, that is, the most common luminance information of a dark area image, may be used as target luminance information. Average processing or weighted average processing may be performed on the luminance information of the dark area images of the multiple images, and the obtained result may be used as the target luminance information. That is, the luminance information of the dark area images of the first visible-light image and/or the second visible-light image may be configured as the target luminance information.</p><p id="p-0050" num="0055">When exposure processing is performed on the second visible-light image, the second photometric value is obtained according to the target luminance information, and exposure processing is performed on the second visible-light image based on the second photometric value. After the exposure processing, the second visible-light image may be fine-tuned according to the processing effect to make the image effect better.</p><p id="p-0051" num="0056">In another implementation, the second photometric value of the dark area image of the second visible-light image may also be indirectly computed based on the bright area images of the first visible-light image and/or the second visible-light image. Optionally, referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in this implementation, exposure processing is performed on the second visible-light image by the steps below.</p><p id="p-0052" num="0057">In step S<b>210</b>, the luminance information of the bright area images of the first visible-light image and/or the second visible-light image is obtained.</p><p id="p-0053" num="0058">In step S<b>220</b>, the luminance information of the dark area images of the first visible-light image and/or the second visible-light image is computed according to the luminance information of the bright area images of the first visible-light image and/or the second visible-light image and the precomputed correspondence between the luminance information of the bright area image and the luminance information of the dark area image.</p><p id="p-0054" num="0059">In step S<b>230</b>, the second photometric value is obtained according to the computed luminance information of the dark area images of the first visible-light image and/or the second visible-light image, and exposure processing is performed on the second visible-light image based on the second photometric value.</p><p id="p-0055" num="0060">In this embodiment, multiple images in a wide dynamic range scenario may be preobtained, and there are no requirements for these images. In this situation, as a possible implementation, these images may be multiple images obtained in a wide dynamic range scenario built in a laboratory, or images taken by a camera device in the same taking scenario as the taking scenario of the second camera device, as long as images in a wide dynamic range scenario including bright areas and dark areas. After these images in the wide dynamic range scenario are processed, the general correspondence between a bright area image and a dark area image in the wide dynamic range scenario, that is, experience data based on big data may be obtained. Based on the experience data, in the case where the luminance information of a bright area is obtained, the corresponding luminance information of a dark area image may be obtained.</p><p id="p-0056" num="0061">In another implementation, the multiple obtained images may also be historical images taken during a historical period by the second camera device or a camera device having the same device parameter as the second camera device. The multiple obtained images may be images taken in the same scenario as the taking scenario of the second camera device.</p><p id="p-0057" num="0062">For the multiple obtained images, the bright area image and the dark area image of each image before exposure processing is performed may be obtained, and the luminance information of the bright area image and the dark area image is computed. In this manner, different correspondences may be obtained according to the luminance information of bright area images and dark area images of the multiple images. Then in the case where the luminance information of a bright area is obtained, the corresponding luminance information of a dark area is obtained according to the correspondence.</p><p id="p-0058" num="0063">For the to-be-processed second visible-light image, the luminance information of the bright area images of the first visible-light image and/or the second visible-light image is firstly obtained, and then the luminance information of the dark area images corresponding to the luminance information of the bright area images of the first visible-light image and/or the second visible-light image is obtained based on the precomputed correspondence. This luminance information is used as the luminance information of the dark area images of the first visible-light image and/or the second visible-light image. The second photometric value is computed based on the computed luminance information of the dark area images, and exposure processing is performed on the second visible-light image based on the second photometric value. In this manner, when the proportion of the dark area image in an acquired image is very small, and it is not convenient to compute the photometric value, the luminance information of the dark area image may be computed indirectly according to the luminance information of the bright area image in the image to compute the photometric value for exposure processing.</p><p id="p-0059" num="0064">Whether exposure processing is performed on the second visible-light image based on the preceding experience data or the historical images, it is to be noted that the illumination conditions should not differ greatly. For example, exposure processing may be performed on the second visible-light image based on the correspondence obtained in a time period corresponding to the current time period. If the current time period is morning time, the preceding correspondence in the morning time period is obtained. Alternatively, exposure processing may be performed on the second visible-light image based on the correspondence obtained in a situation equivalent to the current weather condition. If the current weather condition is rainy weather, the preceding correspondence in the rainy weather is obtained. In this manner, it can be ensured that the based correspondence may be obtained in a scenario relatively consistent with the current scenario. Thus, the accuracy of the indirectly obtained luminance information of a dark area image can be improved.</p><p id="p-0060" num="0065">In this embodiment, when object recognition and verification are performed, infrared detection may also be performed on the target object to thereby perform liveness detection. In this manner, a verification error caused by frauds such as the use of an image including a face image and a recaptured screen image is avoided. Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the target object is recognized and verified in the manners below.</p><p id="p-0061" num="0066">In step S<b>1310</b>, target object detection is performed on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing, and a target device is determined from the first camera device and the second camera device according to the detection result.</p><p id="p-0062" num="0067">In step S<b>1320</b>, the target object is recognized and verified according to an image acquired by the determined target device, and a non-target device in the first camera device and the second camera device is controlled to perform liveness detection on the target object.</p><p id="p-0063" num="0068">In this embodiment, the first camera device and the second camera device are further provided with visible-light filters. Through a visible-light filter, visible light in a scenario can be filtered out to obtain an infrared image. When the target object is verified, it is necessary to use the first camera device and the second camera device in combination. Object detection is performed on an acquired visible-light image by one camera device. Liveness detection is performed on an acquired infrared image by another camera device.</p><p id="p-0064" num="0069">In this embodiment, when the target object is detected in the first visible-light image obtained after exposure processing, and no target object is detected in the second visible-light image obtained after exposure processing, the first visible-light image acquired by the first camera device is determined to use for recognizing and verifying the target object. In this situation, the first camera device is the target device, and the second camera device is a non-target device. The target object is recognized and verified based on the first visible-light image acquired by the first camera device. The second camera device is controlled to perform liveness detection.</p><p id="p-0065" num="0070">When no target object is detected in the first visible-light image obtained after exposure processing, and the target object is detected in the second visible-light image obtained after exposure processing, the second visible-light image acquired by the second camera device is determined to use for recognizing and verifying the target object. In this situation, the second camera device is the target device, and the first camera device is the non-target device. The target object is recognized and verified based on the second visible-light image acquired by the second camera device. The first camera device is controlled to perform liveness detection.</p><p id="p-0066" num="0071">If the target object is detected in the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing, the sharpness of the target object detected in the first visible-light image may be compared with the sharpness of the target object detected in the second visible-light image. If the sharpness of the target object in the first visible-light image is higher, the first visible-light image acquired by the first camera device is determined to use for recognizing and verifying the target object. Similarly, in this situation, the first camera device is the target device, and the second camera device is the non-target device. The target object is recognized and verified based on the first visible-light image acquired by the first camera device. The second camera device is controlled to perform liveness detection. If the sharpness of the target object in the second visible-light image is higher, the second visible-light image acquired by the second camera device is determined to use for recognizing and verifying the target object. In this situation, the second camera device is the target device, and the first camera device is the non-target device. The target object is recognized and verified based on the second visible-light image acquired by the second camera device. The first camera device is controlled to perform liveness detection. Finally, the target object is comprehensively verified by combining the recognition and verification results of the target object and the liveness detection result of the target object.</p><p id="p-0067" num="0072">For example, if liveness detection is performed by the first camera device, the target object is comprehensively verified by combining the liveness detection result of the first camera device and the recognition and verification results obtained by the second camera device. If the liveness detection is performed by the second camera device, the target object is comprehensively verified by combining the liveness detection result of the second camera device and the recognition and verification results obtained by the first camera device.</p><p id="p-0068" num="0073">When the liveness detection result indicates that the target object is a real human body, and the target object detected based on a visible-light image passes the detection, it may be determined that the target object passes the verification.</p><p id="p-0069" num="0074">In this embodiment, when the second camera device is determined as the target device, that is, liveness detection is performed by the first camera device, referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the detection process may be implemented by the steps below.</p><p id="p-0070" num="0075">In step S<b>13110</b>, the target object is recognized and verified according to the second visible-light image acquired by the second camera device.</p><p id="p-0071" num="0076">In step S<b>13120</b>, the first camera device is controlled to turn on a configured visible-light filter to acquire a first infrared image.</p><p id="p-0072" num="0077">In step S<b>13130</b>, liveness detection is performed on the target object according to the first infrared image.</p><p id="p-0073" num="0078">If the first camera device is determined to use to perform liveness detection, it indicates that the target object information in the second visible-light image obtained after exposure processing based on the dark area image in the second visible-light image is better, for example, the sharpness is higher, and the luminance is higher. That is, the second photometric value is used for performing exposure processing, and the effect is better. Therefore, when living object detection is performed on the target object, the first camera device is controlled to switch a filter to enable the configured visible-light filter to acquire the first infrared image, and then liveness detection is performed on the target object according to the acquired first infrared image.</p><p id="p-0074" num="0079">When an infrared image is used for performing liveness detection on the target object, the detection may mainly be performed in the following manner: For example, it is detected whether there is a light spot in the eye area of the face image, if a real human body is determined, a light spot should be present in the eyes in the infrared image; or it is possible to judge whether or not a human body is real by judging the three-dimensional (3D) characteristics of a face area; or it is possible to judge whether or not the human body is real by judging the authenticity of a face skin through the use of subsurface scattering; or liveness detection may also be performed in other manners with reference to the liveness detection method in the related art, and this is not limited in this embodiment.</p><p id="p-0075" num="0080">If the first camera device is determined as the target device, that is, liveness detection is performed by the second camera device, referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the detection processing may be performed in the manners below.</p><p id="p-0076" num="0081">In step S<b>13140</b>, the target object is recognized and verified according to the first visible-light image acquired by the first camera device.</p><p id="p-0077" num="0082">In step S<b>13150</b>, the second camera device is controlled to turn on a configured visible-light filter to acquire a second infrared image.</p><p id="p-0078" num="0083">In step S<b>13160</b>, liveness detection is performed on the target object according to the second infrared image.</p><p id="p-0079" num="0084">If the second camera device is used to perform liveness detection on the target object, it indicates that the target object information in the first visible-light image obtained after exposure processing based on the bright area image in the first visible-light image is better. That is, after the first photometric value is used for performing exposure processing, the image effect is better. Therefore, the second camera device may be controlled to switch a filter to enable the configured visible-light filter to obtain the second infrared image. Then liveness detection is performed on the target object according to the acquired second infrared image.</p><p id="p-0080" num="0085">When the second infrared image is used for performing liveness detection on the target object, the detection method may refer to the preceding description of performing living object detection on the target object based on the first infrared image, and the details are not repeated here.</p><p id="p-0081" num="0086">In this embodiment, the electronic device <b>10</b> may further include a display screen. When the first camera device is used to perform liveness detection, and the target object information in the second camera device is used for performing object detection, the second visible-light image taken by the second camera device may be displayed on the display screen. If the second camera device is used to perform liveness detection, and the target object information in the first camera device is used for performing object detection, the first visible-light image taken by the first camera device may be displayed on the display screen.</p><p id="p-0082" num="0087">The electronic device <b>10</b> may further include a first fill light and a second fill light. Each of the first fill light and the second fill light may be a combination of an infrared fill light and a visible fill light. The first fill light may be configured to perform fill light operation on the first camera device. The second fill light may be configured to perform fill light operation on the second camera device.</p><p id="p-0083" num="0088">In this embodiment, the phenomenon that the current scenario is not a wide dynamic range scenario may also occur, and in this case, it is not necessary to distinguish between a bright area image and a dark area image. Optionally, in this case, the first camera device is controlled to turn on the configured infrared filter to acquire a visible-light image, and exposure processing is performed on the visible-light image based on the luminance information of the visible-light image. Then target object detection is performed based on the visible-light image obtained after exposure processing, such as face detection. The visible-light image obtained after exposure processing is output to the display screen for display. Moreover, optionally, the second camera device is controlled to turn on the configured visible-light filter to acquire an infrared image, and exposure processing is performed on the infrared image based on the luminance information of the infrared image. Then liveness detection is performed on the target object based on the infrared image obtained after exposure processing, and the detection result of the liveness detection and the face detection result are combined to recognize and verify the target object.</p><p id="p-0084" num="0089">Alternatively, the first camera device may also be controlled to turn on the visible-light filter to acquire an infrared image, and the second camera device may also be controlled to turn on the infrared filter to acquire a visible-light image. This is not limited in this embodiment and may be adjusted according to needs.</p><p id="p-0085" num="0090">In the object recognition scheme provided by this embodiment, for a wide dynamic range scenario, based on the electronic device <b>10</b> including the dual camera and without adding more cameras, exposure processing is performed based on the bright area image and dark area image of an image acquired by a camera to implement target object detection when the object is in a bright area or a dark area; then a camera device is used to acquire an infrared image to perform living object detection on the target object; and the target object is recognized and verified in combination with the living object detection result and the target object information in a visible-light image. In this manner, the adaptability of object detection and the recognition accuracy in the wide dynamic range scenario are improved.</p><p id="p-0086" num="0091">To perform the corresponding steps in the preceding embodiment and multiple possible manners, referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, an implementation of an object recognition apparatus <b>300</b> is described below. Optionally, the object recognition device <b>300</b> may be applied to the preceding electronic device <b>10</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The object recognition apparatus <b>300</b> provided by this embodiment has the same basic principles and technical effects as those of the preceding embodiment. For a brief description, for the parts not mentioned in this embodiment, reference may be made to the corresponding contents in the preceding embodiment. The object recognition apparatus <b>300</b> includes an image obtaining module <b>310</b>, an exposure processing module <b>320</b> and a recognition module <b>330</b>.</p><p id="p-0087" num="0092">The image obtaining module <b>310</b> is configured to obtain the first visible-light image acquired by the first camera device and the second visible-light image acquired by the second camera device. The image obtaining module <b>310</b> may be used to perform step S<b>110</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the operation method may refer to the description of step S<b>110</b>.</p><p id="p-0088" num="0093">The exposure processing module <b>320</b> is configured to perform exposure processing on the first visible-light image according to the luminance information of the bright area image of the first visible-light image and perform exposure processing on the second visible-light image according to the luminance information of the dark area images of first visible-light image and/or the second visible-light image. The bright area image is the area image having the luminance value greater than the preset value. The dark area image is the area image having the luminance value less than or equal to the preset value. The exposure processing module <b>320</b> may be used to perform step S<b>120</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the operation method may refer to the description of step S<b>120</b>.</p><p id="p-0089" num="0094">The recognition module <b>330</b> is configured to perform target object detection on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing and recognize and verify the target object according to the detection result. The recognition module <b>330</b> may be configured to perform step S<b>130</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the operation method may refer to the description of step S<b>130</b>.</p><p id="p-0090" num="0095">Another embodiment of the present application provides a readable storage medium including computer-executable instructions. When the computer-executable instructions are executed by a computer processor, the related operations of the object recognition method according to any embodiment of the present application are performed.</p><p id="p-0091" num="0096">The embodiments of the present application provide an object recognition method and apparatus, an electronic device <b>10</b> and a readable storage medium. The electronic device <b>10</b> includes a first camera device and a second camera device. The first visible-light image acquired by the first camera device and the second visible-light image acquired by the second camera device are obtained. Exposure processing is performed on the first visible-light image according to the luminance information of the bright area image of the first visible-light image, and exposure processing is performed on the second visible-light image according to the luminance information of the dark area images of the first visible-light image and/or the second visible-light image. Finally, target object detection is performed on the first visible-light image obtained after exposure processing and the second visible-light image obtained after exposure processing, and the target object is recognized and verified according to the detection result. In this manner, object recognition may be performed according to images obtained under different exposure strategies. Thus, detection and recognition of objects in different luminance areas can be effectively performed, and the adaptability and accuracy of the recognition can be improved.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An object recognition method, the method being applied to an electronic device comprising a first camera device and a second camera device and the method comprising:<claim-text>obtaining a first visible-light image acquired by the first camera device and a second visible-light image acquired by the second camera device;</claim-text><claim-text>performing exposure processing on the first visible-light image according to luminance information of a bright area image of the first visible-light image and performing exposure processing on the second visible-light image according to luminance information of a dark area image of at least one of the first visible-light image or the second visible-light image, wherein the bright area image is an area image having a luminance value greater than a preset value, and the dark area image is an area image having a luminance value less than or equal to the preset value; and</claim-text><claim-text>performing target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognizing and verifying a target object according to a detection result.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognizing and verifying the target object according to the detection result comprise:<claim-text>performing the target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and determining a target device from the first camera device and the second camera device according to the detection result; and</claim-text><claim-text>recognizing and verifying the target object according to an image acquired by the determined target device and controlling a non-target device in the first camera device and the second camera device to perform liveness detection on the target object.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the first visible-light image acquired by the first camera device and the second visible-light image acquired by the second camera device comprises:<claim-text>controlling the first camera device to turn on a configured infrared filter to acquire the first visible-light image and controlling the second camera device to turn on a configured infrared filter to acquire the second visible-light image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognizing and verifying the target object according to the detection result comprise:<claim-text>in a case where the target object is detected in the first visible-light image obtained after the exposure processing, and no target object is detected in the second visible-light image obtained after the exposure processing, determining to recognize and verify the target object by using the first visible-light image acquired by the first camera device; or</claim-text><claim-text>in a case where no target object is detected in the first visible-light image obtained after the exposure processing, and the target object is detected in the second visible-light image obtained after the exposure processing, determining to recognize and verify the target object by using the second visible-light image acquired by the second camera device; or</claim-text><claim-text>in a case where the target object is detected in the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing, comparing a sharpness of the target object detected in the first visible-light image with a sharpness of the target object detected in the second visible-light image; in a case where the sharpness of the target object in the first visible-light image is higher than the sharpness of the target object in the second visible-light image, determining to recognize and verify the target object by using the first visible-light image acquired by the first camera device; and in a case where the sharpness of the target object in the first visible-light image is not higher than the sharpness of the target object in the second visible-light image, determining to recognize and verify the target object by using the second visible-light image acquired by the second camera device.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein in a case where the second camera device is determined to be the target device, recognizing and verifying the target object according to the image acquired by the determined target device and controlling the non-target device in the first camera device and the second camera device to perform the liveness detection on the target object comprise:<claim-text>recognizing and verifying the target object according to the second visible-light image acquired by the second camera device;</claim-text><claim-text>controlling the first camera device to turn on a configured visible-light filter to acquire a first infrared image; and</claim-text><claim-text>performing the liveness detection on the target object according to the first infrared image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein in a case where the first camera device is determined to be the target device, recognizing and verifying the target object according to the image acquired by the determined target device and controlling the non-target device in the first camera device and the second camera device to perform the liveness detection on the target object comprise:<claim-text>recognizing and verifying the target object according to the first visible-light image acquired by the first camera device;</claim-text><claim-text>controlling the second camera device to turn on a configured visible-light filter to acquire a second infrared image; and</claim-text><claim-text>performing the liveness detection on the target object according to the second infrared image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the exposure processing on the first visible-light image according to the luminance information of the bright area image of the first visible-light image and performing the exposure processing on the second visible-light image according to the luminance information of the dark area image of the at least one of the first visible-light image or the second visible-light image comprise:<claim-text>obtaining a first photometric value according to the luminance information of the bright area image of the first visible-light image and performing the exposure processing on the first visible-light image based on the first photometric value; and</claim-text><claim-text>obtaining a second photometric value according to the luminance information of the dark area image of the at least one of the first visible-light image or the second visible-light image and performing the exposure processing on the second visible-light image based on the second photometric value.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein obtaining the second photometric value according to the luminance information of the dark area image of the at least one of the first visible-light image or the second visible-light image and performing the exposure processing on the second visible-light image based on the second photometric value comprise:<claim-text>obtaining luminance information of a bright area image of at least one of the first visible-light image or the second visible-light image;</claim-text><claim-text>computing the luminance information of the dark area image corresponding to the bright area image according to the luminance information of the bright area image and a precomputed correspondence between the luminance information of the bright area image and the luminance information of the dark area image; and</claim-text><claim-text>obtaining the second photometric value according to the computed luminance information of the dark area image and performing the exposure processing on the second visible-light image based on the second photometric value.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An object recognition apparatus, the apparatus being applied to an electronic device comprising a first camera device and a second camera device and the apparatus comprising:<claim-text>an image obtaining module configured to obtain a first visible-light image acquired by the first camera device and a second visible-light image acquired by the second camera device;</claim-text><claim-text>an exposure processing module configured to perform exposure processing on the first visible-light image according to luminance information of a bright area image of the first visible-light image and perform the exposure processing on the second visible-light image according to luminance information of a dark area image of at least one of the first visible-light image or the second visible-light image, wherein the bright area image is an area image having a luminance value greater than a preset value, and the dark area image is an area image having a luminance value less than or equal to the preset value; and</claim-text><claim-text>a recognition module configured to perform target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognize and verify a target object according to a detection result.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An electronic device, comprising a first camera device, a second camera device, a memory, a processor and a computer program stored on the memory and executable on the processor, wherein when executing the computer program, the processor performs the following steps:<claim-text>obtaining a first visible-light image acquired by the first camera device and a second visible-light image acquired by the second camera device;</claim-text><claim-text>performing exposure processing on the first visible-light image according to luminance information of a bright area image of the first visible-light image and performing exposure processing on the second visible-light image according to luminance information of a dark area image of at least one of the first visible-light image or the second visible-light image, wherein the bright area image is an area image having a luminance value greater than a preset value, and the dark area image is an area image having a luminance value less than or equal to the preset value; and</claim-text><claim-text>performing target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognizing and verifying a target object according to a detection result.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A non-transitory readable storage medium storing a computer program, wherein when executing the computer program, a processor performs the object recognition method according <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein performing the target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognizing and verifying the target object according to the detection result comprise:<claim-text>performing the target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and determining a target device from the first camera device and the second camera device according to the detection result; and</claim-text><claim-text>recognizing and verifying the target object according to an image acquired by the determined target device and controlling a non-target device in the first camera device and the second camera device to perform liveness detection on the target object.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein obtaining the first visible-light image acquired by the first camera device and the second visible-light image acquired by the second camera device comprises:<claim-text>controlling the first camera device to turn on a configured infrared filter to acquire the first visible-light image and controlling the second camera device to turn on a configured infrared filter to acquire the second visible-light image.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein performing the target object detection on the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing and recognizing and verifying the target object according to the detection result comprise:<claim-text>in a case where the target object is detected in the first visible-light image obtained after the exposure processing, and no target object is detected in the second visible-light image obtained after the exposure processing, determining to recognize and verify the target object by using the first visible-light image acquired by the first camera device; or</claim-text><claim-text>in a case where no target object is detected in the first visible-light image obtained after the exposure processing, and the target object is detected in the second visible-light image obtained after the exposure processing, determining to recognize and verify the target object by using the second visible-light image acquired by the second camera device; or</claim-text><claim-text>in a case where the target object is detected in the first visible-light image obtained after the exposure processing and the second visible-light image obtained after the exposure processing, comparing a sharpness of the target object detected in the first visible-light image with a sharpness of the target object detected in the second visible-light image; in a case where the sharpness of the target object in the first visible-light image is higher than the sharpness of the target object in the second visible-light image, determining to recognize and verify the target object by using the first visible-light image acquired by the first camera device; and in a case where the sharpness of the target object in the first visible-light image is not higher than the sharpness of the target object in the second visible-light image, determining to recognize and verify the target object by using the second visible-light image acquired by the second camera device.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein in a case where the second camera device is determined to be the target device, recognizing and verifying the target object according to the image acquired by the determined target device and controlling the non-target device in the first camera device and the second camera device to perform the liveness detection on the target object comprise:<claim-text>recognizing and verifying the target object according to the second visible-light image acquired by the second camera device;</claim-text><claim-text>controlling the first camera device to turn on a configured visible-light filter to acquire a first infrared image; and</claim-text><claim-text>performing the liveness detection on the target object according to the first infrared image.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein in a case where the first camera device is determined to be the target device, recognizing and verifying the target object according to the image acquired by the determined target device and controlling the non-target device in the first camera device and the second camera device to perform the liveness detection on the target object comprise:<claim-text>recognizing and verifying the target object according to the first visible-light image acquired by the first camera device;</claim-text><claim-text>controlling the second camera device to turn on a configured visible-light filter to acquire a second infrared image; and</claim-text><claim-text>performing the liveness detection on the target object according to the second infrared image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein performing the exposure processing on the first visible-light image according to the luminance information of the bright area image of the first visible-light image and performing the exposure processing on the second visible-light image according to the luminance information of the dark area image of the at least one of the first visible-light image or the second visible-light image comprise:<claim-text>obtaining a first photometric value according to the luminance information of the bright area image of the first visible-light image and performing the exposure processing on the first visible-light image based on the first photometric value; and</claim-text><claim-text>obtaining a second photometric value according to the luminance information of the dark area image of the at least one of the first visible-light image or the second visible-light image and performing the exposure processing on the second visible-light image based on the second photometric value.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein obtaining the second photometric value according to the luminance information of the dark area image of the at least one of the first visible-light image or the second visible-light image and performing the exposure processing on the second visible-light image based on the second photometric value comprise:<claim-text>obtaining luminance information of a bright area image of at least one of the first visible-light image or the second visible-light image;</claim-text><claim-text>computing the luminance information of the dark area image corresponding to the bright area image according to the luminance information of the bright area image and a precomputed correspondence between the luminance information of the bright area image and the luminance information of the dark area image; and</claim-text><claim-text>obtaining the second photometric value according to the computed luminance information of the dark area image and performing the exposure processing on the second visible-light image based on the second photometric value.</claim-text></claim-text></claim></claims></us-patent-application>