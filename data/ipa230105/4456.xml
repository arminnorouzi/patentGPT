<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004457A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004457</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17363595</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>07</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>0751</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>0721</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>0793</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">REDUCING FALSE POSITIVE FAILURE EVENTS IN A HEALTH MONITORING SYSTEM USING DIRECTIONAL GRAPHS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>EMC IP Holding Company LLC</orgname><address><city>Hopkinton</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Madan</last-name><first-name>Nitin</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Jenkins</last-name><first-name>Fani</first-name><address><city>Boulder</city><state>CO</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ramesh</last-name><first-name>Deepa</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Sundharraj</last-name><first-name>Gobikrishnan</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Embodiments for reducing panic shutdown of components in a pipelined data processing system. Components are monitored for health, processing progress, and dependencies during normal system operation. A directed graph is generated showing non-circular dependencies of components in the pipeline. Deadlock of a particular component may or may not signal a panic condition depending on whether any of its presently downstream and depended on components are operating properly. The continuously monitored knowledge of proper operation of all downstream components is thus used to intelligently apply or defer panic alerts to keep the system operating uninterrupted from panic conditions that might soon or eventually be fixed by continued operation of the system pipeline.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="84.67mm" wi="158.75mm" file="US20230004457A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="215.90mm" wi="167.22mm" file="US20230004457A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="234.87mm" wi="180.68mm" file="US20230004457A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="224.11mm" wi="178.31mm" file="US20230004457A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="179.32mm" wi="190.92mm" file="US20230004457A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="166.62mm" wi="171.37mm" file="US20230004457A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="226.91mm" wi="177.80mm" file="US20230004457A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="230.72mm" wi="161.21mm" file="US20230004457A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="162.81mm" wi="178.14mm" file="US20230004457A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">This invention relates generally to file system health monitoring systems, and specifically to monitoring recovery progress to eliminate unnecessary panic events.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Backup software is used by large organizations to store their data for recovery after system failures, routine maintenance, archiving, and so on. Backup sets are typically taken on a regular basis, such as hourly, daily, weekly, and so on, and can comprise vast amounts of information. Backup programs are often provided by vendors that provide backup infrastructure (software and/or hardware) to customers under service level agreements (SLA) that set out certain service level objectives (SLO) that dictate minimum standards for important operational criteria such as uptime and response time, etc. Within a large organization, dedicated IT personnel or departments are typically used to administer the backup operations and work with vendors to resolve issues and keep their infrastructure current.</p><p id="p-0004" num="0003">In most complex systems, there is a method to detect when service-level objectives (SLO) are not met. For example, if a job does not complete in a desired amount of time, the SLO is not met. The problem with this technique is that a job may need to be processed by more than one subsystem, each adding a level of uncertainty to the job, and not just in terms of time. The Data Domain File System (DDFS) uses an internal heartbeat monitor thread (HMON) (also referred to as a &#x2018;health monitor&#x2019;) to ensure that a job is making progress through a set of registered subsystems. When a subsystem does not make progress for a predetermined amount of time, the HMON subsystem flags this as an error and triggers a filesystem panic. A panic condition is triggered in order to recover the filesystem in case of genuine deadlock. These panics are very disruptive, not only to the operations of DDFS, but also to clients relying on DDFS to complete backups within a usually tight backup window. Reducing DDFS panics is, therefore, very vital to increasing system availability.</p><p id="p-0005" num="0004">The problem with using a time-based approach to triggering panics in DDFS is that one subsystem is not aware of the progress being made in another subsystem. In present systems, hard-coded timeout values are used to detect stalled requests. Most often, these timeout values have to be adjusted for new system configurations. It would be advantageous to reduce the need for these adjustments and thereby minimize unnecessary interruptions to the customer workloads.</p><p id="p-0006" num="0005">The subject matter discussed in the background section should not be assumed to be prior art merely as a result of its mention in the background section. Similarly, a problem mentioned in the background section or associated with the subject matter of the background section should not be assumed to have been previously recognized in the prior art. The subject matter in the background section merely represents different approaches, which in and of themselves may also be inventions. EMC, Data Domain and Data Domain Restorer are trademarks of DellEMC Corporation.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006">In the following drawings like reference numerals designate like structural elements. Although the figures depict various examples, the one or more embodiments and implementations described herein are not limited to the examples depicted in the figures.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of a network implementing a false positive panic reduction method for a heartbeat monitor, under some embodiments.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a data processing pipeline comprising multiple subsystems, under some embodiments.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example where a request results in a subsystem timing out, but is flagged as a false positive panic.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example where a request results in a subsystem timing out, and is processed as a genuine panic condition.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates dependencies between the intermediate Network File System, Content Store, and Segment Store subsystems to the Cloud Access Layer subsystem, under an example embodiment.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a system <b>600</b> for generating a directed graph for a generic system, under some embodiments.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates the dependencies of the components of <figref idref="DRAWINGS">FIG. <b>6</b></figref> to create a directional graph, under an example embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates an example system with dependent components where some example health conditions are provided for the components.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates using a dependency graph and component health information to make a panic decision, under some embodiments.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a method of reducing false positive panic alerts using a directed graph, under some embodiments.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a system block diagram of a computer system used to execute one or more software components of a system reducing false positive panic alerts using a directed graph, under some embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">A detailed description of one or more embodiments is provided below along with accompanying figures that illustrate the principles of the described embodiments. While aspects are described in conjunction with such embodiment(s), it should be understood that it is not limited to any one embodiment. On the contrary, the scope is limited only by the claims and the described embodiments encompass numerous alternatives, modifications, and equivalents. For the purpose of example, numerous specific details are set forth in the following description in order to provide a thorough understanding of the described embodiments, which may be practiced according to the claims without some or all of these specific details. For the purpose of clarity, technical material that is known in the technical fields related to the embodiments has not been described in detail so that the described embodiments are not unnecessarily obscured.</p><p id="p-0020" num="0019">It should be appreciated that the described embodiments can be implemented in numerous ways, including as a process, an apparatus, a system, a device, a method, or a computer-readable medium such as a computer-readable storage medium containing computer-readable instructions or computer program code, or as a computer program product, comprising a computer-usable medium having a computer-readable program code embodied therein. In the context of this disclosure, a computer-usable medium or computer-readable medium may be any physical medium that can contain or store the program for use by or in connection with the instruction execution system, apparatus or device. For example, the computer-readable storage medium or computer-usable medium may be, but is not limited to, a random-access memory (RAM), read-only memory (ROM), or a persistent store, such as a mass storage device, hard drives, CDROM, DVDROM, tape, erasable programmable read-only memory (EPROM or flash memory), or any magnetic, electromagnetic, optical, or electrical means or system, apparatus or device for storing information. Alternatively, or additionally, the computer-readable storage medium or computer-usable medium may be any combination of these devices or even paper or another suitable medium upon which the program code is printed, as the program code can be electronically captured, via, for instance, optical scanning of the paper or other medium, then compiled, interpreted, or otherwise processed in a suitable manner, if necessary, and then stored in a computer memory.</p><p id="p-0021" num="0020">Applications, software programs or computer-readable instructions may be referred to as components or modules. Applications may be hardwired or hard coded in hardware or take the form of software executing on a general-purpose computer or be hardwired or hard coded in hardware such that when the software is loaded into and/or executed by the computer, the computer becomes an apparatus for practicing the certain methods and processes described herein. Applications may also be downloaded, in whole or in part, through the use of a software development kit or toolkit that enables the creation and implementation of the described embodiments. In this specification, these implementations, or any other form that embodiments may take, may be referred to as techniques. In general, the order of the steps of disclosed processes may be altered within the scope of the embodiments.</p><p id="p-0022" num="0021">Some embodiments involve data processing in a distributed system, such as a cloud based network system or very large-scale wide area network (WAN), and metropolitan area network (MAN), however, those skilled in the art will appreciate that embodiments are not limited thereto, and may include smaller-scale networks, such as LANs (local area networks). Thus, aspects of the one or more embodiments described herein may be implemented on one or more computers executing software instructions, and the computers may be networked in a client-server arrangement or similar distributed computer network.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a computer network system that implements one or more embodiments of implementing organization awareness for automating data protection policies, under some embodiments. In system <b>100</b>, a storage server <b>102</b> executes a data storage or backup management process <b>112</b> that coordinates or manages the backup of data from one or more data sources <b>108</b> to storage devices, such as network storage <b>114</b>, client storage, and/or virtual storage devices <b>104</b>. With regard to virtual storage <b>104</b>, any number of virtual machines (VMs) or groups of VMs may be provided to serve as backup targets. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a virtualized data center (vCenter) <b>108</b> that includes any number of VMs for target storage. The backup server implements certain backup policies <b>113</b> defined for the backup management process <b>112</b>, which set relevant backup parameters such as backup schedule, storage targets, data restore procedures, and so on. In an embodiment, system <b>100</b> may comprise at least part of a Data Domain Restorer (DDR)-based deduplication storage system, and storage server <b>102</b> may be implemented as a DDR Deduplication Storage server provided by DellEMC Corporation. However, other similar backup and storage systems are also possible.</p><p id="p-0024" num="0023">The network server computers are coupled directly or indirectly to the network storage <b>114</b>, target VMs <b>104</b>, data center <b>108</b>, and the data sources <b>106</b> and other resources <b>116</b>/<b>117</b> through network <b>110</b>, which is typically a public cloud network (but may also be a private cloud, LAN, WAN or other similar network). Network <b>110</b> provides connectivity to the various systems, components, and resources of system <b>100</b>, and may be implemented using protocols such as Transmission Control Protocol (TCP) and/or Internet Protocol (IP), well known in the relevant arts. In a cloud computing environment, network <b>110</b> represents a network in which applications, servers and data are maintained and provided through a centralized cloud computing</p><p id="p-0025" num="0024">Backup software vendors typically provide service under a service level agreement (SLA) that establishes the terms and costs to use the network and transmit/store data specifies minimum resource allocations (e.g., storage space) and performance requirements (e.g., network bandwidth) provided by the provider. The backup software may be any suitable backup program such as EMC Data Domain, Avamar, and so on. In cloud networks, it may be provided by a cloud service provider server that may be maintained be a company such as Amazon, EMC, Apple, Cisco, Citrix, IBM, Google, Microsoft, Salesforce.com, and so on.</p><p id="p-0026" num="0025">In most large-scale enterprises or entities that process large amounts of data, different types of data are routinely generated and must be backed up for data recovery purposes. This data comes from many different sources and is used for many different purposes. In a large-scale network, such as a cloud network, data may be processed through a number of subsystems or layers in a network file system stack from the file system itself to a cloud access layer, and through number of subsystem layers (e.g., content store, segment store, etc.). Each layer or subsystem in the stack represents a link in the overall processing chain in that data must be processed in an earlier or upstream subsystem before it can be processed in a later or downstream subsystem. Each subsystem typically maintains a queue of data processing requests, and clears its queue by successfully processing the data within a defined time period. If any subsystem exceeds its time limit, its queue will overflow, which can cause problems for all the downstream subsystems.</p><p id="p-0027" num="0026">For the embodiment of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, which is typically a Data Domain system, the DD file system (DDFS) includes an internal heartbeat monitor thread or process (HMON) <b>120</b> that monitors the health of the various subsystems within the DDFS process. If the HMON <b>120</b> determines that a subsystem has either hung up or been waiting too long, it terminates the DDFS process. After such a termination, the DDFS typically attempts automatically restart and resume normal operation, which requires that any operations that were ongoing, such as restores/backups (i.e., reads/writes) will be interrupted and need to be restarted. This reaction is referred to as a &#x2018;panic&#x2019; condition and is done to recover the filesystem in case of a genuine deadlock within a subsystem, or between two subsystems. Currently, the HMON process <b>120</b> uses a hard-coded timeout value defined for each subsystem. If a subsystem remains hung up in excess of this time period, the HMON detects stalled requests and signals a panic situation.</p><p id="p-0028" num="0027">In many cases, however, processing of data requests may actually be progressing in a different subsystem, and a stalling subsystem may not be aware of the progress made in that other subsystem. In this case, generating a panic condition may actually be counterproductive as this is a false positive situation with respect to a genuine deadlock. In an embodiment, system <b>100</b> includes a false positive panic reduction process <b>121</b> to allow the health monitor <b>120</b> to distinguish between a subsystem that is making progress and one that is genuinely deadlocked. Process <b>121</b> transfers progress related state information from one subsystem to another subsystem using a directional graph, thus allowing for a system-wide awareness in deciding when to panic. Using this approach, process <b>121</b> removes incorrect (false positive) panics triggered by the predetermined timeout values in HMON <b>120</b>, and increases system availability by intelligent and accurate inheritance based health monitoring.</p><p id="p-0029" num="0028">Although embodiments are described with respect to Data Domain and DDFS systems using a heartbeat monitor thread, embodiments are not so limited, and the panic reduction process <b>121</b> can be used with respect to any health monitoring process in complex environments comprised of multiple, interdependent systems.</p><p id="p-0030" num="0029">For the embodiment of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the HMON <b>120</b> and panic reduction process <b>121</b> may both or either be implemented as a component that runs within a data protection infrastructure, and can be run as an independent application or embedded into an instance of data protection software <b>112</b> or as part of a data protection appliance. Any of those implementations may also be on-premise implementations on client machines within a user's data center or running as a hosted service within the cloud.</p><p id="p-0031" num="0030">As stated above, in a DDFS (or similar) system that employs a file system writing data between data sources and targets, such as cloud networks, a series of subsystems typically processes the data. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a data processing pipeline comprising multiple subsystems, under some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, data flows through system <b>200</b> from the network file system (NFS) <b>202</b> in a pipelined fashion to cloud storage <b>212</b> through multiple subsystems <b>204</b>-<b>210</b>, which each receives the data, manipulates it as needed, and passes it down to the next subsystem. Typically, each subsystem is sized with enough resources to accommodate a steady flow of data, as long as the clients stay within certain well-defined limits. Each subsystem maintains its own queue (e.g., queue <b>201</b> for NFS <b>202</b>) to order backed up data requests. When one subsystem of the file system is either completely stuck or making slower than expected progress, its request queues become full and this subsystem can no longer accept any new requests. This impacts the upstream subsystems (in front of the impacted subsystem), and they start exhibiting the same behavior.</p><p id="p-0032" num="0031">At some point, if there are requests that have been queued for an excessively long time (i.e., exceeding a pre-determined timeout threshold value), the HMON <b>120</b> subsystem will trigger a timeout panic. In the example shown in the <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the CAL (Cloud Access Layer) subsystem <b>210</b> sends and receives requests to cloud storage <b>212</b>, which can suffer from its high and unpredictable latencies. If the cloud latencies are high, the Cloud Access Layer <b>210</b> read or write queues may get depleted, causing the higher subsystem to block, while they wait to allocate a new request. Thus, a timed-out request in any of the queues in downstream layers <b>204</b> to <b>210</b> may ultimately cause the NFS queue <b>201</b> to be deadlocked <b>214</b>. This can lead to timeout panics in the NFS subsystem <b>202</b>, as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> or any of the intermediate subsystems, such as Content Store (CS) <b>204</b>, Segment Store (SS) <b>206</b>, or Container Manager (CM) <b>208</b>. In present systems, such a panic might be triggered even if the Cloud Access Layer subsystem <b>210</b> is still processing requests, albeit at slower speeds.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example where a request in Segment Store <b>206</b> has timed out <b>302</b>, which might normally result in a panic condition, but is flagged as a false positive panic. The Segment Store subsystem <b>206</b> is dependent on the Container Manager (CM) <b>208</b> and Cloud Access Layer <b>210</b> subsystems to get its request processed. Furthermore, both the Container Manager <b>208</b> and Cloud Access Layer <b>210</b> subsystems, are making progress <b>304</b> in processing their requests, as shown. In an embodiment, the panic reduction process <b>121</b> extends the timeout period in the Segment Store subsystem (and/or any other appropriate intermediate layer) to prevent a momentary deadlock from creating a panic condition. The extension of the timeout period for this layer provides enough time for the other layers to clear their queues and remove the threat of deadlock within the Segment Store. The timeout period can be extended by a defined formula, such as the doubling (or tripling) of the existing timeout period, or any other increase that is determined to be sufficient to provide time for downstream subsystems to clear their queues. With the panic reduction process <b>121</b> approach, this DDFS panic from Segment Store <b>206</b> would be eliminated given this continuing progress by its downstream subsystems (CM and CAL).</p><p id="p-0034" num="0033">In an embodiment, the panic reduction process builds a directional dependency graph and checks the progress of subsystems on which a particular problem subsystem (e.g., SS <b>206</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>) is dependent upon. For the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, such a check would provide the information that the Container Manager and Cloud Access Layer subsystems are processing requests, and will eventually catch up to the SS request which has timed out. This insight is used to invalidate the need for a panic, and thus keep the system availability online and uninterrupted. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the case of a false positive panic condition that is eliminated by process <b>121</b>.</p><p id="p-0035" num="0034">There are cases, however, where a panic condition is genuine (not a false positive) in which case, the panic reduction process <b>121</b> does not interfere with the HMON panic trigger. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example where a request in Segment Store <b>206</b> has timed out <b>302</b>, but is a genuine panic condition that is allowed to trigger a panic. For the example of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, Segment Store <b>206</b> has timed out <b>302</b>, and while Cloud Access Layer <b>210</b> is still making process <b>304</b> as before, the CM <b>208</b> subsystem is idle <b>402</b> and not processing any requests. In this case, the HMON <b>120</b> will correctly classify this as a stuck Segment Store request and identify the panic condition caused by the deadlocked queue <b>306</b> is valid, and the panic reduction process <b>121</b> will not flag this as a false positive to block the panic trigger.</p><p id="p-0036" num="0035">In general, each subsystem (component) may have its own defined timeout period, such as five seconds for one component and five seconds for another component, and the extension period can be defined differently for each component as well, such as twice the initial timeout period versus three times the initial timeout period for different components.</p><p id="p-0037" num="0036">In an embodiment, the panic reduction process is implemented using a dependency graph that is established between subsystems. For the examples of <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>3</b>, and <b>4</b></figref>, the process would establish a dependency between the Segment Store <b>206</b> and Cloud Access Layer <b>210</b> subsystems. As can be seen in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the Segment Store <b>206</b> is dependent on the Cloud Access Layer <b>210</b> and it inherits its state. If the Cloud Access Layer subsystem is making progress (not stuck), the Segment Store subsystem will assume the same state when handling request timeouts. In general, &#x2018;progress&#x2019; is defined as the processing of at least some I/O requests through the queue of a subsystem at a minimal rate. If I/O request processing is too slow or stopped for a certain period of time, the processing is stalled and the component is at least temporarily &#x2018;deadlocked.&#x2019;</p><p id="p-0038" num="0037">Determining dependencies between different subsystems in a complex system (e.g., DDFS) requires a certain balance since adding too many dependencies runs the risk of affecting system performance. In an embodiment, the dependencies are determined by identifying the subsystems that are most likely to be bottlenecks and create a dependency from the rest of the subsystems to these &#x2018;bottleneck&#x2019; subsystems.</p><p id="p-0039" num="0038">In an illustrative embodiment, the description will focus on the Cloud Access Layer subsystem. In this system, the Cloud Access Layer <b>210</b> interfaces directly with the Cloud Storage <b>212</b>, where there may be high or unpredictable latencies. The rest of the subsystems depend on the Cloud Access Layer subsystem to get their requests completed. However, it should be noted that embodiments are not so limited, and dependencies can be described with any subsystem within the pipeline.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates dependencies between the intermediate Network File System, Content Store, Segment Store, and Container Manager subsystems to the Cloud Access Layer subsystem, under an example embodiment. For this embodiment, the Cloud Access Layer subsystem is designated as the end node of the directional dependency graph, though other embodiments may use other subsystems as the end node. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, diagram <b>500</b> shows pointers from each of the NFS <b>202</b>, CS <b>204</b>, SS <b>206</b>, and CM <b>208</b> subsystems to the CAL <b>210</b> as the end node prior to the cloud storage <b>212</b>. This dependency relationship provides the basis to create a dependency graph that is used to eliminate timeout panics that happen due to external cloud provider inefficiencies and lack of predictability.</p><p id="p-0041" num="0040">This is but one example of a system with dependencies on one component (e.g., a Cloud Access Layer). However, this method is extensible to more complex dependencies. For example, if NFS is dependent on SS and SS is dependent on CM which could be dependent on CAL. In such an environment, the chaining of dependencies would be detected using directional graphs.</p><p id="p-0042" num="0041">This method is also extensible to components beyond a single machine, instead of layers within a single file system (as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>). For example, a component C1 is dependent on a component C2 which is in-turn dependent on component C3, as in: C1&#x2192;C2&#x2192;C3, where each of C1, C2, and C3 are completely different physical entities, such as independent computers or nodes connected by a network. In a normal environment, a slowness in C3 may cause service unavailability on C1 or C2, because of the pipeline nature of the components. However, the proposed method may detect that C3 is slow, and therefore it may be acceptable to extend absolute timeouts on C1 and C2, to prevent a momentary deadlock from raising a panic condition.</p><heading id="h-0005" level="2">Directed Graph Creation</heading><p id="p-0043" num="0042">In an embodiment, a directed graph is created upon start up (or bootup) of a device, system, or network, depending on the scale of the system and the pipelined process. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a system <b>600</b> for generating a directed graph for a generic system, under some embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, example generic system <b>602</b> has seven components (denoted components A through G). A dependency and monitoring subsystem <b>604</b> coupled to the system <b>602</b> monitors the components and identifies their dependencies based on initial wakeup processes (initialization) upon startup. For example, as the system boots up, the dependencies are recorded by the dependency and monitoring subsystem <b>604</b> as follows:</p><p id="p-0044" num="0043">A is dependent on C, D and E;</p><p id="p-0045" num="0044">B depends on D and E;</p><p id="p-0046" num="0045">C depends on D and E;</p><p id="p-0047" num="0046">D depends on E;</p><p id="p-0048" num="0047">E depends on Null;</p><p id="p-0049" num="0048">F depends on E;</p><p id="p-0050" num="0049">G depends on Null.</p><p id="p-0051" num="0050">These dependencies are shown as one-way arrows connecting pairs of components in system <b>602</b>. The dependencies can be recorded in component <b>604</b> by way of callbacks or explicit APIs, or any other method as appropriate for an implementation and system configuration.</p><p id="p-0052" num="0051">Once all the components have been initialized and are active, a dependency map is created for the system using the dependencies recorded by the dependency and monitoring subsystem <b>604</b>. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates the dependencies of the components of <figref idref="DRAWINGS">FIG. <b>6</b></figref> to create a directional graph, under an example embodiment. As shown in diagram <b>700</b>, the components A-G are listed and their dependency relationships are determined and recorded in the dependency monitoring subsystem <b>604</b>, such as in a list or tabulated format as shown above.</p><p id="p-0053" num="0052">A directed graph is generated from the dependencies using the one-way relationships as tabulated, along with certain rules. One such rule is that circular dependencies are not permitted. Thus, dependencies go in one direction down a pipeline to a final node component or subsystem. The components must register their dependencies with the dependency and monitoring subsystem <b>604</b> through a library of APIs or similar method, and all registered components (registrants) are monitored periodically to verify or re-validate their respective dependencies. This monitoring period may be set by the system, such as on an hourly, daily, weekly basis, etc. based on system configuration and complexity.</p><p id="p-0054" num="0053">A directed graph is used as the data structure to store the dependency information efficiently. A directed graph can detect circular dependencies, and thus it inherently enforces the most important rule of the graph. As the components shut down, the dependency and monitoring subsystem <b>604</b> would tear down the dependencies.</p><p id="p-0055" num="0054">In an embodiment, the directed (dependency) graph is used to make a timeout decision in a system that has a health monitor (or similar) component that imposes a time restriction on processing requests in each subsystem.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> illustrates an example system with dependent components where some example health conditions are provided for the components. System <b>800</b> includes three components, C, D, and E, where component D is dependent on component E, and component C is dependent on components D and E, as shown in the dependency table <b>804</b> created and stored in the dependency and monitoring subsystem <b>802</b>. A health monitor provides certain health information <b>806</b> that is also stored or accessible by subsystem <b>802</b>. For the example of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, E is reporting all OK, but D reports health not OK. The dependency and monitoring subsystem <b>802</b> monitors the health of the components to basically determine if the queues are being cleared based on the outstanding I/O requests to the component. If at least some degree of progress is being made to clear a queue, a component may be declared healthy by the subsystem <b>802</b>, thus allowing the panic condition to be deferred accordingly. In cases where no progress is made over a certain period of time, or any such progress is at too slow a rate, the component may be declared unhealthy and no timeout extension is provided.</p><p id="p-0057" num="0056">The dependency and monitoring subsystem <b>802</b> is consulted if a component timeout should defer corrective action, such as in the case where a component health is compromised or not OK. <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates using a dependency graph and component health information to make a panic decision, under some embodiments. Using the dependencies of <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, and the example health conditions <b>806</b> of components E and D, <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates a panic decision process for <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, under an example timeout situation. As shown in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>, the dependency and monitoring subsystem <b>802</b> monitors the two component D and E that component C is dependent upon (directly or indirectly). Component E is showing OK health (in <b>806</b>), while Component D is showing health not OK so subsystem <b>802</b> may detect no progress for Component D. When the amount of time of no progress in Component D reaches and exceeds the defined timeout period, Component D is deemed to be deadlocked and can trigger a panic condition. The dependency and monitoring subsystem <b>802</b>, however also monitors the progress of Component E, as Component D is dependent upon Component E. Thus, decision block <b>808</b> checks whether or not any downstream components from Component D are still operating properly, which would indicate that any problems with Component D are not due to any downstream components. Since component E is making progress, and D is dependent on E; although D is not making progress, it may be okay to delay the corrective action <b>810</b> that would be triggered by the panic condition if Component D is in a genuine deadlock. If Component D was not dependent on any further downstream components, or if those components were all themselves also deadlocked, a panic condition would be raised upon reaching the timeout period for any of those components.</p><p id="p-0058" num="0057">In the case of a no panic condition <b>810</b> being called, the timeout period is extended to defer the deadlock determination, such as by doubling the amount of time allocated to the timeout period, e.g., from 5 minutes to 10 minutes or any other appropriate time period and extension factor (e.g., 3 times or 4 times the initial timeout period, etc.). As long as the &#x2018;no progress&#x2019; situation clears up within this extended period, the panic condition will be avoided. Such a timeout extension period may be applied iteratively, such as doubling of the initial timeout period for x (e.g., 2 to 4) number of times, depending on system configuration. There may be an absolute upper timeout limit set, so that the timeout extension and deferral of the panic call is not infinite. As the components shut down, the dependency and monitoring subsystem <b>802</b> would tear down the defined dependencies so that dependencies are re-established upon the next startup.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating a method of reducing panic conditions due to component health problems using a directed graph, under some embodiments. The process of <figref idref="DRAWINGS">FIG. <b>9</b></figref> begins by determining the dependencies among components in a pipelined processing system, <b>902</b>. Such a system may be interconnected individual devices or computer, or layers within a file system as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, or any other similar system with multiple components. The pipelined architecture means that data progresses through a chain of components (or subsystems) through individual links between components from a source to a destination, such as from a data source or file system to a storage system through one or more processing layers. Each component may be dependent on one or more downstream components, either directly or indirectly, however, components may not be mutually dependent themselves or on each other (no circular dependencies).</p><p id="p-0060" num="0059">In an embodiment, the dependencies are determined upon startup of the system through initialization processes that indicate an order of data processing. For example, in the layered system of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the NFS <b>202</b> starts sending data generated by data sources that is progressively processed by the various CS, SS, and CM layers before being stored in cloud storage <b>212</b> by the Cloud Access Layer <b>210</b>. This sequence of data processing illustrates a clear initialization sequence that determines the pipeline of system <b>500</b> and the dependencies of each of the NFS, CS, SS, and CM layers on their immediate downstream neighbors.</p><p id="p-0061" num="0060">Once the dependencies are established in step <b>902</b>, a directed graph among these components is formed <b>904</b>, such as described above, and shown with reference to <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>. As data processing progresses, the components are monitored for progress, such as by a health or heartbeat monitor (HMON), <b>906</b>. If a particular component sees an issue with itself or any downstream component that may cause its request processing queues to deadlock, <b>908</b>, the system queries for problems with dependent components and uses the results to defer corrective actions, such as panic alarms and shutdown. However, it first determines if any downstream components that a failing component depends on are still operating properly, and are still in a valid dependency. Thus, in step <b>912</b>, the process performs a dependency check <b>912</b> for these conditions. If a validly depended upon component is still operating fine, it is assumed that the problem with the failing is not due to this component, and that continued proper operation of this component may overcome any potential deadlock of the upstream component. In this case, the timeout period of the upstream component is extended to allow this possible clearance to occur, <b>914</b>, and the panic condition is deferred for the time of this extended timeout period, <b>916</b>. This extension may be iteratively applied depending on system configuration, but an absolute timeout value may be defined to prevent infinite deferral of the panic condition, in order to allow a genuinely deadlocked system to shut down as planned. If either the upstream component has no downstream components, or such downstream components are either no longer depended upon, or are all failing, the process may signal the panic condition upon the initial timeout period of the upstream component, <b>918</b>.</p><p id="p-0062" num="0061">In this way, deadlock of a particular component may or may not signal a panic condition depending on whether any of its presently downstream and depended on components are operating properly. The continuously monitored knowledge of proper operation of all downstream components is thus used to intelligently apply or defer panic alerts to keep the system operating uninterrupted from panic conditions that might soon or eventually be fixed by continued operation of the system pipeline.</p><p id="p-0063" num="0062">In an embodiment, the directed graph is used to provide hints as to one or more queues most likely to create deadlocks based on the dependencies of components registered in the system. An analysis component or process can be used to analyze the directed graph to identify potential bottlenecks in the system, such as any component that is depended on by many other components, components with queues of insufficient size to handle larger data loads, and so on.</p><p id="p-0064" num="0063">In a further embodiment, certain artificial intelligence and machine learning (AI/ML) techniques may be used to learn patterns of delays or bottlenecks in a system to build a model that can be used to predict potential deadlock situations in other similar or comparable systems.</p><p id="p-0065" num="0064">Embodiments of the processes and techniques described above can be implemented on any appropriate backup system operating environment or file system, or network server system. Such embodiments may include other or alternative data structures or definitions as needed or appropriate.</p><p id="p-0066" num="0065">The processes described herein may be implemented as computer programs executed in a computer or networked processing device and may be written in any appropriate language using any appropriate software routines. For purposes of illustration, certain programming examples are provided herein, but are not intended to limit any possible embodiments of their respective processes.</p><p id="p-0067" num="0066">The network of <figref idref="DRAWINGS">FIG. <b>1</b></figref> may comprise any number of individual client-server networks coupled over the Internet or similar large-scale network or portion thereof. Each node in the network(s) comprises a computing device capable of executing software code to perform the processing steps described herein. <figref idref="DRAWINGS">FIG. <b>10</b></figref> shows a system block diagram of a computer system used to execute one or more software components of the present system described herein. The computer system <b>1000</b> includes a monitor <b>1011</b>, keyboard <b>1017</b>, and mass storage devices <b>1020</b>. Computer system <b>1000</b> further includes subsystems such as central processor <b>1010</b>, system memory <b>1015</b>, I/O controller <b>1021</b>, display adapter <b>1025</b>, serial or universal serial bus (USB) port <b>1030</b>, network interface <b>1035</b>, and speaker <b>1040</b>. The system may also be used with computer systems with additional or fewer subsystems. For example, a computer system could include more than one processor <b>1010</b> (i.e., a multiprocessor system) or a system may include a cache memory.</p><p id="p-0068" num="0067">Arrows such as <b>1045</b> represent the system bus architecture of computer system <b>1000</b>. However, these arrows are illustrative of any interconnection scheme serving to link the subsystems. For example, speaker <b>1040</b> could be connected to the other subsystems through a port or have an internal direct connection to central processor <b>1010</b>. The processor may include multiple processors or a multicore processor, which may permit parallel processing of information. Computer system <b>1000</b> is just one example of a computer system suitable for use with the present system. Other configurations of subsystems suitable for use with the described embodiments will be readily apparent to one of ordinary skill in the art.</p><p id="p-0069" num="0068">Computer software products may be written in any of various suitable programming languages. The computer software product may be an independent application with data input and data display modules. Alternatively, the computer software products may be classes that may be instantiated as distributed objects. The computer software products may also be component software. An operating system for the system <b>1005</b> may be one of the Microsoft Windows&#xae;. family of systems (e.g., Windows Server), Linux, Mac OS X, IRIX32, or IRIX64. Other operating systems may be used. Microsoft Windows is a trademark of Microsoft Corporation.</p><p id="p-0070" num="0069">The computer may be connected to a network and may interface to other computers using this network. The network may be an intranet, internet, or the Internet, among others. The network may be a wired network (e.g., using copper), telephone network, packet network, an optical network (e.g., using optical fiber), or a wireless network, or any combination of these. For example, data and other information may be passed between the computer and components (or steps) of the system using a wireless network using a protocol such as Wi-Fi (IEEE standards 802.11, 802.11a, 802.11b, 802.11e, 802.11g, 802.11i, 802.11n, 802.11ac, and 802.11ad, among other examples), near field communication (NFC), radio-frequency identification (RFID), mobile or cellular wireless. For example, signals from a computer may be transferred, at least in part, wirelessly to components or other computers.</p><p id="p-0071" num="0070">In an embodiment, with a web browser executing on a computer workstation system, a user accesses a system on the World Wide Web (WWW) through a network such as the Internet. The web browser is used to download web pages or other content in various formats including HTML, XML, text, PDF, and postscript, and may be used to upload information to other parts of the system. The web browser may use uniform resource identifiers (URLs) to identify resources on the web and hypertext transfer protocol (HTTP) in transferring files on the web.</p><p id="p-0072" num="0071">For the sake of clarity, the processes and methods herein have been illustrated with a specific flow, but it should be understood that other sequences may be possible and that some may be performed in parallel, without departing from the spirit of the described embodiments. Additionally, steps may be subdivided or combined. As disclosed herein, software written in accordance certain embodiments may be stored in some form of computer-readable medium, such as memory or CD-ROM, or transmitted over a network, and executed by a processor. More than one computer may be used, such as by using multiple computers in a parallel or load-sharing arrangement or distributing tasks across multiple computers such that, as a whole, they perform the functions of the components identified herein; i.e., they take the place of a single computer. Various functions described above may be performed by a single process or groups of processes, on a single computer or distributed over several computers. Processes may invoke other processes to handle certain tasks. A single storage device may be used, or several may be used to take the place of a single storage device.</p><p id="p-0073" num="0072">Unless the context clearly requires otherwise, throughout the description and the claims, the words &#x201c;comprise,&#x201d; &#x201c;comprising,&#x201d; and the like are to be construed in an inclusive sense as opposed to an exclusive or exhaustive sense; that is to say, in a sense of &#x201c;including, but not limited to.&#x201d; Words using the singular or plural number also include the plural or singular number respectively. Additionally, the words &#x201c;herein,&#x201d; &#x201c;hereunder,&#x201d; &#x201c;above,&#x201d; &#x201c;below,&#x201d; and words of similar import refer to this application as a whole and not to any particular portions of this application. When the word &#x201c;or&#x201d; is used in reference to a list of two or more items, that word covers all of the following interpretations of the word: any of the items in the list, all of the items in the list and any combination of the items in the list.</p><p id="p-0074" num="0073">All references cited herein are intended to be incorporated by reference. While one or more implementations have been described by way of example and in terms of the specific embodiments, it is to be understood that one or more implementations are not limited to the disclosed embodiments. To the contrary, it is intended to cover various modifications and similar arrangements as would be apparent to those skilled in the art. Therefore, the scope of the appended claims should be accorded the broadest interpretation so as to encompass all such modifications and similar arrangements.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method of maintaining uninterrupted system performance, comprising:<claim-text>determining all dependencies among components in a pipelined system processing data from a data source to a destination;</claim-text><claim-text>generating from the dependencies, a directed graph for the components;</claim-text><claim-text>continuously monitoring a health of each component and a present dependency of each component in the directed graph;</claim-text><claim-text>defining a panic response triggered by a timeout period for each component, the panic response comprising a shutdown of the system in the event of a genuine system deadlock;</claim-text><claim-text>upon detection of a problem causing potential deadlock in an upstream component, determining in a dependency check, if any downstream component depended on by the upstream component is still properly processing the data; and</claim-text><claim-text>extending, if the downstream component is still properly processing data, the timeout period of the upstream component for a defined amount of extension time to prevent the panic response for the period of the extension time.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the dependency check further verifies that the downstream component is still validly depended on by the upstream component.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. (canceled)</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the extension time comprises a doubling of an initial timeout period for a respective component, and can be iteratively applied over one or more monitoring cycles, the method further comprising applying an absolute timeout value to prevent infinite deferral of the panic condition.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the dependencies are determined upon startup of the system through initialization processes that indicate an order of data processing through the data pipeline.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref> wherein the upstream component is functionally closer to the data source and the downstream component is functionally closer to the destination.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref> wherein the data source comprises clients generating data managed by a file system, and wherein the destination comprises cloud network storage, and wherein the pipeline comprises components including, in order of dependency: a network file system (NFS), a content store (CS), a segment store (SS), a container manager (CM), and a Cloud Access Layer (CAL).</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref> wherein each component of the pipeline maintains a respective queue for storing data requests processing data received from an immediate upstream component.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein a panic condition is triggered for a component when data has been in the respective queue for the component in excess of the timeout period of the component.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein the directed graph is used to provide hints as to one or more queues most likely to create deadlocks based on the dependencies of components registered in the system.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref> further comprising applying machine learning processes to create models that can be used to predict potential deadlocks in other systems similar to the system.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A system comprising:<claim-text>a pipelined structure of processor-based hardware components processing data from a data source to a destination, each component maintaining a respective queue storing data processing requests from one or more upstream components, wherein a defined panic response is triggered by a timeout period for each component if a respective queue is full for a time exceeding the timeout period, the panic response comprising a shutdown of the system in the event of a genuine system deadlock;</claim-text><claim-text>a processor-based health monitoring subsystem continuously monitoring a health of each component and a present dependency of each component in the directed graph; and</claim-text><claim-text>a processor-based dependency subsystem<claim-text>determining all dependencies among components in the pipeline,</claim-text><claim-text>generating from the dependencies, a directed graph for the components,</claim-text><claim-text>determining in a dependency check if any downstream component depended on by the upstream component are still properly processing the data; and</claim-text><claim-text>extending, if the downstream component is still properly processing data, the timeout period of the upstream component for a defined amount of extension time to prevent triggering of the panic response for the period of the extension time, and upon detection of a problem causing potential deadlock in the upstream component.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein the dependency check further verifies that the downstream component is still validly depended on by the upstream component, and wherein the panic response comprises an emergency shutdown of the system in the event of a genuine system deadlock.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref> wherein the extension time comprises a doubling of an initial timeout period for a respective component, and can be iteratively applied over one or more monitoring cycles, the method further comprising applying an absolute timeout value to prevent infinite deferral of the panic condition.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref> wherein the dependencies are determined upon startup of the system through initialization processes that indicate an order of data processing through the data pipeline.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref> wherein the upstream component is functionally closer to the data source and the downstream component is functionally closer to the destination, and wherein the data source comprises clients generating data managed by a file system, and wherein the destination comprises cloud network storage, and wherein the pipeline comprises components including, in order of dependency: a network file system (NFS), a content store (CS), a segment store (SS), a container manager (CM), and a Cloud Access Layer (CAL).</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref> wherein the directed graph is used to provide hints as to one or more queues most likely to create deadlocks based on the dependencies of components registered in the system.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref> further comprising a machine learning component applying certain artificial intelligence processes to create models that can be used to predict potential deadlocks in other systems similar to the system.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable medium containing programming code which, when executed by a processor in a computer system, causes the computer system to perform method comprising:<claim-text>determining all dependencies among components in a pipelined system processing data from a data source to a destination;</claim-text><claim-text>generating from the dependencies, a directed graph for the components;</claim-text><claim-text>continuously monitoring a health of each component and a present dependency of each component in the directed graph;</claim-text><claim-text>defining a panic response triggered by a timeout period for each component, the panic response comprising a shutdown of the system in the event of a genuine system deadlock;</claim-text><claim-text>upon detection of a problem causing potential deadlock in an upstream component, determining in a dependency check, if any downstream component depended on by the upstream component is still properly processing the data; and</claim-text><claim-text>extending, if the downstream component is still properly processing data, the timeout period of the upstream component for a defined amount of extension time to prevent the panic response for the period of the extension time.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00019">claim 19</claim-ref> wherein the dependency check further verifies that the downstream component is still validly depended on by the upstream component, and further wherein the panic response comprises an emergency shutdown of the system in the event of a genuine system deadlock.</claim-text></claim></claims></us-patent-application>