<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005280A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005280</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17747140</doc-number><date>20220518</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110746165.3</doc-number><date>20210701</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>64</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>64</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>751</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD OF DETECTING TARGET OBJECTS IN IMAGES, ELECTRONIC DEVICE, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>HON HAI PRECISION INDUSTRY CO., LTD.</orgname><address><city>New Taipei</city><country>TW</country></address></addressbook><residence><country>TW</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>CHENG-FENG</first-name><address><city>Kaohsiung</city><country>TW</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>YANG</last-name><first-name>HUI-XIAN</first-name><address><city>New Taipei</city><country>TW</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>LIN</last-name><first-name>LI-CHE</first-name><address><city>Kaohsiung</city><country>TW</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of recognizing target objects in images obtains a detection image of a target object. A template image is generated according to the target object. The detection image is compared with the template image to obtain a comparison result. Candidate regions of the target object are determined in the detection image according to the comparison result. At least one target region of the target object is obtained from the candidate regions. The method detects target objects in images very rapidly.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="111.84mm" wi="127.25mm" file="US20230005280A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="67.90mm" wi="144.36mm" file="US20230005280A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="122.77mm" wi="129.29mm" file="US20230005280A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="125.39mm" wi="139.53mm" file="US20230005280A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="142.49mm" wi="157.56mm" file="US20230005280A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="142.66mm" wi="157.99mm" file="US20230005280A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The subject matter herein generally relates to image processing, specifically a method of detecting target objects in images, an electronic device, and a storage medium.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">An object detection model can detect objects in images. In order to achieve accurate detection, a large amount of training data is used to train the object detection model. If new objects are to be detected, the object detection model needs to be retrained using new training data. The method uses a lot of memory of an electronic device, resulting in slow image detection.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0004" num="0003">In order to more clearly describe the technical solutions in the embodiments of the present disclosure or the prior art, the following will briefly introduce the drawings that need to be used in the description of the embodiments or the prior art. Obviously, the drawings in the following description are only examples. For those of ordinary skill in the art, other drawings can be obtained according to the provided drawings without creative work.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an electronic device implementing a method of detecting target objects in images in one embodiment of the present disclosure.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of the method of detecting target objects in images provided in one embodiment of the present disclosure.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a target object in a detection image.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a binarized image based on the detection image of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows candidate regions of the target object in the detection image of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0010" num="0009">For clarity of illustration of objectives, features and advantages of the present disclosure, the drawings combined with the detailed description illustrate the embodiments of the present disclosure hereinafter. It is noted that embodiments of the present disclosure and features of the embodiments can be combined, when there is no conflict.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an electronic device implementing a method of detecting target objects in images in one embodiment of the present disclosure. The electronic device <b>1</b> includes, but is not limited to, a storage device <b>11</b>, at least one processor <b>12</b>, and a communication bus <b>13</b>. The storage device <b>11</b> and at least one processor <b>12</b> are connected via the communication bus <b>13</b> or connected directly.</p><p id="p-0012" num="0011">The electronic device <b>1</b> can be any electronic product that can interact with a user, such as a personal computer, a tablet computer, a smart phone, a personal digital assistant (PDA), a smart wearable device, etc. Those skilled in the art will understand that electronic device <b>1</b> is only an example, and does not constitute a limitation. Other examples of electronic device <b>1</b> may include more or fewer components than shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or combine some components, or have different components. For example, the electronic device <b>1</b> may further include an input/output device, a network access device, and the like.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of a method of detecting target objects in images in one embodiment. The method can detect target objects in images quickly. The method may be executed by an electronic device (e.g., electronic device <b>1</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). According to different requirements, the order of the blocks in the flowchart may be changed, and some blocks may be omitted.</p><p id="p-0014" num="0013">In block S<b>11</b>, the electronic device obtains an image (&#x201c;detection image&#x201d;) in which a target object is to be recognized and classified.</p><p id="p-0015" num="0014">The target object can be set according to actual requirements. The detection image is an image including the target object. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a target object in a detection image. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the target object is an integrated circuit (IC) chip, and the detection image is an image of an IC carrier plate including the IC chip.</p><p id="p-0016" num="0015">In block S<b>12</b>, the electronic device generates a template image according to the target object.</p><p id="p-0017" num="0016">In one embodiment, the template image is a white image. A size of the template image is the same as a size of a bounding box of the target object.</p><p id="p-0018" num="0017">The electronic device can determine the bounding box of the target object in the detection image, and obtain coordinates of four vertices of the bounding box. The bounding box is a rectangular. The electronic device calculates a length and a width of the bounding box according to the coordinates of the of four vertices of the bounding box, and generates the template image according to the length and the width.</p><p id="p-0019" num="0018">By generating the template image, extraction of target regions of the target object from the detection image is made easier.</p><p id="p-0020" num="0019">In block S<b>13</b>, the electronic device compares the detection image with the template image to obtain a comparison result.</p><p id="p-0021" num="0020">In one embodiment, the electronic device may performs binarization on the detection image to obtain a binarized image. The electronic device slides the template image on the binarized image in a preset order until an edge of the template image is aligned with an edge of the binarized image, to obtain multiple sliding regions in the binarized image. The electronic device compares each of the sliding regions with the template image to obtain the comparison result.</p><p id="p-0022" num="0021">In the binarization processing, a first threshold can be set. The first threshold can be between 0 and 255. The electronic device obtains a grayscale value corresponding to each pixel in the detection image, and determines whether the grayscale value is less than the first threshold. If the grayscale value is less than the first threshold, the electronic device adjusts the gray value to a first value (such as zero). If the gray value is greater than or equal to the first threshold, the electronic device adjusts the gray value to a second value (such as 255). It should be noted that the binarization converts the detection image into a binarized image (such as black and white). The target object in the detection image can be converted into a white area, and a background area can be converted into a black area.</p><p id="p-0023" num="0022">The electronic device can set a size of step for the template image. The step size can be set according to the width of the bounding box of the target object. The electronic device determines whether the template image exceeds the edge of the binarized image. If the template image exceeds the edge of the binarized image, the electronic device slides the template image in an opposite direction until the edge of the template image is aligned with the edge of the binarized image. Multiple sliding regions in the binarized image are obtained.</p><p id="p-0024" num="0023">The electronic device can calculate a similarity between each of the sliding regions and the template image. For each sliding region, the electronic device can obtain pixels (&#x201c;first pixels&#x201d;) in the template image, and obtain pixels (&#x201c;second pixels&#x201d;) in the sliding region. Each of the second pixels corresponds to one of the first pixels. The electronic device can calculate a difference of squares of each first pixel point and a corresponding second pixel point, summing all differences of squares to obtain the similarity. It should be noted that the smaller the summation of the differences of squares, the higher the similarity, and the larger the summation of the differences of squares, the smaller will be the similarity. For example, a summation of the differences of squares of sliding region A is 2, and a summation of the differences of squares of sliding region B is 10 means that a similarity of the sliding region A is greater than a similarity of the sliding region B.</p><p id="p-0025" num="0024">In the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the detection image is an image of an IC carrier plate, and the target object is an IC chip on the IC carrier plate. The electronic device generates a white template image with same size as the IC chip. Next, the electronic device converts the image of the IC carrier plate into a binarized image. <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a binarized image based on the detection image of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In the binarized image of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a region of the IC chip becomes a white area, and other regions become black areas. The electronic device slides the template image on the binarized image to obtain multiple sliding regions. White pixels in the sliding regions and the template image are set to 0, and black pixels in the sliding regions and the template image are set to 1. For each sliding region, the electronic device calculates a summation of the differences of squares between pixels in the template image and pixels in the sliding region. The similarity can therefore be calculated as 1/(1+(summation of the differences of squares). When the summation of the differences of squares is 0, the similarity is 1.</p><p id="p-0026" num="0025">In one embodiment, a region of the target object is converted into a white region after binarization. The region of the target object can be extracted according to the similarity with the white template image.</p><p id="p-0027" num="0026">In block S<b>14</b>, the electronic device determines candidate regions of the target object in the detection image according to the comparison result.</p><p id="p-0028" num="0027">In one embodiment, a sliding region which has a higher similarity with the template image means that it is closer to the region of the target object.</p><p id="p-0029" num="0028">In one embodiment, the electronic device determines whether the similarity is greater than a second threshold, and sets the sliding region corresponding to the similarity as a candidate region of the target object when the similarity is greater than the second threshold.</p><p id="p-0030" num="0029">For example, the second threshold is &#x2155;, and similarities obtained in block S<b>13</b> are 1, &#xbd;, &#x2159;, &#x215b;, and 1/10. According to the second threshold, sliding regions corresponding to 1 and &#xbd; can be determined as candidate regions of the target object. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a candidate region of the target object in the detection image. <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows candidate regions of the target object in the detection image of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, each candidate region is framed by a white frame.</p><p id="p-0031" num="0030">In the method, the candidate regions of the target object can be obtained using the template image. This method reduces CPU usage and improves the detection speed of target objects.</p><p id="p-0032" num="0031">In block S<b>15</b>, the electronic device obtains at least one target region of the target object from the candidate regions.</p><p id="p-0033" num="0032">Two sliding regions may overlap, and the candidate regions may overlap. Therefore, redundant candidate regions need to be removed.</p><p id="p-0034" num="0033">In one embodiment, the electronic device may perform non-maximum suppression (NMS) on the candidate region to obtain the at least one target region of the target object.</p><p id="p-0035" num="0034">The electronic device may sort the candidate regions according to the similarities, in descending order. Starting from a second candidate region, the electronic device selects a candidate region (the electronic device successively selects a second candidate region, a third candidate region, a fourth candidate region, . . . , and a last candidate region). For each selected candidate region, the electronic device calculates an overlap ratio of the selected candidate region and a first candidate region, and determines whether the overlap ratio is greater than a third threshold. If the overlap ratio is greater than or equal to the third threshold, the electronic device deletes the selected candidate region from the candidate regions as being redundant. If the overlap ratio is less than the third threshold, the electronic device retains the selected candidate region in the candidate regions. The first candidate region corresponds to a first similarity. The second candidate region corresponds to a second similarity. The third candidate region corresponds to a third similarity. The fourth candidate region corresponds to a fourth similarity. The last candidate region corresponds to a minimum similarity.</p><p id="p-0036" num="0035">In one embodiment, the electronic device may obtain the at least one target region of the target object from the candidate regions as follows (S<b>501</b>-S<b>510</b>).</p><p id="p-0037" num="0036">S<b>501</b>, the electronic device stores candidate regions of the target object in a first database.</p><p id="p-0038" num="0037">S<b>502</b>, the electronic device sorts in descending order the candidate regions according to a similarity between each of the candidate regions and the template image.</p><p id="p-0039" num="0038">S<b>503</b>, starting from a second candidate region in the first database, the electronic device selects a candidate region.</p><p id="p-0040" num="0039">S<b>504</b>, the electronic device calculates an overlap ratio between the selected candidate region and a first candidate region in the first database.</p><p id="p-0041" num="0040">S<b>505</b>, the electronic device determines whether the overlap ratio is greater than the third threshold.</p><p id="p-0042" num="0041">S<b>506</b>, if the overlap ratio is greater than or equal to the third threshold, the electronic device deletes the selected candidate region from the first database for redundancy.</p><p id="p-0043" num="0042">S<b>507</b>, if the overlap ratio is less than the third threshold, the electronic device retains the selected candidate region in the first database.</p><p id="p-0044" num="0043">S<b>508</b>, the electronic device moves the first candidate region to a second database.</p><p id="p-0045" num="0044">S<b>509</b>, the electronic device determines whether the first database is empty. The process goes to block S<b>503</b> if the first database is not empty. S<b>503</b> to S<b>508</b> are repeated until the first database is empty.</p><p id="p-0046" num="0045">S<b>510</b>, the electronic device set candidate regions in the second database as the target regions of the target object.</p><p id="p-0047" num="0046">For example, there are four candidate regions, denoted as A, B, C, and D. The four candidate regions are sorted as D&#x3e;C&#x3e;B&#x3e;A. The third threshold is 0.3. At first, A, B, C, and D are stored in the first database, and D is selected as the first candidate region. An overlap ratio of C and D is 0.6, an overlap ratio of B and D is 0.1, and an overlap ratio of A and D is 0. Therefore, C is deleted from the first database, A and B are retained in the first data base, and D is moved from the first database to the second database. Next, B is selected as the first candidate region. An overlap ratio of A and B is 0.4. Therefore, A is deleted from the first database, and B is moved to the second database. The first database is now empty and B and D are set as the target regions of the target object.</p><p id="p-0048" num="0047">By obtaining target regions with a high similarity to the template image and deleting candidate regions with a high overlap rate with the target regions, the detection accuracy of the target object is improved.</p><p id="p-0049" num="0048">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a computer program (such as an image detection system) may be stored in the storage device <b>11</b> and executable by the processor <b>12</b>. The processor <b>12</b> may execute the computer program to implement the blocks in the method described above, such as the blocks S<b>11</b> to S<b>15</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0050" num="0049">The storage device <b>11</b> may be an internal memory of the electronic device <b>1</b>, that is, a memory built into the electronic device <b>1</b>. The storage device <b>11</b> may also be an external memory of the electronic device <b>1</b>, that is, a memory externally connected to the electronic device <b>1</b>.</p><p id="p-0051" num="0050">The storage device <b>11</b> is used for storing program codes and various data, and accesses programs and data during the operation of the electronic device <b>1</b>.</p><p id="p-0052" num="0051">The storage device <b>11</b> may include a storage program area and a storage data area. The storage program area may store an operating system, and programs required by at least one function, etc.; the storage data area may store data and the like created in the use of the electronic device <b>1</b>. In addition, the storage device <b>11</b> may include non-volatile memory, such as a hard disk, a memory, a plug-in hard disk, a smart memory card (SMC), a secure digital (SD) card, a flash memory card (Flash Card), at least one magnetic disk storage device, flash memory device, or other non-volatile solid state storage device.</p><p id="p-0053" num="0052">The processor <b>12</b> may be a central processing unit (CPU) or other general-purpose processor, a digital signal processor (DSP), an application-specific integrated circuit (ASIC), a field-programmable gate array (FPGA) or other programmable logic device, a discrete gate, or a transistor logic device, or a discrete hardware component, etc. The processor <b>12</b> may be a microprocessor or any conventional processor. The processor <b>12</b> may be a control center of the electronic device <b>1</b>, and connect various parts of the entire electronic device <b>1</b> by using various interfaces and lines.</p><p id="p-0054" num="0053">In an exemplary embodiment, the computer program may be divided into one or more modules, and the one or more modules are stored in the storage device <b>11</b> and executed by the processor <b>12</b> to complete the method of the present disclosure. The one or more modules can be a series of computer-readable instruction segments capable of performing specific functions, and the instruction segments are used to describe execution processes of the computer program in the electronic device <b>1</b>.</p><p id="p-0055" num="0054">When the modules integrated in the electronic device <b>1</b> are implemented in the form of software functional units and used as independent units, they can be stored in a non-transitory readable storage medium. According to this understanding, all or part of the processes in the method of the above embodiments implemented by the present disclosure can also be completed by related hardware instructed by computer-readable instructions. The computer-readable instructions may be stored in a non-transitory readable storage medium. The computer-readable instructions, when executed by the processor, may implement the blocks of the foregoing method embodiments. The computer-readable instructions include computer-readable instruction codes, and the computer-readable instruction codes can be source code, object code, an executable file, or in some other intermediate form. The non-transitory readable storage medium may include any entity or device capable of carrying the computer-readable instruction code, a recording medium, a U disk, a mobile hard disk, a magnetic disk, an optical disk, a computer memory, and a read-only memory (ROM).</p><p id="p-0056" num="0055">Although not shown, the electronic device <b>1</b> may also include a power source (such as a battery) for supplying power to various components. The power source may be logically connected to the at least one processor <b>12</b> through a power management device, so as to realize functions such as charging, discharging, and power consumption management. The power supply may also include direct current or alternating current power supplies, recharging devices, power failure detection circuits, power converters or inverters, and power status indicators. The electronic device <b>1</b> may also include various sensors, BLUETOOTH modules, WI-FI modules, etc.</p><p id="p-0057" num="0056">In several embodiments provided in the preset disclosure, it should be understood that the disclosed electronic device and method may be implemented in other ways. For example, the embodiments of the electronic device described above are merely illustrative. For example, the units are only divided according to logical function, and there may be other manners of division in actual implementation.</p><p id="p-0058" num="0057">The modules described as separate components may or may not be physically separated, and the components displayed as modules may or may not be physical modules, that is, may be located in one place, or may be distributed on multiple network elements. Some or all of the modules may be selected according to actual needs to achieve the purpose of the method.</p><p id="p-0059" num="0058">In addition, each functional unit in each embodiment of the present disclosure can be integrated into one processing unit, or can be physically present separately in each unit, or two or more units can be integrated into one unit. The above integrated unit can be implemented in a form of hardware or in a form of a software functional unit.</p><p id="p-0060" num="0059">The above integrated modules implemented in the form of function modules may be stored in a storage medium. The above function modules may be stored in a storage medium, and include several instructions to enable an electronic device (which may be a personal computer, server, or network device, etc.) or processor to execute the method described in the embodiment of the present disclosure.</p><p id="p-0061" num="0060">The present disclosure is not limited to the details of the above-described exemplary embodiments, and the present disclosure can be embodied in other specific forms without departing from the spirit or essential characteristics of the present disclosure. Therefore, the present embodiments are to be considered as illustrative and not restrictive, and the scope of the present disclosure is defined by the appended claims. All changes and variations in the meaning and scope of equivalent elements are included in the present disclosure. Any reference sign in the claims should not be construed as limiting the claim. Furthermore, the word &#x201c;comprising&#x201d; does not exclude other units nor does the singular exclude the plural. A plurality of units or devices stated in the system claims may also be implemented by one unit or device through software or hardware. Words such as &#x201c;first&#x201d; and &#x201c;second&#x201d; are used to indicate names but not to signify any particular order.</p><p id="p-0062" num="0061">Finally, the above embodiments are only used to illustrate technical solutions of the present disclosure, and are not to be taken as restrictions on the technical solutions. Although the present disclosure has been described in detail with reference to the above embodiments, those skilled in the art should understand that the technical solutions described in one embodiments can be modified, or some of technical features can be equivalently substituted, and that these modifications or substitutions are not to detract from the essence of the technical solutions or from the scope of the technical solutions of the embodiments of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>We claim:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of detecting target objects in images, comprising:<claim-text>obtaining a detection image of a target object;</claim-text><claim-text>generating a template image according to the target object;</claim-text><claim-text>comparing the detection image with the template image to obtain a comparison result;</claim-text><claim-text>determining candidate regions of the target object in the detection image according to the comparison result; and</claim-text><claim-text>obtaining at least one target region of the target object from the candidate regions.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein comparing the detection image with the template image comprises:<claim-text>performing binarization on the detection image to obtain a binarized image;</claim-text><claim-text>sliding the template image on the binarized image in a preset order until an edge of the template image is aligned with an edge of the binarized image, to obtain a plurality of sliding regions in the binarized image; and</claim-text><claim-text>comparing each of the sliding regions with the template image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein performing binarization on the detection image comprises:<claim-text>setting a first threshold;</claim-text><claim-text>obtaining a grayscale value corresponding to each pixel in the detection image; and</claim-text><claim-text>adjusting the gray value according to the first threshold.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein adjusting the gray value according to the first threshold comprises:<claim-text>determining whether the gray value is less than the first threshold;</claim-text><claim-text>adjusting the gray value to a first value when the gray value is smaller than the first threshold; and</claim-text><claim-text>adjusting the gray value to a second value when the gray value is greater than or equal to the first threshold.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein comparing each of the sliding regions with the template image comprises:<claim-text>calculating a similarity between each of the sliding regions and the template image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein determining candidate regions of the target object in the detection image according to the comparison result comprises:<claim-text>determining whether the similarity is greater than a second threshold; and</claim-text><claim-text>setting the sliding region corresponding to the similarity as a candidate region of the target object when the similarity is greater than the second threshold.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining at least one target region of the target object from the candidate regions comprises:<claim-text>performing non-maximum suppression (NMS) on the candidate region to obtain the at least one target region of the target object.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein performing non-maximum suppression (NMS) on the candidate region to obtain the at least one target region of the target object comprises:<claim-text>sorting the candidate regions according to a similarity between each of the candidate regions and the template image in descending order;</claim-text><claim-text>selecting a candidate region starting from a second candidate region, calculating an overlap ratio of selected candidate region and a first candidate region, the first candidate region corresponding to a first similarity, and the second candidate region corresponding to a second similarity;</claim-text><claim-text>determining whether the overlap ratio is greater than a third threshold; and</claim-text><claim-text>deleting the selected candidate region from the candidate regions when the overlap ratio is greater than or equal to the third threshold, and retaining the selected candidate region in the candidate regions when the overlap ratio is less than the third threshold.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. An electronic device comprising:<claim-text>at least one processor; and</claim-text><claim-text>a storage device storing computer-readable instructions, which when executed by the at least one processor, cause the at least one processor to:</claim-text><claim-text>obtain a detection image of a target object;</claim-text><claim-text>generate a template image according to the target object;</claim-text><claim-text>compare the detection image with the template image to obtain a comparison result;</claim-text><claim-text>determine candidate regions of the target object in the detection image according to the comparison result; and</claim-text><claim-text>obtain at least one target region of the target object from the candidate regions.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the at least one processor is further caused to:<claim-text>perform binarization on the detection image to obtain a binarized image;</claim-text><claim-text>slide the template image on the binarized image in a preset order until an edge of the template image is aligned with an edge of the binarized image, to obtain a plurality of sliding regions in the binarized image; and</claim-text><claim-text>compare each of the sliding regions with the template image.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the at least one processor is further caused to:<claim-text>set a first threshold;</claim-text><claim-text>obtain a grayscale value corresponding to each pixel in the detection image; and</claim-text><claim-text>adjust the gray value according to the first threshold.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the at least one processor is further caused to:<claim-text>determine whether the gray value is less than the first threshold;</claim-text><claim-text>adjust the gray value to a first value when the gray value is smaller than the first threshold; and</claim-text><claim-text>adjust the gray value to a second value when the gray value is greater than or equal to the first threshold.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the at least one processor is further caused to:<claim-text>calculate a similarity between each of the sliding regions and the template image;</claim-text><claim-text>determine whether the similarity is greater than a second threshold; and</claim-text><claim-text>set the sliding region corresponding to the similarity as a candidate region of the target object when the similarity is greater than the second threshold.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the at least one processor is further caused to:<claim-text>sort the candidate regions according to a similarity between each of the candidate regions and the template image in descending order;</claim-text><claim-text>select a candidate region starting from a second candidate region, calculate an overlap ratio of selected candidate region and a first candidate region, the first candidate region corresponding to a first similarity, and the second candidate region corresponding to a second similarity;</claim-text><claim-text>determine whether the overlap ratio is greater than a third threshold; and</claim-text><claim-text>delete the selected candidate region from the candidate regions when the overlap ratio is greater than or equal to the third threshold, and retain the selected candidate region in the candidate regions when the overlap ratio is less than the third threshold.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory storage medium having stored thereon computer-readable instructions that, when the computer-readable instructions are executed by a processor to implement the following method:<claim-text>obtaining a detection image of a target object;</claim-text><claim-text>generating a template image according to the target object;</claim-text><claim-text>comparing the detection image with the template image to obtain a comparison result;</claim-text><claim-text>determining candidate regions of the target object in the detection image according to the comparison result; and</claim-text><claim-text>obtaining at least one target region of the target object from the candidate regions.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein comparing the detection image with the template image comprises:<claim-text>performing binarization on the detection image to obtain a binarized image;</claim-text><claim-text>sliding the template image on the binarized image in a preset order until an edge of the template image is aligned with an edge of the binarized image, to obtain a plurality of sliding regions in the binarized image; and</claim-text><claim-text>comparing each of the sliding regions with the template image.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein performing binarization on the detection image comprises:<claim-text>setting a first threshold;</claim-text><claim-text>obtaining a grayscale value corresponding to each pixel in the detection image; and</claim-text><claim-text>adjusting the gray value according to the first threshold.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein adjusting the gray value according to the first threshold comprises:<claim-text>determining whether the gray value is less than the first threshold;</claim-text><claim-text>adjusting the gray value to a first value when the gray value is smaller than the first threshold; and</claim-text><claim-text>adjusting the gray value to a second value when the gray value is greater than or equal to the first threshold.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory storage medium of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein comparing each of the sliding regions with the template image comprises:<claim-text>calculating a similarity between each of the sliding regions and the template image; and</claim-text><claim-text>determining candidate regions of the target object in the detection image according to the comparison result comprises:</claim-text><claim-text>determining whether the similarity is greater than a second threshold; and</claim-text><claim-text>setting the sliding region corresponding to the similarity as a candidate region of the target object when the similarity is greater than the second threshold.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein determining the operator subsets of the neural network model according to the plurality of node subsets comprises:<claim-text>sorting the candidate regions according to a similarity between each of the candidate regions and the template image in descending order;</claim-text><claim-text>selecting a candidate region starting from a second candidate region, calculating an overlap ratio of selected candidate region and a first candidate region, the first candidate region corresponding to a first similarity, and the second candidate region corresponding to a second similarity;</claim-text><claim-text>determining whether the overlap ratio is greater than a third threshold; and<claim-text>deleting the selected candidate region from the candidate regions when the overlap ratio is greater than or equal to the third threshold, and retaining the selected candidate region in the candidate regions when the overlap ratio is less than the third threshold.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>