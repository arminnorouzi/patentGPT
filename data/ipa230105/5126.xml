<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005127A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005127</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17756744</doc-number><date>20201116</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>DE</country><doc-number>10 2019 132 830.6</doc-number><date>20191203</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>07</class><subclass>C</subclass><main-group>5</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0004</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>07</class><subclass>C</subclass><main-group>5</main-group><subgroup>3408</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10144</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10152</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30108</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>07</class><subclass>C</subclass><main-group>2501</main-group><subgroup>0063</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND DEVICE FOR DETECTING CONTAINERS WHICH HAVE FALLEN OVER AND/OR ARE DAMAGED IN A CONTAINER MASS FLOW</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KRONES AG</orgname><address><city>Neutraubling</city><country>DE</country></address></addressbook><residence><country>DE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>AWISZUS</last-name><first-name>Stefan</first-name><address><city>Bad Aibling</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>BAYER</last-name><first-name>Udo</first-name><address><city>Thansau</city><country>DE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>PAUKERT</last-name><first-name>Josef</first-name><address><city>Kolbermoor</city><country>DE</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>SIDDIQUI</last-name><first-name>Aurangzaib Ahmed</first-name><address><city>Rosenheim</city><country>DE</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/082172</doc-number><date>20201116</date></document-id><us-371c12-date><date>20220601</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Method for detecting containers which have fallen over and/or are damaged in a container mass flow, wherein the containers in the container mass flow are transported vertically on a transporter, wherein the container mass flow is captured as an image data stream using at least one camera, and wherein the image data stream is evaluated by an image processing unit, wherein the image data stream is evaluated by the image processing unit using a deep neural network in order to detect and locate the containers which have fallen over and/or are damaged.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="192.53mm" wi="108.54mm" file="US20230005127A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="208.03mm" wi="110.57mm" file="US20230005127A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="177.55mm" wi="80.43mm" file="US20230005127A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="204.72mm" wi="130.56mm" file="US20230005127A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="229.53mm" wi="98.72mm" file="US20230005127A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="162.14mm" wi="98.81mm" file="US20230005127A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The invention relates to a method and device for detecting containers which have fallen over and/or are damaged in a container mass flow.</p><heading id="h-0002" level="1">BACKGROUND AND SUMMARY</heading><p id="p-0003" num="0002">In beverage processing systems, it is common practice to transport the containers between individual container handling machines as a container mass flow vertically on a transporter. It can occasionally happen that individual containers fall over during transport and can therefore no longer be processed properly by the subsequent container handling machine or cause a jam. Consequently, the containers which have fallen over must be detected in the container mass flow in order to remove them subsequently. It is also conceivable that damaged containers are detected in the container mass flow and removed from it.</p><p id="p-0004" num="0003">DE 201 10 686 U1 discloses a device for detecting containers lying on a multi-track transporter by means of sensors arranged above them. Contactless ultrasonic sensors are used as sensors.</p><p id="p-0005" num="0004">US 2017/0267462 A1 discloses a device and a method for intervention on a conveyor belt. The conveyor belt is monitored by a sensor, for example an ultrasonic or laser sensor. Lying products can be removed with a gripping tool.</p><p id="p-0006" num="0005">Furthermore, EP 2 295 156 A2 proposes a conveyor system with a device for detecting articles which have fallen over and methods for controlling it, wherein the conveyed objects or articles are optically detected by a laser scanner within a defined area.</p><p id="p-0007" num="0006">WO 2008/116546 A2 discloses a method for monitoring, controlling and optimizing filling systems for foodstuffs, in particular for beverage bottles. An optoelectronic detection system with a thermal imaging camera is proposed for controlling or monitoring the system, wherein methods of image analysis and/or object recognition are applied in the associated data processing system.</p><p id="p-0008" num="0007">Disadvantageous with the known methods and devices is that they represent individual solutions, which must be adapted in each case to the different applications. For example, this is done by sensors adapted to the application and/or special programming of the image processing. Furthermore, they usually have to be precisely adapted to container parameters and are dependent on the ambient conditions.</p><p id="p-0009" num="0008">It is therefore the object of the present invention to provide a method and a device for detecting containers which have fallen over and/or are damaged in a container mass flow, which is easier and more flexible to implement.</p><p id="p-0010" num="0009">In order to solve the problem, the invention provides a method for detecting containers which have fallen over and/or are damaged in a container mass flow.</p><p id="p-0011" num="0010">Due to the fact that the container mass flow is captured by the at least one camera as an image data flow and the image data flow is evaluated by the image processing unit with the deep neural network, the evaluation takes place on the basis of previously learned empirical values of the deep neural network, so that vertical and fallen over and/or damaged containers are detected. Because it is possible to train the deep neural network with images of a wide variety of container types and/or ambient conditions, it is then no longer necessary to adapt the evaluation of the image data stream in the specific application. Consequently, the method according to the invention is particularly flexible and easy to implement. In addition, the neural network is trained and maintained centrally here for a plurality of different beverage processing systems and/or container types. Consequently, the method according to the invention does not have to be set up on site at a customer's premises at great expense during commissioning or conversion of the beverage processing system using expert knowledge. In addition, the evaluation with the deep neural network is particularly reliable. This means that the method can be used particularly reliably in unattended operation, especially in night operation without personnel. Furthermore, the method offers the advantage of active accident and personal protection, since fallen over and/or damaged containers do not have to be removed manually from the container mass flow by the operating personnel. This applies all the more since the containers in the mass container flow are subject to dynamic pressure and intervention by the operating personnel to remove a container due to the sudden relief of the container flow involves a risk of accidents such as crushing and cutting.</p><p id="p-0012" num="0011">The method for detecting containers which have fallen over and/or are damaged can be implemented in a beverage processing system. The method may be upstream or downstream of a container manufacturing process, cleaning process, filling process, capping process, and/or packaging process. In particular, the method may be used in a transport from a first container handling process to a subsequent, second container handling process.</p><p id="p-0013" num="0012">Containers can be provided to accommodate beverages, foodstuffs, hygiene articles, pastes, chemical, biological and/or pharmaceutical products. The containers can be in the form of bottles, in particular plastic bottles, glass bottles or (metal) cans. Plastic bottles can be PET, PEN, HD-PE or PP bottles. Likewise, they can be biodegradable containers or bottles whose main components consist of renewable raw materials, such as sugar cane, wheat or corn. It is conceivable that the containers are provided with a closure.</p><p id="p-0014" num="0013">Here, &#x201c;vertical containers&#x201d; can mean containers that stand on the transporter with a contact surface provided for transport. For example, the contact surface can be an annular surface area of a bottle base. Here, the &#x201c;containers which have fallen over&#x201d; can mean containers that lie on the transporter with a container side that deviates from the contact surface provided for transport, for example with a side surface.</p><p id="p-0015" num="0014">It is conceivable, for example, that the transporter comprises a conveyor belt on which the containers are transported vertically into a detection area of the camera.</p><p id="p-0016" num="0015">The transporter can be configured as a mass transporter with which the containers are transported in multiple rows. The containers can be transported vertically on the mass transporter. The mass transporter can thus transport at least 1.5 of the containers side by side, preferably at least two or even more of the containers side by side transverse to a transport direction. Here, &#x201c;at least 1.5 of the containers&#x201d; can mean that the containers are transported offset and/or in at least two interlocked rows. Transport in multiple rows can mean transport in several separate rows of containers next to each other or transport in a disordered manner. The mass transporter can comprise a conveyor belt and two railings arranged laterally thereon, which are spaced apart with a transport width of at least 1.5 containers transverse to the transport direction. In this way, the containers are guided on the conveyor belt with the railings during multi-row transport. However, the railings can also be spaced apart with at least two containers or even more containers. It is also conceivable that the mass transporter comprises at least one guide rail between the two railings, with which the containers are guided in several separate rows. The transporter may comprise at least one flexible belt, link belt chain and/or mass mat chain. It is also conceivable that the conveyor belt comprises several link belt chains next to each other, which are driven and/or controlled individually. This allows better control of the multiple-row transport and better distribution, build-up or reduction of the accumulation pressure created during mass transport. The conveyor belt can be mounted around two rollers and/or driven. Preferably, the mass transporter can comprise one or more drives, in particular an electric motor, which drives the conveyor belt.</p><p id="p-0017" num="0016">The at least one camera may comprise a lens and an image sensor to optoelectronically detect the container mass flow. For example, the image sensor may comprise a CMOS sensor or a CCD sensor. It is conceivable that the image sensor comprises a line sensor or an area sensor. The at least one camera may be connected to the image processing unit via a data interface to transmit the image data stream. The data interface may comprise an analog or a digital data interface.</p><p id="p-0018" num="0017">The image processing unit can process the image data stream with a signal processor and/or with a CPU. It is also conceivable that the image processing unit comprises a memory unit, one or more data interfaces, for example a network interface, a display unit and/or an input unit for this purpose. The image processing unit can split the image data stream into individual images, each of which is evaluated individually with the deep neural network. It is also conceivable that the image processing unit evaluates the image data stream with image processing algorithms, in particular filters and the like.</p><p id="p-0019" num="0018">The deep neural network may include an input layer, an output layer, and at least two intervening hidden layers. The output layer may be connected to the input layer via the at least two hidden layers. The image data stream may be fed to the input layer, in particular images of the image data stream. Using the output layer, signals may be output to indicate, for each container, a probability of whether it is vertical or whether it has fallen over and/or whether it is damaged. In addition, the output layer can be used to output signals indicating the position of the respective container on the transporter. It is also conceivable that signals are output with the output layer as to how the containers are oriented in each case. The input layer, the at least two hidden layers and/or the output layer can each include neural nodes and/or be connected to each other via neural connections.</p><p id="p-0020" num="0019">The deep neural network can be trained with a training data set of images of containers that are vertical and containers which have fallen over and/or are damaged, such that the deep neural network develops a model based on the training data set to distinguish the vertical and fallen over and/or damaged containers of the container mass flow from each other. This allows the deep neural network to be trained with a high number of different cases so that the evaluation is largely independent of container type and/or ambient influences. For example, the training dataset may include images of containers of different sizes, alignments, or positions. The images of the training data set can be captured with the at least one camera. It is conceivable that this is done in a test system or directly on site at an operator of the beverage processing system. It is also conceivable that a database with images of vertical and fallen over and/or damaged containers is created at the manufacturer of the beverage processing system in order to then use these with the training data set.</p><p id="p-0021" num="0020">It is conceivable that the training data set includes images from the container mass flow of the beverage processing system of a beverage manufacturer and that the training data set is transmitted to the manufacturer of the beverage processing system (e.g., via the Internet) and that the deep neural network is then trained at the manufacturer with the training data set. This allows the deep neural network to be trained and/or tested centrally by experts.</p><p id="p-0022" num="0021">The training data set can include images of the vertical and the fallen over and/or damaged containers with different container types. This allows the deep neural network to be trained particularly well for different container types.</p><p id="p-0023" num="0022">It is also conceivable that at least one of the images of the training data set includes a combination of different container types. This allows particularly reliable detection and localization of different container types in a container mass flow.</p><p id="p-0024" num="0023">The training data set can include images of the vertical and the fallen over and/or damaged containers with different ambient conditions, in particular illumination conditions. This allows the fallen over and/or damaged containers to be detected particularly well regardless of the ambient conditions. It is conceivable that the training data set includes images with different sun positions, illuminance levels and the like.</p><p id="p-0025" num="0024">In the images of the training data set and/or in metadata of the images, the vertical and/or fallen over and/or damaged containers can be characterized, in particular via at least one surrounding box (bounding box). This allows the deep neural network to be provided with a particularly large amount of information about the vertical and/or the fallen over and/or the damaged containers. In addition, the surrounding box makes it possible for the training data set to contain information that describes the orientation and location of the vertical and/or the fallen over and/or the damaged containers.</p><p id="p-0026" num="0025">The images of the training data set can be automatically duplicated to create further images with additional combinations of vertical and fallen over and/or damaged containers. This can significantly reduce the effort required to create the training data set. It is conceivable that image segments with one vertical or fallen over and/or damaged container each are created during the duplication process. The image segments can originate from an original data set. It is conceivable that the image segments are individually rotated and/or enlarged during duplication. It is also conceivable that at least one exposure parameter is changed for the image segments during duplication. Subsequently, the image segments can be reassembled to form the images of the training data set. In this way, a very large number of different images of the training data set can be provided via a few original images. The exposure parameter can mean a brightness and/or a contrast of an image segment.</p><p id="p-0027" num="0026">The fallen over and/or damaged containers can be automatically separated from the vertically transported containers of the container mass flow after detection and localization by the deep neural network, in particular with a gripper arm or with a switch. This allows the fallen over and/or damaged containers to be removed from the transporter without interrupting the container mass flow. The gripper arm can, for example, be a robot with a gripping tool.</p><p id="p-0028" num="0027">It is conceivable that the image data stream is continuously captured and divided into individual images by means of a sliding window, wherein the individual images are subsequently evaluated with the deep neural network. This makes it particularly easy to provide the image data stream for processing by the deep neural network. In this case, it is conceivable that the camera is configured as a line scan camera with which the container mass flow is continuously captured. By the sliding window, a sliding window algorithm can be meant here. In other words, the sliding window can be an image area of the image data stream that is continuously shifted by fixed steps.</p><p id="p-0029" num="0028">Furthermore, to solve the problem, the invention provides a device for detecting containers which have fallen over and/or are damaged in a container mass flow.</p><p id="p-0030" num="0029">Due to the fact that the container mass flow is captured by the at least one camera as an image data stream and the image processing unit comprises the deep neural network for evaluating the image data stream, the evaluation is performed on the basis of previously learned empirical values of the deep neural network, so that vertical and fallen over and/or damaged containers are detected. Since it is possible to train the deep neural network with images of different types of containers and/or ambient conditions, it is no longer necessary to adapt the evaluation of the image data stream in the specific application. Consequently, the method according to the invention is particularly flexible and easy to implement. In addition, the evaluation with the deep neural network is particularly reliable. This allows the method to be implemented particularly reliably in unattended operation, especially during nighttime operation without personnel. Furthermore, the device offers the advantage of active accident and personal protection, since fallen over and/or damaged containers do not have to be removed manually from the container mass flow by the operating personnel. This applies all the more since the containers in the mass container flow are subject to dynamic pressure among each other and intervention by the operating personnel to remove a container due to the sudden relief of the container flow involves a risk of accidents such as crushing and cutting.</p><p id="p-0031" num="0030">The device for identifying the fallen over and/or damaged containers in the container mass flow can be arranged in a beverage processing system. It is conceivable that at least one container handling machine is arranged upstream and/or downstream of the transporter. In other words, the transporter can connect two container handling machines.</p><p id="p-0032" num="0031">The device may comprise the features previously described with respect to the method, individually or mutatis mutandis in any combination.</p><p id="p-0033" num="0032">The image processing unit may include a storage medium comprising machine instructions that, when executed with the image processing unit, evaluate the image data stream with the deep neural network. In other words, the image processing unit may include the deep neural network. It is conceivable that the storage medium comprises machine instructions that can be used to execute, at least partially, the method described above. In particular, the machine instructions can execute those parts of the method which are executed with the image processing unit and/or with the deep neural network.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0034" num="0033">Further features and advantages of the invention are explained in more detail below with reference to the embodiments shown in the Figures. Therein:</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an embodiment of a device according to the invention for detecting containers which have fallen over in a container mass flow in a top view;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows two exemplary images of the image data stream output by the camera of <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an embodiment of a method according to the invention for detecting containers which have fallen over as a flow diagram;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an embodiment of a subsection of the method of <figref idref="DRAWINGS">FIG. <b>3</b></figref> for training the deep neural network; and</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an embodiment of a subsection of the method in <figref idref="DRAWINGS">FIG. <b>3</b></figref> for evaluating the image data stream with the deep neural network.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an embodiment of a device <b>1</b> according to the invention for detecting containers <b>3</b> which have fallen over in a container mass flow M in a top view. The transporter <b>5</b> can be seen, which is exemplarily configured here as a conveyor belt and on which the containers <b>2</b>, <b>3</b> of the container mass flow are transported in the direction R. The transporter <b>5</b> is primarily configured to transport the containers <b>2</b> in a vertical position. However, some containers <b>3</b> can also be seen which have fallen over, for example as a result of vibrations or when the containers are being guided. It is also conceivable that the device is alternatively or additionally configured to detect damaged containers. Since the fallen over and/or damaged containers <b>3</b> cannot be properly processed upstream of a subsequent container handling machine or may cause a jam, they must be detected and removed from the transporter <b>5</b>.</p><p id="p-0041" num="0040">It can also be seen that the transporter <b>5</b> is configured as a mass transporter with which the containers <b>2</b> are transported in multiple rows. In the embodiment, the containers <b>2</b> are transported in a disordered manner next to each other. For this purpose, the transporter <b>5</b> comprises two railings, not shown in more detail, at right angles to the direction R (transport direction) on both sides, with which the containers <b>2</b> are guided laterally. In addition, the transporter <b>5</b> comprises a conveyor belt, for example a link belt chain and/or a mass mat chain, which is driven by a drive. On such a mass transporter, the containers <b>3</b> which have fallen over have a particularly disruptive effect on the flow of the other containers <b>2</b> and must therefore be detected. This circumstance applies in particular before the containers reach a container handling machine located downstream of the container flow.</p><p id="p-0042" num="0041">For this purpose, the camera <b>6</b> is arranged on the transporter <b>5</b>, which detects the vertical containers <b>2</b> and the fallen over containers <b>3</b> from diagonally above. The arrangement of the camera <b>6</b> is shown here only by way of example. It is also conceivable that several cameras are present which face from obliquely above in the same direction or in opposite directions. Also conceivable is an arrangement directly from above perpendicular to a transport surface of the transporter <b>5</b>.</p><p id="p-0043" num="0042">Thus, the camera <b>6</b> captures the container mass flow M as an image data stream and transmits it to the image processing unit <b>7</b> by means of the data interface <b>8</b> in order to evaluate the image data stream with the neural network <b>71</b>.</p><p id="p-0044" num="0043">For this purpose, the image processing unit <b>7</b> comprises a storage medium containing machine instructions which, when executed with the image processing unit <b>7</b>, evaluate the image data stream with the deep neural network <b>71</b>.</p><p id="p-0045" num="0044">The neural network <b>71</b> is configured to detect and locate the containers <b>3</b> which have fallen over. Based on the evaluation, the containers <b>3</b> which have fallen over can then be removed from the transporter <b>5</b> with a switch not shown here or by means of a gripper arm.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows two exemplary images I<b>1</b> and I<b>2</b> of the image data stream output by the camera <b>6</b> from <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0047" num="0046">It is conceivable that the camera <b>6</b> comprises an area sensor with which the images I<b>1</b>, I<b>2</b> are each captured over an area at one point in time. Alternatively, it is also conceivable that the camera <b>6</b> comprises a line sensor with which the image data stream is continuously captured and divided into individual images I<b>1</b>, I<b>2</b> by means of a sliding window, wherein the individual images I<b>1</b>, I<b>2</b> are subsequently evaluated with the deep neural network <b>71</b>.</p><p id="p-0048" num="0047">Furthermore, it can be seen in the images I<b>1</b>, I<b>2</b> that after evaluation by the deep neural network <b>71</b>, the vertical containers <b>2</b> are each characterized with a surrounding box <b>21</b>. Furthermore, the containers <b>3</b> which have fallen over are also characterized with another surrounding box <b>31</b>, which on the one hand marks it and on the other hand indicates its location and orientation. Based on this information, the exact position of the fallen over containers <b>3</b> on the transporter <b>5</b> can then be output as a signal or the fallen over container(s) <b>3</b> can be tracked by signal up to the switch not shown here and automatically sorted out.</p><p id="p-0049" num="0048">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an embodiment of a method <b>100</b> according to the invention for detecting containers <b>3</b> which have fallen over is shown as a flowchart. It can be seen that in step <b>110</b>, the deep neural network is first trained with a training data set with images of vertical and fallen over containers, so that the deep neural network develops a model based on the training data set. Based on this model, the deep neural network can then recognize in operation what is a vertical container and what is a container that has fallen over.</p><p id="p-0050" num="0049">The method <b>100</b> is described in more detail with reference to containers <b>3</b> which have fallen over. It is conceivable that the method <b>100</b> is alternatively or additionally configured to detect damaged containers. Accordingly, damaged containers not shown in more detail here are trained in the training data set. These containers may show deformations as well as broken containers.</p><p id="p-0051" num="0050">The training data set can be obtained from a set greater than 1000, preferably greater than 5000, and more preferably greater than 10000 images.</p><p id="p-0052" num="0051">In step <b>120</b>, the containers <b>2</b> of the container mass flow are transported vertically on the transporter <b>5</b>. It can occasionally happen that one of the containers <b>2</b> falls over and then lies on the transporter <b>5</b> as a fallen over container <b>3</b>.</p><p id="p-0053" num="0052">In order to detect the containers <b>3</b> which have fallen over, the container mass flow M is first captured with at least one camera <b>6</b> as an image data stream (step <b>130</b>), which is then evaluated by the image processing unit <b>7</b> with the deep neural network <b>71</b> (step <b>140</b>).</p><p id="p-0054" num="0053">The containers <b>3</b> which have fallen over are automatically separated (step <b>150</b>) from the containers <b>2</b> of the container mass flow that are transported vertically after detection and localization, for example with a gripper arm or with a switch.</p><p id="p-0055" num="0054">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an embodiment of the subsection <b>110</b> of the method <b>100</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> for training the deep neural network <b>110</b> is shown in more detail as a flowchart.</p><p id="p-0056" num="0055">First, in step <b>111</b>, images of different container types and/or different lighting conditions are captured. It is conceivable, for example, that this is done on a test system or that images of the container mass flow M of different beverage processing systems are collected in a database.</p><p id="p-0057" num="0056">Subsequently, the images are scaled to a standard dimension in step <b>112</b>. This allows them to be evaluated uniformly.</p><p id="p-0058" num="0057">In step <b>113</b>, the containers <b>2</b>, <b>3</b> which have fallen over and those that are vertical are marked and classified. This can be done manually, semi-automatically or automatically. For example, this can be done manually by an operator at a screen or with a particularly computationally intensive image processing algorithm. The marking can, for example, be a surrounding box and the classification can be a container type or size.</p><p id="p-0059" num="0058">Subsequently, in step <b>114</b>, the images are automatically duplicated to create further images with additional combinations of vertical and fallen over containers <b>2</b>, <b>3</b>. First, image segments are created each with an vertical or a fallen over container <b>2</b>, <b>3</b>, which are then individually rotated and/or enlarged for duplication. It is also conceivable that exposure parameters of the image segments are changed during duplication. Subsequently, the image segments can be assembled in various combinations as further images, from which the training data set is then generated in step <b>115</b>.</p><p id="p-0060" num="0059">In the subsequent step <b>116</b>, features are automatically extracted by means of the deep neural network <b>71</b>. For example, a multi-stage filtering method of the training data set is used. It is conceivable that edge filters or the like are used in this process to extract the outer boundary of each individual container <b>2</b>, <b>3</b>.</p><p id="p-0061" num="0060">Feature extraction here can mean, in general terms, a method for detecting and/or locating distinguishing features of the containers <b>3</b> which have fallen over compared to the vertical containers <b>2</b> in the images of the training data set. As an alternative to automatic extraction using the deep neural network <b>71</b>, this can also be done manually by an operator. For example, the extracted features may include a container closure, a contour of an vertical or fallen over container <b>2</b>, <b>3</b>, a container label, and/or light reflections. The extracted features may each include a feature classification, a 2D and/or 3D coordinate.</p><p id="p-0062" num="0061">Subsequently, in step <b>117</b>, the deep neural network <b>71</b> is trained with the training data set. In this context, the deep neural network <b>71</b> is iteratively given images of the training data set with the extracted features as well as the associated markings and classifications of the fallen over and vertical containers <b>2</b>, <b>3</b>. From this, the deep neural network <b>71</b> develops a model in step <b>118</b>, with which the fallen over and the vertical containers <b>2</b>, <b>3</b> can be recognized.</p><p id="p-0063" num="0062">In the subsequent step <b>119</b>, the model can then be verified using the training data set without predefining the markings and classifications. In this context, it is compared whether the deep neural network <b>71</b> actually recognizes the previously specified markings and classifications in the training data set. Likewise, further images with fallen over and vertical containers <b>2</b>, <b>3</b> can be used for this purpose, on which the deep neural network <b>71</b> was not trained.</p><p id="p-0064" num="0063">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the sub-step <b>140</b> of the method <b>100</b> from <figref idref="DRAWINGS">FIG. <b>3</b></figref> for evaluating the image data stream with the deep neural network <b>71</b> is shown in more detail as a flowchart.</p><p id="p-0065" num="0064">It can be seen that the images of the image data stream from <figref idref="DRAWINGS">FIG. <b>1</b></figref> are first scaled. Thus, the evaluation works independently of the actual configuration of the camera.</p><p id="p-0066" num="0065">Subsequently, in step <b>142</b>, the features are extracted. This is done in the same way as described in step <b>116</b> with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0067" num="0066">Subsequently, in step <b>143</b>, the deep neural network detects the orientation and location of the respective container <b>2</b>, <b>3</b> and indicates a probability whether this container <b>2</b>, <b>3</b> is transported lying or vertically on the transporter <b>5</b>.</p><p id="p-0068" num="0067">This information is then visualized in step <b>144</b> and displayed on a screen as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. This allows an operator to verify that the detection is proceeding properly.</p><p id="p-0069" num="0068">Furthermore, if a container <b>3</b> has fallen over, a signal is output in step <b>145</b> to remove it from the transporter <b>5</b>, for example with a switch or a gripper arm.</p><p id="p-0070" num="0069">Since the container mass flow M is captured as an image data flow by the at least one camera <b>6</b> and the image data flow is evaluated by the image processing unit <b>7</b> with the deep neural network <b>71</b>, the images of the image data flow can be evaluated on the basis of previously learned empirical values of the deep neural network <b>71</b> in order to classify the vertical and fallen over containers <b>2</b>, <b>3</b> respectively. Since it is possible to train the deep neural network <b>71</b> with images of a wide variety of container types and/or ambient conditions, it is no longer necessary to adapt the evaluation of the image data stream in the specific application. Consequently, the method according to the invention is particularly flexible and can be easily implemented.</p><p id="p-0071" num="0070">It is understood that features mentioned in the previously described embodiments are not limited to this combination, but are also possible individually or in any other combinations.</p><p id="p-0072" num="0071">The following claims particularly point out certain combinations and sub-combinations regarded as novel and non-obvious. These claims may refer to &#x201c;an&#x201d; element or &#x201c;a first&#x201d; element or the equivalent thereof. Such claims should be understood to include incorporation of one or more such elements, neither requiring nor excluding two or more such elements. Other combinations and sub-combinations of the disclosed features, functions, elements, and/or properties may be claimed through amendment of the present claims or through presentation of new claims in this or a related application. Such claims, whether broader, narrower, equal, or different in scope to the original claims, also are regarded as included within the subject matter of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for detecting containers which have fallen over and/or are damaged in a container mass flow, wherein the containers of the container mass flow are transported vertically on a transporter, wherein the container mass flow is captured using at least one camera as an image data flow, and wherein the image data flow is evaluated by an image processing unit,<claim-text>wherein</claim-text><claim-text>the image data flow is evaluated by the image processing unit using a deep neural network in order to detect and locate the containers which have fallen over and/or are damaged.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the deep neural network is trained with a training data set comprising images of fallen over and/or damaged containers, such that the deep neural network develops a model based on the training data set to distinguish the fallen over and/or damaged containers from each other.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the training data set comprises images of the fallen over and/or damaged containers with different container types.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein at least one of the images of the training data set comprises a combination of different container types.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the training data set comprises images of the fallen over and/or damaged containers with different ambient conditions.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein in the images of the training data set and/or in metadata of the images the fallen over and/or damaged containers are characterized by at least one surrounding box.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the images of the training data set are automatically duplicated to produce further images with additional combinations of fallen over and/or damaged containers.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein image segments each comprising a fallen over and/or damaged container are generated during the duplication.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the image segments are individually rotated and/or enlarged during the duplication.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein at least one exposure parameter is changed in the image segments during the duplication.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the fallen over and/or damaged containers are automatically separated from vertically transported containers of the container mass flow after detection and localization by the deep neural network.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the image data flow is continuously captured and divided into individual images by means of a sliding window, and wherein the individual images are subsequently evaluated with the deep neural network.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the transporter is configured as a mass transporter with which the containers are transported in multiple rows.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A device for detecting containers which have fallen over and/or are damaged in a container mass flow for carrying out the method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, with<claim-text>a transporter for the vertical transport of the containers of the container mass flow,</claim-text><claim-text>at least one camera to capture the container mass flow as an image data stream, and</claim-text><claim-text>with an image processing unit to evaluate the image data stream,</claim-text><claim-text>wherein</claim-text><claim-text>the image processing unit comprises a deep neural network for evaluating the image data stream in order to detect and locate the containers which have fallen over and/or are damaged.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the image processing unit comprises a storage medium containing machine instructions that, when executed with the image processing unit, evaluate the image data stream with the deep neural network.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the ambient conditions are illumination conditions.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the fallen over and/or damaged containers are automatically separated from the vertically transported containers of the container mass flow after detection and localization by the deep neural network with a gripper arm or with a switch.</claim-text></claim></claims></us-patent-application>