<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007080A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007080</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17561777</doc-number><date>20211224</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2021-0087313</doc-number><date>20210702</date></priority-claim><priority-claim sequence="02" kind="national"><country>KR</country><doc-number>10-2021-0127935</doc-number><date>20210928</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>1097</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>69</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>568</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>1097</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>69</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>2842</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e79">COMPUTING DEVICE AND STORAGE CARD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KOREA ADVANCED INSTITUTE OF SCIENCE AND TECHNOLOGY</orgname><address><city>Daejeon</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Jung</last-name><first-name>Myoungsoo</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Gouk</last-name><first-name>Donghyun</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Kwon</last-name><first-name>Miryeong</first-name><address><city>Daejeon</city><country>KR</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A first IP address is set to the host, and a second IP address is set to the storage card. A storage card includes a storage device and a processor for executing firmware. The host converts a first Ethernet packet including an ISP-related request and destined for the second IP address into an NVMe request according to an NVMe protocol, and transfers the NVMe request to the storage card. The firmware parses the NVMe request to perform the ISP-related request.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="71.97mm" wi="158.75mm" file="US20230007080A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="133.86mm" wi="99.99mm" file="US20230007080A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="128.78mm" wi="113.45mm" file="US20230007080A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="173.40mm" wi="119.63mm" orientation="landscape" file="US20230007080A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="133.35mm" wi="99.57mm" file="US20230007080A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="166.54mm" wi="130.56mm" orientation="landscape" file="US20230007080A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="186.10mm" wi="134.11mm" orientation="landscape" file="US20230007080A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="178.31mm" wi="118.36mm" orientation="landscape" file="US20230007080A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="131.15mm" wi="101.77mm" file="US20230007080A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="182.80mm" wi="128.35mm" orientation="landscape" file="US20230007080A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="175.18mm" wi="144.36mm" orientation="landscape" file="US20230007080A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="111.08mm" wi="103.21mm" file="US20230007080A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="110.32mm" wi="94.15mm" file="US20230007080A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="101.68mm" wi="101.09mm" file="US20230007080A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="100.84mm" wi="119.97mm" file="US20230007080A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority to and the benefit of Korean Patent Application No. 10-2021-0087313 filed in the Korean Intellectual Property Office on Jul. 2, 2021, and Korean Patent Application No. 10-2021-0127935 filed in the Korean Intellectual Property Office on Sep. 28, 2021, the entire contents of which are incorporated herein by reference.</p><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">(a) Field</heading><p id="p-0003" num="0002">The described technology generally relates to a computing device and a storage card.</p><heading id="h-0004" level="1">(b) Description of the Related Art</heading><p id="p-0004" num="0003">In-storage processing (ISP) is a well-known storage model to examine massive data sets, which can be applied in a broad range of big data analytics and emerging applications such as graph semantic analysis, bioinformatics, and machine learning. The ISP is expected to satisfy diverse data processing demands while removing overheads imposed by data movement between a host and a storage device. Since this nature can make ISP an energy-efficient and promising solution, there have been significant efforts in literature to integrate data processing into a non-volatile memory express (NVMe) solid-state drive (SSD) forming the ISP</p><p id="p-0005" num="0004">While industry has also paid great attention to realizing the storage intelligence by offering more powerful computing resources, it is non-trivial for an ISP-enabled SSD to satisfy all needs of various data processing applications. The most difficult part of incarnating a fully flexible ISP model is a practical design of runtime environment of the application and its application programming interface (API), not computing support in SSDs. Because of the protection demands for vendor-sensitive information and intellectual properties, it is almost infeasible to expose all the internal components and development environment of SSD firmware to ISP users. Thus, the SSD runtime environment and APIs to implement ISP tasks should be appropriately prepared and exposed by SSD vendors at a design time. However, building a storage system with such static runtimes and APIs is very challenging since application implementations to be used in the ISP and their requirement greatly change at every moment. Even if a set of well-designed APIs is properly given by the SSD vendors in advance, it enforces significant source-level modifications to ISP algorithms for offloading the existing applications to the ISP.</p><p id="p-0006" num="0005">Most prior studies overlook the charge of servicing all input/output (I/O) requests as a block device while processing data within the storage device. Realizing this functionality of ISP requires a way different model implementation that most data processing accelerators employ such as graphics processing units (GPUs). Specifically, it can raise many storage-related vulnerability and resource protection problems in a real system, which make both ISP tasks and the host using the storage device unsafe to process data in the storage device. For example, since the SSDs have no knowledge on file management of the host, data in the storage device can be updated by the ISP task, and data being processed by the IPS task can be updated any host users, making the results undetermined.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0007" num="0006">Some embodiments may provide a computing device and a storage card for implementing ISP technology to execute various applications within the storage card.</p><p id="p-0008" num="0007">According to an embodiment, a computing device including a host to which a first internet protocol (IP) address is set and a storage card to which a second IP address is set may be provided. The storage card may include a computing complex configured to execute firmware and a storage device. The host may convert a first Ethernet packet including an ISP-related request and destined for the second IP address into a first NVMe request according to an NVMe protocol, and transfer the first NVMe request to the storage card. The firmware may parse the first NVMe request to perform the ISP-related request.</p><p id="p-0009" num="0008">In some embodiments, the host may execute a network driver. The network driver may copy data of the first Ethernet packet to a kernel page, fill a physical region page (PRP) field of the first NVMe request with an address of the kernel page, set an operation code of the first NVMe request to a value indicating a transmit frame command, and transfer the first NVMe request to the storage card through an NVMe driver.</p><p id="p-0010" num="0009">In some embodiments, the host may execute a network driver. In order to receive a second Ethernet packet destined for the first IP address from the storage card, the network driver may submit a second NVMe request to an NVMe queue, allocate a kernel page to allow the storage card to write data of the second Ethernet packet to the kernel page, set an operation code of the second NVMe request to a value indicating a receive frame command, and generate the second Ethernet packet by reading the data from the kernel page of the second NVMe request completed by the storage card.</p><p id="p-0011" num="0010">In some embodiments, the ISP-related request may include an offloading request of a container image or a container execution request for executing a container.</p><p id="p-0012" num="0011">In some embodiments, the firmware may include a mini-Docker layer configured to performs a docker function for container control and service for the ISP, and an operating system (OS) feature layer configured to perform an OS feature.</p><p id="p-0013" num="0012">In some embodiments, the firmware may further include a host interface layer. The host interface layer may include a command parser configured to parse the first NVMe request and select a service path or a data path.</p><p id="p-0014" num="0013">In some embodiments, the host interface layer may further include a network driver. The command parser may select the service path in response to an operation code of the first NVMe request having a predetermined value, and select the data path in response to the operation code of the first NVMe request not having the predetermined value. In response to selection of the service path, the network driver may copy the first Ethernet packet stored in a memory of the host indicated by a PRP of the first NVMe request to an internal memory of the computing complex, and notify a location of the internal memory to which the data is copied to the OS feature layer.</p><p id="p-0015" num="0014">In some embodiments, the firmware may further include an internal cache layer and a flash translation layer. In response to selection of the data path, data of the first NVMe request may be stored in the storage device through the internal cache layer and the flash translation layer.</p><p id="p-0016" num="0015">In some embodiments, the OS feature layer may include a network handler configured to perform a network function and an input/output (I/O) handler configured to perform an I/O function. The mini-Docker layer may include an API server, an image handler configured to manage a container image, and an execution handler configured to execute a container.</p><p id="p-0017" num="0016">In some embodiments, in response to a port number of the first Ethernet packet having a value associated with a Docker command line interface, the network handler may forward the first Ethernet packet to the API server. The API server may transfer information obtained by parsing the first Ethernet packet to the image handler or the execution handler.</p><p id="p-0018" num="0017">In some embodiments, in response to the port number of the first Ethernet packet does not have the value associated with the Docker command line interface, the network handler may forward the first Ethernet packet to the container.</p><p id="p-0019" num="0018">In some embodiments, the container may generate an I/O request for the ISP or a network request to communicate with the host.</p><p id="p-0020" num="0019">In some embodiments, the I/O handler may forward the I/O request to the data path.</p><p id="p-0021" num="0020">In some embodiments, the network handler may forward the network request to the host through an asynchronous upcall via the network driver.</p><p id="p-0022" num="0021">In some embodiments, the OS feature layer may further include a thread handler. The thread handler may separate a memory space of the storage device into a container pool accessible by the container and a firmware pool accessible by the firmware, and may provide a physical memory page from either the container pool or the firmware pool.</p><p id="p-0023" num="0022">In some embodiments, the mini-Docker layer may further include a log handler. The log handler may manage container monitoring, and may transmit container monitoring information to the host through the network handler and the network module in response to receiving a logging command from the host.</p><p id="p-0024" num="0023">According to another embodiment, a storage card connected to a host may be provided. The storage card may include a processor configured to execute firmware and a storage device. The firmware may include a host interface layer, a mini-Docker layer, and an operating system (OS) feature layer. The host interface layer may select a service path or a data path by parsing an NVMe request according to an NVMe protocol received from the host. The mini-Docker layer may include an image handler configured to perform container image management based on a network packet included in the NVMe request, and an execution handler configured to execute a container based on the network packet. The OS feature layer may include a network handler. In response to selection of the service path, the network handler may forward the network packet to the mini-Docker layer or the container, and may forward a network request, generated in the container, for communicating with the host to the host interface layer.</p><p id="p-0025" num="0024">In some embodiments, the mini-Docker layer may further include an API server. The network handler may forward the network packet to the API server in response to a port number of the network packet having a predetermined value, and forward the network packet to the container in response to the port number of the network packet not having the predetermined value have. The API server may transfer information of the network packet to the image handler or the execution handler.</p><p id="p-0026" num="0025">In some embodiments, the firmware may further include an internal cache layer and a flash translation layer, and the OS feature layer may further include an I/O handler. The I/O handler may forward the network packet to the internal cache layer in response to selection of the data path.</p><p id="p-0027" num="0026">In some embodiments, the I/O handler may forward an I/O request for an ISP generated in the container to the data path.</p><p id="p-0028" num="0027">In yet another embodiment, a computer program stored in a non-transitory recording medium and configured to be executed by a computing device including a host and a storage card may be provided. The computer program may cause the computing device to execute setting a first internet protocol (IP) address and a second IP address to the host and the storage card, respectively, converting a first Ethernet packet, which includes an in-storage processing (ISP)-related request and is destined for the second IP address, into a first non-volatile memory express (NVMe) request according to an NVMe protocol, and forwarding the first NVMe request to an NVMe drive to transfer the first NVMe request to the storage card.</p><p id="p-0029" num="0028">In some embodiments, converting the first Ethernet packet into the first NVMe request may include copying data of the first Ethernet packet to a kernel page, filling a physical region page (PRP) field of the first NVMe request with an address of the kernel page, and setting an operation code of the first NVMe request to a value indicating a transmit frame command.</p><p id="p-0030" num="0029">In some embodiments, the computer program may cause the computing device to further execute, in order to receive a second Ethernet packet destined for the first IP address from the storage card, submitting a second NVMe request to an NVMe queue, allocating a kernel page to allow the storage card to write data of the second Ethernet packet to the kernel page, setting an operation code of the second NVMe request to a value indicating a receive frame command, and generating the second Ethernet packet by reading the data from the kernel page of the second NVMe request completed by the storage card.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an example block diagram of a computing device according to an embodiment.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example block diagram of a storage card.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example diagram showing a firmware stack of a storage card.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example diagram showing a Docker stack in OS-level virtualization.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example diagram showing container creation in OS-level virtualization.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example diagram for explaining limitations of the conventional ISP.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example diagram for explaining a storage card according to an embodiment.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example diagram for explaining an example of a filesystem of a computing device according to an embodiment.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example diagram showing NVMe Ethernet communication in a computing device according to an embodiment.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an example diagram for explaining firmware of a storage card according to an embodiment.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an example diagram for explaining a thread handler in a storage card according to an embodiment.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example diagram for explaining an I/O handler in a storage card according to an embodiment.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an example diagram for explaining a network handler in a storage card according to an embodiment.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an example diagram for explaining a mini-Docker layer in a storage card according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0045" num="0044">In the following detailed description, only certain example embodiments of the present invention have been shown and described, simply by way of illustration. As those skilled in the art would realize, the described embodiments may be modified in various different ways, all without departing from the spirit or scope of the present invention. Accordingly, the drawings and description are to be regarded as illustrative in nature and not restrictive. Like reference numerals designate like elements throughout the specification.</p><p id="p-0046" num="0045">As used herein, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise.</p><p id="p-0047" num="0046">The sequence of operations or steps is not limited to the order presented in the claims or figures unless specifically indicated otherwise. The order of operations or steps may be changed, several operations or steps may be merged, a certain operation or step may be divided, and a specific operation or step may not be performed.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an example block diagram of a computing device according to an embodiment.</p><p id="p-0049" num="0048">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a computing device <b>100</b> includes a processor <b>110</b>, a memory <b>120</b>, and a storage card <b>130</b>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example of the computing device, and the computing device may be implemented by various structures.</p><p id="p-0050" num="0049">In some embodiments, the computing device may be any of various types of computing devices. The various types of computing devices may include a mobile phone such as a smartphone, a tablet computer, a laptop computer, a desktop computer, a multimedia player, a game console, a television, and various types of Internet of Things (IoT) devices.</p><p id="p-0051" num="0050">The processor <b>110</b> performs various operations (e.g., operations such as arithmetic, logic, controlling, and input/output (I/O) operations) by executing instructions. The processor may be, for example, a central processing unit (CPU), a graphics processing unit (GPU), a microprocessor, or an application processor (AP), but is not limited thereto. Hereinafter, the processor <b>110</b> is described as a CPU <b>110</b>.</p><p id="p-0052" num="0051">The memory <b>120</b> is a system memory that is accessed and used by the CPU <b>110</b>, and may be, for example, a dynamic random-access memory (DRAM). In some embodiments, the CPU <b>110</b> and the memory <b>120</b> may be connected via a system bus. A system including the CPU <b>110</b> and the memory <b>120</b> may be referred to as a host. The memory <b>120</b> may be referred to as a host memory.</p><p id="p-0053" num="0052">The storage card <b>130</b> includes a computing complex and a storage device <b>133</b>. The computing complex may correspond to a frontend of the storage card <b>130</b>, and the storage device <b>133</b> may correspond to a backend of the storage card <b>130</b>.</p><p id="p-0054" num="0053">The computing complex connects the host including the CPU <b>110</b> and the memory <b>120</b> to the storage device <b>133</b>. In some embodiments, the computing complex may include a processor <b>131</b> and an internal memory <b>132</b>. In some embodiments, the processor <b>131</b> may be a multi-core processor. The internal memory <b>132</b> may include, for example, DRAM. In some embodiments, the computing complex may use a non-volatile memory express (NVMe) protocol as a protocol for accessing the storage device <b>133</b>. Hereinafter, the protocol is described as the NVMe protocol, but embodiments are not limited thereto and other protocols may be used.</p><p id="p-0055" num="0054">The storage device <b>133</b> may include a non-volatile memory-based memory module. In some embodiments, the storage device <b>133</b> may include a flash memory-based memory module. The flash memory-based memory module <b>133</b> may be, for example, a solid state drive (SSD), a secure digital (SD) card, a universal serial bus (USB) flash drive, or the like. In some embodiments, the storage card <b>130</b> may be connected to the host through a host interface. In some embodiments, the host interface may include a peripheral component interconnect express (PCIe) interface. Hereinafter, the host interface is described as a PCIe interface, but embodiments are not limited thereto and other host interfaces may be used.</p><p id="p-0056" num="0055">In some embodiments, the computing device <b>100</b> may further include an interface device <b>140</b> for connecting the storage card <b>130</b> to the host including the CPU <b>110</b> and the memory <b>120</b>. In some embodiments, the interface device <b>140</b> may include a root complex <b>140</b> that connects the host and the storage card <b>130</b> in a PCIe system. In some embodiments, the storage card <b>130</b> may further include an endpoint (not shown), for example a PCIe endpoint, for connection with the interface device <b>140</b>.</p><p id="p-0057" num="0056">First, a typical storage card is described with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example block diagram of a storage card, and <figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example diagram showing a firmware stack of a storage card. <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref> exemplify a PCIe storage card implementing an SSD using a NAND flash memory for convenience of description.</p><p id="p-0059" num="0058">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a storage card <b>200</b> includes a processor <b>210</b>, an internal memory <b>220</b>, and flash media <b>230</b>. The processor <b>210</b> and the internal memory <b>220</b>, for example, a DRAM <b>220</b>, correspond to a computing complex that is a frontend of the storage card <b>200</b>, and the flash media (NAND flash) <b>230</b> corresponds to a backend of the storage card <b>200</b>. The processor <b>210</b> may include a plurality of cores.</p><p id="p-0060" num="0059">The storage card <b>200</b> is connected to a host through a PCIe endpoint (EP) <b>240</b>. The backend includes a plurality of I/O buses called channels, each connecting the flash media <b>230</b> through a flash memory controller (FMC) <b>250</b>.</p><p id="p-0061" num="0060">The storage card <b>200</b> may include the plurality of flash media <b>230</b> over multiple channels to improve parallelism and increase backend storage density. The DRAM <b>220</b> is used for buffering data between the host and flash media <b>230</b>. The DRAM <b>220</b> may also be used to maintain metadata of firmware running on the processor <b>210</b>.</p><p id="p-0062" num="0061">The storage card <b>200</b> obeys NVMe that defines multiple queues, I/O commands, and data transfer methods over PCIe. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, each queue is logically paired by a submission queue (SQ) <b>31</b> and a completion queue (CQ) <b>32</b>. The SQ <b>31</b> and the CQ <b>32</b> may be formed in a host memory (e.g., <b>120</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). When a user issues an I/O request to the storage card <b>200</b>, a host NVMe driver submits the request (NVMe command) to the SQ <b>31</b> and informs the submission to the storage card <b>200</b> by writing a doorbell register (S<b>310</b>). An NVMe controller of the storage card <b>200</b> fetches a new NVMe command from the SQ <b>31</b> (S<b>320</b>). The NVMe command may be stored in a command queue <b>33</b> of the storage card <b>200</b>. The NVMe controller parses the NVMe command. The NVMe command includes memory addresses called a physical region page (PRP). The PRP indicates where the data to be accessed is located in the host's kernel memory space. Using the PRP, the NVMe controller may directly transfer the data through direct media access (DMA) (S<b>330</b>). Once an I/O service finishes, the NVMe controller records request completion in the CQ (S<b>350</b>) and then notifies the host that the request processing is complete through a message signaled interrupt (MSI) (S<b>360</b>).</p><p id="p-0063" num="0062">The frontend of the storage card <b>200</b> includes firmware <b>300</b> that includes a host interface layer (HIL) <b>301</b>, an internal cache layer (ICL) <b>302</b> and a flash translation layer (FTL) <b>303</b>. The firmware <b>300</b> may be executed by the processor <b>210</b> of the storage card <b>200</b>. The firmware <b>300</b> handles the NVMe protocol and the backend I/O. The HIL <b>301</b> implements the NVMe controller and manages the queues and data transfer. Further, the HIL <b>301</b> parses the NVMe command and extracts I/O information, such as an operation code (opcode) indicating read or write, a logical address (e.g., a logical block address (LBA)), and a block length from the NVMe command. The ICL <b>302</b> copies data from a host memory to the internal DRAM <b>220</b> using the internal DRAM as a data buffer <b>34</b> (S<b>330</b>). The HIL <b>301</b> hides a complexity of firmware via a block interface, and the ICL <b>302</b> reduces backend access latency. The FTL <b>303</b> translates the LBA into a physical flash address (e.g., a physical page number (PPN)) of the flash media <b>230</b>, and transfers the data buffered in the DRAM <b>220</b> to the flash media <b>230</b> (S<b>340</b>). The FTL <b>303</b> manages a reliability of the flash media <b>230</b> by performing garbage collection and wear-leveling.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example diagram showing a Docker stack in operating system (OS)-level virtualization, and <figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example diagram showing container creation in OS-level virtualization.</p><p id="p-0065" num="0064">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a container <b>450</b> utilize control groups (cgroups) and namespaces of an OS (e.g., Linux) kernel to allow a creation of multiple isolated virtual environments on a single host and running of workloads. Each virtual environment becomes a lightweight virtual machine that excludes virtual machine emulation managers but includes all necessary applications and runtime environments in a format of files. The open container initiative (OCI) offers runtime specification (runtime-spec) and image specification (image-spec), which are standards for storing and running the container <b>450</b>. Each standard defines how to load a container image and to run a manifest file that maintains metadata to lunch an application. Docker extends the container to offer portable container deployment, versioning, and component reusability via binary large objects (blobs). The blob serializes each user's applications and runtime environments, which can be quickly installed to any other Docker-enabled systems.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a user (e.g., a user's application) may simply ask for a service to a Docker server, called Docker daemon (dockerd) <b>420</b>, through a Docker command-line interface (docker-cli) <b>410</b>. The dockerd <b>420</b> may enable Docker-only features such as volume management and repository push/pull, and may exchange commands with the user through HTTP REST API. The dockerd <b>420</b> may also configure a virtual bridge network, which allows the user and each container <b>450</b> to communicate as an individual host. Underneath the dockerd <b>420</b>, a container daemon (containerd) <b>430</b> manages an image and the container <b>450</b>. A container runtime (runc) <b>440</b> configures namespace and cgroups, and actually runs the container <b>450</b>.</p><p id="p-0067" num="0066">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref> and <figref idref="DRAWINGS">FIG. <b>5</b></figref>, to convert a stored container image to a container, the Docker stack downloads blobs <b>501</b> from a predefined location. Then, the containerd <b>420</b> unpacks each blob <b>501</b> based on the image-spec, which turns it into a configuration file <b>502</b> and file image layers <b>503</b>. Based on the configuration file <b>502</b>, the containerd <b>420</b> merges the layers <b>503</b> and generates a root filesystem (rootfs) <b>504</b> of a container. Based on the runtime-spec, the runc <b>440</b> creates a runtime configuration <b>505</b> such as environment variables and a target entry point for application execution, and mounts the rootfs <b>504</b> to the container <b>506</b> and launches the application.</p><p id="p-0068" num="0067">The cgroups and namespaces are OS kernel features, not container's functionalities. Although the container uses two kernel features, the execution environment of the container follows the runtime-spec and image-spec of the OCI. The cgroups and namespaces in the OS-level virtualization are only used for multi-tenant support by isolating the host computer's resources and throttling performance. Instead, the Docker stack is responsible for creating and executing containers, and each container is just a process on a host.</p><p id="p-0069" num="0068">Next, technical challenges and overheads of the ISP are described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an example diagram for explaining limitations of the conventional ISP. <figref idref="DRAWINGS">FIG. <b>6</b></figref> exemplifies a case in which a minimum spanning tree (MST) algorithm of a graph analysis application is executed in an SSD.</p><p id="p-0071" num="0070">ISP has a deep-rooted history from the early 1980s. The ISP has begun to be applied to low complex applications, such as data scan and filter, and has been extended for general purpose applications in the late 1990s. However, the ISP has experienced many pitfalls and has not been fully successful because of low disk performance and low bandwidth communication. Recently, SSDs have significant technology shifts and become dominant storage in many computing domains since their performance and interface bandwidth almost pull up those capacities of main memory and system buses.</p><p id="p-0072" num="0071">Thus, ISP researches in SSD-resident processing are recently experiencing a resurgence. While there are diverse studies for ISP models, the studies may be classified into two categories: i) domain-specific acceleration (DSA) and ii) function as a service (FaaS) like acceleration. The two categories are similar to domain-specific acceleration (DSA) and function as a service (FaaS). The DSA mainly focuses on creating the ISP optimized for a specific application which can take advantage of data movement reduction. For example, filtering data within an SSD and offloading a key-value stack to storage are implemented. These DSA proposals show the potential benefits of ISP, but they are limited to accelerate the specific application. In addition, they are performed with an assumption that a large amount of or whole of the specific application can be offloaded to the SSD.</p><p id="p-0073" num="0072">In contrast, FaaS-like ISP models design a set of APIs that makes different types of function, which can be integrated into the SSD firmware. They use remote procedure call (RPC) or vendor-customized PCIe packet communications to realize a generic task offloading. Although the FaaS-like studies move towards more general-purpose ISP computation, they are yet way different from a fully flexible ISP. In particular, the FaaS-like ISP models increases data transfers whose latency accounts for 86% of the actual execution time in the SSD. This contradicts the long-standing concept of ISP.</p><p id="p-0074" num="0073">For widespread adoption of ISP, the ideal ISP model should be easy-to-use and not enforce the current computing environment a change, including existing hardware and application. It requires being as much general as the host processor can process data and minimizing the data transfer overhead, such that the host can be assigned to operate other operations in parallel with ISP tasks. However, the existing DSA and FaaS-like ISP models raise five challenges: i) hand-on application partitioning, ii) ignorance of file layouts, iii) ISP task switching overhead, iv) device dependency, and v) data vulnerability. These challenges are described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0075" num="0074">First, the hand-on application partitioning is described. Based on APIs given by vendors, a user decides which part of an application to offload. During this process, the user should be aware of whether a specific part of the application is executed in the host or in the SSD, and modify a code in a way that can be offloaded accordingly. For example, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the user needs to inform a graph file location to the SSD by collaborating with the host filesystem <b>611</b> (S<b>610</b>). The user also needs to decompose a minimum spanning tree (MST) algorithm into two specific ISP tasks including an ISP task (ISP<b>1</b>) for sorting edges of the graph and an ISP task (ISP<b>2</b>) for spanning the tree from the minimum edge weight, which owes to a lack of runtime support at the SSD. On the other hand, it is non-trivial and impractical for the user to manually analyze the application and rewrite source codes, and there is a problem that the ISP performance is determined according to the user's programming experience.</p><p id="p-0076" num="0075">Second, the ignorance of file layouts is described. Most applications access the SSD over the file system, but the SSD firmware lacks information about the layout of how the file system stores files. Thus, the user should retrieve a location of the data to be processed in the form of a set of LBAs and inform the ISP tasks about the LBA set. For example, as in S<b>620</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, since the graph file is spread on the SSD, the user needs to traverse LBAs of data through the index node (Mode) of the file (<b>621</b>), creates the LBA set (<b>622</b>), and transfer the LBA set to the SSD via RPCs (remote procedure calls). Once the SSD receives the corresponding LBA set, it can know which flash the actual data is stored in through FTL. The extra communication overhead to synchronize the data location between the host and the SSD not only introduces many context switches on the host, but also makes the computing resources idle because the ISP tasks cannot perform operations without knowing where the data is located.</p><p id="p-0077" num="0076">Third, the ignorance of file layouts is described. Due to the above problem, the current ISP models cannot offload everything of the target application to the SSD, and require switching the ISP tasks. This task switching overhead introduces a new type of data transfers for processing data through the ISP, and reduces the degree of execution parallelism between the host and SSD. After ISP<b>1</b> sorts all edges (S<b>631</b>), ISP<b>2</b> can be activated (i.e., a tree may be spanned based on the minimum edge weight) (S<b>632</b>). ISP<b>1</b> can be executed after the LBA set of the file to be read is successfully offloaded from the host (S<b>631</b>). As the host- or storage-side tasks require waiting for other tasks, the utilization rate of each computational resource can be lowered. Furthermore, this execution chain of ISP tasks may be repeated multiple times when the application requires processing multiple graph files.</p><p id="p-0078" num="0077">Fourth, the device dependency is described. The user needs to recompile the partitioned application. Since the ISP tasks should be executed as a part of the SSD firmware, it also requires for the ISP tasks to perform a cross-compile with a toolchain guided by vendors (S<b>640</b>). Due to the cross-compilation, even if the programmers may perform well on the partitioning, it introduces the device dependency that may lead to poor portability and compatibility across different firmware versions and storage configurations.</p><p id="p-0079" num="0078">Fifth, the data vulnerabilities are described. Even if all challenges described above are addressed, the existing ISP models overlook vulnerabilities raised by parallel execution between the host and the SSD (S<b>650</b>). The SSD is a block device, which is shared by all the host uses, and there is no data protection mechanism inside the SSD because the file system manages permissions. Thus, the partitioned application and offloaded ISP tasks can simultaneously access the same data. Unfortunately, this can introduce nondeterministic situations and raise serious vulnerability issues. For example, the flash that stores data that is currently being processed by the ISP task may be overwritten by the other ISP task or host application that does not own the data. Further, because the data for the ISP tack is accessed by the other applications and then is changed, the ISP task may be in an unpredictable situation.</p><p id="p-0080" num="0079">Although the processor of the storage card shows the performance of a middle-end CPU, the execution performance shown by the ISP cannot be equal to or better than that of processing the data at a host-level. Further, the existing IPS models exhibit worse performance than the host because of the aforementioned ISP communication overhead. In some embodiments, the ISP model may be modified to run independently from the host. Accordingly, the corresponding communication overhead can be removed, which in turn can make the ISP more comparable or even better compared with the host-side data processing.</p><p id="p-0081" num="0080">Next, a storage card according to various embodiments is described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref> to <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an example diagram for explaining a storage card according to an embodiment, and <figref idref="DRAWINGS">FIG. <b>8</b></figref> is an example diagram for explaining an example of a filesystem of a computing device according to an embodiment.</p><p id="p-0083" num="0082">In some embodiments, a storage card may free a host from ISP management and execute an entire application, not a function or a task, without changing existing data processing applications and hardware environment. To this end, the storage card may virtualize ISP execution and allow the host to use a containerized application (i.e., container) for ISP. The storage card may pull (download) any application and execute it in a form of the container. During the container execution, the host may not be involved in ISP control. The storage card may provide ISP services while serving block I/O requests from the host. Through such ISP virtualization, the five challenges described above can be addressed.</p><p id="p-0084" num="0083">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a storage card <b>700</b> includes a network driver <b>710</b>, firmware <b>720</b>, and a storage device <b>730</b>. The firmware <b>720</b> may be Docker stack enabled firmware. The network driver <b>710</b> and the firmware <b>720</b> may be executed by a processor (e.g., <b>131</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) of the storage card <b>700</b>.</p><p id="p-0085" num="0084">The network driver <b>710</b> enables Ethernet-based local network (e.g., point-to-point (P2P) network) together with a network driver <b>741</b> of a host <b>740</b>. The network drivers <b>710</b> and <b>741</b> enable Ethernet communication over PCIe via a NVMe protocol. Hereinafter, the local network is referred to as an &#x201c;NVMe Ethernet&#x201d; (or &#x201c;Ethernet over NVMe&#x201d; or &#x201c;Ether-oN&#x201d;), and the network driver is referred to as an &#x201c;NVMe Ethernet driver&#x201d;. The host <b>740</b> (e.g., the NVMe Ethernet driver <b>741</b>) and the storage card <b>700</b> (e.g., the NVMe Ethernet driver <b>710</b>) each have its own Internet protocol (IP) address. By setting an environment variable of the host <b>740</b> (e.g., DOCKER_HOST) with the IP address of the storage card <b>700</b>, a user (e.g., the application <b>743</b> of the user) may issue an IPS-related request to the firmware <b>720</b> of the storage card <b>700</b> through the NVMe Ethernet drivers <b>741</b> and <b>710</b>. In some embodiments, NVMe Ethernet drivers <b>741</b> and <b>710</b> may set the IP addresses for host <b>740</b> and storage card <b>700</b>, respectively. In some embodiments, the host <b>740</b> may send the ISP-related request to the firmware <b>720</b> using docker-cli. In some embodiments, the ISP-related request may include an offloading request of a container image or a container execution request to run the container.</p><p id="p-0086" num="0085">The firmware <b>720</b> may support downloading image <b>751</b> (blob/layers) (S<b>701</b>), creating a container <b>752</b> from the image <b>751</b> and executing the container <b>752</b> (S<b>702</b>), or container communication and monitoring (S<b>703</b>). Since the firmware <b>720</b> manages all ISP offloading and execution, the user (e.g., the application <b>743</b> of the user) may deal with an ISP model of the storage card like conventional Docker framework. Accordingly, the host <b>740</b> may transmit a user's command (request) to the firmware <b>720</b> via the NVMe Ethernet driver <b>741</b>. The NVMe Ethernet driver <b>741</b> may set the IP address of the storage card <b>700</b> in the NVMe Ethernet as a destination address of the command, and may transmit it to the storage card <b>700</b>. The storage card <b>700</b> may transmit the user's command to the firmware <b>720</b> via the NVMe Ethernet driver <b>710</b>. For example, to offload the container, the user may ask a blob download (e.g., docker image load) to the firmware <b>720</b> via the NVMe Ethernet drivers <b>741</b> and <b>710</b>.</p><p id="p-0087" num="0086">The host <b>740</b> may configure a network address translation (NAT) <b>742</b> so that the storage card <b>700</b> can be directly connected to the Internet. That is, the host <b>740</b> may configure the NAT to route network requests from/to the IP address of the storage card <b>700</b>. For example, the host <b>740</b> may translate an address (i.e., an address of the host <b>740</b>) WAN_IP2 of a request received from a Docker-hub <b>750</b> to an IP address LAN_IP2 of the storage card <b>700</b> in the NVMe Ethernet, and translate an address (i.e., an address of the host <b>740</b> in NVMe Ethernet) LAN_IP1 of a request received from the storage card <b>700</b> to an address WAN_IP1 of a Docker-hub <b>750</b>. Accordingly, the firmware <b>720</b> may directly pull (download) a blob from the Docker-hub <b>750</b> in a case of the user requesting (docker image pull). The user may issue a container execution request (docker run) to the firmware <b>720</b> via the NVMe Ethernet drivers <b>741</b> and <b>710</b> to execute the downloaded image.</p><p id="p-0088" num="0087">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a storage card may provide a file system (FS) <b>810</b> used for OS virtualization and container execution. In some embodiments, the storage file system <b>810</b> may manage concurrency and resolve vulnerability issues to realize a fully flexible ISP. The storage file system <b>810</b> may make the container image or data invisible to the host by using a detached NVMe namespace. If the container binds a folder or file (e.g., /usr, /bin, or /data) of a host file system <b>820</b> to a folder or file (e.g., /images, /layers, /container, /rootfs, or /data) (<b>830</b>), the storage card may use an Mode lock <b>840</b> synchronized with an Mode cache of the host through the NVMe Ethernet driver (e.g., <b>710</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>), thereby addressing the concurrency issue due to the concurrent access.</p><p id="p-0089" num="0088">In some embodiments, the storage file system <b>810</b> may add a reference counter to each Mode structure that the host file system <b>820</b> has, and protect it through an existing Mode's semaphore. This counter may be updated when the file (and folder) corresponding to the Mode opens or closes. The file may be accessible only if the counter is zero. For the counter update, the host file system <b>820</b> and the storage file system <b>810</b> may send and receive packets through an NVMe Ethernet drivers (e.g., <b>741</b> and <b>710</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In a case where the container obtains permission of the file access, the host file system <b>820</b> may invalidate its Mode cache so that the container running on the storage device can refer the latest information.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an example diagram showing NVMe Ethernet communication in a computing device according to an embodiment.</p><p id="p-0091" num="0090">In some embodiments, since NVMe Ethernet overrides a standard NVMe protocol, Ethernet-based communication can be enabled without hardware changes. A host may communicate by issuing block I/O requests. Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, a host application may use a conventional network stack to communicate with a storage card <b>990</b>. The network stack may include a socket layer <b>920</b>, a transmission control protocol/internet protocol (TCP/IP) layer <b>930</b>, and an Ethernet driver <b>940</b> in a kernel domain. In some embodiments, the network stack may further include a network interface card (NIC) driver <b>950</b> below the Ethernet driver <b>940</b>. The network stack may further include a NIC <b>960</b> in a hardware domain. The NIC driver <b>950</b> and the NIC <b>960</b> may not be used for communication with the storage card <b>990</b>.</p><p id="p-0092" num="0091">When a user requests a service through docker-cli <b>910</b>, a request may be transferred to the Ethernet driver <b>940</b> through the socket layer <b>920</b> and the TCP/IP layer <b>930</b>. A request may be transferred from the Ethernet driver <b>940</b> to the docker-cli <b>910</b> through the TCP/IP layer <b>930</b> and the socket layer <b>920</b>. In some embodiments, for ISP-related Ethernet services, an NVMe Ethernet stack may be provided. The NVMe Ethernet stack may include an NVMe Ethernet driver <b>970</b> and an NVMe driver <b>980</b> in the kernel domain. The NVMe Ethernet stack may further include a storage card <b>990</b> in the hardware domain That is, for the ISP-related Ethernet services, the NVMe Ethernet driver <b>970</b> may be loaded between the existing Ethernet driver <b>940</b> and the NVMe driver <b>980</b>. The NVMe Ethernet driver <b>970</b> may convert incoming network packets (Ethernet packets) into NVMe requests (or NVMe commands). The NVMe request may be passed to the storage card <b>990</b> via the NVMe driver <b>980</b>. For example, the NVMe request may be passed through an nvme_submit_cmd( ) function.</p><p id="p-0093" num="0092">There may exist two challenges for the conversion of the Ethernet packets into the NVMe requests. First, in contrast to the existing network interfaces, NVMe cannot send a new request to the host. There is no storage card that can issue the NVMe request to the host, but the container requires such an upcall mechanism to communicate with the user. Second, the user can access the ISP or process I/O requests through both the existing block I/O requests and the network. To address these challenges, in some embodiments, vendor-specific commands may be defined in the NVMe protocol. These commands may include a transmit frame command and a receive frame command A structure of the two frame commands may be the same as that of other NVMe commands, but use different operation codes which are reserved for vendor-specific purpose in NVMe. For example, among operation codes defined for the vendor-specific purpose in NVMe, predetermined value (e.g., 0xE1) may be allocated for the transmit frame, and another predetermined value (e.g., 0xE0) may be allocated for the receive frame.</p><p id="p-0094" num="0093">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, when the NVMe Ethernet driver <b>970</b> receives an Ethernet request, it may extract an actual Ethernet packet and allocate a kernel page (e.g., a 4 KB kernel page). In some embodiments, the NVMe Ethernet driver <b>970</b> may extract a packet buffer (e.g., struct sk_buff) <b>971</b> that is buffering Ethernet packets. Further, the NVMe Ethernet driver <b>970</b> may copy data (e.g., packet header, packet data, checksum) included in the sk_buff <b>971</b> to the allocated page. Since the maximum size of Ethernet frame is 1.5 KB, a 4 KB page is sufficient to deliver all data of sk_buff <b>971</b>. After generating an NVMe command <b>972</b>, the NVMe Ethernet driver <b>970</b> may fill a PRP field with an address of the allocated page and fill the allocated operation code (e.g., 0xE1) of the transmit frame, and then submit the NVMe command <b>972</b> to the NVMe driver <b>980</b>. Such the NVMe command <b>972</b> may be the transmit frame command.</p><p id="p-0095" num="0094">To enable the storage card <b>990</b> to issue a request to the host, an asynchronous upcall mechanism utilizing pre-allocated NVMe commands may be used. When the storage card <b>990</b> is initialized in the kernel, the NVMe Ethernet driver <b>970</b> may issue a set of NVMe commands (i.e., receive frame commands) <b>973</b> to an SQ in advance even if there is no Ethernet request. For each NVMe command <b>973</b>, the NVMe Ethernet driver <b>970</b> may allocate a kernel page and fill the receive frame operation code (e.g., 0xE0). Further, the NVMe Ethernet driver <b>970</b> may fill an address of the allocated kernel page in an PRP field. The storage card <b>990</b> may keep the corresponding NVMe command <b>973</b> without any processing. When the container wants to send a new Ethernet packet to the host, the storage card <b>990</b> may write (copy) a sk_buff <b>974</b> of the packet to the pre-allocated kernel page of the NVMe command <b>973</b> through the PRP, and then complete the NVMe command <b>973</b>. The NVMe Ethernet driver <b>970</b> may read data from the kernel page of the completed NVMe command <b>973</b> to generate an Ethernet packet, and then call a netif_rx( ) function to pass the Ethernet packet to the network stack. Then, the NVMe Ethernet driver <b>970</b> may issue another receive frame command to the storage card <b>990</b> for the future communication.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an example diagram for explaining firmware of a storage card according to an embodiment.</p><p id="p-0097" num="0096">Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, firmware <b>1000</b> of the storage card may include a mini-Docker layer (MDL) <b>1010</b> that provides essential Docker functions to support container control and services and an OS feature layer (OSL) <b>1020</b> that provides OS features. The firmware <b>1000</b> may further include an HIL <b>1030</b>, an ICL <b>1040</b>, and an FTL <b>1050</b> similar to the conventional firmware. Since the firmware <b>1000</b> is designed towards simultaneously processing block I/O and network communication, the HIL <b>1030</b> may arbitrate a service path (HIL&#x2192;OSL&#x2192;MDL) and a data path (HIL&#x2192;ICL&#x2192;FTL).</p><p id="p-0098" num="0097">A simple idea to implement ISP in the storage card may be to run an OS in the storage card and install Docker on the storage card. However, running the OS in the storage card may waste hardware resources, and degrade performance even if implementing the ISP. Therefore, by using the firmware (Docker-enabled firmware) into which essential kernel components (MDL <b>1010</b> and OSL <b>1020</b>) for the container services are integrated, the ISP may be implemented in the storage card without wasting the hardware resources.</p><p id="p-0099" num="0098">The MDL <b>1010</b> may perform functions extracted from the original Docker stack: API server (e.g., dockerd), container image management (e.g., containerd), container execution (e.g., runc) and standard I/O routing (e.g., runc). In some embodiments, the API server may be a representational state transfer (REST) API server. Hereinafter the API server is described as a REST API server. To this end, the MDL <b>1010</b> may include a REST API server <b>1011</b>, an image handler <b>1012</b> that manages a container image, an execution handler <b>1013</b> that creates or runs a container, and a log handler <b>1014</b> that performs standard I/O routing. In some embodiments, the MDL <b>1010</b> may support a plurality of multiple functions (e.g., image downloading, container creation/execution function, and container communication/monitoring) without having the full Docker stack to execute a container (containerized ISP) on the OSL <b>1020</b></p><p id="p-0100" num="0099">The OSL <b>1020</b> may perform a network feature, an I/O feature, and a thread management feature. In some embodiments, the OSL <b>1020</b> may remove complicated kernel features such as cgroups, namespaces, and device drivers. To this end, the OSL <b>1020</b> may include a network handler <b>1021</b> that performs the network feature, an I/O handler <b>1022</b> that performs the I/O feature, and a thread handler <b>1023</b> that performs the thread management feature. The OSL <b>1020</b> may further include a system call emulation interface. The network handler <b>1021</b> may be connected to the HIL <b>1030</b> through an NVMe Ethernet driver <b>1032</b> of the HIL <b>1030</b>, and the I/O handler <b>1022</b> may be connected to the ICL <b>1040</b>. As such, the OSL <b>1020</b> may be not a kernel but a set of lightweight firmware components for supporting the service path. In some embodiments, the OSL <b>1020</b> may implement the key kernel features and system call emulation interface for the container execution.</p><p id="p-0101" num="0100">The HIL <b>1030</b> may include a command parser <b>1031</b> and the NVMe Ethernet driver <b>1032</b>. The command parser <b>1031</b> may parse an incoming request and decide which service/data paths are enabled for the incoming request. The command parser <b>1031</b> detects an operation code, and when a value of the operation code is a value indicating a transmit frame command (e.g., 0xE1) or a value indicating a receive frame command (e.g., 0xE0), may pass the corresponding request to the NVMe Ethernet driver <b>1032</b>. That is, the command parser <b>1031</b> may pass the request to the service path. In some embodiments, the request to be passed to the service path may be passed via an NVMe Ethernet driver <b>1082</b> and an NVMe driver <b>1083</b> of the host. When the operation code has a different value, the HIL <b>1030</b> may perform a general block I/O service. That is, the command parser <b>1031</b> may pass the request to the ICL <b>1040</b>, i.e., the data path. In some embodiments, the request to be passed to the data path may be passed through a block I/O queue <b>1081</b> and the NVMe driver <b>1083</b> of the host. The block I/O queue <b>1081</b> may be, for example, multi-queue block IO queuing (blk-mq).</p><p id="p-0102" num="0101">In some embodiments, in the data path, the ICL <b>1040</b> uses an internal memory (e.g., <b>132</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) as a data buffer to copy data from host memory (e.g., <b>120</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to the internal memory <b>132</b>. The FTL <b>1050</b> may translate a logical address (e.g., LBA) of the request to a physical flash address (e.g., PPN) of flash media <b>1060</b>, and transfer the data buffered in the internal memory <b>132</b> to the flash media <b>1060</b>. In a case of a read request in the data path, a reverse process may be performed.</p><p id="p-0103" num="0102">In some embodiments, to enable the service path, the NVMe Ethernet driver <b>1032</b> of the HIL <b>1030</b> may transfer target data (of a packet to receive) from a host memory to the internal memory <b>132</b>, and then inform a location of the transferred data to the network handler <b>1021</b> of the OSL <b>1020</b> such that the OSL <b>1020</b> can process the corresponding service request.</p><p id="p-0104" num="0103">In some embodiments, the NVMe Ethernet driver <b>1032</b> of the HIL <b>1030</b> may be located between an PCIe endpoint in the storage card and the network handler <b>1021</b>. In some embodiments, while the NVMe Ethernet driver <b>1032</b> defines an IP address of the storage card, an actual port number may vary based on a service which a container uses and a port which the MDL <b>1010</b> uses. Accordingly, the network handler <b>1021</b> may check a port number recorded in a network packet (e.g., sk_buff), and then transfer the packet to a target process to receive the packet based on the port number. If the port number is a value (e.g., TCP <b>2375</b>) associated with docker-cli, the network handler <b>1021</b> may forward the network packet to the REST API server <b>1011</b> of the MDL <b>1010</b>. The REST API server <b>1011</b> may parse the network packet, i.e., the request to obtain its information and forward the information to the image handler <b>1012</b> that manages the container image or the execution handler <b>1013</b> that creates or executes the container through the thread handler <b>1023</b> of the OSL <b>1020</b>. If the port number has a different value, the network packet may be forwarded to a container <b>1070</b>. In some embodiments, the container <b>1070</b> may issue both a block I/O request for ISP and a network request for communicating with the host. The internal block I/O request may be served by the I/O handler <b>1022</b> of the OSL <b>1020</b>, which may be passed to the data path of the firmware. In contrast, the internal network request may be forwarded to the network handler <b>1021</b> of the OSL <b>1020</b>, which may be passed to the host through the NVMe Ethernet module <b>1032</b> of the HIL <b>1030</b> via an asynchronous upcall.</p><p id="p-0105" num="0104">Next, an OSL and an MDL of a storage card are described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref> to <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0106" num="0105">As described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, an OSL may include a network handler, an I/O handler, and a thread handler that manage network, I/O, and thread services, respectively. Since a storage card has no OS, all functions may be initialized by firmware. To this end, in some embodiments, a method of inserting a pointer of each handler to an interrupt vector table may be used. Further, in some embodiments, a system call corresponding to a part (e.g., half) of a system call provided by the kernel may be emulated in the OSL.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an example diagram for explaining a thread handler in a storage card according to an embodiment.</p><p id="p-0108" num="0107">A thread handler of an OSL may handle a system call related to process management and scheduling (e.g. fork, exit, etc.), inter-process communication, and signal processing. In some embodiments, most functions of the thread handler are similar to those of the existing OS, but memory and page cache management may differ from those in the existing OS. In particular, the thread handler may be designed to reduce data copy and system call overhead imposed by switching between a user mode and a kernel mode.</p><p id="p-0109" num="0108">Referring to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a thread handler of an OSL may separate a memory space of a storage device into a container pool <b>1110</b> and a firmware pool <b>1120</b>, and may implement an allocator (e.g., slab) that allocates a page through a software memory management unit (MMU) <b>1130</b>. The allocator, i.e., the software MMU <b>1130</b> may provide a physical memory page from either the container pool <b>1110</b> or the firmware pool <b>1120</b>. A container <b>1140</b> may access the container pool <b>1110</b>, and firmware <b>1150</b> may access the firmware pool <b>1120</b>. In some embodiments, for memory protection, it may be set that only the firmware <b>1150</b> operating in a privileged mode is accessible to the page pool <b>1110</b> of all containers and the container <b>1140</b> is inaccessible to the firmware pool <b>1120</b>. Such an embodiment may allow all arguments of the system call to be directly accessed from the firmware, thereby removing the data transfers caused by the mode switching.</p><p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example diagram for explaining an I/O handler in a storage card according to an embodiment.</p><p id="p-0111" num="0110">Referring to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, in some embodiments, an I/O handler may remove several features of a block layer and NVMe fabric stacks, such as a page cache, an I/O scheduler, and an NVMe driver, and directly connect a block I/O entry <b>1210</b> to a data path of firmware (HIL&#x2192;ICL&#x2192;FTL). Further, the I/O handler may include an Mode cache <b>1221</b>, and the Mode cache <b>1221</b> may convert a file descriptor into a corresponding LBA set by being aware of an Mode structure. The I/O handler may implement a file-related system call emulation interface <b>1240</b> and a path walker <b>1230</b> on the Mode cache <b>1221</b> and a file system <b>1222</b>. To manage a concurrent access, the I/O handler may use the file system <b>1222</b> that manages an Mode's bitmap, and the bitmap may be managed by pre-allocation (prealloc) or multiple block allocation (mballoc). The path walker <b>1230</b> may mount a file so that an ISP task can perform file I/O without handshaking an LBA set.</p><p id="p-0112" num="0111">In some embodiments, the system call emulation interface <b>1240</b> may correspond to a virtual file system (VFS), the path walker <b>1230</b>, the Mode cache <b>1221</b>, and the file system <b>1222</b> may correspond to an overlay file system, and the block I/O entry <b>1210</b> may correspond to a block layer.</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an example diagram for explaining a network handler in a storage card according to an embodiment.</p><p id="p-0114" num="0113">Referring to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, a network handler may implement most functions of an TCP/IP layer, such as a TCP finite state machine (FSM) or a packet encapsulator. When a network packet comes after a TCP connection is established, the TCP FSM of the network handler may record a socket's connection status, such as an IP address or port number, so that the packet encapsulator can correctly generate a TCP/IP packet. For example, the TCP FCM may track the socket's connection state such as retransmission and congestion in a TCP layer (<b>1320</b>). Because a relationship between an IP address and a MAC address changes over time, the network handler may use an address resolution protocol (ARP) table to store the IP and MAC addresses as a key-value pair (<b>1330</b>). Accordingly, the network handler can perform addressing and routing. In some embodiments, the system call emulation interface <b>1310</b> may be implemented in a socket layer. In some embodiments, IP security (IPSec) or features of Netfilter may be excluded from the network handler.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an example diagram for explaining an MDL in a storage card according to an embodiment.</p><p id="p-0116" num="0115">In some embodiments, a main role of container management and execution of a storage card may be to process data in the storage device rather than implementing a Docker stack on the storage device. Accordingly, the MDL may implement a predetermined number (e.g., ten) of key Docker commands among all (e.g., <b>106</b>) Docker commands. These key Docker commands may be used to handle image download, container execution, and real-time logging.</p><p id="p-0117" num="0116">Referring to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the MDL <b>1400</b> may include a plurality of firmware handlers, i.e., an image handler <b>1410</b>, an execution handler <b>1420</b>, and a log handler <b>1430</b> according to each container management category. Atop the image handler <b>1410</b>, the execution handler <b>1420</b>, and the log handler <b>1430</b> implemented on an OSL's system call emulation interface, a REST API server <b>1440</b> may be implemented to communicate with docker-cli. The REST API server <b>1440</b> may process HTTP requests such as GET (retrieves), POST (creates), and PUT (updates). The host may use docker-cli to send an ISP management command to the storage card via an HTTP request, and the ISP management command may include information such as an image location and an IP address. The REST API server <b>1440</b> of the MDL <b>1400</b> may parse all information of the HTTP request and respond to the request over HTTP, and may invoke an MDL handler suitable for the request.</p><p id="p-0118" num="0117">In some embodiments, the image handler <b>1410</b> may be implemented in the MDL in a form of layered firmware by modifying an image management function of dockerd. When an image offloading request (e.g., POST image) <b>1461</b> is received, the image handler <b>1410</b> may download a container image and distribute it to an area that is invisible to users but accessible for the firmware. Since the main role of the image handler <b>1410</b> is to store an image (/images/manifest) in the flash media, the image handler <b>1410</b> may be tightly collaborate with an I/O handler <b>1451</b> of an OSL <b>1450</b>. The image handler <b>1410</b> may store blobs transferred from local or remote over an NVMe Ethernet driver to/images/blobs, and unpack each blob to /images/layers by working with a manifest file. In some embodiments, the manifest file may store a table that manages each image layer in the form of an absolute path. Each layer may include fundamental files or folders for containerization, such as/bin, /lib, and/usr. Accordingly, the image handler <b>1410</b> of the MDL <b>1400</b> may merge the image layers into rootfs, which may be a file tree used by the container.</p><p id="p-0119" num="0118">In some embodiments, the execution handler <b>1420</b> of the MDL <b>1400</b> may execute the container as manifest with assistance of a thread handler <b>1452</b> of the OSL <b>1450</b>. The execution handler <b>1420</b> may implement a function of runc in firmware. When a Docker's container execution command (e.g., POST/start) <b>1462</b> is received, the execution handler <b>1420</b> may check the manifest file and extract an entryscript that explains how to execute the container. Further, the execution handler <b>1420</b> may mount the corresponding rootfs to the container and then run the entryscript by invoking the thread handler <b>1452</b> of the OSL <b>1450</b>. On the other hand, because the rootfs is stored in the file system, it cannot be accessed from the host but can be accessed from the firmware. However, the rootfs may be bounded with a part of the host file system to put the data and take out the data. Thus, the execution handler <b>1420</b> of the MDL <b>1400</b> may lock Mode of a file or folder bounded to the host, thereby notifying the host's file system through the NVMe Ethernet driver that the file or folder is in use.</p><p id="p-0120" num="0119">In some embodiments, the log handler <b>1430</b> may manage container monitoring. In a Docker stack, all stdout and stderr data of each container may be stored in /containers/&#x3c;id&#x3e;/rootfs/log. Thus, when the host sends a logging command (e.g., GET/logs) <b>1463</b>, the log handler <b>1430</b> may transfer monitoring information to the host by collaborating with a network handler <b>1453</b> of the OSL and the NVMe Ethernet driver. This function can allow the host monitor a state of the container running on the storage card in real time.</p><p id="p-0121" num="0120">As described above, in some embodiments, the storage card may execute a wide spectrum of data processing applications by offering a fully flexible and practical ISP model. In some embodiments, the storage card may apply lightweight OS-level virtualization to the storage device, which may be well harmonized with hardware architectures of the existing storage card and host-side software ecosystems. Some embodiments may provide NVMe Ethernet (&#x201c;Either-oN&#x201d;) that enables a network-based ISP management mechanism and point-to-point communication between a host and an ISP task by overriding the standard NVMe protocol. In some embodiments, the Ether-oN may allow the storage card to serve ISP-related requests without changes existing physical interfaces for both network and the storage card. In some embodiments, Ether-oN may allow users to supply data, to query an ISP status, and to retrieve results by directly interacting with the storage card, which can realize on-demand and real-time data analysis of ISP.</p><p id="p-0122" num="0121">Some embodiments may integrate essential features for OS-level virtualization into a firmware stack of the storage card and emulate system calls, such that existing applications can be offloaded to the storage device in a form of Docker containers. In some embodiments, the storage card may download container images stored in a local or remote Docker Hub repository. Some embodiments may containerize ISP tasks and execute the IPS tasks in the storage card without a modification of existing software environment. This can provide highly flexible programmability and generality of ISP task execution. For example, users may require neither changing a data processing algorithm to interact with vendor-specific ISP runtimes to utilize the ISP nor a specialized toolchain to cross-compile ISP tasks to fit them into the firmware of the storage card. This can allow the ISP to be applied in various computing environments without changing the corresponding system environment. In addition, the firmware stack of the storage card may include filesystem features underneath virtualization runtimes such that the containers can process data without a host-side OS intervention. Since image files and data of each container are owned by different users, the storage card may secure a reserved storage in the storage card to manage them in a separate way. A private storage, managed a filesystem of the storage card, may be logically separated from the host by using an NVME detached namespace, and may only be accessed by each container. In some embodiments, the file system of the storage card may protect the container's image and its own data against anonymous processes as well as unauthorized accesses.</p><p id="p-0123" num="0122">In some embodiments, the filesystem of the storage card may synchronize the host file system and the Mode cache to prevent data loss due to concurrent accesses. In some embodiments, the storage card may reduce host resource involvement for the ISP and may exhibit better performance compared to the existing ISP model.</p><p id="p-0124" num="0123">The functions of the NVMe Ethernet driver or firmware described above may be implemented as a computer program readable by a computer (e.g., a processor of a host and a processor of a storage card) on a computer-readable medium. In some embodiments, the computer-readable medium may include a removable recording medium or a fixed recording medium. In some embodiments, the computer-readable program recorded on the computer-readable medium may be transmitted to another computing device via a network such as the Internet and installed in another computing device, so that the computer program can be executed by another computing device.</p><p id="p-0125" num="0124">While this invention has been described in connection with what is presently considered to be practical embodiments, it is to be understood that the invention is not limited to the disclosed embodiments, but, on the contrary, is intended to cover various modifications and equivalent arrangements included within the spirit and scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computing device comprising:<claim-text>a host to which a first internet protocol (IP) address is set; and</claim-text><claim-text>a storage card including a computing complex configured to execute firmware and a storage device, and to which a second IP address is set,</claim-text><claim-text>wherein the host is configured to convert a first Ethernet packet, which includes an in-storage processing (ISP)-related request and is destined for the second IP address, into a first non-volatile memory express (NVMe) request according to an NVMe protocol, and transfer the first NVMe request to the storage card, and</claim-text><claim-text>wherein the firmware is configured to parse the first NVMe request to perform the ISP-related request.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computing device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host is configured to execute a network driver, and<claim-text>wherein the network driver is configured to:<claim-text>copy data of the first Ethernet packet to a kernel page;</claim-text><claim-text>fill a physical region page (PRP) field of the first NVMe request with an address of the kernel page;</claim-text><claim-text>set an operation code of the first NVMe request to a value indicating a transmit frame command; and</claim-text><claim-text>transfer the first NVMe request to the storage card through an NVMe driver.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computing device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the host is configured to execute a network driver, and<claim-text>wherein the network driver is configured to, in order to receive a second Ethernet packet destined for the first IP address from the storage card:<claim-text>submit a second NVMe request to an NVMe queue;</claim-text><claim-text>allocate a kernel page to allow the storage card to write data of the second Ethernet packet to the kernel page;</claim-text><claim-text>set an operation code of the second NVMe request to a value indicating a receive frame command; and</claim-text><claim-text>generate the second Ethernet packet by reading the data from the kernel page of the second NVMe request completed by the storage card.</claim-text></claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computing device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the ISP-related request includes an offloading request of a container image or a container execution request for executing a container.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computing device of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the firmware includes:<claim-text>a mini-Docker layer configured to performs a docker function for container control and service for the ISP; and</claim-text><claim-text>an operating system feature layer configured to perform an operating system feature.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computing device of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the firmware further includes a host interface layer, and<claim-text>wherein the host interface layer includes a command parser configured to parse the first NVMe request and select a service path or a data path.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computing device of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the host interface layer further includes a network driver,<claim-text>wherein the command parser is configured to select the service path in response to an operation code of the first NVMe request having a predetermined value, and select the data path in response to the operation code of the first NVMe request not having the predetermined value, and</claim-text><claim-text>wherein in response to selection of the service path, the network driver is configured to copy the first Ethernet packet stored in a memory of the host indicated by a PRP of the first NVMe request to an internal memory of the computing complex, and notify a location of the internal memory to which the data is copied to the OS feature layer.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computing device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the firmware further includes an internal cache layer and a flash translation layer, and<claim-text>wherein in response to selection of the data path, data of the first NVMe request may be stored in the storage device through the internal cache layer and the flash translation layer.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computing device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the OS feature layer includes a network handler configured to perform a network function and an input/output (I/O) handler configured to perform an I/O function, and<claim-text>wherein the mini-Docker layer includes an API server, an image handler configured to manage a container image, and an execution handler configured to execute a container.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computing device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the network handler is configured to forward the first Ethernet packet to the API server, in response to a port number of the first Ethernet packet having a value associated with a Docker command line interface, and<claim-text>wherein the API server is configured to transfer information obtained by parsing the first Ethernet packet to the image handler or the execution handler.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing device of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the network handler is configured to forward the first Ethernet packet to the container, in response to the port number of the first Ethernet packet not having the value associated with the Docker command line interface.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computing device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the container is configured to generate an I/O request for the ISP or a network request to communicate with the host</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computing device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the I/O handler is configured to forward the I/O request to the data path.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computing device of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the network handler is configured to forward the network request to the host through an asynchronous upcall via the network driver.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computing device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the OS feature layer further includes a thread handler, and<claim-text>wherein the thread handler is configured to separate a memory space of the storage device into a container pool accessible by the container and a firmware pool accessible by the firmware, and provide a physical memory page from either the container pool or the firmware pool.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computing device of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the mini-Docker layer may further include a log handler, and<claim-text>wherein the log handler is configured to manage container monitoring, and transmit container monitoring information to the host through the network handler and the network module in response to receiving a logging command from the host.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A storage card connected to a host, the storage card comprising:<claim-text>a processor configured to execute firmware; and</claim-text><claim-text>a storage device,</claim-text><claim-text>wherein the firmware includes:<claim-text>a host interface layer is configured to select a service path or a data path by parsing a non-volatile memory express (NVMe) request according to an NVMe protocol received from the host;</claim-text><claim-text>a mini-Docker layer including an image handler configured to perform container image management based on a network packet included in the NVMe request, and an execution handler configured to execute a container based on the network packet; and</claim-text><claim-text>an operating system (OS) feature layer may include a network handler configured to, in response to selection of the service path, forward the network packet to the mini-Docker layer or the container, and forward a network request, generated in the container, for communicating with the host to the host interface layer.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The storage card of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the mini-Docker layer further includes an API server,<claim-text>wherein the network handler is configured to forward the network packet to the API server in response to a port number of the network packet having a predetermined value, and forward the network packet to the container in response to the port number of the network packet not having the predetermined value have, and</claim-text><claim-text>wherein the API server is configured to transfer information of the network packet to the image handler or the execution handler.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The storage card of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the firmware further includes an internal cache layer and a flash translation layer, and<claim-text>wherein the OS feature layer may further include an input/output (I/O) handler configured forward the network packet to the internal cache layer in response to selection of the data path.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A computer program stored in a non-transitory recording medium and configured to be executed by a computing device including a host and a storage card, the computer program causing the computing device to execute:<claim-text>setting a first internet protocol (IP) address and a second IP address to the host and the storage card, respectively;</claim-text><claim-text>converting a first Ethernet packet, which includes an in-storage processing (ISP)-related request and is destined for the second IP address, into a first non-volatile memory express (NVMe) request according to an NVMe protocol; and</claim-text><claim-text>forwarding the first NVMe request to an NVMe drive to transfer the first NVMe request to the storage card.</claim-text></claim-text></claim></claims></us-patent-application>