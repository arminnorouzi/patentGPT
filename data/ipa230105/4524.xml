<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004525A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004525</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17367233</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>11</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>14</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>122</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>148</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0604</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0659</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0664</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>067</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">UPGRADING THE FILE SYSTEM OF OBJECTS IN A DISTRIBUTED STORAGE SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>VMware, Inc.</orgname><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>DESAI</last-name><first-name>Asit</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>JAIN</last-name><first-name>Abhay Kumar</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Wenguang</first-name><address><city>Santa Clara</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>KNAUFT</last-name><first-name>Eric</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>XIANG</last-name><first-name>Enning</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An example method of upgrading a distributed storage object from a first version to a second version includes: querying metadata of a first component configured according to the first version of the distributed storage object, the metadata defining extents of data on a disk group of the first component; populating, for a second component configured according to the second version of the distributed storage object, logical and middle maps based on the metadata such that initial entries in the logical map point to initial entries in the middle map, and the initial entries in the middle map point to physical addresses of the disk group of the first component; and reading the data from the disk group of the first component and writing the data to a disk group of the second component while updating the initial entries in the middle map.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="112.86mm" wi="158.75mm" file="US20230004525A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="241.05mm" wi="172.04mm" orientation="landscape" file="US20230004525A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="210.99mm" wi="129.71mm" file="US20230004525A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="127.34mm" wi="149.86mm" file="US20230004525A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="190.08mm" wi="154.09mm" file="US20230004525A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="233.17mm" wi="134.87mm" file="US20230004525A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="242.06mm" wi="135.97mm" file="US20230004525A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">Applications today are deployed onto a combination of virtual machines (VMs), containers, application services, and more within a software-defined datacenter (SDDC). The SDDC includes a server virtualization layer having clusters of physical servers that are virtualized and managed by virtualization management servers. Each host includes a virtualization layer (e.g., a hypervisor) that provides a software abstraction of a physical server (e.g., central processing unit (CPU), random access memory (RAM), storage, network interface card (NIC), etc.) to the VMs. A virtual infrastructure administrator (&#x201c;VI admin&#x201d;) interacts with a virtualization management server to create server clusters (&#x201c;host clusters&#x201d;), add/remove servers (&#x201c;hosts&#x201d;) from host clusters, deploy/move/remove VMs on the hosts, deploy/configure networking and storage virtualized infrastructure, and the like. The virtualization management server sits on top of the server virtualization layer of the SDDC and treats host clusters as pools of compute capacity for use by applications.</p><p id="p-0003" num="0002">A virtualized computing system can provide shared storage for applications to store their persistent data. One type of shared storage is a virtual storage area network (vSAN), which is an aggregation of local storage devices in hosts into shared storage for use by all hosts. A vSAN can be a policy-based datastore, meaning each object created therein can specify a level of replication and protection. The vSAN can then perform automatic placement decisions for replicas across the local storage devices of the hosts.</p><p id="p-0004" num="0003">The disk groups of a vSAN, include disks with an on-disk format having a certain version and features supported by the vSAN software. When the vSAN software is upgraded, the disk groups can be upgraded to a new on-disk format. For example, an older on-disk format can have an extent-based file system, whereas a newer on-disk format can have a log-structured file system. A user may desire to object an object stored on a vSAN to use the newer on-disk format for its disk groups. One upgrade solution is to perform an offline upgrade by reading content from the previous files system and inserting the content into a new file system. Such a solution requires downtime and can be impractical. Another solution is to store only new data in the newer file system and serve old data from the previous file system. However, this solution will have a continuous cost of forwarding and maintaining two file systems for the object.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0001" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a virtualized computing system in which embodiments described herein may be implemented.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram depicting a software platform according to an embodiment.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram depicting a distributed storage object according to an embodiment.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram depicting a metadata structure of a log-structured file system according to an embodiment.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are block diagrams depicting states of a metadata structure of a log-structured file system during upgrade of a distributed storage object according to an embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram depicting a method of upgrading a distributed storage object according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0002" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a virtualized computing system <b>100</b> in which embodiments described herein may be implemented. System <b>100</b> includes a cluster of hosts <b>120</b> (&#x201c;host cluster <b>118</b>&#x201d;) that may be constructed on server-grade hardware platforms such as an x86 architecture platforms. For purposes of clarity, only one host cluster <b>118</b> is shown. However, virtualized computing system <b>100</b> can include many of such host clusters <b>118</b>. As shown, a hardware platform <b>122</b> of each host <b>120</b> includes conventional components of a computing device, such as one or more central processing units (CPUs) <b>160</b>, system memory (e.g., random access memory (RAM) <b>162</b>), one or more network interface controllers (NICs) <b>164</b>, one or more host bust adaptors (HBAs) <b>165</b>, and optionally local storage <b>163</b>. CPUs <b>160</b> are configured to execute instructions, for example, executable instructions that perform one or more operations described herein, which may be stored in RAM <b>162</b>. NICs <b>164</b> enable host <b>120</b> to communicate with other devices through a physical network <b>180</b>. Physical network <b>180</b> enables communication between hosts <b>120</b> and between other components and hosts <b>120</b> (other components discussed further herein).</p><p id="p-0012" num="0011">In the embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, hosts <b>120</b> can access shared storage <b>170</b> by using NICs <b>164</b> to connect to network <b>180</b>. In addition or alternatively, hosts <b>120</b> can access shared storage <b>170</b> using HBAs <b>165</b> connected to a separate network <b>181</b> (e.g., a fibre channel (FC) network). Shared storage <b>170</b> include one or more storage arrays, such as a storage area network (SAN), network attached storage (NAS), or the like. Shared storage <b>170</b> may comprise magnetic disks, solid-state disks (SSDs), flash memory, and the like as well as combinations thereof.</p><p id="p-0013" num="0012">In some embodiments, hosts <b>120</b> include local storage <b>163</b> (e.g., hard disk drives, solid-state drives, etc.). Local storage <b>163</b> in each host <b>120</b> can be aggregated and provisioned as part of a virtual SAN (vSAN), which is another form of shared storage <b>170</b>. Virtualization management server <b>116</b> can select which local storage devices in hosts <b>120</b> are part of a vSAN for host cluster <b>118</b>. A vSAN in shared storage <b>170</b> includes disk groups <b>171</b>. Each disk group <b>171</b> includes a plurality of local storage devices <b>163</b> of a host <b>120</b>. Each disk group <b>171</b> can include cache tier storage (e.g., SSD storage) and capacity tier storage (e.g., SSD, magnetic disk, and the like storage). Each disk in each disk group <b>171</b> has an on-disk format supported by distributed storage software <b>153</b> in hypervisor <b>150</b>, discussed below. The on-disk format supports one or more file systems, such as an extent-based file system and/or log-structured file system.</p><p id="p-0014" num="0013">A software platform <b>124</b> of each host <b>120</b> provides a virtualization layer, referred to herein as a hypervisor <b>150</b>, which directly executes on hardware platform <b>122</b>. In an embodiment, there is no intervening software, such as a host operating system (OS), between hypervisor <b>150</b> and hardware platform <b>122</b>. Thus, hypervisor <b>150</b> is a Type-1 hypervisor (also known as a &#x201c;bare-metal&#x201d; hypervisor). As a result, the virtualization layer in host cluster <b>118</b> (collectively hypervisors <b>150</b>) is a bare-metal virtualization layer executing directly on host hardware platforms. Hypervisor <b>150</b> abstracts processor, memory, storage, and network resources of hardware platform <b>122</b> to provide a virtual machine execution space within which multiple virtual machines (VM) <b>140</b> may be concurrently instantiated and executed. One example of hypervisor <b>150</b> that may be configured and used in embodiments described herein is a VMware ESXi&#x2122; hypervisor provided as part of the VMware vSphere&#xae; solution made commercially available by VMware, Inc. of Palo Alto, Calif. An embodiment of software platform <b>124</b> is discussed further below with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0015" num="0014">Hypervisor <b>150</b> further includes distributed storage software <b>153</b> for implementing a vSAN on host cluster <b>118</b>. Distributed storage systems include a plurality of distributed storage nodes. In the embodiment, each storage node is a host <b>120</b> of host cluster <b>118</b>. In the vSAN, virtual storage used by VMs <b>140</b> (e.g., virtual disks) is mapped onto distributed objects (&#x201c;objects&#x201d;). Each object is a distributed construct comprising one or more components. Each component maps to a disk group <b>171</b>. For example, an object for a virtual disk can include a plurality of components configured in a redundant array of independent disks (RAID) storage scheme. Input/output (I/O) requests by VMs <b>140</b> traverse through network <b>180</b> to reach the destination disk groups <b>171</b>. In some cases, such traversal involves multiple hops in host cluster <b>118</b> and network resources (e.g., transmission control protocol/internet protocol (TCP/IP) sockets, remote direct memory access (RDMA) message pairs, and the like) are heavily consumed.</p><p id="p-0016" num="0015">For example, in vSAN, a virtual disk maps to an object with multiple components for availability and performance purposes. An I/O request issued by a VM <b>140</b> (client) arrives at an owner (the I/O coordinator of this object). The owner is responsible for sending additional I/Os to the RAID tree that the object's policy maintains. This RAID tree might divide the owner level I/Os into multiple smaller sub I/Os (and even multiple batches of these with barriers in-between). The owner's sub I/Os reach the destination host, where the actual data component resides (a particular disk group <b>171</b>). This is the smallest granularity of an I/O destination. Since this is a distributed system, CLIENT, OWNER, and COMPONENT are role names and could or could not be on the same host.</p><p id="p-0017" num="0016">Distributed storage software <b>153</b> can support multiple versions of objects. One object version can have one type of file system (e.g., extent-based), and another object version can have another type of file system (log-structured). In embodiments described herein, a version vSAN1 supports an extent-based file system, and a version vSAN2 supports a log-structured file system. A log-structured file system allows for larger writes, efficient erasure coding, and is snapshot friendly. These characteristics allow for better performance and storage utilization. A log-structured file system stores all incoming writes at a new location, hence it provides good write performance for sequential and random writes. This property also facilitates replacing RAID1 disk groups with more storage efficient RAID5/6 without comprising on performance or fault tolerance level. In embodiments vSAN2 also allows for copy-on-write (COW) snapshots, which are facilitated by the log-structured file system. The read performance of COW snapshots is significantly better than a delta-based snapshot scheme, as the read operation does not traverse through a chain of snapshots to fetch the data. A user may desire to upgrade objects from vSAN1 to vSAN2 but cannot afford downtime. Techniques described herein allow for upgrading objects to use a new file system live without disrupting incoming user IO and without causing significant performance degradation.</p><p id="p-0018" num="0017">In embodiments, host cluster <b>118</b> is configured with a software-defined (SD) network layer <b>175</b>. SD network layer <b>175</b> includes logical network services executing on virtualized infrastructure in host cluster <b>118</b>. The virtualized infrastructure that supports the logical network services includes hypervisor-based components, such as resource pools, distributed switches, distributed switch port groups and uplinks, etc., as well as VM-based components, such as router control VMs, load balancer VMs, edge service VMs, etc. Logical network services include logical switches, logical routers, logical firewalls, logical virtual private networks (VPNs), logical load balancers, and the like, implemented on top of the virtualized infrastructure.</p><p id="p-0019" num="0018">Virtualization management server <b>116</b> is a physical or virtual server that manages host cluster <b>118</b> and the virtualization layer therein. Virtualization management server <b>116</b> installs agent(s) <b>152</b> in hypervisor <b>150</b> to add a host <b>120</b> as a managed entity. Virtualization management server <b>116</b> logically groups hosts <b>120</b> into host cluster <b>118</b> to provide cluster-level functions to hosts <b>120</b>, such as VM migration between hosts <b>120</b> (e.g., for load balancing), distributed power management, dynamic VM placement according to affinity and anti-affinity rules, and high-availability. The number of hosts <b>120</b> in host cluster <b>118</b> may be one or many. Virtualization management server <b>116</b> can manage more than one host cluster <b>118</b>.</p><p id="p-0020" num="0019">In an embodiment, virtualized computing system <b>100</b> further includes a network manager <b>112</b>. Network manager <b>112</b> is a physical or virtual server that orchestrates SD network layer <b>175</b>. In an embodiment, network manager <b>112</b> comprises one or more virtual servers deployed as VMs. Network manager <b>112</b> installs additional agents <b>152</b> in hypervisor <b>150</b> to add a host <b>120</b> as a managed entity, referred to as a transport node. In this manner, host cluster <b>118</b> can be a cluster <b>103</b> of transport nodes. One example of an SD networking platform that can be configured and used in embodiments described herein as network manager <b>112</b> and SD network layer <b>175</b> is a VMware NSX&#xae; platform made commercially available by VM ware, Inc. of Palo Alto, Calif. If network manager <b>112</b> is absent, virtualization management server <b>116</b> can orchestrate SD network layer <b>175</b>.</p><p id="p-0021" num="0020">Virtualization management server <b>116</b> and network manager <b>112</b> comprise a virtual infrastructure (VI) control plane <b>113</b> of virtualized computing system <b>100</b>. In embodiments, network manager <b>112</b> is omitted and virtualization management server <b>116</b> handles virtual networking. Virtualization management server <b>116</b> can include VI services <b>108</b>. VI services <b>108</b> include various virtualization management services, such as a distributed resource scheduler (DRS), high-availability (HA) service, single sign-on (SSO) service, virtualization management daemon, vSAN service, and the like.</p><p id="p-0022" num="0021">A VI admin can interact with virtualization management server <b>116</b> through a VM management client <b>106</b>. Through VM management client <b>106</b>, a VI admin commands virtualization management server <b>116</b> to form host cluster <b>118</b>, configure resource pools, resource allocation policies, and other cluster-level functions, configure storage and networking, and the like.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram depicting software platform <b>124</b> according an embodiment. As described above, software platform <b>124</b> of host <b>120</b> includes hypervisor <b>150</b> that supports execution of VMs <b>140</b>. In an embodiment, hypervisor <b>150</b> includes a VM management daemon <b>213</b>, a host daemon <b>214</b>, and distributed storage software <b>153</b>. VM management daemon <b>213</b> is an agent <b>152</b> installed by virtualization management server <b>116</b>. VM management daemon <b>213</b> provides an interface to host daemon <b>214</b> for virtualization management server <b>116</b>. Host daemon <b>214</b> is configured to create, configure, and remove VMs (e.g., VMs <b>140</b>). Network agents <b>222</b> comprises agents <b>152</b> installed by network manager <b>112</b>. Network agents <b>222</b> are configured to cooperate with network manager <b>112</b> to implement logical network services. Network agents <b>222</b> configure the respective host as a transport node in a cluster <b>103</b> of transport nodes. Each VM <b>140</b> has applications <b>202</b> running therein on top of an OS <b>204</b>. Each VM <b>140</b> has one or more virtual disks <b>205</b> attached thereto for data storage and retrieval.</p><p id="p-0024" num="0023">Distributed storage software <b>153</b> includes an upgrade manager <b>250</b>, a distributed object manager (DOM) <b>232</b>, a DOM <b>234</b>, and various other components <b>252</b>. Each DOM <b>232</b>, <b>234</b> is configured to receive IO requests from VMs <b>140</b>, communicate with other DOMs in other hosts, and provide instructions to lower-level components for reading and writing to disk groups. In the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, DOM <b>232</b> manages objects in the vSAN1 format (&#x201c;vSAN1 objects <b>242</b>&#x201d;). A vSAN1 object <b>242</b>, as described herein, has component(s) <b>244</b>, each of which is mapped to a disk group having an extent-based file system. DOM <b>234</b> manages objects in the vSAN2 format (&#x201c;vSAN2 objects <b>246</b>&#x201d;). A vSAN2 object <b>246</b>, as described herein, has components <b>248</b>, each of which is mapped to a disk group having a log-structured file system. Upgrade manager <b>250</b> is configured to upgrade vSAN1 objects <b>242</b> to vSAN2 objects <b>246</b>. During the upgrade process, a vSAN2 object <b>246</b> can include a concatenation of new components (components <b>248</b>) and old component(s) (components <b>244</b>). Once the upgrade is complete, a vSAN2 object <b>246</b> includes only new components (components <b>248</b>).</p><p id="p-0025" num="0024">Other components <b>252</b> can include a cluster membership, monitoring and directory services (CMMDS), a cluster-level object manager (CLOM), a local log-structured object manager (LSOM). The CMMDS provides topology and object configuration information to the CLOM and DOMs <b>232</b>/<b>234</b>. The CMMDS selects owners of objects, inventories items (hosts, networks, devices), stores object metadata information, among other management functions. The CLOM provides functionality for creating and migrating objects that back virtual disks <b>205</b>. The LSOM provides functionality for interacting with disks of disk groups <b>171</b>.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram depicting a distributed storage object according to an embodiment. A vSAN2 object <b>246</b>-<b>1</b> is shown during upgrade of a vSAN1 object <b>242</b>-<b>1</b>. During the upgrade, vSAN2 object <b>246</b>-<b>1</b> is a concatenation of a component <b>244</b>-<b>1</b> of vSAN1 object <b>242</b>-<b>1</b> and new components <b>248</b>-<b>1</b> and <b>248</b>-<b>2</b> of vSAN2 object <b>246</b>-<b>1</b>. Component <b>244</b>-<b>1</b> maps to a disk group <b>302</b> use an extent-based file system for the vSAN1 object. Component <b>248</b>-<b>1</b> is mapped to a disk group <b>304</b>. Component <b>248</b>-<b>2</b> is mapped to a disk group <b>306</b>. Disk groups <b>304</b> and <b>306</b> use a log-structured file system for the vSAN2 object. For example, disk group <b>302</b> may be any RAID group, such as RAID5. Component <b>248</b>-<b>1</b> may be a performance component and disk group <b>304</b> may be a RAID1 group of SSD devices. Component <b>248</b>-<b>2</b> may be a capacity component and disk group <b>306</b> may be a RAID6 group of hard disks. Those skilled in the art will appreciate that various other configurations are possible using different RAID types.</p><p id="p-0027" num="0026">During the upgrade process, upgrade manager <b>250</b> will read data from component <b>244</b>-<b>1</b> and write the data to component <b>248</b>-<b>2</b> (the capacity component). Prior to the data copy, however, upgrade manager <b>250</b> pre-populates metadata from the vSAN1 object to the vSAN2 object. That is, upgrade manager <b>250</b> pre-populates a metadata structure <b>308</b> stored on disk group <b>306</b> with metadata <b>310</b> from disk group <b>302</b>.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram depicting a metadata structure <b>400</b> of a log-structured file system according to an embodiment. Metadata structure <b>400</b> is an example of a metadata structure stored on a disk group of a vSAN2 object, which uses a log-structured file system. One type of metadata structure includes a logical map that has entries that point to physical addresses. A log-structured file system uses write buffering. Updates to data blocks are tracked in memory and can be cached in a performance/metadata component of the storage. After a sufficient number of updates, the log-structured file system writes the updates to a capacity component of the disk group all at once as a group (&#x201c;segment&#x201d;). The log-structured file system stores each segment at a new location. An overwrite will render a previous write as invalid or stale. A garbage collection process identifies segments having the most invalid data and moves the valid data from such segments to a new location. This movement requires that the physical address for such data be updated in the logical map. However, this is in conflict with the COW snapshot mechanism, which mandates that data stored in shared nodes are not modified. Thus, in embodiments, vSAN2 as described herein includes a layer of indirection in the metadata structure, i.e., all entries in the logical map point to entries in a middle map, and middle map entries point to actual physical addresses. Segments can be cleaned by modifying physical addresses stored in the middle map obviating the need to modify logical map entries, avoiding violation of COW constraints.</p><p id="p-0029" num="0028">Metadata structure <b>400</b> includes a logical map <b>402</b> having entries <b>404</b>. Distributed storage software <b>153</b> can generate various COW snapshots, which are instances of logical map <b>402</b> (e.g., snapshots A, B, and C shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). Each entry <b>404</b> in logical map <b>402</b> maps a logical block address (LBA) to a middle block address (MBA) of a middle map <b>408</b>. Middle map <b>408</b> includes entries <b>410</b>. Each entry <b>410</b> maps an MBA to a physical block address (PBA) <b>414</b>. Note that some entries <b>404</b> in logical map <b>402</b> can map a single LBA to a single MBA, while other entries <b>404</b> in logical map <b>402</b> can map an extent of LBAs to an extent of MBAs. Likewise, some entries <b>410</b> in middle map <b>408</b> can map a single MBA to a single PBA, while other entries <b>410</b> in middle map <b>408</b> can map an extend of MBAs to an extent of PBAs.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are block diagrams depicting states of a metadata structure of a log-structured file system during upgrade of a distributes storage object according to an embodiment. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram depicting a method <b>600</b> of upgrading a distributed storage object according to an embodiment. Method <b>600</b> can be understood with further reference to <figref idref="DRAWINGS">FIGS. <b>3</b>, <b>5</b>A, and <b>5</b>B</figref> as described below.</p><p id="p-0031" num="0030">Method <b>600</b> begins at step <b>602</b>, where upgrade manager <b>250</b> begins an upgrade of a vSAN1 object to a vSAN2 object. The vSAN1 object is an object having a first version where the data thereof is stored using an extent-based file system on the disk group(s). The vSAN2 object is an object having a second version where the data thereof is stored using a log-structured file system on the disk group(s). At step <b>604</b>, upgrade manager <b>250</b> concatenates component(s) of the vSAN1 object with component(s) of the vSAN2 object. An example is shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, where vSAN2 object <b>246</b>-<b>1</b> includes component <b>248</b>-<b>1</b> (performance component) and component <b>248</b>-<b>2</b> (capacity component) concatenated with component <b>244</b>-<b>1</b> (capacity component) of vSAN object <b>242</b>-<b>1</b>.</p><p id="p-0032" num="0031">At step <b>606</b>, upgrade manager <b>250</b> creates empty logical and middle maps in the capacity component of the vSAN2 object (e.g., in metadata structure <b>308</b> of disk group <b>306</b>). At step <b>608</b>, upgrade manager <b>250</b> identifies written extents in the vSAN1 object storage (e.g., disk group <b>302</b>) by querying the metadata thereon (e.g., metadata <b>310</b>). Metadata <b>310</b> identifies the physical locations and extents of data for the vSAN1 object.</p><p id="p-0033" num="0032">At step <b>610</b>, upgrade manager <b>250</b> populates the logical and middle maps in the capacity component of the vSAN2 object based on the vSAN1 metadata. At step <b>612</b>, initial entries <b>504</b> in logical map <b>502</b> point to initial entries <b>508</b> in middle map <b>512</b>. At step <b>614</b>, initial entries <b>508</b> in middle map <b>512</b> point to physical addresses <b>514</b> in the vSAN1 component (e.g., physical address on disk group <b>302</b>). During the upgrade process, as new writes are received, new entries <b>506</b> are created in logical map <b>502</b> and new entries <b>510</b> are created in middle map <b>512</b>. New entries <b>506</b> point to new entries <b>510</b>, and new entries <b>510</b> point to physical addresses <b>516</b> in the vSAN2 component (e.g., disk group <b>306</b>). During the upgrade process, COW snapshots can be generated capturing states of initial entries <b>504</b> and new entries <b>506</b> in logical map <b>502</b>.</p><p id="p-0034" num="0033">At step <b>616</b>, upgrade manager <b>250</b> reads data from the vSAN1 object and writes the data to the vSAN2 object. Upgrade manager <b>250</b> updates initial entries <b>508</b> in middle map <b>512</b> as the data is moved from the vSAN1 object to the vSAN2 object. As shown in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, at the end of the upgrade process, all entries (e.g., including all initial entries <b>508</b>) in middle map <b>512</b> point to physical addresses <b>516</b> in the vSAN2 component (e.g., disk group <b>306</b>). During step <b>616</b>, DOM <b>234</b> can write new data to the vSAN2 object and update the logical and middle maps accordingly with new entries <b>506</b> and new entries <b>510</b>, respectively (step <b>618</b>). During step <b>616</b>, DOM <b>234</b> can generate one or more COW snapshots of the vSAN2 object by capture state(s) of the logical map (step <b>620</b>). At step <b>622</b>, upgrade manager <b>205</b> completes upgrade of the vSAN1 object to the vSAN2 object. Since there are no more references to locations in component(s) of the vSAN1 object, the vSAN1 object can be freed and the resources reclaimed.</p><p id="p-0035" num="0034">Another approach for upgrading a vSAN1 object is to first create an empty logical and middle map. All write operations will populate these maps similar to a fresh new vSAN2 storage object. The read path will first lookup data in the new map and if the data is not present, the read path will fetch the data from the vSAN1 storage to fulfill the read request. Meanwhile, the upgrade process moves non-overwritten data from vSAN1 to vSAN2 storage. This approach, however, prevents generation of COW snapshots until all data has been moved over. This can take a long time and any snapshot generated during this time needs to be in a format other than COW, such as delta disk format.</p><p id="p-0036" num="0035">The upgrade technique described in <figref idref="DRAWINGS">FIG. <b>6</b></figref> has several advantages. A COW snapshot can be generated during the upgrade process while data is still being moved from vSAN1 to vSAN2 storage. Further, read operations are only redirected when the middle map indicates that data is present in vSAN1 storage.</p><p id="p-0037" num="0036">One or more embodiments of the invention also relate to a device or an apparatus for performing these operations. The apparatus may be specially constructed for required purposes, or the apparatus may be a general-purpose computer selectively activated or configured by a computer program stored in the computer. Various general-purpose machines may be used with computer programs written in accordance with the teachings herein, or it may be more convenient to construct a more specialized apparatus to perform the required operations.</p><p id="p-0038" num="0037">The embodiments described herein may be practiced with other computer system configurations including hand-held devices, microprocessor systems, microprocessor-based or programmable consumer electronics, minicomputers, mainframe computers, etc.</p><p id="p-0039" num="0038">One or more embodiments of the present invention may be implemented as one or more computer programs or as one or more computer program modules embodied in computer readable media. The term computer readable medium refers to any data storage device that can store data which can thereafter be input to a computer system. Computer readable media may be based on any existing or subsequently developed technology that embodies computer programs in a manner that enables a computer to read the programs. Examples of computer readable media are hard drives, NAS systems, read-only memory (ROM), RAM, compact disks (CDs), digital versatile disks (DVDs), magnetic tapes, and other optical and non-optical data storage devices. A computer readable medium can also be distributed over a network-coupled computer system so that the computer readable code is stored and executed in a distributed fashion.</p><p id="p-0040" num="0039">Although one or more embodiments of the present invention have been described in some detail for clarity of understanding, certain changes may be made within the scope of the claims. Accordingly, the described embodiments are to be considered as illustrative and not restrictive, and the scope of the claims is not to be limited to details given herein but may be modified within the scope and equivalents of the claims. In the claims, elements and/or steps do not imply any particular order of operation unless explicitly stated in the claims.</p><p id="p-0041" num="0040">Virtualization systems in accordance with the various embodiments may be implemented as hosted embodiments, non-hosted embodiments, or as embodiments that blur distinctions between the two. Furthermore, various virtualization operations may be wholly or partially implemented in hardware. For example, a hardware implementation may employ a look-up table for modification of storage access requests to secure non-disk data.</p><p id="p-0042" num="0041">Many variations, additions, and improvements are possible, regardless of the degree of virtualization. The virtualization software can therefore include components of a host, console, or guest OS that perform virtualization functions.</p><p id="p-0043" num="0042">Plural instances may be provided for components, operations, or structures described herein as a single instance. Boundaries between components, operations, and data stores are somewhat arbitrary, and particular operations are illustrated in the context of specific illustrative configurations. Other allocations of functionality are envisioned and may fall within the scope of the invention. In general, structures and functionalities presented as separate components in exemplary configurations may be implemented as a combined structure or component. Similarly, structures and functionalities presented as a single component may be implemented as separate components. These and other variations, additions, and improvements may fall within the scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of upgrading a distributed storage object from a first version to a second version, comprising:<claim-text>querying metadata of a first component, the first component configured according to the first version of the distributed storage object, the metadata defining extents of data on a disk group of the first component;</claim-text><claim-text>populating, for a second component configured according to the second version of the distributed storage object, a logical map and a middle map based on the metadata such that initial entries in the logical map point to initial entries in the middle map, and the initial entries in the middle map point to physical addresses of the disk group of the first component; and</claim-text><claim-text>reading the data from the disk group of the first component and writing the data to a disk group of the second component while updating the initial entries in the middle map.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second component uses a log-structured file system.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the distributed storage object is stored on local storage devices of virtualized hosts in a cluster.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, after all the data is read from the disk group of the first component and written to the disk group of the second component, all entries in the middle map point only to physical addresses of the disk group of the second component.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, during the step of reading the data from the disk group of the first component and writing the data to the disk group of the second component, the method comprises:<claim-text>writing new data to the disk group of the second component; and</claim-text><claim-text>updating new entries in the logical map to point to new entries in the middle map, the new entries in the middle map pointing to physical addresses of the disk group in the second component.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, during the step of reading the data from the disk group of the first component and writing the data to the disk group of the second component, the method comprises:<claim-text>generating a copy-on-write (COW) snapshot of the distributed storage object by capturing a state of the logical map.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, prior to the step of querying, the method comprises:<claim-text>concatenating the first component and the second component as part of the distributed storage object; and</claim-text><claim-text>creating empty versions of the logical map and the middle map in the second component.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A non-transitory computer readable medium comprising instructions to be executed in a computing device to cause the computing device to carry out a method of upgrading a distributed storage object from a first version to a second version, comprising:<claim-text>querying metadata of a first component, the first component configured according to the first version of the distributed storage object, the metadata defining extents of data on a disk group of the first component;</claim-text><claim-text>populating, for a second component configured according to the second version of the distributed storage object, a logical map and a middle map based on the metadata such that initial entries in the logical map point to initial entries in the middle map, and the initial entries in the middle map point to physical addresses of the disk group of the first component; and</claim-text><claim-text>reading the data from the disk group of the first component and writing the data to a disk group of the second component while updating the initial entries in the middle map.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the second component uses a log-structured file system.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the distributed storage object is stored on local storage devices of virtualized hosts in a cluster.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, after all the data is read from the disk group of the first component and written to the disk group of the second component, all entries in the middle map point only to physical addresses of the disk group of the second component.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, during the step of reading the data from the disk group of the first component and writing the data to the disk group of the second component, the method comprises:<claim-text>writing new data to the disk group of the second component; and</claim-text><claim-text>updating new entries in the logical map to point to new entries in the middle map, the new entries in the middle map pointing to physical addresses of the disk group in the second component.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, during the step of reading the data from the disk group of the first component and writing the data to the disk group of the second component, the method comprises:<claim-text>generating a copy-on-write (COW) snapshot of the distributed storage object by capturing a state of the logical map.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein, prior to the step of querying, the method comprises:<claim-text>concatenating the first component and the second component as part of the distributed storage object; and</claim-text><claim-text>creating empty versions of the logical map and the middle map in the second component.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A virtualized computing system having a cluster comprising hosts connected to a network, the virtualized computing system comprising:<claim-text>hardware platforms of the hosts configured to execute software platforms including distributed storage software;</claim-text><claim-text>shared storage comprising local storage devices of the hardware platforms, the shared storage storing a distributed storage object managed by the distributed storage software;</claim-text><claim-text>an upgrade manager of the distributed storage software configured to upgrade the distributed storage object from a first version to a second version by:<claim-text>querying metadata of a first component, the first component configured according to the first version of the distributed storage object, the metadata defining extents of data on a disk group of the first component;</claim-text><claim-text>populating, for a second component configured according to the second version of the distributed storage object, a logical map and a middle map based on the metadata such that initial entries in the logical map point to initial entries in the middle map, and the initial entries in the middle map point to physical addresses of the disk group of the first component; and</claim-text><claim-text>reading the data from the disk group of the first component and writing the data to a disk group of the second component while updating the initial entries in the middle map.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the second component uses a log-structured file system.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein, after all the data is read from the disk group of the first component and written to the disk group of the second component, all entries in the middle map point only to physical addresses of the disk group of the second component.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein, during the reading of the data from the disk group of the first component and the writing of the data to the disk group of the second component, the upgrade manager is configured to:<claim-text>write new data to the disk group of the second component; and</claim-text><claim-text>update new entries in the logical map to point to new entries in the middle map, the new entries in the middle map pointing to physical addresses of the disk group in the second component.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein, during the reading of the data from the disk group of the first component and the writing of the data to the disk group of the second component, the upgrade manager is configured to:<claim-text>generate a copy-on-write (COW) snapshot of the distributed storage object by capturing a state of the logical map.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The virtualized computing system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein prior to querying the metadata, the upgrade manager is configured to:<claim-text>concatenate the first component and the second component as part of the distributed storage object; and</claim-text><claim-text>create empty versions of the logical map and the middle map in the second component.</claim-text></claim-text></claim></claims></us-patent-application>