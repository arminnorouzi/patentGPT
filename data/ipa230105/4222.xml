<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004223A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004223</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17810179</doc-number><date>20220630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>SE</country><doc-number>2150849-4</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>9</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>015</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>197</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>90</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>9</main-group><subgroup>04559</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">EYE TRACKING SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TOBII AB</orgname><address><city>Danderyd</city><country>SE</country></address></addressbook><residence><country>SE</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZACHRISSON</last-name><first-name>Joakim</first-name><address><city>Danderyd</city><country>SE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>JOHANSSON</last-name><first-name>Simon</first-name><address><city>Danderyd</city><country>SE</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>ROSELL</last-name><first-name>Mikael</first-name><address><city>Danderyd</city><country>SE</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>WRANG</last-name><first-name>Daniel</first-name><address><city>Danderyd</city><country>SE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An eye tracking system comprising a controller configured to receive a reference image of an eye of a user and a current image of the eye of the user. The controller is also configured to determine a difference between the reference image and the current image to define a differential image. The differential image has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns. Each pixel location has a differential intensity value. The controller is further configured to calculate a plurality of row values by combining the differential intensity values in corresponding rows of the differential image and to determine eyelid data based on the plurality of row values.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="58.17mm" wi="116.25mm" file="US20230004223A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="216.58mm" wi="137.16mm" file="US20230004223A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="189.15mm" wi="143.68mm" file="US20230004223A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="217.51mm" wi="163.66mm" file="US20230004223A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="221.83mm" wi="138.43mm" file="US20230004223A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="212.43mm" wi="148.93mm" file="US20230004223A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="228.52mm" wi="137.08mm" file="US20230004223A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="101.52mm" wi="127.68mm" file="US20230004223A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCES TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to Swedish Application No. 2150849-4, entitled &#x201c;AN EYE TRACKING SYSTEM,&#x201d; filed on Jun. 30, 2021. The entire disclosure of the above-referenced application is incorporated herein by this reference.</p><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure generally relates to the field of eye tracking. In particular, the present disclosure relates to eye tracking systems and methods for determining eyelid data that can represent movement of an eyelid (such as a blink) or the location of the eyelid in an image.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In eye tracking applications, digital images are retrieved of the eyes of a user and the digital images are analyzed in order to estimate the gaze direction of the user. The estimation of the gaze direction may be based on computer based image analysis of features of the imaged eye. One known example method of eye tracking includes the use of infrared light and an image sensor. The infrared light is directed towards the pupil of a user and the reflection of the light is captured by an image sensor.</p><p id="p-0005" num="0004">Many eye tracking systems estimate gaze direction based on identification of a pupil position together with glints or corneal reflections. Therefore, accuracy in the estimation of gaze direction may depend upon an accuracy of the identification or detection of the pupil position and/or the corneal reflections. One or more spurious image features such as stray reflections may be present in the digital images which can detrimentally affect eye feature identification. For example, spurious image features can result in incorrect glint to illuminator matching and/or an incorrect pupil position, resulting in an erroneous gaze determination. It can be difficult to determine when such errors have occurred and eye tracking systems can get stuck in an erroneous tracking sequence.</p><p id="p-0006" num="0005">Portable or wearable eye tracking devices have also been previously described. One such eye tracking system is described in U.S. Pat. No. 9,041,787 (which is hereby incorporated by reference in its entirety). A wearable eye tracking device is described using illuminators and image sensors for determining gaze direction.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">According to a first aspect of the present disclosure there is provided an eye tracking system comprising a controller configured to: receive a reference image of an eye of a user; receive a current image of the eye of the user; determine a difference between the reference image and the current image to define a differential image, wherein the differential image has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a differential intensity value; calculate a plurality of row values by combining the differential intensity values in corresponding rows of the differential image; and determine eyelid data based on the plurality of row values. In this way, eyelid data can be determined in a computationally efficient way.</p><p id="p-0008" num="0007">The reference image and the current image may comprise a pixel array of pixel locations, each pixel location having an intensity value. The controller may be configured to determine the difference between the reference image and the current image by matrix subtraction of the corresponding pixel arrays to define the differential image as the pixel array of differential intensity values.</p><p id="p-0009" num="0008">The controller may be configured to: down-sample the received reference image and the received current image to provide a down-sampled reference image and a down-sampled current image respectively; and determine the difference between the down-sampled reference image and the down-sampled current image to define the differential image.</p><p id="p-0010" num="0009">The controller may be configured to perform a coordinate transformation of the received reference image and the received current image to provide a reoriented reference image and a reoriented current image respectively, such that the eyelid movements are vertical in the reoriented images; and determine the difference between the reoriented reference image and the reoriented current image to define the differential image.</p><p id="p-0011" num="0010">The controller may be configured to: perform one or more image enhancement operations on the differential image to provide an enhanced differential image; and calculate the plurality of row values by combining the differential intensity values in corresponding rows of the enhanced differential image.</p><p id="p-0012" num="0011">The controller may be configured to: calculate the plurality of row values by summing the differential intensity values in the corresponding rows of the differential image.</p><p id="p-0013" num="0012">The eyelid data may comprise eyelid location data that represents a location of an edge of an eyelid in the current image. The controller may be configured to: determine the eyelid location data as a row identifier associated with the row value that has the highest value, if that highest value is greater than an eyelid detection threshold.</p><p id="p-0014" num="0013">The eyelid data may comprise eyelid location data that represents a location of an edge of an eyelid in the current image. The controller may be configured to: identify the row value that has the highest value; determine an eyelid detection region of the differential image that comprises an above-eyelid detection region of the differential image as a predetermined number of rows that are above the row that has the highest row value, a below-eyelid detection region of the differential image as a predetermined number of rows that are below the row that has the highest row value, and the row that has the highest row value; for each of the plurality of columns in the eyelid detection region, determine an eyelid edge coordinate as a combination of the row identifier for the pixel in the eyelid detection region that has the highest differential intensity value and a column identifier for the associated column; and provide the eyelid location data as the plurality of eyelid edge coordinates.</p><p id="p-0015" num="0014">The eyelid data may comprise blink data that can have either a blinking value or a not-blinking value to provide an indication of whether or not the eye of the user is blinking in the current image. The controller may comprise a blink detector that is configured to: set the blink data as the blinking value if the highest row value is greater than an eyelid detection threshold.</p><p id="p-0016" num="0015">The eyelid data may further comprise eyelid location data that represents a location of an edge of an eyelid in the current image. The blink detector may be further configured to: receive pupil location data representative of the location of the pupil in the current image; determine a pupil-eyelid distance as the vertical distance between the pupil location data and the eyelid location data; and set the blink data as the blinking value if: the highest row value is greater than an eyelid detection threshold; and the pupil-eyelid distance is less than an eye closing threshold.</p><p id="p-0017" num="0016">The blink detector may be further configured to: receive pupil movement data representative of movement of the pupil between images up to the current image; and set the blink data as the blinking value if the highest row value is greater than an eyelid detection threshold and the pupil movement data does not represent downward movement.</p><p id="p-0018" num="0017">If the blink data has the blinking value, the blink detector may be further configured to: sequentially receive subsequent images; for one or more of the subsequent images: determine a difference between a subsequent reference image and the subsequent image to define a subsequent differential image, wherein the subsequent differential image has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a subsequent differential intensity value; set the blink data as the non-blinking value when at least a predetermined number of subsequent images have been received after the current image for which the blink data was set as the blinking value; calculate a plurality of row values by combining the subsequent differential intensity values in corresponding rows of the subsequent differential image; determine a subsequent maximum row value as the highest value of the plurality of row values; compare the subsequent maximum row value for the most recently received subsequent image with an eye opening threshold; in response to the subsequent maximum row value for the most recently received subsequent current image being greater than the eye opening threshold: (a) store the subsequent maximum row value for the most recently received subsequent current image as a peak value, (b) start a count of subsequently received frames from an initial value, (c) if the subsequent maximum row value of a later subsequent image is greater than the peak value, then return to step (a), and (d) if the count reaches an eyelid-open value without the subsequent maximum row value of a later subsequent image exceeding the peak value, then set the blink data as the not-blinking value.</p><p id="p-0019" num="0018">The controller may be configured to calculate the plurality of row values by: combining the differential intensity values in all of the columns of the corresponding rows of the differential image; calculating the plurality of row values by combining the differential intensity values in a subset of the columns of the corresponding rows of the differential image; calculating the plurality of row values by combining the differential intensity values in corresponding individual rows of the differential image; or calculating the plurality of row values by combining the differential intensity values in corresponding sets of adjacent rows of the differential image.</p><p id="p-0020" num="0019">There is also provided a head mounted device comprising any eye tracking system disclosed herein.</p><p id="p-0021" num="0020">According to a further aspect of the disclosure, there is provided a method of providing eyelid data, the method comprising: receiving a reference image of an eye of a user; receiving a current image of the eye of the user; determining a difference between the reference image and the current image to define a differential image, wherein the differential image has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a differential intensity value; calculating a plurality of row values by combining the differential intensity values in corresponding rows of the differential image; and determining eyelid data based on the plurality of row values.</p><p id="p-0022" num="0021">According to a further aspect of the disclosure, there is provided one or more non-transitory computer-readable storage media storing computer-executable instructions that, when executed by a computing system, causes the computing system to perform any method disclosed herein.</p><p id="p-0023" num="0022">There is also disclosed an eye tracking system comprising a blink detector configured to: receive left-blink data that represents whether or not the left eye of a user is blinking; receive right-blink data that represents whether or not the right eye of the user is blinking; and apply a logical AND operation to the left-blink data and the right-blink data in order to provide combined-blink data that only represents a blink when both the left-blink data and the right-blink data indicate that both eyes of the user are blinking.</p><p id="p-0024" num="0023">There is also disclosed a method of operating an eye tracking system, the method comprising: receiving left-blink data that represents whether or not the left eye of a user is blinking; receiving right-blink data that represents whether or not the right eye of the user is blinking; and applying a logical AND operation to the left-blink data and the right-blink data in order to provide combined-blink data that only represents a blink when both the left-blink data and the right-blink data indicate that both eyes of the user are blinking.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0025" num="0024">One or more embodiments will now be described by way of example only with reference to the accompanying drawings in which:</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a schematic view of an eye tracking system which may be used to capture a sequence of images that can be used by example embodiments;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example image of a pair of eyes;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example of an eye tracking system according to an embodiment of the present disclosure;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows example images that can be processed by an eye tracking system according to an embodiment of the present disclosure;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows another example of an eye tracking system according to an embodiment of the present disclosure;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates schematically a process flow that can be performed by an eyelid data detector in order to detect a blink;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates schematically a process flow that can be performed by an eyelid data detector in order to detect the end of a blink; and</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates schematically a process flow that can be performed by a blink detector.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a simplified view of an eye tracking system <b>100</b> (which may also be referred to as a gaze tracking system) in a head mounted device in the form of a virtual or augmented reality (VR or AR) device or VR or AR glasses or anything related, such as extended reality (XR) or mixed reality (MR) headsets. The system <b>100</b> comprises an image sensor <b>120</b> (e.g. a camera) for capturing images of the eyes of the user. The system may optionally include one or more illuminators <b>110</b>-<b>119</b> for illuminating the eyes of a user, which may for example be light emitting diodes emitting light in the infrared frequency band, or in the near infrared frequency band and which may be physically arranged in a variety of configurations. The image sensor <b>120</b> may for example be an image sensor of any type, such as a complementary metal oxide semiconductor (CMOS) image sensor or a charged coupled device (CCD) image sensor. The image sensor may consist of an integrated circuit containing an array of pixel sensors, each pixel containing a photodetector and an active amplifier. The image sensor may be capable of converting light into digital signals. In one or more examples, it could be an Infrared image sensor or IR image sensor, an RGB sensor, an RGBW sensor or an RGB or RGBW sensor with IR filter.</p><p id="p-0035" num="0034">The eye tracking system <b>100</b> may comprise circuitry or one or more controllers <b>125</b>, for example including a receiver <b>126</b> and processing circuitry <b>127</b>, for receiving and processing the images captured by the image sensor <b>120</b>. The circuitry <b>125</b> may for example be connected to the image sensor <b>120</b> and the optional one or more illuminators <b>110</b>-<b>119</b> via a wired or a wireless connection and be co-located with the image sensor <b>120</b> and the one or more illuminators <b>110</b>-<b>119</b> or located at a distance, e.g. in a different device. In another example, the circuitry <b>125</b> may be provided in one or more stacked layers below the light sensitive surface of the light sensor <b>120</b>.</p><p id="p-0036" num="0035">The eye tracking system <b>100</b> may include a display (not shown) for presenting information and/or visual stimuli to the user. The display may comprise a VR display which presents imagery and substantially blocks the user's view of the real world or an AR display which presents imagery that is to be perceived as overlaid over the user's view of the real world.</p><p id="p-0037" num="0036">The location of the image sensor <b>120</b> for one eye in such a system <b>100</b> is generally away from the line of sight for the user in order not to obscure the display for that eye. This configuration may be, for example, enabled by means of so-called hot mirrors which reflect a portion of the light and allows the rest of the light to pass, e.g. infrared light is reflected, and visible light is allowed to pass.</p><p id="p-0038" num="0037">While in the above example the images of the user's eye are captured by a head mounted image sensor <b>120</b>, in other examples the images may be captured by an image sensor that is not head mounted. Such a non-head mounted system may be referred to as a remote system.</p><p id="p-0039" num="0038">In an eye tracking system, a gaze signal can be computed per each eye of the user (left and right). The quality of these gaze signals can be reduced by disturbances in the input images (such as image noise) and by incorrect algorithm behavior (such as incorrect predictions). A goal of the eye tracking system is to deliver a gaze signal that is as good as possible, both in terms of accuracy (bias error) and precision (variance error). For many applications it can be sufficient to deliver only one gaze signal per time instance, rather than both the gaze of the left and right eyes individually. Further, the combined gaze signal can be provided in combination with the left and right signals. Such a gaze signal can be referred to as a combined gaze signal.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a simplified example of an image <b>229</b> of a pair of eyes, captured by an eye tracking system such as the system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The image <b>229</b> can be considered as including a right eye image <b>228</b>, of a person's right eye, and a left eye image <b>234</b>, of the person's left eye. In this example the right eye image <b>228</b> and the left eye image <b>234</b> are both parts of a larger image of both of the person's eyes. In other examples, separate image sensors may be used to acquire the right eye image <b>228</b> and the left eye image <b>234</b>.</p><p id="p-0041" num="0040">The system may employ image processing (such as digital image processing) for extracting features in the image. The system may for example identify the location of the pupil <b>230</b>, <b>236</b> in the one or more images captured by the image sensor. The system may determine the location of the pupil <b>230</b>, <b>236</b> using a pupil detection process. The system may also identify corneal reflections <b>232</b>, <b>238</b> located in close proximity to the pupil <b>230</b>, <b>236</b>. The system may estimate a corneal center or eye ball center based on the corneal reflections <b>232</b>, <b>238</b>. For example, the system may match each of the individual corneal reflections <b>232</b>, <b>238</b> for each eye with a corresponding illuminator and determine the corneal center of each eye based on the matching. The system can then determine a gaze ray (which may also be referred to as a gaze vector) for each eye including a position vector and a direction vector. The gaze ray may be based on a gaze origin and gaze direction which can be determined from the respective glint to illuminator matching/corneal centers and the determined pupil position. The gaze direction and gaze origin may themselves be separate vectors. The gaze rays for each eye may be combined to provide a combined gaze ray.</p><p id="p-0042" num="0041">As mentioned above, any errors in glint to illuminator matching (which may simply be referred to as glint matching) or pupil position determination can result in an incorrect gaze determination. The results of glint matching and pupil detection can be considered as binary. For glint matching, either the glint matching is correct and a cornea position is determined that is good enough for accurate gaze computation, or it is incorrect which results in a cornea position that cannot be used to accurately map gaze. Similarly, for pupil detection, either the detected pupil is close enough for circle fitting to accurately identify a pupil, or it is incorrect such that the correct pupil cannot be identified and cannot be used to accurately map gaze. However, errors in determining these intermediate parameters can be difficult to detect. As a result, some systems can get stuck in an incorrect tracking regime and provide an insufficient gaze determination. This can be particularly detrimental for many eye tracking applications.</p><p id="p-0043" num="0042">In particular, gaze estimation is often very unstable during blinking and squinting. This causes significant problems for foveated rendering, in which an image quality is reduced in the user's peripheral vision as determined by their calculated gaze. In the use case of foveated rendering, unstable gaze estimation will cause the foveation area to jump, causing noticeable graphic artifacts and a bad user experience. This can be mitigated using a blink detector to lock the foveation area during a blink.</p><p id="p-0044" num="0043">Blink detection can also be used within the application of social interactions in VR since it allows for a more complete visualization of the eyes of a virtual avatar.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an example of an eye tracking system <b>339</b> according to an embodiment of the present disclosure. The functionality that is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be provided by one or more controllers. The eye tracking system may be part of, or associated with, a head mounted device or a remote system. The eye tracking system <b>339</b> provides eyelid data <b>313</b> that is representative of a characteristic of a user's eyelid. As will be discussed in detail below, the eyelid data <b>313</b> can be eyelid movement data (e.g. representative of a blink) or eyelid location data that identifies the location of the eyelid in an image. In some examples, the eyelid location data can be one or more row identifiers or a set of coordinates. The functionality of <figref idref="DRAWINGS">FIG. <b>3</b></figref> advantageously enables eyelid data <b>313</b> to be determined in a computationally efficient way.</p><p id="p-0046" num="0045">The eye tracking system <b>339</b> in this example includes an optional reference memory <b>304</b>, which provides a reference image <b>305</b> of an eye of a user to a differential image calculator <b>306</b>. The differential image calculator <b>306</b> also receives a current image <b>309</b> of the eye of the user. The current image <b>309</b> is the image of the eye of the user for which the eyelid data <b>313</b> will be calculated. The reference image <b>305</b> is an image from the same eye of the same user at an earlier instant in time (that is, an earlier image in a stream of images/frames of the user's eye). In some examples the reference image <b>305</b> may be the immediately preceding image in the stream, and in other examples it may not be the immediately preceding image in the stream. It will be appreciated from the description that follows that there can be advantages to there being a relatively short period between the times that the reference image <b>305</b> and the current image <b>309</b> were recorded because the eyelid data <b>313</b> will be determined based upon a difference between these images.</p><p id="p-0047" num="0046">The images of an eye of a user, including the reference image <b>305</b> and the sample image <b>309</b>, may comprise digital images produced by an image sensor. The image may equivalently be referred to as an image frame or frame. The image may comprise a pixel array, which includes a plurality of pixel locations and an intensity value at each of the pixel locations. The pixel arrays comprise two dimensional arrays of pixel locations that are arranged in a plurality of rows and columns. The rows and columns are mutually orthogonal. In the following examples, the rows will be described as horizontal lines of pixels in the images and the columns will be described as vertical lines of pixels. However, it will be appreciated that the rows and columns do not have to be horizontally and vertically aligned in the image, especially if the user's eye is not presented horizontally in the image. In some examples, if the user's eye is not presented horizontally in the images, then the eye tracking system <b>339</b> can perform some preprocessing on the images to rotate the images such that the horizontal aspect of the user's eye is aligned with the horizontal aspect of the image before it is passed to the differential image calculator <b>306</b> for processing.</p><p id="p-0048" num="0047">Furthermore, in some examples the eye tracking system <b>339</b> can use images in which the horizontal aspect of the user's eye is aligned with a vertical aspect of the image. In which case, the lines of pixels that extend in a vertical direction in the image can be considered as the rows of the image because they align with the horizontal aspect of the user's eye.</p><p id="p-0049" num="0048">The differential image calculator <b>306</b> determines a difference between the reference image <b>305</b> and the current image <b>309</b> to define a differential image <b>307</b>. For example, the differential image calculator <b>306</b> can perform a matrix subtraction between the pixel arrays of the reference image <b>305</b> and the current image <b>309</b>. In other words, the differential image calculator <b>306</b> determines the difference between intensity values at corresponding pixel locations in the reference image and the current image. The resultant differential image <b>307</b> has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a differential intensity value.</p><p id="p-0050" num="0049">Turning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an example of a reference image <b>405</b> and an example of a subsequent current image <b>409</b> are shown. The images in this example are near infrared (NIR) images captured by a NIR image sensor.</p><p id="p-0051" num="0050">In the reference image <b>405</b>, the entire pupil <b>440</b> is visible as a dark region of the image and part of the iris <b>441</b> is visible around the pupil <b>440</b>. The remaining parts of the image, which it will be appreciated will relate to the sclera and the user's face, are lighter than the pupil <b>440</b> and the iris <b>441</b>.</p><p id="p-0052" num="0051">In the current image <b>409</b>, the user's upper eyelid has moved downwards, thereby obscuring an upper region of the pupil <b>440</b>&#x2032; and an upper region of the iris <b>441</b>&#x2032;. Therefore, a smaller portion of the pupil <b>440</b>&#x2032; and the iris <b>441</b>&#x2032; are visible in the current image <b>409</b> than are visible in the reference image <b>405</b>.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b></figref> also shows a differential image <b>407</b>, which represents the difference between the reference image <b>405</b> and the current image <b>409</b>. It can be seen from the differential image <b>407</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref> that those regions of the reference image <b>405</b> and the current image <b>409</b> that have not changed are shown as dark pixels; that is, these pixels have low differential intensity values. Also, those regions of the reference image <b>405</b> that are darker than the corresponding regions in the current image <b>409</b> (i.e. the strips of iris <b>441</b>&#x2032; and pupil <b>440</b>&#x2032; that have been obscured by the eyelid) are shown as bright pixels; that is, these pixels have a high differential intensity value in the differential image <b>407</b>. Since the user's eye is horizontally aligned in the images of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the downward movement of the user's eyelid (as is the case between the reference image <b>405</b> and the current image <b>409</b>) results in high intensity differential intensity values that are grouped in rows of the differential image <b>407</b>. It is this row wise arrangement of differential intensity values in the differential image <b>407</b>, which is caused by movement of the eyelid, that will subsequently be used to determine the eyelid data <b>313</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0054" num="0053">In this way, the difference between the current eye image and a reference eye image (e.g. the previous frame image) is used to determine the eyelid position during movements. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the eyelid is brighter than the pupil, iris and the eyelashes (for NIR images). When the eyelid moves vertically it causes a simultaneous intensity difference in all image columns where the eye is. The eyelid edge can be almost a straight horizontal line in the image, so when the eyelid is moving vertically the difference image will be almost black everywhere except for a few rows where all the pixels in the row are bright (as shown in the differential image <b>407</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0055" num="0054">Returning to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the eye tracking system <b>339</b> includes a row value calculator <b>308</b>. The row value calculator <b>308</b> calculates a plurality of row values <b>311</b> by combining the differential intensity values in corresponding rows of the differential image <b>307</b>. In this implementation, a separate row value <b>311</b> is calculated for each of the rows in the differential image <b>307</b> by combining the differential intensity values in all of the columns for each of the corresponding rows of the differential image <b>307</b>. (Alternative implementations will be described below, including with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>.) Also, in this implementation the differential intensity values are combined by a simple mathematical sum operation. Beneficially, the well-known rowsum function can be used to add the differential intensity values in each of the rows. However, it will be appreciated that many other mathematical operators can be used to combine the differential intensity values, such as: a multiplication of the differential intensity values; a weighted sum of the differential intensity values (for instance such that columns that are closer to the middle of the image are given a higher weighting in the expectation that they are more likely to include the user's pupil which provides a greater contrast to the eyelid and a correspondingly larger differential intensity value as the eyelid moves); or any other mathematical operator that can provide an indicator of the differential intensity values of a plurality of differential intensity values in a row of the differential image <b>307</b>.</p><p id="p-0056" num="0055">The eye tracking system <b>339</b> also includes an eyelid data detector <b>310</b> that determines the eyelid data <b>313</b> based on the plurality of row values <b>311</b>.</p><p id="p-0057" num="0056">In one example the eyelid data <b>313</b> comprises eyelid location data, which represents a location of an edge of an eyelid in the current image <b>309</b>. In such an example, the eyelid data detector <b>310</b> can determine the eyelid location data as a row identifier associated with the row value <b>311</b> that has the highest value. With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, it can be seen that the row (or rows) in the differential image <b>407</b> with the highest intensity values correspond to the regions where the eyelid has moved to a position such that it obscures portions of the iris <b>441</b>, <b>441</b>&#x2032; and pupil <b>440</b>, <b>440</b>&#x2032; that were not obscured in the reference image <b>405</b>. Therefore, these high intensity regions in the differential image represent the edge of the eyelid as it moves downwards from the reference image <b>405</b> to the current image <b>409</b>.</p><p id="p-0058" num="0057">Furthermore, in this example the eyelid data detector <b>310</b> only determines the row identifier associated with the row value <b>311</b> that has the highest value as eyelid location data if the highest row value <b>311</b> is greater than an eyelid detection threshold. That is, the edge of the eyelid is only detected if the combined intensity value for specific rows of the differential image <b>407</b> is sufficiently high to be classified as the edge of the eyelid. In this way the processing that is performed by the eyelid data detector <b>310</b> will not necessarily always output eyelid location data for every received current image <b>309</b>. This is appropriate because the eyelid data detector <b>310</b> would not be able to determine the location of the eyelid if it had not moved between the reference image <b>305</b> and the current image <b>309</b>.</p><p id="p-0059" num="0058">In another example the eyelid data <b>313</b> comprises eyelid movement data, which can include blink data. Such blink data can be set as a blinking value or a non-blinking value as will be discussed in detail below with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0060" num="0059">It will be appreciated that some steps in eye tracking, such as feature detection, can be computationally demanding. An advantage of examples disclosed herein is that, because the edge of the eyelid can be determined in a computationally efficient way, some of those steps (e.g. pupil detection or glint localization) can be run on only the parts of the image that are relevant to the steps (e.g. only performing pupil detection on parts of the image that are below the eyelid). In this way, better detectors can be provided and/or computation time can be saved. In contrast, positioning the eyelid using landmarks from a machine learning algorithm can be computationally expensive and might not be feasible in an embedded system.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows another example of an eye tracking system <b>539</b> according to an embodiment of the present disclosure. Features of <figref idref="DRAWINGS">FIG. <b>5</b></figref> that are also shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> will be given corresponding numbers in the <b>500</b> series. <figref idref="DRAWINGS">FIG. <b>5</b></figref> will be used to describe various optional processing modules that are not illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0062" num="0061">The eye tracking system <b>539</b> in this example again includes a reference memory <b>504</b>, which may or may not be provided as part of the eye tracking system <b>539</b>. The reference memory <b>504</b> provides a reference image <b>505</b> of an eye of a user to a down-sampler <b>542</b>.</p><p id="p-0063" num="0062">The reference image <b>505</b> may be an image that immediately precedes the current image <b>509</b> in a stream of images/frames. Alternatively, the reference image <b>505</b> may be an earlier image that does not immediately precede the current image <b>509</b>. This can be useful in detecting slower moving eyelids because using a reference image <b>505</b> that is further back in time, and therefore the eyelid has had time to move by a sufficient amount that results in a significant difference between the current image <b>509</b> and the reference image <b>505</b>.</p><p id="p-0064" num="0063">In some examples, a plurality of reference images <b>505</b> may be provided, each of which is earlier in a stream of images than the current image <b>509</b>. By comparing them each separately to the current image <b>509</b>, i.e., by computing a plurality of differential images <b>507</b>/enhanced differential images <b>549</b>, more information about the eyelid movement will be provided. Therefore, the eyelid data detector <b>510</b> may be able to detect slower moving eyelids and/or detect eyelid movements with greater confidence and/or better accuracy.</p><p id="p-0065" num="0064">The down-sampler <b>542</b> (which may be referred to as a reference down-sampler) down-samples the received reference image <b>505</b> to provide a down-sampled reference image <b>543</b>. The down-sampled reference image <b>543</b> has a lower resolution, and hence fewer pixel locations, than the reference image <b>505</b>. In one example the down-sampler <b>542</b> may perform minimum down-sampling, for instance to reduce the image of a 200&#xd7;200 image down to a 25&#xd7;25 image. Use of down-sampling can make all subsequent processing steps computationally cheaper without significantly reducing the quality/accuracy of the eyelid data <b>513</b>. In fact, in some examples the down-sampling can actually improve the quality/accuracy of the eyelid data <b>513</b> because it can result in the edge of the eyelid in the subsequent differential image <b>507</b> being smoothed out. That is, using the down-sampling to generate a lower resolution image can remove high resolution information that is not needed for finding the eyelid.</p><p id="p-0066" num="0065">The eye tracking system <b>539</b> in this example also includes a coordinate transformer <b>544</b> (which may be referred to as a reference coordinate transformer). The coordinate transformer <b>544</b> performs a coordinate transformation of the down-sampled reference image <b>543</b> (or directly on the received reference image <b>505</b> in some examples) to provide a reoriented reference image <b>545</b> such that the eyelid movements are vertical in the reoriented reference image <b>545</b>.</p><p id="p-0067" num="0066">Such a coordinate transformation can map each pixel location in an input image to a new pixel location in the transformed image, as follows.</p><p id="p-0068" num="0067">If p is a pixel location, p=(x,y):<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0068">p_transformed=f(p_input), where f can be any function that takes a 2D point and returns a 2D point.</li>    </ul>    </li></ul></p><p id="p-0069" num="0069">The coordinate transformer <b>544</b> can create a new transformed image where it writes the pixel values in the transformed image by reading the intensity at location p in the input image and writing that intensity value to location f(p) in the new image. I.e., for each location p in the input image: I_transformed[f(p)]=I_input[p]</p><p id="p-0070" num="0070">For rotation around some point, p_ref (e.g. the center of the image):<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0071">p_transformed=R*(p_input&#x2212;p_ref), where R is a 2D rotation matrix corresponding to the required angle of rotation.</li>    </ul>    </li></ul></p><p id="p-0071" num="0072">The coordinate transformer <b>544</b> receives as an input a roll angle between the camera and eye. For example, if the roll angle is &#x2212;10 degrees such that the eye has a &#x2212;10 degree angle in the image, the coordinate transformer <b>544</b> calculates the rotated image by computing a rotation matrix corresponding to a +10 degree rotation around the image center and then applying the above calculation.</p><p id="p-0072" num="0073">In this way, as indicated above, if the user's eye is not presented horizontally in the images, then the eye tracking system <b>539</b> can perform pre-processing on the images to rotate them such that the horizontal aspect of the user's eye is aligned with the horizontal aspect of the image. This can be useful if the camera that records the images is rolled or is viewing the eye from a perspective. Furthermore, having horizontal rows that generally align with the edge of the user's eyelid can simplify the downstream processing, for instance because known operators such as rowsum can be directly applied to the horizontal rows in the image.</p><p id="p-0073" num="0074">It will be appreciated that the down-sampler <b>542</b> and the coordinate transformer <b>544</b> may be swapped around such that coordinate transformation is performed before down-sampling, while still providing the same overall functionality. Further still, either one or both of the down-sampler <b>542</b> and the coordinate transformer <b>544</b> may be omitted from the eye tracking system <b>539</b> in some examples.</p><p id="p-0074" num="0075">It can be desirable to use the same down-sampling of the reference image <b>505</b> and the current image <b>509</b>. For the coordinate transformation, they may be different if the eye and/or head have moved relative to the camera. Unwanted differential values that arise from head/eye movement can be eliminated by applying different coordinate transforms to each of the images such that both images are made similar and comparable. For example, such that the eye has the same roll angle in both images and the eye has the same size in both images.</p><p id="p-0075" num="0076">In a further still example, a single down-sampler and a single coordinate transformer can be used to process current images <b>509</b> as they are received. In this way, the down-sampled and reoriented image can be provided: i) to the differential calculator <b>506</b> for processing; and also ii) to the reference memory for storing such that it can be used as a reference image for a subsequently received current image. In a yet further example, a coordinate transformer can perform a coordinate transformation on the differential image <b>507</b> instead of the current image <b>509</b> or the reference image <b>505</b>.</p><p id="p-0076" num="0077"><figref idref="DRAWINGS">FIG. <b>5</b></figref> also shows a down-sampler <b>546</b> (which may be referred to as a current image down-sampler) and a coordinate transformer <b>551</b> (which may be referred to as a current image coordinate transformer) for processing the current image <b>509</b>. The current image down-sampler <b>546</b> and the current image coordinate transformer <b>551</b> respectively provide a down-sampled current image <b>547</b> and a reoriented current image <b>552</b> in the same way as the corresponding components that process the reference image <b>505</b>. Also, the current image down-sampler and the current image coordinate transformer <b>551</b> may be swapped around such that coordinate transformation is performed before down-sampling. Further still, either one or both of the current image down-sampler <b>546</b> and the coordinate transformer <b>551</b> may be omitted from the eye tracking system <b>539</b>.</p><p id="p-0077" num="0078">In this example, the differential image calculator <b>506</b> determines the difference between the reoriented reference image <b>545</b> and the reoriented current image <b>552</b> to define the differential image <b>507</b>. In other examples, the differential image calculator <b>506</b> can determine the difference between the down-sampled reference image <b>543</b> and the down-sampled current image <b>547</b> to define the differential image. In fact, the differential image calculator <b>506</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> can be considered as indirectly determining the difference between the down-sampled reference image <b>543</b> and the down-sampled current image <b>547</b> to define the differential image <b>507</b>.</p><p id="p-0078" num="0079">In this example the differential image <b>507</b> represents a subtraction of the reoriented reference image <b>545</b> from the reoriented current image <b>552</b> (which indirectly represents a subtraction of the reference image <b>505</b> from the current image <b>509</b>). Optionally, the differential image calculator <b>506</b> (or a downstream component) can split the differential image <b>507</b> into two difference images&#x2014;a positive differential image that contains positive pixels (that is, differential intensity values that have a positive value but not differential intensity values that have a negative value) and a negative differential image that contains negative pixels (that is, differential intensity values that have a negative value but not differential intensity values that have a positive value).</p><p id="p-0079" num="0080">Since the eyelid skin is brighter than the eye, the edge of an eyelid moving downwards will yield a row of positive pixels in the differential image <b>507</b>. Conversely, the edge of an eyelid moving upwards will yield a row of negative pixels in the differential image <b>507</b>. Therefore the positive differential image can be used to detect the eyelid when it is closing, and the negative differential image can be used to detect the eyelid when it is opening.</p><p id="p-0080" num="0081">Of course, it will be appreciated that if the differential image calculator <b>506</b> subtracts the reoriented current image <b>552</b> from the reoriented reference image <b>545</b> (which indirectly represents a subtraction of the current image <b>509</b> from the reference image <b>505</b>) then a negative differential image can be used to detect the eyelid when it is closing, and the positive differential image can be used to detect the eyelid when it is opening.</p><p id="p-0081" num="0082">As will be discussed below with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>, if the eyelid data detector <b>510</b> is used for blink detection, the eye tracking system <b>539</b> may skip further processing of one of the positive differential image and the negative differential image (depending upon which one represents eyelid opening movements). For example, if no blink has been detected then the user has their eye open and the eye tracking system <b>539</b> operates such that it only detects a downward movement of the eyelid (which is represented by only one of the positive differential image and the negative differential image). Similarly, if it is detected that a blink is in progress then the user has their eye closed and the eye tracking system <b>539</b> operates such that it only detects an upward movement of the eyelid (which is represented by the other one of the positive differential image and the negative differential image).</p><p id="p-0082" num="0083">In the example of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the eye tracking system <b>539</b> includes an image enhancer <b>548</b>. The image enhancer <b>548</b> performs one or more image enhancement operations on the differential image <b>507</b> to provide an enhanced differential image <b>549</b>. For example, the image enhancer <b>548</b> can enhance the differential image <b>507</b> by performing a known edge detection operation to enhance horizontal edges. The enhanced differential image <b>549</b> can also be referred to as a gradient image. An example of enhanced differential image <b>549</b> is shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> with reference <b>449</b>.</p><p id="p-0083" num="0084">The row value calculator <b>508</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref> processes the enhanced differential image <b>549</b> (which can be considered as indirectly processing the differential image <b>507</b>) in order to provide a plurality of row values <b>511</b>. As indicated above, this can include calculating a separate row value <b>511</b> for each of the individual rows in the differential image <b>507</b>. In some applications, the row value calculator <b>508</b> can calculate separate row values <b>511</b> for one or a plurality of rows, and for one or more columns in those rows.</p><p id="p-0084" num="0085">For instance:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0086">the row value calculator <b>508</b> can calculate the plurality of row values <b>511</b> by combining the differential intensity values in all of the columns of the corresponding rows of the differential image <b>507</b>;</li>        <li id="ul0006-0002" num="0087">the row value calculator <b>508</b> can calculate the plurality of row values <b>511</b> by combining the differential intensity values in a subset of the columns of the corresponding rows of the differential image, wherein a subset represents one or a plurality of the columns but not all of the columns;</li>        <li id="ul0006-0003" num="0088">the row value calculator <b>508</b> can calculate the plurality of row values <b>511</b> by combining the differential intensity values in corresponding individual rows of the differential image;</li>        <li id="ul0006-0004" num="0089">the row value calculator <b>508</b> can calculate the plurality of row values <b>511</b> by combining the differential intensity values in corresponding sets of adjacent rows of the differential image. The sets of adjacent rows may overlap with each other or may be contiguous.</li>    </ul>    </li></ul></p><p id="p-0085" num="0090">The eyelid data detector <b>510</b> processes the row values <b>511</b> in order to provide the eyelid data <b>513</b>. In one example, the eyelid data detector <b>510</b> can find which row, or group of adjacent rows, in the enhanced differential image <b>549</b> (and hence, which corresponding rows in the current image <b>509</b>) has/have the highest intensity by computing row sums and searching for a maximum. Further still, as indicated above, if the maximum row value is above some threshold, then the eyelid data detector can update the eyelid data <b>513</b> (that indicates the eyelid position) to a row index/row identifier that is associated with the maximum row value.</p><p id="p-0086" num="0091">In a further still example, the eyelid data detector <b>510</b> can determine eyelid location data that includes eyelid edge coordinates that define an edge of the eyelid in the current image <b>509</b>. Such eyelid edge coordinates are not restricted to defining the edge of the eyelid as a straight line. This can be especially useful if the camera that records the images is underneath, and looking up at, the user's eye because the perspective will result in the edge of the eyelid being arc-shaped. For such an example, the eyelid data detector <b>510</b> can identify the row value <b>511</b> that has the highest value. Then, the eyelid data detector <b>510</b> can determine an eyelid detection region of the enhanced differential image <b>549</b> (or the differential image <b>507</b> if image enhancement is not performed) that includes:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0092">i) an above-eyelid detection region of the enhanced differential image <b>549</b> as a predetermined number of rows that are above the row that has the highest row value;</li>        <li id="ul0008-0002" num="0093">ii) a below-eyelid detection region of the enhanced differential image <b>549</b> as a predetermined number of rows that are below the row that has the highest row value; and</li>        <li id="ul0008-0003" num="0094">iii) the row that has the highest row value.</li>    </ul>    </li></ul></p><p id="p-0087" num="0095">In this way, a region of the enhanced differential image <b>549</b> that is close to the row that has the highest row value is defined as a subset of the image in which the edge of the eyelid is expected to be. Then, for each of the plurality of columns in the eyelid detection region the eye data detector <b>510</b> determines an eyelid edge coordinate as a combination of: the row identifier for the pixel in the eyelid detection region that has the highest differential intensity value; and a column identifier for the associated column. The eye data detector <b>510</b> can then provide the eyelid location data as the plurality of eyelid edge coordinates, which represents the highest intensity differential intensity values in each column of the eyelid detection region.</p><p id="p-0088" num="0096">This example can advantageously estimate the arc shape of the eyelid by finding the maximum row per column (or groups of adjacent columns). This search can be limited to a proximity around the row that has the highest row value <b>511</b>.</p><p id="p-0089" num="0097">A particular advantage of determining eyelid location data that defines the edge of the user's eyelid arises when the current image <b>509</b> (or an image derived from the current image <b>509</b>) is also used for some other processing. For example, the location of the pupil in the current image <b>509</b> may be used for gaze detection. Beneficially, a pupil detection algorithm can use the eyelid location data to exclude regions of the current image that cannot include the pupil&#x2014;for instance because they are above the edge of the user's upper eyelid in the current image. It will be appreciated that this is only one example of how the eyelid location data can be used in a different image processing algorithm to improve the performance of the other image processing algorithm; for instance to reduce processing time and increase processing efficiency.</p><p id="p-0090" num="0098"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates schematically a process flow that can be performed by an eyelid data detector in order to detect a blink, when the blink data has non-blinking value. <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates schematically a process flow that can be performed by an eyelid data detector in order to detect the end of a blink, following the detection of a blink. In <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref> the processes set the value of blink data, which is an example of the eyelid data that is described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref>. As a consequence, an eyelid data detector that provides blink data can be considered as a blink detector. The blink data can have either a blinking value or a not-blinking value to provide an indication of whether or not the eye of the user is blinking in a current image.</p><p id="p-0091" num="0099">Turning to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the process starts by receiving a next set of row values at step <b>653</b>. With reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref> or <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the eyelid data detector receives a new set of row values from the row value calculator for the next current image at step <b>653</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0092" num="0100">At step <b>654</b>, the process compares the highest row value of the set of row values that is received at step <b>653</b> with an eyelid detection threshold. The value of the eyelid detection threshold can be predetermined for a specific resolution of the differential image from which it is calculated and/or to provide a required level of sensitivity in blink detection. If the highest row value is not greater than the eyelid detection threshold, then the process returns to step <b>653</b> to receive the next set of row values for the next current image on the basis that a blink has not been detected. If the highest row value is greater than the eyelid detection threshold, then the process moves on to step <b>655</b> in this example. In another example, if the highest row value is greater than the eyelid detection threshold, then the process moves directly to step <b>658</b> to set the blink data as the blinking value (e.g. TRUE). In some applications it can be beneficial to move straight from step <b>654</b> to step <b>658</b> if the information that is required to perform the intermediate processing steps that are shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is not available (for instance if eyelid location data <b>670</b> and pupil location data <b>671</b> are not available, as will be appreciated from the description that follows).</p><p id="p-0093" num="0101">The eyelid amplitude (which is represented by the highest row value) can be seen as a confidence value of a horizontal edge moving in the expected direction (in this case downwards). Therefore, comparing the highest row value with the eyelid detection threshold can be performed so that a blink is only detected if the eyelid amplitude provides sufficient confidence that a blink is taking place.</p><p id="p-0094" num="0102">In this example, the process also receives eyelid location data <b>670</b> that represents a location of an edge of an eyelid in the current image. Examples of how such eyelid location data <b>670</b> can be calculated are described above. Also, the process receives pupil location data <b>671</b> that is representative of the location of the pupil in the current image. Pupil detection algorithms that can provide the pupil location data <b>671</b> are well-known in the art.</p><p id="p-0095" num="0103">At step <b>655</b>, the process can then determine a pupil-eyelid distance as the vertical distance between the pupil location data <b>671</b> and the eyelid location data <b>670</b>. This vertical distance can be expressed as the number of rows between the location of the edge of the eyelid and the location of the pupil in the current image. The location of the pupil in the current image can be the location of the upper edge of the pupil or the location of center of the pupil.</p><p id="p-0096" num="0104">At step <b>656</b>, the process then compares the determined pupil-eyelid distance with an eye closing threshold. If the pupil-eyelid distance is not less than the eye closing threshold, then the process returns to step <b>653</b> to receive the next set of row values for the next current image on the basis that a blink has not been detected. If the pupil-eyelid distance is less than the eye closing threshold, then the process moves on to step <b>657</b> in this example. In another example, if the pupil-eyelid distance is less than the eye closing threshold, then the process moves directly to step <b>658</b> to set the blink data as the blinking value (e.g. TRUE). In some applications it can be beneficial to move straight from step <b>656</b> to step <b>658</b> if the information that is required to perform the intermediate processing step that is shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is not available (for instance if pupil movement data <b>672</b> is not available, as will be appreciated from the description that follows). In which case, process can set the blink data as the blinking value if: the highest row value is greater than an eyelid detection threshold (as determined at step <b>654</b>); and the pupil-eyelid distance is less than an eye closing threshold (as determined at step <b>656</b>).</p><p id="p-0097" num="0105">In this example, the process also receives pupil movement data <b>672</b> that represents movement of the pupil between images in a sequence of images up to the current image. That is, the pupil movement data <b>672</b> can represent the direction of any movement of the pupil in a predetermined number of images before the current image. It will be appreciated that the pupil movement data <b>672</b> can be determined by calculating the rate of change of the pupil location data <b>671</b> over a series of images (which could be a series of only 2 images). For instance a blink detector algorithm can be provided with the current pupil every frame, and then keep a recent history of pupil locations in a buffer, e.g. the most recent 5 frames. The displacement of the pupil can then be computed easily, for example by subtracting the pupil location of a past frame from the pupil location of the current frame. If the displacement (in y-direction) is smaller than some threshold, then the pupil can be considered stationary.</p><p id="p-0098" num="0106">At step <b>657</b>, the pupil movement data is then processed to determine whether or not it represents downward movement. If the pupil movement data does represent downward movement then the process returns to step <b>653</b> to receive the next set of row values for the next current image on the basis that a blink has not been detected (it could instead be a downward saccade, for example). If the pupil movement data does not represent downward movement, then the process moves on to step <b>658</b> to set the blink data as the blinking value. In this way, the process sets the blink data as the blinking value if: the highest row value is greater than an eyelid detection threshold (as determined at step <b>654</b>); the pupil-eyelid distance is less than eye closing threshold (as determined at step <b>656</b> if the requisite information is available&#x2014;if it is not available, then this criteria can be omitted); and the pupil movement data does not represent downward movement (as determined at step <b>657</b>).</p><p id="p-0099" num="0107">As discussed above, if there is not sufficient information to determine: i) if the pupil-eyelid distance is less than the eye closing threshold (at step <b>656</b>); and/or ii) if the pupil movement data represents downward movement (at step <b>657</b>), then the process bases the decision solely on whether or not the highest row value is greater than the eyelid detection threshold (at step <b>654</b>). A benefit of the processing that is performed at steps <b>656</b> and <b>657</b> is that, for some subjects, it can help distinguish eyelid movements caused by blinks from eyelid movements caused by downward saccades.</p><p id="p-0100" num="0108"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a process for detecting the end of a blink. In some examples the process for detecting the end of a blink may only be implemented following the earlier detection of a blink, for instance in response to the process of <figref idref="DRAWINGS">FIG. <b>6</b></figref> setting the blink data as a blinking value. The process of <figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an optional first processing step <b>759</b> of checking whether or not the blink data equals the blinking value&#x2014;if it does not, then the process moves on the next current image at step <b>760</b> and repeats the check at step <b>759</b> for the next current image.</p><p id="p-0101" num="0109">If the process determines at step <b>759</b> that the blink data does equal the blinking value, then the process moves on to step <b>761</b> and beyond where subsequent images (after the current image for which the blink data was set as the blinking value) are sequentially received and processed. For one or more of the subsequent images, although not shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> but as described above with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref>, the process determines a difference between a subsequent reference image and the subsequent image to define a subsequent differential image. In the same way as described above, the subsequent differential image has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a subsequent differential intensity value. This processing is described in detail with reference to the differential image calculator of <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref>. The names of some of the images and parameters have been relabeled with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref> with a &#x201c;subsequent-&#x201d; prefix in that they are subsequent to the determination of a blink. The names of the images and parameters in <figref idref="DRAWINGS">FIG. <b>7</b></figref> have been labelled this way to distinguish them from the images and parameters that are described with reference to <figref idref="DRAWINGS">FIGS. <b>3</b> and <b>5</b></figref>.</p><p id="p-0102" num="0110">At step <b>761</b>, the process determines whether or not at least a predetermined number (N) of subsequent images have been received after the current image for which the blink data was set as the blinking value. If the predetermined number (N) of subsequent images have not been received, then the process moves to step <b>763</b> to receive the next subsequent image and then move on to step <b>761</b> again to determine whether or not the new predetermined number (N) of subsequent images have now been received, now that one more subsequent image has been received. The comparison with the predetermined number (N) of subsequent images is performed on the basis that a blink should have at least a minimum duration. If that minimum duration (as defined by the predetermined number (N)) has not elapsed, then the process should not trigger the end of the blink. The value for the predetermined number (N) can be set based on the refresh rate of the images and/or to suit any particular application. In one example, the predetermined number (N) may be 10.</p><p id="p-0103" num="0111">At step <b>762</b>, the process can calculate or receive a plurality of row values by combining the subsequent differential intensity values in corresponding rows of the subsequent differential image in the same way described above. The process can then determine a subsequent maximum row value as the highest value of the plurality of row values, and compare the subsequent maximum row value for the most recently received subsequent image with an eye opening threshold. If the subsequent maximum row value for the most recently received subsequent image is not greater than the eye opening threshold, then the process returns to step <b>763</b> to receive the next subsequent image. If the subsequent maximum row value for the most recently received subsequent image is greater than the eye opening threshold, then the process moves on to step <b>764</b>.</p><p id="p-0104" num="0112">It will be appreciated that the processing of steps <b>761</b> and <b>762</b> can be performed in reverse order or in parallel with each other, and only when the result of both comparisons as they are phrased in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a &#x2018;yes&#x2019; does the process move on to step <b>764</b>.</p><p id="p-0105" num="0113">At step <b>764</b>, in response to the subsequent maximum row value for the most recently received subsequent current image being greater than the eye opening threshold, the process stores the subsequent maximum row value for the most recently received subsequent current image as a peak value. At step <b>765</b>, the process starts a count of subsequently received frames from an initial value. It is intuitive for the initial value of the count to be zero, but it is not essential for the functionality of the process. The count can be a count of the number of further subsequent images that are received or can be a time counter&#x2014;both counts are equivalent for a fixed refresh rate of the images. Then at step <b>766</b>, the process moves on to the next subsequent image.</p><p id="p-0106" num="0114">At step <b>767</b>, the process compares the subsequent maximum row value with the peak value. If the subsequent maximum row value is greater than the peak value, then the process returns to step <b>764</b> to store the new (higher) subsequent maximum row value as the new peak value and restart the count at step <b>765</b>. If the subsequent maximum row value is not greater than the peak value, then the process moves on step <b>768</b>.</p><p id="p-0107" num="0115">At step <b>768</b>, the process compares the count with an eyelid-open value (M). If the count is not higher than the eyelid-open value (M) then the process moves to step <b>769</b> in order to increment the count and move to the next subsequent image, and the process then returns to step <b>767</b> to compare the subsequent maximum row value for the new subsequent image with the peak value. If, at step <b>768</b>, the count is higher than the eyelid-open value (M) then the process moves to step <b>773</b> to set the blink data as the non-blinking value (e.g. FALSE).</p><p id="p-0108" num="0116">The processing of these last steps of <figref idref="DRAWINGS">FIG. <b>7</b></figref> can be summarized as: if the count reaches an eyelid-open value without the subsequent maximum row value of a later subsequent image exceeding the peak value, then the process sets the blink data as the not-blinking value (e.g. FALSE). That is, there have to be M consecutive frames with no eyelid amplitude larger than the peak amplitude before the end of the blink is detected. If an amplitude larger than the current peak amplitude is recorded, then the peak value is updated and the counter of consecutive frames towards M is reset.</p><p id="p-0109" num="0117">In this way, a blink detector can be provided that utilizes the eyelid vertical position by estimating whether the eye is currently blinking or not. In the algorithm that is described with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the eyelid vertical position and movement can be used as well as a detection of the pupil in the image. As is known in the art, pupil detection can be a circle fitted to the edge between pupil and iris.</p><p id="p-0110" num="0118">This blink detection can function as a state machine with two states:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0119">1. The user is not blinking; the user's eye is open and the process of <figref idref="DRAWINGS">FIG. <b>6</b></figref> is performed to check if the user is about to close the eye; and</li>        <li id="ul0010-0002" num="0120">2. The user is blinking; the user's eye is closed and the process of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is performed to check if the user is about to open the eye.</li>    </ul>    </li></ul></p><p id="p-0111" num="0121">As discussed above, the processing in state <b>1</b> can be performed on only one of a positive differential image and a negative differential image (depending upon which of the current image and the reference image is subtracted from the other) such that only downward movements of the eyelid are represented. Similarly, the processing in state <b>2</b> can be performed on only the other of the positive differential image and the negative differential image (depending upon which of the current image and the reference image is subtracted from the other) such that only upward movements of the eyelid are represented.</p><p id="p-0112" num="0122">The blink detection algorithm can be run for each eye separately in order to provide: left-blink data that represents whether or not the left eye of a user is blinking; and right-blink data that represents whether or not the right eye of the user is blinking. If the left-blink data indicates a blink, then the thresholds that are applied for blink detection for the right eye may be relaxed, thus reducing the probability of a false negative detection (failing to detect a blink) for the right eye. Similarly, if the right-blink data indicates a blink, then the thresholds that are applied for blink detection for the left eye may be relaxed. This can improve detection performance in situations where the user blinks with both eyes. In this way, the algorithm instances for the left and right eye can depend on each other since they can each receive feedback of the blink state (true/false) of the other eye side. This processing can potentially lead to false positive detection if the user is blinking with only one eye, but that is not a common use case.</p><p id="p-0113" num="0123"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates schematically a process flow that can be performed by a blink detector.</p><p id="p-0114" num="0124">An eye tracking system can be provided that includes a blink detector that receives left-blink data and right-blink data at steps <b>880</b> and <b>881</b>. The left-blink data represents whether or not the left eye of a user is blinking. The right-blink data represents whether or not the right eye of the user is blinking. The left-blink data and the right-blink data can be set by the processing that is described above with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>, performed respectively on images of the user's left and right eye. Alternatively, the left-blink data and the right-blink data can be provided by any other algorithm that is known in the art. At step <b>882</b>, the blink detector can then apply a logical AND operation to the left-blink data and the right-blink data in order to provide combined-blink data. In this way, the combined-blink data only represents a blink when both the left-blink data and the right-blink data indicate that both eyes of the user are blinking. This can provide greater certainty that a blink is correctly identified.</p><p id="p-0115" num="0125">Generally, examples described herein can advantageously find the (vertical) position of the eyelid in a computationally cheap way, which can lead on to enabling better blink detection, eye openness estimation and can allow for limiting the area where e.g. pupil detection is run.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An eye tracking system comprising a controller, the controller configured to:<claim-text>receive a reference image of an eye;</claim-text><claim-text>receive a current image of the eye;</claim-text><claim-text>determine a difference between the reference image and the current image to define a differential image, wherein the differential image comprises a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a differential intensity value;</claim-text><claim-text>calculate a plurality of row values by combining the differential intensity values in corresponding rows of the differential image; and</claim-text><claim-text>determine eyelid data based on the plurality of row values.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reference image and the current image comprise corresponding pixel arrays of pixel locations, each pixel location having an intensity value; and<claim-text>wherein the controller is configured to determine the difference between the reference image and the current image by matrix subtraction of the corresponding pixel arrays to define the differential image as the pixel array of differential intensity values.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is further configured to:<claim-text>down-sample the received reference image and the received current image to provide a down sampled reference image and a down sampled current image, respectively; and</claim-text><claim-text>determine the difference between the down sampled reference image and the down sampled current image to define the differential image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is further configured to:<claim-text>perform a coordinate transformation of the received reference image and the received current image to provide a reoriented reference image and a reoriented current image, respectively, such that eyelid movements are vertical in the reoriented images; and</claim-text><claim-text>determine the difference between the reoriented reference image and the reoriented current image to define the differential image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is further configured to:<claim-text>perform one or more image enhancement operations on the differential image to provide an enhanced differential image; and</claim-text><claim-text>calculate the plurality of row values by combining the differential intensity values in corresponding rows of the enhanced differential image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is further configured to calculate the plurality of row values by summing the differential intensity values in the corresponding rows of the differential image.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eyelid data comprises eyelid location data that represents a location of an edge of an eyelid in the current image; and<claim-text>wherein the controller is further configured to determine the eyelid location data as a row identifier associated with the row value that has a highest value, if that highest value is greater than an eyelid detection threshold.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eyelid data comprises eyelid location data that represents a location of an edge of an eyelid in the current image, and wherein the controller is further configured to:<claim-text>identify the row value that has a highest value;</claim-text><claim-text>determine an eyelid detection region of the differential image that comprises:<claim-text>an above-eyelid detection region of the differential image as a predetermined number of rows that are above the row that has the highest row value,</claim-text><claim-text>a below-eyelid detection region of the differential image as a predetermined number of rows that are below the row that has the highest row value, and</claim-text><claim-text>the row that has the highest row value;</claim-text></claim-text><claim-text>for each of the plurality of columns in the eyelid detection region, determine an eyelid edge coordinate as a combination of the row identifier for the pixel in the eyelid detection region that has the highest differential intensity value and a column identifier for the associated column; and</claim-text><claim-text>provide the eyelid location data as the plurality of eyelid edge coordinates.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eyelid data comprises blink data that has either a blinking value or a not-blinking value to provide an indication of whether or not the eye is blinking in the current image; and<claim-text>wherein the controller further comprises a blink detector that is configured to set the blink data as the blinking value if the highest row value is greater than an eyelid detection threshold.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The eye tracking system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the eyelid data further comprises eyelid location that represents a location of an edge of an eyelid in the current image, and wherein the blink detector is further configured to:<claim-text>receive pupil location data representative of the location of the pupil in the current image</claim-text><claim-text>determine a pupil-eyelid distance as the vertical distance between the pupil location data and the eyelid location data; and</claim-text><claim-text>set the blink data as the blinking value if the highest row value is greater than an eyelid detection threshold and the pupil-eyelid distance is less than an eye closing threshold.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The eye tracking system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the blink detector is further configured to:<claim-text>receive pupil movement data representative of movement of the pupil between images up to the current image; and</claim-text><claim-text>set the blink data as the blinking value if the highest row value is greater than an eyelid detection threshold and the pupil movement data does not represent downward movement.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The eye tracking system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein, if the blink data comprises the blinking value, the blink detector is further configured to sequentially receive subsequent images and for one or more of the subsequent images:<claim-text>determine a difference between a subsequent reference image and the subsequent image to define a subsequent differential image, wherein the subsequent differential image has a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a subsequent differential intensity value;</claim-text><claim-text>set the blink data as the non-blinking value when at least a predetermined number of subsequent images have been received after the current image for which the blink data was set as the blinking value;</claim-text><claim-text>calculate a plurality of row values by combining the subsequent differential intensity values in corresponding rows of the subsequent differential image;</claim-text><claim-text>determine a subsequent maximum row value as the highest value of the plurality of row values;</claim-text><claim-text>compare the subsequent maximum row value for the most recently received subsequent image with an eye opening threshold; and</claim-text><claim-text>in response to the subsequent maximum row value for the most recently received subsequent current image being greater than the eye opening threshold:<claim-text>store the subsequent maximum row value for the most recently received subsequent current image as a peak value,</claim-text><claim-text>start a count of subsequently received frames from an initial value,</claim-text><claim-text>if the subsequent maximum row value of a later subsequent image is greater than the peak value, then store the subsequent maximum row value as the peak value, and</claim-text><claim-text>if the count reaches an eyelid open value without the subsequent maximum row value of a later subsequent image exceeding the peak value, then set the blink data as the not-blinking value.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the controller is configured to calculate the plurality of row values by:<claim-text>combining the differential intensity values in all of the columns of the corresponding rows of the differential image;</claim-text><claim-text>calculating the plurality of row values by combining the differential intensity values in a subset of the columns of the corresponding rows of the differential image;</claim-text><claim-text>calculating the plurality of row values by combining the differential intensity values in corresponding individual rows of the differential image; or</claim-text><claim-text>calculating the plurality of row values by combining the differential intensity values in corresponding sets of adjacent rows of the differential image.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eye tracking system is a component of a head mounted device.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The eye tracking system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eye comprises an eye of a user.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A method of providing eyelid data, the method comprising:<claim-text>receiving a reference image of an eye;</claim-text><claim-text>receiving a current image of the eye;</claim-text><claim-text>determining a difference between the reference image and the current image to define a differential image, wherein the differential image comprises a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a differential intensity value;</claim-text><claim-text>calculating a plurality of row values by combining the differential intensity values in corresponding rows of the differential image; and</claim-text><claim-text>determining eyelid data based on the plurality of row values.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the reference image and the current image comprise corresponding pixel arrays of pixel locations, each pixel location having an intensity value; and<claim-text>further comprising determining the difference between the reference image and the current image by matrix subtraction of the corresponding pixel arrays to define the differential image as the pixel array of differential intensity values.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>performing a coordinate transformation of the received reference image and the received current image to provide a reoriented reference image and a reoriented current image, respectively, such that eyelid movements are vertical in the reoriented images; and</claim-text><claim-text>determining the difference between the reoriented reference image and the reoriented current image to define the differential image.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>performing one or more image enhancement operations on the differential image to provide an enhanced differential image; and</claim-text><claim-text>calculating the plurality of row values by combining the differential intensity values in corresponding rows of the enhanced differential image.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable storage medium storing computer-executable instructions that, when executed by a processor, causes the processor to:<claim-text>receive a reference image of an eye;</claim-text><claim-text>receive a current image of the eye;</claim-text><claim-text>determine a difference between the reference image and the current image to define a differential image, wherein the differential image comprises a two dimensional pixel array of pixel locations that are arranged in a plurality of rows and columns, each pixel location having a differential intensity value;</claim-text><claim-text>calculate a plurality of row values by combining the differential intensity values in corresponding rows of the differential image; and</claim-text><claim-text>determine eyelid data based on the plurality of row values.</claim-text></claim-text></claim></claims></us-patent-application>