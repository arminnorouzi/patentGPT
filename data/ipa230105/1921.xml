<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001922A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001922</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17551501</doc-number><date>20211215</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110746176.1</doc-number><date>20210701</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>30</main-group><subgroup>095</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>30</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>30</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>W</subclass><main-group>50</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>60</class><subclass>R</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>30</main-group><subgroup>0956</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>30</main-group><subgroup>18163</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>30</main-group><subgroup>12</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>58</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>50</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>R</subclass><main-group>1</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2420</main-group><subgroup>42</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>R</subclass><main-group>2300</main-group><subgroup>8093</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>60</class><subclass>W</subclass><main-group>2050</main-group><subgroup>146</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">SYSTEM PROVIDING BLIND SPOT SAFETY WARNING TO DRIVER, METHOD, AND VEHICLE WITH SYSTEM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TRIPLE WIN TECHNOLOGY(SHENZHEN) CO.LTD.</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LIN</last-name><first-name>KUO-HUNG</first-name><address><city>New Taipei</city><country>TW</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and method for reducing the risk of road accidents on account of blind spot errors and a vehicle using the system and method includes a visual sensing unit, the visual sensing unit comprising a first camera and a second camera, wherein the first camera looks left and obtains a first image information, the second camera looks to the right and obtains a second image information; a pre-processing unit, the pre-processing unit being coupled with the visual sensing unit, wherein the pre-processing unit processes the first image information and the second image information to generate a single image. An image processing unit generates an obstacle recognition information according to the processed image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="168.91mm" wi="158.75mm" file="US20230001922A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="218.19mm" wi="152.15mm" file="US20230001922A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="205.82mm" wi="160.95mm" file="US20230001922A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="215.73mm" wi="82.13mm" file="US20230001922A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="207.69mm" wi="109.98mm" file="US20230001922A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to Chinese Patent Application No. 202110746176.1 filed on Jul. 1, 2021 in China National Intellectual Property Administration, the contents of which are incorporated by reference herein.</p><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The subject matter herein generally relates to road safety technology field.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">As economy and technology developed, vehicle ownership increases year by year. Nevertheless, there is a great potential hazard to safety in blind spots of vehicles. Currently, vehicles can be equipped with a Lane Departure Warning (LDW) system and a Blind Spot Monitoring (BSM) system to increase visual areas of drivers, which can reduce accidents and burden on drivers. However, blind spots around vehicles may still exist despite of utilization of the LDW and the BSM systems.</p><p id="p-0005" num="0004">Therefore, there is room for improvement within the art.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005">Implementations of the present disclosure will now be described, by way of embodiments, with reference to the attached figures.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of blind spots of a vehicle with an LDW system and a BSM system in prior art.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an embodiment of a vehicle warning system according to the present disclosure.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a method providing vehicle warning in one embodiment according to the present disclosure.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of an embodiment of a vehicle according to the present disclosure according to the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">It will be appreciated that for simplicity and clarity of illustration, where appropriate, reference numerals have been repeated among the different figures to indicate corresponding or analogous elements. Additionally, numerous specific details are set forth in order to provide a thorough understanding of the embodiments described herein. However, it will be understood by those of ordinary skill in the art that the embodiments described herein can be practiced without these specific details. In other instances, methods, procedures, and components have not been described in detail so as not to obscure the related relevant feature being described. The drawings are not necessarily to scale and the proportions of certain parts may be exaggerated to better illustrate details and features. The description is not to be considered as limiting the scope of the embodiments described herein.</p><p id="p-0012" num="0011">Several definitions that apply throughout this disclosure will now be presented.</p><p id="p-0013" num="0012">The term &#x201c;coupled&#x201d; is defined as connected, whether directly or indirectly through intervening components, and is not necessarily limited to physical connections. The connection can be such that the objects are permanently connected or releasably connected. The term &#x201c;including&#x201d; means &#x201c;including, but not necessarily limited to&#x201d;; it specifically indicates open-ended inclusion or membership in a so-described combination, group, series, and the like.</p><p id="p-0014" num="0013">With a development of economy and technology, vehicle ownership increases year by year. Nevertheless, a blind spot of vehicles is a potential hazard. Currently, vehicles can be equipped with a Lane Departure Warning (LDW) system and a Blind Spot Monitoring (BSM) system to increase a visual area of drivers, which reduces accident injuries and driving burden. However, the LDW system and the BSM system still have a blind spot.</p><p id="p-0015" num="0014">For example, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a diagram of blind spots of a vehicle with an LDW system and a BSM system in prior art. Dashed lines show ranges of a visual area of the LDW system and the BSM system, and areas of dashed lines across the direction of travel are in a blind spot of the vehicle. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, vehicles with both the LDW system and the BSM system still have a blind spot. Drivers may be unable to make accurate judgement due to existence of vehicle or other obstacles in the blind spot, which leads to higher safety risks.</p><p id="p-0016" num="0015">Therefore, the present disclosure provides a system, a method and a vehicle for vehicle warning, which detects obstacles in the blind spot of the vehicle and issues alerts.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a diagram of an embodiment of the vehicle warning system <b>100</b>. The vehicle warning system <b>100</b> at least includes a visual sensing unit <b>110</b>, a pre-processing unit <b>120</b>, an image processing unit <b>130</b>, a warning unit <b>140</b>, a speed detection unit <b>150</b>, and a trajectory prediction <b>160</b>.</p><p id="p-0018" num="0017">In this embodiment, the visual sensing unit <b>110</b> include a first camera <b>111</b> and a second camera <b>112</b>. The first camera <b>111</b> is set on a left-hand (according to the direction of driving) A-pillar of the vehicle. The first camera <b>111</b> is configured for obtaining images at the left-hand side of the vehicle. The second camera <b>112</b> sets on a righthand A-pillar of the vehicle. The second camera <b>112</b> is configured for obtaining images on the righthand side of the vehicle.</p><p id="p-0019" num="0018">In this embodiment, the pre-processing unit <b>120</b> couples (e.g. electrically connects) the first camera <b>111</b> and the second camera <b>112</b>. The pre-processing unit <b>120</b> is configured for preprocessing the image information behind the left A-pillar and from behind the right A-pillar into an image that can be recognized by a machine vision algorithm, which allows the image processing unit <b>130</b> to recognize and process the pre-processed image information.</p><p id="p-0020" num="0019">In this embodiment, the image processing unit <b>130</b> is coupled to the pre-processing unit <b>120</b>. The image processing unit <b>130</b> is configured for generating an obstacle recognition information according to the machine vision algorithm. The obstacle recognition information includes, but is not limited to, an obstacle type, and, if the obstacle is in motion, obstacle trajectory, and an obstacle relative speed. For example, in one embodiment, the image processing unit <b>130</b> generates the obstacle type according to the machine vision algorithm. The type of obstacle can include a vehicle, pedestrian, bicycle, motorbike, electric motorbike, and others.</p><p id="p-0021" num="0020">In this embodiment, after the obstacle type is identified, the image processing unit <b>130</b> is further configured to locate the obstacle according to the obstacle type and a wheel detection algorithm. For example, if the detected obstacle type is a wheeled type of obstacle (e.g., vehicle, bicycle, motorcycle, hand cart), the obstacle can be located according to the wheel detection algorithm.</p><p id="p-0022" num="0021">In one embodiment, if the obstacle type is vehicle, the image processing unit <b>130</b> is further configured for identifying whether the obstacle includes windows according to a window detection algorithm and locates the vehicle according to a location of the windows.</p><p id="p-0023" num="0022">In this embodiment, when the image processing unit <b>130</b> detects the obstacle type, the image processing unit <b>130</b> is further configured for detecting whether the type of obstacle is a vehicle according to a detection of wheels. For example, the image processing unit <b>130</b> is further configured for detecting the received image information from the visual sensing unit <b>110</b> using a circular or elliptical detection algorithm to determine whether the detected obstacle is a vehicle. Since a wheel has an elliptical or circular appearance as the vehicle traverses the scene, then the obstacle is determined as being a wheeled vehicle through the circular or elliptical detection algorithm.</p><p id="p-0024" num="0023">In other embodiments, a wheel of a wheeled obstacle or vehicle is not limited to being detected by using the circular or elliptical detection algorithm, and may be detected by a Hough transform algorithm or other algorithms or methods. For example, the vehicle may be detected by one or more of detection of a tire, of a wheel rim detection, of spokes, and/or wheel hub detection.</p><p id="p-0025" num="0024">As described above, when the type of the obstacle is determined to be a vehicle, the image processing unit <b>130</b> is further configured to determine whether the obstacle includes a window according to a window detection algorithm and locate the vehicle according to the position of the window.</p><p id="p-0026" num="0025">For example, the window detection can be performed using a color difference or a straight-line effect. In other embodiments, the image processing unit <b>130</b> is not limited to performing window detection by using the color difference or the straight special effect and may also perform window detection by using other detection methods, not being limited in this disclosure.</p><p id="p-0027" num="0026">The speed detection unit <b>150</b> is coupled to the visual sensing unit <b>110</b>. The speed detection unit <b>150</b> is configured for receiving image from the visual sensing unit <b>110</b>. The speed detection unit <b>150</b> performs speed detection according to the image from the visual sensing unit <b>110</b> and a high-speed vision algorithm, to obtain a relative speed between the obstacle and the vehicle. In other embodiments, the speed detection unit <b>150</b> can also be connected to a radar, an infrared distance meter, etc. Then, the speed detection unit <b>150</b> can calculate the relative speed according to the relative displacement and time between the vehicle and the obstacle.</p><p id="p-0028" num="0027">In this embodiment, the trajectory prediction <b>160</b> is coupled to the speed detection unit <b>150</b>. The trajectory prediction <b>160</b> is configured for predicting the trajectory of the obstacle according to the relative speed detected by the speed detection unit <b>150</b>.</p><p id="p-0029" num="0028">In one embodiment, the trajectory prediction unit <b>160</b> can be further coupled to the first camera <b>111</b> and the second camera <b>112</b>. The trajectory prediction <b>160</b> is configured for performing prediction of obstacle trajectory according to the image information collected by the first camera <b>111</b> and the second camera <b>112</b> and the relative speed from the speed detection unit <b>150</b>.</p><p id="p-0030" num="0029">In other embodiments, the trajectory prediction unit <b>160</b> can be connected to other information collection devices of the vehicle to perform the obstacle trajectory predictions. For example, the trajectory prediction unit <b>160</b> acquires a distance between an obstacle and the driven vehicle from a radar mounted on the driven vehicle, and calculate a trajectory between the obstacle and the driven vehicle from two distances to the obstacle as measured by the vehicle-mounted radar and positions thereof.</p><p id="p-0031" num="0030">In one embodiment, the image processing unit <b>130</b> is also coupled with the trajectory prediction unit <b>160</b>. The image processing unit <b>130</b> is configured for receiving the predicted trajectory of the obstacle transmitted by the trajectory prediction unit <b>160</b> and determining whether a risk of traffic accident exists according to the trajectory and a relative speed of the obstacle. If the image processing unit <b>130</b> detects a risk of traffic accident according to the trajectory and the relative speed of the obstacle, the image processing unit <b>130</b> further controls the warning unit <b>140</b> to generate an alert.</p><p id="p-0032" num="0031">In some embodiment, the alert notification includes sound and light warning, displaying alert notification on a center console, steering wheel vibration, and the like, and the disclosure is not limited herein.</p><p id="p-0033" num="0032">In one embodiment, the image processing unit <b>130</b> is further configured for classifying the level of risk associated with the alert notification. For example, when a risk level is low, the image processing unit <b>130</b> controls the warning unit <b>140</b> to perform warning by a light. When the risk level is medium, the image processing unit <b>130</b> controls the warning unit <b>140</b> to perform warning audibly. When the risk level is high, the image processing unit <b>130</b> controls the warning unit <b>140</b> to perform warning with sound and with steering wheel vibration, which will guarantee the driver receiving the alert notification, for him or her to take action.</p><p id="p-0034" num="0033">In one embodiment, the image processing unit <b>130</b> is further configured to control the warning unit <b>140</b> to perform the alert notification, after receiving the obstacle trajectory prediction information transmitted by the trajectory prediction unit <b>160</b>. The vehicle may be about to turn or cross to another lane when the obstacle is determined to be present in the blind spot. For example, when the image processing unit <b>130</b> obtains from the trajectory prediction unit <b>160</b> that there is a vehicle in the blind spot on the left-hand side of the vehicle and the vehicle wants to turn left, the image processing unit <b>130</b> may control the warning unit <b>140</b> to issue a warning, such as the sound warning or the steering wheel vibration.</p><p id="p-0035" num="0034">In one embodiment, the warning unit <b>140</b> can include a loudspeaker, a screen, or warning light etc. The warning unit <b>140</b> is couple to the image processing unit <b>130</b>. The warning unit <b>140</b> is configured for displaying the alert notification after receiving the obstacle recognition information from the image processing unit <b>130</b>. For example, in one embodiment, the warning unit <b>140</b> can be mounted on a left-hand or righthand rearview mirror of the vehicle. Therefore, after detecting an obstacle in the left blind spot of the vehicle, the image processing unit <b>130</b> can control the warning unit <b>140</b> to display alert notification in the left-hand rearview mirror.</p><p id="p-0036" num="0035">In one embodiment, the warning unit <b>140</b> can set in the center console or inside the A-pillar of the vehicle. The warning unit <b>140</b> shows alert notification in the center console or inside the A-pillar after the image processing unit <b>130</b> detects obstacle. For example, if the image processing unit <b>130</b> detects obstacle in the left blind spot, the warning unit <b>140</b> shows alert notification in the left A-pillar of the vehicle.</p><p id="p-0037" num="0036">In one embodiment, the vehicle warning system <b>100</b> can be combined with the LDW system and the BSM system. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the LDW system is configured to detect obstacle in the front of the vehicle, the vehicle warning system <b>100</b> is configured to detect obstacle in the side of the vehicle, and the BSM system is configured to detect obstacle behind the vehicle. A combination of the three systems achieves omni-directional monitoring of the vehicle, acts to eliminate the dangers of blind spot of vision, and improves the safety factor of the vehicle when running.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a flowchart of an embodiment of the vehicle warning method. The embodiment is provided by way of example, as there are a variety of ways to carry out the method. The method described below can be carried out using the configurations illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, for example, and various elements of these figures are referenced in explaining the embodiment. The method including: obtaining a first image information and a second image information from the first camera <b>111</b> and the second camera <b>112</b> and generating an alert information according to the first image information and the second image information. Each block shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> represents one or more processes, methods, or subroutines carried out in the embodiment. Furthermore, the illustrated order of blocks is by example only, and the order of the blocks can be changed. Additional blocks can be added or fewer blocks can be utilized, without departing from this disclosure. This method can begin at block S<b>100</b>.</p><p id="p-0039" num="0038">At block S<b>100</b>, a first image information and a second image information are obtained.</p><p id="p-0040" num="0039">In block S<b>100</b>, the vehicle warning system <b>100</b> can obtain the first image information from the first camera <b>111</b> and obtains the second image information from the second camera <b>112</b>.</p><p id="p-0041" num="0040">At block S<b>200</b>, the first image information and the second image information are pre-processed to generate an image pre-process information.</p><p id="p-0042" num="0041">At block S<b>200</b>, for example, the information formats of the first image information and the second image information may be converted into image pre-processing information that can be recognized by a machine vision algorithm through the pre-processing unit <b>120</b>, so that the image processing unit <b>130</b> can recognize and process the image pre-processing information.</p><p id="p-0043" num="0042">At block S<b>300</b>, the image processing unit <b>130</b> performs obstacle classification according to the image preprocessing information and the machine vision algorithm.</p><p id="p-0044" num="0043">At block S<b>400</b>, the image processing unit <b>130</b> determines whether it is necessary to generate the alert notification through the warning unit <b>140</b> according to the recognition result of the obstacle. If it is necessary to generate the alert notification, the image processing unit <b>130</b> controls the warning unit <b>140</b> to generate the alert notification.</p><p id="p-0045" num="0044">In an embodiment of the present disclosure, the method may further include performing a speed detection according to a high-speed vision algorithm and the first image information or the second image information to obtain a relative speed between the obstacle and the car. Specifically, the relative speed between the obstacle and the vehicle can be obtained by coupling the speed detection unit <b>150</b> to the visual sensing unit <b>110</b>, and performing speed detection according to the first image information or the second image information through the speed detection unit <b>150</b>.</p><p id="p-0046" num="0045">In an embodiment of the present disclosure, the method may further include predicting a trajectory of the obstacle according to the relative speed. Specifically, the prediction of the trajectory between the obstacle and the vehicle may be obtained by the trajectory prediction unit <b>160</b>.</p><p id="p-0047" num="0046">In an embodiment of the present disclosure, the method may further include generating alert notification according to a trajectory prediction between the obstacle and the car. Specifically, the image processing unit <b>130</b> is coupled to the trajectory prediction unit <b>160</b> and the warning unit <b>140</b>. The image processing unit <b>130</b> acquires trajectory prediction information from the trajectory prediction unit <b>160</b>, determines whether there exists a collision risk, and controls the warning unit <b>140</b> to generate alert notification if there is a collision risk. It is understood that the image processing unit <b>130</b> may be a chip. For example, the image processing unit <b>130</b> may be a Field Programmable Gate Array (FPGA), an Application Specific Integrated Circuit (ASIC), a system on chip (SoC), a Central Processor Unit (CPU), a Network Processor (NP), a Digital Signal Processor (DSP), a Microcontroller (MCU), a Programmable Logic Device (PLD) or other integrated chips.</p><p id="p-0048" num="0047">It will be appreciated that the steps of the above method may be performed by instructions in the form of hardware integrated logic circuits or software module in the image processing unit <b>130</b>. The steps of the method disclosed in connection with the embodiments of the present disclosure may be directly implemented by a hardware processor, or implemented by a combination of hardware and software modules in the image processing unit <b>130</b>. The software modules may be stored in ram, flash, rom, prom, or eprom, registers, etc. as is well known in the prior art.</p><p id="p-0049" num="0048">In one embodiment, the image processing unit <b>130</b> in the embodiment of the present disclosure may be an integrated circuit chip having signal processing capability. In implementation, the steps of the above method embodiments may be performed by integrated logic circuits of hardware in a processor or by instructions in the form of software. The processor described above may be a general purpose processor, a Digital Signal Processor (DSP), an Application Specific Integrated Circuit (ASIC), a Field Programmable Gate Array (FPGA) or other programmable logic device, discrete gate or transistor logic, discrete hardware components. The various methods, steps, and logic blocks disclosed in the embodiments of the present disclosure may be implemented or performed. A general purpose processor may be a microprocessor or the processor may be any conventional processor or the like. The steps of the method disclosed in connection with the embodiments of the present disclosure may be directly implemented by a hardware decoding processor, or implemented by a combination of hardware and software modules in the decoding processor. The software modules may be stored in ram, flash, rom, prom, or eprom, registers, etc. as is well known in the art. The storage medium is located in a memory, and a processor reads information in the memory and combines hardware thereof to complete the steps of the method.</p><p id="p-0050" num="0049">In one embodiment, the first camera <b>111</b> and the second camera <b>112</b> in the visual sensing unit <b>110</b> are used for collecting the vision image of the blind spot of the vehicle. The working principle of the first camera <b>111</b> and the second camera <b>112</b> is to collect images through a lens, and then the collected images are processed by an internal photosensitive assembly and a control assembly and further converted into digital signals which can be recognized by other systems; other systems obtain digital signals through the transmission ports of the first camera <b>111</b> and the second camera <b>112</b>, and then perform image restoration to obtain an image consistent with an actual scene. In practical application, the visual field range of the image data collected by the camera and the installation amount and the installation position of the camera can be further designed into a feasible scheme according to actual needs. The embodiment of the application does not specifically limit the visual field range, the installation amount and the installation position of the cameras. It is understood that the types of the first camera <b>111</b> and the second camera <b>112</b> can be selected according to different requirements of users, as long as basic functions of video shooting, broadcasting, still image capturing, and the like can be realized. For example, the camera may be one or more types of commonly used vehicle-mounted cameras, such as a binocular camera and a monocular camera.</p><p id="p-0051" num="0050">In one embodiment, the first camera <b>111</b> and the second camera <b>112</b> may be one or two types of digital cameras and analog cameras if selected according to the signal category, and the difference is that the image processing process for the lens collection is different. The digital camera converts the collected analog signals into digital signals for storage, and the analog camera converts the analog signals into a digital mode by using a specific video capture card, compresses the analog signals and stores the compressed analog signals. If the cameras are classified according to the image sensor category in the cameras, the cameras can also be one or both of a Complementary Metal Oxide Semiconductor (CMOS) type camera and a charge-coupled device (CCD) type camera.</p><p id="p-0052" num="0051">In one embodiment, the first camera <b>111</b> and the second camera <b>112</b> may also be one or more types of Serial ports, parallel ports, Universal Serial Bus (USB), and firewire interface (IEEE1394) if divided by interface type. The embodiment of the present disclosure also does not specifically limit the type of the camera.</p><p id="p-0053" num="0052">An embodiment of the present disclosure further provides a computer readable storage medium having stored there on a computer program which, when executed by a processor, implements the vehicle warning method as described above.</p><p id="p-0054" num="0053">The readable medium may be a readable signal medium or a readable storage medium. A readable storage medium may be, for example, but not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any combination of the foregoing. More specific examples (a non-exhaustive list) of the readable storage medium include: an electrical connection having one or more wires, a portable diskette, a hard disk, a Random Access Memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or flash memory), an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a diagram of an embodiment of a vehicle <b>10</b>. The vehicle <b>10</b> includes a vehicle main body <b>200</b> and the vehicle warning system <b>100</b>.</p><p id="p-0056" num="0055">An embodiment of the present disclosure provides the vehicle <b>10</b> including the vehicle warning system <b>100</b> as described above, or the computer readable storage medium as described above.</p><p id="p-0057" num="0056">In an embodiment of the present disclosure, the vehicle <b>10</b> includes any vehicles such as cars trucks and buses, and vehicles such as two and three wheelers are also included.</p><p id="p-0058" num="0057">Even though numerous characteristics and advantages of the present technology have been set forth in the foregoing description, together with details of the structure and function of the present disclosure, the disclosure is illustrative only, and changes may be made in the detail, especially in matters of shape, size, and arrangement of the parts within the principles of the present disclosure, up to and including the full extent established by the broad general meaning of the terms used in the claims. It will therefore be appreciated that the exemplary embodiments described above may be modified within the scope of the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A vehicle warning system, applicable in vehicles, the vehicle warning system comprising:<claim-text>a visual sensing unit comprising a first camera and a second camera, wherein the first camera is located on a left A-pillar of a vehicle and is configured for obtaining a first image information, the second camera is located on a right A-pillar of the vehicle and is configured for obtaining a second image information;</claim-text><claim-text>a pre-processing unit coupled with the visual sensing unit, wherein the pre-processing unit is configured for pre-processing the first image information and the second image information to generate an image pre-processing information; and</claim-text><claim-text>an image processing unit configured for generating an obstacle recognition information according to the image pre-processing information.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The vehicle warning system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>a warning unit coupled with the image processing unit and configured for generating an alert information according to the obstacle recognition information.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The vehicle warning system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image processing unit generates the obstacle recognition information according to a machine vision algorithm.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The vehicle warning system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the obstacle recognition information comprising an obstacle, an obstacle type, an obstacle trajectory, and an obstacle relative speed.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The vehicle warning system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein an obstacle type comprising at least one of a vehicle, pedestrian, bicycle, motorbike, and battery motorbike.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The vehicle warning system of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>a speed detection unit coupled with the visual sensing unit and configured for calculating the obstacle relative speed between the obstacle and the vehicle according to the first image information and the second image information.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The vehicle warning system of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>a trajectory prediction unit coupled with each of the speed detection unit and the image processing unit, and configured for performing an obstacle trajectory prediction according to the obstacle trajectory, the obstacle relative speed, the first image information, and the second image information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The vehicle warning system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the image processing unit is further configured for generating the alert information according to the obstacle trajectory and the relative speed between the obstacle and the vehicle.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A vehicle warning method comprising:<claim-text>obtaining a first image information and a second image information;</claim-text><claim-text>pre-processing the first image information and the second image information to generate an image pre-processing information; and</claim-text><claim-text>generating an obstacle recognition information according to the image pre-processing information; and</claim-text><claim-text>generating an alert information according to the obstacle recognition information.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The vehicle warning method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the obstacle recognition information comprising an obstacle, an obstacle type, an obstacle trajectory, and an obstacle relative speed.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The vehicle warning method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the method further comprising:<claim-text>calculating the obstacle relative speed between the obstacle and the vehicle according to the first image information and the second image information.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The vehicle warning method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the method further comprising:<claim-text>predicting an obstacle trajectory of the obstacle according to the relative speed.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The vehicle warning method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the method further comprising:<claim-text>generating the alert information according to the obstacle trajectory and the relative speed between the obstacle and the vehicle.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A vehicle comprising:<claim-text>a vehicle main body;</claim-text><claim-text>a visual sensing unit comprising a first camera and a second camera, wherein the first camera is located on a left A-pillar of a vehicle and is configured for obtaining a first image information, the second camera is located on a right A-pillar of the vehicle and is configured for obtaining a second image information; and</claim-text><claim-text>a pre-processing unit coupled with the visual sensing unit, wherein the pre-processing unit is configured for pre-processing the first image information and the second image information to generate an image pre-processing information; and</claim-text><claim-text>an image processing unit configured for generating an obstacle recognition information according to the image pre-processing information.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The vehicle of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the vehicle further comprising:<claim-text>a warning unit coupled with the image processing unit, is configured for generating an alert information according to the obstacle recognition information.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The vehicle of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the obstacle recognition information comprising an obstacle, an obstacle type, an obstacle trajectory, and an obstacle relative speed.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The vehicle of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the obstacle type comprising a vehicle, pedestrian, bicycle, motorbike, battery motorbike, and other type of obstacle.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The vehicle of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the vehicle further comprising:<claim-text>a speed detection unit coupled with the visual sensing unit, the speed detection unit is configured for calculating the obstacle relative speed between the obstacle and the vehicle according to the first image information and the second image information.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The vehicle of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the vehicle further comprising:<claim-text>a trajectory prediction unit coupled with the speed detection unit and the image processing unit, the trajectory prediction unit is configured for performing an obstacle trajectory prediction according to the obstacle trajectory, obstacle relative speed, the first image information, and the second image information.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The vehicle of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein the image processing unit is further configured for generating the alert information according to the obstacle trajectory and the relative speed between the obstacle and the vehicle.</claim-text></claim></claims></us-patent-application>