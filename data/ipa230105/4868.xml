<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004869A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004869</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17929394</doc-number><date>20220902</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2264</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>285</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MACHINE-LEARNING TECHNIQUES FOR EVALUATING SUITABILITY OF CANDIDATE DATASETS FOR TARGET APPLICATIONS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16274954</doc-number><date>20190213</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11481668</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17929394</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Adobe Inc.</orgname><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MODARRESI</last-name><first-name>Kourosh</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>YUAN</last-name><first-name>Hongyuan</first-name><address><city>San Jose</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>MENGUY</last-name><first-name>Charles</first-name><address><city>New York</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques disclosed herein relate generally to evaluating and selecting candidate datasets for use by software applications, such as selecting candidate datasets for training machine-learning models used in software applications. Various machine-learning and other data science techniques are used to identify unique entities in a candidate dataset that are likely to be part of target entities for a software application. A merit attribute is then determined for the candidate dataset based on the number of unique entities that are likely to be part of the target entities, and weights associated with these unique entities. The merit attribute is used to identify the most efficient or most cost-effective candidate dataset for the software application.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="85.01mm" wi="158.75mm" file="US20230004869A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="231.14mm" wi="141.73mm" orientation="landscape" file="US20230004869A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="194.23mm" wi="159.51mm" orientation="landscape" file="US20230004869A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="157.56mm" wi="127.76mm" orientation="landscape" file="US20230004869A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="207.09mm" wi="162.64mm" file="US20230004869A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="220.30mm" wi="134.11mm" orientation="landscape" file="US20230004869A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="200.24mm" wi="144.36mm" orientation="landscape" file="US20230004869A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="199.47mm" wi="148.93mm" orientation="landscape" file="US20230004869A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="200.74mm" wi="145.12mm" orientation="landscape" file="US20230004869A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="200.32mm" wi="146.39mm" orientation="landscape" file="US20230004869A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="179.15mm" wi="153.16mm" orientation="landscape" file="US20230004869A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="128.35mm" wi="150.96mm" orientation="landscape" file="US20230004869A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a Continuation of U.S. application Ser. No. 16/274,954, filed Feb. 13, 2019, the content of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This disclosure relates generally to artificial intelligence, machine learning, and other data science technologies. More specifically, but not by way of limitation, this disclosure relates to applying machine-learning techniques to determine a merit attribute of a candidate dataset for a particular software application and, in some cases, thereby facilitating selection of more suitable datasets for the given software application.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Artificial intelligence and machine-learning techniques can be used to extract knowledge and insights from data in various forms in order to understand and analyze actual phenomena with data. For example, a neural network or other machine-learning model can be trained, using one or more datasets, to extract useful features from similar input data or to make predictions based on the input data. In many cases, the quality or performance (e.g., accuracy and sensitivity) of the machine-learning model or neural network depends on the quantity and quality of the datasets used for the training. Thus, one way to improve the performance of a machine-learning model is to improve the quantity and/or quality of the training datasets.</p><p id="p-0005" num="0004">However, in many circumstances, training datasets and/or other datasets used for artificial intelligence and machine-learning techniques are expensive (in, for example, time, efforts, resources, or price) to collect, accumulate, purchase, or otherwise obtain. In addition, a training process can take a much longer time period (such as days, weeks, or even months or longer in some cases) compared with the inference process. Furthermore, different datasets, even if including similar numbers of entries, may add different values or improvements to existing datasets and the trained model. Thus, there are cases where a dataset is obtained at a high cost and/or is used to train a machine-learning model in a long training process, but adds little or no additional value or improvement to existing datasets and the trained model.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the present disclosure involve assessment and selection of candidate datasets for a customer application based on machine-learning techniques. According to certain embodiments, a method for applying machine-learning techniques to evaluate candidate datasets for use by software applications includes receiving (i) a reference dataset identifying first entities associated with first features that include a baseline feature of a target population and (ii) a candidate dataset identifying second entities associated with second features; and identifying, in the candidate dataset, first unique candidate entities that are absent from the reference dataset and that are associated with the baseline feature in the candidate dataset. The method also includes forming a cluster of data points representing the first entities in a multi-dimensional space and based on a subset of the first features lacking the baseline feature; mapping a subset of the second entities that are absent from the reference dataset and that are not in the first unique candidate entities to additional data points, respectively in the multi-dimensional space; and identifying, from the subset of the second entities, second unique candidate entities corresponding to a subset of the additional data points within a threshold distance of the cluster. The method further includes determining a merit attribute of the candidate dataset based on a first weight for each first unique candidate entity, a second weight for each second unique candidate entity, a number of the first unique candidate entities in the candidate dataset, and a number of the second unique candidate entities in the candidate dataset; and selecting the candidate dataset as input data for a target software application based on the merit attribute of the candidate dataset being greater than a threshold value.</p><p id="p-0007" num="0006">These illustrative examples are mentioned not to limit or define the disclosure, but to provide examples to aid understanding thereof. Additional embodiments and examples are discussed in the Detailed Description, and further description is provided there.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE FIGURES</heading><p id="p-0008" num="0007">Features, embodiments, and advantages of the present disclosure are better understood when the following Detailed Description is read with reference to the accompanying drawings.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a system for evaluating the suitability of candidate datasets for target applications according to certain embodiments.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow chart illustrating an example of a method for applying machine-learning techniques to evaluate candidate datasets for use by software applications according to certain embodiments.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of an aggregated matrix in which the features in the reference dataset and the features in the candidate dataset do not have any overlap.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart illustrating an example of a method for identifying unique candidate entities in a candidate dataset that are not described as associated with a baseline feature but are likely associated with the baseline feature according to certain embodiments.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of an aggregated matrix that includes entities from a reference dataset and entities from a candidate dataset.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a chart illustrating examples of simulated results based on Euclidean distance and using techniques disclosed herein according to certain embodiments.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a chart illustrating examples of simulated results based on Euclidean distance and using techniques disclosed herein according to certain embodiments.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a chart illustrating examples of simulated results based on Pearson correlation distance and using techniques disclosed herein according to certain embodiments.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a chart illustrating examples of simulated results based on Pearson correlation distance and using techniques disclosed herein according to certain embodiments.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an example of a diagram illustrating the increase in the number of entities predicted to be part of the target population when the threshold is doubled, based on results of simulation using techniques disclosed in certain embodiments.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates an example of a computing system for implementing some of the embodiments disclosed herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">Techniques disclosed herein relate generally to assessment and selection of candidate datasets for a customer application based on artificial intelligence, machine learning, and other data science technologies. The assessment and selection of candidate datasets include, for example, selecting, from available candidate datasets, one or more datasets that can improve the performance of a machine-learning model the most when used as training data for training the machine-learning model, or one or more datasets that can add the most information to existing datasets. In certain embodiments, various machine-learning and other data science techniques (e.g., clustering and spectral regularization techniques) are used to identify entities in the candidate dataset that are unique and are likely to be part of target entities (e.g., entities with certain features) for the customer application. A merit attribute (e.g., efficiency, cost-effectiveness, potential value of the candidate dataset, or any combination) is then determined based on the number of unique entities that are likely to be part of the target entities for the customer application, and weights associated with these unique entities and determined using, for example, regression techniques. The merit attribute can facilitate dataset expansion by, for example, identifying the most efficient or most cost-effective candidate dataset to be added to existing datasets for the customer application.</p><p id="p-0021" num="0020">The following non-limiting example is used to introduce certain embodiments. In this example, a computing system having one or more processors implements various machine-learning techniques to evaluate candidate datasets for use by software applications. The computing system receives a reference dataset and a candidate dataset, where the reference dataset identifies a first group of entities (e.g., users of a software application or patrons of a merchant) and a first set of features associated with the first group of entities (e.g., gender, age, marriage status, occupation, income, and the like). The first group of entities includes at least one baseline feature, such as age or income, for identifying target entities. The candidate dataset identifies a second group of entities and a second set of features associated with the second group of entities. The second set of features may not include the baseline feature, or may include the baseline feature but lack the values of the baseline feature for at least some entities in the candidate dataset.</p><p id="p-0022" num="0021">Continuing with this example, the computing system identifies, from the candidate dataset, first unique candidate entities that are absent from the reference dataset and that are described in the candidate dataset as associated with the baseline feature. For example, a reference dataset may identify users of a first software application (e.g., a relatively new software application), and a candidate dataset may identify users of a second software application (e.g., a more popular software application), where both datasets may include baseline feature (e.g., age and education information) of at least some users. Some users of the second software application may not be the users of the first software application yet, and the candidate dataset may include the age and education information of at least some of these users, which may be identified as the first unique candidate entities. The computing system also determines that second unique candidate entities in the candidate dataset, which are absent from the reference dataset, are likely associated with the baseline feature even though the candidate dataset does not describe the baseline feature for these second unique candidate entities. For example, in the example described above, the candidate dataset may not include the age and/or education information of some users, but at least a portion of these users are likely to have the baseline feature and thus may be identified as the second unique candidate entities based on other features of these users. To determine that the second unique candidate entities are associated with the baseline feature, the computing system can use machine-learning techniques to form a cluster of the first group of entities in the reference dataset and predict whether an entity in the candidate dataset is part of the target population. For example, the cluster can be formed based on features of the first group of entities other than the baseline feature. The computing system determines that an entity is one of the second unique candidate entities if a distance between the entity and the cluster is less than a threshold distance. The computing system can then determine a merit attribute of the candidate dataset based on weights for the first and second unique candidate entities, a number of first unique candidate entities in the candidate dataset, and a number of second unique candidate entities in the candidate dataset. The weights can be determined using regression techniques, and can represent, for example, the relative confidence level that each of the first or second unique candidate entities may be part of the target population. If the computed merit attribute has a sufficiently high value, the computing system selects the candidate dataset as input data for a target software application (e.g., as a training dataset for a machine-learning model, or for identifying target population).</p><p id="p-0023" num="0022">In one specific implementation, the computing system performs one or more matrix operations for computing the merit attribute and selecting the candidate dataset. The computing system accesses or generates a matrix that includes entities and certain features associated with the entities includes information in both the reference dataset and the candidate dataset. The matrix is normalized (e.g., using spectral regularization techniques) to expand the features for the reference dataset and the candidate dataset. The normalization can include adding missing features for the entities in the candidate dataset and/or the reference dataset. The computing system identifies, from the matrix, unique entities in the candidate dataset that are absent in the reference dataset. Based on information in the normalized matrix, the computing system identifies, from unique entities for which the baseline features are not specified or labeled, second unique candidate entities that are likely to be part of the target population for the customer application using, for example, the cluster-based techniques described above based on various distance metrics.</p><p id="p-0024" num="0023">Techniques disclosed herein use a combination of specific machine-learning techniques and/or rules to evaluate the efficiency, cost-effectiveness, value, and/or other merit attributes of a candidate dataset for a specific application in view of existing datasets. This evaluation can be used to determine whether the candidate dataset is suitable for the specific application and whether the benefits or values of using the candidate dataset is commensurate with the expenditure of resources (e.g., time, effort, or price) to obtain or use the candidate dataset. Thus, techniques disclosed herein can, for example, improve the efficiency and reduce the training time for training a machine-learning model by, for example, selecting training datasets that are most useful (e.g., including the most new entities or other new information) or cost-effective (e.g., at a reasonable cost) for training a modeling algorithm to perform a task with the desired accuracy or precision. Thus, the techniques can minimize the cost and time and thus improve the efficiency for achieving a desired result for the target application.</p><p id="p-0025" num="0024">As used herein, the term &#x201c;target population&#x201d; refers to a group of entities that are the target or potential users or patrons of a software application, a service, or a product.</p><p id="p-0026" num="0025">As used herein, the term &#x201c;traits&#x201d; or &#x201c; features&#x201d; refers to one or more features associated with an entity or an entry in a dataset. Examples of the traits include the identity, personal information, preferences, personality, hobbies, interests, behavioral traits, or capabilities of the entity.</p><p id="p-0027" num="0026">As used herein, the term &#x201c;baseline traits&#x201d; or &#x201c;baseline features&#x201d; refers to one or more traits that define the target population. For example, trait &#x201c;shopped at Merchant A&#x201d; and trait &#x201c;male&#x201d; could be a set of baseline used by Merchant A to expand its target population on the basis of these traits. In some embodiments, the baseline traits are specified or defined by a customer or user of a dataset (e.g., Merchant A in the above example).</p><p id="p-0028" num="0027">As used herein, the term &#x201c;customer dataset&#x201d; or &#x201c;reference dataset&#x201d; refers to, for example, the original dataset owned by a customer or user of datasets, which may include information regarding known target population. The customer dataset can be used as the benchmark to evaluate the merit attribute of candidate datasets (e.g., third-party datasets).</p><p id="p-0029" num="0028">As used herein, the term &#x201c;candidate dataset&#x201d; refers to datasets that may potentially be used by a customer for a specific customer application to achieve a goal. The candidate datasets can be provided by, for example, third-party data providers.</p><p id="p-0030" num="0029">As used herein, the term &#x201c;merit attribute&#x201d; refers to the efficacy, efficiency, cost-effectiveness, or potential value of a candidate dataset, or any combination. In some embodiments, the merit attribute may be a measure of both the technical value and the economic value of the candidate dataset.</p><p id="p-0031" num="0030">As used herein, the term &#x201c;neural network&#x201d; refers to one or more computer-implemented, network-based models capable of being trained to achieve a goal. Unless otherwise indicated, references herein to a neural network include one neural network or multiple interrelated neural networks that are trained together. Examples of neural networks include, without limitation, convolutional neural networks (CNNs), recurrent neural networks (RNNs), fully-connected neural networks, dense-connection neural networks, feed-forward neural networks, and other types of neural networks. In some embodiments, a neural network can be implemented using special hardware (e.g., GPU, tensor processing units (TPUs), or processing element arrays (PE arrays)), using software code and a general purpose processor, or a combination of special hardware and software code.</p><p id="p-0032" num="0031">The following examples are provided to introduce certain embodiments. In the following description, for the purposes of explanation, specific details are set forth in order to provide a thorough understanding of examples of the disclosure. However, it will be apparent that various examples may be practiced without these specific details. For example, devices, systems, structures, assemblies, methods, and other components may be shown as components in block diagram form in order not to obscure the examples in unnecessary detail. In other instances, well-known devices, processes, systems, structures, and techniques may be shown without necessary detail in order to avoid obscuring the examples. The figures and description are not intended to be restrictive. The terms and expressions that have been employed in this disclosure are used as terms of description and not of limitation, and there is no intention in the use of such terms and expressions of excluding any equivalents of the features shown and described or portions thereof. The word &#x201c;example&#x201d; is used herein to mean &#x201c;serving as an example, instance, or illustration.&#x201d; Any embodiment or design described herein as an &#x201c;example&#x201d; is not necessarily to be construed as preferred or advantageous over other embodiments or designs.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example of a system <b>100</b> for evaluating the suitability of candidate datasets for target applications according to certain embodiments. As illustrated, system <b>100</b> includes a data storage module <b>110</b> for storing one or more reference datasets and one or more candidate datasets. In some embodiments, the datasets are stored as matrices in data storage module <b>110</b>. System <b>100</b> also includes a first entities identification engine <b>120</b> that identifies unique entities in a candidate dataset that are described in the candidate dataset as having baseline features of target population. System <b>100</b> further includes a second entities identification engine <b>130</b> that identifies unique entities in the candidate dataset that are not described in the candidate dataset as having the baseline features of target population but are predicted to be part of the target population. System <b>100</b> includes a merit attribute determination module <b>140</b> that determines the merit attribute of the candidate dataset based on the unique entities identified by first entities identification engine <b>120</b> and second entities identification engine <b>130</b>. A dataset selection module <b>160</b> then selects, from the received one or more candidate datasets, one or more candidate datasets having merit attributes meeting a certain criterion. In some embodiments, system <b>100</b> also includes a weight determination engine <b>150</b>, which determines weights associated with the unique entities identified by first entities identification engine <b>120</b> and second entities identification engine <b>130</b>. For example, weight determination engine <b>150</b> can implement a regression technique to determine appropriate weights for the identified unique entities using some training datasets with known merit attributes, as described in detail below.</p><p id="p-0034" num="0033">In some embodiments, second entities identification engine <b>130</b> includes a dataset aggregation module <b>132</b> that combines a candidate dataset with a reference dataset, such as representing the candidate dataset and the reference dataset using an aggregated matrix as described in detail below. Second entities identification engine <b>130</b> also includes a clustering engine <b>136</b> that maps at least some entities in the aggregated matrix to data points in a multi-dimensional space. A threshold distance determination engine <b>138</b> determines a threshold distance for identifying unique entities that are likely to be in the target population. For example, threshold distance determination engine <b>138</b> can determine the threshold distance based on the average distance between data points representing entities in the reference dataset, such as the average distance between each data point representing a respective entity in the reference dataset and a centroid of a cluster of the data points representing the entities in the reference dataset. A second entities selection engine <b>142</b> then selects a subset of entities in the candidate dataset based on the threshold distance and the distance between each data point representing a respective entity in the candidate dataset and the centroid of the cluster of the data points representing the entities in the reference dataset. In some embodiments, second entities identification engine <b>130</b> also includes a dataset normalization engine <b>134</b> configured to complete the aggregated matrix (e.g., determining missing values in a sparse matrix) before mapping the entities in the aggregated matrix to the multi-dimensional space. The operations of second entities identification engine <b>130</b> are described in detail below.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flow chart <b>200</b> illustrating an example of a method for applying machine-learning techniques to evaluate candidate datasets for use by software applications according to certain embodiments. The operations in flow chart <b>200</b> can be performed by, for example, system <b>100</b> described above. The operations in flow chart <b>200</b> identify unique entities in a candidate dataset (e.g., a third-party dataset) that are described as sharing the same baseline trait(s) with entities in existing customer dataset (also referred to as reference dataset). The operations then detect other unique entities in the candidate dataset that are not described as having the baseline trait(s) but are likely to be part of the customer's target population, based on, for example, the similarity or &#x201c;distance&#x201d; of each unique entity to a cluster of entities that are known to be in the target population of the customer (e.g., entities in the customer dataset). The operations then determine a merit attribute based on the unique entities in the candidate dataset that have or likely have the baseline trait(s), and use the merit attribute to determine whether to add the candidate dataset to the customer dataset.</p><p id="p-0036" num="0035">At block <b>210</b>, one or more processing devices (e.g., system <b>100</b>, more specifically, data storage module <b>110</b>) receive a reference dataset and a candidate dataset. The reference dataset identifies a first group of entities and a first set of features (also referred to as traits) associated with the first group of entities, where the first set of features includes at least one baseline feature. In one example, the entities in the reference dataset include existing patrons of a merchant. The patrons can be identified by their unique identifications, such as names, user names, cell phone numbers, social security numbers, or other labels the uniquely identifying individual patrons. The first set of features can include, for example, gender, age, marriage status, occupation, income, and the like. In some embodiments, the reference dataset is represented by a two-dimensional data matrix, where each row (which may also be referred to as an entry) corresponds to one patron (i.e., entity), and each column corresponds to an identification or a feature of the patron. In some embodiments, the value in an element of the data matrix may be a numerical value, such as a height value, an age value, a time value, and the like. In some embodiments, the value in an element of the data matrix may be a binary value &#x201c;1&#x201d; or &#x201c;0&#x201d; or a Boolean value &#x201c;true&#x201d; or &#x201c;false,&#x201d; which indicates that the corresponding entity has or does not have the corresponding feature. For example, if one column corresponds to feature &#x201c;Male,&#x201d; a value &#x201c;1&#x201d; in an element in the column indicates that the corresponding entity is a male, while a value &#x201c;0&#x201d; in an element in the column indicates that the corresponding entity is not a male. In some embodiments, some features for certain entities may not be available, and thus some elements may be blank or may have a value &#x201c;NA&#x201d; (not available) or &#x201c;NaN&#x201d; (not a number). The at least one baseline feature is used to identify target population. For example, the at least one baseline feature can include income higher than $100,000 per year. In another example, the at least one baseline feature can include a gender feature (e.g., male) and an age feature (e.g., 20 to 35-years old).</p><p id="p-0037" num="0036">Similarly, the candidate dataset identifies a second group of entities and a second set of features associated with the second group of entities, where the second set of features may or may not include the at least one baseline feature. In some embodiments, the second set of features includes at least some features in the first set of features. In some embodiments, the second set of features includes at least some features that are not in the first set of features. The second group of entities may be identified by their identifications as described above. In some embodiments, the second group of entities includes entities that are not in the first group of entities in the reference dataset. In some embodiments, the second group of entities includes some entities that are also in the first group of entities in the reference dataset. In some embodiments, the candidate dataset is also represented by a two-dimensional data matrix, where each row (i.e., entry) corresponds to one entity in the second group of entities and each column corresponds to a feature in the second set of features.</p><p id="p-0038" num="0037">At block <b>220</b>, the one or more processing devices (e.g., more specifically, first entities identification engine <b>120</b>) identify first unique candidate entities in the candidate dataset that are described in the candidate dataset as associated with the baseline feature. For example, the one or more processing devices first identify entities in the candidate dataset that are not included in the reference dataset based on, for example, the identification of the entity in each entry in the datasets. For each unique entity in the candidate dataset that is not in the reference dataset, the one or more processing devices can determine whether the unique entity has the baseline feature, for example, based on the value in the matrix element on the row representing the unique entity and in the column corresponding to the baseline feature. In some embodiments, the one or more processing devices can search row-by-row in the data matrix representing the candidate dataset. In some embodiments, the one or more processing devices can first sort the data matrices representing the reference dataset and the candidate dataset based on, for example, the identifications of the entities and/or the values of the matrix elements in the column representing the baseline feature, and then identify unique entities in the candidate dataset that are described in the data matrix as associated with the baseline feature.</p><p id="p-0039" num="0038">At block <b>230</b>, the one or more processing devices (e.g., more specifically, second entities identification engine <b>130</b>) determine second unique candidate entities in the candidate dataset that are not described in the candidate dataset as associated with the baseline feature, but are likely associated with the baseline feature and thus are likely to be part of the target population. For unique entities in the candidate dataset that are not in the reference dataset and are not described as associated with the baseline feature (e.g., the corresponding values are missing or are not available), the one or more processing devices may use machine-learning-based techniques (e.g., spectral regularization and clustering techniques) to determine whether the entities are likely to belong to the target population. An example of a technique for identifying the second unique candidate entities in the candidate dataset are described in detail below with respect to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0040" num="0039">At block <b>240</b>, the one or more processing devices (e.g., more specifically, merit attribute determination module <b>140</b>) determine a merit attribute of the candidate dataset based on the number of the first unique candidate entities and the number of the second unique candidate entities in the candidate dataset. In some embodiments, the merit attribute of the candidate dataset is the weighted sum of the number of the first unique candidate entities and the number of the second unique candidate entities in the candidate dataset. For example, the weight for each first unique candidate entity may be higher than the weight for each first unique candidate entity. In some embodiments, the weight for each of the first unique candidate entities and the weight for each of the second unique candidate entities can be determined, for example, using a regression technique.</p><p id="p-0041" num="0040">At block <b>250</b>, the one or more processing devices (e.g., more specifically, dataset selection module <b>160</b>) select the candidate dataset as input data for a target software application based on the merit attribute of the candidate dataset. For example, the candidate dataset can be added to the reference dataset to generate a larger dataset of target population if the merit attribute is greater than a threshold value. In some embodiments, the candidate dataset may be used to further train a machine-learning model, such as a neural network. In some embodiments, multiple candidate datasets may be evaluated using the techniques disclosed herein and one or more candidate datasets may be selected from the multiple candidate datasets based on their merit attributes.</p><p id="p-0042" num="0041">In some embodiments, to identify the second unique candidate entities in a candidate dataset that likely have the baseline feature or likely belong to the target population, the candidate dataset and a reference dataset for a customer application may be combined to generate an aggregated matrix that includes the entities in the reference dataset and the entities in the candidate dataset. The aggregated matrix may include elements that have no associated values because some features for some entities may not be described in the reference dataset or the candidate dataset. Various matrix completion techniques may be used to complete the aggregated matrix (i.e., filling the missing features) based on existing values in the aggregated matrix. From unique entities for which at least some of the at least one baseline feature are not specified or labeled in the aggregated matrix, candidate entities that are likely to be part of the target population for the customer application are identified using, for example, cluster-based techniques, based on various distance metrics.</p><p id="p-0043" num="0042">It is noted that, in general, the features in the reference dataset may be different from the features in the candidate dataset. In order to determine whether entities in the candidate dataset are likely to have the baseline feature, there needs to be at least some overlap between the features in the reference dataset and the features in the candidate dataset, even though the overlap can be rather insignificant.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of an aggregated matrix <b>300</b> in which the features in the reference dataset and the features in the candidate dataset do not have any overlap. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the reference dataset (e.g., including entities u<sub>1 </sub>and u<sub>2</sub>) has features t<sub>1</sub>, t<sub>2</sub>, and t<sub>5</sub>, and the candidate dataset (e.g., including entities u<sub>3 </sub>to u<sub>100</sub>) has features t<sub>3 </sub>and t<sub>4</sub>. It may be difficult to compare any entity in the candidate dataset with entities in the reference dataset using aggregated matrix <b>300</b> because the features in the reference dataset and the features in the candidate dataset do not have any overlap. To be able to determine the similarity between the entities, aggregated matrix <b>300</b> may need to be normalized by filling the features for both datasets.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart <b>400</b> illustrating an example of a method for identifying unique candidate entities in a candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature, as described above with respect to block <b>230</b>, according to certain embodiments. Operations in flow chart <b>400</b> may be performed by one or more processing devices, such as various engines or modules of system <b>100</b>.</p><p id="p-0046" num="0045">At block <b>410</b>, the one or more processing devices (e.g., more specifically, dataset aggregation module <b>132</b>) generate an aggregated matrix that includes entities from a reference dataset including first entities and a candidate dataset including second entities. The entities in the reference dataset may be described as associated with the baseline feature, while the entities in the candidate dataset may or may not be described as associated with the baseline feature. In some embodiments, some entities may be included in both the reference dataset and the candidate dataset. Thus, if the reference dataset includes m<b>1</b> entities and the candidate dataset includes m<b>2</b> entities, the total number of unique entities in the aggregated matrix may be m<b>3</b> m<b>1</b>+m<b>2</b>. The aggregated matrix also includes features associated with the entities from the reference dataset and the candidate dataset. In some embodiments, the features include all unique features included in the reference dataset and the candidate dataset. For example, the reference dataset may include n<b>1</b> features, the candidate dataset may include n<b>2</b> features, and the total number of features in the aggregated matrix may be n<b>3</b>&#x2264;n<b>1</b>+n<b>2</b>. The features for an entity that is both in the reference dataset and the candidate dataset may be combined into one entry (e.g., one row) in the aggregated matrix. Thus, the m<b>1</b>&#xd7;n<b>1</b> matrix for the reference dataset and the m<b>2</b>&#xd7;n<b>2</b> matrix for the candidate dataset may be combined to form a m<b>3</b>&#xd7;n<b>3</b> aggregated matrix. In some embodiments, the aggregated matrix may be a sparse matrix because some features for some entities may not be described in the reference dataset or the candidate dataset.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example of an aggregated matrix <b>500</b> that includes entities from a reference dataset and entities from a candidate dataset. In aggregated matrix <b>500</b>, each row represents one entity (e.g., a user of a software application or a service), and each column represents a feature (or trait) t<sub>1</sub>, t<sub>2</sub>, . . . , t<sub>m-1</sub>, or t<sub>m</sub>, where t<sub>1 </sub>may be a baseline feature. Identifications of entities u<sub>1</sub>, u<sub>2</sub>, . . . , u<sub>n-1</sub>, and u<sub>n </sub>are unique identifications of entities. In the example show in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, entities u<sub>1</sub>, u<sub>2</sub>, u<sub>3</sub>, u<sub>4</sub>, and the like may be in the first entities from the reference dataset, and entities u<sub>n-3</sub>, u<sub>n-2</sub>, u<sub>n-1</sub>, u<sub>n</sub>, and the like may be in the second entities from the candidate dataset.</p><p id="p-0048" num="0047">As shown by row <b>510</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, many of the m&#xd7;n elements in aggregated matrix <b>500</b> may not have an associated value. For example, features t<sub>4</sub>, t<sub>5</sub>, t<sub>m-2</sub>, t<sub>m-1</sub>, and t<sub>m </sub>for entity u<sub>1 </sub>may not be specified or described, or may have a value NA or NaN as described above. Entity u<sub>n-3 </sub>from the candidate dataset and represented by row <b>520</b> may be described as associated with feature ti as indicated by a value &#x201c;1&#x201d; in the corresponding matrix element, entity u<sub>n-2 </sub>from the candidate dataset and represented by row <b>530</b> may be described as not associated with feature t<sub>1 </sub>as indicated by a value &#x201c;0&#x201d; in the corresponding matrix element, while feature t<sub>1 </sub>for entities u<sub>n-1 </sub>and u<sub>n </sub>(represented by rows <b>540</b> and <b>550</b>) may not be described in the candidate dataset.</p><p id="p-0049" num="0048">At block <b>420</b>, the one or more processing devices (e.g., more specifically, dataset normalization engine <b>134</b>) determine missing values for elements in the aggregated matrix based on existing values in the aggregated matrix and using various matrix completion techniques to complete the aggregated matrix. One example of the matrix completion techniques for filling the aggregated matrix is a spectral regularization technique described in Mazumder et al., &#x201c;Spectral Regularization Algorithms for Leaning Large Incomplete Matrices,&#x201d; Journal of Machine Learning Research, 2010 (11), pp. 2287-2322.</p><p id="p-0050" num="0049">At block <b>430</b>, the one or more processing devices (e.g., more specifically, clustering engine <b>136</b>) form a cluster of data points representing the first entities in a multi-dimensional space based on the features in the aggregated matrix other than the baseline feature. For example, the one or more processing devices may map each of the first entities to a data point in the multi-dimensional space based on values in the aggregated matrix. In some embodiments, the multi-dimensional space may represent all features in the aggregated matrix other than the baseline feature. In some embodiments, the multi-dimensional space may represent some but not all features in the aggregated matrix and does not represent the baseline feature. In some embodiments, dimensionality reduction techniques (e.g., principal component analysis techniques) may be used to reduce the dimensions of the multi-dimensional space. In some embodiments, a centroid of the cluster of the data points representing the first entities may be determined.</p><p id="p-0051" num="0050">At block <b>440</b>, the one or more processing devices (e.g., more specifically, threshold distance determination engine <b>138</b>) determine an average reference distance between the data points representing the first entities in the multi-dimensional space. In some embodiments, the average reference distance may be determined based on the distance between each respective pair of data points representing entities in the first entities. In some embodiments, the average reference distance may be determined based on the average of the distance between each data point representing each respective entity in the first entities and the centroid of the cluster of the data points representing the first entities. The average distance may be used to determine a threshold distance.</p><p id="p-0052" num="0051">At block <b>450</b>, each unique entity in the second entities that is not described as associated with the baseline feature may be mapped to a respective data point in the multi-dimensional space by, for example, clustering engine <b>136</b>, and the one or more processing devices (e.g., more specifically, second entities selection engine <b>142</b>) may determine an average candidate distance between the entity and the first entities in the multi-dimensional space. For example, in some embodiments, the average candidate distance can be determined by measuring the distance between the data point representing the unique entity and the data point representing each respective entity in the first entities. In some embodiments, the average candidate distance can be determined by measuring the distance between the data point representing the unique entity and the centroid of the cluster of the data points representing the first entities.</p><p id="p-0053" num="0052">A distance between two data points describes the similarity of two entities represented by the two data points. Thus, the average candidate distance between a data point and the data points representing the first entities describes the similarity of an entity to the known entities in the target population (e.g., from the reference dataset). The further a data point representing an entity is to the cluster (e.g., the centroid of the cluster), the less likely the entity is part of the target population. Various measures of distance can be used to determine the distance in the multi-dimensional space, such as the Euclidean distance, the Pearson correlation distance, and the Jaccard distance. For example, the Euclidean distance is the straight-line distance in the Euclidean space, where the Euclidean distance between two points X and Y with x<sub>i </sub>and y<sub>i </sub>as their coordinates (i=1, 2, . . . , n) in a n-dimensional space R<sup>n </sup>can be determined by:</p><p id="p-0054" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>Euclidean</sub>(<i>X, Y</i>)=&#x221a;{square root over (&#x3a3;<sub>i</sub>(<i>x</i><sub>i</sub><i>&#x2212;y</i><sub>i</sub>)<sup>2</sup>. )}&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0055" num="0000">In some embodiments, the square root in the above equation can be discarded to speed up the calculation, without changing the clustering results:</p><p id="p-0056" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>d</i><sub>Euclidean</sub>&#x2032;(<i>X, Y</i>)=&#x3a3;<sub>i</sub>(<i>x</i><sub>i</sub><i>&#x2212;y</i><sub>i</sub>)<sup>2</sup>. &#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0057" num="0053">Unlike the Euclidean distance, the Pearson correlation distance (or simply Pearson distance or Pearson correlation) measures the linear dependence of two vectors. For example, for two vectors represented by two points X and Y with x<sub>i </sub>and y<sub>i </sub>as their coordinates (i=1, 2, . . . , n) in the n-dimensional space R<sup>n</sup>, Pearson correlation distance can be defined as:</p><p id="p-0058" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mrow>       <msub>        <mi>d</mi>        <mi>Pearson</mi>       </msub>       <mo>(</mo>       <mrow>        <mi>X</mi>        <mo>,</mo>        <mi>Y</mi>       </mrow>       <mo>)</mo>      </mrow>      <mo>=</mo>      <mrow>       <mrow>        <mn>1</mn>        <mo>-</mo>        <mrow>         <mi>Co</mi>         <mo>&#x2062;</mo>         <mrow>          <mi>rr</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>X</mi>           <mo>,</mo>           <mi>Y</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>       <mo>=</mo>       <mrow>        <mn>1</mn>        <mo>-</mo>        <mfrac>         <mrow>          <mi>E</mi>          <mo>[</mo>          <mrow>           <mrow>            <mo>(</mo>            <mrow>             <mi>X</mi>             <mo>-</mo>             <msub>              <mi>&#x3bc;</mi>              <mi>X</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mo>&#x2062;</mo>           <mrow>            <mo>(</mo>            <mrow>             <mi>Y</mi>             <mo>-</mo>             <msub>              <mi>&#x3bc;</mi>              <mi>Y</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>          </mrow>          <mo>]</mo>         </mrow>         <mrow>          <msub>           <mi>&#x3c3;</mi>           <mi>X</mi>          </msub>          <mo>&#x2062;</mo>          <msub>           <mi>&#x3c3;</mi>           <mi>Y</mi>          </msub>         </mrow>        </mfrac>       </mrow>      </mrow>     </mrow>     <mo>,</mo>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0059" num="0000">Where E denotes the expected value, &#x3bc;<sub>x </sub>and &#x3bc;<sub>y </sub>are the averages of x<sub>i </sub>and y<sub>i</sub>, respectively, and &#x3c3;<sub>x </sub>and &#x3c3;<sub>y </sub>are the standard deviations of x<sub>i </sub>and y<sub>i</sub>, respectively. Because the correlation coefficient Corr(X, Y) is between &#x2212;1 and 1, d<sub>Pearson </sub>lies within [0, 2]. When the features are represented by binary values &#x201c;0&#x201d; and &#x201c;1&#x201d; as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, X=Y (e.g., X=(1,0,1)=Y) represents a perfect linearly dependent case, whereas X=&#x2dc;Y (i.e., X is the complement of Y for every element, e.g. X=(1, 0, 1) and Y=(0, 1, 0)) represents a perfect negatively linearly dependent case. In the perfect linearly dependent case, X overlaps with Y, and d<sub>Pearson</sub>(X, Y) has the minimum value 0. In the perfect negatively linearly dependent case, X is farthest from Y, and d<sub>Pearson</sub>(X, Y) has the maximum value 2.</p><p id="p-0060" num="0054">There are many other measures of distance, such as, for example, Manhattan distance, Chebyshev distance, cosine distance, Jaccard distance, Hamming distance, and the like. The distance measure to use can be determined based on, for example, the computation cost and accuracy. In addition, it is desirable that the distance has the minimum value when two points overlap, and has the maximum value when two points are the farthest apart. Both Euclidean distance and the Pearson correlation distance meet this criterion.</p><p id="p-0061" num="0055">At block <b>460</b>, for each entity in the second entities that is not described as associated with the baseline feature, the one or more processing devices (e.g., more specifically, second entities selection engine <b>142</b>) determine the likelihood that the entity is associated with the baseline feature based on the average candidate distance and the average reference distance. In general, the smaller the average candidate distance, the higher the likelihood that the entity is associated with the baseline feature and thus is part of the target population. If the average candidate distance is below the average reference distance, the corresponding entity in the second entities may be determined to be associated with the baseline feature. In some embodiments, a threshold value different from the average reference distance may be used to determine whether the corresponding entity has the baseline feature and thus is part of the target population.</p><p id="p-0062" num="0056">As described above with respect to block <b>240</b>, in some embodiments, the merit attribute of the candidate dataset is the weighted sum of the number of the first unique candidate entities and the number of the second unique candidate entities in the candidate dataset. For example, if the total number of the first unique candidate entities in the candidate dataset that are described in the candidate dataset as associated with the baseline feature is N, the total number of the second unique candidate entities in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature is N&#x2032;, the merit attribute M<sub>estimate </sub>may be determined by:</p><p id="p-0063" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>M</i><sub>estimate</sub><i>=a</i><sub>0</sub><i>+a</i><sub>1</sub><i>N+a</i><sub>2</sub><i>N&#x2032;, </i>&#x2003;&#x2003;(4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0064" num="0000">where a<sub>1 </sub>is the weight for each first unique candidate entity and a<sub>2 </sub>is the weight for each second unique candidate entity. In general, weight a<sub>1 </sub>for each first unique candidate entity may be higher than weight a<sub>2 </sub>for each second unique candidate entity because the first unique candidate entity is known to have the baseline feature, while the second unique candidate entity is estimated to likely have the baseline feature.</p><p id="p-0065" num="0057">As also described above, in some embodiments, the weight for each of the first unique candidate entities and the weight for each of the second unique candidate entities can be determined, for example, using a regression technique. In one example, the merit attributes for some training datasets may have been manually determined or otherwise obtained. For example, the economic values of the training dataset may be obtained. The total number N of the first unique candidate entities and the total number N&#x2032; of the second unique candidate entities in each training dataset can be determined using the techniques described above. A linear regression technique may then be used to determine the weights a<sub>1 </sub>and a<sub>2 </sub>based on the merit attribute and the numbers N and N&#x2032; for each training dataset. In some embodiments, the weights may be assigned based on the confidence level (e.g., the likelihood) that the second unique candidate entities have the baseline feature. For example, in some embodiments, the weights may be assigned based on the distances between the data points representing the second unique candidate entities and the centroid of the cluster of the data points representing the entities in the reference dataset. In some embodiments, the weights may be manually assigned. In some embodiments, the weights may be assigned more aggressively (e.g., assigning a larger value for weight a<sub>2</sub>) or more conservatively (e.g., assigning a lower value for weight a<sub>2</sub>).</p><p id="p-0066" num="0058">Table 1 shows an example of pseudo code for applying machine-learning techniques to evaluate candidate datasets for use by software applications according to certain embodiments described above.</p><p id="p-0067" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Pseudo code of an example method</entry></row><row><entry>for evaluating candidate datasets</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="left"/><tbody valign="top"><row><entry>Input:</entry></row><row><entry>&#x2002;T: the type of threshold, such as &#x201c;mean,&#x201d; which is the mean distance of</entry></row><row><entry>&#x2003;&#x2002;all known data points (entities) in targeting population) from the</entry></row><row><entry>&#x2003;&#x2002;reference dataset to its centroid.</entry></row><row><entry>&#x2002;Dt: Distance type, such as &#x2018;Euclidean&#x2019; or &#x2018;Pearson&#x2019; (Pearson correlation).</entry></row><row><entry>&#x2002;df_candidate: the candidate dataset.</entry></row><row><entry>&#x2002;df_reference: the customer's dataset (i.e., reference dataset).</entry></row><row><entry>&#x2002;BST: the baseline trait(s), which may be binary variable(s) pre-specified</entry></row><row><entry>&#x2003;by the customer.</entry></row><row><entry>&#x2002;&#x3b1;<sub>1</sub>: the merit value per new entity who has the BST.</entry></row><row><entry>&#x2002;&#x3b1;<sub>2</sub>: the merit value per new entity who does not have BST.</entry></row><row><entry>Main program:</entry></row><row><entry>{</entry></row><row><entry>&#x2002;df_candidate_new = entities from df_candidate who are not in</entry></row><row><entry>&#x2002;df_reference</entry></row><row><entry>&#x2002;if df_candidate_new has BST:</entry></row><row><entry>&#x2003;&#x2002;df_candidate_new_1 = entities from df_candidate_new whose</entry></row><row><entry>&#x2003;&#x2002;BST == 1</entry></row><row><entry>&#x2003;&#x2002;df_candidate_new_0 = entities from df_candidate_new whose</entry></row><row><entry>&#x2003;&#x2002;BST == 0</entry></row><row><entry>&#x2003;&#x2002;N = count1(df_candidate_new_1), N&#x2019; =</entry></row><row><entry>&#x2003;&#x2002;count2(df_candidate_new_0)</entry></row><row><entry>&#x2002;else:</entry></row><row><entry>&#x2003;&#x2002;N = 0, N&#x2019; = count2(df_candidate_new)</entry></row><row><entry>&#x2002;Merit value = &#x3b1;<sub>1</sub>N + &#x3b1;<sub>2</sub>N&#x2019;</entry></row><row><entry>&#x2002;Output: Merit value</entry></row><row><entry>}</entry></row><row><entry>function count1(data set: df):</entry></row><row><entry>{</entry></row><row><entry>&#x2002;Output: number of unique entities in df who has the BST</entry></row><row><entry>}</entry></row><row><entry>function count2(data set: df):</entry></row><row><entry>{</entry></row><row><entry>&#x2002;// Expand the traits in df_reference and df so that they have the same</entry></row><row><entry>&#x2002;traits</entry></row><row><entry>&#x2002;if traits in df_reference and traits in df are not the same:</entry></row><row><entry>&#x2003;allTraits = set(traits in df_reference) + set(traits in df)</entry></row><row><entry>&#x2003;df_reference&#x2019; = df_reference with allTraits.</entry></row><row><entry>&#x2003;df&#x2019; = df with allTraits.</entry></row><row><entry>&#x2003;fill the missing values in df_reference&#x2019; and df&#x2019; by spectral</entry></row><row><entry>&#x2003;regularization algorithm</entry></row><row><entry>&#x2002;end if</entry></row><row><entry>&#x2002;// Calculate the centroid of the df_reference &#x2019;</entry></row><row><entry>&#x2002;for each trait_i in df_reference&#x2019;:</entry></row><row><entry>&#x2003;&#x2002;centroid<sub>trait</sub><sub><sub2>&#x2014;</sub2></sub><sub>i</sub>= mean(column trait_i of df_reference&#x2019;)</entry></row><row><entry>&#x2002;end for</entry></row><row><entry>&#x2002;// Calculate the threshold: (take &#x2018;mean&#x2019; as the example)</entry></row><row><entry>&#x2002;if T == &#x2018;mean&#x2019;:</entry></row><row><entry>&#x2003;if Dt is &#x2018;Euclidean&#x2019;:</entry></row><row><entry>&#x2003;&#x2002;threshold = mean(d<sub>Euclidean</sub>(entity_i, centroid)) for each entity_i</entry></row><row><entry>&#x2003;&#x2002;in df_reference&#x2019;</entry></row><row><entry>&#x2003;end if</entry></row><row><entry>&#x2003;if Dt is &#x2018;Pearson&#x2019;:</entry></row><row><entry>&#x2003;&#x2002;threshold = mean(d<sub>Pearson</sub>(entity_i, centroid)) for each entity_i</entry></row><row><entry>&#x2003;&#x2002;in df_reference&#x2019;</entry></row><row><entry>&#x2003;end if</entry></row><row><entry>&#x2002;end if</entry></row><row><entry>&#x2002;// Count N&#x2019;</entry></row><row><entry>&#x2002;count = 0</entry></row><row><entry>&#x2002;for every entity_j in df&#x2019;:</entry></row><row><entry>&#x2003;&#x2002;if Dt is &#x2018;Euclidean&#x2019;:</entry></row><row><entry>&#x2003;&#x2003;if d<sub>Euclidean</sub>(entity_i, centroid) &#x3c;= threshold:</entry></row><row><entry>&#x2003;&#x2003;&#x2002;count += 1</entry></row><row><entry>&#x2003;&#x2003;end if</entry></row><row><entry>&#x2003;&#x2002;end if</entry></row><row><entry>&#x2003;&#x2002;if Dt is &#x2018;Pearson&#x2019;:</entry></row><row><entry>&#x2003;&#x2003;if d<sub>Pearson</sub>(entity_i, centroid) &#x3c;= threshold:</entry></row><row><entry>&#x2003;&#x2003;&#x2002;count += 1</entry></row><row><entry>&#x2003;&#x2003;end if</entry></row><row><entry>&#x2003;&#x2002;end if</entry></row><row><entry>&#x2002;end for</entry></row><row><entry>&#x2002;Output: count</entry></row><row><entry>}</entry></row><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0068" num="0059">In the embodiments and pseudo code described above, several parameters may be used, including the threshold distance, weight a<sub>1</sub>, and weight a<sub>2</sub>. Weight a<sub>1 </sub>is related to N, the total number of the first unique candidate entities in the candidate dataset that are described in the candidate dataset as associated with the baseline feature and thus are most part of the target population. The threshold distance and weight a<sub>2 </sub>are related to N&#x2032;, the total number of the second unique candidate entities in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature and thus are predicted to be part of the target population. In some embodiments, the threshold distance and weight a<sub>2 </sub>depend on each other. For example, when the threshold distance is increased, more entities in the candidate dataset may be included in the N&#x2032; entities, which may cause the weight a<sub>2 </sub>for each entities in the N&#x2032; entities to decrease. In some embodiments, the threshold distance may be set to a constant value, such as the mean value of the distances between each data point representing an entity in the reference dataset and the centroid of the cluster, and weight a<sub>2 </sub>may be tuned to fit the training datasets to the model described by Equation (4).</p><p id="p-0069" num="0060">As described above, the total number N&#x2032; of the second unique candidate entities in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature may depend on the threshold distance and the type of distance measure used. Different distance measures, such as the Euclidean distance and Pearson correlation distance, are evaluated in a simulation using 60 candidate datasets, one of which is used as the reference dataset. In the simulation, the baseline traits are selected randomly, and a Gaussian distributed data price is used for model validation. The simulation results and cross validation show a very high average accuracy, such as 87% or higher.</p><p id="p-0070" num="0061"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a chart <b>600</b> illustrating examples of simulated results based on Euclidean distance and using techniques disclosed herein according to certain embodiments. As described above, <b>60</b> datasets are used in the simulation. Chart <b>600</b> shows the number N of the first unique candidate entities (represented by solid-filled bars <b>610</b>) in each candidate dataset that are described in the candidate dataset as associated with the baseline feature, the number N&#x2032; of the second unique candidate entities (represented by pattern-filled bars <b>620</b>) in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature, and the number of unqualified entities (represented by unfilled bars <b>630</b>) in the candidate dataset that are determined as not in the target population. The horizontal axis represents the indexes of the candidate datasets, which are ordered such that the total number of unique entities in each respective dataset (represented by the vertical axis) is in a descending order. For the results shown in chart <b>600</b>, the Euclidean distance is computed using Equation (2), and the threshold is set to the mean Euclidean distance between each respective data point in the data points representing all entities in the reference dataset and the centroid of the data points in a multi-dimensional space.</p><p id="p-0071" num="0062"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a chart <b>700</b> illustrating examples of simulated results based on Euclidean distance and using techniques disclosed herein according to certain embodiments. The <b>60</b> datasets shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref> are the same as the <b>60</b> datasets shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. Chart <b>700</b> shows the number N of the first unique candidate entities (represented by solid-filled bars <b>710</b>) in each candidate dataset that are described in the candidate dataset as associated with the baseline feature, the number N&#x2032; of the second unique candidate entities (represented by pattern-filled bars <b>720</b>) in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature, and the number of unqualified entities (represented by unfilled bars <b>730</b>) in the candidate dataset that are determined as not in the target population. The horizontal axis represents the indexes of the candidate datasets, which are ordered such that the total number of unique entities in each respective dataset (represented by the vertical axis) is in a descending order. For the results shown in chart <b>700</b>, the Euclidean distance is computed using Equation (2), and the threshold is set to two times of the mean Euclidean distance between each respective data point in the data points representing all entities in the reference dataset and the centroid of the data points in the multi-dimensional space.</p><p id="p-0072" num="0063">As shown by <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>, for a same candidate dataset, when the threshold distance increases, more unique entities are predicted to have the baseline feature(s) and thus are in the target population. Because the merit attribute of a candidate dataset would not change with the threshold distance used for the evaluation, the weight for each entity predicted to have the baseline feature(s) would reduce when the threshold distance increases.</p><p id="p-0073" num="0064"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a chart <b>800</b> illustrating examples of simulated results based on Pearson correlation distance and using techniques disclosed herein according to certain embodiments described above. The same <b>60</b> datasets used in the simulation based on Euclidean distance are used in the simulation based on Pearson correlation distance. Chart <b>800</b> shows the number N of the first unique candidate entities (represented by solid-filled bars <b>810</b>) in each candidate dataset that are described in the candidate dataset as associated with the baseline feature, the number N&#x2032; of the second unique candidate entities (represented by pattern-filled bars <b>820</b>) in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature, and the number of unqualified entities (represented by unfilled bars <b>830</b>) in the candidate dataset that are determined as not in the target population. The horizontal axis represents the indexes of the candidate datasets, which are ordered such that the total number of unique entities in each respective dataset (represented by the vertical axis) is in a descending order. For the results shown in chart <b>800</b>, the Pearson correlation distance is computed using Equation (3), and the threshold is set to the mean Pearson correlation distance between each respective data point in the data points representing all entities in the reference dataset and the centroid of the data points in a multi-dimensional space.</p><p id="p-0074" num="0065"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a chart <b>900</b> illustrating examples of simulated results based on Pearson correlation distance and using techniques disclosed herein according to certain embodiments described above. The 60 datasets shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref> are the same as the 60 datasets shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Chart <b>900</b> shows the number N of the first unique candidate entities (represented by solid-filled bars <b>910</b>) in each candidate dataset that are described in the candidate dataset as associated with the baseline feature, the number N&#x2032; of the second unique candidate entities (represented by pattern-filled bars <b>920</b>) in the candidate dataset that are not described as associated with the baseline feature but are likely associated with the baseline feature, and the number of unqualified entities (represented by unfilled bars <b>930</b>) in the candidate dataset that are determined as not in the target population. The horizontal axis represents the indexes of the candidate datasets, which are ordered such that the total number of unique entities in each respective dataset (represented by the vertical axis) is in a descending order. For the results shown in chart <b>900</b>, the Pearson correlation distance is computed using Equation (3), and the threshold is set to two times of the mean Pearson correlation distance between each respective data point in the data points representing all entities in the reference dataset and the centroid of the data points in the multi-dimensional space.</p><p id="p-0075" num="0066">As shown by <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>, for a same candidate dataset, when the threshold distance increases, more unique entities are predicted to have the baseline feature(s) and thus are likely to be in the target population. Because the merit attribute of a candidate dataset would not change with the threshold distance used for the evaluation, the weight for each entity predicted to have the baseline feature(s) would reduce when the threshold distance increases.</p><p id="p-0076" num="0067"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram <b>1000</b> illustrating the increase in the number of entities (N&#x2032;) predicted to be part of the target population when the threshold distance is doubled, based on the simulation results shown in <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>9</b></figref> using techniques disclosed in certain embodiments. In diagram <b>1000</b>, the horizontal axis represents the indexes of the candidate datasets, which are ordered such that the total number of unique entities in each respective dataset (represented by the vertical axis) is in a descending order. The primary vertical axis represents the factor of increase in N&#x2032; determined by (N&#x2032;(2T<sub>d</sub>)-N&#x2032;(T<sub>d</sub>))/N&#x2032;(T<sub>d</sub>), where T<sub>d </sub>is a pre-specified threshold distance, such as the mean distance between each respective data point in the data points representing all entities in the reference dataset and the centroid of the data points in the multi-dimensional space. The secondary vertical axis represents the number of unique entities.</p><p id="p-0077" num="0068">A curve <b>1010</b> shows the increase in N&#x2032; when the Euclidean distance is used and the threshold Euclidean distance is doubled, and curve <b>1020</b> shows the increase in N&#x2032; when the Pearson correlation distance is used and the threshold distance is doubled. A curve <b>1030</b> shows the number of unique entities in each respective candidate dataset. Curve <b>1020</b> shows that the factor of increase in N&#x2032; is consistently around about 1.1 (i.e., N&#x2032; is approximately doubled) when the threshold T<sub>d </sub>is doubled, while curve <b>1010</b> shows that the factor of increase in N&#x2032; for a candidate dataset when the threshold T<sub>d </sub>approximately correlates with the number of unique entities in the candidate dataset shown by curve <b>1030</b>. Diagram <b>1000</b> suggests that the Pearson correlation distance may be a better measure of distance than the Euclidean distance.</p><p id="p-0078" num="0069">A computing system, such as one including computing system <b>1100</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>, can be configured to perform the illustrative flows and techniques described above according to some embodiments. Instructions for performing the operations of the illustrative flows can be stored as computer-readable instructions on a non-transitory computer-readable medium of the computer system. As stored, the instructions represent programmable modules that include code or data executable by a processor(s) of the computer system. The execution of such instructions configures the computer system to perform the specific operations shown in the figures and described herein. Each programmable module in combination with the processor represents a means for performing a respective operation(s). While the operations are illustrated in a particular order, it should be understood that no particular order is necessary and that one or more operations may be omitted, skipped, and/or reordered.</p><p id="p-0079" num="0070">Any suitable computing system or group of computing systems can be used for performing the operations described herein. For example, <figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an example of the computing system <b>1100</b> that may be used to implement certain engines or modules of system <b>100</b>. The depicted example of a computing system <b>1100</b> includes a processor <b>1102</b> communicatively coupled to one or more memory devices <b>1104</b>. The processor <b>1102</b> executes computer-executable program code stored in a memory device <b>1104</b>, accesses information stored in the memory device <b>1104</b>, or both. Examples of the processor <b>1102</b> include a microprocessor, an application-specific integrated circuit (&#x201c;ASIC&#x201d;), a field-programmable gate array (&#x201c;FPGA&#x201d;), or any other suitable processing device. The processor <b>1102</b> can include any number of processing devices, including a single processing device.</p><p id="p-0080" num="0071">A memory device <b>1104</b> includes any suitable non-transitory computer-readable medium for storing program code <b>1115</b>, program data <b>1116</b>, or both. A computer-readable medium can include any electronic, optical, magnetic, or other storage device capable of providing a processor with computer-readable instructions or other program code. Non-limiting examples of a computer-readable medium include a magnetic disk, a memory chip, a ROM, a RAM, an ASIC, optical storage, magnetic tape or other magnetic storage, or any other medium from which a processing device can read instructions. The instructions may include processor-specific instructions generated by a compiler or an interpreter from code written in any suitable computer-programming language, including, for example, C, C++, C#, Visual Basic, Java, Python, Perl, JavaScript, and ActionScript.</p><p id="p-0081" num="0072">The computing system <b>1100</b> may also include a number of external or internal devices, an input device <b>1120</b>, a presentation device <b>1118</b>, or other input or output devices. For example, computing system <b>1100</b> is shown with one or more input/output (&#x201c;I/O&#x201d;) interfaces <b>1108</b>. An I/O interface <b>1108</b> can receive input from input devices or provide output to output devices. One or more buses <b>1106</b> are also included in the computing system <b>1100</b>. The bus <b>1106</b> communicatively couples one or more components of a respective one of the computing system <b>1100</b>.</p><p id="p-0082" num="0073">The computing system <b>1100</b> executes program code <b>1105</b> that configures the processor <b>1102</b> to perform one or more of the operations described herein. Examples of the program code <b>1105</b> include, in various embodiments, program code for implementing the pseudo code described in Table 1. The program code may be resident in the memory device <b>1104</b> or any suitable computer-readable medium and may be executed by the processor <b>1102</b> or any other suitable processor.</p><p id="p-0083" num="0074">In some embodiments, one or more memory devices <b>1104</b> stores program data <b>1107</b> that includes one or more datasets and models described herein. Examples of these datasets include interaction data, experience metrics, training interaction data or historical interaction data, transition importance data, etc. In some embodiments, one or more of data sets, models, and functions are stored in the same memory device (e.g., one of the memory devices <b>1104</b>). In additional or alternative embodiments, one or more of the programs, data sets, models, and functions described herein are stored in different memory devices <b>1104</b> accessible via a data network.</p><p id="p-0084" num="0075">In some embodiments, the computing system <b>1100</b> also includes a network interface device <b>1110</b>. The network interface device <b>1110</b> includes any device or group of devices suitable for establishing a wired or wireless data connection to one or more data networks. Non-limiting examples of the network interface device <b>1110</b> include an Ethernet network adapter, a modem, and/or the like. The computing system <b>1100</b> is able to communicate with one or more other computing devices (e.g., a computing device executing an environment evaluation system <b>102</b>) via a data network using the network interface device <b>1110</b>.</p><p id="p-0085" num="0076">In some embodiments, the computing system <b>1100</b> also includes the input device <b>1120</b> and the presentation device <b>1118</b> depicted in <figref idref="DRAWINGS">FIG. <b>11</b></figref>. An input device <b>1120</b> can include any device or group of devices suitable for receiving visual, auditory, or other suitable input that controls or affects the operations of the processor <b>1102</b>. Non-limiting examples of the input device <b>1120</b> include a touchscreen, a mouse, a keyboard, a microphone, a separate mobile computing device, etc. A presentation device <b>1118</b> can include any device or group of devices suitable for providing visual, auditory, or other suitable sensory output. Non-limiting examples of the presentation device <b>1118</b> include a touchscreen, a monitor, a speaker, a separate mobile computing device, etc.</p><p id="p-0086" num="0077">Although <figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts the input device <b>1120</b> and the presentation device <b>1118</b> as being local to the computing device that executes the environment evaluation system <b>102</b>, other implementations are possible. For instance, in some embodiments, one or more of the input device <b>1120</b> and the presentation device <b>1118</b> can include a remote client-computing device that communicates with the computing system <b>1100</b> via the network interface device <b>1110</b> using one or more data networks described herein.</p><p id="p-0087" num="0078">Numerous specific details are set forth herein to provide a thorough understanding of the claimed subject matter. However, those skilled in the art will understand that the claimed subject matter may be practiced without these specific details. In other instances, methods, apparatuses, or systems that would be known by one of ordinary skill have not been described in detail so as not to obscure claimed subject matter.</p><p id="p-0088" num="0079">Unless specifically stated otherwise, it is appreciated that throughout this specification discussions utilizing terms such as &#x201c;processing,&#x201d; &#x201c;computing,&#x201d; &#x201c;calculating,&#x201d; &#x201c;determining,&#x201d; and &#x201c;identifying&#x201d; or the like refer to actions or processes of a computing device, such as one or more computers or a similar electronic computing device or devices, that manipulate or transform data represented as physical electronic or magnetic quantities within memories, registers, or other information storage devices, transmission devices, or display devices of the computing platform.</p><p id="p-0089" num="0080">The system or systems discussed herein are not limited to any particular hardware architecture or configuration. A computing device can include any suitable arrangement of components that provide a result conditioned on one or more inputs. Suitable computing devices include multi-purpose microprocessor-based computer systems accessing stored software that programs or configures the computing system from a general purpose computing apparatus to a specialized computing apparatus implementing one or more embodiments of the present subject matter. Any suitable programming, scripting, or other type of language or combinations of languages may be used to implement the teachings contained herein in software to be used in programming or configuring a computing device.</p><p id="p-0090" num="0081">Embodiments of the methods disclosed herein may be performed in the operation of such computing devices. The order of the blocks presented in the examples above can be varied&#x2014;for example, blocks can be re-ordered, combined, and/or broken into sub-blocks. Certain blocks or processes can be performed in parallel.</p><p id="p-0091" num="0082">While the present subject matter has been described in detail with respect to specific embodiments thereof, it will be appreciated that those skilled in the art, upon attaining an understanding of the foregoing may readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, it should be understood that the present disclosure has been presented for purposes of example rather than limitation, and does not preclude inclusion of such modifications, variations, and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. Indeed, the methods and systems described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the methods and systems described herein may be made without departing from the spirit of the present disclosure. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the present disclosure.</p><p id="p-0092" num="0083">Conditional language used herein, such as, among others, &#x201c;can,&#x201d; &#x201c;could,&#x201d; &#x201c;might,&#x201d; &#x201c;may,&#x201d; &#x201c;e.g.,&#x201d; and the like, unless specifically stated otherwise, or otherwise understood within the context as used, is generally intended to convey that certain examples include, while other examples do not include, certain features, elements, and/or steps. Thus, such conditional language is not generally intended to imply that features, elements and/or steps are in any way required for one or more examples or that one or more examples necessarily include logic for deciding, with or without author input or prompting, whether these features, elements and/or steps are included or are to be performed in any particular example.</p><p id="p-0093" num="0084">The terms &#x201c;comprising,&#x201d; &#x201c;including,&#x201d; &#x201c;having,&#x201d; and the like are synonymous and are used inclusively, in an open-ended fashion, and do not exclude additional elements, features, acts, operations, and so forth. Also, the term &#x201c;or&#x201d; is used in its inclusive sense (and not in its exclusive sense) so that when used, for example, to connect a list of elements, the term &#x201c;or&#x201d; means one, some, or all of the elements in the list. The use of &#x201c;adapted to&#x201d; or &#x201c;configured to&#x201d; herein is meant as open and inclusive language that does not foreclose devices adapted to or configured to perform additional tasks or steps. Additionally, the use of &#x201c;based on&#x201d; is meant to be open and inclusive, in that a process, step, calculation, or other action &#x201c;based on&#x201d; one or more recited conditions or values may, in practice, be based on additional conditions or values beyond those recited. Similarly, the use of &#x201c;based at least in part on&#x201d; is meant to be open and inclusive, in that a process, step, calculation, or other action &#x201c;based at least in part on&#x201d; one or more recited conditions or values may, in practice, be based on additional conditions or values beyond those recited. Headings, lists, and numbering included herein are for ease of explanation only and are not meant to be limiting.</p><p id="p-0094" num="0085">The various features and processes described above may be used independently of one another, or may be combined in various ways. All possible combinations and sub-combinations are intended to fall within the scope of the present disclosure. In addition, certain method or process blocks may be omitted in some embodiments. The methods and processes described herein are also not limited to any particular sequence, and the blocks or states relating thereto can be performed in other sequences that are appropriate. For example, described blocks or states may be performed in an order other than that specifically disclosed, or multiple blocks or states may be combined in a single block or state. The example blocks or states may be performed in serial, in parallel, or in some other manner. Blocks or states may be added to or removed from the disclosed examples. Similarly, the example systems and components described herein may be configured differently than described. For example, elements may be added to, removed from, or rearranged compared to the disclosed examples.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004869A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230004869A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for applying machine-learning techniques to evaluate candidate datasets for use by software applications, the method comprising performing, by one or more processing devices, operations including:<claim-text>identifying, in a candidate dataset identifying first entities associated with first features, first unique candidate entities that are absent from a reference dataset identifying second entities associated with second features that include a baseline feature of a target population and that are associated with the baseline feature in the candidate dataset;</claim-text><claim-text>forming, in a multi-dimensional space and based on a subset of the second features lacking the baseline feature, a cluster of data points representing the second entities;</claim-text><claim-text>mapping a subset of the first entities that are absent from the reference dataset and that are not in the first unique candidate entities to additional data points, respectively in the multi-dimensional space;</claim-text><claim-text>identifying, from the subset of the first entities, second unique candidate entities corresponding to a subset of the additional data points within a threshold distance of the cluster;</claim-text><claim-text>determining a merit attribute of the candidate dataset based on a first weight for each first unique candidate entity, a second weight for each second unique candidate entity, a number of the first unique candidate entities in the candidate dataset, and a number of the second unique candidate entities in the candidate dataset; and</claim-text><claim-text>selecting the candidate dataset as input data for a target software application based on the merit attribute of the candidate dataset being greater than a threshold value.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein identifying the second unique candidate entities comprises:<claim-text>determining, in the multi-dimensional space, a centroid of the cluster of the data points representing the second entities;</claim-text><claim-text>determining, in the multi-dimensional space, an average reference distance between each data point in the data points representing the second entities and the centroid of the cluster; and</claim-text><claim-text>determining the threshold distance based on the average reference distance.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein identifying the second unique candidate entities further comprises:<claim-text>determining, in the multi-dimensional space, a distance between a respective additional data point and the centroid of the cluster.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the distance between the respective additional data point and the centroid of the cluster includes a Pearson correlation distance, Euclidean distance, cosine distance, or Jaccard distance.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein forming the cluster of data points representing the second entities comprises:<claim-text>generating an aggregated matrix that identifies the second entities, the second features, the first entities, and the first features;</claim-text><claim-text>estimating missing values in the aggregated matrix using spectral regularization; and</claim-text><claim-text>mapping, based on the aggregated matrix, the second entities to the data points representing the second entities,</claim-text><claim-text>wherein coordinates of each data point in the data points representing the second entities are determined based on corresponding values in the aggregated matrix.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein mapping the subset of the firstentities to the additional data points in the multi-dimensional space comprises:<claim-text>mapping each first entity in the subset of the first entities to a respective additional data point in the multi-dimensional space based on corresponding values in the aggregated matrix.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operations further comprise determining, based on merit attributes of training datasets, the first weight and the second weight using linear regression.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the merit attribute of the candidate dataset comprises:<claim-text>determining a weighted sum of the number of the first unique candidate entities in the candidate dataset and the number of the second unique candidate entities in the candidate dataset,</claim-text><claim-text>wherein each of the first unique candidate entities is associated with the first weight; and</claim-text><claim-text>wherein each of the second unique candidate entities is associated with the second weight.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system comprising:<claim-text>a processing device; and</claim-text><claim-text>a non-transitory computer-readable medium communicatively coupled to the processing device, wherein the processing device is configured to execute program code stored in the non-transitory computer-readable medium and thereby perform operations comprising:<claim-text>identifying, in a candidate dataset identifying first entities associated with first features, first unique candidate entities that are absent from a reference dataset identifying second entities associated with second features that include a baseline feature of a target population and that are associated with the baseline feature in the candidate dataset;</claim-text><claim-text>forming, in a multi-dimensional space and based on a subset of the second features lacking the baseline feature, a cluster of data points representing the second entities;</claim-text><claim-text>mapping a subset of the first entities that are absent from the reference dataset and that are not in the first unique candidate entities to additional data points, respectively in the multi-dimensional space;</claim-text><claim-text>identifying, from the subset of the first entities, second unique candidate entities corresponding to a subset of the additional data points within a threshold distance of the cluster;</claim-text><claim-text>determining a merit attribute of the candidate dataset based on a first weight for each first unique candidate entity, a second weight for each second unique candidate entity, a number of the first unique candidate entities in the candidate dataset, and a number of the second unique candidate entities in the candidate dataset; and</claim-text><claim-text>selecting the candidate dataset as input data for a target software application based on the merit attribute of the candidate dataset being greater than a threshold value.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein identifying the second unique candidate entities comprises:<claim-text>determining, in the multi-dimensional space, a centroid of the cluster of the data points representing the second entities;</claim-text><claim-text>determining, in the multi-dimensional space, an average reference distance between each data point in the data points representing the second entities and the centroid of the cluster;</claim-text><claim-text>determining the threshold distance based on the average reference distance; and</claim-text><claim-text>determining, in the multi-dimensional space, a distance between a respective additional data point and the centroid of the cluster.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the distance between the respective additional data point and the centroid of the cluster includes a Pearson correlation distance, Euclidean distance, cosine distance, or Jaccard distance.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein forming the cluster of data points representing the second entities comprises:<claim-text>generating an aggregated matrix that identifies the second entities, the second features, the first entities, and the first features;</claim-text><claim-text>estimating missing values in the aggregated matrix using spectral regularization; and</claim-text><claim-text>mapping, based on the aggregated matrix, the second entities to the data points representing the second entities,</claim-text><claim-text>wherein coordinates of each data point in the data points representing the second entities are determined based on corresponding values in the aggregated matrix.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein mapping the subset of the first entities to the additional data points in the multi-dimensional space comprises:<claim-text>mapping each first entity in the subset of the first entities to a respective additional data point in the multi-dimensional space based on corresponding values in the aggregated matrix.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the operations further comprise determining, based on merit attributes of training datasets, the first weight and the second weight using linear regression.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein determining the merit attribute of the candidate dataset comprises:<claim-text>determining a weighted sum of the number of the first unique candidate entities in the candidate dataset and the number of the second unique candidate entities in the candidate dataset,</claim-text><claim-text>wherein each of the first unique candidate entities is associated with the first weight; and</claim-text><claim-text>wherein each of the second unique candidate entities is associated with the second weight.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system comprising:<claim-text>means for identifying, in a candidate dataset identifying first entities associated with first features, first unique candidate entities that are absent from a reference dataset identifying second entities associated with second features that include a baseline feature of a target population and that are associated with the baseline feature in the candidate dataset;</claim-text><claim-text>means for forming, in a multi-dimensional space and based on a subset of the second features lacking the baseline feature, a cluster of data points representing the second entities;</claim-text><claim-text>means for mapping a subset of the first entities that are absent from the reference dataset and that are not in the first unique candidate entities to additional data points, respectively in the multi-dimensional space;</claim-text><claim-text>means for identifying, from the subset of the first entities, second unique candidate entities corresponding to a subset of the additional data points within a threshold distance of the cluster;</claim-text><claim-text>means for determining a merit attribute of the candidate dataset based on a first weight for each first unique candidate entity, a second weight for each second unique candidate entity, a number of the first unique candidate entities in the candidate dataset, and a number of the second unique candidate entities in the candidate dataset; and</claim-text><claim-text>means for selecting the candidate dataset as input data for a target software application based on the merit attribute of the candidate dataset being greater than a threshold value.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the means for identifying the second unique candidate entities comprise:<claim-text>means for determining, in the multi-dimensional space, a centroid of the cluster of the data points representing the second entities;</claim-text><claim-text>means for determining, in the multi-dimensional space, an average reference distance between each data point in the data points representing the second entities and the centroid of the cluster;</claim-text><claim-text>means for determining the threshold distance based on the average reference distance; and</claim-text><claim-text>means for determining, in the multi-dimensional space, a distance between a respective additional data point and the centroid of the cluster.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the distance between the respective additional data point and the centroid of the cluster includes a Pearson correlation distance, Euclidean distance, cosine distance, or Jaccard distance.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the means for forming the cluster of data points representing the second entities comprise:<claim-text>means for generating an aggregated matrix that identifies the second entities, the second features, the first entities, and the first features;</claim-text><claim-text>means for estimating missing values in the aggregated matrix using spectral regularization; and</claim-text><claim-text>means for mapping, based on the aggregated matrix, the second entities to the data points representing the second entities,</claim-text><claim-text>wherein coordinates of each data point in the data points representing the second entities are determined based on corresponding values in the aggregated matrix.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the means for determining the merit attribute of the candidate dataset comprise:<claim-text>means for determining a weighted sum of the number of the first unique candidate entities in the candidate dataset and the number of the second unique candidate entities in the candidate dataset,</claim-text><claim-text>wherein each of the first unique candidate entities is associated with the first weight; and</claim-text><claim-text>wherein each of the second unique candidate entities is associated with the second weight.</claim-text></claim-text></claim></claims></us-patent-application>