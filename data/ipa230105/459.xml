<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000460A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221220" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000460</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17856896</doc-number><date>20220701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>15</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>481</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>4209</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>4472</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>8</main-group><subgroup>4477</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>15</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>4815</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20160201</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>2090</main-group><subgroup>061</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">NON-CONTACT RAPID EYE MOVEMENT (REM) MONITORING</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217554</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Meta Platforms Technologies, LLC</orgname><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GOLARD</last-name><first-name>Andre</first-name><address><city>Seattle</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TALATHI</last-name><first-name>Sachin</first-name><address><city>Snoqualmie</city><state>WA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>CAVIN</last-name><first-name>Robert Dale</first-name><address><city>Bellevue</city><state>WA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Meta Platforms Technologies, LLC</orgname><role>02</role><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">According to examples, systems, devices, and methods for detecting rapid eye movement (REM) are described. The device may include an array of ultrasound sensors oriented to emit transmit ultrasounds signals in an eyeward direction, wherein the ultrasound sensors are to receive a return signal of the transmit signal reflecting off of a target, and wherein the ultrasound sensors are to output a distance signal representative of a distance to a target, the distance signal generated based on the return signal, and a transceiver to receive the distance signals, wherein the transceiver is to transmit the distance signals from the array of ultrasound sensors to a remote device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="63.08mm" wi="125.39mm" file="US20230000460A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="152.99mm" wi="95.00mm" orientation="landscape" file="US20230000460A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="159.00mm" wi="69.93mm" orientation="landscape" file="US20230000460A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="154.94mm" wi="64.94mm" orientation="landscape" file="US20230000460A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="159.43mm" wi="112.01mm" orientation="landscape" file="US20230000460A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="167.13mm" wi="112.61mm" orientation="landscape" file="US20230000460A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">PRIORITY</heading><p id="p-0002" num="0001">This patent application claims priority to U.S. Provisional Patent Application No. 63/217,554, entitled &#x201c;Non-Contract Rapid Eye Movement (REM) Monitoring,&#x201d; filed on Jul. 1, 2021, which is hereby incorporated by reference herein in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This patent application relates generally to measurement and monitoring of physiological characteristics, and more specifically, to systems and methods for monitoring non-contact rapid eye movement (REM).</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Rapid eye movement (REM) may be an indicator of deep sleep. In some examples, one method for measuring rapid eye movement (REM) sleep may be electro-oculography (EOG). In some instances, electro-oculography (EOG) may be cumbersome as may include contacts being adhered (e.g. glued) to the skin around the eye. In some examples, another method of eye movement tracking may utilize gel electrodes in a mask that can be pressed on to the face rather than being glued to the skin.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0005" num="0004">Features of the present disclosure are illustrated by way of example and not limited in the following figures, in which like numerals indicate like elements. One skilled in the art will readily recognize from the following that alternative examples of the structures and methods illustrated in the figures can be employed without departing from the principles described herein.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an arrangement for implementing electro-oculography (EOG), according to an example.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates a first view of a mask, according to an example.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates another (inside) view of a mask, according to an example.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates sensors in an array for emitting signals, according to an example.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates a view point of an eye, according to an example.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">For simplicity and illustrative purposes, the present application is described by referring mainly to examples thereof. In the following description, numerous specific details are set forth in order to provide a thorough understanding of the present application. It will be readily apparent, however, that the present application may be practiced without limitation to these specific details. In other instances, some methods and structures readily understood by one of ordinary skill in the art have not been described in detail so as not to unnecessarily obscure the present application. As used herein, the terms &#x201c;a&#x201d; and &#x201c;an&#x201d; are intended to denote at least one of a particular element, the term &#x201c;includes&#x201d; means includes but not limited to, the term &#x201c;including&#x201d; means including but not limited to, and the term &#x201c;based on&#x201d; means based at least in part on.</p><p id="p-0012" num="0011">Embodiments of non-contact ultrasound for rapid eye movement (REM) monitoring are described herein. In the following description, numerous specific details are set forth to provide a thorough understanding of the embodiments. One skilled in the relevant art will recognize, however, that the techniques described herein can be practiced without one or more of the specific details, or with other methods, components, materials, etc. In other instances, well-known structures, materials, or operations are not shown or described in detail to avoid obscuring certain aspects.</p><p id="p-0013" num="0012">Reference throughout this specification to &#x201c;one embodiment&#x201d; or &#x201c;an embodiment&#x201d; means that a particular feature, structure, or characteristic described in connection with the embodiment is included in at least one embodiment of the present invention. Thus, the appearances of the phrases &#x201c;in one embodiment&#x201d; or &#x201c;in an embodiment&#x201d; in various places throughout this specification are not necessarily all referring to the same embodiment. Furthermore, the particular features, structures, or characteristics may be combined in any suitable manner in one or more embodiments.</p><p id="p-0014" num="0013">Throughout this specification, several terms of art are used. These terms are to take on their ordinary meaning in the art from which they come, unless specifically defined herein or the context of their use would clearly suggest otherwise.</p><p id="p-0015" num="0014">Rapid eye movement (REM) is an indicator of deep sleep. The current method measuring rapid eye movement (REM) sleep is electro-oculography (EOG). Electro-oculography (EOG) is cumbersome as it includes contacts being adhered (e.g. glued) to the skin around the eye, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Another method of eye movement tracking uses gel electrodes in a mask that can be pressed on to the face rather than being glued to the skin.</p><p id="p-0016" num="0015">Implementations of the disclosure include using non-contact ultrasound sensors or other distance sensors (e.g. LIDAR) to measure eye movement. When the eyes are closed eye rotations (movements) deform the eyelids. Since the eye is non-spherical (the cornea protrudes), movement of the eye under a closed eyelid can be sensed with by sensing the closed eyelid. By using an array of non-contact ultrasound sensors or LIDAR sensors, the movement of the cornea under the eyelid can be measured without having a sensor contact the eye (as required in electro-oculography (EOG)). The ultrasound transducers or other non-contact sensors may be embedded in a sleep mask like device and the distance measurements generated by the array of sensors may be transmitted to a processing unit (e.g. mobile device) for recording and/or analysis.</p><p id="p-0017" num="0016">Using an array of sensors (e.g. ultrasound or LIDAR) for rapid eye movement (REM) monitoring may provide a lightweight, inexpensive, low power, and more comfortable way for detecting periods of rapid eye movement (REM). Conventional airborne ultrasound sensors are used to measure longer distances. For examples, airborne ultrasound sensors are used in the automobile context to measure meters and have centimeter resolution. The ultrasound sensors typically operate at 40-70 kHz. In implementations of the disclosure, the airborne ultrasound sensors operate at in the megahertz (MHz) range (500 kHz to several MHz) and have micron resolution. In some implementations, the ultrasound sensors operate at approximately 1 megahertz (MHz). In some implementations, the ultrasound sensors operate at approximately 1.7 megahertz (MHz). In addition to having better resolution, the increased frequency from conventional ultrasound sensors may also reduce crosstalk between ultrasound sensors in the array.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates an outside of example mask <b>200</b>. <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> shows that the inside of mask <b>200</b> includes recess cups <b>220</b> for the eye to fit into. An array of sensors <b>233</b>A-<b>2331</b> are disposed in the recess, in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>. The sensors may be airborne ultrasound sensors or other distance sensors. In other implementations, more or fewer sensors <b>233</b> may be included in the array. The sensors <b>233</b> are oriented to emit transmit signals toward and eye or a user that wears mask <b>200</b>. The return signal (reflecting from the eye) may be then received by the sensor and a distance to the eyelid (shaped around the eye) may be determined based on the return signal. Batteries, processing logic, communication transceivers, and other electronic components (not illustrated) may also be included in mask <b>200</b>.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b>A</figref> illustrates sensors <b>233</b>B, <b>233</b>E, and <b>233</b>H in the array emitting transmit signals <b>335</b> toward eye <b>301</b> and receiving return signals <b>337</b> reflected from eyelid <b>303</b> that may be shaped around eye <b>301</b>. Consequently, each sensor <b>233</b> measures a distance to eyelid <b>303</b>. In <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>, eye <b>301</b> may be positioned in a forward-looking direction.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>3</b>B</figref> illustrates eye <b>301</b> positioned in a downward-looking direction. In this downward-looking direction, sensor <b>233</b>E measures a distance to eyelid <b>303</b> that is more than the distance measured in the forward-looking direction of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref> since the center of the cornea is no longer as close to sensor <b>233</b>E. In <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, sensor <b>233</b>H measures a distance to eyelid <b>303</b> that may be less than the distance measured in the forward-looking direction of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. In <figref idref="DRAWINGS">FIG. <b>3</b>B</figref>, sensor <b>233</b>B measures a distance to eyelid <b>303</b> that may be slightly more than the distance measured in the forward-looking direction of <figref idref="DRAWINGS">FIG. <b>3</b>A</figref>. Hence, the downward-position of the eye <b>301</b> can be determined from the distances to eyelid <b>303</b> measured by the array of sensors <b>233</b>. Of course, other distance measurements would correspond to upward-position and sideways positions of the eye <b>301</b>. By capturing positions of the eyelid <b>303</b> with the array of sensors, REM sleep conditions can be detected.</p><p id="p-0021" num="0020">Embodiments of the invention may include or be implemented in conjunction with an artificial reality system. Artificial reality may be a form of reality that has been adjusted in some manner before presentation to a user, which may include, e.g., a virtual reality (VR), an augmented reality (AR), a mixed reality (MR), a hybrid reality, or some combination and/or derivatives thereof. Artificial reality content may include completely generated content or generated content combined with captured (e.g., real-world) content. The artificial reality content may include video, audio, haptic feedback, or some combination thereof, and any of which may be presented in a single channel or in multiple channels (such as stereo video that produces a three-dimensional effect to the viewer). Additionally, in some embodiments, artificial reality may also be associated with applications, products, accessories, services, or some combination thereof, that are used to, e.g., create content in an artificial reality and/or are otherwise used in (e.g., perform activities in) an artificial reality. The artificial reality system that provides the artificial reality content may be implemented on various platforms, including a head-mounted display (HMD) connected to a host computer system, a standalone HMD, a mobile device or computing system, or any other hardware platform capable of providing artificial reality content to one or more viewers.</p><p id="p-0022" num="0021">The term &#x201c;processing logic&#x201d; in this disclosure may include one or more processors, microprocessors, multi-core processors, Application-specific integrated circuits (ASIC), and/or Field Programmable Gate Arrays (FPGAs) to execute operations disclosed herein. In some embodiments, memories (not illustrated) are integrated into the processing logic to store instructions to execute operations and/or store data. Processing logic may also include analog or digital circuitry to perform the operations in accordance with embodiments of the disclosure.</p><p id="p-0023" num="0022">A &#x201c;memory&#x201d; or &#x201c;memories&#x201d; described in this disclosure may include one or more volatile or non-volatile memory architectures. The &#x201c;memory&#x201d; or &#x201c;memories&#x201d; may be removable and non-removable media implemented in any method or technology for storage of information such as computer-readable instructions, data structures, program modules, or other data. Example memory technologies may include RAM, ROM, EEPROM, flash memory, CD-ROM, digital versatile disks (DVD), high-definition multimedia/data storage disks, or other optical storage, magnetic cassettes, magnetic tape, magnetic disk storage or other magnetic storage devices, or any other non-transmission medium that can be used to store information for access by a computing device.</p><p id="p-0024" num="0023">Network may include any network or network system such as, but not limited to, the following: a peer-to-peer network; a Local Area Network (LAN); a Wide Area Network (WAN); a public network, such as the Internet; a private network; a cellular network; a wireless network; a wired network; a wireless and wired combination network; and a satellite network.</p><p id="p-0025" num="0024">Communication channels may include or be routed through one or more wired or wireless communication utilizing IEEE 802.11 protocols, BlueTooth, SPI (Serial Peripheral Interface), I2C (Inter-Integrated Circuit), USB (Universal Serial Port), CAN (Controller Area Network), cellular data protocols (e.g. 3G, 4G, LTE, 5G), optical communication networks, Internet Service Providers (ISPs), a peer-to-peer network, a Local Area Network (LAN), a Wide Area Network (WAN), a public network (e.g. &#x201c;the Internet&#x201d;), a private network, a satellite network, or otherwise.</p><p id="p-0026" num="0025">A computing device may include a desktop computer, a laptop computer, a tablet, a phablet, a smartphone, a feature phone, a server computer, or otherwise. A server computer may be located remotely in a data center or be stored locally.</p><p id="p-0027" num="0026">The processes explained above are described in terms of computer software and hardware. The techniques described may constitute machine-executable instructions embodied within a tangible or non-transitory machine (e.g., computer) readable storage medium, that when executed by a machine will cause the machine to perform the operations described. Additionally, the processes may be embodied within hardware, such as an application specific integrated circuit (&#x201c;ASIC&#x201d;) or otherwise.</p><p id="p-0028" num="0027">A tangible non-transitory machine-readable storage medium includes any mechanism that provides (i.e., stores) information in a form accessible by a machine (e.g., a computer, network device, personal digital assistant, manufacturing tool, any device with a set of one or more processors, etc.). For example, a machine-readable storage medium includes recordable/non-recordable media (e.g., read only memory (ROM), random access memory (RAM), magnetic disk storage media, optical storage media, flash memory devices, etc.).</p><p id="p-0029" num="0028">The above description of illustrated embodiments of the invention, including what is described in the Abstract, is not intended to be exhaustive or to limit the invention to the precise forms disclosed. While specific embodiments of, and examples for, the invention are described herein for illustrative purposes, various modifications are possible within the scope of the invention, as those skilled in the relevant art will recognize.</p><p id="p-0030" num="0029">These modifications can be made to the invention in light of the above detailed description. The terms used in the following claims should not be construed to limit the invention to the specific embodiments disclosed in the specification. Rather, the scope of the invention is to be determined entirely by the following claims, which are to be construed in accordance with established doctrines of claim interpretation.</p><p id="p-0031" num="0030">What has been described and illustrated herein are examples of the disclosure along with some variations. The terms, descriptions, and figures used herein are set forth by way of illustration only and are not meant as limitations. Many variations are possible within the scope of the disclosure, which is intended to be defined by the following claims&#x2014;and their equivalents&#x2014;in which all terms are meant in their broadest reasonable sense unless otherwise indicated.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A head mounted device comprising:<claim-text>an array of ultrasound sensors oriented to emit transmit ultrasounds signals in an eyeward direction, wherein the ultrasound sensors are to receive a return signal of the transmit signal reflecting off of a target, and wherein the ultrasound sensors are to output a distance signal representative of a distance to a target, the distance signal generated based on the return signal; and</claim-text><claim-text>a transceiver to receive the distance signals, wherein the to transceiver is to transmit the distance signals from the array of ultrasound sensors to a remote device.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The head mounted device of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the array of ultrasound sensors is disposed in recessed cups that are sized to fit an ocular region.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. A method of detecting rapid eye movement (REM), the method comprising:<claim-text>emitting signals from an array of sensors in an eyeward direction;</claim-text><claim-text>determining distances to an eyelid in response to return signals, wherein the return signals are the transmit signals reflecting off the eyelid; and</claim-text><claim-text>determining a rapid eye movement (REM) state of a user in response to the distances.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the signals are near-infrared light signals and the return signals are also near-infrared light.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the array of sensors comprises LIDAR sensors.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the array of sensors is disposed in recessed cups that are sized to fit an ocular region.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A head mounted device comprising:<claim-text>an array of sensors oriented to emit transmit signals in an eyeward direction, wherein the sensors are to receive a return signal of the transmit signal reflecting off of a target, and wherein the sensors are to output a distance signal representative of a distance to a target, the distance signal generated based on the return signal; and</claim-text><claim-text>a transceiver to receive the distance signals, wherein the transceiver is to transmit the distance signals from the array of sensors to a remote device.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The head mounted device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the array of sensors is disposed in recessed cups that are sized to fit an ocular region.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The head mounted device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the signals are near-infrared light signals and the return signals are also near-infrared light.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The head mounted device of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the array of sensors comprises LIDAR sensors.</claim-text></claim></claims></us-patent-application>