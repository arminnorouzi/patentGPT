<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000410A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000410</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17366896</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>R</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>165</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>015</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>5</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>4064</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>R</subclass><main-group>2410</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">NEURAL STIMULATION THROUGH AUDIO WITH DYNAMIC MODULATION CHARACTERISTICS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BrainFM, Inc.</orgname><address><city>Brooklyn</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WOODS</last-name><first-name>Kevin J.P.</first-name><address><city>Brooklyn</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>CLARK</last-name><first-name>Daniel</first-name><address><city>Brooklyn</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BrainFM, Inc.</orgname><role>02</role><address><city>Brooklyn</city><state>NY</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Techniques (methods and devices) for neural stimulation through audio with dynamic modulation characteristics are disclosed. The techniques include receiving a mapping of sensor-input values and modulation-characteristic values, wherein each sensor-input value corresponds to a respective modulation-characteristic value; receiving an audio element from an audio source, wherein the audio element comprises at least one audio parameter; identifying an audio-parameter value of the audio parameter; receiving a sensor-input value from a sensor; determining a sensor-input value based on the sensor-input value; selecting from the mapping of sensor-input values and modulation-characteristic values, a modulation-characteristic value that corresponds to the sensor-input value; generating an audio output based on the audio-parameter value and the modulation-characteristic value; and playing the audio output.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="112.44mm" wi="158.75mm" file="US20230000410A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="241.38mm" wi="171.53mm" file="US20230000410A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="241.38mm" wi="154.60mm" file="US20230000410A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="238.68mm" wi="177.12mm" orientation="landscape" file="US20230000410A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="245.11mm" wi="175.60mm" orientation="landscape" file="US20230000410A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="198.88mm" wi="168.66mm" orientation="landscape" file="US20230000410A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="211.92mm" wi="168.66mm" orientation="landscape" file="US20230000410A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is related to U.S. Pat. Nos. 7,674,224, 10,653,857 and U.S. Patent Publication No. 2020/0265827, all of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to neural stimulation, particularly, noninvasive neural stimulation using audio.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">For decades, neuroscientists have observed wave-like activity in the brain called neural oscillations. Various aspects of these oscillations have been related to mental states including attention, relaxation, and sleep. The ability to effectively induce and modify such mental states by noninvasive brain stimulation is desirable.</p><heading id="h-0004" level="1">OVERVIEW</heading><p id="p-0005" num="0004">Certain embodiments disclosed herein enable the modification of modulation characteristics of an audio source to effectively induce and modify brain stimuli to induce desirable mental states.</p><p id="p-0006" num="0005">Some example embodiments include: receiving, by a processing device, a mapping of sensor-input values and modulation-characteristic values, wherein each sensor-input value maps to a respective modulation-characteristic value; receiving, by the processing device, an audio element from an audio source, wherein the audio element comprises at least one audio parameter; identifying, by the processing device, an audio-parameter value of the audio parameter; receiving, by the processing device, a sensor-input value from a sensor; determining, by the processing device, from the mapping of sensor-input values and modulation-characteristic values, a modulation-characteristic value that corresponds to the sensor-input value; generating, by the processing device, an audio output based on the audio-parameter value and the modulation-characteristic value; and playing, by the processing device, the audio output. In some cases, user-associated data can be received by the processing device in addition to the sensor-input value and the determining of the modulation-characteristic value can be based on either or both of sensor-input value or the user-associated data.</p><p id="p-0007" num="0006">In various example embodiments, the modulation-characteristic value may correspond to a modulation characteristic comprising modulation rate, phase, depth, or waveform shape. In example embodiments, the audio source may comprise at least one of an audio signal, digital music file, musical instrument, or environmental sounds. In example embodiments, the audio parameter may comprise at least one of tempo, root mean square energy, loudness, event density, spectrum, temporal envelope, cepstrum, chromagram, flux, autocorrelation, amplitude modulation spectrum, spectral modulation spectrum, attack and decay, roughness, harmonicity, or sparseness.</p><p id="p-0008" num="0007">In some example embodiments, the sensor-input value may correspond to a sensor type comprising at least one of an inertial sensor (e.g., accelerometer, gyrometer, and magnetometer), a microphone, a camera, or a physiological sensor. In example embodiments the physiological sensor may comprise one or more sensors that measure heart rate, blood pressure, body temperature, EEG, MEG, Near infrared (fNIRS), or bodily fluid. In some example embodiments, the receiving of the sensor-input may comprise receiving background noise from the microphone, inertial data from an accelerometer, images from a camera, etc. In some example embodiments, the sensor-input value may correspond to a measure of user activity on a device such as, for example, a smart phone, computer, tablet, or the like. In some example embodiments, the measure of user activity may be the number of, type of, or time applications are being interacted with on the device.</p><p id="p-0009" num="0008">In some example embodiments, generating the mapping of sensor-input values and modulation-characteristic values can be based on a type of sensor and/or a modulation characteristic. In some example embodiments, the mapping of sensor-input values and modulation-characteristic values may be stored in a data table. In some embodiments, the audio output can be transmitted from the processing device to an external device for playback.</p><p id="p-0010" num="0009">In some embodiments, a processing device comprising a processor and associated memory is disclosed. The processor can be configured to: receive, a mapping of sensor-input values and modulation-characteristic values, wherein each sensor-input value corresponds to a respective modulation-characteristic value, receive an audio element from an audio source, wherein the audio element comprises at least one audio parameter, identify an audio-parameter value of the audio parameter, receive a sensor-input value from a sensor, determine from the mapping of sensor-input values and modulation-characteristic values, a modulation-characteristic value that corresponds to the sensor-input value, generate an audio output based on the audio-parameter value and the modulation-characteristic value, and play the audio output.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0011" num="0010">Other objects and advantages of the present disclosure will become apparent to those skilled in the art upon reading the following detailed description of exemplary embodiments and appended claims, in conjunction with the accompanying drawings, in which like reference numerals have been used to designate like elements, and in which:</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a flowchart of a method according to an example embodiment of the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a flowchart of a method according to an example embodiment of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a process flowchart according to an example embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a process flowchart according to an example embodiment of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a functional block diagram of a processing device according to an example embodiment of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example system with various components according to an example embodiment of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0018" num="0017">The figures are for purposes of illustrating example embodiments, but it is understood that the inventions are not limited to the arrangements and instrumentality shown in the drawings. In the figures, identical reference numbers identify at least generally similar elements.</p><heading id="h-0006" level="1">DESCRIPTION</heading><p id="p-0019" num="0018">The present disclosure describes systems, methods, apparatuses and computer executable media configured to vary the modulation characteristics of audio to affect neural activity. Modulation characteristics may include depth of modulation at a certain rate, the rate itself, modulation depth across all rates (i.e., the modulation spectrum), phase at a rate, among others. These modulation characteristics may be from the broadband signal or in sub-bands (e.g., frequency regions, such as bass vs. treble). Audio/audio element, as used herein, can refer to a single audio element (e.g. a single digital file), an audio feed (either analog or digital) from a received signal, or a live recording.</p><p id="p-0020" num="0019">In various exemplary embodiments described herein, the presently disclosed techniques can be effective when audio stimulation is provided by predetermined frequencies, which are associated with known portions of the cochlea of the human ear and may be referenced in terms of the cochlea, or in terms of absolute frequency. Furthermore, the presently disclosed techniques may provide for a selection of modulation characteristics configured to target different patterns of brain activity. These aspects are subsequently described in detail.</p><p id="p-0021" num="0020">In various exemplary embodiments described herein, audio can be modulated according to a stimulation protocol to affect patterns of neural activity in the brain to affect behavior and/or sentiment. Modulation can be added to audio (e.g., mixed) which can in turn be stored and retrieved for playback at a later time. Modulation can be added (e.g., mixed) to audio for immediate (e.g., real-time) playback. Modulated audio playback may be facilitated from a playback device (e.g., smart speaker, headphone, portable device, computer, etc.) and may be single or multi-channel audio. Users may facilitate the playback of the modulated audio through, for example, an interface on a processing device (e.g., smartphone, computer, etc.). These aspects are subsequently described in detail.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates an example method <b>100</b> performed by a processing device (e.g. smartphone, computer, etc.) according to an example embodiment of the present disclosure. The method <b>100</b> may include one or more operations, functions, or actions as illustrated in one or more of blocks <b>110</b>-<b>180</b>. Although the blocks are illustrated in sequential order, these blocks may also be performed in parallel, and/or in a different order than the order disclosed and described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon a desired implementation.</p><p id="p-0023" num="0022">Method <b>100</b> can include a block <b>110</b> of receiving a mapping of sensor-input values and modulation-characteristic values such that each sensor-input value corresponds to a respective modulation-characteristic value. The mapping can be based on a predetermined or real-time computed map. Non-limiting examples of mappings include: a phone with an accelerometer that detects movement and reports an estimate of user productivity and mapping this productivity estimate to modulation depth such that the level of modulation increases if estimated productivity slows down; a mobile device with an accelerometer detects movements and reports user started a run (e.g. by using the CMMotionActivity object of Apple's iOS Core ML framework) which maps to a slight shift in the phase of modulation relative to the phase of the underlying music, at an increased run speed; and a microphone that detects background noise in a particular frequency band (e.g., HVAC noise concentrated in bass frequencies) which maps to increased modulation depth in that sub-band, for masking. In an example embodiment, the mapping can be based on a type of sensor and/or a modulation characteristic. Other examples exist. The mapping can be stored in a data table as shown in the example below in table <b>1</b> or stored as a function, such as, for example, f(x)=x{circumflex over (&#x2003;)}2 where x is the sensor-input value and f(x) is the modulation characteristic value.</p><p id="p-0024" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="84pt" align="center"/><colspec colname="2" colwidth="133pt" align="center"/><thead><row><entry namest="1" nameend="2" rowsep="1">TABLE 1</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>Sensor input values</entry><entry>Modulation-characteristic values</entry></row><row><entry>(High-frequency Heart Rate</entry><entry>(Desired final mixdown modulation</entry></row><row><entry>Variability (HF-HRV), ms)</entry><entry>depth, % normalized re.max)</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="84pt" align="char" char="."/><colspec colname="2" colwidth="133pt" align="char" char="."/><tbody valign="top"><row><entry>20</entry><entry>90</entry></row><row><entry>30</entry><entry>80</entry></row><row><entry>40</entry><entry>70</entry></row><row><entry>50</entry><entry>60</entry></row><row><entry>60</entry><entry>50</entry></row><row><entry>70</entry><entry>40</entry></row><row><entry>80</entry><entry>30</entry></row><row><entry>90</entry><entry>25</entry></row><row><entry>100</entry><entry>22</entry></row><row><entry>110</entry><entry>19</entry></row><row><entry>120</entry><entry>17</entry></row><row><entry>130</entry><entry>15</entry></row><row><entry>140</entry><entry>13</entry></row><row><entry>150</entry><entry>12</entry></row><row><entry>160</entry><entry>11</entry></row><row><entry>170</entry><entry>10</entry></row><row><entry>180</entry><entry>10</entry></row><row><entry>190</entry><entry>10</entry></row><row><entry>200</entry><entry>10</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0025" num="0023">In an example embodiment, modulation rate, phase, depth, and waveform can be four non-exclusive modulation characteristics. Modulation rate can be the speed of the cyclic change in energy, and can be defined, for example, in hertz. Phase is the particular point in the full cycle of modulation, and can be measured, for example, as an angle in degrees or radians. Depth can indicate the degree of amplitude fluctuation in the audio signal. In amplitude modulation, depth can be expressed as a linear percent reduction in signal power or waveform envelope from peak-to-trough, or as the amount of energy at a given modulation rate. Waveform may express the shape of the modulation cycle, such as a sine wave, a triangle wave or some other custom wave. These modulation characteristics can be extracted from the broadband signal or from sub-bands after filtering in the audio-frequency domain (e.g., bass vs. treble), by taking measures of the signal power over time or by calculating a waveform envelope (e.g., the Hilbert envelope).</p><p id="p-0026" num="0024">According to example embodiments, a stimulation protocol may provide one or more of a modulation rate, phase, depth and/or waveform for the modulation to be applied to audio data that can be used to induce neural stimulation or entrainment. Neural stimulation via such a stimulation protocol may be used in conjunction with a cochlear profile to induce different modes of stimulation in a user's brain.</p><p id="p-0027" num="0025">At block <b>120</b>, an audio element is received at the processing device from an audio source. The audio element can be, for example, a digital audio file retrieved by the processing device from local storage on the processing device or from remote storage on a connected device. In an example, the digital audio file is streamed to the processing device from a connected device such as a cloud server for an online music service (e.g., Spotify, Apple Music, etc.). In another example, the audio element may be received by the processing device from an audio input such as a microphone. The audio source can include, for example, an audio signal, digital music file, musical instrument, or environmental sounds. The audio element can be in the form of one or more audio elements read from a storage medium, such as, for example, an MP3 or WAV file, received as an analog signal, generated by a synthesizer or other signal generator, or recorded by one or more microphones or instrument transducers, etc. The audio elements may be embodied as a digital music file (.mp3, .wav, .flac, among others) representing sound pressure values, but could also be a data file read by other software which contains parameters or instructions for sound synthesis, rather than a representation of sound itself. The audio elements may be individual instruments in a musical composition, groups of instruments (bussed outputs), but could also be engineered objects such as frequency sub-bands (e.g., bass frequencies vs treble frequencies). The content of the audio elements may include music, but also non music such as environmental sounds (wind, water, cafe noise, and so on), or any sound signal such as a microphone input.</p><p id="p-0028" num="0026">In an example embodiment, to achieve better brain stimulation, a wide spectrum of audio elements may be used. Accordingly, the audio elements may be selected such that they have a wide (i.e., broadband) spectral audio profile&#x2014;in other words, the audio elements can be selected such that they include many frequency components. For example, the audio elements may be selected from music composed from many instruments with timbre that produces overtones across the entire range of human hearing (e.g., 20-20 kHz).</p><p id="p-0029" num="0027">At block <b>130</b>, an audio-parameter value of the audio parameter can be identified. The audio element may be characterized by one or more audio parameters. For example, audio parameters may include tempo; RMS (root mean square energy in signal); loudness (based on perceptual transform); event density (complexity/business); spectrum/spectral envelope/brightness; temporal envelope (&#x2018;out-line&#x2019; of signal); cepstrum (spectrum of spectrum); chromagram (what pitches dominate); flux (change over time); autocorrelation (self-similarity as a function of lag); amplitude modulation spectrum (how is energy distributed over temporal modulation rates); spectral modulation spectrum (how is energy distributed over spectral modulation rates); attack and decay (rise/fall time of audio events); roughness (more spectral peaks close together is rougher; beating in the ear); harmonicity/inharmonicity (related to roughness but calculated differently); and/or zero crossings (sparseness). One or more of these may be performed, for example, as multi-timescale analysis of features (different window lengths); analysis of features over time (segment-by-segment); broadband or within frequency sub-bands (i.e. after filtering); and/or second order relationships (e.g., flux of cepstrum, autocorrelation of flux).</p><p id="p-0030" num="0028">At block <b>140</b>, a sensor-input value can be received from a sensor. The sensor can be on the processing device or it can be on an external device and data from the sensor can be transferred to the processing device. In one example, the sensor on a processing device, such as an accelerometer on a mobile phone, can be used to determine how often the phone is moved and can be a proxy for productivity. In another example, the sensor on an activity tracker (external device), for e.g. an Oura ring or Apple watch, can be used to detect if the user is awake or not, how much they are moving, etc.</p><p id="p-0031" num="0029">In some embodiments, the sensors can be occasional-use sensors responsive to a user associated with the sensor. For example, a user's brain response to modulation depth can be measured via EEG during an onboarding procedure which may be done per use or at intervals such as once per week or month. In other embodiments, the sensors can be responsive to the user's environment. For example, characterizing the acoustic qualities of the playback transducer (headphones/speakers) or room using a microphone, electrical measurement, an audiogram, or readout of a device ID. The sensors can measure environmental factors that may be perceived by the user such as color, light level, sound, smell, taste, and/or tactile.</p><p id="p-0032" num="0030">In some embodiments, behavioral/performance testing can be used to calibrate the sensors and/or to compute sensor-input values. For example, a short experiment for each individual to determine which depth is best via performance. Similarly, external information can be used to calibrate the sensors and/or to compute sensor-input values. For example, weather, time of day, elevation of the sun at user location, the user's daily cycle/circadian rhythm, and/or location. Calibration tests, such as measuring calibrating depth of modulation in the music to individual users' sound sensitivity based on a test with tones of increasing loudness can also be used to calibrate the sensors and/or to compute sensor-input values. Of course, each of these techniques can be used in combination or separately. A person of ordinary skill in the art would appreciate that these techniques are merely non-limiting examples, and other similar techniques can also be used for calibration of the sensors.</p><p id="p-0033" num="0031">In example embodiments, the sensor-input value can be obtained from one or more sensors such as, for example, an accelerometer (e.g., phone on table registers typing, proxy for productivity); a galvanic skin response (e.g. skin conductance); video (user-facing: eye tracking, state sensing; outward-facing: environment identification, movement tracking); microphone (user-sensing: track typing as proxy for productivity, other self-produced movement; outward-sensing: environmental noise, masking); heart rate monitor (and heart rate variability); blood pressure monitor; body temperature monitor; EEG; MEG (or alternative magnetic-field-based sensing); near infrared (fnirs); or bodily fluid monitors (e.g., blood or saliva for glucose, cortisol, etc). The one or more sensors may include real-time computation. Non-limiting examples of a real-time sensor computation include: the accelerometer in a phone placed near a keyboard on table registering typing movements as a proxy for productivity; an accelerometer detects movements and reports user started a run (e.g. by using the CMMotionActivity object of Apple's iOS Core ML framework), and microphone detects background noise in a particular frequency band (e.g., HVAC noise concentrated in bass frequencies) and reports higher levels of distracting background noise.</p><p id="p-0034" num="0032">In some embodiments, the received sensor-input value can be sampled at pre-defined time intervals, or upon events, such as the beginning of each track or the beginning of a user session or dynamically on short timescales/real-time: (e.g., monitoring physical activity, interaction with phone/computer, interaction with app, etc.).</p><p id="p-0035" num="0033">In an example embodiment, block <b>140</b> can include receiving user- associated data in addition and/or alternatively to the previously described sensor-input value from the sensor (not shown). Alternatively, the block <b>140</b> can include receiving only the sensor-input value or user-associated data.</p><p id="p-0036" num="0034">In example embodiments, user-associated data can include self-report data such as a direct report or a survey, e.g., ADHD self-report (ASRS survey or similar), autism self-report (AQ or ASSQ surveys or similar), sensitivity to sound (direct questions), genre preference (proxy for sensitivity tolerance), work habits re. music/noise (proxy for sensitivity tolerance), and/or history with a neuromodulation. Self-report data can include time-varying reports such as selecting one's level of relaxation once per minute, leading to dynamic modulation characteristics over time in response. User-associated data can include behavioral data/attributes such as user interests, a user's mental state, emotional state, etc. Such information can be obtained from various sources such as the user's social media profile. User-associated data can include factors external to but related to the user such as the weather at the user's location; the time after sunrise or before sunset at the user's location; the user's location; or whether the user is in a building, outdoors, or a stadium.</p><p id="p-0037" num="0035">At block <b>150</b>, a modulation-characteristic value can be determined. In one example, the modulation-characteristic value is selected from the mapping of sensor-input values and modulation-characteristic values that correspond to the sensor-input value. In another example, the modulation-characteristic value can be calculated by applying the sensor-input values to a mapping function (e.g. f(x)=x{circumflex over (&#x2003;)}2 where x is the sensor-input value and f(x) is the modulation characteristic value).</p><p id="p-0038" num="0036">At block <b>160</b>, an audio output is generated based on the audio-parameter value and the modulation-characteristic value. The audio output can be generated by varying one or more of a modulation rate, phase, depth and/or waveform in real-time, at intervals, or upon events, such as the beginning of each track or the beginning of a user session. An example goal of the audio output is to achieve a desired modulation characteristic. Details of performing the block <b>160</b> are described subsequently in the discussion of responsive modulation determination module in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0039" num="0037">At block <b>170</b>, the modulated audio content is played back via one or more audio drivers of one or more playback devices, such as, for example, a smart speaker, a mobile device, a computer/laptop, an ipad, and the like. In one example, the processing device is the same device as the playback device, and the audio content is played via one or more audio drivers on the processing device itself. In another example, the processing device transmits the audio content (e.g., as a digital file over a data network) to a playback device for playback. In another example, the audio content is played back on the processing device as well as one or more other playback devices.</p><p id="p-0040" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates an example method <b>125</b> performed by a processing device (e.g. smartphone, computer, etc.) according to an example embodiment of the present disclosure. Method <b>125</b> depicts generating a mapping of sensor-input values and modulation-characteristic values, as previously discussed in step <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. According to example embodiments of the present disclosure, the method <b>125</b> can be performed by the same processing device that performs the method <b>100</b>. Alternatively, method <b>125</b> can be performed by a different processing device (e.g. smartphone, computer, etc.). The method <b>125</b> may include one or more operations, functions, or actions as illustrated in one or more of blocks <b>112</b>, <b>114</b>, <b>116</b>, and <b>118</b>. Although the blocks are illustrated in sequential order, these blocks may also be performed in parallel, and/or in a different order than the order disclosed and described herein. Also, the various blocks may be combined into fewer blocks, divided into additional blocks, and/or removed based upon a desired implementation.</p><p id="p-0041" num="0039">Method <b>125</b> can include a block <b>112</b>, where a mapping of sensor-input value and modulation-characteristic value can be received and/or generated. Aspects of such a mapping have been previously described with respect to step <b>110</b> of method <b>100</b>.</p><p id="p-0042" num="0040">At block <b>114</b> an indication of a desired mental state of a user is received. Non-limiting examples of a desired mental state can include focus, relax, sleep, and meditate. Each of these example desired mental states can be further distinguished by a target activity and duration. For example, focus can be distinguished by deep work, creative flow, study and read, and light work; relax can be distinguished by chill, recharge, destress, and unwind; sleep can be distinguished by deep sleep, guided sleep, sleep and wake, and wind down; and meditate can be distinguished by unguided and guided. The duration of the mental state may be specified, for example, by a time duration (e.g., minutes, hours, etc.), or a duration triggered by an event (e.g., waking, etc.). The indication may be received via a user interface on a processing device such as, for example, through an interface on the brain.fm&#x2122; application executing on an iPhone&#x2122; or Android&#x2122; device. Alternatively and/or additionally, the indication may be received over a network from a different processing device.</p><p id="p-0043" num="0041">At block <b>116</b>, available sensor inputs can be determined. Available sensor inputs can comprise one or more inputs previously described with respect to block <b>140</b> of method <b>100</b>. At block <b>118</b>, a mapping is selected based on the desired mental state and the available sensor inputs. In some examples, certain sensor inputs may be more applicable to certain desired mental states. For example, a sleep indication from an Oura ring may be more applicable to the sleep mental state than the focused mental state. In another example, an accelerometer on a mobile device may be more applicable to a focus state than a sleep mental state. A person of ordinary skill in the art would appreciate that the aforementioned examples are non-limiting examples, and many such other examples may exist.</p><p id="p-0044" num="0042">In some examples, multiple sensor inputs may be available, and the processing device may select one (or multiple) sensor inputs to map to a modulation characteristic for a desired mental state. For example, a microphone, accelerometer, application monitor, and Oura ring may be a list of available sensor inputs. In one example, the processing device may determine that the microphone should be used with modulation depth for relax mental state, and the accelerometer should be used with modulation frequency for focus mental state. In another example, the processing device may determine that the accelerometer should be used with modulation frequency for meditation mental state and a sleep indicator from the Oura ring should be used with modulation depth for sleep mental state. In some examples, multiple sensors may be determined to be used with a particular modulation characteristic, and the determination of which sensor(s) to use may be determined dynamically based on the information (or lack of information) available from a sensor.</p><p id="p-0045" num="0043"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts an example process flowchart <b>200</b> to combine a modulation characteristic with audio content. Elements may be added or removed from process flow <b>200</b> without deviating from the inventive concepts of the present application.</p><p id="p-0046" num="0044">In an example embodiment, one or more audio elements <b>202</b> can be provided to spectral analyzer module <b>210</b>. Spectral analyzer module <b>210</b> can analyze the frequency components of the one or more audio elements <b>202</b>. Spectral analysis, as used herein, may refer to sonographic representations and mathematical analysis of sound spectra, or by mathematically generated spectra. A spectral analyzer may use one or more of methods known to those skilled in the art, which methods include parametric or non-parametric; real-time or precomputed; assumption-based (e.g., &#x2018;frequency estimation&#x2019; or a priori knowledge about tones, etc.) or non-assumption based (i.e., without any a priori knowledge); time-frequency analysis (representing how the signal changes over time); or spectral analysis (without time). Spectral range, spectral region, or sub-bands can refer to specific bands of frequencies within the spectra. As described in greater detail below, the spectral analyzer module <b>210</b> may be used to determine how the frequency components of the one or more audio elements <b>202</b> can be utilized to implement the non-invasive neural stimulation techniques of the present disclosure.</p><p id="p-0047" num="0045">In an example embodiment, spectral analyzer module <b>210</b> analyzes the frequency components of each of the one or more audio elements <b>202</b>. If it is determined that the one or more audio elements <b>202</b> are composed of a large variety of frequency components across the spectrum, the one or more audio elements <b>202</b> can be sent to the filter queue module <b>211</b>, which is a queue for audio filter module <b>230</b>. Because the stimulation protocol <b>260</b> may be applied to a specific frequency or a relatively narrow range of frequencies, the one or more audio elements <b>202</b> that contain a large variety of frequency components may undergo filtering in the audio filter module <b>230</b> to separate these large varieties of frequency components. For example, audio elements that contain audio from a plurality of instruments may contain audio data with frequency components that cross the audible frequency spectrum. Because the stimulation protocol <b>260</b> can only be applied to a subset of these frequencies, such audio elements are sent to audio filter module <b>230</b>. In other words, the filtering of the audio filter module <b>230</b> selects a frequency range from an audio element for modulation. It will be understood by those skilled in the art, that filter queue <b>211</b> and unfiltered queue <b>212</b> are optional, and audio components may alternatively be processed with, for example, immediate or parallel filtering, or separation.</p><p id="p-0048" num="0046">In an example embodiment, if it is determined that one or more audio elements <b>202</b> has a single frequency component, or if most of the acoustic energy of the one or more audio elements is centered around a narrow band, then the one or more audio elements <b>202</b> are sent to unfiltered queue <b>212</b>. In other words, if the one or more audio elements <b>202</b> are largely constrained to a sufficiently narrow frequency range, the stimulation protocol <b>260</b> may be applied to the entire one or more audio elements <b>202</b>, and therefore, no further filtering would be required. Accordingly, the one or more audio elements <b>202</b> are sent to audio separator module <b>232</b>. Audio separator module <b>232</b> looks at the spectral data of the one or more audio elements and pairs it with a cochlear profile to determine if the one or more audio elements should be modulated or not.</p><p id="p-0049" num="0047">Additionally, spectral data may be sent from spectral analyzer module <b>210</b> to one or more of audio filter module <b>230</b> and audio separator module <b>232</b>. This spectral data may be used, for example, in conjunction with cochlear profile <b>231</b>, to determine which portions of the one or more audio elements <b>202</b> are to be modulated according to stimulation protocol <b>260</b>.</p><p id="p-0050" num="0048">In an example embodiment, both the audio filter module <b>230</b> and audio separator module <b>232</b> can be configured to filter audio elements for modulation (in the case of the audio filter module <b>230</b>) or select audio elements for modulation (in the case of selector <b>232</b>) based upon one or more cochlear profiles <b>231</b>. Cochlear profile <b>231</b> may provide instructions to the audio filter module <b>230</b> and/or audio separator module <b>232</b> based upon frequency ranges that correspond to regions of the cochlea of the human ear. According to an example embodiment, cochlear profile refers to a list of frequency bands to be modulated. Frequencies not included in the list of frequency bands of the cochlear profile can be excluded from modulation. The cochlear profile may apply to many users or be derived from measurements of an individual's hearing.</p><p id="p-0051" num="0049">The frequency data obtained by filtering the one or more audio elements in the audio filter module <b>230</b> can be (i) sent to modulator <b>250</b> for modulation according to stimulation protocol <b>260</b> (line <b>240</b>), or (ii) sent to mixer <b>251</b> without modulation (line <b>242</b>) for recombination with the modulated components for inclusion in a final audio element.</p><p id="p-0052" num="0050">In an example embodiment, audio filter module <b>230</b> may receive instructions from the cochlear profile <b>231</b> for each audio element being filtered. These instructions may indicate which frequency range within the one or more audio elements <b>202</b> are to be modulated; for example, the frequencies corresponding to the less sensitive portions of the human cochlea. In carrying out this operation, audio filter module <b>230</b> may use one or more bandpass filters (or high/low-pass filters) to extract the chosen frequency components for modulation <b>240</b>. According to example embodiments, band stop filters, equalizers, or other audio processing elements known to those skilled in the artmay be used in conjunction with or as an alternative to the band pass filter to separate the contents of filter queue module <b>211</b> into frequency components for modulation <b>240</b> and frequency components that will not receive modulation <b>242</b>.</p><p id="p-0053" num="0051">The audio content for modulation <b>240</b>, <b>243</b> can be passed to modulator <b>250</b> after being filtered by audio filter <b>230</b> or separated out by audio separator <b>232</b> in accordance with cochlear profiles <b>231</b>. The remainder of the frequency components <b>242</b>, <b>244</b> can be passed directly (i.e., unmodulated) to the mixer <b>251</b> where modulated and unmodulated frequency components can be recombined to form a combined audio element <b>252</b>. Similarly, modulated elements <b>254</b> and unmodulated elements <b>244</b>, <b>242</b> (via the mixer as shown in <b>252</b>) can be passed separately into the audio arranger <b>253</b> which also acts as a mixer of concurrent audio. Audio arranger <b>253</b>, described in detail subsequently, can directly receive the one or more audio elements that the system declined to filter or modulate. This process from the spectral analyzer <b>210</b> through to the audio arranger <b>253</b> (where elements are recombined) can be done for each of the one or more audio elements in the filter and unfiltered queue modules (<b>211</b> and <b>212</b>, respectively).</p><p id="p-0054" num="0052">Similarly, audio separator module <b>232</b> may receive instructions from the cochlear profile <b>231</b> selected for each of the one or more audio elements. Based upon the instructions provided by cochlear profile <b>231</b>, audio separator module <b>232</b> may separate the audio elements contained in unfiltered queue <b>212</b> into audio elements to be modulated (line <b>243</b>) and audio elements not to be modulated (line <b>244</b>). Accordingly, audio output from the audio separator <b>232</b> can be (i) sent to modulator <b>250</b> (line <b>243</b>); or (ii) sent to the audio arranger <b>253</b> without modulation (line <b>244</b>) for recombination and inclusion in the final audio output.</p><p id="p-0055" num="0053">In an example embodiment, modulator <b>250</b> may apply stimulation protocol <b>260</b> to the frequency components for modulation <b>240</b> and the audio elements to be modulated <b>243</b>. The stimulation protocol <b>260</b> may specify the duration of the auditory stimulation, as well as the desired stimulation across that timeframe. To control the stimulation, the stimulation protocol <b>260</b> may continually instruct the modulator <b>250</b> as to the rate, depth, waveform, and phase of the modulations.</p><p id="p-0056" num="0054">In an example embodiment, to ensure that the stimulation protocol <b>260</b> aligns with the rhythmic elements of the audio elements being modulated, the phases of the stimulation modulation and the rhythmic elements of the audio element may be aligned. For example, applying 2 Hz modulation to a 120 BPM MP3 file may not align with the rhythmic elements of the MP3 file if the phase of the stimulation modulation is not aligned with the MP3 file. For example, if the maxima of the stimulation modulation is not aligned with the drum beats in the MP3 file, the drum beats would interfere with the stimulation modulation, and the stimulation protocol may cause audio distortion even through the stimulation modulation is being applied with a frequency that matches the rate of a 2 BPM audio element.</p><p id="p-0057" num="0055">Such distortion may be introduced because, for example, MP3 encoding may add silence to the beginning of the encoded audio file. Accordingly, the encoded music would start later than the beginning of the audio file. If the encoded music begins 250 milliseconds after the beginning of the encoded MP3 file, stimulation modulation that is applied at 2 Hz starting at the very beginning of the MP3 file can be 180&#xb0; out of phase with the rhythmic components of the MP3 file. To synchronize the modulations to the beats in the file, the phase of the modulation can be shifted by 180&#xb0;. If the phase of the modulation is adjusted by 180&#xb0;, the modulation cycle can synchronize with the first beat of the encoded music.</p><p id="p-0058" num="0056">In an example embodiment, to ensure that the stimulation modulation aligns with the rhythmic elements of the audio elements being modulated, the audio elements can be provided to a beat detector, an example of which is illustrated as beat detector module <b>220</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Beat detection can be a process of analyzing audio to determine the presence of rhythms and their parameters, such that one can align the rhythms of one piece of audio with the rhythms of another. Accordingly, beat detector module <b>220</b> may detect rhythms in music or rhythmic auditory events in non-music audio. Beat detector module <b>220</b> may detect the phase (peak and trough locations) and rate of the rhythms. Rhythmic information may already be known about the one or more audio elements <b>202</b> through, for example, metadata included in (or associated with) the one or more audio elements <b>202</b>. This rhythmic information may indicate the phase where the rhythm of the audio element begins (e.g., at a particular phase) or that the rhythmic element has a defined rhythm rate (e.g., defined in BPM of the audio element). Beat detector module <b>220</b> may be configured to read or interpret this data included in the one or more audio elements <b>202</b>. Beat detector module <b>220</b> can define an audio element by a single tempo but may also track a changing beat over time.</p><p id="p-0059" num="0057">According to example embodiments, the beat detector module <b>220</b> may be configured to analyze the content of the one or more audio elements to determine information such as the phase and BPM of audio elements <b>202</b>. For example, according to an example, five musical pieces can be selected, and each musical piece can be represented as a WAV file, six minutes long. Beat detector module <b>220</b> may determine that each of the musical pieces has a BPM of 120. Beat detector module <b>220</b> may further determine that each musical piece starts immediately, and therefore, each musical piece has a starting phase of 0. According to other examples, beat detector module <b>220</b> may determine that each musical piece has a silent portion prior to the start of the musical piece, such as the 250-millisecond delay provided by some MP3 encoding. Beat detector module <b>220</b> may detect this delay and convert the time delay into a phase shift of the rhythmic elements of the music based upon the BPM of the musical piece. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the data determined by beat detector module <b>220</b> is provided to stimulation protocol <b>260</b>. This data may be used to ensure that the modulation provided by the stimulation protocol aligns with the rhythmic elements of the audio elements being modulated.</p><p id="p-0060" num="0058">In an example embodiment, stimulation protocol <b>260</b> can be based upon data provided by the beat detector module <b>220</b>, and waveform protocol <b>259</b>. Waveform protocol <b>259</b> can define the modulating waveshape and may be used to shape neural activity more precisely within each cycle, rather than just setting the rate (cycles per second). The waveform shape may be used to target specific patterns of brain activity or specific brain regions by specifying the waveform of the modulation pattern applied to the audio elements being modulated. Entrainment to sound by the brain may vary in strength, extent, and consequence, depending on the shape of the modulation driving the entrainment. Sine waveform modulation may be used if stimulation is intended to target a single frequency of neural oscillations, and more complex waveforms may be used to drive multiple frequencies of neural oscillations.</p><p id="p-0061" num="0059">In an example embodiment, waveform protocol <b>259</b> may be configured to provide waveforms that target specific patterns of activity or areas of the brain. Since the waveform is shaped using the presently disclosed techniques, more complex activity patterns can be targeted or activity in specific regions of the brain can be altered. Neural oscillatory waveforms may differ dramatically depending on the region of the brain being measured; different regions of the brain exhibit different waveform shapes in their neural oscillations. Even if two brain regions are firing at the exact same rate, the purpose of the oscillation may be different, and the different purpose may be expressed through different waveforms. Matching the waveform of the stimulation to the brain region being targeted may enhance the effectiveness of neural stimulation and may enhance the targeting of specific brain regions. Similarly, the waveform shape can be tuned to elicit activity patterns (measured at one point or many across the brain) different from those elicited by a sine-modulated waveform at the same rate.</p><p id="p-0062" num="0060">In an example embodiment, once a stimulation protocol <b>260</b> has been generated, the protocol that may take into account the output of one or more of beat detector module <b>202</b> and waveform protocol <b>259</b>. The stimulation protocol <b>260</b> can be provided to modulator <b>250</b>. The stimulation protocol <b>260</b> may specify the duration of the auditory stimulation, as well as the desired stimulation across that timeframe. To control the stimulation, the stimulation protocol <b>260</b> may continually instruct the modulator <b>250</b> as to the rate, depth, waveform and phase of the modulations. As described previously, the stimulation protocol <b>260</b> may instruct the modulator <b>250</b> based upon the output of beat detector module <b>220</b> to ensure the rates are multiples or factors of the BPM measured by rhythmic content in the audio elements <b>202</b>. A modulation waveform may be specified in the waveform protocol <b>259</b> and used to effect neural oscillatory overtones and/or to target specific brain regions, which can be provided to the modulator <b>250</b> via stimulation protocol <b>260</b>. Finally, modulation phase control of the modulator <b>250</b> may be provided by stimulation protocol <b>260</b> based upon the beat detector module <b>220</b> ensuring the phase of modulation matches the phase of rhythmic content in the one or more audio elements <b>202</b>. Modulation depth control can be used to manipulate the intensity of the stimulation.</p><p id="p-0063" num="0061">In an example embodiment, a response modulation determination module (RMD) <b>280</b> may determine what parameters to pass to various elements in the audio processing chain which may include a modulator <b>250</b>, mixer <b>251</b>, audio arranger <b>253</b>, stimulation protocol <b>260</b>, or other audio processing modules. The RMD <b>280</b> may control these parameters in a smooth coordinated manner, given input <b>270</b> transformed by mapping function <b>271</b> into a single-dimensional desired modulation characteristic (e.g., low-to-high modulation depth at a particular rate) which can vary over time. The goal of the RMD <b>280</b> is that the audio output after the final mixdown achieves the desired modulation characteristic over the range required by the input <b>270</b> via the map <b>271</b>, as previously discussed with respect to block <b>160</b>. The RMD <b>280</b> may calculate how to achieve this given the processed audio elements (or knowledge of these elements sufficient to estimate a solution), which are analogous to arrows <b>240</b>-<b>244</b> which go to the modulator, mixer, and arranger, but are also provided to the RMD to calculate the required path <b>380</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Instead of exact copies of audio as passed by <b>240</b>-<b>244</b>, summary information such as sub-band envelopes, or subsamples, may be passed instead.</p><p id="p-0064" num="0062">The RMD <b>280</b> can have knowledge of and control over possible parameterizations of the stimulation protocol <b>260</b>, modulator <b>250</b>, mixer <b>251</b>, and/or audio arranger <b>253</b> (together these parameters represent a high-dimensional input space to the RMD <b>280</b>). The RMD <b>280</b> may define a path through the input space that results in a smooth increase in the output (the value of a particular modulation characteristic at the final mixdown), for example, by sampling two or more points in the input-output space and inferring the input points between them via interpolation or the like. Once the path through input space is defined by the RMD <b>280</b>, its role is simply to transfer the values from the input <b>270</b> and map <b>271</b> to the modulator <b>250</b>, mixer <b>251</b>, and/or audio arranger <b>253</b>. Those skilled in the art will see that there are alternate ways in which RMD <b>280</b> may optimize parameters using, for example, linear regression, machine learning, or a map.</p><p id="p-0065" num="0063">The RMD <b>280</b> may find this optimal path under various constraints, which may be implemented as cost functions by a constraints checker (subsequently described with respect to <figref idref="DRAWINGS">FIG. <b>3</b></figref>). These may include reducing overall dissimilarity (other than modulation) between pre- and post-processed audio, having all audio elements contribute to the final mixdown in some form, reducing impact on other aspects of the music's beat patterns, preserving relationships between groups of audio elements, and other constraints which may affect the aesthetic or neural-effective value of the output, up to predefined tolerance levels. In an example embodiment, a constraints checker may impose a cost function on various aspects of the analyzed output from audio analyzer <b>215</b>, which can be taken into consideration by the receiving module (maximum-finder or path-creator) in deciding whether an input-output pair is acceptable in the final path.</p><p id="p-0066" num="0064">Constraints can be used to impose alternative goals for the output audio. For example, adding a cost for brightness (high-frequency energy which may be undesirable to the listener), may find parameter solutions that balance the desired modulation characteristic against that constraint, for example by mixing down a cymbal track even though it adds to modulation depth (thus reducing the maximum possible modulation depth for the song). The cochlear profile <b>231</b> used to separate and filter audio elements with respect to their frequency ranges can also be used by the RMD <b>280</b> as a constraint in this way, for example by penalizing output mixes with too much energy in particular frequency ranges. Thus, those skilled in the art will recognize that the RMD <b>280</b> as described may be useful not only for the responsive determination of modulation but also for any other characteristic or features of audio, for example, brightness.</p><p id="p-0067" num="0065"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts an example process flowchart illustrating the interaction between the RMD <b>280</b> and the map <b>271</b>, audio analyzer module <b>215</b>, modulator <b>250</b>, mixer <b>251</b>, audio arranger <b>253</b>, audio filter <b>230</b>, cochlear profile <b>231</b>, audio separator <b>232</b>, and stimulation protocol <b>260</b>. In various example embodiments, the RMD <b>280</b> may sample the input-output space by making use of the full processing chain with a feedback loop <b>295</b>. The RMD <b>280</b> can contain a separate system to do similar computations. For example, samples of the audio element of at least 100 ms (capturing a 10 Hz cycle) or longer may be used. Alternatively, samples of up to 1 s (1000 ms) may be sufficient to achieve a good estimate of modulation characteristics.</p><p id="p-0068" num="0066">In an example embodiment, analytic methods (function approximation <b>370</b>) may be used to solve for the whole input-output space, instead of using the full processing chain to sample the space point-by-point. For example, since sound combines linearly and the modulating waveshapes are slow relative to the underlying audio signal, it can be efficient to model how the audio combines just at the modulated peaks and troughs, ignoring the majority of the modulating cycle and underlying audio. Alternatively, operating on subsamples of the audio signal can be an efficient way to estimate the input-output relationships that would result from the full processing chain. This may give the full input-output space at some resolution and the global maximum output could be selected (and corresponding inputs known). Analytic methods like this can be used to set the initial input to the RMD <b>280</b>, or in lieu of the processing chain entirely (to establish the RMD <b>280</b> path without full audio processing or simulation).</p><p id="p-0069" num="0067">The role of the RMD <b>280</b> can be to find a path of two or more points through the high-dimensional space of inputs to modulator <b>250</b> and/or mixer <b>251</b>, that results in a smooth change in the desired modulation characteristics (depth, rate, spectrum, etc.) after the final mixdown <b>290</b>, while satisfying constraints like reducing changes to the original audio outside of its modulation characteristics. This path (high-D to 1-D map) can be established by first establishing a putative maximum and minimum for the path, then interpolating between these points in input space (and/or extrapolating if needed). This path can also be defined by other means like sampling a large number of input space points or can be guided by human input for its initialization.</p><p id="p-0070" num="0068">In an example embodiment, the RMD <b>280</b> may start by asking what is the heaviest amount of modulation that can be delivered under the constraints specified, which will include aesthetic constraints. This is a putative maximum output value, and maps to a specific point on the input space. Finding this value can be the role of the maximal modulation finder <b>310</b>. This can be done by any number of optimization methods such as coordinate or gradient descent, branch-and-bound methods, or others. Heuristic methods can also be used that include knowledge of the problem space, such as special rules (e.g., &#x2018;never turn down the bassiest element&#x2019;). The maximal modulation finder can use knowledge of aesthetic tolerances (what sort of signal is unacceptable for the listener) which can come from models of psychophysical data or by testing listeners directly. The maximal modulation finder <b>310</b> may finally accept an input vector (i.e., a point in high-dimensional space) that produces the highest output modulation value balanced against constraints which might be imposed as a cost function on other features measured by audio analyzer <b>215</b> such as roughness, distortion, difference from original, or other constraints. This input is taken as point A and passed to the path-creator as a putative maximum, along with other input-output points, likely including a putative minimum.</p><p id="p-0071" num="0069">In some embodiments, the putative minimum modulation level may in a simple case be defined as &#x2018;no added modulation&#x2019; (input vector at [0, . . . , 0]), or the minimum level might be defined by a point in the input space where the output modulation characteristic (e.g., energy at a particular modulation rate) is yet lower than the unprocessed case. For example, this could involve up-mixing audio elements with rates other than the target, such that the overall modulation spectrum has a dip at the target, and this input setting (controlling the mixer) is defined as the RMD minimum. Such a global minimum can be found with a similar constrained optimization process to that used to find the global maximum.</p><p id="p-0072" num="0070">In an example embodiment, these putative max and min modulation levels defined by the RMD <b>280</b> may be defined as corresponding to the max and min values in the range of values taken by map <b>271</b>, or as some subsection or super-section along that dimension. Then, output &#x2018;final mixdowns&#x2019; with input-map values between these extremes can be obtained by setting the inputs to midway along the line connecting them in input space (interpolation). This can be a role of the path-creation module <b>360</b>. More broadly the path-creation module may take two or more points in a high-dimensional input space (that have been accepted or refined as good input points to have along the path) and creates a one-dimensional path through the high-dimensional space via interpolation, extrapolation, or any other inference process. This may allow one-dimensional control of the high-dimensional inputs, which can then be smoothly varied over this range.</p><p id="p-0073" num="0071">In some cases, a simple linear interpolation through the input space may not produce a smoothly-changing output modulation level, and so points along and nearby the interpolated line can be sampled to determine the smoothness of the output (measured modulation characteristics) after final mixdown, and the input-output path can be warped accordingly by the RMD <b>280</b> (this is depicted inside the RMD <b>280</b> in the &#x2018;Refine&#x2019; loop <b>340</b>). This process of optimization along the path can involve finding best solutions through machine learning methods including gradient descent.</p><p id="p-0074" num="0072">In an example embodiment, the RMD <b>280</b> may require knowledge of the stimulation protocol (the applied modulation), but there are aspects of the stimulation protocol that may be superseded by the RMD <b>280</b>, since the input <b>270</b> (e.g., from sensors) may be used to dictate modulation characteristics usually imposed by the stimulation protocol, such as modulation depth or rate. In a sense the RMD <b>280</b> functions partly as an automatic responsive stimulation protocol module (in that it changes modulation characteristics over time). The RMD <b>280</b> and stimulation protocol <b>260</b> may both control the same parameter of modulator <b>250</b>, for example the applied modulation depth. sensor-input value <b>270</b> may then be &#x2018;additional&#x2019; to the underlying stimulation protocol. Or, since the stimulation protocol is part of the processing-feedback loop training the RMD <b>280</b>, the RMD <b>280</b> may effectively negate the stimulation protocol and produce only the output required by map <b>271</b>. In an example embodiment, the stimulation protocol <b>260</b> may be barred from controlling parameters controlled by the RMD.</p><p id="p-0075" num="0073">In example embodiments, the input-output problem solved by the RMD <b>280</b> under constraints can be approached by any number of optimization methods (including those involving machine learning or adaptive algorithms). Two simple methods of exploring the input-output space and estimating global minima and maxima can be: Modulation determination by trial-and-error (random search), and modulation determination by an optimization algorithm.</p><p id="p-0076" num="0074">With a trial-and-error method, the RMD <b>280</b> may for example start with the understanding that the values it gets from map <b>271</b> are exactly the values that should be passed to the modulator <b>250</b> and/or mixer <b>251</b>, and/or audio arranger <b>253</b>. After being processed through to the final mixdown <b>290</b>, a feedback loop <b>295</b> can be used by the RMD <b>280</b> to detect that the final output is not as desired, but now it has two input-output reference points. From here, depending on these points, the system may extrapolate to infer further input-output points along that axis, or may decide to take a different direction, for example if the detriment to the original audio was too great (or some other constraint was violated). When an acceptable vector is found, extrapolation can continue up to the limit of constraints, thus defining the maximum value (heaviest modulation level to allow).</p><p id="p-0077" num="0075">With the optimization algorithm, instead of starting with a random vector, the RMD <b>280</b> may find its single best estimate of a global maximum: what settings produce the heaviest acceptable modulation? This may be done by sampling the space widely and seeding a local search algorithm such as nearest-neighbor or gradient descent; machine learning methods can be used in aid of this optimization problem. These processes including trial-and-error method and optimization algorithm may be implemented in maximum modulation finder <b>310</b> and may use the whole processing chain to sample the space, and/or contain internal simulations or function approximations. Once a solution is found, interpolation through the input space from [0, . . . , 0] to this global maximum [X, . . . , Z] may map to the range of the modulation characteristic demanded by the sensor-input value, e.g., modulation depth 0-100%. In this case, the system may, for example, infer via linear interpolation that modulation depth of 50% is produced when the modulator and mixer inputs are set to [X/2, . . . , Z/2].</p><p id="p-0078" num="0076">The RMD <b>280</b> may perform path-determination at various intervals, for example a single input-output path could apply for the entire duration of the original audio, essentially assuming any unsampled durations are similar to any sampled durations. Or, for example, operating in a continuous fashion and changing over time as the audio does. For recorded audio elements this might involve time-windowing and modifying the input-output function for each window. If working in real time such a system may require a buffering period of at least one modulation-window (100 ms-1 s or more) to determine how to change the inputs to the modulator <b>250</b> and/or mixer <b>251</b> to account for changes in the underlying audio elements. In a &#x2018;monitoring mode&#x2019; audio analysis <b>215</b> may run through the feedback loop <b>295</b> continuously or at set intervals, and the RMD <b>280</b> may only update if there is a sufficient discrepancy between expected and observed levels of modulation (e.g., as a musical piece changes). In a resource-unlimited mode, the RMD <b>280</b> may treat each time-window separately and find the best parameters to solve for the desired output modulation in a continuous manner (second-by-second).</p><p id="p-0079" num="0077">The modulator <b>250</b> may use a low-frequency oscillator, which contains ongoing rate, phase, depth, and waveform instruction. Low frequency oscillation (LFO) is a technique where a oscillator, that operates at a lower frequency than the signal being modulated, modulates the audio signal, thus causing a difference to be heard in the signal without the actual introduction of another audio source. An LFO is often used by electronic musicians to add vibrato or various effects to a melody. In this case it can be used to affect modulation characteristics, for example modulating the amplitude, frequency, stereo panning or filters according to the stimulation protocol <b>260</b> or control signals from the RMD <b>280</b>.</p><p id="p-0080" num="0078">The modulator <b>250</b> can be used to modulate frequency components <b>240</b> and unfiltered audio elements <b>243</b>. Frequency components <b>240</b> may be modulated and then mixed with their counterpart unmodulated components <b>242</b> in mixer <b>251</b> to produce final filtered, modulated audio elements <b>252</b>, which are then sent to the audio arranger <b>253</b>. Audio elements <b>243</b>, on the other hand, are modulated in full, so they need not be remixed, and are therefore sent directly to the audio arranger <b>253</b>.</p><p id="p-0081" num="0079">An audio arranger <b>253</b> can be a device or process that allows a user to define a number of audio components to fill an audio composition with music wherever the score has no implicit notes. Accordingly, in an example embodiment, an audio arranger <b>253</b> may arrange all audio content across the timeline of the stimulation protocol <b>260</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, stimulation protocol <b>260</b> may send its timeframe to the audio arranger <b>253</b>. In this embodiment, audio arranger <b>253</b> creates the final audio arrangement. The audio arranger <b>253</b> can be used to ensure that modulated content is always present and is always coupled with unmodulated content. Filtered, modulated audio elements <b>252</b> automatically contain modulated and unmodulated content, but the audio arranger <b>253</b> would still arrange them for maximum coverage across the timeline. Modulated audio elements <b>254</b> and unmodulated audio elements <b>244</b> may be arranged such that a modulated element is always paired with an unmodulated element, such that there are always at least two elements present throughout the timeline. Since the audio arranger <b>253</b> also mixes concurrent audio elements, it also functions as a mixer and in an example embodiment may be controlled by the RMD <b>280</b> as the modulator <b>250</b> and mixer <b>251</b> are, since its parameter settings affect the final output characteristics.</p><p id="p-0082" num="0080">The audio arranger <b>253</b> may take component elements, then replicate and distribute them over arbitrarily long timescales. Input from the user (human &#x2018;composer&#x2019;) might include: Density of elements in time (i.e., spacing in time), density of concurrent elements (i.e., spacing not-in-time), spatialization (e.g., panning, virtual movement), variability introduced across the elements (e.g., automated changes in key, tempo, or other musical or acoustic features), and change-over-time of the above (e.g., user-defined trajectories over input space).</p><p id="p-0083" num="0081">According to an example embodiment, elements may be placed by evaluating their musical or acoustic features, determining whether there will be conflicts, and avoiding conflicts. Elements that are well suited for arrangement may be determined based on relationships (e.g., temporal, or spectral). For example, elements that overlap in frequency and thus mask each other or interact on the cochlea, may be disallowed to co-occur in time.</p><p id="p-0084" num="0082">Once arrangement is complete, the arranged audio element can be sent to the final mixdown <b>290</b> which may provide a final mixdown and encodes the full audio onto an electronic medium. Final mixdown may refer to the final output of a multi-track audio arrangement. A multitrack recording has more than one individual track, or more than one piece of audio layered on top of another, to be played simultaneously. The final output of multitrack audio can also be referred to as the mixdown. The mixdown can optionally be fed back to audio analyzer <b>215</b> to form a feedback loop <b>295</b> whereby RMD <b>280</b> can iteratively approach optimal modulation characteristics.</p><p id="p-0085" num="0083"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a functional block diagram of an example processing device <b>400</b> that can implement the previously described method <b>100</b>. The processing device <b>400</b> includes one or more processors <b>410</b>, software components <b>420</b>, memory <b>430</b>, one or more sensor inputs <b>440</b>, audio processing components (e.g. audio input) <b>450</b>, a user interface <b>460</b>, a network interface <b>470</b> including wireless interface(s) <b>472</b> and/or wired interface(s) <b>474</b>, and a display <b>480</b>. The processing device may further optionally include audio amplifier(s) and speaker(s) for audio playback. In one case, the processing device <b>400</b> may not include the speaker(s), but rather a speaker interface for connecting the processing device to external speakers. In another case, the processing device <b>400</b> may include neither the speaker(s) nor the audio amplifier(s), but rather an audio interface for connecting the processing device <b>400</b> to an external audio amplifier or audio-visual playback device.</p><p id="p-0086" num="0084">In some examples, the one or more processors <b>410</b> include one or more clock-driven computing components configured to process input data according to instructions stored in the memory <b>430</b>. The memory <b>430</b> may be a tangible, non-transitory computer-readable medium configured to store instructions executable by the one or more processors <b>410</b>. For instance, the memory <b>430</b> may be data storage that can be loaded with one or more of the software components <b>420</b> executable by the one or more processors <b>410</b> to achieve certain functions. In one example, the functions may involve the processing device <b>400</b> retrieving audio data from an audio source or another processing device. In another example, the functions may involve the processing device <b>400</b> sending audio data to another device or a playback device on a network.</p><p id="p-0087" num="0085">The audio processing components <b>450</b> may include one or more digital-to-analog converters (DAC), an audio preprocessing component, an audio enhancement component or a digital signal processor (DSP), and so on. In one embodiment, one or more of the audio processing components <b>450</b> may be a subcomponent of the one or more processors <b>410</b>. In one example, audio content may be processed and/or intentionally altered by the audio processing components <b>450</b> to produce audio signals. The produced audio signals may be further processed and/or provided to an amplifier for playback.</p><p id="p-0088" num="0086">The network interface <b>470</b> may be configured to facilitate a data flow between the processing device <b>400</b> and one or more other devices on a data network, including but not limited to data to/from other processing devices, playback devices, storage devices, and the like. As such, the processing device <b>400</b> may be configured to transmit and receive audio content over the data network from one or more other devices in communication with the processing device <b>400</b>, network devices within a local area network (LAN), or audio content sources over a wide area network (WAN) such as the Internet. The processing device <b>400</b> may also be configured to transmit and receive sensor input over the data network from one or more other devices in communication with the processing device <b>400</b>, network devices within a LAN or over a WAN such as the Internet. The processing device <b>400</b> may also be configured to transmit and receive audio processing information such as, for example, a sensor-modulation-characteristic table over the data network from one or more other devices in communication with the processing device <b>400</b>, network devices within a LAN or over a WAN such as the Internet.</p><p id="p-0089" num="0087">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the network interface <b>470</b> may include wireless interface(s) <b>472</b> and wired interface(s) <b>474</b>. The wireless interface(s) <b>472</b> may provide network interface functions for the processing device <b>400</b> to wirelessly communicate with other devices in accordance with a communication protocol (e.g., any wireless standard including IEEE 802.11a/b/g/n/ac, 802.15, 4% mobile communication standard, and so on). The wired interface(s) <b>474</b> may provide network interface functions for the processing device <b>400</b> to communicate over a wired connection with other devices in accordance with a communication protocol (e.g., IEEE802.3). While the network interface <b>470</b> shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> includes both wireless interface(s) <b>472</b> and wired interface(s) <b>474</b>, the network interface <b>470</b> may in some embodiments include only wireless interface(s) or only wired interface(s).</p><p id="p-0090" num="0088">The processing device may include one or more sensor(s) <b>440</b>. The sensors <b>440</b> may include, for example, inertial sensors (e.g., accelerometer, gyrometer, and magnetometer), a microphone, a camera, or a physiological sensor such as, for example, a sensor that measures heart rate, blood pressure, body temperature, EEG, MEG, Near infrared (fNIRS), or bodily fluid. In some example embodiments, the sensor may correspond to a measure of user activity on a device such as, for example, a smart phone, computer, tablet, or the like.</p><p id="p-0091" num="0089">The user interface <b>460</b> and display <b>480</b> can be configured to facilitate user access and control of the processing device. Example user interface <b>460</b> include a keyboard, touchscreen on a display, navigation device (e.g., mouse), etc.</p><p id="p-0092" num="0090">The processor <b>410</b> may be configured to receive a mapping of sensor-input values and modulation-characteristic values, wherein each sensor-input value corresponds to a respective modulation-characteristic value. This aspect is similar to block <b>110</b> of the method <b>100</b>. The processor <b>410</b> may be configured to receive an audio input from an audio source (not shown), wherein the audio input comprises at least one audio element, each comprising at least one audio parameter. This aspect is similar to block <b>120</b> of the method <b>100</b>.</p><p id="p-0093" num="0091">The processor <b>410</b> may be configured to identify an audio-parameter value of the audio parameter. This aspect is similar to block <b>130</b> of the method <b>100</b>. The processor <b>410</b> may be configured to receive a sensor input <b>440</b> from a sensor (not shown). This aspect is similar to block <b>140</b> of the method <b>100</b>.</p><p id="p-0094" num="0092">The processor <b>410</b> may be configured to select from the mapping of sensor-input values and modulation-characteristic values, a modulation-characteristic value that corresponds to the sensor-input value. This aspect is similar to block <b>150</b> of the method <b>100</b>.</p><p id="p-0095" num="0093">The processor <b>410</b> may be configured to generate an audio output based on the audio-parameter value and the modulation-characteristic value. This aspect is similar to block <b>160</b> of the method <b>100</b>. The processor <b>410</b> may be configured to play the audio output. This aspect is similar to block <b>170</b> of the method <b>100</b>.</p><p id="p-0096" num="0094">Aspects of the present disclosure may exist in part or wholly in, distributed across, or duplicated across one or more physical devices. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates one such example system <b>500</b> in which the present invention may be practiced. The system <b>500</b> illustrates several devices (e.g., computing device <b>510</b>, audio processing device <b>520</b>, file storage <b>530</b>, playback device <b>550</b>, <b>560</b>, and playback device group <b>570</b>) interconnected via a data network <b>505</b>. Although the devices are shown individually, the devices may be combined into fewer devices, separated into additional devices, and/or removed based upon an implementation. The data network <b>505</b> may be a wired network, a wireless network, or a combination of both.</p><p id="p-0097" num="0095">In some example embodiments, the system <b>500</b> can include an audio processing device that can perform various functions, including but not limited to audio processing. In an example embodiment, the system <b>500</b> can include a computing device <b>510</b> that can perform various functions, including but not limited to, aiding the processing by the audio processing device <b>520</b>. In an example embodiment, the computing devices <b>510</b> can be implemented on a machine such as the previously described system <b>500</b>.</p><p id="p-0098" num="0096">In an example embodiment, the system <b>500</b> can include a storage <b>530</b> that is connected to various components of the system <b>500</b> via a network <b>505</b>. The connection can also be wired (not shown). The storage <b>530</b> can be configured to store data/information generated or utilized by the presently described techniques. For example, the storage <b>530</b> can store the mapping of sensor-input values and modulation-characteristic values, as previously discussed with respect to the step <b>110</b>. The storage <b>530</b> can also store the audio output generated in the step <b>170</b>.</p><p id="p-0099" num="0097">In an example embodiment, the system <b>500</b> can include one or more playback devices <b>550</b>, <b>560</b> or a group of playback devices <b>570</b> (e.g. playback devices, speakers, mobile devices, etc.). These devices can be used to playback the audio output, as previously described in the step <b>180</b>. In some example embodiments, a playback device may include some or all of the functionality of the computing device <b>510</b>, the audio processing device <b>520</b>, and/or the file storage <b>530</b>. As described previously, a sensor can be based on the audio processing device <b>520</b> or it can be an external sensor device <b>580</b> and data from the sensor can be transferred to the audio processing device <b>520</b>.</p><p id="p-0100" num="0098">Additional examples of the presently described method and device embodiments are suggested according to the structures and techniques described herein. Other non-limiting examples may be configured to operate separately or can be combined in any permutation or combination with any one or more of the other examples provided above or throughout the present disclosure.</p><p id="p-0101" num="0099">It will be appreciated by those skilled in the art that the present disclosure can be embodied in other specific forms without departing from the spirit or essential characteristics thereof. The presently disclosed embodiments are therefore considered in all respects to be illustrative and not restricted. The scope of the disclosure is indicated by the appended claims rather than the foregoing description and all changes that come within the meaning and range and equivalence thereof are intended to be embraced therein.</p><p id="p-0102" num="0100">It should be noted that the terms &#x201c;including&#x201d; and &#x201c;comprising&#x201d; should be interpreted as meaning &#x201c;including, but not limited to&#x201d;. If not already set forth explicitly in the claims, the term &#x201c;a&#x201d; should be interpreted as &#x201c;at least one&#x201d; and &#x201c;the&#x201d;, &#x201c;said&#x201d;, etc. should be interpreted as &#x201c;the at least one&#x201d;, &#x201c;said at least one&#x201d;, etc. Furthermore, it is the Applicant's intent that only claims that include the express language &#x201c;means for&#x201d; or &#x201c;step for&#x201d; be interpreted under 35 U.S.C. 112(f). Claims that do not expressly include the phrase &#x201c;means for&#x201d; or &#x201c;step for&#x201d; are not to be interpreted under 35 U.S.C. 112(f).</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method comprising:<claim-text>receiving, by a processing device, a mapping of sensor-input values and acoustic modulation-characteristic values, wherein each sensor-input value in the mapping corresponds to a respective acoustic modulation-characteristic value in the mapping;</claim-text><claim-text>receiving, by the processing device, an audio element from an audio source, wherein the audio element comprises at least one audio parameter;</claim-text><claim-text>identifying, by the processing device, an audio-parameter value of the at least one audio parameter;</claim-text><claim-text>receiving, by the processing device, a sensor-input value from a sensor;</claim-text><claim-text>determining, by the processing device, from the mapping of sensor-input values and acoustic modulation-characteristic values, an acoustic modulation-characteristic value that corresponds to the sensor-input value received from the sensor;</claim-text><claim-text>generating, by the processing device, an audio output by modifying the audio element based on the audio-parameter value and the acoustic modulation-characteristic value; and</claim-text><claim-text>playing, by the processing device, the audio output.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the acoustic modulation-characteristic value corresponds to an acoustic modulation characteristic comprising modulation rate, phase, depth, or waveform shape.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the audio source comprises at least one of an audio signal, digital music file, musical instrument, or environmental sounds.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the audio parameter comprises at least one of tempo, root mean square energy, loudness, event density, brightness, temporal envelope, cepstrum, chromagram, flux that indicates change in an audio frequency spectrum over time, autocorrelation that indicates a correlation of the audio element or a feature extracted from the audio element with itself as a function of lag, amplitude modulation spectrum, spectral modulation spectrum, attack and decay, roughness, harmonicity, or sparseness of the audio element.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sensor comprises at least one of an accelerometer, a microphone, a camera, or a physiological sensor.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the physiological sensor comprises one or more sensors that measure heart rate, blood pressure, body temperature, EEG, MEG, Near infrared (fNIRS), or bodily fluid.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the receiving of the sensor-input value from the sensor comprises receiving background noise from the microphone.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>generating the mapping of sensor-input values and acoustic modulation-characteristic values based on a type of sensor and/or an acoustic modulation characteristic.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, comprising:<claim-text>storing the mapping of sensor-input values and acoustic modulation-characteristic values in a data table.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>transmitting the audio output to an external device.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A device comprising a processor and associated memory, the processor being configured to:<claim-text>receive, a mapping of sensor-input values and acoustic modulation-characteristic values, wherein each sensor-input value in the mapping corresponds to a respective acoustic modulation-characteristic value in the mapping,</claim-text><claim-text>receive an audio element from an audio source, wherein the audio element comprises at least one audio parameter,</claim-text><claim-text>identify an audio-parameter value of the at least one audio parameter,</claim-text><claim-text>receive a sensor-input value from a sensor,</claim-text><claim-text>determine from the mapping of sensor-input values and acoustic modulation-characteristic values, an acoustic modulation-characteristic value that corresponds to the sensor-input value received from the sensor,</claim-text><claim-text>generate an audio output by modifying the audio element based on the audio-parameter value and the acoustic modulation-characteristic value, and</claim-text><claim-text>play the audio output.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the acoustic modulation-characteristic value corresponds to an acoustic modulation characteristic comprising modulation rate, phase, depth, or waveform shape.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the audio source comprises at least one of an audio signal, digital music file, musical instrument, or environmental sounds.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the audio parameter comprises at least one of tempo, root mean square energy, loudness, event density, brightness, temporal envelope, cepstrum, chromagram, flux that indicates change in an audio frequency spectrum over time, autocorrelation that indicates a correlation of the audio element or a feature extracted from the audio element with itself as a function of lag, amplitude modulation spectrum, spectral modulation spectrum, attack and decay, roughness, harmonicity, or sparseness of the audio element.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the sensor comprises at least one of an accelerometer, a microphone, a camera, or a physiological sensor.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the physiological sensor comprises one or more sensors that measure heart rate, blood pressure, body temperature, EEG, MEG, Near infrared (fNIRS), or bodily fluid.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the processor is further configured to generate the mapping of sensor-input values and acoustic modulation-characteristic values based on a type of sensor and/or an acoustic modulation characteristic.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the processor is further configured to store the mapping of sensor-input values and acoustic modulation-characteristic values in a data table.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the processor is further configured to transmit the audio output to an external device.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. (canceled)</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. A non-transitory computer readable medium for storing instructions which when executed by a processing device cause the processing device to:<claim-text>receive a mapping of sensor-input values and acoustic modulation-characteristic values, wherein each sensor-input value in the mapping corresponds to a respective acoustic modulation-characteristic value in the mapping;</claim-text><claim-text>receive an audio element from an audio source, wherein the audio element comprises at least one audio parameter;</claim-text><claim-text>identify an audio-parameter value of the at least one audio parameter;</claim-text><claim-text>receive a sensor-input value from a sensor;</claim-text><claim-text>determine from the mapping of sensor-input values and acoustic modulation-characteristic values, an acoustic modulation-characteristic value that corresponds to the sensor-input value received from the sensor;</claim-text><claim-text>generate an audio output by modifying the audio element based on the audio-parameter value and the acoustic modulation-characteristic value; and</claim-text><claim-text>play the audio output.</claim-text></claim-text></claim></claims></us-patent-application>