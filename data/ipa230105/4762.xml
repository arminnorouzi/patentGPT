<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004763A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004763</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17503205</doc-number><date>20211015</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6282</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>629</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6256</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>0055</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">RADIO FREQUENCY ENVIRONMENT AWARENESS WITH EXPLAINABLE RESULTS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217945</doc-number><date>20210702</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BAE SYSTEMS Information and Electronic Systems Integration Inc.</orgname><address><city>Nashua</city><state>NH</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Stankowicz, JR.</last-name><first-name>James M.</first-name><address><city>Boston</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Carmack</last-name><first-name>Joseph M.</first-name><address><city>Milford</city><state>NH</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Kuzdeba</last-name><first-name>Scott A</first-name><address><city>Hollis</city><state>NH</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Schmidt</last-name><first-name>Steven</first-name><address><city>Charleston</city><state>SC</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BAE SYSTEMS Information and Electronic Systems Integration Inc.</orgname><role>02</role><address><city>Nashua</city><state>NH</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A Deep-Learning (DL) explainable AI system for Radio Frequency (RF) machine learning applications with expert driven neural explainability of input signals combines three algorithms (A1, A2, and A3). A1 is a neural network that learns to classify spectrograms. During training, A1 learns to map a spectrogram to its paired label. It outputs a label estimate from a spectrogram. Labels account for device number and spectrum utilization. The neural network is built on two-dimensional dilated causal convolutions to account for frequency and time dimensions of spectrogram data. A2 is a user-defined function that converts an input spectrogram into a vector that quantifies human-identifiable elements of the spectrogram. A3 is a random forest feature extraction algorithm. It takes as input the outputs of A2 and A1. From these, A3 learns which elements in the vector output by A2 were most important for choosing the labels output from A1.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="221.49mm" wi="147.24mm" file="US20230004763A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.60mm" wi="149.27mm" file="US20230004763A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="228.60mm" wi="174.92mm" file="US20230004763A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="218.52mm" wi="121.50mm" file="US20230004763A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.52mm" wi="151.38mm" file="US20230004763A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="218.52mm" wi="162.73mm" file="US20230004763A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="218.52mm" wi="148.25mm" file="US20230004763A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="212.85mm" wi="144.61mm" orientation="landscape" file="US20230004763A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="209.13mm" wi="159.51mm" file="US20230004763A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="205.06mm" wi="156.21mm" file="US20230004763A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="207.43mm" wi="150.37mm" file="US20230004763A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="209.80mm" wi="155.11mm" file="US20230004763A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="213.02mm" wi="158.67mm" file="US20230004763A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="213.02mm" wi="141.22mm" file="US20230004763A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Applications No. 63/217,945, filed Jul. 2, 2021, which is herein incorporated by reference in its entirety for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The following disclosure relates generally to signal processing and, more particularly, to a machine learning system to assess spectrum awareness and present results that are interpretable to humans.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">There is currently a lack of solutions for gaining the ability to autonomously detect and understand contextual shifts in the RF environment. Contextual shifts are the top level changes in spectrum use that dictate how individual devices are operating as a collective ensemble at any given moment, either by design or happenstance. Understanding this context is important for real-time situational awareness on the past and current RF environment for determining how to monitor and access the environment. Understanding this well leads to making more accurate predictions about the current and future RF environment, and increasing the performance and security of systems. With increasing demand for situational awareness for autonomous reasoning systems, some progress has been made to enhance system security through deep learning based RF fingerprinting and RF classification. However, in many operational systems, strong machine learned classification is simply not enough. In failure modes, human operators working alongside autonomous systems need direction as to what went wrong, or quick machine intuition into why a particular decision or classification was made. Deep learning based approaches may have been opening up new applications and increased performance in the RF domain, but they suffer a lack of insight into how decisions are made and where errors and mis-classifications may arise. This has largely left these approaches in the lab, as end users and operators have not fully embraced them due to lack of understanding.</p><p id="p-0005" num="0004">As RF environments become increasingly congested, complex, and adaptive it is increasingly challenging for even expertly-tuned algorithms to manage and interpret the spectrum, let alone in a way that makes sense to an operator or analyst. Machine learning algorithms are candidates for tackling spectrum sensing without reliance on expertly-tuned algorithms, but often work like &#x201c;black boxes&#x201d;, supplying no insight into what the algorithm learned.</p><p id="p-0006" num="0005">What is needed is a system and method to turn the output of such a &#x201c;black box&#x201d; machine learning algorithm into something human interpretable via a user-defined dictionary for multiple machine learning applications.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">An embodiment provides a Deep-Learning (DL) explainable AI system for scene context change detection and classification with expert driven neural explainability of input signals comprising a Classification Module (CM) comprising Dilated Causal Convolutions (DCC) layers, whereby a number of devices and spectrum density are measured into bins, producing a set of classes describing these salient features of the spectrum environment; and an Explainability Module (EM) comprising a series of Random Forests classifiers and a genetic algorithm optimization, whereby a subset of most important expert features for each class from the CM are identified; and outputting a condensed set of salient expert features for a given EM prediction as human readable sentences; wherein the CM comprises: a rectified linear unit (ReLU) and batch normalization layer whereby features from the DCC are combined; convolution and pooling layers whereby feature size is reduced; and softmax classification layer whereby output is provided; wherein the CM DCCs perform convolutions in frequency and dilated convolutions in time; and whereby an output is human interpretable. In embodiments the input signals are digital Radio Frequency (RF) Wi-Fi 802.11a/g waveforms. In other embodiments, the spectrogram for each sample scene class were 5 MHz wide and 1 ms in duration, with bin spacing to form a 128&#xd7;38&#xd7;2 (2=phase and magnitude) sized image (nfft=128, noverlap=128, window=256). In subsequent embodiments spectrograms comprise two channels in the third dimension where phase and magnitude representation to maintain the complex-valued nature of the underlying data (vs IQ data) and the training dataset was constructed with 1,000 examples per class. For additional embodiments classes comprise two primary traffic parameters, the number of devices in the scene and their spectrum usage, i.e., spectral density. In another embodiment, the number of devices comprise 3 sub-classes, low, medium, and high. For a following embodiment spectral density comprises 3 sub-classes, low, medium, and high. In subsequent embodiments expert feature generation provides a set of human interpretable features that an expert who is monitoring the spectrum would understand and use to describe the scene. In additional embodiments expert features comprise time, frequency, and power.</p><p id="p-0008" num="0007">Another embodiment provides an EM feature vector where the salient expert features are defined as (1) Brightness&#x201d;, of received power normalized between 0 and 1; (2) &#x201c;Time-half&#x201d; determines the amount of activity in the early and later parts of the monitored period, i.e., the first or second half, enumerated 1 or 2; (3) &#x201c;Energy in segment x&#x201d; is the sum of all values in segment x and is min-maxed normalized; (4) &#x201c;Time-energy product in segment x&#x201d; is a count of the time bins in segment x for which any pixels exceed half the max value in the segment; and (5) &#x201c;Consistent energy from segment x to y&#x201d; is a Boolean set to true if both halves of the time period have relatively equal amounts of energy, i.e., activity surrogate.</p><p id="p-0009" num="0008">In yet further embodiments Brightness, segment energy, and time-energy product each comprises 8 features, 1 for each segment, consistent energy produces 4 features for the channelized transients between the time segments and time-half produces 2 features, one for each half, all of these features are encoded into the feature vector of size 30. In related embodiments comprising two correlated datasets, one of raw spectrograms for classification by the deep learning-based CM, and the second which is a simplified human annotated dictionary for classification by the EM. For further embodiments the DCC comprises a 3-dimensional DCC operator whereby a tight coupling of phase and magnitude is maintained throughout feature extraction of a network. In ensuing embodiments EM comprises a down-selection of features most relevant for each classification label, wherein classes are processed one class at a time in a one-versus-all fashion. For yet further embodiments, each sentence s can be thought of as a polytope edge in the full feature space which defines the activation of the class label.</p><p id="p-0010" num="0009">An embodiment provides a Deep-Learning (DL) explainable AI system for Radio Frequency (RF) machine learning applications with expert driven neural explainability of input signals comprising a Classifier Module; an Explainability Module; an Important Features module; a Training Phase; and an Inference Phase; the Training Phase comprising a first training input comprising a Ground Truth Training Input and a second training input comprising a Raw RF Features Training Input; the Inference Phase comprising input of Raw RF Features input signals; and an output of Ground Truth comprising Feature Annotation, whereby the explainability is provided. In embodiments the Training Phase comprises an Error Between Ground Truth and Class Prediction Module receiving the first training input of a Class Ground Truth Target; an RF Classifier Module (RCM) RCM Training Update Module receiving input from the Error Between Ground Truth and Class Prediction Module; an RF Classifier Module (RCM) receiving input from the RCM Training Update Module; a Class Prediction Module receiving input from the RCM; an Error Between RCM Prediction and Explainability Module (EM) Prediction Module receiving a first input, from the Class Prediction Module; an EM Training Update Module receiving input from the Error Between RCM Prediction and EM Prediction Module; an Explainability Module (EM) receiving input from the EM Training Update Module; a Class Prediction with EM Annotations Module receiving input from the EM, the Class Prediction with EM Annotations Module providing a second input to the Error Between RCM Prediction and EM Prediction Module; a Genetic Algorithm Discovery of K-most Important Class Features Module also receiving input from the EM; the second training input Raw RF Features Training Input providing input to the RCM; a Raw-to-Expert Feature Mapping Module also receiving input from the Raw RF Features training input; an Expert RF Features Module receiving input from the Raw-to-Expert Feature Mapping Module, and providing a second input to the EM; and a Genetic Algorithm Discovery of K-most Important Class Features Module receiving an input from the Expert RF Features Module, and receiving a second input from the EM, thereby producing a trained system. In other embodiments, the Inference Phase comprises a trained Classifier Module receiving the Raw RF Features input signals; a Raw-to-Expert Feature Mapping Module also receiving the Raw RF Features input signals; an Expert RF Features Module receiving input from the Raw-to-Expert Feature Mapping Module; a trained Explainability Module receiving input from the Expert RF Features Module; and outputting a Class Prediction with K-most Important Expert Feature Annotations for the Raw RF Features input signals, whereby the input signal classification with explainability is provided. In subsequent embodiments the Radio Frequency (RF) machine learning applications comprise scene context change detection and classification; and input signals are digital Radio Frequency (RF) Wi-Fi 802.11a/g waveforms. For additional embodiments classes comprise two primary traffic parameters, a number of devices in a scene and their spectrum usage or spectral density. In another embodiment a number of devices comprise 3 sub-classes, low, medium, and high. For a following embodiment spectral density comprises 3 sub-classes, low, medium, and high. In subsequent embodiments expert feature generation provides a set of human interpretable features that an expert who is monitoring the spectrum would understand and use to describe the scene. In additional embodiments expert features comprise time, frequency, and power. In included embodiments an EM feature vector comprises a Brightness of received power normalized between 0 and 1; a Time-half determining an amount of activity in early and later parts of a monitored period, enumerated as 1 or 2; an Energy in segment as a sum of all values in segment x, min-maxed normalized; a Time-energy product in segment x count of time bins in segment x for which any pixels exceed half a max value in the segment x; and a Consistent energy from segment x to y Boolean, set to true if both halves of a time period have relatively equal amounts of energy. In yet further embodiments the Brightness, the Segment energy, and the Time-energy product each comprises 8 features, 1 for each segment, the Consistent energy produces 4 features for channelized transients between time segments, and the Time-half produces 2 features, one for each half, wherein all features are encoded into the feature vector, of size 30. In related embodiments comprising two correlated datasets, one of raw spectrograms for classification by the deep learning-based CM, and a second which is a simplified human annotated dictionary for classification by the EM. For further embodiments the DCC comprises a 3-dimensional DCC operator whereby a tight coupling of phase and magnitude is maintained throughout feature extraction of a network. In ensuing embodiments the EM comprises a down-selection of features most relevant for each classification label, wherein classes are processed one class at a time in a one-versus-all fashion. For yet further embodiments, each sentence s can be thought of as a polytope edge in a full feature space which defines an activation of a class label.</p><p id="p-0011" num="0010">Another embodiment provides a non-transient computer readable medium containing program instructions for causing a computer to perform the method of inputting a first training input comprising a Ground Truth Training Input in a Training Phase; and inputting a second training input comprising a Raw RF Features Training Input in the Training Phase; training a Classifier Module in the Training Phase; training an Explainability Module in the Training Phase; training an Important Features module in the Training Phase; inputting Raw RF Features input signals in an Inference Phase; and outputting Ground Truth comprising Feature Annotation, whereby explainability is provided. For more embodiments input comprises a spectrogram for each sample scene class being 5 MHz wide and 1 ms in duration. Continued embodiments include input comprising a spectrogram for each sample scene class with bin spacing forming a 128&#xd7;38&#xd7;2 sized image comprising phase and magnitude, where nfft=128, noverlap=128, and window=256. For additional embodiments, input comprises spectrograms comprising two channels in a third dimension where phase and magnitude representation maintain a complex-valued nature of underlying data, and a training dataset is constructed with 1,000 examples per class.</p><p id="p-0012" num="0011">A yet further embodiment provides a Deep-Learning (DL) explainable AI method for scene context change detection and classification with expert driven neural explainability of input signals comprising a Training Phase; and an Inference Phase; the Training Phase comprising an Error Between Ground Truth and Class Prediction Module receiving the first training input of a Class Ground Truth Target; an RCM Training Update Module receiving input from the Error Between Ground Truth and Class Prediction Module; an RF Classifier Module (RCM) receiving input from the RCM Training Update Module; a Class Prediction Module receiving input from the RCM; an Error Between RCM Prediction and Explainability Module (EM) Prediction Module receiving a first input, from the Class Prediction Module; an EM Training Update Module receiving input from the Error Between RCM Prediction and EM Prediction Module; an Explainability Module (EM) receiving input from the EM Training Update Module; a Class Prediction with EM Annotations Module receiving input from the EM, the Class Prediction with EM Annotations Module providing a second input to the Error Between RCM Prediction and EM Prediction Module; a Genetic Algorithm Discovery of K-most Important Class Features Module also receiving input from the EM; the second training input Raw RF Features Training Input providing input to the RCM; a Raw-to-Expert Feature Mapping Module also receiving input from the Raw RF Features training input; an Expert RF Features Module receiving input from the Raw-to-Expert Feature Mapping Module, and providing a second input to the EM; and a Genetic Algorithm Discovery of K-most Important Class Features Module receiving an input from the Expert RF Features Module, and receiving a second input from the EM, thereby producing a trained system; wherein the Inference Phase comprises a trained RF Classifier Module receiving the Raw RF Features input signals; a trained Raw-to-Expert Feature Mapping Module also receiving the Raw RF Features input signals; a trained Expert RF Features Module receiving input from the Raw-to-Expert Feature Mapping Module; a trained Explainability Module receiving input from the Expert RF Features Module; and outputting a Class Prediction with K-most Important Expert Feature Annotations for the Raw RF Features input signals, whereby the input signal classification with explainability is provided.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a High-Level System Overview configured in accordance with an embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a General System Architecture configured in accordance with an embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a classifier module configured in accordance with an embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an explainability module (EM) configured in accordance with an embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts feeding DCC embedding layers to an explainability module configured in accordance with an embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts DCC AND EM diagrams configured in accordance with an embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts nine RF scene classes configured in accordance with an embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an expert encoded feature breakdown configured in accordance with an embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts performance/accuracy configured in accordance with an embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> depicts an RF environment with labeled segments for high device number, medium intensity configured in accordance with an embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an RF environment with labeled segments for medium device number, low intensity configured in accordance with an embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts a Training Phase method flowchart configured in accordance with an embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an Inference Phase method flowchart configured in accordance with an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0026" num="0025">These and other features of the present embodiments will be understood better by reading the following detailed description, taken together with the figures herein described. The accompanying drawings are not intended to be drawn to scale. For purposes of clarity, not every component may be labeled in every drawing.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0027" num="0026">The features and advantages described herein are not all-inclusive and, in particular, many additional features and advantages will be apparent to one of ordinary skill in the art in view of the drawings, specification, and claims. Moreover, it should be noted that the language used in the specification has been selected principally for readability and instructional purposes, and not to limit in any way the scope of the inventive subject matter. The invention is susceptible of many embodiments. What follows is illustrative, but not exhaustive, of the scope of the invention.</p><p id="p-0028" num="0027">Previous work is described in U.S. patent application Ser. No. 17/142,800 &#x201c;ARTIFICIAL INTELLIGENCE RADIO CLASSIFIER AND IDENTIFIER&#x201d;; U.S. patent application Ser. No. 16/539,578 &#x201c;RF FINGERPRINT ENHANCEMENT BY MANIPULATION OF AN ABSTRACTED DIGITAL SIGNAL&#x201d; filed Aug. 13, 2019; and U.S. patent application Ser. No. 17/358,153 &#x201c;NOVEL METHOD FOR SIGNAL REPRESENTATION AND CONSTRUCTION&#x201d; filed Jun. 25, 2021 are incorporated, by reference, in its entirety, for all purposes.</p><p id="p-0029" num="0028">In embodiments, a machine learning algorithm is trained to assess spectrum awareness. In particular, it identified the number of devices present and spectrum utilization of wireless RF environments from spectrogram data in a way that is interpretable to humans in a way that makes sense to an operator or analyst.</p><p id="p-0030" num="0029">Embodiments combine three algorithms. Algorithm 1 (A1) is a neural network that learns to classify spectrograms. During training, A1 learns to map a spectrogram to its paired label. After training, A1 takes a spectrogram as input, and estimates a label as output. For our example use case, the labels accounted for the number of devices and utilization of the spectrum, though it is straightforward to pivot to different use cases. Embodiment neural network used for A1 is specifically tailored to RF-domain data, in itself a novel contribution to the field. This neural network is built on two-dimensional dilated causal convolutions to account for the frequency and time dimensions of spectrogram data. These convolutions are well suited for processing spectrogram data. Algorithm 2 (A2) is a user-defined function that takes a spectrogram as input, and converts it into a vector that quantifies human-identifiable elements of the spectrogram. Examples of entries to this vector are: &#x201c;brightness&#x201d; of the pixels in various frequency and time bins of the spectrogram (to capture power or phase), or the number of bright bands in the spectrogram (which is something a human operator might use to estimate number of devices). Embodiments generalize these features, and in a general-use setting A2 allows subject matter experts to define the &#x201c;vocabulary&#x201d; that machine learning algorithms use. This novel contribution directly ties together current approaches of operator or analyst defined tasking with the flexibility and learnability of deep learning. Algorithm 3 (A3) is a random forest feature extraction algorithm. It takes as input the output of A2 and the output of A1. From these two inputs, A3 learns which elements in the vector output by A2 were most important for choosing the labels output from A1.</p><p id="p-0031" num="0030">Embodiments allow analysts, operators, or general users (i.e., design engineer, reprogrammer, etc.) to understand the &#x201c;thinking&#x201d; of a machine learning algorithm that has been trained to understand spectrogram utilization. Of the three algorithms constituting embodiments (A1, A2, A3), A1 is similar to the &#x201c;black box&#x201d; approach, with the innovation that it is specifically designed to operate well on RF spectrogram data. This is distinct from an expert-defined algorithm because A1 is capable of generalizing to novel scenarios like new transmitters or physical surrounding. The point of A2 in embodiments is to allow the user to define a &#x201c;vocabulary&#x201d; for the machine learning algorithm. Then A3 serves as a translator that converts the output of A1 into the vocabulary defined by the user in A2. Analogs to A2 and A3 have not been applied to RF problems before. Another advantage to the solution is that it is immediately applicable to any machine learning process that performs classification, and is likely applicable with minor adaptation to machine learning tasks that output something other than classification labels.</p><p id="p-0032" num="0031">There are numerous applications of applying embodiments to radio frequency domain tasks, stemming from the several novel components. The neural network based spectrum awareness part of the system has applicability to applications and problems that have a spectrogram or similar input to them, i.e., a temporal ordering and some second feature (frequency) forming a causal 2D image. Embodiments may extend into cyclostationary features or other types of features captured in a CAF process. The explainability part of this system is more wide ranging and a critical piece that is needed for RF systems as more solutions turn to deep learning approaches. Having explainability in terms that analysts and operators use will be critical for operational use.</p><p id="p-0033" num="0032">Embodiments provide a framework to perform radio frequency (RF) scene context change detection and classification with Expert driven neural explainability. Embodiments use a deep learning based classifier to perform spectrum monitoring of Wi-Fi devices and usage patterns with an auxiliary classifier operating post-hoc to output human interpretable reasoning for classification declarations. Classification network embodiments operate on input spectrograms through a series of dilated causal convolution layers for feature extraction which are fed into classification layers. Dilated causal convolutions (DCC) are well suited for RF applications, including RF fingerprinting, their use is extended here to new applications. The Explainability Module operates over an auxiliary dataset that is built based on domain expertise for learning how to reason over the classification network outputs. These two approaches, the deep learning classifier and Explainability Module are combined into a unique explainable deep learning approach that is applied to Wi-Fi spectrum monitoring. This fused approach leverages the power of deep learning classification with user interpretable explainability.</p><p id="p-0034" num="0033">The emerging field of Explainable Artificial Intelligence (XAI) has risen to address deficiencies and provide the insight that human operators require to work hand-in-hand and trust the AI systems. Explainable deep learning approaches include post-hoc interpretability where explanations are extracted from the model. Embodiments apply post-hoc interpretability to a black box neural network. Embodiments provide methods that, after the main AI classifier makes a declaration, provide intuition on why the decision was made post-hoc in a way that can be understood by the human operator. These methods extract expert operator relevant outputs, enabling split-second decisions to be made without needing extensive training to be an expert in explainable AI methods. Embodiments combine three novel contributions in the RF XAI System: (1) Wi-Fi spectrum monitoring datasets with a programmatically scripted dictionary of expert terms and features, (2) deep learning-based spectrum monitor Classification Module (CM), and (3) Explainability Module (EM) for extraction of important features from the expert annotations. These embodiments insert the interpretability directly into the network to force the network to inherently learn mappings to a provided interpretable feature set through the use of concept bottlenecks in a post-hoc manner.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a High-Level System Overview <b>100</b>. Embodiments of System <b>105</b> comprise a Training Phase <b>110</b> and an Inference Phase <b>115</b>. Modules trained in the system comprise Classifier Module (CM) <b>120</b>, Explainability Module (EM) <b>125</b>, and Important Feature module <b>130</b>. Inputs to Training Phase <b>110</b> comprise Ground Truth <b>135</b> and Raw RF Features <b>140</b>. Inference Phase <b>115</b> follows Training Phase <b>110</b>. Input during Inference Phase <b>115</b> comprises Raw RF Features <b>145</b>. Output of Inference Phase <b>115</b> comprises Class Prediction <b>150</b> to which Feature Annotations <b>155</b> are applied. Details of each component, including interactions, follow.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a General System Architecture <b>200</b>. As depicted in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, embodiments of the General System Architecture comprise a Training Phase <b>110</b> and an Inference Phase <b>115</b>. In Training Phase <b>110</b>, the system is trained to perform explainable classification. In Inference Phase <b>115</b>, the system has been trained already, and is being queried for class predictions with expert annotations provided by the explainability module. Emphasizing just the processing flow, Training Phase <b>110</b> comprises training input of Class Ground Truth Target <b>205</b> which provides input to Error Between Ground Truth and Class Prediction <b>210</b>. Output of Error Between Ground Truth And Class Prediction Module <b>210</b> provides input to RCM Training Update <b>215</b>. RCM Training Update <b>215</b> output provides input to Classifier Module (CM) <b>220</b>. Output of RCM <b>220</b> provides input to Class Prediction <b>225</b>. Output of Class Prediction <b>225</b> provides a first input to Error Between RCM Prediction and Explainability Module (EM) Prediction <b>230</b>. Output of Error Between RCM Prediction and EM Prediction <b>230</b> provides input to EM Training Update <b>235</b>. Output of EM Training Update <b>235</b> provides input to Explainability Module (EM) <b>240</b>. Output of EM <b>240</b> provides input to Class Prediction with Explainability EM Annotations <b>245</b> and Genetic Algorithm Discovery of K-most Important Class Features <b>250</b>. Output of Error Between Ground Truth And Class Prediction <b>210</b> provides a second input to Error Between Ground Truth And Class Prediction <b>230</b>. A second input to Training Phase <b>110</b> are Raw RF Features <b>255</b>. Raw RF Features <b>255</b> provides input to both CM <b>220</b> and Raw-to-Expert Feature Mapping Module <b>260</b>. Output of Raw-to-Expert Feature Mapping Module <b>260</b> provides input to Expert RF Features <b>265</b>. Output of Expert RF Features <b>265</b> provides input to both EM <b>240</b> and to Genetic Algorithm Discovery of K-most Important Class Features <b>250</b>.</p><p id="p-0037" num="0036">Again emphasizing just the processing flow, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, Inference Phase <b>115</b> comprises input of Raw RF Features <b>270</b> to both (trained) RCM <b>220</b> and (trained) Raw-to-Expert Feature Mapping <b>260</b>. Output of Raw-to-Expert Feature Mapping <b>260</b> provides input to (trained) Expert RF Features <b>265</b>. Output of Expert RF Features <b>265</b> provides input to (trained) Explainability Module (EM) <b>240</b>. Finally, output of both RCM <b>220</b> and EM <b>240</b> provide input to Class Prediction with K-most Important Expert Feature Annotations <b>265</b>, which is the output of the system.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a RF Classifier Module (CM) <b>300</b>. Components comprise Spectrograms input <b>305</b>; Skip Connections Classifier <b>310</b>; Convolution and Pooling Layers <b>315</b>, Softmax Classification Layer <b>320</b>; and Classification Output <b>325</b>. For spectrum monitoring, the network input <b>305</b> is a complex-valued spectrogram, as opposed to complex-valued IQ data. Spectrogram inputs <b>305</b> are sized 128&#xd7;38&#xd7;2, with the last dimension representing the complex-valued nature of the spectrogram as phase and magnitude. Embodiments modify the DCC operation to perform traditional convolutions in frequency and dilated causal convolutions in time to exploit the causal nature of RF signals and to efficiently scale to RF data rates. To maintain the tight coupling of phase and magnitude throughout the feature extraction of the network, embodiments utilize a 3-dimensional DCC operator, where the feature maps of each layer are the same shape as the input, 128&#xd7;38&#xd7;2, 330. This enables maintaining the coupling of phase and magnitude in the learned feature representations. Following the DCC layers, a Rectified Linear Unit (ReLU) <b>335</b> and Batch Normalization (BN) layer are applied to combine the features from the various DCC layers <b>310</b>, which employ skip connections. This is followed by traditional convolution and pooling layers <b>315</b> to reduce the feature size. Finally, a softmax classification layer <b>320</b> provides the output <b>325</b>. The classifier is trained to output the number of devices and spectrum density, both measured into the coarse bins of high, medium, and low.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an Explainability Module (EM) <b>400</b>. The EM is a two part ensemble of algorithms, the first being a series of Random Forests Classifiers <b>405</b> and the second a Genetic Algorithm optimization <b>410</b> to identify a subset of most important expert features for each class (RF scene). In embodiments, it is trained using the auxiliary dataset on both ground truth target labels and surrogate labels from the output of the CM. The classification accuracy on ground truth data is used to validate that the expert guided feature space has merit on ground truth, as embodiments are really interested in understanding the most important (K) features <b>415</b> in the RCM classification space. By this, it is meant that once it is established that the expert feature set has discriminative merit, only training on the surrogate labels produced by the RCM to provide insight into its classifications is considered.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts feeding DCC embedding layers to an Explainability Module <b>500</b>. Embodiments comprise Parsing Embodiments <b>505</b> and Direct Embodiments <b>510</b>. In embodiments, an architecture is designed similarly to an autoencoder. This network has &#x201c;embedding layers&#x201d; <b>515</b> that behave analogously to basis functions in a Fourier transform. Where a Fourier decomposition converts a signal into a basis of orthogonal, sinusoidally varying, functions using a Fourier transform, the DCC uses a data-driven approach to learn optimal basis functions and optimal transforms. Embodiments have extended this to arbitrary input shapes (namely to spectrograms <b>520</b> rather than IQ representations of signals), while preserving the fact that the embedding layers <b>515</b> match the original input shapes. Embodiments apply spectrograms <b>520</b> to encoder <b>525</b>, then, via Embedding Layers <b>515</b>, Class <b>530</b> is output to the EM <b>535</b>. In embodiments, Embedding Layers <b>515</b> provide output directly to the EM <b>535</b>, in other embodiments, the spectrogram is also parsed <b>540</b>, providing input to the EM <b>545</b>.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts DCC AND EM diagrams <b>600</b>. DCC <b>605</b> comprises raw data IQ input vector <b>610</b> to (black box) IQ DNN <b>615</b> with Machine Learning (ML). This then inputs to the EM <b>620</b> with Label(S) for Output Vector <b>625</b> and Dictionary Terms <b>630</b>. Output is confirmed <b>635</b>, including decision trees <b>640</b>. These then interface with a Feature Filter Harness <b>645</b> comprising a genetic algorithm <b>650</b>. This is then input for Machine Readable Output <b>655</b> having a Human Readable Translation <b>660</b>. Examples comprise the form of: dictionary_term1&#x3e;value AND dictionary_term4=value AND dictionary_termk&#x3c;value <b>665</b>, for explainable ML.</p><p id="p-0042" num="0041">The spectrum monitoring dataset consists of spectrograms and the auxiliary dataset contains the dictionary of domain specific terminology. This auxiliary dataset provides high level features in terms of time, power, and frequency which a human operator might use to describe a changing RF landscape. Embodiments use a simple Wi-Fi simulation to create both datasets and test this step into RF XAI, and focus on a simple spectrum monitoring case of determining the number of users accessing the network and their aggregate spectral usage.</p><p id="p-0043" num="0042">Embodiments use a Matlab simulation to create simple Wi-Fi traffic patterns and scenes to assess the spectrum monitoring XAI system. Embodiments leverage Matlab's WLAN toolbox to model Wi-Fi, 802.11ag, emissions and apply built-in simple channel models to generate receive representations. Embodiment simulations are within the ISM band, 2.4 GHz, with 1 MHz bandwidth signals sampled at 5 MSps. The simulation uniformly samples two primary traffic parameters, the number of devices in the scene and their spectrum usage, i.e., spectral density. The number of devices fell into 3 sub-classes: low to represent a single person with a few devices or a few people with one device each, medium to represent a small group of people, and high to represent a larger gathering. Similarly, the spectral density is uniformly sampled, again with 3 sub-classes: low to model sparse network access and primarily control flows, medium to represent asynchronous downloads/uploads, and high to represent sustained spectral usage, i.e., watching a movie. For embodiments, Low spectral density comprises primarily of beacons sent on the order of 200 &#x3bc;s, while medium and high density scenarios add in data packets, with high density devices having idle times on the order of 1 &#x3bc;s to model constant data packets such as video streaming.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>7</b></figref> presents nine RF scene classes <b>700</b>, showing examples of simulated traffic patterns and scenes. Various channel models were randomly sampled using built-in Matlab functionality, including different variations of additive white Gaussian noise as well as indoor and outdoor channel models that model different levels of multi-path. Transmit power was set to model a smart phone operation, set at 24 dBm. Both stationary and mobile emitters were modeled at various locations creating various receive SNR levels. A good use case for these types of RF scenes can be a coffee shop that dynamically shifts throughout the day as users come and go and they shift between work, browsing, and connecting with others.</p><p id="p-0045" num="0044">For each sample scene above, a spectrogram is extracted as an example of that class for the raw spectrogram input. The spectrograms were 5 MHz wide and 1 ms in duration, with bin spacing to form a 128&#xd7;38&#xd7;2 sized image (nfft=128, noverlap=128, window=256). The two channels in the third dimension where phase and magnitude representation to maintain the complex-valued nature of the underlying data. The training dataset was constructed with 1,000 examples per class.</p><p id="p-0046" num="0045">The goal of Expert Feature Generation 265 is to provide a set of human interpretable features that an expert who is monitoring the spectrum would understand and use to describe the scene. In embodiments, the set of features fall along three dimensions that are typically used in RF scene understanding tasks: time, frequency, and power. Time features indicate if users or usage is dynamically stable or changing, with potential indication of contextual shifts. In this first test, spectrograms are split into two time segments to assess whether activity is consistent or changing.</p><p id="p-0047" num="0046">Embodiments for time accounting build in a continuous and more distinct accounting of temporal recurrence to alleviate the current na&#xef;ve temporal segmenting. Embodiments also segment along the frequency axis to channelize along Wi-Fi channelizations. Finally, power is leveraged as a surrogate for separating out users, i.e., those that are nearby to the receiver versus further away. In more generalizable applications, power helps provide insight into such things as whether a primary user or jammer is present, how much co-channel interference there is, etc.</p><p id="p-0048" num="0047">Brightness, segment energy, and time-energy product each produced 8 features, 1 for each segment, while consistent energy produced 4 features for the channelized transients between the time segments and time-half produced 2 features, one for each half. Embodiments encode all of these features into a feature vector which is size 30. At this point, there are two correlated datasets, one of raw spectrograms for classification by the deep learning-based CM, and the second which is a simplified human annotated dictionary for classification by the EM.</p><p id="p-0049" num="0048">For embodiments, RF scene understanding entails the process of separating the temporal evolution of a part of the electromagnetic spectrum into different classes that describe how the spectrum is being used, i.e., the overarching context of the spectrum at any given moment. RF scenes are dynamic and change as a function of time, user access, geo-political stances, etc., each at multiple resolutions. For embodiments, it is important to know these contextual changes and how the spectrum is currently being used for both receivers (i.e., SIGINT platforms, spectrum monitors, security nodes, etc.) and transmitters (i.e., secondary users in dynamic spectrum access (DSA) systems, cognitive radios, resource allocation, etc.). In embodiments, 9 different context classes of scene behavior are employed.</p><p id="p-0050" num="0049">For initial testing, a use case of a Wi-Fi spectrum monitor is developed that is assessing the number of Wi-Fi emitters present and their spectral usage to make decisions. As mentioned, system embodiments consist of two main components: the spectrum monitor Classifier Module (CM) and the Explainability Module (EM). The Classifier operates on complex-valued spectrograms to capture both magnitude and phase and outputs classification on how many users are currently accessing their spectral density. The time and frequency extent of the spectrograms, along with the respective resolutions across these dimensions, is a function of the application and the granularity needed. Again as mentioned, in embodiments, a Wi-Fi monitoring system is tested, and hence sets the parameters of the system based on this application. Expert spectrogram features are also needed to train the EM. The classifier thus performs the spectral monitoring function to determine spectrum utilization with the EM providing post-hoc explanations of classification using most impactful expert terms.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>8</b></figref>, an expert encoded feature breakdown <b>800</b>, depicts examples of the above-mentioned segmentations. An embodiment feature vector for EM use is defined as: &#x201c;Brightness&#x201d; <b>805</b> of received power normalized between 0 and 1; &#x201c;Time-half&#x201d; <b>810</b> determines the amount of activity in the early and later parts of the monitored period, i.e., the first or second half, enumerated 1 or 2; &#x201c;Energy in segment x&#x201d; <b>815</b> is the sum of all values in segment x and is min-maxed normalized; &#x201c;Time-energy product in segment x&#x201d; <b>820</b> is a count of the time bins in segment x for which any pixels exceed half the max value in the segment; and &#x201c;Consistent energy from segment x to y&#x201d; <b>825</b> is a Boolean set to true if both halves of the time period have relatively equal amounts of energy, i.e., activity surrogate.</p><p id="p-0052" num="0051">For embodiments, the EM operates on the expert feature space which is encoded into matrix form. Each data point, i.e., spectrogram, makes up the rows of the matrix while the columns are made up of the numerical encodings of the expert features. In embodiments we have x<sub>1</sub>, . . . , x<sub>n</sub>, where n denotes the full set of features with n=30. The EM's first objective is to obtain a down-selection of features which are most relevant for each particular classification label. The algorithm processes one class at a time, in a one-versus-all fashion. As depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the feature down select process starts by extracting a percentage subset of most important features. With the goal of explaining each class in k features or less, embodiments proceed with randomly extracting a further subset (population in the genetic algorithm), containing no more than k features but potentially less. A one vs. all Random Forests classifier, which is called the class model, is then trained using this expert feature subset, and the F1 score (Table 1) is computed with the additional cost function loss variable being the cardinality of k. The F1 score and cardinality loss are used as the health indicators in the genetic algorithm. At the end of the genetic algorithm optimization, the target class of interest's k-most important features are output as well as the one vs all class model. For RF scene classification problems, this results in 9 class models, one for each of the 9 classes.</p><p id="p-0053" num="0052">In embodiments of the second phase of the algorithm, an objective is to output the best condensed set of thresholds which were crossed within the class model for a given EM prediction. This is done for each row in the matrix, i.e., for each data example, and is output as Human Readable Sentences. These statements are made in the form of &#x201c;sentences&#x201d; comprised of &#x201c;words&#x201d; where each word is an inequality represented by a subset of features. The number of sentences and words needed to describe the decision boundary in all its accuracy grows combinatorially with the number of features, and thus, for embodiments, using all of them would result in an impractically large number of sentences and thus complicated explanation of the classifiers decision. Thus, we trade-off accuracy to the full decision boundary by limiting potential explanations to a small number of sentences (input parameter) derived from a small number of words inequalities (input parameter) composed of the top k important features output by the previous algorithm phase. Each sentence s can be thought of as a polytope edge in the full feature space which defines the activation of the class label. This problem is both tractable and efficient, as embodiments first narrow the full dataset feature space down to a presumable small k top features for this meta-classifier to learn how to best describe a class with no more than s sentences.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts CM performance results <b>900</b> on the 9 classes for the spectrum monitoring dataset. As previously described, for each of three device number bins (high, medium, low), there are three spectrum utilization bins (high, medium, low). The CM performs quite well, achieving an average accuracy of 93%. Embodiments had issues with the low utilization and high device population case <b>905</b>, as activity can look like far more devices are present when small populations of devices are performing a large, diverse set of tasks, and thus looks like a larger device set. Embodiments employed deeper CM architectural choices and temporal dilation rates ranging from 1-32 and found the best performance to be with minimal dilatation (1, 2) and only 4 Dilated Convolutional Layers. This is a more compact model than some other applications, such as RF fingerprinting, which required deeper architectures to learn discriminating features.</p><p id="p-0055" num="0054">EM embodiment results and variance between ground truth and the CM labels are shown in Table 1. EM CLASSIFICATION</p><p id="p-0056" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="42pt" align="left"/><colspec colname="3" colwidth="42pt" align="center"/><colspec colname="4" colwidth="42pt" align="center"/><colspec colname="5" colwidth="28pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><thead><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row><row><entry>Devices</entry><entry>Utilization</entry><entry>Acc.</entry><entry>Prec.</entry><entry>Rec.</entry><entry>F1</entry></row><row><entry namest="1" nameend="6" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="10"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="42pt" align="left"/><colspec colname="3" colwidth="14pt" align="center"/><colspec colname="4" colwidth="28pt" align="center"/><colspec colname="5" colwidth="14pt" align="center"/><colspec colname="6" colwidth="28pt" align="center"/><colspec colname="7" colwidth="14pt" align="center"/><colspec colname="8" colwidth="14pt" align="center"/><colspec colname="9" colwidth="14pt" align="center"/><colspec colname="10" colwidth="14pt" align="center"/><tbody valign="top"><row><entry>Low</entry><entry>Low</entry><entry>.13</entry><entry>.92</entry><entry>.00</entry><entry>.97</entry><entry>.00</entry><entry>.90</entry><entry>.00</entry><entry>.94</entry></row><row><entry/><entry>Medium</entry><entry>.53</entry><entry>.93</entry><entry>.49</entry><entry>.97</entry><entry>.95</entry><entry>.91</entry><entry>.65</entry><entry>.94</entry></row><row><entry/><entry>High</entry><entry>.52</entry><entry>.93</entry><entry>.48</entry><entry>.97</entry><entry>.95</entry><entry>.91</entry><entry>.84</entry><entry>.94</entry></row><row><entry>Medium</entry><entry>Low</entry><entry>.75</entry><entry>.93</entry><entry>.74</entry><entry>.97</entry><entry>.96</entry><entry>.92</entry><entry>.84</entry><entry>.94</entry></row><row><entry/><entry>Medium</entry><entry>.57</entry><entry>.93</entry><entry>.51</entry><entry>.97</entry><entry>.99</entry><entry>.92</entry><entry>.67</entry><entry>.94</entry></row><row><entry/><entry>High</entry><entry>.75</entry><entry>.93</entry><entry>.74</entry><entry>.97</entry><entry>.96</entry><entry>.91</entry><entry>.84</entry><entry>.94</entry></row><row><entry>High</entry><entry>Low</entry><entry>.75</entry><entry>.93</entry><entry>.74</entry><entry>.97</entry><entry>.96</entry><entry>.92</entry><entry>.84</entry><entry>.94</entry></row><row><entry/><entry>Medium</entry><entry>.86</entry><entry>.93</entry><entry>.87</entry><entry>.97</entry><entry>.97</entry><entry>.91</entry><entry>.92</entry><entry>.94</entry></row><row><entry/><entry>High</entry><entry>.24</entry><entry>.93</entry><entry>.13</entry><entry>.97</entry><entry>1.0</entry><entry>.91</entry><entry>.23</entry><entry>.94</entry></row><row><entry namest="1" nameend="10" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0057" num="0055">The ground truth accuracy shows there are a feasible amount of features to perform the classification task. EM performance on CM labels was significantly better than on ground truth labels. It is imperative to keep in mind that the goal of the EM is not to do well on ground truth outside of demonstrating reasonable class discrimination feasibility, the true goal being to mimic the decision boundaries of the CM. Thus, it is less surprising that EM embodiments struggled with a different class than the CM, but was able to pick up on the CM classifications for this weak class to a high degree of certainty. As an example, EM embodiments struggled with low utilization, low transmitter population on the ground truth dataset, but did very well in learning the decision plane of the CM on this class. This indicates the pre-classification step of the CM creates more separation in class hyperplanes on the raw spectrogram dataset. There are instances in EM inference, due to the one-versus-all model schema, where more than one class is activated. In these cases, the class with highest prediction probability was chosen as the class label.</p><p id="p-0058" num="0056">Table 2 is a summary of EM results from parsing function and true labels. The first row of each cell is accuracy/precision/recall/F1 score. Rows two-four or each cell are the ordered top-three factors of importance in obtaining the result. The labels are: brightness_x: the total brightness in segment x; brightness_x_y: whether there is brightness crossing from segment x to segment y; count_x: the number of bright sections in segment x.</p><p id="p-0059" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="56pt" align="left"/><colspec colname="1" colwidth="203pt" align="center"/><tbody valign="top"><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row><row><entry/><entry>Spectrum utilization</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="56pt" align="left"/><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="63pt" align="left"/><colspec colname="3" colwidth="70pt" align="left"/><tbody valign="top"><row><entry/><entry>Low</entry><entry>Medium</entry><entry>High</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="70pt" align="left"/><colspec colname="4" colwidth="63pt" align="left"/><colspec colname="5" colwidth="70pt" align="left"/><tbody valign="top"><row><entry>Number</entry><entry>2</entry><entry>0.13 0.00 0.00 0.00</entry><entry>0.53 0.49 0.95 0.65</entry><entry>0.52 0.48 0.95 0.84</entry></row><row><entry>of</entry><entry/><entry>brightness_1_2</entry><entry>brightness_1_2</entry><entry>brightness_1_2</entry></row><row><entry>devices</entry><entry/><entry>brightness_3</entry><entry>brightness_3</entry><entry>count_2</entry></row><row><entry/><entry/><entry>count_2</entry><entry>count_2</entry><entry>brightness_4</entry></row><row><entry/><entry>4</entry><entry>0.75 0.74 0.96 0.84</entry><entry>0.57 0.51 0.99 0.67</entry><entry>0.75 0.74 0.96 0.84</entry></row><row><entry/><entry/><entry>brightness_1_2</entry><entry>brightness_1_2</entry><entry>brightness_3</entry></row><row><entry/><entry/><entry>count_2</entry><entry>brightness_3</entry><entry>count_2</entry></row><row><entry/><entry/><entry>brightness_4</entry><entry>count_2</entry><entry>brightness_4</entry></row><row><entry/><entry>6</entry><entry>0.75 0.74 0.96 0.84</entry><entry>0.86 0.87 0.97 0.92</entry><entry>0.24 0.13 1.00 0.23</entry></row><row><entry/><entry/><entry>brightness_1_2</entry><entry>brightness_1_2</entry><entry>brightness_1_2</entry></row><row><entry/><entry/><entry>brightness_3</entry><entry>brightness_3</entry><entry>brightness_3</entry></row><row><entry/><entry/><entry>count_2</entry><entry>count_2</entry><entry>count_2</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0060" num="0057">Table 3 is a summary of EM results from parsing function and DCC-inferred labels. Same format as Table 2.</p><p id="p-0061" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="offset" colwidth="56pt" align="left"/><colspec colname="1" colwidth="203pt" align="center"/><tbody valign="top"><row><entry/><entry namest="offset" nameend="1" align="center" rowsep="1"/></row><row><entry/><entry>Spectrum utilization</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="56pt" align="left"/><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="63pt" align="left"/><colspec colname="3" colwidth="70pt" align="left"/><tbody valign="top"><row><entry/><entry>Low</entry><entry>Medium</entry><entry>High</entry></row><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="1" colwidth="35pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="70pt" align="left"/><colspec colname="4" colwidth="63pt" align="left"/><colspec colname="5" colwidth="70pt" align="left"/><tbody valign="top"><row><entry>Number</entry><entry>2</entry><entry>0.92 0.97 0.90 0.94</entry><entry>0.93 0.97 0.91 0.94</entry><entry>0.93 0.97 0.91 0.94</entry></row><row><entry>of</entry><entry/><entry>brightness_5</entry><entry>brightness_5</entry><entry>brightness_5</entry></row><row><entry>devices</entry><entry/><entry>time_5</entry><entry>brightness_3</entry><entry>brightness_3</entry></row><row><entry/><entry/><entry>time_8</entry><entry>time_5</entry><entry>time_5</entry></row><row><entry/><entry>4</entry><entry>0.93 0.97 0.92 0.94</entry><entry>0.93 0.97 0.92 0.94</entry><entry>0.93 0.97 0.91 0.94</entry></row><row><entry/><entry/><entry>brightness_5</entry><entry>brightness_5</entry><entry>brightness_5</entry></row><row><entry/><entry/><entry>brightness_3</entry><entry>brightness_1</entry><entry>brightness_3</entry></row><row><entry/><entry/><entry>brightness_1</entry><entry>time_5</entry><entry>&#x2032;time_5</entry></row><row><entry/><entry>6</entry><entry>0.93 0.97 0.92 0.94</entry><entry>0.93 0.97 0.91 0.94</entry><entry>0.93 0.97 0.91 0.94</entry></row><row><entry/><entry/><entry>brightness_5</entry><entry>brightness_5</entry><entry>brightness_5</entry></row><row><entry/><entry/><entry>brightness_1</entry><entry>brightness_1</entry><entry>brightness_3</entry></row><row><entry/><entry/><entry>time_5</entry><entry>time_5</entry><entry>time_5</entry></row><row><entry namest="1" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0062" num="0058"><figref idref="DRAWINGS">FIG. <b>10</b></figref>, depicts an RF environment with labeled segments for high device number and medium intensity <b>1000</b>. It shows an example of the complexity involved in correctly estimating the RF scene by embodiments of the CM, where embodiments of the EM fail when decoupled from the CM, i.e., performing classification itself based on the full set of expert features. In this case, the true class output is high device count and medium amount of spectrum access. As can be seen in the Figure, the EM has learned to key on using three channelized segments, shown by the rectangular outline overlays for the EM's selected features <b>1</b> (<b>1005</b>), <b>2</b> (<b>1010</b>), &#x26; <b>5</b> (<b>1015</b>), and two time segments, overlays <b>3</b> (<b>1020</b>) &#x26; <b>4</b> (<b>1025</b>), to describe this class. In embodiments, these expert features are not enough on their own, however, to fully describe the class and separate it from other classes. The CM in this case does correctly classify the class, however lacks the insight and understandability into the important features. Coupling the two approaches together provides the power of the CM to get high performance accuracy, while simultaneously allowing the EM to describe the important features given the RCM learned decision plane. This case illustrates where more solely feature based approaches fail when there is ambiguity.</p><p id="p-0063" num="0059"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an RF environment with labeled segments for medium device number and low intensity <b>1100</b>. It shows both models correctly classifying an example. In this case, the EM has learned that the 3 channelized segments (<b>1</b> (<b>1105</b>), <b>2</b> (<b>1110</b>), <b>3</b> (<b>1115</b>)) and two time segments (<b>4</b> (<b>1120</b>), <b>5</b> (<b>1125</b>)) are important and the set of expert features are enough to classify. These two cases are shown to highlight how scenes are traditionally thought about, as those that can be easily separated by expert defined spaces, and those with a heavy amount of underlying class ambiguity.</p><p id="p-0064" num="0060">These samples are not to showcase deep learning (CM) over decision trees (EM) for classification, but to highlight the explainable set of features that are tagged with the examples, i.e. the annotated overlays. The EM is able to extract a small set of relevant features that explain the predicted class of the scene, and can enable an operator to be focused in on certain areas for further analysis. The example scenes are rather simple in order to prove out this XAI approach, but in other embodiment applications they grow to help address complex scenes in today's world of rapidly growing number of devices and density of different spectrum access technologies. An operator could use these call outs to identify trust or lack thereof in an AI application.</p><p id="p-0065" num="0061"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts a Training Phase method flowchart <b>1200</b>. In the Training Phase, the system is trained to perform explainable classification. Steps comprise training input of Class Ground Truth Target <b>1205</b> which provides input to Error Between Ground Truth and Class Prediction Module <b>1210</b>. Output of Error Between Ground Truth And Class Prediction Module provides input to RCM Training Update Module <b>1215</b>. RCM Training Update Module output provides input to RF Classifier Module (RCM) <b>1220</b>. Output of RCM provides input to Class Prediction Module <b>1225</b>. Output of Class Prediction Module provides a first input to Error Between RCM Prediction and Explainability Module (EM) Prediction Module <b>1230</b>. Output of Error Between RCM Prediction and EM Prediction Module provides input to EM Training Update Module <b>1235</b>. Output of EM Training Update Module provides input to Explainability Module (EM) <b>1240</b>. Output of EM provides input to Class Prediction with Explainability EM Annotations module <b>1245</b> and a first input to Genetic Algorithm Discovery of K-most Important Class Features Module <b>1250</b>. Output from Class Prediction with Explainability EM Annotations module provides a second input to Error Between RCM Prediction and EM Prediction Module <b>1230</b>. A second input to Training Phase <b>1200</b> is Raw RF Features <b>1255</b>. Output of Raw RF Features provides input to both RCM <b>1220</b> and Raw-to-Expert Feature Mapping Module <b>1260</b>. Output of the Raw-to-Expert Feature Mapping Module provides input to Expert RF Features <b>1265</b>. Output of Expert RF Features provides a second input to EM <b>1240</b> and a second input to Genetic Algorithm Discovery of K-most Important Class Features Module <b>1250</b>.</p><p id="p-0066" num="0062"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an Inference Phase method flowchart <b>1300</b>. In the Inference Phase, the system has been trained already, and is being queried for class predictions with expert annotations provided by the explainability module. Steps comprise input of Raw RF Features <b>1305</b> to both (trained) RCM <b>1310</b> and (trained) Raw-to-Expert Feature Mapping <b>1315</b>. Output of Raw-to-Expert Feature Mapping provides input to (trained) Expert RF Features <b>1320</b>. Output of Expert RF Features provides input to (trained) Explainability Module (EM) <b>1325</b>. Finally, output of both RCM and EM provide input to Class Prediction with K-most Important Expert Feature Annotations <b>1330</b>, which is the output of the system.</p><p id="p-0067" num="0063">Embodiments the ability to classify with high accuracy the RF scene decision space and the EM to explain its predictions for the spectrum monitoring task. Since the EM tries to attribute CM predictions using a feature set derived from the actual dataset used to train the CM, there is a possibility embodiments' explanations may lack sensitivity. Other deep learning explainability methods which rely on estimating or computing gradients, or Integrated Gradients have high sensitivity to changes in predicted label but are limited to using the neural network's raw input features to construct explanations. For domains where this data is familiar to human sensory modalities such images or text, this is not much of a problem, but in domains where the neural network input data itself is not intuitive to humans it becomes very limiting since important raw features are confusing themselves. Thus there is a need to be able to correlate expert derived features to construct meaningful explanations. By this, some sensitivity is sacrificed to gain back utility. The classification accuracy of the EM on the CM labels provides a soft measure of its sensitivity. Further EM sensitivity can be increased by using finer grained expert features. In other work, expert feature generation was an iterative process to tune to operator tastes.</p><p id="p-0068" num="0064">Additionally there is a question of: is it desired there is a question of: is it desired to have a small set of important features which uniquely determine the class label or is having a unique feature set for each class desired. There is merit to both situations. In the former, an operator might only require verifying small k features to assess correctness. With k features being set low, this may also result in many state transitions being explained similarly. This is desirable as the operator will inherently know where to look to find trust in the AI performance. On the other hand, it may not provide the operator enough separation of states, thus diverging to checking all possible classes. In this case, having a unique or near unique set of important features per class is desirable for the operator to trust the classifier since there is specific evidence for a certain class and varying evidence toward other classes. This leads to the trade-off between complexity and ease of determination. For example, suppose an operator has 30 features, to describe why a complex and potentially crucial decision was made. This may not capture the human intuition for a human operator to make a competing decision. However, the contrary comes at the expense of complexity and speed of decision time. As the number of expert features grows, the difficulty in memorizing or referencing such a catalog of features may overbear the ease of explanation. Explainability in AI remains a trade-off between complexity of problem solving and explainability of solution mechanics.</p><p id="p-0069" num="0065">Embodiments have shown a satisfactory middle ground for this defined problem, embodiments relegate the size of the SME dictionary and component features therein to a specific application.</p><p id="p-0070" num="0066">The computing system used for the radio frequency environment awareness with explainable results system for performing (or controlling) the operations or functions described hereinabove with respect to the system and/or the method may include a processor, FPGA, I/O devices, a memory system, and a network adaptor. The computing system includes a program module (not shown) for performing (or controlling) the operations or functions described hereinabove with respect to the system and/or the method according to exemplary embodiments. For example, the program module may include routines, programs, objects, components, logic, data structures, or the like, for performing particular tasks or implement particular abstract data types. The processor may execute instructions written in the program module to perform (or control) the operations or functions described hereinabove with respect to the system and/or the method. The program module may be programmed into the integrated circuits of the processor. In an exemplary embodiment, the program module may be stored in the memory system or in a remote computer system storage media.</p><p id="p-0071" num="0067">The computing system may include a variety of computing system readable media. Such media may be any available media that is accessible by the computer system, and it may include both volatile and non-volatile media, removable and non-removable media.</p><p id="p-0072" num="0068">The memory system can include computer system readable media in the form of volatile memory, such as random access memory (RAM) and/or cache memory or others. The computer system may further include other removable/non-removable, volatile/non-volatile computer system storage media. The computer system can communicate with one or more devices using the network adapter. The network adapter may support wired communications based on Internet, LAN, WAN, or the like, or wireless communications based on CDMA, GSM, wideband CDMA, CDMA-2000, TDMA, LTE, wireless LAN, Bluetooth, or the like.</p><p id="p-0073" num="0069">The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.</p><p id="p-0074" num="0070">The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be, for example, but is not limited to, an electronic storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. A non-exhaustive list of more specific examples of the computer readable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a portable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable combination of the foregoing. A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through a waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.</p><p id="p-0075" num="0071">Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the Internet, a local area network, a wide area network and/or a wireless network. The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing/processing device.</p><p id="p-0076" num="0072">Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++ or the like, and procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present invention.</p><p id="p-0077" num="0073">Aspects of the present invention are described herein with reference to a flowchart illustration and/or block diagram of methods, apparatus (systems), and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.</p><p id="p-0078" num="0074">These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0079" num="0075">The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or other device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0080" num="0076">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0081" num="0077">The foregoing description of the embodiments has been presented for the purposes of illustration and description. It is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of this disclosure. It is intended that the scope of the present disclosure be limited not by this detailed description, but rather by the claims appended hereto.</p><p id="p-0082" num="0078">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the scope of the disclosure. Although operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results.</p><p id="p-0083" num="0079">Each and every page of this submission, and all contents thereon, however characterized, identified, or numbered, is considered a substantive part of this application for all purposes, irrespective of form or placement within the application. This specification is not intended to be exhaustive or to limit the invention to the precise form disclosed. Many modifications and variations are possible in light of this disclosure. Other and various embodiments will be readily apparent to those skilled in the art, from this description, figures, and the claims that follow. It is intended that the scope of the invention be limited not by this detailed description, but rather by the claims appended hereto.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A Deep-Learning (DL) explainable AI system for Radio Frequency (RF) machine learning applications with expert driven neural explainability of input signals comprising:<claim-text>a Classifier Module;</claim-text><claim-text>an Explainability Module;</claim-text><claim-text>an Important Features module;</claim-text><claim-text>a Training Phase; and</claim-text><claim-text>an Inference Phase;</claim-text><claim-text>said Training Phase comprising a first training input comprising a Ground Truth Training Input and a second training input comprising a Raw RF Features Training Input;</claim-text><claim-text>said Inference Phase comprising input of Raw RF Features input signals; and</claim-text><claim-text>an output of Ground Truth comprising Feature Annotation, whereby said explainability is provided.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said Training Phase comprises:<claim-text>an Error Between Ground Truth and Class Prediction Module receiving said first training input of a Class Ground Truth Target;</claim-text><claim-text>an RF Classifier Module (RCM) RCM Training Update Module receiving input from said Error Between Ground Truth and Class Prediction Module;</claim-text><claim-text>an RF Classifier Module (RCM) receiving input from said RCM Training Update Module;</claim-text><claim-text>a Class Prediction Module receiving input from said RCM;</claim-text><claim-text>an Error Between RCM Prediction and Explainability Module (EM) Prediction Module receiving a first input, from said Class Prediction Module;</claim-text><claim-text>an EM Training Update Module receiving input from said Error Between RCM Prediction and EM Prediction Module;</claim-text><claim-text>an Explainability Module (EM) receiving input from said EM Training Update Module;</claim-text><claim-text>a Class Prediction with EM Annotations Module receiving input from said EM, said Class Prediction with EM Annotations Module providing a second input to said Error Between RCM Prediction and EM Prediction Module;</claim-text><claim-text>a Genetic Algorithm Discovery of K-most Important Class Features Module also receiving input from said EM;</claim-text><claim-text>said second training input Raw RF Features Training Input providing input to said RCM;</claim-text><claim-text>a Raw-to-Expert Feature Mapping Module also receiving input from said Raw RF Features training input;</claim-text><claim-text>an Expert RF Features Module receiving input from said Raw-to-Expert Feature Mapping Module, and providing a second input to said EM; and</claim-text><claim-text>a Genetic Algorithm Discovery of K-most Important Class Features Module receiving an input from said Expert RF Features Module, and receiving a second input from said EM, thereby producing a trained system.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said Inference Phase comprises:<claim-text>a trained Classifier Module receiving said Raw RF Features input signals;</claim-text><claim-text>a Raw-to-Expert Feature Mapping Module also receiving said Raw RF Features input signals;</claim-text><claim-text>an Expert RF Features Module receiving input from said Raw-to-Expert Feature Mapping Module;</claim-text><claim-text>a trained Explainability Module receiving input from said Expert RF Features Module; and</claim-text><claim-text>outputting a Class Prediction with K-most Important Expert Feature Annotations for said Raw RF Features input signals, whereby said input signal classification with explainability is provided.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said Radio Frequency (RF) machine learning applications comprise scene context change detection and classification; and<claim-text>input signals are digital Radio Frequency (RF) Wi-Fi 802.11a/g waveforms.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein classes comprise two primary traffic parameters, a number of devices in a scene and their spectrum usage or spectral density.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a number of devices comprise 3 sub-classes, low, medium, and high.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein spectral density comprises 3 sub-classes, low, medium, and high.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein expert feature generation provides a set of human interpretable features that an expert who is monitoring the spectrum would understand and use to describe the scene.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein expert features comprise time, frequency, and power.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an EM feature vector comprises:<claim-text>a Brightness of received power normalized between 0 and 1;</claim-text><claim-text>a Time-half determining an amount of activity in early and later parts of a monitored period, enumerated as 1 or 2;</claim-text><claim-text>an Energy in segment as a sum of all values in segment x, min-maxed normalized;</claim-text><claim-text>a Time-energy product in segment x count of time bins in segment x for which any pixels exceed half a max value in said segment x; and</claim-text><claim-text>a Consistent energy from segment x to y Boolean, set to true if both halves of a time period have relatively equal amounts of energy.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein said Brightness, said Segment energy, and said Time-energy product each comprises 8 features, 1 for each segment, said Consistent energy produces 4 features for channelized transients between time segments, and said Time-half produces 2 features, one for each half, wherein all features are encoded into said feature vector, of size 30.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising two correlated datasets, one of raw spectrograms for classification by said deep learning-based CM, and a second which is a simplified human annotated dictionary for classification by said EM.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said DCC comprises a 3-dimensional DCC operator whereby a tight coupling of phase and magnitude is maintained throughout feature extraction of a network.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein said EM comprises a down-selection of features most relevant for each classification label, wherein classes are processed one class at a time in a one-versus-all fashion.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein each sentence s can be thought of as a polytope edge in a full feature space which defines an activation of a class label.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transient computer readable medium containing program instructions for causing a computer to perform the method of:<claim-text>inputting a first training input comprising a Ground Truth Training Input in a Training Phase; and</claim-text><claim-text>inputting a second training input comprising a Raw RF Features Training Input in said Training Phase;</claim-text><claim-text>training a Classifier Module in said Training Phase;</claim-text><claim-text>training an Explainability Module in said Training Phase;</claim-text><claim-text>training an Important Features module in said Training Phase;</claim-text><claim-text>inputting Raw RF Features input signals in an Inference Phase; and</claim-text><claim-text>outputting Ground Truth comprising Feature Annotation, whereby explainability is provided.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein input comprises a spectrogram for each sample scene class being 5 MHz wide and 1 ms in duration.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein input comprises a spectrogram for each sample scene class with bin spacing forming a 128&#xd7;38&#xd7;2 sized image comprising phase and magnitude, where nfft=128, noverlap=128, and window=256.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein input comprises spectrograms comprising two channels in a third dimension where phase and magnitude representation maintain a complex-valued nature of underlying data, and a training dataset is constructed with 1,000 examples per class.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A Deep-Learning (DL) explainable AI method for scene context change detection and classification with expert driven neural explainability of input signals comprising:<claim-text>a Training Phase; and</claim-text><claim-text>an Inference Phase;</claim-text></claim-text><claim-text>said Training Phase comprising:<claim-text>an Error Between Ground Truth and Class Prediction Module receiving said first training input of a Class Ground Truth Target;</claim-text><claim-text>an RCM Training Update Module receiving input from said Error Between Ground Truth and Class Prediction Module;</claim-text><claim-text>an RF Classifier Module (RCM) receiving input from said RCM Training Update Module;</claim-text><claim-text>a Class Prediction Module receiving input from said RCM;</claim-text><claim-text>an Error Between RCM Prediction and Explainability Module (EM) Prediction Module receiving a first input, from said Class Prediction Module;</claim-text><claim-text>an EM Training Update Module receiving input from said Error Between RCM Prediction and EM Prediction Module;</claim-text><claim-text>an Explainability Module (EM) receiving input from said EM Training Update Module;</claim-text><claim-text>a Class Prediction with EM Annotations Module receiving input from said EM, said Class Prediction with EM Annotations Module providing a second input to said Error Between RCM Prediction and EM Prediction Module;</claim-text><claim-text>a Genetic Algorithm Discovery of K-most Important Class Features Module also receiving input from said EM;</claim-text><claim-text>said second training input Raw RF Features Training Input providing input to said RCM;</claim-text><claim-text>a Raw-to-Expert Feature Mapping Module also receiving input from said Raw RF Features training input;</claim-text><claim-text>an Expert RF Features Module receiving input from said Raw-to-Expert Feature Mapping Module, and providing a second input to said EM; and</claim-text><claim-text>a Genetic Algorithm Discovery of K-most Important Class Features Module receiving an input from said Expert RF Features Module, and receiving a second input from said EM, thereby producing a trained system;</claim-text></claim-text><claim-text>wherein said Inference Phase comprises:<claim-text>a trained RF Classifier Module receiving said Raw RF Features input signals;</claim-text><claim-text>a trained Raw-to-Expert Feature Mapping Module also receiving said Raw RF Features input signals;</claim-text><claim-text>a trained Expert RF Features Module receiving input from said Raw-to-Expert Feature Mapping Module;</claim-text><claim-text>a trained Explainability Module receiving input from said Expert RF Features Module; and</claim-text><claim-text>outputting a Class Prediction with K-most Important Expert Feature Annotations for said Raw RF Features input signals, whereby said input signal classification with explainability is provided.</claim-text></claim-text></claim></claims></us-patent-application>