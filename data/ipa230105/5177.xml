<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005178A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005178</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17764741</doc-number><date>20210122</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010215923.4</doc-number><date>20200325</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>426</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>80</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>148</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>262</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>426</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>443</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>806</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>153</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>274</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND APPARATUS FOR RETRIEVING TARGET</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Wodong Tianjun Information Technology Co., Ltd</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Beijing Jingdong Century Trading Co., Ltd.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Wu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Liu</last-name><first-name>Jiawei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Mei</last-name><first-name>Tao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Zheng</last-name><first-name>Kecheng</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Beijing Wodong Tianjun Information Technology Co., Ltd</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee><assignee><addressbook><orgname>Beijing Jingdong Century Trading Co., Ltd.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2021/073322</doc-number><date>20210122</date></document-id><us-371c12-date><date>20220329</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method and an apparatus for retrieving a target are provided. The method may include: obtaining at least one image and a description text of a designated object; extracting image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network; and matching the image features with the text features to determine an image that contains the designated object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="88.05mm" wi="109.73mm" file="US20230005178A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="115.32mm" wi="115.49mm" file="US20230005178A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="102.02mm" wi="111.76mm" file="US20230005178A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="232.16mm" wi="138.94mm" file="US20230005178A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="195.83mm" wi="167.89mm" file="US20230005178A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="102.79mm" wi="145.97mm" file="US20230005178A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a U.S. national stage application of International Application No. PCT/CN2021/073322, filed on Jan. 21, 2021, which claims the priority from Chinese Patent Application No. 202010215923.4, filed on Mar. 25, 2020 and titled &#x201c;Method and Apparatus for Retrieving Target,&#x201d; the applicants of which are Beijing Wodong Tianjun Information Technology Co., Ltd. and Beijing Jingdong Century Trading Co., Ltd. The contents of each of the aforementioned applications are hereby incorporated by reference in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">Embodiments of the present disclosure relate to the field of computer technology, and in particular, to a method and apparatus for retrieving a target.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">With the rapid development of Internet technology, the presentation approaches of media data are becoming more and more diverse, and different types of media data describe a given object from different perspectives.</p><p id="p-0005" num="0004">It is expected that cross-media retrieval between different types of media data can be realized, that is, through a type of media data, media data of another media type having same semantics can be retrieved by querying.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the present disclosure propose a method and apparatus for retrieving a target.</p><p id="p-0007" num="0006">In a first aspect, an embodiment of the present disclosure provides a method for retrieving a target, the method including: obtaining at least one image and a description text of a designated object; extracting image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network, where the cross-media feature extraction network projects the text features and the image features to a common feature space of images and texts; and matching the image features with the text features to determine an image containing the designated object.</p><p id="p-0008" num="0007">In a second aspect, an apparatus for retrieving a target is provided according an embodiment of the disclosure. The apparatus includes: one or more processors; and a storage apparatus, storing one or more programs, where the one or more programs, when executed by the one or more processors, cause the one or more processors to implement the method described in any one of the embodiments of the first aspect.</p><p id="p-0009" num="0008">In a third aspect, an embodiment of the present disclosure provides a computer readable medium, storing a computer program, where the computer program, when executed by a processor, implements the method described in any one of the embodiments of the first aspect.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">By reading detailed description of non-limiting embodiments with reference to the following accompanying drawings, other features, objectives and advantages of the present disclosure will become more apparent:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an example system architecture diagram to which an embodiment of the present disclosure may be applied;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of an embodiment of a method for retrieving a target according to the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an implementation of a method for generating the cross-media feature extraction network;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an architecture schematic diagram of an implementation process of the method for retrieving a target of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic structural diagram of an embodiment of an apparatus for retrieving a target according to the present disclosure; and</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic structural diagram of an electronic device adapted for implementing the embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0017" num="0016">The present disclosure will be further described in detail below with reference to the accompanying drawings and embodiments. It may be understood that the specific embodiments described herein are only used to explain the related disclosure, but not to limit the disclosure. In addition, it should also be noted that, for ease of description, only parts related to the relevant disclosure are shown in the accompanying drawings.</p><p id="p-0018" num="0017">It should be noted that the embodiments in the present disclosure and the features in the embodiments may be combined with each other on a non-conflict basis. The present disclosure will be described below in detail with reference to the accompanying drawings and in combination with the embodiments.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows an example architecture <b>100</b> to which a method for retrieving a target or an apparatus for retrieving a target according to the present disclosure may be applied.</p><p id="p-0020" num="0019">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system architecture <b>100</b> may include terminal devices <b>101</b>, <b>102</b>, <b>103</b>, a network <b>104</b> and a server <b>105</b>. The network <b>104</b> serves as a medium for providing a communication link between the terminal devices <b>101</b>, <b>102</b>, <b>103</b> and the server <b>105</b>. The network <b>104</b> may include various types of connections, such as wired or wireless communication links, or optical fiber cables.</p><p id="p-0021" num="0020">The terminal devices <b>101</b>, <b>102</b>, <b>103</b> interact with the server <b>105</b> through the network <b>104</b> to receive or send messages and so on. Various communication client applications, such as an image editing application, a text editing application, a browser application, may be installed on the terminal devices <b>101</b>, <b>102</b>, <b>103</b>.</p><p id="p-0022" num="0021">The terminal devices <b>101</b>, <b>102</b>, and <b>103</b> may be hardware or software. When the terminal devices <b>101</b>, <b>102</b>, <b>103</b> are hardware, they may be various electronic devices that have a display screen and support Internet access, including but not limited to smart phones, tablet computers, laptop computers, desktop computers, and so on. When the terminal devices <b>101</b>, <b>102</b>, <b>103</b> are software, they may be installed in the electronic devices listed above. They may be implemented as a plurality of software pieces or software modules (for example, for providing distributed services), or as a single software piece or software module, which is not limited herein.</p><p id="p-0023" num="0022">The server <b>105</b> may be a server that provides various services, such as a backend server that matches images and texts obtained by the terminal devices <b>101</b>, <b>102</b>, and <b>103</b>. The backend server may recognize and match the received images and texts.</p><p id="p-0024" num="0023">It should be noted that the method for retrieving a target provided by the embodiments of the present disclosure is generally executed by the server <b>105</b>. Correspondingly, the apparatus for retrieving a target is generally provided in the server <b>105</b>.</p><p id="p-0025" num="0024">It should be appreciated that the number of terminal devices, networks and servers in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is merely illustrative. Any number of terminal devices, networks and servers may be provided depending on the implementation needs.</p><p id="p-0026" num="0025">With further reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a flow <b>200</b> of an embodiment of a method for retrieving a target is illustrated according to the present disclosure. The method for retrieving a target includes the following steps.</p><p id="p-0027" num="0026">Step <b>201</b> includes obtaining at least one image and a description text of a designated object.</p><p id="p-0028" num="0027">In the present embodiment, an executing body of the method for retrieving a target (the server as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) may obtain the at least one image and the description text of the designated object from a pre-stored image library. Here, the image may be any to-be-searched image in the pre-stored image library.</p><p id="p-0029" num="0028">Here, the designated object refers to a to-be-retrieved entity. The designated object may be an object with a variable position, such as a pedestrian, or a vehicle. The designated object may alternatively be an object with a fixed position, such as a building, or a landscape. The number of the designated objects may be one or more. The image may contain the designated object or other objects. The text may be a sentence or a word that describes features of the designated object. For example, if the designated object is pedestrian A, then in step <b>201</b>, a to-be-searched image containing a pedestrian in the image library, as well as a description text of the appearance, action and other features of the pedestrian A may be obtained. Then, in a subsequent step, the to-be-searched image is matched with the description text of the pedestrian A, and an image containing the pedestrian A that matches the description text of the pedestrian A is determined from the to-be-searched image containing the pedestrian.</p><p id="p-0030" num="0029">Step <b>202</b> includes extracting image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network.</p><p id="p-0031" num="0030">In the present embodiment, the executing body may use the pre-trained cross-media feature extraction network to extract the image features of the at least one image and the text features of the description text of the designated object in step <b>201</b>. The cross-media feature extraction network may project the text features and the image features to a common feature space of images and texts. Specifically, the cross-media feature extraction network may extract features of data of different media types, and may transform the features of data of different media types to data in a given common feature space, so that feature matching can be performed on the features of data of different media types in the given common feature space.</p><p id="p-0032" num="0031">Here, the cross-media feature extraction network may be an artificial neural network. Based on the at least one image and the description text of the designated object obtained in step <b>201</b>, the executing body may input the one or more images and the description text into the pre-trained artificial neural network, and extract the image features corresponding to the image and the text features corresponding to the text. A partial or a whole image may be selected from the one or more images to extract image features of the selected partial or whole image. A partial or a whole description text may be selected from the description text of the designated object, to extract text features of the selected partial or whole text. The features may be represented by feature vectors.</p><p id="p-0033" num="0032">Step <b>203</b> includes matching the image features with the text features to determine an image containing the designated object.</p><p id="p-0034" num="0033">In the present embodiment, the executing body may match the image features with the text features extracted in step <b>202</b>, to determine the image containing the designated object from the at least one image as an image matching the text description.</p><p id="p-0035" num="0034">Here, the executing body may determine whether the image features match the text features by calculating a similarity between the image features of each image and the text features of the description text. The similarity may be related to a distance between the image features and the text features, for example, Euclidean distance, Mingshi distance, Manhattan distance, Chebyshev distance, Mahalanobis distance, etc. may be used to calculate the similarity between the image features and the text features.</p><p id="p-0036" num="0035">In practice, in order to perform more comprehensive matching to obtain image features similar to the text features, a similarity between each image feature and the text features may alternatively be calculated first, and the similarities between image features and the text features are sorted in a descending order, a pre-set number of top ranked image features are selected, and an image indicated by the pre-set number of top ranked image features is used as the image matching the text description. The pre-set number may be set according to actual needs, and may be one or more. Here, the higher the similarity between the text features and the image feature is, the greater the possibility that an object indicated by the text features and an object indicated by the image feature are a given object is.</p><p id="p-0037" num="0036">In the method for retrieving a target provided by the above embodiment of the present disclosure, at least one image and a description text of a designated object are obtained, then image features of the image and text features of the description text are extracted by using a pre-trained cross-media feature extraction network, where the cross-media feature extraction network projects the text features and the image features to a common feature space of images and texts, and finally the image features and the text features are matched to determine an image containing the designated object, thus, features are extracted by using cross-media features, and the image features and the text features are projected to the common feature space of images and texts for feature matching, thereby achieving cross-media target search.</p><p id="p-0038" num="0037">With further reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an implementation of a method for generating the cross-media feature extraction network. The flow <b>300</b> of the method for generating the cross-media feature extraction network may include the following steps.</p><p id="p-0039" num="0038">Step <b>301</b> includes obtaining a training sample set.</p><p id="p-0040" num="0039">In the present embodiment, the executing body may obtain the training sample set from a preset database. The training sample set may include a sample image-text pair, and the sample image-text pair may include: a sample image and a sample text describing an object contained in the sample image.</p><p id="p-0041" num="0040">Step <b>302</b> includes obtaining an initial network.</p><p id="p-0042" num="0041">In the present embodiment, the executing body may first obtain the initial network. Specifically, the initial network may include a to-be-trained cross-media feature extraction network, a discriminative network for discriminating a data type of a feature source, and a feature transformation network, and the to-be-trained cross-media feature extraction network may include an image graph attention network, and a text graph attention network. The initial network may be an untrained neural network with initialized parameters, or a pre-trained neural network.</p><p id="p-0043" num="0042">Step <b>303</b> includes selecting a sample from the training sample set and performing training steps.</p><p id="p-0044" num="0043">In the present embodiment, the executing body may select the sample from the sample set obtained in step <b>301</b>, and perform the training steps including step <b>3031</b> to step <b>3036</b>. A selection approach and selection number of samples are not limited in the present disclosure. For example, the executing body may select at least one sample.</p><p id="p-0045" num="0044">More specifically, the training steps include the following steps.</p><p id="p-0046" num="0045">Step <b>3031</b> includes inputting the sample image in the sample image-text pair into the image graph attention network to obtain a first feature.</p><p id="p-0047" num="0046">In the present embodiment, the executing body may input the sample image in the sample image-text pair into the image graph attention network of the to-be-trained cross-media feature extraction network, and output the first feature, that is, image features of the sample image.</p><p id="p-0048" num="0047">In some alternative implementations of the present embodiment, the image graph attention network includes a residual network, an image graph attention convolutional network and a joint embedding layer. Specifically, the first feature may be obtained through the following steps.</p><p id="p-0049" num="0048">The first step includes extracting initial image features of the sample image using the residual network.</p><p id="p-0050" num="0049">In this alternative implementation, the executing body may extract the initial image features of the sample image using the residual network, that is, low-level visual features. Here, the initial image features may be global image features, such as color features, texture features, shape features, or structural features.</p><p id="p-0051" num="0050">Here, after extracting the initial image features of the sample image using the residual network, the executing body may alternatively use an average pooling layer to generate a global appearance visual feature vector from the initial image features, and use the average pooling layer to reduce a dimension of the initial image features. The generated global appearance visual feature vector may preserve significant features in the initial image features.</p><p id="p-0052" num="0051">The second step includes inputting the initial image features into the image graph attention convolutional network, to output structured image features of the sample image.</p><p id="p-0053" num="0052">In this alternative implementation, the executing body may input the extracted low-level visual features into the pre-trained image graph attention convolutional network to obtain the structured image features of the sample image. Here, the structured image features may be used to represent structured semantic visual features of the sample image.</p><p id="p-0054" num="0053">Alternatively, the inputting the initial image features into the image graph attention convolutional network, to output structured image features of the sample image, may also be implemented through the following steps: performing a target object detection on the sample image, determining a target object in the sample image and a position of a rectangular bounding box of the target object, and extracting a relevant feature of the target object based on the position of the rectangular bounding box of the target object; constructing an image feature directed graph, where vertices of the image feature directed graph represent target objects, and directed edges of the image feature directed graph represent an association relationship between the target objects; and generating the structured image features of the sample image based on the image feature directed graph.</p><p id="p-0055" num="0054">In this alternative implementation, the executing body may obtain the structured image features of the sample image through the constructed image feature directed graph.</p><p id="p-0056" num="0055">Specifically, the executing body may first recognize the target object in the sample image and the position of the rectangular bounding box of the target object by using a preset target detection algorithm, and then extract an appearance feature of the target object from the rectangular bounding box of the target object by using the target detection algorithm, and extract the relevant feature of the target object based on the position of the rectangular bounding box of the target object. For example, the relevant feature of the target object may include at least one of: the appearance feature of the target object, a position feature of the target object, and a type feature of the target object. For example, the target detection algorithm R-CNN (Region-Convolutional Neural Networks) may be used to detect the target object in the sample image. Here, the target object may be any object pre-designated according to actual application requirements, such as shoes, glasses. The position feature of the rectangular bounding box of the target object may be represented by position coordinates of the rectangular bounding box of the target object in the current sample image, for example, represented by the abscissa of a vertex of the rectangular bounding box, represented by the ordinate of a vertex of the rectangular bounding box, or represented by the position coordinates of a tuple of a relationship between a width of the rectangular bounding box and a height of the rectangular bounding box. The type of the target object may be identified, for example, according to its shape, color and other features, and the executing body may use a preset entity relationship classifier to determine the association relationship between the target objects, and use a preset attribute classifier to determine an attribute of the target object. The appearance feature of the target object may include a global appearance feature and a local appearance feature.</p><p id="p-0057" num="0056">Then, the image feature directed graph may be constructed. The vertices of the image feature directed graph represent target objects, which may be represented by O<sub>i </sub>(O<sub>1</sub>, O<sub>2</sub>, O<sub>3</sub>, O<sub>4</sub>, O<sub>5</sub>, O<sub>6</sub>), and O<sub>i </sub>may represent the subject or object in the triple &#x201c;subject-predicate-object&#x201d;, the directed edges of the image feature directed graph represent the association relationship between the target objects, which may be represented by e<sub>ij </sub>(e<sub>15</sub>, e<sub>16</sub>, e<sub>21</sub>, e<sub>31</sub>, e<sub>41</sub>), for example, the directed edge e<sub>ij </sub>represents the relationship between the object O<sub>i </sub>and the object O<sub>j</sub>, e<sub>ij </sub>represents the predicate in the triple &#x201c;subject-predicate-object&#x201d;, O<sub>i </sub>may be the subject in the triple &#x201c;subject-predicate-object&#x201d;, and O<sub>j </sub>may be the object in the triple &#x201c;subject-predicate-object&#x201d;. The vertex features of the image feature directed graph may be represented by the obtained relevant feature representing the target object, and the relevant feature of the target object may include at least one of: the appearance feature of the target object, the position feature of the target object, and the type feature of the target object. The directed edge features of the image feature directed graph may be composed of at least one of: the appearance feature of the target object, the position feature of the target object, and the type feature of the target object.</p><p id="p-0058" num="0057">Finally, a graph attention convolutional layer may be used to update the constructed image feature directed graph, and extract the structured image features of the sample image. Specifically, the image feature directed graph may be updated by using the graph attention convolutional layer to obtain updated vertices. The following update formula may be set to update the vertex features in the image feature directed graph:</p><p id="p-0059" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>v</mi>      <msub>       <mi>o</mi>       <mi>i</mi>      </msub>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <munder>        <mo>&#x2211;</mo>        <mrow>         <msub>          <mi>o</mi>          <mi>j</mi>         </msub>         <mo>&#x2208;</mo>         <mrow>          <mi>sbj</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <msub>           <mi>o</mi>           <mi>i</mi>          </msub>          <mo>)</mo>         </mrow>        </mrow>       </munder>       <mrow>        <msub>         <mi>w</mi>         <mrow>          <mi>i</mi>          <mo>&#x2062;</mo>          <mi>j</mi>         </mrow>        </msub>        <mo>&#xb7;</mo>        <mrow>         <msub>          <mi>g</mi>          <mi>s</mi>         </msub>         <mo>(</mo>         <mrow>          <msub>           <mi>v</mi>           <msub>            <mi>o</mi>            <mi>i</mi>           </msub>          </msub>          <mo>,</mo>          <msub>           <mi>v</mi>           <msub>            <mi>e</mi>            <mi>ij</mi>           </msub>          </msub>          <mo>,</mo>          <msub>           <mi>v</mi>           <msub>            <mi>o</mi>            <mi>j</mi>           </msub>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>      <mo>+</mo>      <mrow>       <munder>        <mo>&#x2211;</mo>        <mrow>         <msub>          <mi>o</mi>          <mi>k</mi>         </msub>         <mo>&#x2208;</mo>         <mrow>          <mi>obj</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <msub>           <mi>o</mi>           <mi>i</mi>          </msub>          <mo>)</mo>         </mrow>        </mrow>       </munder>       <mrow>        <msub>         <mi>w</mi>         <mrow>          <mi>i</mi>          <mo>&#x2062;</mo>          <mi>k</mi>         </mrow>        </msub>        <mo>&#xb7;</mo>        <mrow>         <msub>          <mi>g</mi>          <mi>o</mi>         </msub>         <mo>(</mo>         <mrow>          <msub>           <mi>v</mi>           <msub>            <mi>o</mi>            <mi>k</mi>           </msub>          </msub>          <mo>,</mo>          <msub>           <mi>v</mi>           <msub>            <mi>e</mi>            <mrow>             <mi>k</mi>             <mo>&#x2062;</mo>             <mi>i</mi>            </mrow>           </msub>          </msub>          <mo>,</mo>          <msub>           <mi>v</mi>           <msub>            <mi>o</mi>            <mi>i</mi>           </msub>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0060" num="0058">where, g<sub>s</sub>, g<sub>o </sub>represent a fully connected layer, w<sub>ij </sub>represents a weight of the node j to the node i, w<sub>ik </sub>represents a weight of the node k to the node i, o<sub>j </sub>represents the node representing the subject, o<sub>j </sub>represents the node representing the predicate, v<sub>o</sub><sub><sub2>i</sub2></sub>, v<sub>o</sub><sub><sub2>k</sub2></sub>, v<sub>o</sub><sub><sub2>j </sub2></sub>represent node features, and v<sub>e</sub><sub><sub2>ij</sub2></sub>, v<sub>e</sub><sub><sub2>ki </sub2></sub>represent directed edge features between nodes.</p><p id="p-0061" num="0059">w<sub>ij </sub>may be calculated through the following formula:</p><p id="p-0062" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>w</mi>      <mrow>       <mi>i</mi>       <mo>&#x2062;</mo>       <mi>j</mi>      </mrow>     </msub>     <mo>=</mo>     <mfrac>      <mrow>       <mi>exp</mi>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <msub>           <mi>w</mi>           <mi>a</mi>          </msub>          <mo>&#xb7;</mo>          <msub>           <mi>v</mi>           <msub>            <mi>e</mi>            <mi>ij</mi>           </msub>          </msub>         </mrow>         <mo>+</mo>         <msub>          <mi>b</mi>          <mi>a</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <msub>        <mo>&#x2211;</mo>        <mi>j</mi>       </msub>       <mrow>        <mi>exp</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mrow>         <mo>(</mo>         <mrow>          <mrow>           <msub>            <mi>w</mi>            <mi>a</mi>           </msub>           <mo>&#xb7;</mo>           <msub>            <mi>v</mi>            <msub>             <mi>e</mi>             <mi>ij</mi>            </msub>           </msub>          </mrow>          <mo>+</mo>          <msub>           <mi>b</mi>           <mi>a</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0063" num="0060">where, w<sub>a </sub>represents a weight of the directed edge feature v<sub>e</sub><sub><sub2>ij</sub2></sub>, and b<sub>a </sub>represents a bias term.</p><p id="p-0064" num="0061">The updated vertex features of the image feature directed graph may be obtained through the above update formula. In order to fuse the updated vertex features, a virtual vertex is set to connect the vertices of the image feature directed graph. The virtual vertex may generate the structured image features of the sample image through the following formula:</p><p id="p-0065" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>v</mi>      <msub>       <mi>o</mi>       <mi>v</mi>      </msub>     </msub>     <mo>=</mo>     <mrow>      <munder>       <mo>&#x2211;</mo>       <mrow>        <msub>         <mi>o</mi>         <mi>i</mi>        </msub>        <mo>&#x2208;</mo>        <mi>O</mi>       </mrow>      </munder>      <mrow>       <msub>        <mi>w</mi>        <mi>i</mi>       </msub>       <mo>&#xb7;</mo>       <mrow>        <msub>         <mi>g</mi>         <mi>v</mi>        </msub>        <mo>(</mo>        <msub>         <mi>v</mi>         <msub>          <mi>o</mi>          <mi>i</mi>         </msub>        </msub>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0066" num="0062">where, v<sub>o</sub><sub><sub2>v </sub2></sub>represents the virtual vertex feature, w<sub>i </sub>represents a weight of the node i, and g<sub>v </sub>represents a fully connected layer.</p><p id="p-0067" num="0063">w<sub>i </sub>may be calculated through the following formula:</p><p id="p-0068" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>w</mi>      <mi>i</mi>     </msub>     <mo>=</mo>     <mfrac>      <mrow>       <mi>exp</mi>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <msub>           <mi>w</mi>           <mi>c</mi>          </msub>          <mo>&#xb7;</mo>          <msub>           <mi>v</mi>           <msub>            <mi>o</mi>            <mi>v</mi>           </msub>          </msub>         </mrow>         <mo>+</mo>         <msub>          <mi>b</mi>          <mi>c</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <msub>        <mo>&#x2211;</mo>        <mi>j</mi>       </msub>       <mtext>  </mtext>       <mrow>        <mi>exp</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mrow>         <mo>(</mo>         <mrow>          <mrow>           <msub>            <mi>w</mi>            <mi>c</mi>           </msub>           <mo>&#xb7;</mo>           <msub>            <mi>v</mi>            <msub>             <mi>o</mi>             <mi>v</mi>            </msub>           </msub>          </mrow>          <mo>+</mo>          <msub>           <mi>b</mi>           <mi>c</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0069" num="0064">where, w<sub>c </sub>represents a weight of the virtual vertex feature v<sub>o</sub><sub><sub2>v</sub2></sub>, and b<sub>c </sub>represents a bias term.</p><p id="p-0070" num="0065">Since the virtual vertex is connected to the vertices of the image feature directed graph, the image feature directed graph contains the relevant feature of the target object and the association relationship feature between the target objects. Therefore, the virtual vertex may fuse all the features in the updated image feature directed graph to generate the structured image features representing structured semantic information of the sample image, which may contain more effective structured semantic information, more comprehensively and more accurately represent the image features and more effectively distinguish and recognize the target object contained in the image.</p><p id="p-0071" num="0066">The third step includes fusing the initial image features with the structured image features to generate image features.</p><p id="p-0072" num="0067">In this alternative implementation, the executing body may fuse the initial image features obtained in the first step with the structured image features obtained in the second step to obtain the image features of the sample image.</p><p id="p-0073" num="0068">The fourth step includes inputting the image features into the joint embedding layer to obtain the first feature.</p><p id="p-0074" num="0069">In this alternative implementation, the executing body may input the image features obtained in the third step into the joint embedding layer to obtain the first feature. The joint embedding layer may be composed of three fully connected layers.</p><p id="p-0075" num="0070">Step <b>3032</b> includes inputting the sample text in the sample image-text pair into the text graph attention network to obtain a second feature.</p><p id="p-0076" num="0071">In the present embodiment, the executing body may input the sample text in the sample image-text pair into the text graph attention network of the to-be-trained cross-media feature extraction network, to output the second feature, i.e., text features of the sample text.</p><p id="p-0077" num="0072">In some alternative implementations of the present embodiment, the text graph attention network includes a bidirectional long short-term memory network, a text graph attention convolutional network and a joint embedding layer. Specifically, the second feature may be obtained through the following steps.</p><p id="p-0078" num="0073">The first step includes performing a word segmentation on the sample text to determine a word vector of the sample text.</p><p id="p-0079" num="0074">In this alternative implementation, the executing body may use a common word segmentation tool or manual annotation to perform the word segmentation on the sample text, and each word in the sample text is projected into a word vector.</p><p id="p-0080" num="0075">The second step includes extracting initial text features of the word vector of the sample text using the bidirectional long short-term memory network.</p><p id="p-0081" num="0076">In this alternative implementation, the executing body may extract the initial text features having contextual information in the sample text using the bidirectional long short-term memory network.</p><p id="p-0082" num="0077">The third step includes inputting the initial text features into the text graph attention convolutional network, to output structured text features of the sample text.</p><p id="p-0083" num="0078">In this alternative implementation, the executing body may input the extracted initial text features into the pre-trained text graph attention convolutional network to obtain the structured text features of the sample text. Here, the structured text features may be used to represent structured semantic text features of the sample text.</p><p id="p-0084" num="0079">Alternatively, the inputting the initial text features into the text graph attention convolutional network, to output structured text features of the sample text, may alternatively be implemented through the following steps: constructing a text feature directed graph, where vertices of the text feature directed graph represent the target objects indicated by the word vectors, and directed edges of the text feature directed graph represent an association relationship between the target objects indicated by the word vectors; and generating the structured text features of the sample text based on the text feature directed graph.</p><p id="p-0085" num="0080">In this alternative implementation, the executing body may obtain the structured text features of the sample text through the constructed text feature directed graph.</p><p id="p-0086" num="0081">Specifically, the executing body may first construct the text feature directed graph. The vertices of the text feature directed graph represent target objects, which may be represented by O<sub>i </sub>(O<sub>1</sub>, O<sub>2</sub>, O<sub>3</sub>, O<sub>4</sub>, O<sub>5</sub>, O<sub>6</sub>), and O<sub>i </sub>may represent the subject or object in the triple &#x201c;subject-predicate-object&#x201d;, the directed edges of the text feature directed graph represent the association relationship between the target objects, which may be represented by e<sub>ij </sub>(e<sub>15</sub>, e<sub>16</sub>, e<sub>21</sub>, e<sub>31</sub>, e<sub>41</sub>), for example, the directed edge e<sub>ij </sub>represents the relationship between the object O<sub>i </sub>and the object O<sub>j</sub>, e<sub>ij </sub>represents the predicate in the triple &#x201c;subject-predicate-object&#x201d;, O<sub>i </sub>may be the subject in the triple &#x201c;subject-predicate-object&#x201d;, and O<sub>j </sub>may be the object in the triple &#x201c;subject-predicate-object&#x201d;. The vertex features of the text feature directed graph may be composed of the relevant feature of the target object, and the relevant feature of the target object may include at least one of: an attribute feature of the target object and the type feature of the target object. The directed edge features of the text feature directed graph may be composed of the type features of the target object.</p><p id="p-0087" num="0082">Then, the graph attention convolutional layer may be used to update the constructed text feature directed graph, and the vertex features of the text feature directed graph may be updated using the above update formula (1). The executing body may set a virtual vertex connecting the vertices of the text feature directed graph. The virtual vertex may generate the structured text features of the sample text through the formula (3), which may contain more effective structured semantic information, more comprehensive and more accurate represent the text features and more effectively distinguish and recognize the target object contained in the text.</p><p id="p-0088" num="0083">The fourth step includes fusing the initial text features with the structured text features to generate text features.</p><p id="p-0089" num="0084">In this alternative implementation, the executing body may fuse the initial text features obtained in the second step with the structured text features obtained in the third step to obtain the text features of the sample text.</p><p id="p-0090" num="0085">The fifth step includes inputting the text features into the joint embedding layer to obtain the second feature.</p><p id="p-0091" num="0086">In this alternative implementation, the executing body may input the text features obtained in the fourth step into the joint embedding layer to obtain the second feature. The joint embedding layer may be composed of three fully connected layers.</p><p id="p-0092" num="0087">Step <b>3033</b> includes inputting the first feature and the second feature into the discriminative network to obtain a type discrimination result, and calculating a discriminative loss value based on the type discrimination result.</p><p id="p-0093" num="0088">In the present embodiment, the executing body may input the first feature and the second feature into the discriminative network to obtain the type discrimination result, and calculate the discriminative loss value based on the type discrimination result. The data type of the feature source includes a text type and an image type, and the discriminative loss value represents an error of determining types of the first feature and the second feature. The discriminative network may be composed of three fully connected layers, aiming to better determine and identify the data type of a source of a given feature, that is, a modal type of the feature, such as the text type, the image type, and the discriminative loss value L<sub>adv </sub>(&#x3b8;<sub>D</sub>) may be calculated through the following loss function:</p><p id="p-0094" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>L</mi>       <mi>adv</mi>      </msub>      <mo>(</mo>      <msub>       <mi>&#x3b8;</mi>       <mi>D</mi>      </msub>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mrow>        <mo>-</mo>        <mfrac>         <mn>1</mn>         <mi>N</mi>        </mfrac>       </mrow>       <mo>&#x2062;</mo>       <mrow>        <munderover>         <mo>&#x2211;</mo>         <mrow>          <mi>i</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>N</mi>        </munderover>        <mtext>  </mtext>        <mrow>         <mi>log</mi>         <mo>&#x2062;</mo>         <mtext>  </mtext>         <mrow>          <mi>D</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <msub>            <mi>v</mi>            <mi>i</mi>           </msub>           <mo>;</mo>           <msub>            <mi>&#x3b8;</mi>            <mi>D</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mrow>      <mo>+</mo>      <mrow>       <mi>log</mi>       <mo>&#x2062;</mo>       <mtext>  </mtext>       <mrow>        <mo>(</mo>        <mrow>         <mn>1</mn>         <mo>-</mo>         <mrow>          <mi>D</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <msub>            <mi>t</mi>            <mi>i</mi>           </msub>           <mo>;</mo>           <msub>            <mi>&#x3b8;</mi>            <mi>D</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0095" num="0089">where, v<sub>i </sub>represents the first feature, t<sub>i </sub>represents the second feature, D(v<sub>i</sub>; &#x3b8;<sub>D</sub>), D(t<sub>i</sub>; &#x3b8;<sub>D</sub>) represent a data type probability of the feature source of the input sample i, and &#x3b8;<sub>D </sub>represents a network parameter of the discriminative network.</p><p id="p-0096" num="0090">Step <b>3034</b> includes inputting the first feature and the second feature into the feature transformation network to obtain a feature transformation result, and calculating a value of an identification loss function and a value of a pairwise loss function based on the feature transformation result.</p><p id="p-0097" num="0091">In the present embodiment, the executing body may input the first feature and the second feature into the feature transformation network to obtain the feature transformation result, and calculating the value of the identification loss function and the value of the pairwise loss function based on the feature transformation result. The identification loss function represents an ability of distinguishing different objects contained in the first feature and the second feature in the common feature space of images and texts, and the pairwise loss function represents a semantic difference between the first feature and the second feature of a given object. The executing body may calculate the identification loss value L<sub>ide </sub>(&#x3b8;<sub>V</sub>, &#x3b8;<sub>T</sub>) through the following loss function:</p><p id="p-0098" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>L</mi>       <mrow>        <mi>i</mi>        <mo>&#x2062;</mo>        <mi>d</mi>        <mo>&#x2062;</mo>        <mi>e</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3b8;</mi>        <mi>V</mi>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3b8;</mi>        <mi>T</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mo>-</mo>       <mfrac>        <mn>1</mn>        <mi>N</mi>       </mfrac>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>N</mi>       </munderover>       <mtext>  </mtext>       <mrow>        <mi>log</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mfrac>         <msup>          <mi>e</mi>          <mrow>           <mrow>            <msubsup>             <mi>W</mi>             <msub>              <mi>y</mi>              <mi>i</mi>             </msub>             <mi>T</mi>            </msubsup>            <mo>&#x2062;</mo>            <msub>             <mi>x</mi>             <mi>i</mi>            </msub>           </mrow>           <mo>+</mo>           <mi>b</mi>          </mrow>         </msup>         <mrow>          <msubsup>           <mo>&#x2211;</mo>           <mrow>            <mi>j</mi>            <mo>=</mo>            <mn>1</mn>           </mrow>           <mi>K</mi>          </msubsup>          <msup>           <mi>e</mi>           <mrow>            <mrow>             <msubsup>              <mi>W</mi>              <mi>j</mi>              <mi>T</mi>             </msubsup>             <mo>&#x2062;</mo>             <msub>              <mi>x</mi>              <mi>j</mi>             </msub>            </mrow>            <mo>+</mo>            <mi>b</mi>           </mrow>          </msup>         </mrow>        </mfrac>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0099" num="0092">where y<sub>i </sub>represents the serial number of the target object corresponding to the i<sup>th </sup>sample (sample text or sample image), x<sub>i </sub>represents the first feature or the second feature, &#x3b8;<sub>V </sub>represents a network parameter of the image graph attention convolutional network, &#x3b8;<sub>T </sub>represents a network parameter of the text graph attention convolutional network, W<sub>j</sub>) represents the j<sup>th </sup>column of the weight matrix W, b represents a bias term, and N represents the number of samples.</p><p id="p-0100" num="0093">The executing body may calculate the pairwise loss value L<sub>pair </sub>(&#x3b8;<sub>V</sub>, &#x3b8;<sub>T</sub>) through the following loss function:</p><p id="p-0101" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>L</mi>       <mrow>        <mi>p</mi>        <mo>&#x2062;</mo>        <mi>a</mi>        <mo>&#x2062;</mo>        <mi>i</mi>        <mo>&#x2062;</mo>        <mi>r</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <msub>        <mi>&#x3b8;</mi>        <mi>v</mi>       </msub>       <mo>,</mo>       <msub>        <mi>&#x3b8;</mi>        <mi>T</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mrow>       <mo>-</mo>       <mfrac>        <mn>1</mn>        <mi>M</mi>       </mfrac>      </mrow>      <mo>&#x2062;</mo>      <mrow>       <munderover>        <mo>&#x2211;</mo>        <mrow>         <mi>i</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>M</mi>       </munderover>       <mtext>  </mtext>       <mrow>        <mi>log</mi>        <mo>&#x2062;</mo>        <mtext>  </mtext>        <mfrac>         <msup>          <mi>e</mi>          <msub>           <mi>W</mi>           <mrow>            <mi>p</mi>            <mo>,</mo>            <mrow>             <mrow>              <msubsup>               <mi>y</mi>               <mi>i</mi>               <mi>T</mi>              </msubsup>              <mo>&#x2062;</mo>              <msub>               <mi>z</mi>               <mi>i</mi>              </msub>             </mrow>             <mo>+</mo>             <msub>              <mi>b</mi>              <mi>p</mi>             </msub>            </mrow>           </mrow>          </msub>         </msup>         <mrow>          <msubsup>           <mo>&#x2211;</mo>           <mrow>            <mi>j</mi>            <mo>=</mo>            <mn>1</mn>           </mrow>           <mn>2</mn>          </msubsup>          <msup>           <mi>e</mi>           <msub>            <mi>W</mi>            <mrow>             <mi>p</mi>             <mo>,</mo>             <mrow>              <mrow>               <msubsup>                <mi>y</mi>                <mi>j</mi>                <mi>T</mi>               </msubsup>               <mo>&#x2062;</mo>               <msub>                <mrow>                 <mi>z</mi>                 <mtext> </mtext>                </mrow>                <mi>j</mi>               </msub>              </mrow>              <mo>+</mo>              <msub>               <mi>b</mi>               <mi>p</mi>              </msub>             </mrow>            </mrow>           </msub>          </msup>         </mrow>        </mfrac>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0102" num="0094">where, y<sub>i </sub>represents a two-dimensional vector indicating whether an input pair of sample image and sample text indicates the serial number of a given target object, z<sub>i </sub>represents fusion features of the text features and the image features, W<sub>p,j </sub>represents the j<sup>th </sup>column of the weight matrix W<sub>p</sub>, b<sub>p </sub>represents a bias term, and M represents the number of input pairs of sample image and sample text.</p><p id="p-0103" num="0095">Step <b>3035</b> includes obtaining a preset feature loss value based on the value of the identification loss function and the value of the pairwise loss function.</p><p id="p-0104" num="0096">In the present embodiment, the executing body may add the identification loss value and the pairwise loss value obtained in step <b>3034</b> to obtain the feature loss value of the feature transformation network.</p><p id="p-0105" num="0097">Step <b>3036</b> includes using the to-be-trained cross-media feature extraction network and the feature transformation network as a generative network based on the discriminative loss value and the feature loss value, and performing adversarial training on the generative network and the discriminative network to obtain the trained cross-media feature extraction network, the trained discriminative network, and the trained feature transformation network.</p><p id="p-0106" num="0098">In the present embodiment, the executing body may use the to-be-trained cross-media feature extraction network and the feature transformation network as the generative network based on the discriminative loss value obtained in step <b>3033</b> and the feature loss value obtained in step <b>3035</b>, and perform adversarial training on the generative network and the discriminative network. Specifically, the network parameter &#x3b8;<sub>V </sub>of the image graph attention convolutional network, the network parameter &#x3b8;<sub>T </sub>of the text graph attention convolutional network, and the network parameter &#x3b8;<sub>D </sub>of the discriminative network may be guided for training optimization by setting the following loss function:</p><p id="p-0107" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mover accent="true">       <mi>&#x3b8;</mi>       <mi>&#x2c6;</mi>      </mover>      <mi>V</mi>     </msub>     <mo>,</mo>     <mrow>      <msub>       <mover accent="true">        <mi>&#x3b8;</mi>        <mi>&#x2c6;</mi>       </mover>       <mi>T</mi>      </msub>      <mo>=</mo>      <mrow>       <mrow>        <mtable>         <mtr>          <mtd>           <mrow>            <mi>arg</mi>            <mo>&#x2062;</mo>            <mtext>   </mtext>            <mi>min</mi>           </mrow>          </mtd>         </mtr>         <mtr>          <mtd>           <mrow>            <msub>             <mi>&#x3b8;</mi>             <mi>V</mi>            </msub>            <mo>,</mo>            <msub>             <mi>&#x3b8;</mi>             <mi>T</mi>            </msub>           </mrow>          </mtd>         </mtr>        </mtable>        <mo>&#x2062;</mo>        <mrow>         <mo>(</mo>         <mrow>          <msub>           <mrow>            <msub>             <mi>L</mi>             <mrow>              <mi>f</mi>              <mo>&#x2062;</mo>              <mi>e</mi>              <mo>&#x2062;</mo>              <mi>a</mi>             </mrow>            </msub>            <mo>(</mo>            <mrow>             <msub>              <mi>&#x3b8;</mi>              <mi>V</mi>             </msub>             <mo>,</mo>             <msub>              <mi>&#x3b8;</mi>              <mi>T</mi>             </msub>            </mrow>            <mo>)</mo>           </mrow>           <mo>,</mo>          </msub>          <mo>&#x2062;</mo>          <msub>           <mi>&#x3b8;</mi>           <mi>T</mi>          </msub>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>L</mi>         <mi>adv</mi>        </msub>        <mo>(</mo>        <msub>         <mi>&#x3b8;</mi>         <mi>D</mi>        </msub>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00008-2" num="00008.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mover accent="true">       <mi>&#x3b8;</mi>       <mi>&#x2c6;</mi>      </mover>      <mi>D</mi>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mtable>        <mtr>         <mtd>          <mrow>           <mi>arg</mi>           <mo>&#x2062;</mo>           <mtext>  </mtext>           <mi>max</mi>          </mrow>         </mtd>        </mtr>        <mtr>         <mtd>          <msub>           <mi>&#x3b8;</mi>           <mi>D</mi>          </msub>         </mtd>        </mtr>       </mtable>       <mo>&#x2062;</mo>       <mrow>        <mo>(</mo>        <mrow>         <mrow>          <msub>           <mi>L</mi>           <mrow>            <mi>f</mi>            <mo>&#x2062;</mo>            <mi>e</mi>            <mo>&#x2062;</mo>            <mi>a</mi>           </mrow>          </msub>          <mo>(</mo>          <mrow>           <msub>            <mi>&#x3b8;</mi>            <mi>V</mi>           </msub>           <mo>,</mo>           <msub>            <mi>&#x3b8;</mi>            <mi>T</mi>           </msub>          </mrow>          <mo>)</mo>         </mrow>         <mo>,</mo>         <msub>          <mi>&#x3b8;</mi>          <mi>T</mi>         </msub>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>-</mo>      <mrow>       <msub>        <mi>L</mi>        <mi>adv</mi>       </msub>       <mo>(</mo>       <msub>        <mi>&#x3b8;</mi>        <mi>D</mi>       </msub>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>9</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0108" num="0099">where L<sub>fea </sub>(&#x3b8;<sub>V</sub>, &#x3b8;<sub>T</sub>) represents the feature loss value, and L<sub>adv </sub>(&#x3b8;<sub>D</sub>) represents the discriminative loss value.</p><p id="p-0109" num="0100">When a difference between the feature loss value and the discriminative loss value reaches the maximum, the optimized network parameter &#x3b8;<sub>V </sub>of the image graph attention convolutional network and the network parameter &#x3b8;<sub>T </sub>of the text graph attention convolutional network are used as network parameters of the trained cross-media feature extraction network.</p><p id="p-0110" num="0101">Through the above training steps, the cross-media feature extraction network can extract the text features and the image features having structured semantics, so that the cross-media feature extraction network has modality invariance, semantic discrimination, and cross-modal semantic similarity.</p><p id="p-0111" num="0102">With further reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, <figref idref="DRAWINGS">FIG. <b>4</b></figref> is an architecture schematic diagram of an implementation process of the method for retrieving a target of the present disclosure.</p><p id="p-0112" num="0103">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the system architecture may include an image graph attention network, a text graph attention network, and an adversarial learning module.</p><p id="p-0113" num="0104">The image graph attention network is used to extract image features of an image. The image graph attention network may be composed of five residual network modules, a visual scene graph module and a joint embedding layer. The visual scene graph module may be composed of an image feature directed graph and a graph attention convolutional layer. The graph attention convolutional layer is used to update the image feature directed graph, and the joint embedding layer may be composed of three fully connected layers. Specifically, the executing body may first extract initial image features of the image using the five residual network modules, then input the initial image features into the visual scene graph module, extract structured image features of the image, and finally use the joint embedding layer to project the structured image features into a common feature space of images and texts.</p><p id="p-0114" num="0105">The text graph attention network is used to extract text features of a text. The text graph attention network may be composed of a bidirectional LSTM (Long Short-Term Memory), a text scene graph module and a joint embedding layer. The text scene graph module may be composed of a text feature directed graph and a graph attention convolutional layer. The graph attention convolutional layer is used to update the text feature directed graph, and the joint embedding layer may be composed of three fully connected layers. Specifically, the executing body may first extract initial text features of the text using the bidirectional LSTM, then input the initial text features into the text scene graph module, extract structured text features of the text, and finally use the joint embedding layer to project the structured text features into the common feature space of images and texts.</p><p id="p-0115" num="0106">The adversarial learning module is used to determine the common feature space of images and texts for the image features and the text features, and the adversarial learning module may be composed of a feature converter and a modal discriminator. Specifically, the executing body may first input the image features extracted by the image graph attention network and the text features extracted by the text graph attention network into the adversarial learning module. The feature converter is used to project the features of different modal types (text features or image features) into the common feature space of images and texts to generate converted features. The modal discriminator is used to distinguish the modal types (text type or image type) of the converted features generated by the feature converter, then use the image graph attention network, the text graph attention network and the feature converter as a generative network, use the modal discriminator as the discriminative network, to perform joint adversarial learning, and finally use the trained image graph attention network and the trained text graph attention network as the cross-media feature extraction network.</p><p id="p-0116" num="0107">With further reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, as an implementation of the method shown in the above figures, the present disclosure provides an embodiment of an apparatus for retrieving a target. The apparatus embodiment corresponds to the method embodiment shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The apparatus may be applied to various electronic devices.</p><p id="p-0117" num="0108">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an apparatus <b>500</b> for retrieving a target of the present embodiment includes: an obtaining unit <b>501</b>, an extraction unit <b>502</b> and a matching unit <b>503</b>. The obtaining unit <b>501</b> is configured to obtain at least one image and a description text of a designated object. The extraction unit <b>502</b> is configured to extract image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network. The matching unit <b>503</b> is configured to match the image features with the text features to determine an image containing the designated object.</p><p id="p-0118" num="0109">In the present embodiment, for the specific processing and the technical effects of the obtaining unit <b>501</b>, the extraction unit <b>502</b> and the matching unit <b>503</b> of the apparatus <b>500</b> for retrieving a target, reference may be made to the relevant description of step <b>201</b>, step <b>202</b>, and step <b>203</b> in the corresponding embodiment in <figref idref="DRAWINGS">FIG. <b>2</b></figref> respectively, and detailed description thereof will be omitted.</p><p id="p-0119" num="0110">In some alternative implementations of the present embodiment, the cross-media feature extraction network is generated as follows: obtaining a training sample set, where the training sample set includes a sample image-text pair, and the sample image-text pair includes: a sample image and a sample text describing an object contained in the sample image; obtaining an initial network, where the initial network includes a to-be-trained cross-media feature extraction network, a discriminative network for discriminating a data type of a feature source, and a feature transformation network, and the to-be-trained cross-media feature extraction network includes an image graph attention network, and a text graph attention network; inputting the sample image in the sample image-text pair into the image graph attention network to obtain a first feature; inputting the sample text in the sample image-text pair into the text graph attention network to obtain a second feature; inputting the first feature and the second feature into the discriminative network to obtain a type discrimination result, and calculating a discriminative loss value based on the type discrimination result, where the data type of the feature source includes a text type and an image type, and the discriminative loss value represents an error of determining types of the first feature and the second feature; inputting the first feature and the second feature into the feature transformation network to obtain a feature transformation result, and calculating a value of an identification loss function and a value of a pairwise loss function based on the feature transformation result, where the identification loss function represents an ability of distinguishing different objects contained the first feature and the second feature in the common feature space of images and texts, and the pairwise loss function represents a semantic difference between the first feature and the second feature of a given object; obtaining a preset feature loss value based on the value of the identification loss function and the value of the pairwise loss function; and using the to-be-trained cross-media feature extraction network and the feature transformation network as a generative network based on the discriminative loss value and the feature loss value, and performing adversarial training on the generative network and the discriminative network to obtain the trained cross-media feature extraction network, the trained discriminative network, and the feature transformation network.</p><p id="p-0120" num="0111">In some alternative implementations of the present embodiment, the image graph attention network includes a residual network, an image graph attention convolutional network and a joint embedding layer; and the first feature is obtained as follows: extracting initial image features of the sample image using the residual network, inputting the initial image features into the image graph attention convolutional network, outputting structured image features of the sample image, fusing the initial image features with the structured image features to generate image features, and inputting the image features into the joint embedding layer to obtain the first feature.</p><p id="p-0121" num="0112">In some alternative implementations of the present embodiment, the structured image features of the sample image are obtained as follows: performing a target object detection on the sample image, determining a target object in the sample image and a position of a rectangular bounding box of the target object, and extracting a relevant feature of the target object based on the position of the rectangular bounding box of the target object, where the relevant feature of the target object includes at least one of: an appearance feature of the target object, a position feature of the target object, an attribute feature of the target object and a type feature of the target object; constructing an image feature directed graph, where vertices of the image feature directed graph represent target objects, and directed edges of the image feature directed graph represent an association relationship between the target objects; and generating the structured image features of the sample image based on the image feature directed graph.</p><p id="p-0122" num="0113">In some alternative implementations of the present embodiment, the text graph attention network includes a bidirectional long short-term memory network, a text graph attention convolutional network and a joint embedding layer; and the second feature is obtained as follows: performing a word segmentation on the sample text to determine a word vector of the sample text; and extracting initial text features of the word vector of the sample text using the bidirectional long short-term memory network, inputting the initial text features into the text graph attention convolutional network to output structured text features of the sample text, fusing the initial text features with the structured text features to generate text features, and inputting the text features into the joint embedding layer to obtain the second feature.</p><p id="p-0123" num="0114">In some alternative implementations of the present embodiment, the structured text features of the sample text are obtained as follows: constructing a text feature directed graph, where vertices of the text feature directed graph represent target objects indicated by word vectors, and directed edges of the text feature directed graph represent an association relationship between the target objects indicated by the word vectors, where a relevant feature of the target object indicated by the word vector includes at least one of: an attribute feature of the target object and a type feature of the target object; and generating the structured text features of the sample text based on the text feature directed graph.</p><p id="p-0124" num="0115">For the apparatus for retrieving a target provided by the above embodiment of the present disclosure, the obtaining unit <b>501</b> obtains at least one image and a description text of a designated object, the extraction unit <b>502</b> extracts image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network, and the matching unit <b>503</b> matches the image features and the text features to determine an image containing the designated object. Thus, features are extracted by using cross-media features, and the image features and the text features are projected to the common feature space of images and texts for feature matching, thereby achieving cross-media target retrieval.</p><p id="p-0125" num="0116">With further reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a schematic structural diagram of an electronic device (such as the server shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) <b>600</b> adapted for implementing the embodiments of the present disclosure. The server shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> is only an example, and should not bring any limitation to the functions and scope of use of the embodiments of the present disclosure.</p><p id="p-0126" num="0117">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the electronic device <b>600</b> may include a processing apparatus (for example, a central processing unit, a graphics processor, etc.) <b>601</b>, which may execute various appropriate actions and processes in accordance with a program stored in a read-only memory (ROM) <b>602</b> or a program loaded into a random access memory (RAM) <b>603</b> from a storage apparatus <b>608</b>. In the RAM <b>603</b>, various programs and data required for the operation of the electronic device <b>600</b> are also stored. The processing apparatus <b>601</b>, the ROM <b>602</b>, and the RAM <b>603</b> are connected to each other through a bus <b>604</b>. An input/output (I/O) interface <b>605</b> is also connected to the bus <b>604</b>.</p><p id="p-0127" num="0118">Typically, the following apparatuses may be connected to the I/O interface <b>605</b>: an input apparatus <b>605</b> including a touch screen, a touch pad, a keyboard, a mouse, a camera, a microphone, an accelerometer, or a gyroscope; an output apparatus <b>607</b> including a liquid crystal display (LCD), a speaker, a vibrator; the storage apparatus <b>608</b> including a magnetic tape, or a hard disk; and a communication apparatus <b>609</b>. The communication apparatus <b>609</b> may allow the electronic device <b>600</b> to perform wireless or wired communication with other devices to exchange data. Although <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows the electronic device <b>600</b> having various apparatuses, it should be understood that it is not required to implement or have all the illustrated apparatuses. It may alternatively be implemented or provided with more or fewer apparatuses. Each block shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> may represent one apparatus, or may represent a plurality of apparatuses as needed.</p><p id="p-0128" num="0119">In particular, according to embodiments of the present disclosure, the process described above with reference to the flow chart may be implemented in a computer software program. For example, an embodiment of the present disclosure includes a computer program product, which includes a computer program that is tangibly embedded in a machine-readable medium. The computer program includes program codes for executing the method as illustrated in the flow chart. In such an embodiment, the computer program may be downloaded and installed from a network via the communication apparatus <b>609</b>, and/or may be installed from the storage apparatus <b>608</b>. The computer program, when executed by the processing apparatus <b>601</b>, implements the above mentioned functionalities as defined by the methods of the present disclosure.</p><p id="p-0129" num="0120">It should be noted that the computer readable medium in the present disclosure may be computer readable signal medium or computer readable storage medium or any combination of the above two. An example of the computer readable storage medium may include, but not limited to: electric, magnetic, optical, electromagnetic, infrared, or semiconductor systems, apparatus, elements, or a combination any of the above. A more specific example of the computer readable storage medium may include but is not limited to: electrical connection with one or more wire, a portable computer disk, a hard disk, a random access memory (RAM), a read only memory (ROM), an erasable programmable read only memory (EPROM or flash memory), a fibre, a portable compact disk read only memory (CD-ROM), an optical memory, a magnet memory or any suitable combination of the above. In the present disclosure, the computer readable storage medium may be any physical medium containing or storing programs which can be used by a command execution system, apparatus or element or incorporated thereto. In the present disclosure, the computer readable signal medium may include data signal in the base band or propagating as parts of a carrier, in which computer readable program codes are carried. The propagating signal may take various forms, including but not limited to: an electromagnetic signal, an optical signal or any suitable combination of the above. The signal medium that can be read by computer may be any computer readable medium except for the computer readable storage medium. The computer readable medium is capable of transmitting, propagating or transferring programs for use by, or used in combination with, a command execution system, apparatus or element. The program codes contained on the computer readable medium may be transmitted with any suitable medium including but not limited to: wireless, wired, optical cable, RF medium etc., or any suitable combination of the above.</p><p id="p-0130" num="0121">The computer readable medium may be included in the electronic device, or a stand-alone computer readable medium not assembled into the electronic device. The computer readable medium carries one or more programs. The one or more programs, when executed by the electronic device, cause the electronic device to: obtain at least one image and a description text of a designated object; extract image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network; and match the image features and the text features to determine an image that contains the designated object.</p><p id="p-0131" num="0122">A computer program code for executing operations in the disclosure may be compiled using one or more programming languages or combinations thereof. The programming languages include object-oriented programming languages, such as Java, Smalltalk or C++, and also include conventional procedural programming languages, such as &#x201c;C&#x201d; language or similar programming languages. The program code may be completely executed on a user's computer, partially executed on a user's computer, executed as a separate software package, partially executed on a user's computer and partially executed on a remote computer, or completely executed on a remote computer or server. In the circumstance involving a remote computer, the remote computer may be connected to a user's computer through any network, including local area network (LAN) or wide area network (WAN), or may be connected to an external computer (for example, connected through Internet using an Internet service provider).</p><p id="p-0132" num="0123">The flow charts and block diagrams in the accompanying drawings illustrate architectures, functions and operations that may be implemented according to the systems, methods and computer program products of the various embodiments of the present disclosure. In this regard, each of the blocks in the flow charts or block diagrams may represent a module, a program segment, or a code portion, said module, program segment, or code portion including one or more executable instructions for implementing specified logic functions. It should also be noted that, in some alternative implementations, the functions denoted by the blocks may occur in a sequence different from the sequences shown in the figures. For example, any two blocks presented in succession may be executed, substantially in parallel, or they may sometimes be in a reverse sequence, depending on the function involved. It should also be noted that each block in the block diagrams and/or flow charts as well as a combination of blocks may be implemented using a dedicated hardware-based system executing specified functions or operations, or by a combination of a dedicated hardware and computer instructions.</p><p id="p-0133" num="0124">The units involved in the embodiments of the present disclosure may be implemented by means of software or hardware. The described units may also be provided in a processor, for example, may be described as: a processor including an obtaining unit, an extraction unit and a matching unit. Here, the names of these units do not in some cases constitute limitations to such units themselves. For example, the obtaining unit may also be described as &#x201c;a unit configured to obtain at least one image and a description text of a designated object&#x201d;.</p><p id="p-0134" num="0125">The above description only provides an explanation of the preferred embodiments of the present disclosure and the technical principles used. It should be appreciated by those skilled in the art that the inventive scope of the present disclosure is not limited to the technical solutions formed by the particular combinations of the above-described technical features. The inventive scope should also cover other technical solutions formed by any combinations of the above-described technical features or equivalent features thereof without departing from the concept of the disclosure. Technical schemes formed by the above-described features being interchanged with, but not limited to, technical features with similar functions disclosed in the present disclosure are examples.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005178A1-20230105-M00001.NB"><img id="EMI-M00001" he="7.03mm" wi="76.20mm" file="US20230005178A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005178A1-20230105-M00002.NB"><img id="EMI-M00002" he="8.81mm" wi="76.20mm" file="US20230005178A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005178A1-20230105-M00003.NB"><img id="EMI-M00003" he="6.35mm" wi="76.20mm" file="US20230005178A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005178A1-20230105-M00004.NB"><img id="EMI-M00004" he="8.13mm" wi="76.20mm" file="US20230005178A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005178A1-20230105-M00005.NB"><img id="EMI-M00005" he="8.13mm" wi="76.20mm" file="US20230005178A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005178A1-20230105-M00006.NB"><img id="EMI-M00006" he="10.24mm" wi="76.20mm" file="US20230005178A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005178A1-20230105-M00007.NB"><img id="EMI-M00007" he="10.92mm" wi="76.20mm" file="US20230005178A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008 MATH-US-00008-2" nb-file="US20230005178A1-20230105-M00008.NB"><img id="EMI-M00008" he="12.70mm" wi="76.20mm" file="US20230005178A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for retrieving a target, the method comprising:<claim-text>obtaining at least one image and a description text of a designated object;</claim-text><claim-text>extracting image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network, wherein the cross-media feature extraction network projects the text features and the image features to a common feature space of images and texts; and</claim-text><claim-text>matching the image features with the text features to determine an image containing the designated object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the cross-media feature extraction network is generated as follows:<claim-text>obtaining a training sample set, wherein the training sample set comprises a sample image-text pair, and the sample image-text pair comprises: a sample image and a sample text describing an object contained in the sample image;</claim-text><claim-text>obtaining an initial network, wherein the initial network comprises a to-be-trained cross-media feature extraction network, a discriminative network for discriminating a data type of a feature source, and a feature transformation network, and the to-be-trained cross-media feature extraction network comprises an image graph attention network, and a text graph attention network;</claim-text><claim-text>inputting the sample image in the sample image-text pair into the image graph attention network to obtain a first feature;</claim-text><claim-text>inputting the sample text in the sample image-text pair into the text graph attention network to obtain a second feature;</claim-text><claim-text>inputting the first feature and the second feature into the discriminative network to obtain a type discrimination result, and calculating a discriminative loss value based on the type discrimination result, wherein the data type of the feature source comprises a text type and an image type, and the discriminative loss value represents an error of determining types of the first feature and the second feature;</claim-text><claim-text>inputting the first feature and the second feature into the feature transformation network to obtain a feature transformation result, and calculating a value of an identification loss function and a value of a pairwise loss function based on the feature transformation result, wherein the identification loss function represents an ability of distinguishing different objects contained in the first feature and the second features in a common feature space of images and texts, and the pairwise loss function represents a semantic difference between the first feature and the second feature of a given object;</claim-text><claim-text>obtaining a preset feature loss value based on the value of the identification loss function and the value of the pairwise loss function; and</claim-text><claim-text>using the to-be-trained cross-media feature extraction network and the feature transformation network as a generative network based on the discriminative loss value and the feature loss value, and performing adversarial training on the generative network and the discriminative network to obtain a trained cross-media feature extraction network, a trained discriminative network, and a trained feature transformation network.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the image graph attention network comprises a residual network, an image graph attention convolutional network and a joint embedding layer; and the inputting the sample image into the image graph attention network to obtain the first feature comprises:<claim-text>extracting initial image features of the sample image using the residual network, inputting the initial image features into the image graph attention convolutional network to output structured image features of the sample image, fusing the initial image features with the structured image features to generate image features, and inputting the image features into the joint embedding layer to obtain the first feature.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the inputting the initial image features into the image graph attention convolutional network to output structured image features of the sample image comprises:<claim-text>performing a target object detection on the sample image, determining a target object in the sample image and a position of a rectangular bounding box of the target object, and extracting a relevant feature of the target object based on the position of the rectangular bounding box of the target object, wherein the relevant feature of the target object comprises at least one of: an appearance feature of the target object, a position feature of the target object, and a type feature of the target object;</claim-text><claim-text>constructing an image feature directed graph, wherein vertices of the image feature directed graph represent target objects, and directed edges of the image feature directed graph represent an association relationship between the target objects; and</claim-text><claim-text>generating the structured image features of the sample image based on the image feature directed graph.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the text graph attention network comprises a bidirectional long short-term memory network, a text graph attention convolutional network and a joint embedding layer; and the inputting the sample text into the text graph attention network to obtain the second feature comprises:<claim-text>performing a word segmentation on the sample text to determine a word vector of the sample text; and</claim-text><claim-text>extracting initial text features of the word vector of the sample text using the bidirectional long short-term memory network, inputting the initial text features into the text graph attention convolutional network to output structured text features of the sample text, fusing the initial text features with the structured text features to generate text features, and inputting the text features into the joint embedding layer to obtain the second feature.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the inputting the initial text features into the text graph attention convolutional network to output structured text features of the sample text comprises:<claim-text>constructing a text feature directed graph, wherein vertices of the text feature directed graph represent target objects indicated by word vectors, and directed edges of the text feature directed graph represent an association relationship between the target objects indicated by the word vectors, wherein a relevant feature of the target object indicated by the word vector comprises at least one of: an attribute feature of the target object and a type feature of the target object; and</claim-text><claim-text>generating the structured text features of the sample text based on the text feature directed graph.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An apparatus for retrieving a target, the apparatus comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory storing instructions, wherein the instructions when executed by the at least one processor, cause the at least one processor to perform operations, the operations comprising:</claim-text><claim-text>obtaining at least one image and a description text of a designated object;</claim-text><claim-text>a extracting image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network, wherein the cross-media feature extraction network projects the text features and the image features to a common feature space of images and texts; and</claim-text><claim-text>matching the image features with the text features to determine an image containing the designated object.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the cross-media feature extraction network is generated as follows:<claim-text>obtaining a training sample set, wherein the training sample set comprises a sample image-text pair, and the sample image-text pair comprises: a sample image and a sample text describing an object contained in the sample image;</claim-text><claim-text>obtaining an initial network, wherein the initial network comprises a to-be-trained cross-media feature extraction network, a discriminative network for discriminating a data type of a feature source, and a feature transformation network, and the to-be-trained cross-media feature extraction network comprises an image graph attention network, and a text graph attention network;</claim-text><claim-text>inputting the sample image in the sample image-text pair into the image graph attention network to obtain a first feature;</claim-text><claim-text>inputting the sample text in the sample image-text pair into the text graph attention network to obtain a second feature;</claim-text><claim-text>inputting the first feature and the second feature into the discriminative network to obtain a type discrimination result, and calculating a discriminative loss value based on the type discrimination result, wherein the data type of the feature source comprises a text type and an image type, and the discriminative loss value represents an error of determining types of the first feature and the second feature;</claim-text><claim-text>inputting the first feature and the second feature into the feature transformation network to obtain a feature transformation result, and calculating a value of an identification loss function and a value of a pairwise loss function based on the feature transformation result, wherein the identification loss function represents an ability of distinguishing different objects contained the first feature and the second feature in the common feature space of images and texts, and the pairwise loss function represents a semantic difference between the first feature and the second feature of a given object;</claim-text><claim-text>obtaining a preset feature loss value based on the value of the identification loss function and the value of the pairwise loss function; and</claim-text><claim-text>using the to-be-trained cross-media feature extraction network and the feature transformation network as a generative network based on the discriminative loss value and the feature loss value, and performing adversarial training on the generative network and the discriminative network to obtain a trained cross-media feature extraction network, a trained discriminative network, and a trained feature transformation network.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the image graph attention network comprises a residual network, an image graph attention convolutional network and a joint embedding layer; and the first feature is obtained as follows:<claim-text>extracting initial image features of the sample image using the residual network, inputting the initial image features into the image graph attention convolutional network to output structured image features of the sample image, fusing the initial image features with the structured image features to generate image features, and inputting the image features into the joint embedding layer to obtain the first feature.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the structured image features of the sample image are obtained as follows:<claim-text>performing a target object detection on the sample image, determining a target object in the sample image and a position of a rectangular bounding box of the target object, and extracting a relevant feature of the target object based on the position of the rectangular bounding box of the target object, wherein the relevant feature of the target object comprises at least one of: an appearance feature of the target object, a position feature of the target object, and a type feature of the target object;</claim-text><claim-text>constructing an image feature directed graph, wherein vertices of the image feature directed graph represent target objects, and directed edges of the image feature directed graph represent an association relationship between the target objects; and</claim-text><claim-text>generating the structured image features of the sample image based on the image feature directed graph.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the text graph attention network comprises a bidirectional long short-term memory network, a text graph attention convolutional network and a joint embedding layer; and the second feature is obtained as follows:<claim-text>performing a word segmentation on the sample text to determine a word vector of the sample text; and</claim-text><claim-text>extracting initial text features of the word vector of the sample text using the bidirectional long short-term memory network, inputting the initial text features into the text graph attention convolutional network to output structured text features of the sample text, fusing the initial text features with the structured text features to generate text features, and inputting the text features into the joint embedding layer to obtain the second feature.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the structured text features of the sample text are obtained as follows:<claim-text>constructing a text feature directed graph, wherein vertices of the text feature directed graph represent target objects indicated by word vectors, and directed edges of the text feature directed graph represent an association relationship between the target objects indicated by the word vectors, wherein a relevant feature of the target object indicated by the word vector comprises at least one of: an attribute feature of the target object and a type feature of the target object; and</claim-text><claim-text>generating the structured text features of the sample text based on the text feature directed graph.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. (canceled)</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A non-transitory computer readable medium, storing a computer program, wherein the program, when executed by a processor, causes the processor to perform operations comprising:<claim-text>obtaining at least one image and a description text of a designated object;</claim-text><claim-text>extracting image features of the image and text features of the description text by using a pre-trained cross-media feature extraction network, wherein the cross-media feature extraction network projects the text features and the image features to a common feature space of images and texts; and</claim-text><claim-text>matching the image features with the text features to determine an image containing the designated object.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer readable medium according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the cross-media feature extraction network is generated as follows:<claim-text>obtaining a training sample set, wherein the training sample set comprises a sample image-text pair, and the sample image-text pair comprises: a sample image and a sample text describing an object contained in the sample image;</claim-text><claim-text>obtaining an initial network, wherein the initial network comprises a to-be-trained cross-media feature extraction network, a discriminative network for discriminating a data type of a feature source, and a feature transformation network, and the to-be-trained cross-media feature extraction network comprises an image graph attention network, and a text graph attention network;</claim-text><claim-text>inputting the sample image in the sample image-text pair into the image graph attention network to obtain a first feature;</claim-text><claim-text>inputting the sample text in the sample image-text pair into the text graph attention network to obtain a second feature;</claim-text><claim-text>inputting the first feature and the second feature into the discriminative network to obtain a type discrimination result, and calculating a discriminative loss value based on the type discrimination result, wherein the data type of the feature source comprises a text type and an image type, and the discriminative loss value represents an error of determining types of the first feature and the second feature;</claim-text><claim-text>inputting the first feature and the second feature into the feature transformation network to obtain a feature transformation result, and calculating a value of an identification loss function and a value of a pairwise loss function based on the feature transformation result, wherein the identification loss function represents an ability of distinguishing different objects contained in the first feature and the second features in a common feature space of images and texts, and the pairwise loss function represents a semantic difference between the first feature and the second feature of a given object;</claim-text><claim-text>obtaining a preset feature loss value based on the value of the identification loss function and the value of the pairwise loss function; and</claim-text><claim-text>using the to-be-trained cross-media feature extraction network and the feature transformation network as a generative network based on the discriminative loss value and the feature loss value, and performing adversarial training on the generative network and the discriminative network to obtain a trained cross-media feature extraction network, a trained discriminative network, and a trained feature transformation network.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer readable medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the image graph attention network comprises a residual network, an image graph attention convolutional network and a joint embedding layer; and the inputting the sample image into the image graph attention network to obtain the first feature comprises:<claim-text>extracting initial image features of the sample image using the residual network, inputting the initial image features into the image graph attention convolutional network to output structured image features of the sample image, fusing the initial image features with the structured image features to generate image features, and inputting the image features into the joint embedding layer to obtain the first feature.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer readable medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the inputting the initial image features into the image graph attention convolutional network to output structured image features of the sample image comprises:<claim-text>performing a target object detection on the sample image, determining a target object in the sample image and a position of a rectangular bounding box of the target object, and extracting a relevant feature of the target object based on the position of the rectangular bounding box of the target object, wherein the relevant feature of the target object comprises at least one of: an appearance feature of the target object, a position feature of the target object, and a type feature of the target object;</claim-text><claim-text>constructing an image feature directed graph, wherein vertices of the image feature directed graph represent target objects, and directed edges of the image feature directed graph represent an association relationship between the target objects; and</claim-text><claim-text>generating the structured image features of the sample image based on the image feature directed graph.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer readable medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the text graph attention network comprises a bidirectional long short-term memory network, a text graph attention convolutional network and a joint embedding layer; and the inputting the sample text into the text graph attention network to obtain the second feature comprises:<claim-text>performing a word segmentation on the sample text to determine a word vector of the sample text; and</claim-text><claim-text>extracting initial text features of the word vector of the sample text using the bidirectional long short-term memory network, inputting the initial text features into the text graph attention convolutional network to output structured text features of the sample text, fusing the initial text features with the structured text features to generate text features, and inputting the text features into the joint embedding layer to obtain the second feature.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer readable medium according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the inputting the initial text features into the text graph attention convolutional network to output structured text features of the sample text comprises:<claim-text>constructing a text feature directed graph, wherein vertices of the text feature directed graph represent target objects indicated by word vectors, and directed edges of the text feature directed graph represent an association relationship between the target objects indicated by the word vectors, wherein a relevant feature of the target object indicated by the word vector comprises at least one of: an attribute feature of the target object and a type feature of the target object; and</claim-text><claim-text>generating the structured text features of the sample text based on the text feature directed graph.</claim-text></claim-text></claim></claims></us-patent-application>