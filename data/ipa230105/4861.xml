<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004862A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004862</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17537575</doc-number><date>20211130</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110739297.3</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc></classifications-cpc><invention-title id="d2e61">METHOD FOR TRAINING RANKING LEARNING MODEL, RANKING METHOD, DEVICE AND MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>XIANG</last-name><first-name>Yingfei</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>LUO</last-name><first-name>Hongyu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>FANG</last-name><first-name>Xiaomin</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Fan</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The technical solution relates to the field of artificial intelligence technologies, such as machine learning technologies, natural language processing technologies, or the like. A plurality of training samples are collected, each of the plurality of training samples includes information of a known training target protein, information of two training drugs, and a real difference between affinities of the two training drugs for the known training target. The ranking learning model is trained with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="73.49mm" wi="99.31mm" file="US20230004862A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="85.09mm" wi="101.35mm" file="US20230004862A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="163.15mm" wi="102.79mm" file="US20230004862A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="170.69mm" wi="124.54mm" file="US20230004862A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="199.22mm" wi="69.60mm" file="US20230004862A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="173.99mm" wi="113.45mm" file="US20230004862A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present disclosure claims priority and benefit of Chinese Patent Application No. 202110739297.3, filed on Jun. 30, 2021, entitled &#x201c;METHOD AND APPARATUS FOR RANKING LEARNING MODEL, RANKING METHOD AND APPARATUS, DEVICE AND MEDIUM&#x201d;. The disclosure of the above application is incorporated herein by reference in its entirety.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of computer technologies, and particularly to the field of artificial intelligence technologies, such as machine learning technologies, natural language processing technologies, or the like, and more particularly to a method for training a ranking learning model, a ranking method, a device and a medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">A drug target interaction (DTI), which represents an affinity between a target protein and a drug compound, is a quite important part in the field of drug development. The DTI may assist a drug developer in understanding a mechanism of a disease to speed up a drug design process.</p><p id="p-0005" num="0004">In the traditional biological field, a method for detecting the DTI in a laboratory using a wet experiment is quite expensive and time-consuming. Nowadays, with maturity of a deep learning algorithm based on artificial intelligence (AI), more and more DTI tasks are implemented using network models, such as a graph neural network (GNN), a convolutional neural network (CNN), or the like.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0006" num="0005">The present disclosure provides a method for training a ranking learning model, a ranking method, a device and a medium.</p><p id="p-0007" num="0006">According to an aspect of the present disclosure, there is provided a method for training a ranking learning model, including collecting a plurality of training samples, each of the plurality of training samples including information of a known training target protein, information of two training drugs, and a real difference between affinities of the two training drugs for the known training target protein; and training the ranking learning model with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</p><p id="p-0008" num="0007">According to another aspect of the present disclosure, there is provided a drug ranking method, including acquiring information of an objective target and information of a plurality of candidate drugs; and ranking the plurality of candidate drugs according to magnitude of their respective affinities for the objective target by a ranking model based on the information of the objective target and the information of the plurality of candidate drugs, the ranking model sharing parameters of a pre-trained ranking learning model, and the ranking learning model being configured to learn a magnitude relationship between affinities of any two drugs for a same target protein.</p><p id="p-0009" num="0008">According to another aspect of the present disclosure, there is provided an electronic device, including at least one processor; and a memory connected with the at least one processor communicatively; the memory stores instructions executable by the at least one processor to enable the at least one processor to carry out the method of the aspect as described above and any possible implementation.</p><p id="p-0010" num="0009">According to still another aspect of the present disclosure, there is provided a non-transitory computer readable storage medium including computer instructions, which, when executed by a computer, cause the computer to carry out the method of the aspect as described above and any possible implementation.</p><p id="p-0011" num="0010">It should be understood that the statements in this section are not intended to identify key or critical features of the embodiments of the present disclosure, nor limit the scope of the present disclosure. Other features of the present disclosure will become apparent from the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">The drawings are used for better understanding the present solution and do not constitute a limitation of the present disclosure. In the drawings,</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram according to a first embodiment of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram according to a second embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram according to a third embodiment of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram according to a fourth embodiment of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram according to a fifth embodiment of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram according to a sixth embodiment of the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram according to a seventh embodiment of the present disclosure; and</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a schematic block diagram of an exemplary electronic device <b>800</b> which may be configured to implement the embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020">The following part will illustrate exemplary embodiments of the present disclosure with reference to the drawings, including various details of the embodiments of the present disclosure for a better understanding. The embodiments should be regarded only as exemplary ones. Therefore, those skilled in the art should appreciate that various changes or modifications can be made with respect to the embodiments described herein without departing from the scope and spirit of the present disclosure. Similarly, for clarity and conciseness, the descriptions of the known functions and structures are omitted in the descriptions below.</p><p id="p-0022" num="0021">Apparently, the embodiments to be described are merely some rather than all of the embodiments of the present disclosure. All other embodiments obtained by a person of ordinary skill in the art based on the embodiments of the present disclosure without creative efforts shall fall within the protection scope of the present disclosure.</p><p id="p-0023" num="0022">It should be noted that a terminal device in the embodiments of the present disclosure may include, but is not limited to, a mobile phone, a personal digital assistant (PDA), a wireless handheld device, a tablet computer, and other smart devices; a display device may include, but not limited to, a personal computer, a television, and other devices with a display function.</p><p id="p-0024" num="0023">In addition, the term &#x201c;and/or&#x201d; only describes an association relationship between associated objects, and indicates that three relationships may exist. For example, A and/or B may indicate three cases: only A exists; both A and B exist; and only B exists. In addition, in this specification, the symbol &#x201c;/&#x201d; generally indicates that associated objects have a relationship of &#x201c;or&#x201d;.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram according to a first embodiment of the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, this embodiment provides a method for training a ranking learning model. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the method for training a ranking learning model according to this embodiment may include the following steps:</p><p id="p-0026" num="0025">S<b>101</b>: collecting a plurality of training samples, each of the plurality of training samples including information of a known training target protein, information of two corresponding training drugs, and a real difference between affinities of the two corresponding training drugs for the known training target; and</p><p id="p-0027" num="0026">S<b>102</b>: training the ranking learning model with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</p><p id="p-0028" num="0027">An apparatus for training a ranking learning model serves as the subject for executing the method for training a ranking learning model according to this embodiment, and an electronic entity or a software-integrated application may serve as the executing subject of the apparatus for training a ranking learning model. The apparatus for training a ranking learning model according to this embodiment is configured to train a ranking learning model.</p><p id="p-0029" num="0028">The ranking learning model in this embodiment is configured to learn to predict the magnitude relationship between the affinities of the two training drugs for the known training target protein, and then rank a plurality of training drugs according to the magnitude of affinities of the plurality of training drugs for the known training target protein based on the magnitude relationship between the affinities of every two training drugs for the known training target protein.</p><p id="p-0030" num="0029">A plurality of training samples are collected in this embodiment, and each training sample includes the information of the two training drugs; for example, the information of the training drug may be identified using a simplified molecular input line entry specification (SMILES) sequence of the training drug, or may be other unique identification information of the training drug. The information of the known training target protein may be identified using a FASTA sequence of the known training target protein, or may be other unique identifying information of the known training target protein.</p><p id="p-0031" num="0030">It should be noted that since each training sample in this embodiment is used for training the ranking learning model to learn the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each training sample, in a supervised training operation, each training sample in this embodiment further includes the real difference between the affinities of the two training drugs for the known training target protein; that is, the real difference between the affinities may identify the magnitude relationship between the affinities of the two training drugs for the known training target protein. Based on this, optionally, in an actual operation, the real difference between the affinities may be a specific difference value, or may only identify a direction of the real difference between the affinities. For example, for two training drugs A and B, if the affinity a of the training drug A for a known training target protein 1 is greater than the affinity b of the training drug B for the known training target protein 1 (i.e., a&#x2212;b&#x3e;0), the corresponding real difference between the affinities may be identified to be 1, and if the affinity a of the training drug A for the known training target protein 1 is less than the affinity b of the training drug B for the known training target protein 1 (i.e., a&#x2212;b&#x3c;0), the corresponding real difference between the affinities may be identified to be 0.</p><p id="p-0032" num="0031">Then, based on the information of the two training drugs and the real difference between the affinities of the two corresponding training drugs for a known training target in each of the plurality of training samples, the ranking learning model may be subjected to a supervised training operation to learn the real difference between the affinities of the two training drugs for the known training target which are identified in each training sample, and the ranking learning model is continuously trained using the plurality of training samples, so as to learn the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each training sample.</p><p id="p-0033" num="0032">In this embodiment, the number of collected training samples may be huge, for example, hundreds of thousands or even millions. The greater the number of the training samples, the higher the accuracy of the trained ranking learning model.</p><p id="p-0034" num="0033">In the method for training a ranking learning model according to this embodiment, the ranking learning model is trained using each training sample including the information of the known training target protein, the information of the two corresponding training drugs, and the real difference between the affinities of the two corresponding training drugs for the known training target, such that the ranking learning model may learn the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each training sample.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram according to a second embodiment of the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the technical solution of the method for training a ranking learning model according to this embodiment of the present disclosure is further described in more detail based on the technical solution of the above-mentioned embodiment shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the method for training a ranking learning model according to this embodiment may include the following steps:</p><p id="p-0036" num="0035">5201: collecting a plurality of training samples from a plurality of data sets, each training sample including information of a known training target protein, information of two corresponding training drugs, and a real difference between affinities of the two corresponding training drugs for the known training target.</p><p id="p-0037" num="0036">Optionally, in this embodiment, the affinities of the training drugs for the known training target in different data sets may be characterized by different indexes. For example, the indexes of the affinities in some data sets are identified by IC50, the indexes of the affinities in some data sets are identified by Kd, and the indexes of the affinities in some data sets are identified by Ki. No matter which affinity index is adopted in the data set, the training sample in this embodiment is only required to identify the direction of the real difference between the affinities of the two training drugs for the known training target.</p><p id="p-0038" num="0037">For example, in the schematic diagram of training sample construction shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a training set formed by a plurality of collected training samples may include m training target proteins, which may be identified as t<sup>(1)</sup>, . . . , t<sup>(m) </sup>respectively. For each training target protein, n training drugs and the affinity of respective training drugs for the training target protein may be collected first; for example, for the training target protein t<sup>(1)</sup>, the collected training drugs may be denoted as {(d<sub>1</sub><sup>(1)</sup>, S<sub>1</sub><sup>(1)</sup>), (d<sub>2</sub><sup>(1)</sup>, S<sub>2</sub><sup>(1)</sup>), . . . , (d<sub>n</sub><sup>(1)</sup>, S<sub>n</sub><sup>(1)</sup>)}; for the training target protein t<sup>(m)</sup>, the collected training drugs may be denoted as {(d<sub>1</sub><sup>(m)</sup>, S<sub>1</sub><sup>(m)</sup>), (d<sub>2</sub><sup>(m)</sup>, S<sub>2</sub><sup>(m)</sup>), . . . , (d<sub>n</sub><sup>(m)</sup>, S<sub>n</sub><sup>(m)</sup>)}. For a single target protein, all corresponding drugs d may form a pairwise relationship, and for each pair of drugs (d<sub>i</sub><sup>(m)</sup>, d<sub>j</sub><sup>(m)</sup>, the difference between corresponding affinity scores may be written as s(S<sub>i</sub><sup>(m)</sup>, S<sub>j</sub><sup>(m)</sup>). As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, for the training target protein t<sup>(1)</sup>, any training sample may be denoted as f(t<sup>(1)</sup>, d<sub>i</sub><sup>(1)</sup>, d<sub>j</sub><sup>(1)</sup>), s(S<sub>i</sub><sup>(1)</sup>, S<sub>j</sub><sup>(1)</sup>); or the training target protein t<sup>(2)</sup>, any training sample may be denoted as f(t<sup>(2)</sup>, d<sub>i</sub><sup>(2)</sup>, d<sub>j</sub><sup>(2)</sup>), s(S<sub>i</sub><sup>(2)</sup>, S<sub>j</sub><sup>(2)</sup>), and for the training target protein t<sup>(m)</sup>, any training sample may be denoted as f(t<sup>(m)</sup>, d<sub>i</sub><sup>(m)</sup>, d<sub>j</sub><sup>(m)</sup>), s(S<sub>i</sub><sup>(m)</sup>, S<sub>j</sub><sup>(m)</sup>).</p><p id="p-0039" num="0038">The training drugs and the training target proteins may be from a plurality of different data sets, and the affinities of the training drugs corresponding to different training target proteins may be identified using different affinity indexes, only if the difference between the affinities of the two training drugs for the training target protein in any training sample is able to be identified. Similarly, a value of the difference between the affinities may not be identified, but only the direction of the difference is identified; that is, only the magnitude relationship is identified.</p><p id="p-0040" num="0039">The ranking learning model in this embodiment may employ a neural network model, such as a multi-layer perceptron (MLP), a convolutional neural network (CNN), a Transformer, or the like, or may employ any other neural network structure capable of extracting and learning target protein or drug molecule characterization. The ranking learning model in this embodiment has a double-tower structure.</p><p id="p-0041" num="0040"><b>202</b>: inputting the information of the known training target protein and the information of the two corresponding training drugs in each of the plurality of training samples into the ranking learning model.</p><p id="p-0042" num="0041"><b>203</b>: acquiring a predicted difference between the affinities of the two training drugs for the known training target protein output by the ranking learning model.</p><p id="p-0043" num="0042"><b>204</b>: adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the corresponding real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</p><p id="p-0044" num="0043">For example, the step may include the following steps during implementation:</p><p id="p-0045" num="0044">(a) constructing a loss function based on the predicted difference between the affinities and the corresponding real difference between the affinities;</p><p id="p-0046" num="0045">(b) detecting whether the loss function converges; if the loss function converges, executing step (d);</p><p id="p-0047" num="0046">(c) if the loss function does not converge, adjusting the parameters of the ranking learning model, such that the loss function tends to converge; returning to step <b>202</b>, selecting the next training sample, and continuing the training operation; and</p><p id="p-0048" num="0047">(d) detecting whether a training termination condition is met, if yes, stopping the training operation, determining the parameters of the ranking learning model at this point, and ending the step; if no, returning to step <b>202</b>, selecting the next training sample, and continuing the training operation.</p><p id="p-0049" num="0048">Optionally, as the training termination condition in this embodiment, whether the loss function converges all the time in a preset continuous threshold number of training operations is detected, and if yes, the training termination condition is determined to be met. The preset continuous threshold number may be set according to an actual scenario, and may be, for example, 80, 100, 150 or other numbers, which is not limited herein. Alternatively, a maximum training number threshold may be set, and when the number of the training operations reaches the maximum training number threshold, the training operation is finished. The above-mentioned training mode may effectively improve an effect of training the ranking learning model.</p><p id="p-0050" num="0049">The ranking learning model in this embodiment has a double-tower structure and achieves a ranking learning effect. The parameters of the learned ranking learning model may be shared by a ranking model having a single-tower structure, such that the ranking model may rank a plurality of drugs corresponding to a same target protein according to their respective affinities.</p><p id="p-0051" num="0050">In the method for training a ranking learning model according to this embodiment, a full advantage may be taken of different data sets and DTI data of different indexes, and the magnitude relationship between the affinities of different drugs for the same target protein is learned by designing a ranking learning algorithm, thus ranking the plurality of drugs according to the affinities with the same target protein. Since more attention is paid to the difference between the affinities of the two paired drugs for the target protein in the training operation of the ranking learning model in this embodiment, cross-data-set data of plural affinity indexes may be integrated together to train the model, thus effectively overcoming a limitation that a DTI data set is small in the model training operation, and effectively improving the effect of training the ranking learning model.</p><p id="p-0052" num="0051">In the method for training a ranking learning model according to this embodiment, the precedence relationship between the affinities of different drugs for the same target protein may be acquired by designing the pairwise ranking learning algorithm, and accuracy of ranking the affinities of different drugs for the same target protein may be effectively improved compared with other existing methods; for example, a weighted consistency index (WeightedCI) and an average consistency index (Averaged) of a corresponding drug based on a certain target protein may be increased by about 0.03 and 0.05 respectively.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram according to a fourth embodiment of the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, this embodiment provides a drug ranking method, which may include the following steps:</p><p id="p-0054" num="0053">S<b>401</b>: acquiring information of an objective target and information of a plurality of candidate drugs; and</p><p id="p-0055" num="0054">S<b>402</b>: ranking the plurality candidate drugs according to magnitude of their respective affinities for the objective target by a ranking model based on the information of the objective target and the information of the plurality of candidate drugs, herein the ranking model shares parameters of a pre-trained ranking learning model, and the ranking learning model is configured to learn a magnitude relationship between affinities of any two drugs for a same target protein.</p><p id="p-0056" num="0055">A drug ranking apparatus serves as the subject for executing the drug ranking method according to this embodiment, and an electronic entity or a software-integrated application may serve as the executing subject of the drug ranking apparatus. The drug ranking operation in this embodiment is used for ranking the plurality of candidate drugs according to the magnitude of their affinities for the same target protein, and further may realize drug recommendation.</p><p id="p-0057" num="0056">The ranking model in this embodiment has a single-tower structure, and the single-tower structure may be implemented by sharing the parameters of the ranking model trained in the embodiments shown in <figref idref="DRAWINGS">FIG. <b>1</b> or <b>2</b></figref>. The above-mentioned ranking learning model learns the magnitude relationship between the affinities of different drugs for the same target, thus ranking the plurality of drugs according to the magnitude of their affinities for the same target. For example, if the affinity of the drug A for the target <b>1</b> may be predicted to be greater than that of the drug B for the target <b>1</b>, and meanwhile the affinity of the drug B for the target <b>1</b> may also be predicted to be greater than that of the drug C for the target <b>1</b>, the drug A, the drug B, and the drug C may be ranked according to the magnitude of their affinities for the target <b>1</b>, thus realizing drug recommendation.</p><p id="p-0058" num="0057">Similarly, the information of the objective target in this embodiment may be identified using a SMILES sequence, and the information of the candidate drug may be identified using a FASTA sequence.</p><p id="p-0059" num="0058">In use, the information of the objective target and the information of the plurality of candidate drugs are subjected to an embedding operation and then input into the ranking model, and the ranking model may predict and output, based on the input information, the ranking relationship for ranking the plurality of candidate drugs according to the magnitude of their affinities for the objective target protein. Subsequently, the drug with the highest affinity for the objective target protein may be obtained based on the ranking relationship, thus realizing the drug recommendation.</p><p id="p-0060" num="0059">In the drug ranking method according to this embodiment, the ranking model shares the parameters of the pre-trained ranking learning model, the ranking learning model is configured to learn the magnitude relationship between the affinities of any two drugs for a same target protein, and the accuracy of ranking the drugs may be effectively improved using the ranking model, thus more effectively recommending the drug.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram according to a fifth embodiment of the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, this embodiment provides an apparatus <b>500</b> for training a ranking learning model, including:</p><p id="p-0062" num="0061">a collecting module <b>501</b> configured to collect a plurality of training samples, each of the plurality of training samples including information of a known training target protein, information of two corresponding training drugs, and a real difference between affinities of the two corresponding training drugs for the known training target; and</p><p id="p-0063" num="0062">a training module <b>502</b> configured to train the ranking learning model with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</p><p id="p-0064" num="0063">The apparatus <b>500</b> for training a ranking learning model according to this embodiment has the same implementation as the above-mentioned relevant method embodiment by adopting the above-mentioned modules to implement the implementation principle and the technical effects of training the ranking learning model, and for details, reference may be made to the description of the above-mentioned relevant method embodiment, and details are not repeated herein.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram according to a sixth embodiment of the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the technical solution of the apparatus <b>500</b> for training a ranking learning model according to this embodiment of the present disclosure is further described in more detail based on the technical solution of the above-mentioned embodiment shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, in the apparatus <b>500</b> for training a ranking learning model according to this embodiment, the training module <b>502</b> includes an input unit <b>5021</b> configured to input the information of the known training target protein and the information of the two corresponding training drugs in each of the plurality of training samples into the ranking learning model; an acquiring unit <b>5022</b> configured to acquire a predicted difference between the affinities of the two training drugs for the known training target protein output by the ranking learning model; and an adjusting unit <b>5023</b> configured to adjust parameters of the ranking learning model based on the predicted difference between the affinities and the corresponding real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</p><p id="p-0067" num="0066">Further optionally, the adjusting unit <b>5023</b> is configured to construct a loss function based on the predicted difference between the affinities and the corresponding real difference between the affinities; detect whether the loss function converges; and if the loss function does not converge, adjust the parameters of the ranking learning model, such that the loss function tends to converge.</p><p id="p-0068" num="0067">Further optionally, in the apparatus <b>500</b> for training a ranking learning model according to this embodiment, the collecting module <b>501</b> is configured to collect a plurality of training samples from a plurality of data sets.</p><p id="p-0069" num="0068">The affinities of the training drugs for the known training targets in different data sets are characterized by different indexes.</p><p id="p-0070" num="0069">The apparatus <b>500</b> for training a ranking learning model according to this embodiment has the same implementation as the above-mentioned relevant method embodiment by adopting the above-mentioned modules to implement the implementation principle and the technical effects of training the ranking learning model, and for details, reference may be made to the description of the above-mentioned relevant method embodiment, and details are not repeated herein.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram according to a seventh embodiment of the present disclosure; as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, this embodiment provides a drug ranking apparatus <b>700</b>, including an acquiring module <b>701</b> configured to acquire information of an objective target and information of a plurality of candidate drugs; and a ranking module <b>702</b> configured to rank the plurality candidate drugs according to their respective affinities for the objective target by a ranking model based on the information of the objective target and the information of the plurality of candidate drugs, herein the ranking model shares parameters of a pre-trained ranking learning model, and the ranking learning model is configured to learn a magnitude relationship between affinities of any two drugs for a same target protein.</p><p id="p-0072" num="0071">The drug ranking apparatus <b>700</b> according to this embodiment has the same implementation as the above-mentioned relevant method embodiment by adopting the above-mentioned modules to implement the implementation principle and the technical effects of ranking the drugs, and for details, reference may be made to the description of the above-mentioned relevant method embodiment, and details are not repeated herein.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a schematic block diagram of an exemplary electronic device <b>800</b> which may be configured to implement the embodiments of the present disclosure. The electronic device is intended to represent various forms of digital computers, such as laptop computers, desktop computers, workstations, servers, blade servers, mainframe computers, and other appropriate computers. The electronic device may also represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smart phones, wearable devices, and other similar computing devices. The components shown herein, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementation of the present disclosure described and/or claimed herein.</p><p id="p-0074" num="0073">As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the electronic device <b>800</b> includes a computing unit <b>801</b> which may perform various appropriate actions and processing operations according to a computer program stored in a read only memory (ROM) <b>802</b> or a computer program loaded from a storage unit <b>808</b> into a random access memory (RAM) <b>803</b>. Various programs and data necessary for the operation of the electronic device <b>800</b> may be also stored in the RAM <b>803</b>. The computing unit <b>801</b>, the ROM <b>802</b>, and the RAM <b>803</b> are connected with one other through a bus <b>804</b>. An input/output (I/O) interface <b>805</b> is also connected to the bus <b>804</b>.</p><p id="p-0075" num="0074">The plural components in the electronic device <b>800</b> are connected to the I/O interface <b>805</b>, and include: an input unit <b>806</b>, such as a keyboard, a mouse, or the like; an output unit <b>807</b>, such as various types of displays, speakers, or the like; the storage unit <b>808</b>, such as a magnetic disk, an optical disk, or the like; and a communication unit <b>809</b>, such as a network card, a modem, a wireless communication transceiver, or the like. The communication unit <b>809</b> allows the electronic device <b>800</b> to exchange information/data with other devices through a computer network, such as the Internet, and/or various telecommunication networks.</p><p id="p-0076" num="0075">The computing unit <b>801</b> may be a variety of general and/or special purpose processing components with processing and computing capabilities. Some examples of the computing unit <b>801</b> include, but are not limited to, a central processing unit (CPU), a graphic processing unit (GPU), various dedicated artificial intelligence (AI) computing chips, various computing units running machine learning model algorithms, a digital signal processor (DSP), and any suitable processor, controller, microcontroller, or the like. The computing unit <b>801</b> performs the methods and processing operations described above, such as the method for training a ranking learning model and the drug ranking method. For example, in some embodiments, the method for training a ranking learning model or the drug ranking method may be implemented as a computer software program tangibly contained in a machine readable medium, such as the storage unit <b>808</b>. In some embodiments, part or all of the computer program may be loaded and/or installed into the electronic device <b>800</b> via the ROM <b>802</b> and/or the communication unit <b>809</b>. When the computer program is loaded into the RAM <b>803</b> and executed by the computing unit <b>801</b>, one or more steps of the method for training a ranking learning model or the drug ranking method described above may be performed. Alternatively, in other embodiments, the computing unit <b>801</b> may be configured to perform the method for training a ranking learning model or the drug ranking method by any other suitable means (for example, by means of firmware).</p><p id="p-0077" num="0076">Various implementations of the systems and technologies described herein above may be implemented in digital electronic circuitry, integrated circuitry, field programmable gate arrays (FPGA), application specific integrated circuits (ASIC), application specific standard products (ASSP), systems on chips (SOC), complex programmable logic devices (CPLD), computer hardware, firmware, software, and/or combinations thereof. The systems and technologies may be implemented in one or more computer programs which are executable and/or interpretable on a programmable system including at least one programmable processor, and the programmable processor may be special or general, and may receive data and instructions from, and transmit data and instructions to, a storage system, at least one input device, and at least one output device.</p><p id="p-0078" num="0077">Program codes for implementing the method according to the present disclosure may be written in any combination of one or more programming languages. These program codes may be provided to a processor or a controller of a general purpose computer, a special purpose computer, or other programmable data processing devices, such that the program code, when executed by the processor or the controller, causes functions/operations specified in the flowchart and/or the block diagram to be implemented. The program code may be executed entirely on a machine, partly on a machine, partly on a machine as a stand-alone software package and partly on a remote machine, or entirely on a remote machine or a server.</p><p id="p-0079" num="0078">In the context of the present disclosure, the machine readable medium may be a tangible medium which may contain or store a program for use by or in connection with an instruction execution system, apparatus, or device. The machine readable medium may be a machine readable signal medium or a machine readable storage medium. The machine readable medium may include, but is not limited to, an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus, or device, or any suitable combination of the foregoing. More specific examples of the machine readable storage medium may include an electrical connection based on one or more wires, a portable computer disk, a hard disk, a random access memory (RAM), a read only memory (ROM), an erasable programmable read only memory (EPROM or flash memory), an optical fiber, a portable compact disc read only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the foregoing.</p><p id="p-0080" num="0079">To provide interaction with a user, the systems and technologies described here may be implemented on a computer having: a display device (for example, a cathode ray tube (CRT) or liquid crystal display (LCD) monitor) for displaying information to a user; and a keyboard and a pointing device (for example, a mouse or a trackball) by which a user may provide input for the computer. Other kinds of devices may also be used to provide interaction with a user; for example, feedback provided for a user may be any form of sensory feedback (for example, visual feedback, auditory feedback, or tactile feedback); and input from a user may be received in any form (including acoustic, speech or tactile input).</p><p id="p-0081" num="0080">The systems and technologies described here may be implemented in a computing system (for example, as a data server) which includes a back-end component, or a computing system (for example, an application server) which includes a middleware component, or a computing system (for example, a user computer having a graphical user interface or a web browser through which a user may interact with an implementation of the systems and technologies described here) which includes a front-end component, or a computing system which includes any combination of such back-end, middleware, or front-end components. The components of the system may be interconnected through any form or medium of digital data communication (for example, a communication network). Examples of the communication network include: a local area network (LAN), a wide area network (WAN), the Internet and a blockchain network.</p><p id="p-0082" num="0081">A computer system may include a client and a server. Generally, the client and the server are remote from each other and interact through the communication network. The relationship between the client and the server is generated by virtue of computer programs which run on respective computers and have a client-server relationship to each other. The server may be a cloud server, also called a cloud computing server or a cloud host, and is a host product in a cloud computing service system, so as to overcome the defects of high management difficulty and weak service expansibility in conventional physical host and virtual private server (VPS) service. The server may also be a server of a distributed system, or a server incorporating a blockchain.</p><p id="p-0083" num="0082">The technology according to the present disclosure provides a more efficient ranking learning model, and the plurality of drugs corresponding to the same target protein may be ranked more efficiently and more accurately.</p><p id="p-0084" num="0083">It should be understood that various forms of the flows shown above may be used and reordered, and steps may be added or deleted. For example, the steps described in the present disclosure may be executed in parallel, sequentially, or in different orders, which is not limited herein as long as the desired results of the technical solution disclosed in the present disclosure may be achieved.</p><p id="p-0085" num="0084">The above-mentioned implementations are not intended to limit the scope of the present disclosure. It should be understood by those skilled in the art that various modifications, combinations, sub-combinations and substitutions may be made, depending on design requirements and other factors. Any modification, equivalent substitution and improvement made within the spirit and principle of the present disclosure all should be included in the extent of protection of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for training a ranking learning model, comprising:<claim-text>collecting a plurality of training samples, each of the plurality of training samples comprising information of a known training target protein, information of two training drugs, and a real difference between affinities of the two training drugs for the known training target protein; and</claim-text><claim-text>training the ranking learning model with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein training the ranking learning model with the plurality of training samples, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples comprises:<claim-text>inputting the information of the known training target protein and the information of the two training drugs in each of the plurality of training samples into the ranking learning model;</claim-text><claim-text>acquiring a predicted difference between the affinities of the two training drugs for the known training target protein output by the ranking learning model; and</claim-text><claim-text>adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples comprises:<claim-text>constructing a loss function based on the predicted difference between the affinities and the real difference between the affinities;</claim-text><claim-text>detecting whether the loss function converges; and</claim-text><claim-text>if the loss function does not converge, adjusting the parameters of the ranking learning model, such that the loss function tends to converge.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the collecting the plurality of training samples comprises:<claim-text>collecting the plurality of training samples from a plurality of data sets.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the affinities of the training drugs for the known training target in different data sets are characterized by different indexes.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the real difference between affinities is a specific difference value or a value indicating a direction of the real difference between the affinities.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A drug ranking method, comprising:<claim-text>acquiring information of an objective target and information of a plurality of candidate drugs; and</claim-text><claim-text>ranking the plurality of candidate drugs according to magnitude of their respective affinities for the objective target by a ranking model based on the information of the objective target and the information of the plurality of candidate drugs, wherein the ranking model shares parameters of a pre-trained ranking learning model, and the ranking learning mode is configured to learn a magnitude relationship between affinities of any two drugs for a same target protein.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory connected with the at least one processor communicatively;</claim-text><claim-text>wherein the memory stores instructions executable by the at least one processor to cause the at least one processor to carry out a method for training a ranking learning model, which comprises:</claim-text><claim-text>collecting a plurality of training samples, each of the plurality of training samples comprising information of a known training target protein, information of two training drugs, and a real difference between affinities of the two training drugs for the known training target protein; and</claim-text><claim-text>training the ranking learning model with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein training the ranking learning model with the plurality of training samples, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples comprises:<claim-text>inputting the information of the known training target protein and the information of the two training drugs in each of the plurality of training samples into the ranking learning model;</claim-text><claim-text>acquiring a predicted difference between the affinities of the two training drugs for the known training target protein output by the ranking learning model; and</claim-text><claim-text>adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The electronic device according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples comprises:<claim-text>constructing a loss function based on the predicted difference between the affinities and the real difference between the affinities;</claim-text><claim-text>detecting whether the loss function converges; and</claim-text><claim-text>if the loss function does not converge, adjusting the parameters of the ranking learning model, such that the loss function tends to converge.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the collecting the plurality of training samples comprises:<claim-text>collecting the plurality of training samples from a plurality of data sets.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The electronic device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the affinities of the training drugs for the known training target in different data sets are characterized by different indexes.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the real difference between affinities is a specific difference value or a value indicating a direction of the real difference between the affinities.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory connected with the at least one processor communicatively;</claim-text><claim-text>wherein the memory stores instructions executable by the at least one processor to cause the at least one processor to carry out a drug ranking method, which comprises:</claim-text><claim-text>acquiring information of an objective target and information of a plurality of candidate drugs; and</claim-text><claim-text>ranking the plurality of candidate drugs according to magnitude of their respective affinities for the objective target by a ranking model based on the information of the objective target and the information of the plurality of candidate drugs, wherein the ranking model shares parameters of a pre-trained ranking learning model, and the ranking learning mode is configured to learn a magnitude relationship between affinities of any two drugs for a same target protein.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium comprising computer instructions, which, when executed by a computer, cause the computer to carry out a method for training a ranking learning model, which comprises:<claim-text>collecting a plurality of training samples, each of the plurality of training samples comprising information of a known training target protein, information of two training drugs, and a real difference between affinities of the two training drugs for the known training target protein; and</claim-text><claim-text>training the ranking learning model with the plurality of training samples, such that the ranking learning model learns a capability of predicting a magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein training the ranking learning model with the plurality of training samples, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples comprises:<claim-text>inputting the information of the known training target protein and the information of the two training drugs in each of the plurality of training samples into the ranking learning model;</claim-text><claim-text>acquiring a predicted difference between the affinities of the two training drugs for the known training target protein output by the ranking learning model; and</claim-text><claim-text>adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein adjusting parameters of the ranking learning model based on the predicted difference between the affinities and the real difference between the affinities, such that the ranking learning model learns the capability of predicting the magnitude relationship between the affinities of the two training drugs for the known training target protein in each of the plurality of training samples comprises:<claim-text>constructing a loss function based on the predicted difference between the affinities and the real difference between the affinities;</claim-text><claim-text>detecting whether the loss function converges; and</claim-text><claim-text>if the loss function does not converge, adjusting the parameters of the ranking learning model, such that the loss function tends to converge.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the collecting the plurality of training samples comprises:<claim-text>collecting the plurality of training samples from a plurality of data sets.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the affinities of the training drugs for the known training target in different data sets are characterized by different indexes.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable storage medium comprising computer instructions, which, when executed by a computer, cause the computer to carry out a drug ranking method, which comprises:<claim-text>acquiring information of an objective target and information of a plurality of candidate drugs; and</claim-text><claim-text>ranking the plurality of candidate drugs according to magnitude of their respective affinities for the objective target by a ranking model based on the information of the objective target and the information of the plurality of candidate drugs, wherein the ranking model shares parameters of a pre-trained ranking learning model, and the ranking learning mode is configured to learn a magnitude relationship between affinities of any two drugs for a same target protein.</claim-text></claim-text></claim></claims></us-patent-application>