<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005251A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005251</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17821850</doc-number><date>20220824</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7747</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20180101</date></cpc-version-indicator><section>G</section><class>16</class><subclass>H</subclass><main-group>30</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0014</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DIAGNOSTIC ASSISTANCE APPARATUS AND MODEL GENERATION APPARATUS</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/JP2020/011839</doc-number><date>20200317</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17821850</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Murata Manufacturing Co., Ltd.</orgname><address><city>Kyoto</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>TAKE</last-name><first-name>Kouji</first-name><address><city>Kyoto</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>MATSUI</last-name><first-name>Kazuki</first-name><address><city>Kyoto</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A diagnostic assistance apparatus according to an aspect of the present disclosure determines whether a body part of a target examinee captured in a target medical image is normal, by using a trained first classification model generated by unsupervised learning using a plurality of first learning medical images of normal cases and a trained second classification model generated by supervised learning using a plurality of learning data sets including normal cases and abnormal cases.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="65.79mm" wi="106.76mm" file="US20230005251A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="226.91mm" wi="150.96mm" file="US20230005251A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="107.95mm" wi="130.22mm" file="US20230005251A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="143.59mm" wi="153.84mm" file="US20230005251A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="233.43mm" wi="156.21mm" file="US20230005251A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.75mm" wi="155.28mm" file="US20230005251A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="229.02mm" wi="119.04mm" file="US20230005251A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="213.78mm" wi="154.86mm" file="US20230005251A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="251.97mm" wi="104.14mm" file="US20230005251A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="189.99mm" wi="157.31mm" file="US20230005251A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="186.94mm" wi="157.99mm" file="US20230005251A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="208.87mm" wi="143.00mm" file="US20230005251A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="236.56mm" wi="155.53mm" file="US20230005251A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="124.12mm" wi="147.40mm" file="US20230005251A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This is a continuation of International Application No. PCT/JP2020/011839 filed on Mar. 17, 2020. The content of this application is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE DISCLOSURE</heading><heading id="h-0003" level="1">Field of the Disclosure</heading><p id="p-0003" num="0002">The present disclosure relates to a diagnostic assistance apparatus and a model generation apparatus.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">In recent years, technologies of assisting diagnoses based on medical images have been developed with the use of artificial intelligence. For example, Patent Document 1 describes supervised learning of a multilayer neural network, using images with correct labels indicating results of classifying findings of lesion areas. In this supervised learning, the multilayer neural network is trained to match an input image as training data to a correct label corresponding to a result acquired by classifying a lesion area in the input image. As the result of this supervised learning, the trained multilayer neural network can acquire a capability to classify lesion areas with given images.</p><p id="p-0005" num="0004">Patent Document 1: Japanese Unexamined Patent Application Publication No. 2019-082881</p><heading id="h-0005" level="1">BRIEF SUMMARY OF THE DISCLOSURE</heading><p id="p-0006" num="0005">Such a classification model trained by, for example, the supervised learning described in Patent Document 1 can carry out various classification tasks such as classifying the type of a lesion and specifying the location of a lesion area. The inventors of the present disclosure, however, have discovered that this method has the following problem.</p><p id="p-0007" num="0006">Supervised learning is one of the machine learning methods. To put it briefly, supervised learning is a learning method in which a learning model learns the correspondence between training data and correct labels. To create a trained classification model that can carry out classification tasks on medical images, as described above, supervised learning uses learning data sets, each being a combination of a medical image as training data and a correct label indicating a finding on the medical image.</p><p id="p-0008" num="0007">The number of learning data sets used in supervised learning is one of the factors that affect the accuracy of classification with the trained classification model created by supervised learning. Specifically, it can be expected that the more learning data set are used in supervised learning, the more accurate the classification by the trained classification model created by the supervised learning is. Especially when various kinds of training data are used in large amounts, the improvement of accuracy of classification by the trained classification model can be expected.</p><p id="p-0009" num="0008">A large number of medical images can be easily collected with relatively low costs from, for example, regular health checkups. However, most of the people who undergo regular health checkups are healthy. For this reason, it is difficult to collect a large number of medical images capturing abnormal body parts of subjects, which can be used as training data of abnormal cases; furthermore, collecting such medical images is time-consuming and costly. It is, therefore, burdensome to prepare a large number of learning data sets containing both normal cases and abnormal cases in terms of time and cost. This makes it difficult to use supervised learning to create trained classification models with high classification accuracy.</p><p id="p-0010" num="0009">In an aspect, the present disclosure has been made in consideration of the above circumstances, and a possible benefit thereof is to provide a technology for improving accuracy of classifying medical images with relatively low costs.</p><p id="p-0011" num="0010">To address the problem described above, the present disclosure applies the following configurations.</p><p id="p-0012" num="0011">A diagnostic assistance apparatus according to an aspect of the present disclosure includes a data acquisition unit, a first model computation unit having a first classification model, a second model computation unit having a second classification model, a determination unit, and an output unit. The data acquisition unit is configured to acquire a target medical image capturing a body part of a target examinee. The first classification model is trained, by unsupervised learning using a plurality of first learning medical images capturing normal body parts, to provide an evaluation of a degree of normality of a body part captured in a fed medical image by one-class classification. The first model computation unit is configured to, by feeding the acquired target medical image to the trained first classification model and performing an arithmetic operation of the trained first classification model, acquire as a first result a degree of normality evaluated by the one-class classification on the body part of the target examinee captured in the target medical image. The second classification model is trained, by supervised learning using a plurality of learning data sets, each learning data set being a combination of a second learning medical image and a correct label indicating whether a body part captured in the second learning medical image is normal, to provide an evaluation of a degree of normality of a body part captured in a fed medical image. The second learning medical images of the plurality of learning data sets include a normal medical image capturing a normal body part and an abnormal medical image capturing an abnormal body part. The second model computation unit is configured to, by feeding the acquired target medical image to the trained second classification model and performing an arithmetic operation of the trained second classification model, acquire as a second result a degree of normality evaluated on the body part of the target examinee captured in the target medical image. The determination unit is configured to, in accordance with the first result and the second result, provide a determination of whether the body part of the target examinee captured in the target medical image is normal. The output unit is configured to provide an output of a result of the determination.</p><p id="p-0013" num="0012">As described above, learning medical images of normal cases can be easily collected with relatively low costs from, for example regular health checkups, whereas collecting learning data sets including learning medical images of non-normal (abnormal) cases is difficult and costly. In this regard, the diagnostic assistance apparatus according to this configuration uses two classification models of the trained first classification model generated by unsupervised learning and the trained second classification model generated by supervised learning, to classify whether a body part of a target examinee captured in a medical image is normal. With this configuration, the performance of the trained second classification model, which is trained by supervised learning using a relatively small number of learning data sets including training data of abnormal cases, can be supplemented by the trained first classification model, which is trained by unsupervised learning using learning medical images of normal cases easily collectable with relatively low costs. As a result, with this configuration, it is possible to improve accuracy of classifying medical images with relatively low costs. The target examinee may include a model representing a human body (for example, a phantom for X-ray imaging).</p><p id="p-0014" num="0013">In the diagnostic assistance apparatus according to an aspect, the first result and the second result may be configured to indicate the degree of normality of the body part by a numerical value. The determination unit may include a connector having a first parameter determining a priority level of the first result and a second parameter determining a priority level of the second result. The determination by the determination unit may consist of: by feeding the acquired first and second results to the connector, individually weighting the first result and the second result with the first parameter and the second parameter; connecting the weighted first result and the weighted second result; and determining whether the body part of the target examinee is normal by comparing the numerical value (determination value) acquired by the connection operation to a threshold. With this configuration, improvements in the classification accuracy can be properly achieved by tuning parameters.</p><p id="p-0015" num="0014">In the diagnostic assistance apparatus according to an aspect, the first parameter and the second parameter may be tuned to optimize accuracy of the determination on a plurality of third learning medical images, body parts captured in the plurality of third learning medical images being previously determined to be normal or non-normal. With this configuration, by the optimization of the parameters, further improvements in the classification accuracy can be expected.</p><p id="p-0016" num="0015">In the diagnostic assistance apparatus according to an aspect, at least any one of the first parameter, the second parameter, and the threshold may be specified by an input by an operator. With this configuration, to improve the classification accuracy, the parameters can be easily tuned. The operator may include a medical doctor or user who directly or indirectly operates the diagnostic assistance apparatus. Indirectly operating the diagnostic assistance apparatus may include accessing the diagnostic assistance apparatus by using a terminal and receiving operation results from the diagnostic assistance apparatus by using the terminal.</p><p id="p-0017" num="0016">In the diagnostic assistance apparatus according to an aspect, the first classification model may include an encoder configured to provide a conversion of a fed medical image into a feature and a decoder configured to decode the medical image from the feature. The unsupervised learning may include training the encoder and the decoder such that, when each first learning medical image is fed to the encoder, a decoded image responsively generated by the decoder matches the first learning medical image. With this configuration, it is possible to provide the first classification model that can evaluate the degree of normality of a body part captured in a medical image by one-class classification and reconstruct a medical image capturing a normal body part.</p><p id="p-0018" num="0017">In the diagnostic assistance apparatus according to an aspect, the first classification model may further include a one-class classifier trained by the unsupervised learning to provide an evaluation by the one-class classification in accordance with a feature acquired by the encoder. The arithmetic operation of the first classification model may include, by feeding the target feature acquired by the conversion to the trained one-class classifier, acquiring the first result from the trained one-class classifier. With this configuration, it is possible to provide the first classification model that can properly evaluate the degree of normality of a body part captured in a medical image. The one-class classifier may be implemented by any model useable for machine learning. The one-class classifier may be implemented by, for example, a neural network.</p><p id="p-0019" num="0018">In the diagnostic assistance apparatus according to an aspect, the arithmetic operation of the first classification model may include, when the body part of the target examinee is determined to be non-normal, by feeding the acquired target medical image to the trained encoder, providing a conversion of the target medical image into a target feature, by feeding the target feature acquired by the conversion to the trained decoder, generating a target decoded image from the target feature, calculating a difference between the target medical image and the target decoded image generated, and specifying in the target medical image a related area by which the body part of the target examinee is determined to be non-normal (that is, abnormal), in accordance with the calculated difference. The output of a result of the determination may include an output of information indicating the related area specified. With this configuration, as well as to classify whether a body part of a target examinee is normal, when the body part of the target examinee is determined to be non-normal, it is possible to extract a related area relating to the determination. It should be noted that the related area can be interpreted as a possible lesion area.</p><p id="p-0020" num="0019">In the diagnostic assistance apparatus according to an aspect, each learning data set may further include learning attribute information. The second classification model may be trained, by the supervised learning additionally using the learning attribute information, to provide an evaluation of a degree of normality of a body part captured in a fed medical image with reference to fed attribute information. The data acquisition unit may be configured to additionally acquire target attribute information indicating an attribute of the target examinee. The second model computation unit may be configured to acquire the second result by additionally feeding the acquired target attribute information to the trained second classification model and performing the arithmetic operation of the trained second classification model. With this configuration, with additional reference to attribute information, improvements in the classification accuracy can be expected. The attribute may relate to any kinds of person's characteristics such as age, sex, height, weight, waist circumference, and chest measurement.</p><p id="p-0021" num="0020">In the diagnostic assistance apparatus according to an aspect, the second classification model may be implemented by a convolutional neural network. With this configuration, it is possible to provide the second classification model that can properly classify whether a body part captured in a medical image is normal. The configuration of the second classification model is not necessarily limited to this example. The second classification model may be implemented by any model useable for machine learning.</p><p id="p-0022" num="0021">In the diagnostic assistance apparatus according to an aspect, the output of a result of the determination may include associating result information indicating the result of the determination with the target medical image. With this configuration, the convenience of using the acquired target medical image can be increased. For example, in accordance with the associated result information, it is possible to extract only the target medical image capturing a body part determined to be non-normal. Accordingly, only the target medical image capturing a non-normal body part can be displayed on a display device. As a result, the display efficiency of the display device can be enhanced. The data format of the result information is not limited to a particular type, and any type of activation function may be selected as appropriate to the embodiment. The result information may be implemented by, for example, a Digital Imaging and Communications in Medicine (DICOM) tag.</p><p id="p-0023" num="0022">In the diagnostic assistance apparatus according to an aspect, the output of a result of the determination may include combining information indicating the result of the determination with the target medical image. With this configuration, the convenience of using the acquired target medical image can be increased.</p><p id="p-0024" num="0023">The present disclosure is not necessarily implemented as the diagnostic assistance apparatus. An aspect of the present disclosure may be a model generation apparatus for generating a trained first classification model and a trained second classification model usable by the diagnostic assistance apparatus. It should be noted that the model generation apparatus can be interpreted as a learning apparatus.</p><p id="p-0025" num="0024">For example, a model generation apparatus according to an aspect of the present disclosure includes a first acquisition unit, a first learning unit, a second acquisition unit, a second learning unit, a third acquisition unit, a determination unit, and a tuning unit. The first acquisition unit is configured to acquire a plurality of first learning medical images capturing normal body parts. The first learning unit is configured to perform unsupervised learning of a first classification model by using the plurality of first learning medical images acquired. The first classification model is configured to accept an input of a medical image and provide an evaluation of a degree of normality of a body part captured in the input medical image by one-class classification. The unsupervised learning includes training the first classification model such that, when an input medical image belongs to a class of the plurality of first learning medical images, a body part captured in the input medical image is evaluated as normal; when the input medical image does not belong to the class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as non-normal. The second acquisition unit is configured to acquire a plurality of learning data sets, each learning data set being a combination of a second learning medical image and a correct label indicating whether a body part captured in the second learning medical image is normal. The second learning medical images of the plurality of learning data sets include a normal medical image capturing a normal body part and an abnormal medical image capturing an abnormal body part. The second learning unit is configured to perform supervised learning of a second classification model by using the plurality of learning data sets acquired. The second classification model is configured to accept an input of a medical image and provide an evaluation of a degree of normality of a body part captured in the input medical image. The supervised learning includes training the second classification model such that, with respect to each learning data set, in response to an input of the second learning medical image, when providing an evaluation of a degree of normality on a body part captured in the input second learning medical image, a result of the evaluation matches the correct label corresponding to the second learning medical image. The third acquisition unit is configured to acquire a plurality of third learning medical images, body parts captured in the plurality of third learning medical images being previously determined to be normal or non-normal. The determination unit is configured to, by using the trained first classification model and the trained second classification model, provide a determination of whether a body part captured in each third learning medical image acquired is normal. The determination unit is configured to, by feeding each third learning medical image to the trained first classification model, acquire as a first result a degree of normality evaluated on a body part captured in the third learning medical image by one-class classification. The determination unit is configured to, by feeding each third learning medical image to the trained second classification model, acquire as a second result a degree of normality evaluated on a body part captured in the third learning medical image. The first result and the second result are configured to indicate the degree of normality of the body part by a numerical value. The determination unit includes a connector having a first parameter determining a priority level of the first result and a second parameter determining a priority level of the second result. The determination unit is configured to, by feeding the acquired first result and the acquired second result to the connector, weight the first result and the second result by using the first parameter and the second parameter. The determination unit is configured to provide a connection between the weighted first result and the weighted second result. The determination unit is configured to compare a numerical value (determination value) acquired by the connection to a threshold and accordingly provide a determination of whether a body part captured in each third learning medical image is normal. The tuning unit is configured to tune the first parameter and the second parameter to optimize accuracy of the determination on each third learning medical image. With this configuration, trained machine learning models (the first classification model and the second classification model) with relatively high accuracy of classification on medical images can be generated with relatively low costs.</p><p id="p-0026" num="0025">In the model generation apparatus according to an aspect, the plurality of third learning medical images may include one or a plurality of limit samples; and the tuning unit may be configured to tune the first parameter and the second parameter to avoid making the determination incorrect on all the one or plurality of limit samples. With this configuration, improvements in the accuracy of classification on the limit samples and similar examination targets can be expected. By determining serious cases as the limit samples, it is possible to reduce the probability of misclassification of the serious cases and similar cases.</p><p id="p-0027" num="0026">The model generation apparatus according to an aspect may further include an enlargement processing unit configured to, by subjecting a primary medical image capturing a body part to enlargement processing, generate at least a portion of a collection of the plurality of first learning medical images and the second learning medical images of the plurality of learning data sets. With this configuration, it is possible to easily increase the number of the learning medical images with little cost, thereby improving the classification accuracy of the trained first classification model and the trained second classification model generated. The enlargement processing is generating a new medical image different from the primary medical image by an image processing operation such as parallel translation, while maintaining at least a portion of the feature captured in the primary medical image. For example, the enlargement processing is constituted by parallel translation, rotation, swiveling, flipping or flopping, cropping, contrast change, enlargement, or reduction, or a combination of any operations selected from parallel translation, rotation, swiveling, flipping or flopping, cropping, contrast change, enlargement, and reduction, performed on the primary medical image.</p><p id="p-0028" num="0027">In the diagnostic assistance apparatus according to an aspect and the model generation apparatus according to an aspect, the first result by the first classification model and the second result by the second classification model are treated in parallel with other. The configuration of the apparatuses, however, is not necessarily limited to this example. For example, in the apparatuses, the first result by the first classification model may be inputted to the second classification model.</p><p id="p-0029" num="0028">For example, a model generation apparatus according to an aspect of the present disclosure includes a first acquisition unit, a first learning unit, a second acquisition unit, and a second learning unit. The first acquisition unit is configured to acquire a plurality of first learning medical images capturing normal body parts. The first learning unit is configured to perform unsupervised learning of a first classification model by using the plurality of first learning medical images acquired. The first classification model is configured to accept an input of a medical image and provide an evaluation of a degree of normality of a body part captured in the input medical image by one-class classification. The unsupervised learning includes training the first classification model such that, when an input medical image belongs to a class of the plurality of first learning medical images, a body part captured in the input medical image is evaluated as normal; when the input medical image does not belong to the class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as non-normal. The second acquisition unit is configured to acquire a plurality of learning data sets, each learning data set being a combination of a second learning medical image and a correct label indicating whether a body part captured in the second learning medical image is normal. The second learning medical images of the plurality of learning data sets include a normal medical image capturing a normal body part and an abnormal medical image capturing an abnormal body part. The second learning unit is configured to perform supervised learning of a second classification model by using the plurality of learning data sets acquired and the trained first classification model. The second classification model is configured to accept an input of a medical image and an input of a result of the evaluation on the medical image by the first classification model and provide an evaluation of a degree of normality of a body part captured in the input medical image. The supervised learning includes training the second classification model such that, with respect to each learning data set, in response to an input of the second learning medical image and an input of a result of the evaluation on the second learning medical image by the first classification model, when providing an evaluation of a degree of normality on a body part captured in the input second learning medical image, a result of the evaluation matches the correct label corresponding to the second learning medical image. With this configuration, trained machine learning models (the first classification model and the second classification model) with relatively high accuracy of classification on medical images can be generated with relatively low costs.</p><p id="p-0030" num="0029">In the model generation apparatus according to an aspect, the first classification model may include a generator configured to generate a pseudo-medical image and a discriminator configured to accept an input of a medical image and identify an origin of the input medical image. The training of the first classification model may include alternately repeating a first step of training the discriminator to identify whether the generator or the plurality of first learning medical images is an origin of an input medical image and a second step of training the generator to generate a pseudo-medical image that degrades discrimination performance of the discriminator. The result of the evaluation by the first classification model may be made based on a difference between an input medical image and a pseudo-medical image generated by the generator. With this configuration, it is possible to provide the trained machine learning models that can properly classify whether a body part captured in a medical image is normal.</p><p id="p-0031" num="0030">In the model generation apparatus according to an aspect, the first classification model may further include an estimator configured to accept an input of a medical image and estimate an input value fed to the generator to generate the input medical image by the generator. The second learning unit may be configured to further train the estimator to, with respect to each learning data set, minimize a difference between the second learning medical image and a pseudo-medical image generated by the trained generator from an estimation value estimated by the estimator from the second learning medical image. With this configuration, it is possible to provide the trained machine learning models that can properly classify whether a body part captured in a medical image is normal.</p><p id="p-0032" num="0031">A diagnostic assistance apparatus according to an aspect of the present disclosure includes a data acquisition unit configured to acquire a target medical image capturing a body part of a target examinee, a determination unit configured to provide a determination of whether the body part of the target examinee captured in the acquired target medical image is normal, by using the first classification model and the second classification model that are trained by the model generation apparatus according to any of the above aspects, and an output unit configured to provide an output of a result of the determination. With this configuration, it is possible to improve accuracy of classifying medical images with relatively low costs.</p><p id="p-0033" num="0032">As another embodiment of the model generation apparatus according to the embodiments described above and also another embodiment of the diagnostic assistance apparatus according to the embodiment described above, an aspect of the present disclosure may be an information processing method for implementing all or part of the configurations described above; a program therefor; or a storage medium storing such a program, readable by a computer, a device, a machine, and the like. Here, the storage medium readable by a computer and the like is a medium configured to store information such as programs with the use of an electrical, magnetic, optical, mechanical, or chemical effect. Further, an aspect of the present disclosure may be a diagnostic assistance system including the model generation apparatus and diagnostic assistance apparatus according to any of the embodiments.</p><p id="p-0034" num="0033">A diagnostic assistance method according to an aspect of the present disclosure is an information processing method implemented by a computer, including acquiring a target medical image capturing a body part of a target examinee; by feeding the acquired target medical image to a trained first classification model and performing an arithmetic operation of the trained first classification model, acquiring as a first result a degree of normality evaluated by one-class classification on the body part of the target examinee captured in the target medical image, the first classification model being trained, by unsupervised learning using a plurality of first learning medical images capturing normal body parts, to provide an evaluation of a degree of normality of a body part captured in a fed medical image by one-class classification; by feeding the acquired target medical image to a trained second classification model and performing an arithmetic operation of the trained second classification model, acquiring as a second result a degree of normality evaluated on the body part of the target examinee captured in the target medical image, the second classification model being trained, by supervised learning using a plurality of learning data sets, each learning data set being a combination of a second learning medical image and a correct label indicating whether a body part captured in the second learning medical image is normal, to provide an evaluation of a degree of normality of a body part captured in a fed medical image, the second learning medical images of the plurality of learning data sets including a normal medical image capturing a normal body part and an abnormal medical image capturing an abnormal body part; in accordance with the first result and the second result, providing a determination of whether the body part of the target examinee captured in the target medical image is normal; and providing an output of a result of the determination.</p><p id="p-0035" num="0034">A model generation method according to an aspect of the present disclosure is an information processing method including acquiring a plurality of first learning medical images capturing normal body parts; performing unsupervised learning of a first classification model by using the plurality of first learning medical images acquired, the first classification model being configured to accept an input of a medical image and provide an evaluation of a degree of normality of a body part captured in the input medical image by one-class classification, the unsupervised learning including training the first classification model such that, when an input medical image belongs to a class of the plurality of first learning medical images, a body part captured in the input medical image is evaluated as normal; when the input medical image does not belong to the class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as non-normal; acquiring a plurality of learning data sets, each learning data set being a combination of a second learning medical image and a correct label indicating whether a body part captured in the second learning medical image is normal, the second learning medical images of the plurality of learning data sets including a normal medical image capturing a normal body part and an abnormal medical image capturing an abnormal body part; and performing supervised learning of a second classification model by using the plurality of learning data sets acquired and the trained first classification model, the second classification model being configured to accept an input of a medical image and an input of a result of the evaluation on the medical image by the first classification model and provide an evaluation of a degree of normality of a body part captured in the input medical image, the supervised learning including training the second classification model such that, with respect to each learning data set, in response to an input of the second learning medical image and an input of a result of the evaluation on the second learning medical image by the first classification model, when providing an evaluation of a degree of normality on a body part captured in the input second learning medical image, a result of the evaluation matches the correct label corresponding to the second learning medical image.</p><p id="p-0036" num="0035">The present disclosure can improve accuracy of classifying medical images with relatively low costs.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE SEVERAL VIEWS OF THE DRAWINGS</heading><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates an example of a case using the present disclosure.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates an example of a hardware configuration of a model generation apparatus according to an embodiment.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates an example of a hardware configuration of a diagnostic assistance apparatus according to the embodiment.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates an example of a software configuration of the model generation apparatus according to the embodiment.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> schematically illustrates an example of an unsupervised learning process of a first classification model by the model generation apparatus according to the embodiment.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> schematically illustrates an example of a supervised learning process of a second classification model by the model generation apparatus according to the embodiment.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>5</b>C</figref> schematically illustrates an example of a parameter tuning process by the model generation apparatus according to the embodiment.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates an example of a software configuration of the diagnostic assistance apparatus according to the embodiment.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example of a procedure of data enlargement processing by the model generation apparatus according to the embodiment.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating an example of a procedure of unsupervised learning of the first classification model by the model generation apparatus according to the embodiment.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating an example of a procedure of supervised learning of the second classification model by the model generation apparatus according to the embodiment.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating an example of a procedure of parameter tuning (optimization) by the model generation apparatus according to the embodiment.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of a procedure of the diagnostic assistance apparatus according to the embodiment.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>12</b>A</figref> schematically illustrates an example of display presentation of result information according to the embodiment.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>12</b>B</figref> schematically illustrates an example of display presentation of result information according to the embodiment.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> schematically illustrates an example of display presentation of a related area according to the embodiment.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> schematically illustrates an example of display presentation of a related area according to the embodiment.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>13</b>C</figref> schematically illustrates an example of display presentation of a related area according to the embodiment.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> schematically illustrates an example of a supervised learning process of a second classification model according to a modification.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> schematically illustrates an example of a parameter tuning process according to the modification.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>14</b>C</figref> schematically illustrates an example of a process of a diagnostic assistance apparatus according to the modification.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>15</b></figref> schematically illustrates an example of an unsupervised learning process of a first classification model according to a modification.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> schematically illustrates an example of an unsupervised learning process of a first classification model according to a modification.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> schematically illustrates an example of a supervised learning process of a second classification model according to the modification.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>16</b>C</figref> schematically illustrates an example of a process of a diagnostic assistance apparatus according to the modification.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>17</b></figref> schematically illustrates a configuration of a diagnostic system according to a modification.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE DISCLOSURE</heading><p id="p-0063" num="0062">Hereinafter, an embodiment (also referred to as &#x201c;the present embodiment&#x201d; in the following) according to an aspect of the present disclosure will be described with reference to the drawings. The present embodiment described below is mere an example of the present disclosure in all respects. As might be expected, various changes and modifications can be made without departing from the range of the present disclosure. This means that, to implement the present disclosure, specific configurations according to the embodiment can also be used when appropriate. It should be noted that data is explained by a natural language in the present embodiment; but more specifically, data is specified by, for example, pseudo-languages, commands, parameters, and machine languages that can be recognizable by computers.</p><heading id="h-0008" level="1">&#xa7; 1 APPLICATION EXAMPLE</heading><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates an example of a case using the present disclosure. A diagnostic assistance system <b>100</b> according to the present embodiment includes a model generation apparatus <b>1</b> and a diagnostic assistance apparatus <b>2</b>.</p><p id="p-0065" num="0064">The model generation apparatus <b>1</b> according to the present embodiment is a computer configured to generate a trained model by machine learning. Specifically, the model generation apparatus <b>1</b> acquires a plurality of first learning medical images <b>31</b> capturing normal body parts of subjects. The model generation apparatus <b>1</b> performs unsupervised learning, using the acquired first learning medical images <b>31</b>, of a first classification model <b>5</b>. The first classification model <b>5</b> is configured to evaluate by one-class classification the degree of normality of a body part captured in an accepted input medical image. One example of the evaluation of the degree of normality by one-class classification is to evaluate the distance to learning data mapped in a feature space from a linear or non-linear hyperplane. With this method, it is possible to determine whether a body part captured in a medical image is normal, for example, such that when the evaluation value of a fed medical image is relatively large, the body part captured in the medical image is determined to be abnormal; when the evaluation value is relatively small, the body part is determined to be normal. The unsupervised learning includes training the first classification model <b>5</b> in the following manner: when an input medical image belongs to the class of the first learning medical images <b>31</b>, the body part captured in the input medical image is evaluated as normal; when the input medical image does not belong to the class of the first learning medical images <b>31</b>, the body part captured in the input medical image is evaluated as non-normal. As the result of this training, the first classification model <b>5</b> acquires a capability to evaluate the degree of normality of a body part captured in a fed medical image by one-class classification.</p><p id="p-0066" num="0065">The model generation apparatus <b>1</b> acquires a plurality of learning data sets <b>33</b>, each being a combination of a second learning medical image <b>331</b> and a correct label <b>333</b> indicating whether a body part of the corresponding second learning medical image <b>331</b> is normal. The second learning medical images <b>331</b> of the learning data sets <b>33</b> include normal medical images capturing normal body parts and abnormal medical images capturing abnormal body parts. The model generation apparatus <b>1</b> performs supervised learning of a second classification model <b>6</b> with the acquired learning data sets <b>33</b>. The second classification model <b>6</b> is configured to accept an input of a medical image and evaluate the degree of normality of a body part captured in the input medical image. The supervised learning includes training the second classification model <b>6</b> such that, with respect to each learning data set <b>33</b>, in response to an input of the second learning medical image <b>331</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>. As the result of this training, the second classification model <b>6</b> acquires a capability to evaluate the degree of normality of a body part captured in a fed medical image.</p><p id="p-0067" num="0066">As the result of performing the machine learning tasks, the model generation apparatus <b>1</b> generates the trained first classification model <b>5</b> and the trained second classification model <b>6</b>.</p><p id="p-0068" num="0067">The diagnostic assistance apparatus <b>2</b> according to the present embodiment is a computer configured to classify whether a body part of a target examinee is normal by using the trained first classification model <b>5</b> and the trained second classification model <b>6</b>. Specifically, the diagnostic assistance apparatus <b>2</b> acquires a target medical image <b>221</b> capturing a body part of a target examinee. The diagnostic assistance apparatus <b>2</b> feeds the acquired target medical image <b>221</b> to the trained first classification model <b>5</b> and then performs an arithmetic operation of the trained first classification model <b>5</b>. As a result, the diagnostic assistance apparatus <b>2</b> acquires as a first result the degree of normality of the body part of the target examinee captured in the target medical image <b>221</b> by evaluating the degree of normality by one-class classification. The diagnostic assistance apparatus <b>2</b> feeds the acquired target medical image <b>221</b> to the trained second classification model <b>6</b> and then performs an arithmetic operation of the trained second classification model <b>6</b>. As a result, the diagnostic assistance apparatus <b>2</b> acquires as a second result the degree of normality evaluated on the body part of the target examinee captured in the target medical image <b>221</b>. In accordance with the first and second results, the diagnostic assistance apparatus <b>2</b> determines whether the body part of the target examinee captured in the target medical image <b>221</b> is normal. The diagnostic assistance apparatus <b>2</b> accordingly outputs the determination result.</p><p id="p-0069" num="0068">As described above, in the present embodiment, the model generation apparatus <b>1</b> generates the first classification model <b>5</b> trained by unsupervised learning using the first learning medical images <b>31</b> of normal cases; the model generation apparatus <b>1</b> also generates the second classification model <b>6</b> trained by supervised learning using the learning data sets <b>33</b> including normal cases and abnormal cases. By using the two generated classification models, namely the trained first classification model <b>5</b> and the trained second classification model <b>6</b>, the diagnostic assistance apparatus <b>2</b> infers whether the body part of the target examinee captured in the target medical image <b>221</b> is normal. Collecting a large number of the learning data sets <b>33</b> including the second learning medical images <b>331</b> of abnormal cases is difficult and costly, whereas collecting the first learning medical images <b>31</b> of normal cases is relatively inexpensive and not very difficult. As a result, because the number of the learning data sets <b>33</b> is relatively small, the classification accuracy of the trained second classification model <b>6</b> may be relatively low. The classification capability can, however, be supplemented by the first classification model <b>5</b> trained with a large number of the first learning medical images <b>31</b> easily collected with relatively low costs. As such, the present embodiment can improve with relatively low costs the accuracy of classifying whether a body part of a target examinee captured in a medical image is normal.</p><p id="p-0070" num="0069">It should be noted that medical images are not limited to a particular type when the medical images can be used to assist diagnoses; any type of medical images may be selected as appropriate to the embodiment. Medical images may be, for example, X-ray photographs, computed tomography (CT) images, magnetic resonance imaging (MRI) images, or ultrasound images. A body part may be all or a portion of a part covering a body region, such as the head, the chest, or the abdomen. A body part may also be all or a portion of an organ (internal organ) or tissue at a part covering a body region.</p><p id="p-0071" num="0070">The expression &#x201c;a body part is normal&#x201d; may denote, for example, the case in which a person or machine reaches a diagnosis indicating that there is no lesion area or that the probability of the non-existence of a lesion area is relatively high; this means that the expression may include the case in which there is actually a lesion area. The expression &#x201c;a body part is abnormal&#x201d; may denote, for example, the case in which a person or machine reaches a diagnosis indicating that there is a lesion area or that the probability of the existence of a lesion area is relatively high; this means that the expression may include the case in which there is actually no lesion area.</p><p id="p-0072" num="0071">Unsupervised learning is a machine learning method basically using learning data without correct labels. Unsupervised learning may include self-supervised learning and adversarial learning. By contrast, supervised learning is a machine learning method using learning data with correct labels (the learning data sets <b>33</b> described above).</p><p id="p-0073" num="0072">The first result and the second result may be acquired directly or indirectly from the first classification model <b>5</b> and the second classification model <b>6</b>. This means that the first classification model <b>5</b> and the second classification model <b>6</b> are respectively configured to directly output the output values respectively corresponding to the first result and the second result; alternatively, the first result and the second result may be acquired by performing a predetermined arithmetic operation (for example, threshold determination or summation of multiple numerical values) on the respective output values from the first classification model <b>5</b> and the second classification model <b>6</b>.</p><p id="p-0074" num="0073">In the example in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b> are connected to each other through a network. The type of the network may be selected as appropriate from, for example, the Internet, a wireless communication network, a mobile communication network, a telephone network, and a dedicated network. The method for exchanging data between the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b> is not necessarily limited to this example, and any method may be selected as appropriate to the embodiment. For example, a storage medium may be used to exchange data between the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b>.</p><p id="p-0075" num="0074">In the example in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b> are implemented by discrete computers. The configuration of the diagnostic assistance system <b>100</b> according to the present embodiment, however, is not necessarily limited to this example, and the configuration can be determined as appropriate to the embodiment. For example, the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b> may be integrated into a single computer. Alternatively, for example, at least one of the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b> may be implemented by a plurality of computers.</p><heading id="h-0009" level="1">&#xa7; 2 CONFIGURATION EXAMPLE</heading><p id="p-0076" num="0075">[Hardware Configuration]</p><p id="p-0077" num="0076">&#x3c;Model Generation Apparatus&#x3e;</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates an example of a hardware configuration of the model generation apparatus <b>1</b> according to the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the model generation apparatus <b>1</b> according to the present embodiment is a computer including a control unit <b>11</b>, a storage unit <b>12</b>, a communication interface <b>13</b>, an input device <b>14</b>, an output device <b>15</b>, and a drive <b>16</b> that are electrically connected. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the communication interface is referred to as &#x201c;communication I/F&#x201d;. The same reference holds for <figref idref="DRAWINGS">FIG. <b>3</b></figref> described later.</p><p id="p-0079" num="0078">The control unit <b>11</b> includes, for example, a central processing unit (CPU), a random-access memory (RAM), and a read-only memory (ROM) as hardware processors. The control unit <b>11</b> is configured to perform information processing tasks based on programs and various kinds of data. The CPU is an example of processor resources. The storage unit <b>12</b> is an example of memory resources. The storage unit <b>12</b> may be implemented by, for example, a hard disk drive or a solid state drive. In the present embodiment, the storage unit <b>12</b> stores various kinds of information including a model generation program <b>81</b>, the first learning medical images <b>31</b>, the learning data sets <b>33</b>, tuning data sets <b>35</b>, first learning result data <b>121</b>, second learning result data <b>123</b>, and tuning result data <b>125</b>.</p><p id="p-0080" num="0079">The model generation program <b>81</b> is configured to cause the model generation apparatus <b>1</b> to perform information processing operations described later (<figref idref="DRAWINGS">FIGS. <b>7</b> to <b>10</b></figref>), regarding machine learning of the classification models (<b>5</b>, <b>6</b>). The model generation program <b>81</b> contains a series of instructions for the information processing operation. The first learning medical images <b>31</b> are used for machine learning of the first classification model <b>5</b>. The learning data sets <b>33</b> are used for machine learning of the second classification model <b>6</b>. The plurality of tuning data sets <b>35</b> are used to tune the value of a first parameter, which determines the priority level of the first result, and the value of a second parameter, which determines the priority level of the second result. The first learning result data <b>121</b> indicates information about a result of machine learning of the first classification model <b>5</b>. The second learning result data <b>123</b> indicates information about a result of machine learning of the second classification model <b>6</b>. The tuning result data <b>125</b> indicates information about a result of tuning values of the parameter. In the present embodiment, the first learning result data <b>121</b>, the second learning result data <b>123</b>, and the tuning result data <b>125</b> are generated by running the model generation program <b>81</b>.</p><p id="p-0081" num="0080">The communication interface <b>13</b>, which is, for example, a wired local area network (LAN) module or wireless LAN module, is an interface for wired or wireless communications using a network. By using the communication interface <b>13</b>, the model generation apparatus <b>1</b> may perform data communication with another information processing apparatus through a network.</p><p id="p-0082" num="0081">The input device <b>14</b> is a device for input operation, such as a mouse and a keyboard. The output device <b>15</b> is a device for output operation, such as a display (display device) and a speaker. The input device <b>14</b> and the output device <b>15</b> may be implemented by a single device such as a touch panel display. An operator such as the user can operate the model generation apparatus <b>1</b> by using the input device <b>14</b> and the output device <b>15</b>.</p><p id="p-0083" num="0082">The drive <b>16</b>, which may be, for example, a compact disc (CD) drive or a digital versatile disc (DVD) drive, is a drive device for reading various kinds of information, such as programs, stored in a storage medium <b>91</b>. The storage medium <b>91</b> is a medium configured to store various kinds of information such as programs with the use of an electrical, magnetic, optical, mechanical, or chemical effect in such a manner that a computer, a device, a machine, and the like can read the information such as programs. The storage medium <b>91</b> may store at least any of the model generation program <b>81</b>, the first learning medical images <b>31</b>, the learning data sets <b>33</b>, and the tuning data sets <b>35</b>. In this case, the model generation apparatus <b>1</b> may acquire at least any of these from the storage medium <b>91</b>. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a disc storage medium, such as a CD or DVD, is illustrated as an example of the storage medium <b>91</b>. The type of the storage medium <b>91</b>, however, is not necessarily limited to disc, and may be any types other than disc. Examples of types of storage medium other than disc include semiconductor memories, such as a flash memory. Any type of drive may be selected as the drive <b>16</b> in accordance with the type of the storage medium <b>91</b>.</p><p id="p-0084" num="0083">Regarding the model generation apparatus <b>1</b>, constituent elements may be excluded, replaced, or added as appropriate to the embodiment. For example, processor resources may include a plurality of hardware processors. The hardware processors may be, for example, a microprocessor, a field-programmable gate array (FPGA), and a graphics processing unit (GPU). The storage unit <b>12</b> may be implemented by the RAM and ROM included in the control unit <b>11</b>. At least any of the communication interface <b>13</b>, the input device <b>14</b>, the output device <b>15</b>, and the drive <b>16</b> is not necessarily provided. The model generation apparatus <b>1</b> may be implemented by a plurality of computers. In this case, the computers may be identical to or different from each other with respect to hardware configuration. The model generation apparatus <b>1</b> may be an information processing apparatus designed especially for a service provided, or may be a general-purpose server device or a personal computer (PC).</p><p id="p-0085" num="0084">&#x3c;Diagnostic Assistance Apparatus&#x3e;</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates an example of a hardware configuration of the diagnostic assistance apparatus <b>2</b> according to the present embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the diagnostic assistance apparatus <b>2</b> according to the present embodiment is a computer including a control unit <b>21</b>, a storage unit <b>22</b>, a communication interface <b>23</b>, an input device <b>24</b>, an output device <b>25</b>, and a drive <b>26</b> that are electrically connected.</p><p id="p-0087" num="0086">The control unit <b>21</b> to the drive <b>26</b> of the diagnostic assistance apparatus <b>2</b> and a storage medium <b>92</b> may be respectively configured in the same manner as the control unit <b>11</b> to the drive <b>16</b> of the model generation apparatus <b>1</b> and the storage medium <b>91</b>. The control unit <b>21</b> includes, for example, a CPU, a RAM, and a ROM as hardware processors. The control unit <b>21</b> is configured to perform various information processing tasks based on programs and data. The storage unit <b>22</b> may be implemented by, for example, a hard disk drive or a solid state drive. The storage unit <b>22</b> stores various kinds of information including a diagnostic assistance program <b>82</b>, the first learning result data <b>121</b>, the second learning result data <b>123</b>, and the tuning result data <b>125</b>.</p><p id="p-0088" num="0087">The diagnostic assistance program <b>82</b> is configured to cause the diagnostic assistance apparatus <b>2</b> to perform an information processing operation for classifying whether a body part of a target examinee is normal by using the trained first classification model <b>5</b> and the trained second classification model <b>6</b>, which will be described later (<figref idref="DRAWINGS">FIG. <b>11</b></figref>). The diagnostic assistance program <b>82</b> contains a series of instructions for the information processing operation. The storage medium <b>92</b> may store at least any of the diagnostic assistance program <b>82</b>, the first learning result data <b>121</b>, the second learning result data <b>123</b>, and the tuning result data <b>125</b>. The diagnostic assistance apparatus <b>2</b> may accordingly acquire at least any of these from the storage medium <b>92</b>.</p><p id="p-0089" num="0088">Regarding the specific hardware configuration of the diagnostic assistance apparatus <b>2</b>, constituent elements may be excluded, replaced, or added as appropriate to the embodiment. For example, processor resources of the diagnostic assistance apparatus <b>2</b> may include a plurality of hardware processors. The hardware processors may be, for example, a microprocessor, an FPGA, and a GPU. The storage unit <b>22</b> may be implemented by the RAM and ROM included in the control unit <b>21</b>. At least any of the communication interface <b>23</b>, the input device <b>24</b>, the output device <b>25</b>, and the drive <b>26</b> is not necessarily provided. The diagnostic assistance apparatus <b>2</b> may be implemented by a plurality of computers. In this case, the computers may be identical to or different from each other with respect to hardware configuration. The diagnostic assistance apparatus <b>2</b> may be an information processing apparatus designed especially for a service provided, or may be a general-purpose server device or a general-purpose PC.</p><p id="p-0090" num="0089">[Software Configuration]</p><p id="p-0091" num="0090">&#x3c;Model Generation Apparatus&#x3e;</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates an example of a software configuration of the model generation apparatus <b>1</b> according to the present embodiment. <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> schematically illustrates an example of an unsupervised learning process of the first classification model <b>5</b>. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> schematically illustrates an example of a supervised learning process of the second classification model <b>6</b>. <figref idref="DRAWINGS">FIG. <b>5</b>C</figref> schematically illustrates an example of a process of tuning the value of the first parameter and the value of the second parameter.</p><p id="p-0093" num="0092">The control unit <b>11</b> of the model generation apparatus <b>1</b> loads into the RAM the model generation program <b>81</b> stored in the storage unit <b>12</b>. The control unit <b>11</b> controls individual units by using the CPU understanding and executing the instructions contained in the model generation program <b>81</b> loaded in the RAM. Accordingly, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the model generation apparatus <b>1</b> according to the present embodiment operates as a computer including software modules of a first acquisition unit <b>110</b>, a first learning unit <b>111</b>, a second acquisition unit <b>112</b>, a second learning unit <b>113</b>, a third acquisition unit <b>114</b>, a determination unit <b>115</b>, a tuning unit <b>116</b>, a storage processing unit <b>117</b>, a primary data acquisition unit <b>118</b>, and an enlargement processing unit <b>119</b>. This means that in the present embodiment the control unit <b>11</b> (CPU) implements the software modules.</p><p id="p-0094" num="0093">The first acquisition unit <b>110</b> acquires the first learning medical images <b>31</b> capturing normal body parts of subjects. By using the acquired first learning medical images <b>31</b>, the first learning unit <b>111</b> performs unsupervised learning of the first classification model <b>5</b>. The first classification model <b>5</b> is configured to accept an input of a medical image and output an output value corresponding to a result of classifying by one-class classification whether a body part captured in the input medical image is normal. The unsupervised learning includes training the first classification model <b>5</b> in the following manner: when an input medical image belongs to the class of the first learning medical images <b>31</b>, the body part captured in the input medical image is classified as normal; when the input medical image does not belong to the class of the first learning medical images <b>31</b>, the body part captured in the input medical image is classified as non-normal. The configuration of the first classification model <b>5</b> and the method of unsupervised learning are not limited when the first classification model <b>5</b> can be trained in this manner, and any configuration and method may be selected as appropriate to the embodiment.</p><p id="p-0095" num="0094">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in the present embodiment, the first classification model <b>5</b> includes an encoder <b>51</b> and a decoder <b>53</b>. The encoder <b>51</b> is configured to convert a fed medical image into features (extract features from a medical image). In other words, the encoder <b>51</b> is configured to accept an input of a medical image and output an output value corresponding to a result of converting the input medical image into features. The decoder <b>53</b> is configured to acquire by decoding a medical image from features acquired by the encoder <b>51</b>. In other words, the decoder <b>53</b> is configured to accept input features and output a result (a decoded image) of acquiring an original medical image from the input features. The unsupervised learning includes training the encoder <b>51</b> and the decoder <b>53</b> such that, when each first learning medical image <b>31</b> is fed to the encoder <b>51</b>, a decoded image responsively generated by the decoder <b>53</b> matches the first learning medical image <b>31</b>. This means that in the unsupervised learning the encoder <b>51</b> and the decoder <b>53</b> are trained to minimize reconstruction errors between the first learning medical images <b>31</b> and the corresponding decoded images.</p><p id="p-0096" num="0095">In the present embodiment, the first classification model <b>5</b> further includes a one-class classifier <b>55</b>. The one-class classifier <b>55</b> is configured to accept input features acquired by the encoder <b>51</b> and output an output value corresponding to a result of evaluating the degree of normality of a body part captured in the medical image by one-class classification in accordance with the input features. The unsupervised learning further includes training the one-class classifier <b>55</b> to provide evaluation by one-class classification in accordance with features acquired by the encoder <b>51</b>. In other words, the unsupervised learning includes training the one-class classifier <b>55</b> in the following manner: when input features belong to the class of features learned by the trained encoder <b>51</b> from the input first learning medical images <b>31</b>, a body part captured in the fed medical image is evaluated as normal; when input features do not belong to the class of the learned features, a body part captured in the fed medical image is evaluated as non-normal.</p><p id="p-0097" num="0096">The second acquisition unit <b>112</b> acquires the plurality of learning data sets <b>33</b>, each being a combination of the second learning medical image <b>331</b> and the correct label <b>333</b> indicating whether a body part of the corresponding second learning medical image <b>331</b> is normal. The data format of the correct label <b>333</b> may be determined as appropriate to the embodiment. For example, the correct label <b>333</b> may be configured to use two values to indicate whether it is normal or not, or use two or more values to indicate abnormal categories (including normal case). As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the second learning medical images <b>331</b> of the learning data sets <b>33</b> include normal medical images <b>3311</b> capturing normal body parts of subjects and abnormal medical images <b>3312</b> capturing abnormal body parts of subjects. The second classification model <b>6</b> is configured to accept an input of a medical image and output an output value corresponding to a result of evaluating the degree of normality of a body part captured in the input medical image. The second learning unit <b>113</b> performs supervised learning of the second classification model <b>6</b> with the acquired learning data sets <b>33</b>. The supervised learning includes training the second classification model <b>6</b> such that, with respect to each learning data set <b>33</b>, in response to an input of the second learning medical image <b>331</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>.</p><p id="p-0098" num="0097">The third acquisition unit <b>114</b> acquires a plurality of third learning medical images <b>351</b> capturing body parts having been determined to be normal or non-normal. The method of determining whether a body part captured in the third learning medical image <b>351</b> is normal may be determined as appropriate to the embodiment. In the present embodiment, similarly to the second learning medical image <b>331</b>, the third learning medical image <b>351</b> is labelled with a correct label <b>353</b>. The correct label <b>353</b> is configured to indicate whether a body part captured in the corresponding third learning medical image <b>351</b> is normal. Specifically, the third acquisition unit <b>114</b> acquires the plurality of tuning data sets <b>35</b>, each being a combination of the third learning medical image <b>351</b> and the correct label <b>353</b>.</p><p id="p-0099" num="0098">The determination unit <b>115</b> determines, with the trained first classification model <b>5</b> and the trained second classification model <b>6</b>, whether a body part captured in each third learning medical image <b>351</b> acquired is normal. Specifically, by feeding the third learning medical images <b>351</b> to the trained first classification model <b>5</b>, the determination unit <b>115</b> acquires as the first result the degree of normality of a body part captured in each third learning medical image <b>351</b> by evaluating the degree of normality by one-class classification. By feeding the third learning medical images <b>351</b> to the trained second classification model <b>6</b>, the determination unit <b>115</b> acquires as the second result the degree of normality of a body part captured in each third learning medical image <b>351</b> by evaluating the degree of normality. In the present embodiment, the first result and the second result are configured to indicate the degree of normality of a body part by a numerical value. Indicating the degree of normality with a numerical value may be implemented by at least either directly indicating the degree of normality or indirectly indicating the degree of normality by indicating the degree of abnormality. This means that the first result and the second result may be indicated by at least either a numerical value indicating the degree of normality or a numerical value indicating the degree of abnormality. The degree may be represented by, for example, probability or index. The determination unit <b>115</b> includes a connector <b>7</b> having the first parameter, which determines the priority level of the first result, and the second parameter, which determines the priority level of the second result. The determination unit <b>115</b> feeds the acquired first result and the acquired second result to the connector <b>7</b> and weight the first result and the second result with the first parameter and the second parameter. The determination unit <b>115</b> connects the weighted first result and the weighted second result. The determination unit <b>115</b> compares the numerical value acquired by the connection (determination value) to a threshold to determine whether a body part captured in each third learning medical image <b>351</b> is normal. The tuning unit <b>116</b> tunes the first parameter and the second parameter to optimize the accuracy of determination by the determination unit <b>115</b> on the third learning medical images <b>351</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, the third learning medical images <b>351</b> may include one or a plurality of limit samples (may also referred to as &#x201c;limit medical images&#x201d;) <b>352</b>. All the third learning medical images <b>351</b> may be the limit samples <b>352</b>. In this case, the tuning unit <b>116</b> may tune the first parameter and the second parameter to avoid incorrect determination by the determination unit <b>115</b> about all the one or plurality of limit samples <b>352</b>.</p><p id="p-0100" num="0099">As illustrated in <figref idref="DRAWINGS">FIGS. <b>5</b>A to <b>5</b>C</figref>, the storage processing unit <b>117</b> generates information about results of the machine learning tasks and parameter tuning. The storage processing unit <b>117</b> generates information about the trained first classification model <b>5</b> as the first learning result data <b>121</b>. The storage processing unit <b>117</b> generates information about the trained second classification model <b>6</b> as the second learning result data <b>123</b>. The storage processing unit <b>117</b> generates information about the tuned connector <b>7</b> as the tuning result data <b>125</b>. The storage processing unit <b>117</b> stores the generated first learning result data <b>121</b>, the generated second learning result data <b>123</b>, and the generated tuning result data <b>125</b> in a predetermined storage area.</p><p id="p-0101" num="0100">The primary data acquisition unit <b>118</b> acquires one or a plurality of primary medical images <b>390</b> capturing body parts of subjects. The enlargement processing unit <b>119</b> performs enlargement processing on the primary medical image <b>390</b> to generate a new medical image <b>395</b>. The enlargement processing is generating the new medical image <b>395</b> different from the primary medical image <b>390</b> by an image processing operation such as parallel translation, while maintaining at least a portion of the collection of features captured in the primary medical image <b>390</b>.</p><p id="p-0102" num="0101">At least a portion of the collection of the learning medical images (<b>31</b>, <b>331</b>, <b>351</b>) may be constituted by at least either the primary medical images <b>390</b> or the new medical images <b>395</b>. Medical images other than the primary medical image <b>390</b> and the new medical image <b>395</b>, acquired individually, may be used as the learning medical images (<b>31</b>, <b>331</b>, <b>351</b>). The primary medical image <b>390</b> and the learning medical images (<b>31</b>, <b>331</b>, <b>351</b>) are medical images formed by capturing an image of a subject's body part by using an imaging device, such as an X-ray imaging device, a computed tomographic imaging device, or a magnetic resonance tomographic imaging device. The number of subjects for collection of each kind of images may be determined in any manner. The subject may include a model representing a human body (for example, a phantom for X-ray imaging).</p><p id="p-0103" num="0102">(Example of First Classification Model Configuration)</p><p id="p-0104" num="0103">The encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b> are implemented by machine learning models having operational parameters. The machine learning model for each purpose is not limited to a particular type when the machine learning model can carry out the corresponding arithmetic operation; any type of machine learning model may be selected as appropriate to the embodiment.</p><p id="p-0105" num="0104">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, in the present embodiment, fully connected neural networks are used as the encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b>. Each of the encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b> has an input layer (<b>511</b>, <b>531</b>, <b>551</b>), intermediate (hidden) layers (<b>512</b>, <b>532</b>, <b>552</b>), and an output layer (<b>513</b>, <b>533</b>, <b>553</b>). The number of intermediate layers (<b>512</b>, <b>532</b>, <b>552</b>) may be determined as appropriate to the embodiment. The architecture of each network is not limited to this example, and may be determined as appropriate to the embodiment. For example, the intermediate layers (<b>512</b>, <b>532</b>, <b>552</b>) may be excluded. For example, each network may include another kind of layer such as a convolutional layer.</p><p id="p-0106" num="0105">Each layer (<b>511</b> to <b>513</b>, <b>531</b> to <b>533</b>, <b>551</b> to <b>553</b>) contains one or a plurality of neurons (nodes). The number of neurons included in each layer may be determined as appropriate to the embodiment. For example, the number of neurons included in the input layer (<b>511</b>, <b>531</b>, <b>551</b>) and the number of neurons included in the output layer (<b>513</b>, <b>533</b>, <b>553</b>) may be determined depending on the dimensions of input and output data. Specifically, for example, the number of neurons included in the input layer <b>511</b> of the encoder <b>51</b> and the number of neurons included in the output layer <b>533</b> of the decoder <b>53</b> may be determined depending on the number of picture elements of the medical image. The number of neurons included in the input layers (<b>531</b>, <b>551</b>) of the decoder <b>53</b> and the one-class classifier <b>55</b> may be determined depending on the number of neurons included in the output layer <b>513</b> of the encoder <b>51</b>. The number of neurons included in the output layer <b>553</b> of the one-class classifier <b>55</b> may be determined to be a particular number (for example, one), depending on the type of the result of one-class classification.</p><p id="p-0107" num="0106">Neurons in adjacent layers are connected to each other as appropriate. In the example in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, all neurons are connected to each other between adjacent layers. The connection of neurons, however, is not limited to this example, and the connection can be set as appropriate to the embodiment. A weight (connection weight) is assigned to each connection. A threshold is set for each neuron. The output of a neuron is basically determined by whether the sum of products of each input and a corresponding weight exceeds a threshold. The threshold may be expressed by an activation function. In this case, the output of each neuron is determined by inputting the sum of products of each input and a corresponding weight to the activation function and calculating the activation function. The activation function is not limited to a particular type, and any type of activation function may be selected as appropriate to the embodiment. The weight of connection between neurons in different layers and the threshold of each neuron are examples of operational parameters used in corresponding arithmetic operations.</p><p id="p-0108" num="0107">The operational parameters of the elements of the first classification model <b>5</b> are tuned in the process of unsupervised learning to achieve a desired capability. Firstly, the first learning unit <b>111</b> inputs each first learning medical image <b>31</b> to the input layer <b>511</b> of the encoder <b>51</b> and performs an arithmetic operation of forward propagation of the encoder <b>51</b> and the decoder <b>53</b>. The arithmetic operation of forward propagation is determining whether each neuron included in each layer is fired, consecutively from the input side. As the result of the arithmetic operation of forward propagation of the encoder <b>51</b>, features (corresponding output values) extracted from the first learning medical image <b>31</b> can be acquired from the output layer <b>513</b> of the encoder <b>51</b>. Subsequently, as the result of the arithmetic operation of forward propagation of the decoder <b>53</b>, a decoded image (corresponding output values) generated based on the features in response to the first learning medical image <b>31</b> can be acquired from the output layer <b>533</b> of the decoder <b>53</b>. The first learning unit <b>111</b> tunes the operational parameters of the encoder <b>51</b> and the decoder <b>53</b> to reduce errors (reconstruction errors) between the first learning medical images <b>31</b> and the corresponding decoded images. Consequently, the trained encoder <b>51</b> and the trained decoder <b>53</b> are generated.</p><p id="p-0109" num="0108">Secondly, the first learning unit <b>111</b> inputs each first learning medical image <b>31</b> to the input layer <b>511</b> of the trained encoder <b>51</b> and performs the arithmetic operation of forward propagation of the encoder <b>51</b> and the one-class classifier <b>55</b>. As the result of the arithmetic operation of forward propagation of the one-class classifier <b>55</b>, an output value corresponding to a result of evaluating the degree of normality of a body part captured in each first learning medical image <b>31</b> can be acquired from the output layer <b>553</b> of the one-class classifier <b>55</b>. For example, the output value represents a distance from the origin of a feature space. The first learning unit <b>111</b> tunes the operational parameters of the one-class classifier <b>55</b> to maximize the distance from the origin. Consequently, the trained one-class classifier <b>55</b> with a maximized margin of classification boundary is generated.</p><p id="p-0110" num="0109">The storage processing unit <b>117</b> generates as the first learning result data <b>121</b> information indicating the architecture and operational parameters of each element of the trained first classification model <b>5</b> generated by the unsupervised learning described above. The architecture may be specified by, for example, the number of layers from the input layer to the output layer in a neural network, the type of each layer, the number of neurons included in each layer, and the connection between neurons in adjacent layers. When the same model architecture is used in the system, the information about architecture may be excluded from the first learning result data <b>121</b>. The storage processing unit <b>117</b> stores the generated first learning result data <b>121</b> in a predetermined storage area.</p><p id="p-0111" num="0110">(Example of Second Classification Model Configuration)</p><p id="p-0112" num="0111">Similarly to the first classification model <b>5</b>, the second classification model <b>6</b> is implemented by a machine learning model having operational parameters. The machine learning model for the second classification model <b>6</b> is not limited to a particular type when the machine learning model can carry out the arithmetic operation of evaluating the degree of normality of a body part captured in a medical image; any type of machine learning model may be selected as appropriate to the embodiment.</p><p id="p-0113" num="0112">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, in the present embodiment, a convolutional neural network is used for the second classification model <b>6</b>. The second classification model <b>6</b> has a convolutional layer <b>61</b>, pooling layers <b>62</b>, and fully connected layers (<b>63</b>, <b>64</b>). The convolutional layer <b>61</b> is configured to perform convolution operation on fed data. Convolution operation corresponds to an operation of calculating a correlation between fed data and a given filter. For example, by performing convolution operation on an image, a grayscale pattern similar to the grayscale pattern of a particular filter can be detected in the image. The convolutional layer <b>61</b> has a neuron (node) corresponding to this convolution operation; the neuron (node) is connected to the input or an area of the output of a layer before (on the input side with respect to) the convolutional layer <b>61</b>. The pooling layers <b>62</b> are configured to perform pooling. Pooling eliminates part of information about positions relatively strongly activated in fed data by a filter to achieve translation invariance regardless of minor changes in the position of features in the data. In pooling, for example, the maximum value in a filter may be extracted, while other values are deleted. The fully connected layers (<b>63</b>, <b>64</b>) are configured in the same manner as the layers (<b>511</b> to <b>513</b>, <b>531</b> to <b>533</b>, <b>551</b> to <b>553</b>) in the fully connected neural networks described above.</p><p id="p-0114" num="0113">The number of the layers <b>61</b> to <b>64</b> included in the second classification model <b>6</b> may be determined as appropriate to the embodiment. The arrangement of the convolutional layer <b>61</b> and the pooling layers <b>62</b> may be determined as appropriate to the embodiment. In the example in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, the convolutional layer <b>61</b> is arranged on the side closest to input (left side in the diagram); the fully connected layers (<b>63</b>, <b>64</b>) are arranged on the side closest to output (right side in the diagram); and the pooling layers <b>62</b> is arranged before the fully connected layer <b>63</b>. As a result, the convolutional layer <b>61</b> on the side closest to input forms the input layer, and the fully connected layer <b>64</b> forms the output layer. The convolutional layer <b>61</b> and the pooling layer <b>62</b> may be arranged in an alternating manner. The architecture of the second classification model <b>6</b>, however, is not necessarily limited to this example. For example, a plurality of convolutional layers <b>61</b> are consecutively arranged, followed by one or a plurality of pooling layers <b>62</b>. The types of layers included in the second classification model <b>6</b> are not necessarily limited to the layer types described above. The second classification model <b>6</b> may include other kinds of layers such as normalization layer and dropout layer.</p><p id="p-0115" num="0114">The number of neurons included in the fully connected layers (<b>63</b>, <b>64</b>) may be determined as appropriate to the embodiment. For example, the number of neurons included in the fully connected layer <b>64</b> forming the output layer may be determined depending on the type of evaluation by the second classification model <b>6</b>. The evaluation by the second classification model <b>6</b> may include, for example, classifying whether it is normal and distinguishing abnormal categories (that is, types). For example, when the fully connected layer <b>64</b> directly outputs an output value representing the degree of normality of a body part, the number of neurons included in the fully connected layer <b>64</b> may be one. Alternatively, when the fully connected layer <b>64</b> outputs output values representing the degree of normality and the degree of abnormality, the number of neurons included in the fully connected layer <b>64</b> may be two. Alternatively, when the fully connected layer <b>64</b> outputs output values representing the degree of abnormal categories (including normal case), the number of neurons included in the fully connected layer <b>64</b> may be determined depending on the categories. In this case, the degree of abnormality may be given by summing output values of the abnormal categories.</p><p id="p-0116" num="0115">Similarly to the first classification model <b>5</b>, a weight (connection weight) is assigned to each connection in the convolutional layer <b>61</b> and the fully connected layers (<b>63</b>, <b>64</b>). The threshold (or activation function) of each neuron can be given as appropriate. The weight of connection between neurons in the convolutional layer <b>61</b> and the fully connected layers (<b>63</b>, <b>64</b>) and the threshold of each neuron are examples of operational parameters used for the arithmetic operation of the second classification model <b>6</b>.</p><p id="p-0117" num="0116">The operational parameters of the elements of the second classification model <b>6</b> are tuned in the process of supervised learning to achieve a desired capability. The second learning unit <b>113</b> inputs the second learning medical image <b>331</b> of each learning data set <b>33</b> to the input layer (the convolutional layer <b>61</b> on the side closest to input) of the second classification model <b>6</b> and performs the arithmetic operation of forward propagation of the second classification model <b>6</b>. As the result of this arithmetic operation, an output value corresponding to a result of evaluating the degree of normality of a body part captured in each second learning medical image <b>331</b> can be acquired from the output layer (the fully connected layer <b>64</b>). The second learning unit <b>113</b> calculates errors between the evaluation result acquired from the output layer and the correct result indicated by the correct label <b>333</b> with respect to the individual learning data set <b>33</b>. The second learning unit <b>113</b> accordingly tunes the operational parameters of the second classification model <b>6</b> to reduce the calculated errors. Consequently, the trained second classification model <b>6</b> is generated.</p><p id="p-0118" num="0117">The storage processing unit <b>117</b> generates as the second learning result data <b>123</b> information indicating the architecture and operational parameters of the trained second classification model <b>6</b> generated by the supervised learning described above. Similarly to the first learning result data <b>121</b>, when the same model architecture is used in the system, the information about architecture of the second classification model <b>6</b> may be excluded from the second learning result data <b>123</b>. The storage processing unit <b>117</b> stores the generated second learning result data <b>123</b> in a predetermined storage area.</p><p id="p-0119" num="0118">(Connector)</p><p id="p-0120" num="0119">The configuration of the connector <b>7</b> is not limited to a particular configuration when the connector <b>7</b> can perform the arithmetic operation of weighting and connecting the first result and the second result to acquire a final determination result indicating whether a body part captured in a medical image is normal; the configuration of the connector <b>7</b> may be determined as appropriate to the embodiment.</p><p id="p-0121" num="0120">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>C</figref>, a two-layer fully connected neural network is used as the connector <b>7</b>. The connector <b>7</b> has an input layer <b>71</b> and an output layer <b>72</b>. The architecture of the connector <b>7</b> is not limited to this example, and may be determined as appropriate to the embodiment. For example, the connector <b>7</b> may be implemented by a fully connected neural network having three or more layers.</p><p id="p-0122" num="0121">Each layer (<b>71</b>, <b>72</b>) contains one or a plurality of neurons (nodes). The number of neurons included in the layers (<b>71</b>, <b>72</b>) may be determined as appropriate to the embodiment. For example, the number of neurons included in the input layer <b>71</b> may be determined depending on the dimensions of data of the results. For example, the number of neurons included in the output layer <b>72</b> may be determined depending on the dimensions of data of the final determination result. The output method of the output layer <b>72</b> may be determined as appropriate to the embodiment. For example, the output layer <b>72</b> may be configured to output a determination value (that is, a result of connecting the weighted first result and the weighted second result) for deriving a determination result by comparing the determination value to a threshold, or output an output value directly representing the determination result. When the output value of the output layer <b>72</b> indicates a determination value, the arithmetic operation of forward propagation of the connector <b>7</b> can be construed as including both an operation corresponding to individually weighting the first result and the second result and an operation corresponding to connecting the weighted first result and the weighted second result. When the output value of the output layer <b>72</b> directly represents a determination result, the arithmetic operation of forward propagation of the connector <b>7</b> can be construed as including an operation corresponding to individually weighting the first result and the second result, an operation corresponding to connecting the weighted first result and the weighted second result, and an operation corresponding to comparing the determination value acquired by the connection operation to a threshold.</p><p id="p-0123" num="0122">The connections between neurons may be set as appropriate to the embodiment. Similarly to the first classification model <b>5</b>, a weight (connection weight) is assigned to each connection between the layers (<b>71</b>, <b>72</b>). The threshold (or activation function) of each neuron can be given as appropriate. The weight of connection between neurons and the threshold of each neuron that relate to the operation of the first result are examples of the first parameter. The weight of connection between neurons and the threshold of each neuron that relate to the operation of the second result are examples of the second parameter.</p><p id="p-0124" num="0123">In the present embodiment, the parameters are tuned by machine learning using the plurality of tuning data sets <b>35</b>. Specifically, the determination unit <b>115</b> inputs the third learning medical image <b>351</b> of each tuning data set <b>35</b> to the encoder <b>51</b> of the trained first classification model <b>5</b> and performs the arithmetic operation of forward propagation of the encoder <b>51</b> and the one-class classifier <b>55</b> of the trained first classification model <b>5</b>. By performing this arithmetic operation, the determination unit <b>115</b> acquires the first result of each third learning medical image <b>351</b> from the one-class classifier <b>55</b>. The determination unit <b>115</b> also inputs the third learning medical image <b>351</b> of each tuning data set <b>35</b> to the trained second classification model <b>6</b> and performs the arithmetic operation of forward propagation of the second classification model <b>6</b>. By performing this arithmetic operation, the determination unit <b>115</b> acquires the second result of each third learning medical image <b>351</b> from the second classification model <b>6</b>. The determination unit <b>115</b> subsequently inputs the acquired first and second results to the input layer <b>71</b> of the connector <b>7</b> and performs the arithmetic operation of forward propagation of the connector <b>7</b>. By performing this arithmetic operation, a determination value or an output value corresponding to a determination result can be acquired from the output layer <b>72</b> of the connector <b>7</b>. The tuning unit <b>116</b> calculates errors between a determination result derived from a determination value or represented by an output value and a correct result indicated by the correct label <b>353</b> with respect to the individual tuning data sets <b>35</b>. The tuning unit <b>116</b> accordingly tunes the parameters of the connector <b>7</b> to reduce the calculated errors. When the third learning medical images <b>351</b> include one or a plurality of limit samples <b>352</b>, the tuning unit <b>116</b> tunes the operational parameters of the connector <b>7</b> to eliminate errors of determination results of the limit samples <b>352</b>. Consequently, the tuned connector <b>7</b> is generated.</p><p id="p-0125" num="0124">The storage processing unit <b>117</b> generates as the tuning result data <b>125</b> information indicating the architecture and parameters of the tuned connector <b>7</b> generated by the machine learning described above. Similarly to, for example, the first learning result data <b>121</b>, when the same model architecture is used in the system, the information about architecture of the connector <b>7</b> may be excluded from the tuning result data <b>125</b>. The storage processing unit <b>117</b> stores the generated tuning result data <b>125</b> in a predetermined storage area.</p><p id="p-0126" num="0125">&#x3c;Diagnostic Assistance Apparatus&#x3e;</p><p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates an example of a software configuration of the diagnostic assistance apparatus <b>2</b> according to the present embodiment. The control unit <b>21</b> of the diagnostic assistance apparatus <b>2</b> loads into the RAM the diagnostic assistance program <b>82</b> stored in the storage unit <b>22</b>. The control unit <b>21</b> controls individual units by using the CPU understanding and executing the instructions contained in the diagnostic assistance program <b>82</b> loaded in the RAM. Accordingly, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the diagnostic assistance apparatus <b>2</b> according to the present embodiment operates as a computer including software modules of a data acquisition unit <b>211</b>, a first model computation unit <b>212</b>, a second model computation unit <b>213</b>, a determination unit <b>214</b>, and an output unit <b>215</b>. This means that in the present embodiment the control unit (CPU) implements the software modules of the diagnostic assistance apparatus <b>2</b>, similarly to the model generation apparatus <b>1</b>.</p><p id="p-0128" num="0127">The data acquisition unit <b>211</b> acquires the target medical image <b>221</b> capturing a body part of a target examinee. The target medical image <b>221</b> is a medical image acquired by capturing an image of a body part of a target examinee with the use of an imaging device. the target examinee may include a model representing a human body, similarly to the subject described above.</p><p id="p-0129" num="0128">The first model computation unit <b>212</b> obtains the first learning result data <b>121</b>, thereby having the trained first classification model <b>5</b>. The first model computation unit <b>212</b> feeds the acquired target medical image <b>221</b> to the trained first classification model <b>5</b> and then performs an arithmetic operation of the trained first classification model <b>5</b>. In the present embodiment, the arithmetic operation of the first classification model <b>5</b> includes converting the acquired target medical image <b>221</b> into target features by the encoder <b>51</b> fed with the target medical image <b>221</b>; the arithmetic operation of the first classification model <b>5</b> also includes acquiring from the trained one-class classifier <b>55</b> an output value corresponding to the first result of the body part of the target examinee captured in the target medical image <b>221</b> by feeding the target features acquired by the conversion operation to the trained one-class classifier <b>55</b>. Specifically, the first model computation unit <b>212</b> inputs the acquired target medical image <b>221</b> to the trained encoder <b>51</b> and performs the arithmetic operation of forward propagation of the trained encoder <b>51</b> and the one-class classifier <b>55</b>. As a result, the first model computation unit <b>212</b> acquires an output value corresponding to the first result from the trained first classification model <b>5</b>.</p><p id="p-0130" num="0129">The second model computation unit <b>213</b> obtains the second learning result data <b>123</b>, thereby having the trained second classification model <b>6</b>. The second model computation unit <b>213</b> feeds the acquired target medical image <b>221</b> to the trained second classification model <b>6</b> and then performs an arithmetic operation of the trained second classification model <b>6</b>. In the present embodiment, the second model computation unit <b>213</b> inputs the acquired target medical image <b>221</b> to the input layer of the trained second classification model <b>6</b> and then performs the arithmetic operation of forward propagation of the trained second classification model <b>6</b>. Consequently, the second model computation unit <b>213</b> acquires an output value corresponding to the second result of the body part of the target examinee captured in the target medical image <b>221</b> from the output layer of the trained second classification model <b>6</b>.</p><p id="p-0131" num="0130">In accordance with the first and second results, the determination unit <b>214</b> determines whether the body part of the target examinee captured in the target medical image <b>221</b> is normal. In the present embodiment, the first result and the second result are configured to indicate the degree of normality of a body part by a numerical value. The determination unit <b>214</b> obtains the tuning result data <b>125</b>, thereby having the connector <b>7</b> with the parameters tuned to optimize the determination accuracy of the third learning medical images <b>351</b>. The determination operation based on the first result and the second result consists of: by feeding the acquired first and second results to the connector <b>7</b>, individually weighting the first result and the second result with the parameters; connecting the weighted first result and the weighted second result; and determining whether the body part of the target examinee is normal by comparing the determination value acquired by the connection operation to a threshold. Specifically, the determination unit <b>214</b> inputs values of the results to the input layer <b>71</b> of the connector <b>7</b> and performs the arithmetic operation of forward propagation of the connector <b>7</b>. As a result, the determination unit <b>214</b> acquires a determination value or an output value corresponding to a determination result from the output layer <b>72</b>. When the connector <b>7</b> is configured to output a determination value, the determination unit <b>214</b> derives a determination result by comparing the acquired determination value to a threshold. The output unit <b>215</b> outputs the determination result by the determination unit <b>214</b>.</p><p id="p-0132" num="0131">In the present embodiment, the arithmetic operation of the first classification model <b>5</b> may include, when a body part of a target examinee is determined to be non-normal, feeding the acquired target medical image <b>221</b> to the trained encoder <b>51</b> and converting the target medical image <b>221</b> into target features; feeding the target features acquired by the conversion operation to the trained decoder <b>53</b> and generating a target decoded image <b>225</b> from the target features; calculating the difference between the target medical image <b>221</b> and the target decoded image <b>225</b> generated; and, in accordance with the calculated difference, specifying in the target medical image <b>221</b> a related area by which the body part of the target examinee is determined to be non-normal. In this case, outputting the determination result may include outputting information indicating the specified related area.</p><p id="p-0133" num="0132">Specifically, the first model computation unit <b>212</b> may acquire output values corresponding to target features from the trained encoder <b>51</b> by inputting the target medical image <b>221</b> to the trained encoder <b>51</b> and performing the arithmetic operation of forward propagation of the trained encoder <b>51</b>. This arithmetic operation may be the same as the operation for acquiring the first result described above. Subsequently, the first model computation unit <b>212</b> may acquire an output corresponding to the target decoded image <b>225</b> from the trained decoder <b>53</b> by inputting the acquired target features to the trained decoder <b>53</b> and performing the arithmetic operation of forward propagation of the trained decoder <b>53</b>. The first model computation unit <b>212</b> may then calculate the difference between the target medical image <b>221</b> and the target decoded image <b>225</b> generated and specify the related area in the target medical image <b>221</b> in accordance with the calculated difference. The output unit <b>215</b> may output information indicating the specified related area as a determination result.</p><p id="p-0134" num="0133">The trained encoder <b>51</b> and decoder <b>53</b> can reconstruct with high accuracy the first learning medical image <b>31</b>, which is used for machine learning, and similar medical images, but the accuracy of reconstruction of other kinds of medical images may be relatively low. Because a medical image capturing a normal body part is used as the first learning medical image <b>31</b>, the accuracy of reconstruction of medical images capturing abnormal body parts by the trained encoder <b>51</b> and decoder <b>53</b> can be relatively low. This means that the trained encoder <b>51</b> and decoder <b>53</b> are likely to fail to reconstruct portions capturing abnormal body parts with high accuracy. Thus, in the arithmetic operation described above, an area with high probability of lesion (possible lesion area) can be extracted as the related area, based on the difference (reconstruction error) between the target medical image <b>221</b> and the target decoded image <b>225</b>. This operation of specifying (extracting) the related area can be performed when a body part of a target examinee is determined to be normal. This means that the operation of specifying the related area may be performed regardless of whether the body part of the target examinee is normal.</p><p id="p-0135" num="0134">&#x3c;Others&#x3e;</p><p id="p-0136" num="0135">The software modules of the model generation apparatus <b>1</b> and the software modules of the diagnostic assistance apparatus <b>2</b> will be described in detail with an operation example explained later. The present embodiment describes the example in which the software modules of the model generation apparatus <b>1</b> and the software modules of the diagnostic assistance apparatus <b>2</b> are implemented by general-purpose CPUs. However, part or all of the collection of the software modules may be implemented by one or a plurality of dedicated processors. Additionally, regarding the software configuration of the model generation apparatus <b>1</b> and the software configuration of the diagnostic assistance apparatus <b>2</b>, particular software modules may be excluded, replaced, or added as appropriate to the embodiment.</p><heading id="h-0010" level="1">&#xa7; 3 OPERATION EXAMPLE</heading><p id="p-0137" num="0136">[Model Generation Apparatus]</p><p id="p-0138" num="0137">&#x3c;Enlargement Processing&#x3e;</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example of a procedure of data enlargement processing by the model generation apparatus <b>1</b> according to the present embodiment. The following data enlargement procedure is a mere example, and individual steps may be changed when possible. Furthermore, in the following data enlargement procedure, for example, a particular step may be excluded, replaced, or added as appropriate to the embodiment.</p><p id="p-0140" num="0139">(Step S<b>101</b>)</p><p id="p-0141" num="0140">In step S<b>101</b>, the control unit <b>11</b> operates as the primary data acquisition unit <b>118</b> and acquires one or a plurality of primary medical images <b>390</b> capturing body parts of subjects.</p><p id="p-0142" num="0141">The primary medical image <b>390</b> may be generated as appropriate. For example, the primary medical image <b>390</b> can be generated by capturing an image of a normal or abnormal body part of a subject by an imaging device. The generated primary medical image <b>390</b> may be a raw medical image acquired by image capturing or a medical image having been processed by some kind of image processing. Alternatively, the primary medical image <b>390</b> may be a new medical image generated by performing enlargement processing in step S<b>102</b> described later on another primary medical image. A correct label indicating whether a body part captured in the primary medical image <b>390</b> is normal may be associated with the generated primary medical image <b>390</b>. The inference of whether a body part is normal can be made manually by a person such as a medical doctor or mechanically by a machine such as a trained machine learning model.</p><p id="p-0143" num="0142">The primary medical image <b>390</b> may be generated automatically by an operation of a computer or manually by being at least partially operated by the operator. The primary medical image <b>390</b> may be generated by the model generation apparatus <b>1</b> or a computer other than the model generation apparatus <b>1</b>. When the model generation apparatus <b>1</b> generates the primary medical image <b>390</b>, the control unit <b>11</b> performs the series of operations of the generation operation described above automatically or manually with the help of the operator operating with the input device <b>14</b>, so that one or a plurality of primary medical images <b>390</b> can be acquired. When another computer generates the primary medical image <b>390</b>, the control unit <b>11</b> may acquire one or a plurality of primary medical images <b>390</b> generated by the computer, for example, through a network or via the storage medium <b>91</b>. When a plurality of the primary medical images <b>390</b> are acquired, a portion of the collection of the primary medical images <b>390</b> may be generated by the model generation apparatus <b>1</b>, whereas the remainder of the collection of the primary medical images <b>390</b> may be generated by one or a plurality of computers other than the model generation apparatus <b>1</b>.</p><p id="p-0144" num="0143">The number of the primary medical images <b>390</b> to be acquired may be determined as appropriate to the embodiment. After the primary medical image <b>390</b> is acquired, the control unit <b>11</b> causes the process to proceed to the following step S<b>102</b>.</p><p id="p-0145" num="0144">(Step S<b>102</b>)</p><p id="p-0146" num="0145">In step S<b>102</b>, the control unit <b>11</b> operates as the enlargement processing unit <b>119</b> and subjects the acquired primary medical image <b>390</b> to enlargement processing and accordingly generate one or a plurality of new medical images <b>395</b>. The enlargement processing may be constituted by parallel translation, rotation, swiveling, flipping or flopping, cropping, contrast change, enlargement, or reduction, or a combination thereof performed on the primary medical image <b>390</b>. Rotation is image processing of turning an image around a given point determined as the center. Swiveling is image processing of turning an image around a fixed side to top, bottom, left, or right. The amount of each image processing task may be determined as appropriate in accordance with, for example, a specification by the operator or a configuration value in the program. The number of the new medical images <b>395</b> generated based on one primary medical image <b>390</b> is not limited to a particular number, and may be determined as appropriate to the embodiment.</p><p id="p-0147" num="0146">When a correct label is associated with the primary medical image <b>390</b>, a correct label may be associated with the new medical image <b>395</b>. When it is unnecessary to change the indication of correct label after the enlargement processing, the correct label associated with the primary medical image <b>390</b> may be associated with the new medical image <b>395</b>. Conversely, when it is necessary to change the indication of correct label after the enlargement processing, the correct label associated with the primary medical image <b>390</b> may be altered, or a new correct label may be generated; and the altered or generated correct label may be associated with the new medical image <b>395</b>. The alteration or generation of correct label may be manually or mechanically carried out. When the alteration of correct label is mechanically carried out, the indication of correct label after alteration may be estimated based on the enlargement processing. After generating the new medical image <b>395</b>, the control unit <b>11</b> causes the process to proceed to the following step S<b>103</b>.</p><p id="p-0148" num="0147">(Step S<b>103</b>)</p><p id="p-0149" num="0148">In step S<b>103</b>, the control unit <b>11</b> stores in a predetermined storage area the generated one or plurality of new medical images <b>395</b>. The predetermined storage area may be a memory such as the RAM in the control unit <b>11</b>, the storage unit <b>12</b>, an external storage device, or a storage medium, or a combination thereof. The storage medium may be, for example, a CD or DVD, and the control unit <b>11</b> may use the drive <b>16</b> to store the new medical image <b>395</b> in the storage medium. The external storage device may be, for example, a data server such as a network attached storage (NAS). In this case, the control unit <b>11</b> may use the communication interface <b>13</b> to store the new medical image <b>395</b> in the data server through a network. The external storage device may be, for example, a storage device externally connected to the model generation apparatus <b>1</b>. After the storage operation of the generated new medical image <b>395</b> is completed, the control unit <b>11</b> ends the procedure of data enlargement according to this operation example.</p><p id="p-0150" num="0149">&#x3c;Machine Learning of First Classification Model&#x3e;</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart illustrating an example of a procedure of unsupervised learning of the first classification model <b>5</b> by the model generation apparatus <b>1</b> according to the present embodiment. The following procedure of unsupervised learning of the first classification model <b>5</b> is a mere example, and individual steps may be changed when possible. Furthermore, in the following procedure of unsupervised learning of the first classification model <b>5</b>, for example, a particular step may be excluded, replaced, or added as appropriate to the embodiment.</p><p id="p-0152" num="0151">(Step S<b>111</b>)</p><p id="p-0153" num="0152">In step S<b>111</b>, the control unit <b>11</b> operates as the first acquisition unit <b>110</b> and acquires the first learning medical images <b>31</b> capturing normal body parts of subjects.</p><p id="p-0154" num="0153">At least a portion of the collection of the first learning medical images <b>31</b> may be at least either the primary medical image <b>390</b> or the new medical image <b>395</b>. When correct labels are associated with the primary medical image <b>390</b> and the new medical image <b>395</b>, the control unit <b>11</b> may extract at least either the primary medical image <b>390</b> or the new medical image <b>395</b> to be used as the first learning medical image <b>31</b> from a database of the primary medical image <b>390</b> and the new medical image <b>395</b> in accordance with the correct labels. At least a portion of the collection of the first learning medical images <b>31</b> may be acquired as a medical image other than the primary medical image <b>390</b> and the new medical image <b>395</b>. In this case, the first learning medical image <b>31</b> may be generated in the same manner as the primary medical image <b>390</b>. It is preferable that all the first learning medical images <b>31</b> be medical images capturing normal body parts of subjects. However, a portion of the collection of the first learning medical images <b>31</b> may be medical images capturing abnormal body parts of subjects.</p><p id="p-0155" num="0154">The number of the first learning medical images <b>31</b> to be acquired may be determined as appropriate to the embodiment. After the first learning medical images <b>31</b> are acquired, the control unit <b>11</b> causes the process to proceed to the following step S<b>112</b>.</p><p id="p-0156" num="0155">(Step S<b>112</b>)</p><p id="p-0157" num="0156">In step S<b>112</b>, the control unit <b>11</b> operates as the first learning unit <b>111</b> and performs unsupervised learning of the first classification model <b>5</b> with the acquired first learning medical images <b>31</b>.</p><p id="p-0158" num="0157">In the present embodiment, the first classification model <b>5</b> includes the encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b>. The control unit <b>11</b> trains the encoder <b>51</b> and the decoder <b>53</b> such that, when each first learning medical image <b>31</b> is fed to the encoder <b>51</b>, a decoded image responsively generated by the decoder <b>53</b> matches the first learning medical image <b>31</b> (in other words, reconstruction errors between the first learning medical images <b>31</b> and the corresponding decoded images are minimized). Subsequently, the control unit <b>11</b> trains the one-class classifier <b>55</b> by using the first learning medical images <b>31</b> to evaluate the degree of normality of a body part captured in a fed medical image by one-class classification in accordance with features acquired by the trained encoder <b>51</b>. Each training process may be carried out by using, for example, batch gradient descent, stochastic gradient descent, or mini-batch gradient descent.</p><p id="p-0159" num="0158">The neural networks implementing the encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b> targeted for machine learning may be prepared as appropriate. The architecture (for example, the number of layers, the number of neurons included in each layer, and the connection between neurons in adjacent layers), initial value of weight on each connection between neurons, and initial value of threshold of each neuron of each neural network may be provided by a template or inputted by the operator. When performing relearning, the control unit <b>11</b> may use learning result data acquired by the previous machine learning tasks to prepare the encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b>.</p><p id="p-0160" num="0159">As an example of the training process of the encoder <b>51</b> and the decoder <b>53</b>, firstly, the control unit <b>11</b> inputs each first learning medical image <b>31</b> to the input layer <b>511</b> of the encoder <b>51</b> and performs the arithmetic operation of forward propagation of the encoder <b>51</b> and the decoder <b>53</b>. By performing this arithmetic operation, the control unit <b>11</b> extracts features from each first learning medical image <b>31</b> and acquires from the output layer <b>533</b> of the decoder <b>53</b> an output value corresponding to a decoded image generated based on the extracted features.</p><p id="p-0161" num="0160">The control unit <b>11</b> calculates an error (reconstruction error) between each first learning medical image <b>31</b> and the corresponding decoded image acquired from the decoder <b>53</b>. Loss function may be used to calculate errors. Loss function is a function for evaluating a difference (that is, the degree of difference) between an output of a machine learning model and a correct result; the greater the difference between an output value and a correct result (desired value) is, the greater the error value given by a loss function is. The loss function used to calculate errors is not limited to a particular type, and any type of loss function may be selected as appropriate to the embodiment. A known loss function, such as root-mean-square error or cross entropy error may be used.</p><p id="p-0162" num="0161">With the use of the gradients of the calculated errors, the control unit <b>11</b> calculates by back propagation an error of each of the operational parameters (for example, the weight on the connection between neurons and the threshold of each neuron) of the decoder <b>53</b> and the encoder <b>51</b>. The control unit <b>11</b> updates the operational parameters of the encoder <b>51</b> and the decoder <b>53</b> in accordance with the calculated errors. How much the operational parameters are updated may be controlled based on a learning rate. The learning rate may be specified by the operator or given as a configuration value in the program.</p><p id="p-0163" num="0162">By performing the series of operations of the update operation described above, the control unit <b>11</b> tunes the operational parameters of the encoder <b>51</b> and the decoder <b>53</b> to decrease the sum of calculated errors. For example, the control unit <b>11</b> may repeat tuning of the operational parameters by performing the series of operations until a predetermined condition is satisfied; the predetermined condition may be, for example, the condition in which the series of operations are performed a predetermined number of times or the condition in which the sum of errors calculated reaches or falls below a threshold. In this manner, the control unit <b>11</b> can train the encoder <b>51</b> and the decoder <b>53</b> such that, when each first learning medical image <b>31</b> is fed to the encoder <b>51</b>, a decoded image responsively generated by the decoder <b>53</b> matches the first learning medical image <b>31</b>.</p><p id="p-0164" num="0163">In the training process of the encoder <b>51</b> and the decoder <b>53</b>, in addition to the update operation described above, the control unit <b>11</b> may calculate errors between the features acquired from the encoder <b>51</b> and the values acquired from a predetermined probability distribution (for example, Gaussian distribution) and accordingly further tune the operational parameters of the encoder <b>51</b> to reduce the sum of errors. In this manner, the control unit <b>11</b> may normalize the output value (features) from the encoder <b>51</b>.</p><p id="p-0165" num="0164">Next, as an example of the training process of the one-class classifier <b>55</b>, the control unit <b>11</b> inputs each first learning medical image <b>31</b> to the input layer <b>511</b> of the trained encoder <b>51</b> and performs the arithmetic operation of forward propagation of the encoder <b>51</b> and the one-class classifier <b>55</b>. By performing this arithmetic operation, the control unit <b>11</b> can acquire from the output layer <b>553</b> of the one-class classifier <b>55</b> an output value corresponding to a result of evaluating by one-class classification the degree of normality of a body part captured in each first learning medical image <b>31</b>.</p><p id="p-0166" num="0165">The output value acquired from the one-class classifier <b>55</b> corresponds to a result of mapping a normal body part in a feature space. The control unit <b>11</b> calculates as an error the distance between a point corresponding to the acquired output value and the origin in the feature space. With the use of the gradients of the calculated errors, the control unit <b>11</b> calculates errors of the respective operational parameters of the one-class classifier <b>55</b> by backpropagation. The control unit <b>11</b> updates the operational parameters of the one-class classifier <b>55</b> in accordance with the calculated errors. How much the operational parameters of the one-class classifier <b>55</b> are updated may be controlled based on the learning rate.</p><p id="p-0167" num="0166">By performing the series of operations of the update operation described above, the control unit <b>11</b> tunes the operational parameters of the one-class classifier <b>55</b> to decrease the calculated errors. Similarly to the encoder <b>51</b> and the decoder <b>53</b>, the control unit <b>11</b> may repeat tuning of the operational parameters of the one-class classifier <b>55</b> by performing the series of operations until the predetermined condition is satisfied. In this manner, the control unit <b>11</b> can train the one-class classifier <b>55</b> in the following manner: based on the features acquired from the trained encoder <b>51</b>, when a fed medical image belongs to the class of the first learning medical images <b>31</b>, the body part captured in the medical image is evaluated as normal; when the fed medical image does not belong to the class of the first learning medical images <b>31</b>, the body part captured in the medical image is evaluated as non-normal.</p><p id="p-0168" num="0167">As the result of the unsupervised learning, the encoder <b>51</b> and the one-class classifier <b>55</b> of the first classification model <b>5</b> learn the distribution of medical images capturing normal body parts from the first learning medical images <b>31</b>, so that the encoder <b>51</b> and the one-class classifier <b>55</b> of the first classification model <b>5</b> acquire a capability to evaluate by one-class classification the degree of normality of a body part captured in a fed medical image. Additionally, the decoder <b>53</b> acquires a capability to properly decode fed medical images belonging to the distribution of the first learning medical images <b>31</b> from the features acquired from the encoder <b>51</b>. After the training process of the first classification model <b>5</b> is completed, the control unit <b>11</b> causes the process to proceed to the following step S<b>113</b>.</p><p id="p-0169" num="0168">(Step S<b>113</b>)</p><p id="p-0170" num="0169">In step S<b>113</b>, the control unit <b>11</b> operates as the storage processing unit <b>117</b> and generates information about the result of unsupervised learning in step S<b>112</b> as the first learning result data <b>121</b>. In the present embodiment, the control unit <b>11</b> generates as the first learning result data <b>121</b> information indicating the architecture and operational parameters of the trained encoder <b>51</b>, the trained decoder <b>53</b>, and the trained one-class classifier <b>55</b> that are created by the unsupervised learning described above. The control unit <b>11</b> then stores the generated first learning result data <b>121</b> in a predetermined storage area.</p><p id="p-0171" num="0170">The predetermined storage area may be a memory such as the RAM in the control unit <b>11</b>, the storage unit <b>12</b>, an external storage device, or a storage medium, or a combination thereof. The storage medium may be, for example, a CD or DVD, and the control unit <b>11</b> may use the drive <b>16</b> to store the first learning result data <b>121</b> in the storage medium. The external storage device may be, for example, a data server such as a network attached storage (NAS). In this case, the control unit <b>11</b> may use the communication interface <b>13</b> to store the first learning result data <b>121</b> in the data server through a network. The external storage device may be, for example, a storage device externally connected to the model generation apparatus <b>1</b>. After the storage operation of the first learning result data <b>121</b> is completed, the control unit <b>11</b> ends the procedure of unsupervised learning of the first classification model <b>5</b> according to this operation example.</p><p id="p-0172" num="0171">The generated first learning result data <b>121</b> may be provided for the diagnostic assistance apparatus <b>2</b> at a given timing. For example, the control unit <b>11</b> may transfer the first learning result data <b>121</b> to the diagnostic assistance apparatus <b>2</b> as the operation in step S<b>113</b> or separately from the operation in step S<b>113</b>. The diagnostic assistance apparatus <b>2</b> may receive this transferred data to acquire the first learning result data <b>121</b>. The diagnostic assistance apparatus <b>2</b> may access the model generation apparatus <b>1</b> or the data server through a network by using the communication interface <b>23</b> to acquire the first learning result data <b>121</b>. Alternatively, for example, the diagnostic assistance apparatus <b>2</b> may acquire the first learning result data <b>121</b> via the storage medium <b>92</b>. Alternatively, for example, the first learning result data <b>121</b> may be previously included in the diagnostic assistance apparatus <b>2</b>.</p><p id="p-0173" num="0172">Furthermore, the control unit <b>11</b> may update or newly generate the first learning result data <b>121</b> by regularly or irregularly repeating the operations in steps S<b>111</b> to S<b>113</b>. While the operations are repeated, at least a portion of the collection of the first learning medical images <b>31</b> may be, for example, changed, corrected, added, or deleted as appropriate. Subsequently, the control unit <b>11</b> may update the first learning result data <b>121</b> stored in the diagnostic assistance apparatus <b>2</b> by providing the updated or newly generated first learning result data <b>121</b> for the diagnostic assistance apparatus <b>2</b> in a given manner.</p><p id="p-0174" num="0173">&#x3c;Machine Learning of Second Classification Model&#x3e;</p><p id="p-0175" num="0174"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating an example of a procedure of supervised learning of the second classification model <b>6</b> by the model generation apparatus <b>1</b> according to the present embodiment. The following procedure of supervised learning of the second classification model <b>6</b> is a mere example, and individual steps may be changed when possible. Furthermore, in the following procedure of supervised learning of the second classification model <b>6</b>, for example, a particular step may be excluded, replaced, or added as appropriate to the embodiment.</p><p id="p-0176" num="0175">(Step S<b>121</b>)</p><p id="p-0177" num="0176">In step S<b>121</b>, the control unit <b>11</b> operates as the second acquisition unit <b>112</b> and acquires the learning data sets <b>33</b>.</p><p id="p-0178" num="0177">At least a portion of the collection of the second learning medical images <b>331</b> of the learning data sets <b>33</b> may be at least either the primary medical image <b>390</b> or the new medical image <b>395</b>. In this case, the control unit <b>11</b> may extract at least either the primary medical image <b>390</b> or the new medical image <b>395</b> to be used as the second learning medical image <b>331</b> from the database of the primary medical image <b>390</b> and the new medical image <b>395</b> to include the normal medical image <b>3311</b> and the abnormal medical image <b>3312</b> as appropriate. At least a portion of the collection of the second learning medical images <b>331</b> of the learning data sets <b>33</b> may be acquired as a medical image other than the primary medical image <b>390</b> and the new medical image <b>395</b>. In this case, the learning data set <b>33</b> may be generated in the same manner as the primary medical image <b>390</b>. Additionally, a portion of the collection of the second learning medical images <b>331</b> of the learning data sets <b>33</b> may be the same as the first learning medical images <b>31</b>.</p><p id="p-0179" num="0178">The number of the learning data sets <b>33</b> to be acquired may be determined as appropriate to the embodiment. After the learning data sets <b>33</b> are acquired, the control unit <b>11</b> causes the process to proceed to the following step S<b>122</b>.</p><p id="p-0180" num="0179">(Step S<b>122</b>)</p><p id="p-0181" num="0180">In step S<b>122</b>, the control unit <b>11</b> operates as the second learning unit <b>113</b> and performs supervised learning of the second classification model <b>6</b> with the acquired learning data sets <b>33</b>. The control unit <b>11</b> trains the second classification model <b>6</b> such that, with respect to each learning data set <b>33</b>, in response to an input of the second learning medical image <b>331</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>. Specifically, the control unit <b>11</b> uses the second learning medical image <b>331</b> as training data, and the correct label <b>333</b> as teaching signal to perform supervised learning of the second classification model <b>6</b>. Similarly to step S<b>121</b> described above, this training process may be carried out by using, for example, batch gradient descent, stochastic gradient descent, or mini-batch gradient descent.</p><p id="p-0182" num="0181">Similarly to the first classification model <b>5</b> described above, the convolutional neural networks implementing the second classification model <b>6</b> targeted for machine learning may be prepared as appropriate. The architecture, initial value of weight on each connection between neurons, and initial value of threshold of each neuron of the second classification model <b>6</b> may be provided by a template or inputted by the operator. When performing relearning, the control unit <b>11</b> may use learning result data acquired by the previous machine learning tasks to prepare the second classification model <b>6</b>.</p><p id="p-0183" num="0182">An example of the training process of the second classification model <b>6</b>, firstly, the control unit <b>11</b> inputs the second learning medical image <b>331</b> of each learning data set <b>33</b> to the input layer of the second classification model <b>6</b> and performs the arithmetic operation of forward propagation of the second classification model <b>6</b>. By performing this arithmetic operation, the control unit <b>11</b> acquires from the output layer an output value corresponding to a result of evaluating the degree of normality of a body part captured in each second learning medical image <b>331</b>. The control unit <b>11</b> calculates an error between the output value acquired from the output layer by the arithmetic operation and the correct label <b>333</b>, with respect to each learning data set <b>33</b>. Similarly to step S<b>121</b> described above, loss function may be used as appropriate to calculate errors.</p><p id="p-0184" num="0183">With the use of the gradients of the calculated errors, the control unit <b>11</b> calculates errors of the respective operational parameters of the second classification model <b>6</b> by backpropagation. The control unit <b>11</b> updates the operational parameters of the second classification model <b>6</b> in accordance with the calculated errors. How much the operational parameters of the second classification model <b>6</b> are updated may be controlled based on the learning rate.</p><p id="p-0185" num="0184">By performing the series of operations of the update operation described above, the control unit <b>11</b> tunes the operational parameters of the second classification model <b>6</b> to decrease the sum of calculated errors. Similarly to step S<b>121</b>, the control unit <b>11</b> may repeat tuning of the operational parameters of the second classification model <b>6</b> by performing the series of operations until the predetermined condition is satisfied. In this manner, the control unit <b>11</b> can train the second classification model <b>6</b> such that, with respect to each learning data set <b>33</b>, in response to an input of the second learning medical image <b>331</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>.</p><p id="p-0186" num="0185">As the result of the supervised learning, the second classification model <b>6</b> acquires a capability of properly evaluating the degree of normality of a body part captured in a fed medical image when this case is related to a case included in the learning data sets <b>33</b>. After the training process of the second classification model <b>6</b> is completed, the control unit <b>11</b> causes the process to proceed to the following step S<b>123</b>.</p><p id="p-0187" num="0186">(Step S<b>123</b>)</p><p id="p-0188" num="0187">In step S<b>123</b>, the control unit <b>11</b> operates as the storage processing unit <b>117</b> and generates information about the result of supervised learning in step S<b>122</b> as the second learning result data <b>123</b>. In the present embodiment, the control unit <b>11</b> generates as the second learning result data <b>123</b> information indicating the architecture and operational parameters of the trained second classification model <b>6</b> created by the supervised learning described above. The control unit <b>11</b> then stores the generated second learning result data <b>123</b> in a predetermined storage area. The predetermined storage area may be a memory such as the RAM in the control unit <b>11</b>, the storage unit <b>12</b>, an external storage device, or a storage medium, or a combination thereof. The memory for the second learning result data <b>123</b> may be identical to or different from the memory for the first learning result data <b>121</b>.</p><p id="p-0189" num="0188">After the storage operation of the second learning result data <b>123</b> is completed, the control unit <b>11</b> ends the procedure of supervised learning of the second classification model <b>6</b> according to this operation example. Similarly to the first learning result data <b>121</b>, the second learning result data <b>123</b> may be provided for the diagnostic assistance apparatus <b>2</b> at a given timing. The operations in steps S<b>121</b> to S<b>123</b> may be regularly or irregularly repeated. While the operations are repeated, at least a portion of the collection of the learning data sets <b>33</b> may be, for example, changed, corrected, added, or deleted as appropriate. Subsequently, by providing the updated or newly generated second learning result data <b>123</b> for the diagnostic assistance apparatus <b>2</b> in a given manner, the second learning result data <b>123</b> stored in the diagnostic assistance apparatus <b>2</b> may be updated.</p><p id="p-0190" num="0189">&#x3c;Connector Parameter Tuning&#x3e;</p><p id="p-0191" num="0190"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating an example of a procedure of parameter tuning by the model generation apparatus <b>1</b> according to the present embodiment. The information processing method of the model generation apparatus <b>1</b> including the procedure of data enlargement processing, the procedure of unsupervised learning of the first classification model <b>5</b>, the procedure of supervised learning of the second classification model <b>6</b>, and the following procedure of parameter tuning is an example of a &#x201c;model generation method&#x201d;. The following procedure regarding parameter tuning is a mere example, and individual steps may be changed when possible. Furthermore, in the following procedure regarding parameter tuning, for example, a particular step may be excluded, replaced, or added as appropriate to the embodiment.</p><p id="p-0192" num="0191">(Step S<b>131</b>)</p><p id="p-0193" num="0192">In step S<b>131</b>, the control unit <b>11</b> operates as the third acquisition unit <b>114</b> and acquires the tuning data sets <b>35</b>.</p><p id="p-0194" num="0193">At least a portion of the collection of the third learning medical images <b>351</b> of the tuning data sets <b>35</b> may be at least either the primary medical image <b>390</b> or the new medical image <b>395</b>. At least a portion of the collection of the third learning medical images <b>351</b> of the tuning data sets <b>35</b> may be acquired as a medical image other than the primary medical image <b>390</b> and the new medical image <b>395</b>. In this case, the tuning data set <b>35</b> may be generated in the same manner as the primary medical image <b>390</b>. Additionally, a portion of the collection of the third learning medical images <b>351</b> of the tuning data sets <b>35</b> may be the same as the first learning medical images <b>31</b> or the second learning medical images <b>331</b>. The third learning medical images <b>351</b> of the tuning data sets <b>35</b> may include one or a plurality of limit samples <b>352</b>. The limit samples <b>352</b> may be selected as appropriate. The limit samples <b>352</b> may be selected, for example, manually by the operator.</p><p id="p-0195" num="0194">The number of the tuning data sets <b>35</b> to be acquired may be determined as appropriate to the embodiment. After the tuning data sets <b>35</b> are acquired, the control unit <b>11</b> causes the process to proceed to the following step S<b>132</b>.</p><p id="p-0196" num="0195">(Steps S<b>132</b> and S<b>133</b>)</p><p id="p-0197" num="0196">In step S<b>132</b>, the control unit <b>11</b> operates as the determination unit <b>115</b> and determines whether a body part captured in the third learning medical image <b>351</b> of each tuning data set <b>35</b> is normal, by using the trained first classification model <b>5</b> and the trained second classification model <b>6</b>. In step S<b>133</b>, the control unit <b>11</b> operates as the tuning unit <b>116</b> and tunes the parameters of the connector <b>7</b> to optimize the accuracy of determination by the determination unit <b>115</b> about each third learning medical image <b>351</b>.</p><p id="p-0198" num="0197">In the present embodiment, the control unit <b>11</b> performs in step S<b>132</b> and step S<b>133</b> supervised learning of the connector <b>7</b> using the tuning data sets <b>35</b>. Similarly to step S<b>121</b> described above, the training process of the connector <b>7</b> may be carried out by using, for example, batch gradient descent, stochastic gradient descent, or mini-batch gradient descent. Similarly to the above cases such as the first classification model <b>5</b>, the neural network implementing the connector <b>7</b> targeted for machine learning may be prepared as appropriate. The architecture, initial value of weight on each connection between neurons, and initial value of threshold of each neuron of the connector <b>7</b> may be provided by a template or inputted by the operator. When performing relearning, the control unit <b>11</b> may use learning result data acquired by the previous machine learning tasks to prepare the connector <b>7</b>.</p><p id="p-0199" num="0198">Specifically, in step S<b>132</b>, the control unit <b>11</b> inputs the third learning medical image <b>351</b> of each tuning data set <b>35</b> to the encoder <b>51</b> of the trained first classification model <b>5</b> and performs the arithmetic operation of forward propagation of the encoder <b>51</b> and the one-class classifier <b>55</b> of the trained first classification model <b>5</b>. By performing this arithmetic operation, the control unit <b>11</b> acquires the first result of each third learning medical image <b>351</b> from the trained one-class classifier <b>55</b>. The control unit <b>11</b> also inputs the third learning medical image <b>351</b> of each tuning data set <b>35</b> to the trained second classification model <b>6</b> and performs the arithmetic operation of forward propagation of the second classification model <b>6</b>. By performing this arithmetic operation, the control unit <b>11</b> acquires the second result of each third learning medical image <b>351</b> from the trained second classification model <b>6</b>. The control unit <b>11</b> subsequently inputs the acquired first and second results to the input layer <b>71</b> of the connector <b>7</b> and performs the arithmetic operation of forward propagation of the connector <b>7</b>. By performing this arithmetic operation, the control unit <b>11</b> acquires from the output layer <b>72</b> a determination value for deriving a determination result by comparing the determination value to a threshold or an output value directly representing the determination result, with respect to each third learning medical image <b>351</b>. The threshold may be given as appropriate.</p><p id="p-0200" num="0199">Subsequently, in step S<b>133</b>, the control unit <b>11</b> calculates an error between a determination result derived from a determination value or represented by an output value and a correct result indicated by the correct label <b>353</b> with respect to each tuning data set <b>35</b>. Similarly to the steps including step S<b>121</b> described above, loss function may be used as appropriate to calculate errors. With the use of the gradients of the calculated errors, the control unit <b>11</b> calculates errors of the respective operational parameters of the connector <b>7</b> by backpropagation. The control unit <b>11</b> updates the operational parameters of the connector <b>7</b> in accordance with the calculated errors. How much the parameters of the connector <b>7</b> are updated may be controlled based on a learning rate.</p><p id="p-0201" num="0200">By performing the series of operations of the update operation described above, the control unit <b>11</b> tunes the parameters of the connector <b>7</b> to decrease the sum of calculated errors. Similarly to the steps including step S<b>121</b>, the control unit <b>11</b> may repeat tuning of the parameters of the connector <b>7</b> by performing the series of operations in steps S<b>132</b> and S<b>133</b> until the predetermined condition is satisfied. Additionally, when the third learning medical images <b>351</b> include one or a plurality of limit samples <b>352</b>, the control unit <b>11</b> may repeat tuning of the operational parameters of the connector <b>7</b> until errors of determination results of the limit samples <b>352</b> are eliminated. As a result, the control unit <b>11</b> can tune the parameters of the connector <b>7</b> to optimize the accuracy of determination about the third learning medical images <b>351</b>. After the tuning of the parameters of the connector <b>7</b> is completed, the control unit <b>11</b> causes the process to proceed to the following step S<b>134</b>.</p><p id="p-0202" num="0201">(Step S<b>134</b>)</p><p id="p-0203" num="0202">In step S<b>134</b>, the control unit <b>11</b> operates as the storage processing unit <b>117</b> and generates information about the result of tuning parameters in step S<b>133</b> as the tuning result data <b>125</b>. In the present embodiment, the control unit <b>11</b> generates as the tuning result data <b>125</b> information indicating the architecture and parameters of the trained connector <b>7</b> created by the supervised learning described above. The control unit <b>11</b> then stores the generated tuning result data <b>125</b> in a predetermined storage area. The predetermined storage area may be a memory such as the RAM in the control unit <b>11</b>, the storage unit <b>12</b>, an external storage device, or a storage medium, or a combination thereof. The memory for the tuning result data <b>125</b> may be identical to at least one of the first learning result data <b>121</b> and the second learning result data <b>123</b>, or different from both the first learning result data <b>121</b> and the second learning result data <b>123</b>.</p><p id="p-0204" num="0203">After the storage operation of the tuning result data <b>125</b> is completed, the control unit <b>11</b> ends the procedure regarding parameter tuning of the connector <b>7</b> according to this operation example. Similarly to the cases including the first learning result data <b>121</b>, the tuning result data <b>125</b> may be provided for the diagnostic assistance apparatus <b>2</b> at a given timing. The operations in steps S<b>131</b> to S<b>134</b> may be regularly or irregularly repeated. While the operations are repeated, at least a portion of the collection of the tuning data sets <b>35</b> may be, for example, changed, corrected, added, or deleted as appropriate. Additionally, at least a portion of the collection of the one or plurality of limit samples <b>352</b> may be, for example, changed, corrected, added, or deleted as appropriate. Subsequently, by providing the updated or newly generated tuning result data <b>125</b> for the diagnostic assistance apparatus <b>2</b> in a given manner, the tuning result data <b>125</b> stored in the diagnostic assistance apparatus <b>2</b> may be updated.</p><p id="p-0205" num="0204">[Diagnostic Assistance Apparatus]</p><p id="p-0206" num="0205"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating an example of a procedure of the diagnostic assistance apparatus <b>2</b> according to the present embodiment. The following procedure of the diagnostic assistance apparatus <b>2</b> is an example of a &#x201c;diagnostic assistance method&#x201d;. The following procedure of the diagnostic assistance apparatus <b>2</b> is a mere example, and individual steps may be changed when possible. Furthermore, in the following procedure of the diagnostic assistance apparatus <b>2</b>, for example, a particular step may be excluded, replaced, or added as appropriate to the embodiment.</p><p id="p-0207" num="0206">(Step S<b>201</b>)</p><p id="p-0208" num="0207">In step S<b>201</b>, the control unit <b>21</b> operates as the data acquisition unit <b>211</b> and acquires the target medical image <b>221</b> capturing a body part of a target examinee. The method for acquiring the target medical image <b>221</b> may be selected as appropriate to the embodiment. For example, the control unit <b>21</b> may acquire the target medical image <b>221</b> directly from an imaging device. Alternatively, for example, the control unit <b>21</b> may acquire the target medical image <b>221</b> from another computer. The number of the target medical images <b>221</b> to be acquired may be determined as appropriate to the embodiment. After the target medical image <b>221</b> is acquired, the control unit <b>21</b> causes the process to proceed to the following step S<b>202</b>. When a plurality of the target medical images <b>221</b> are acquired, the control unit <b>21</b> performs the operation in step S<b>202</b> and the subsequent steps on each target medical image <b>221</b>.</p><p id="p-0209" num="0208">(Step S<b>202</b>)</p><p id="p-0210" num="0209">In step S<b>202</b>, the control unit <b>21</b> operates as the first model computation unit <b>212</b> and acquires the first result of the body part of the target person captured in the acquired target medical image <b>221</b> by using the trained first classification model <b>5</b>. Specifically, the control unit <b>21</b> feeds the acquired target medical image <b>221</b> to the trained first classification model <b>5</b> and performs the arithmetic operation of the trained first classification model <b>5</b>, so that the control unit <b>21</b> acquires an output value corresponding to the first result from the trained first classification model <b>5</b>.</p><p id="p-0211" num="0210">In the present embodiment, the control unit <b>21</b> refers to the first learning result data <b>121</b> and accordingly configures the trained encoder <b>51</b> and the trained one-class classifier <b>55</b>. The control unit <b>21</b> inputs the target medical image <b>221</b> to the input layer <b>511</b> of the trained encoder <b>51</b> and performs the arithmetic operation of forward propagation of the trained encoder <b>51</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires output values corresponding to target features from the output layer <b>513</b>. Subsequently, the control unit <b>21</b> inputs the acquired target features to the input layer <b>551</b> of the trained one-class classifier <b>55</b> and performs the arithmetic operation of forward propagation of the trained one-class classifier <b>55</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires an output value corresponding to the first result from the output layer <b>553</b>. After the first result is acquired, the control unit <b>21</b> causes the process to proceed to the following step S<b>203</b>.</p><p id="p-0212" num="0211">(Step S<b>203</b>)</p><p id="p-0213" num="0212">In step S<b>203</b>, the control unit <b>21</b> operates as the second model computation unit <b>213</b> and acquires the second result of the body part of the target person captured in the acquired target medical image <b>221</b> by using the trained second classification model <b>6</b>. Specifically, the control unit <b>21</b> feeds the acquired target medical image <b>221</b> to the trained second classification model <b>6</b> and performs the arithmetic operation of the trained second classification model <b>6</b>, so that the control unit <b>21</b> acquires an output value corresponding to the second result from the trained second classification model <b>6</b>.</p><p id="p-0214" num="0213">In the present embodiment, the control unit <b>21</b> refers to the second learning result data <b>123</b> and accordingly configures the trained second classification model <b>6</b>. The control unit <b>21</b> inputs the target medical image <b>221</b> to the input layer of the trained second classification model <b>6</b> and performs the arithmetic operation of forward propagation of the trained second classification model <b>6</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires an output value corresponding to the second result from the output layer. After the second result is acquired, the control unit <b>21</b> causes the process to proceed to the following step S<b>204</b>.</p><p id="p-0215" num="0214">It should be noted that the timing of performing the operation in step S<b>203</b> is not limited to this example, and the timing may be changed as appropriate to the embodiment. The operation in step S<b>203</b> may be performed, for example, before step S<b>202</b> or in parallel with step S<b>202</b>.</p><p id="p-0216" num="0215">(Step S<b>204</b>)</p><p id="p-0217" num="0216">In step S<b>204</b>, the control unit <b>21</b> operates as the determination unit <b>214</b> and, in accordance with the first and second results, finally determines whether the body part of the target examinee captured in the target medical image <b>221</b> is normal.</p><p id="p-0218" num="0217">In the present embodiment, the control unit <b>21</b> refers to the tuning result data <b>125</b> and accordingly configures the trained connector <b>7</b>. The control unit <b>21</b> inputs the acquired first and second results to the input layer <b>71</b> of the trained connector <b>7</b> and performs the arithmetic operation of forward propagation of the trained connector <b>7</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires a determination value or an output value corresponding to a determination result from the output layer <b>72</b>. When the output layer <b>72</b> is configured to output a determination value, the control unit <b>21</b> derives a determination result by comparing the acquired determination value to a threshold. The threshold may be given as appropriate. After the determination result is acquired, the control unit <b>21</b> causes the process to proceed to the following step S<b>205</b>.</p><p id="p-0219" num="0218">(Step S<b>205</b>)</p><p id="p-0220" num="0219">In step S<b>205</b>, the control unit <b>21</b> determines which step the process proceeds to in accordance with the determination result in step S<b>204</b>. When in step S<b>204</b> the body part of the target examinee is determined to be non-normal, the control unit <b>21</b> causes the process to proceed to step S<b>206</b>. Conversely, when the body part of the target examinee is determined to be normal, the control unit <b>21</b> omits the operations in steps S<b>206</b> to S<b>208</b> and causes the process to proceed to step S<b>209</b>.</p><p id="p-0221" num="0220">(Step S<b>206</b>)</p><p id="p-0222" num="0221">In step S<b>206</b>, the control unit <b>21</b> operates as the first model computation unit <b>212</b> and generates the target decoded image <b>225</b> from the target features by using the trained decoder <b>53</b>.</p><p id="p-0223" num="0222">In the present embodiment, the control unit <b>21</b> refers to the first learning result data <b>121</b> and accordingly configures the trained encoder <b>51</b> and the trained decoder <b>53</b>. The control unit <b>21</b> inputs the target medical image <b>221</b> to the input layer <b>511</b> of the trained encoder <b>51</b> and performs the arithmetic operation of forward propagation of the trained encoder <b>51</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires output values corresponding to target features from the output layer <b>513</b>. Subsequently, the control unit <b>21</b> inputs the acquired target features to the input layer <b>531</b> of the trained decoder <b>53</b> and performs the arithmetic operation of forward propagation of the trained decoder <b>53</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires output values corresponding to the target decoded image <b>225</b> from the output layer <b>533</b>. When the operation result in step S<b>202</b> is used, the control unit <b>21</b> may omit the operation regarding the encoder <b>51</b> in step S<b>206</b>. After the target decoded image <b>225</b> is acquired, the control unit <b>21</b> causes the process to proceed to the following step S<b>207</b>.</p><p id="p-0224" num="0223">(Steps S<b>207</b> and S<b>208</b>)</p><p id="p-0225" num="0224">In step S<b>207</b>, the control unit <b>21</b> operates as the first model computation unit <b>212</b> and calculates the difference between the target medical image <b>221</b> and the target decoded image <b>225</b> generated. In the subsequent step S<b>208</b>, the control unit <b>21</b> operates as the first model computation unit <b>212</b> and, in accordance with the calculated difference, specifies in the target medical image <b>221</b> a related area by which the body part of the target examinee is determined to be non-normal.</p><p id="p-0226" num="0225">As described above, a portion with high probability of abnormality is not accurately reconstructed in the target decoded image <b>225</b>, and thus, the portion is greatly different from a corresponding portion in the target medical image <b>221</b>. For this reason, the control unit <b>21</b> calculates differences of the corresponding picture elements between the target medical image <b>221</b> and the target decoded image <b>225</b> and compares a difference value of each picture element to a threshold. The threshold may be given as appropriate. By performing this operation, the control unit <b>21</b> can extract as the related area a portion of a picture element having a difference value equal to or greater than the threshold (or exceeding the threshold). After the related area is specified, the control unit <b>21</b> causes the process to proceed to the following step S<b>209</b>.</p><p id="p-0227" num="0226">It should be noted that the operations in steps S<b>206</b> to S<b>209</b> may also be performed when the body part of the target examinee is determined to be normal. In this case, the operation in step S<b>205</b> may be omitted. At least a portion (especially step S<b>206</b>) of the collection of the operations in steps S<b>206</b> to S<b>208</b> may be performed in step S<b>202</b>. In this case, the result of specifying the related area may be disregarded or treated in the same manner as when the body part of the target examinee is determined to be non-normal.</p><p id="p-0228" num="0227">(Step S<b>209</b>)</p><p id="p-0229" num="0228">In step S<b>209</b>, the control unit <b>21</b> operates as the output unit <b>215</b> and outputs information about the result of determining whether the body part of the target examinee is normal.</p><p id="p-0230" num="0229">The output destination and details of the output information may be determined as appropriate to the embodiment. The output destination may be, for example, the output device <b>25</b>, an output device of another computer, the RAM, the storage unit <b>22</b>, a data server, an external storage device, or a storage device of another computer. Concerning the details of the output information, the control unit <b>21</b> may output, for example, result information directly indicating the determination result. Alternatively, for example, the control unit <b>21</b> may perform a predetermined information processing operation in accordance with the determination result; and the control unit <b>21</b> may output information indicating a result of performing the predetermined information processing operation as result information. For example, the control unit <b>21</b> may output a particular message (for example, an alert for indicating a high probability that the body part is non-normal) in accordance with the determination result. The result information may be outputted solely or together with the target medical image <b>221</b>. In step S<b>201</b>, when a plurality of the target medical images <b>221</b> are acquired, the control unit <b>21</b> may display the target medical images <b>221</b> to appear at a glance on a display device. In this case, the control unit <b>21</b> may display the target medical image <b>221</b> determined to be non-normal in a prioritized or separate manner.</p><p id="p-0231" num="0230">Outputting the determination result may include associating result information indicating the determination result with the target medical image <b>221</b>. The data format of the result information may be selected as appropriate to the embodiment. For example, the result information may be implemented by a Digital Imaging and Communications in Medicine (DICOM) tag. Additionally, outputting the determination result may include outputting information indicating the related area specified in step S<b>208</b>. In this case, the related area may be outputted solely or together with the target medical image <b>221</b>.</p><p id="p-0232" num="0231"><figref idref="DRAWINGS">FIGS. <b>12</b>A and <b>12</b>B</figref> schematically illustrate format examples for outputting information indicating a determination result together with the target medical image <b>221</b>. In the example in <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>, the control unit <b>21</b> combines a text indication <b>251</b> indicating a determination result with the target medical image <b>221</b> and outputs the combined image to, for example, the output device <b>25</b>. The contents and position of the text indication <b>251</b> may be determined as appropriate to the embodiment. When the related area is specified, the text indication <b>251</b> may be arranged in an area other than the related area. As indicated by this example, outputting the determination result may include combining information indicating the determination result with the target medical image <b>221</b>. In this case, information indicating the determination result is not necessarily limited to the example of the text indication <b>251</b>. In addition to text, for example, a symbol, mark, or diagram may be used as information indicating the determination result.</p><p id="p-0233" num="0232">In the example in <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>, when outputting the target medical image <b>221</b> to, for example, the output device <b>25</b>, the control unit <b>21</b> changes the color tone (hue, brightness, saturation) of an outer frame <b>252</b> of the target medical image <b>221</b> in accordance with the determination result. This means that the control unit <b>21</b> may output the target medical image <b>221</b> determined to be normal and the target medical image <b>221</b> determined to be non-normal in the manner in which both images are different from each other with respect to the color tone of the outer frame <b>252</b>. The color tone of the outer frame <b>252</b> is an example of visual effect. As represented by this example, the control unit <b>21</b> may indicate the determination result by using a visual effect with the target medical image <b>221</b>. The visual effect is not necessarily limited to the example of the color tone of the outer frame <b>252</b>. In addition to the color tone of the outer frame <b>252</b>, for example, a texture or pattern may be used as the visual effect.</p><p id="p-0234" num="0233"><figref idref="DRAWINGS">FIGS. <b>13</b>A to <b>13</b>C</figref> schematically illustrate format examples for outputting information indicating a related area together with the target medical image <b>221</b>. In the example in <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>, the control unit <b>21</b> displays a box <b>255</b> to indicate a related area on the target medical image <b>221</b>. The box <b>255</b> is an example of information indicating a related area. The shape of the box <b>255</b> may be the same as the shape of the specified related area. Alternatively, a predetermined shape that can include a related area may be used as the shape of the box <b>255</b>. The predetermined shape may be, for example, a rectangle, circle, or oval. The visual effect of the box <b>255</b> may be determined as appropriate to the embodiment.</p><p id="p-0235" num="0234">In the example in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref>, the control unit <b>21</b> displays an emphasized area <b>256</b> to indicate a related area on the target medical image <b>221</b>. The emphasized area <b>256</b> is an example of information indicating a related area. The emphasized area <b>256</b> is formed with a visual effect different from the other area (the area except the emphasized area <b>256</b>). The visual effect may be specified by, for example, a color tone. Similarly to the box <b>255</b>, the shape of the emphasized area <b>256</b> may be the same as the shape of the specified related area; alternatively, the predetermined shape may be used as the shape of the emphasized area <b>256</b>.</p><p id="p-0236" num="0235">In the example in <figref idref="DRAWINGS">FIG. <b>13</b>C</figref>, the control unit <b>21</b> displays a heat map <b>257</b> to indicate a related area on the target medical image <b>221</b>. The heat map <b>257</b> is an example of information indicating a related area. In the heat map <b>257</b>, the greater difference between the target medical image <b>221</b> and the target decoded image <b>225</b> exists at a picture element, the greater value (for example, darker color) indicates the picture element. The heat map <b>257</b> may be generated as appropriate in accordance with the difference value of each picture element.</p><p id="p-0237" num="0236">For ease of description, <figref idref="DRAWINGS">FIGS. <b>12</b>A, <b>12</b>B, and <b>13</b>A to <b>13</b>C</figref> use a chest X-ray image as an example of the target medical image <b>221</b>. This X-ray image is published in Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, Ronald Summers, ChestX-ray8: Hospital-scale Chest X-ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases, IEEE CVPR, pp. 3462-3471, 2017 (Reference URL: https://nihcc.app.box.com/v/ChestXray-NIHCC). The target medical image <b>221</b>, however, is not limited to the example of the drawings. After outputting information about a determination result is completed, the control unit <b>21</b> ends the procedure of the diagnostic assistance apparatus <b>2</b> according to this operation example.</p><p id="p-0238" num="0237">[Characteristics]</p><p id="p-0239" num="0238">As described above, in the present embodiment, by performing the operations in steps S<b>111</b> and S<b>112</b> described above, the model generation apparatus <b>1</b> generates the first classification model <b>5</b> trained by unsupervised learning using the first learning medical images <b>31</b> of normal cases. Along with this, by performing the operations in steps S<b>121</b> and S<b>122</b> described above, the model generation apparatus <b>1</b> generates the second classification model <b>6</b> trained by supervised learning using the learning data sets <b>33</b> including normal cases and abnormal cases. Collecting a large number of the learning data sets <b>33</b> including the second learning medical images <b>331</b> of abnormal cases is difficult and costly, whereas collecting the first learning medical images <b>31</b> of normal cases is relatively inexpensive and not very difficult. For this reason, a relatively small number of the learning data sets <b>33</b> are used in step S<b>122</b>, and as a result, the classification accuracy of the trained second classification model <b>6</b> may be relatively low. By contrast, a large number of the first learning medical images <b>31</b>, which are easily collected with relatively low costs, are used in step S<b>112</b>, and as a result, the classification accuracy of the trained first classification model <b>5</b> can be made high. The trained first classification model <b>5</b> can supplement the classification accuracy of the trained second classification model <b>6</b>. As such, the present embodiment can improve with relatively low costs the accuracy of classifying whether a body part of a target examinee captured in a medical image is normal. By using in steps S<b>201</b> to S<b>204</b> described above the two generated classification models, namely the trained first classification model <b>5</b> and the trained second classification model <b>6</b>, it is expected that the diagnostic assistance apparatus <b>2</b> can highly accurately determine whether a body part of a target examinee captured in the target medical image <b>221</b> is normal.</p><p id="p-0240" num="0239">Furthermore, in the present embodiment, the diagnostic assistance apparatus <b>2</b> includes the connector <b>7</b> configured to connect the first result acquired by the trained first classification model <b>5</b> and the second result acquired by the trained second classification model <b>6</b>. By performing the operations in steps S<b>131</b> to S<b>133</b>, the parameters of the connector <b>7</b> are tuned to optimize the accuracy of determination on the third learning medical images <b>351</b>. This can further improve the accuracy of classifying whether a body part of a target examinee captured in the target medical image <b>221</b> is normal. In the present embodiment, the third learning medical images <b>351</b> may include the limit samples <b>352</b>. With this configuration, improvements in the accuracy of classification on the limit samples <b>352</b> and similar examination targets can be expected. By determining serious cases as the limit samples <b>352</b>, it is possible to reduce the probability of misclassification of the serious cases and similar cases.</p><p id="p-0241" num="0240">Further, in the present embodiment, result information indicating a determination result can be added as an annotation to the target medical image <b>221</b> in step S<b>209</b>. This configuration can increase the convenience of using the target medical image <b>221</b>. Visually displaying the determination result together with the target medical image <b>221</b>, as in, for example, <figref idref="DRAWINGS">FIGS. <b>12</b>A and <b>12</b>B</figref>, can reduce the likelihood that medical doctors fail to discover abnormalities in image interpretation and also enhance the efficiency of image interpretation. Additionally, for example, in accordance with the associated result information, it is possible to extract only the target medical image <b>221</b> capturing a body part determined to be non-normal. With this configuration, the target medical image <b>221</b> capturing a non-normal body part can be displayed with priority on a display device (for example, the output device <b>25</b>), and as a result, loads on medical doctors for image interpretation can be lightened.</p><p id="p-0242" num="0241">Moreover, in the present embodiment, an abnormal area or an area with high probability of abnormality can be extracted as a related area by using the trained encoder <b>51</b> and the trained decoder <b>53</b> generated by the model generation apparatus <b>1</b>. The diagnostic assistance apparatus <b>2</b> can add information indicating the related area as an annotation to the target medical image <b>221</b> in step S<b>209</b>. This configuration can increase the convenience of using the target medical image <b>221</b>. Clarifying the related area on the target medical image <b>221</b>, as in, for example, <figref idref="DRAWINGS">FIGS. <b>13</b>A to <b>13</b>C</figref>, can enhance the efficiency of image interpretation by medical doctors.</p><p id="p-0243" num="0242">To cause a machine learning model to acquire by supervised learning a capability to specify a portion to be annotated, learning data sets need to include labels indicating portions to be annotated in learning medical images. General image interpretation as in health checkups usually includes only visual check of medical images and less likely includes inputting a portion to be annotated. Hence, in this case, costs for collecting learning data sets is further increased by an amount equivalent to the additional task of inputting a label indicating a portion to be annotated. This task additionally requires specialist knowledge, and thus, collecting a large number of learning data sets is significantly difficult. To avoid this problem, instead of using labels indicating potions to be annotated, the present embodiment uses a large number of the first learning medical images <b>31</b>, so that the first classification model <b>5</b> can acquire a capability to specify a related area with relatively high accuracy. As such, the present embodiment can cause the machine learning model to acquire a capability to specify a related area in a medical image with relatively low costs.</p><p id="p-0244" num="0243">Furthermore, in the present embodiment, by performing the operations in steps S<b>101</b> and S<b>102</b>, one or a plurality of new medical images <b>395</b> can be generated from the primary medical image <b>390</b>. The generated new medical image <b>395</b> can be used as each learning medical image (<b>31</b>, <b>331</b>, <b>351</b>). As a result, the present embodiment can easily increase the number of the learning medical images (<b>31</b>, <b>331</b>, <b>351</b>) with little cost, thereby improving the classification accuracy of the trained first classification model <b>5</b> and the trained second classification model <b>6</b> generated.</p><p id="p-0245" num="0244">Further, in the present embodiment, the first classification model <b>5</b> includes the one-class classifier <b>55</b>. The one-class classifier <b>55</b> is implemented by a neural network. The second classification model <b>6</b> is implemented by a convolutional neural network. With these configurations, it is possible to provide the first classification model <b>5</b> and the second classification model <b>6</b> that can properly classify whether a body part of a target examinee is normal.</p><heading id="h-0011" level="1">&#xa7; 4 MODIFICATIONS</heading><p id="p-0246" num="0245">The above has described the embodiment of the present disclosure in detail, but the foregoing description is a mere example of the present disclosure in all respects. As might be expected, various changes and modifications can be made without departing from the range of the present disclosure. For example, the following changes can be made. It should be noted that in the following description the same reference characters are assigned to the same constituent elements as the embodiment, and descriptions of the same points as the embodiment are not repeated. The following modifications can be combined as appropriate.</p><p id="p-0247" num="0246">&#x3c;4.1&#x3e;</p><p id="p-0248" num="0247">In the embodiment, fully connected neural networks are used as the encoder <b>51</b>, the decoder <b>53</b>, the one-class classifier <b>55</b>, and the connector <b>7</b>. A convolutional neural network is used as the second classification model <b>6</b>. However, the type of neural network used as each element is not limited to these examples, and the type of neural network can be selected as appropriate to the embodiment. For example, a convolutional neural network may be used as the encoder <b>51</b>. As another example, a fully connected neural network may be used as the second classification model <b>6</b>.</p><p id="p-0249" num="0248">As the first classification model <b>5</b> and the second classification model <b>6</b>, machine learning models other than neural networks may be used. For example, the encoder <b>51</b> and the decoder <b>53</b> may be implemented by orthogonal projection matrixes using characteristic vectors derived from principal component analysis. In this case, unsupervised learning may include principal component analysis. Further, the one-class classifier <b>55</b> may be implemented by, for example, a one-class support vector machine. For example, when a medical image radically different from the first learning medical images <b>31</b>, such as a medical image capturing an abnormal body part, is fed to the trained encoder <b>51</b>, the values of features acquired from this medical image are basically separated from the values of features acquired from the first learning medical images <b>31</b>. Hence, the one-class classifier <b>55</b> may be constituted by information indicating the distribution and classification boundary (for example, threshold) of features of the first learning medical images <b>31</b> acquired by the trained encoder <b>51</b>. In this case, the one-class classifier <b>55</b> may determine whether the features of the fed medical image are outliers in accordance with the distances between the features of the fed medical image and the features of the first learning medical images <b>31</b>, thereby evaluating the degree of normality of the body part captured in the fed medical image. In a similar manner, the first classification model <b>5</b> may be constituted by information indicating a data group including the first learning medical images <b>31</b> and the classification boundary. In this case, the first classification model <b>5</b> may determine whether the fed medical image is an outlier in accordance with the distances between the fed medical image and the first learning medical images <b>31</b>, thereby evaluating the degree of normality of the body part captured in the fed medical image. Furthermore, the second classification model <b>6</b> may be implemented by, for example, a regression model or support vector machine. The method of each machine learning task may be selected as appropriate in accordance with the configuration of each model.</p><p id="p-0250" num="0249">&#x3c;4.2&#x3e;</p><p id="p-0251" num="0250">In the embodiment, the input and output contents of the first classification model <b>5</b> and the second classification model <b>6</b> are not necessarily limited to the example described above, and the input and output contents may be changed as appropriate to the embodiment. For example, the classification by the second classification model <b>6</b> may include estimation of related area in the target medical image <b>221</b>. This means that the output value of the second classification model <b>6</b> may include information indicating a related area. In this case, the correct label <b>333</b> may include information indicating a related area in the second learning medical image <b>331</b>. Further, for example, at least one of the first classification model <b>5</b> and the second classification model <b>6</b> may be configured to further accept an input of attribute information indicating attributes of a person.</p><p id="p-0252" num="0251"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> schematically illustrates an example of a supervised learning process of the second classification model <b>6</b> configured to further accept an input of attribute information according to this modification. <figref idref="DRAWINGS">FIG. <b>14</b>B</figref> schematically illustrates an example of a parameter tuning process of the connector <b>7</b> according to this modification. <figref idref="DRAWINGS">FIG. <b>14</b>C</figref> schematically illustrates an example of a process of the diagnostic assistance apparatus <b>2</b> according to this modification.</p><p id="p-0253" num="0252">As illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, when a second classification model <b>6</b>A is configured to further accept an input of attribute information, learning data sets <b>33</b>A each further includes learning attribute information <b>339</b> indicating attributes of a subject. The learning attribute information <b>339</b> is attribute information used as training data for supervised learning. Except for this point, each learning data set <b>33</b>A may be configured in the same manner as the learning data set <b>33</b> described above. In step S<b>121</b> described above, the control unit <b>11</b> acquires the plurality of learning data sets <b>33</b>A, each being a combination of the second learning medical image <b>331</b>, the correct label <b>333</b>, and the learning attribute information <b>339</b>.</p><p id="p-0254" num="0253">Except for the configuration in which the second classification model <b>6</b>A further accepts an input of attribute information, the second classification model <b>6</b>A may be configured in the same manner as the second classification model <b>6</b>. The configuration for accepting an input of attribute information may be determined as appropriate to the embodiment. For example, the second classification model <b>6</b>A may be configured such that attribute information is inputted to the fully connected layer <b>63</b>. As another example, the second classification model <b>6</b>A may further include one or more layers arranged in parallel with the convolutional layer <b>61</b> and the pooling layers <b>62</b>. In this case, the second classification model <b>6</b>A may be configured such that attribute information is inputted to a layer on the side closest to input, and the output value of a layer on the side closest to output is inputted to the fully connected layer <b>63</b>.</p><p id="p-0255" num="0254">In step S<b>122</b> described above, the second classification model <b>6</b>A is trained by supervised learning additionally using the learning attribute information <b>339</b> such that the second classification model <b>6</b>A evaluates the degree of normality of a body part captured in a fed medical image with reference to the fed attribute information. This means that the control unit <b>11</b> trains the second classification model <b>6</b>A such that, with respect to each learning data set <b>33</b>, in response to inputs of the second learning medical image <b>331</b> and the learning attribute information <b>339</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>. The training method may be the same as the embodiment. In step S<b>123</b>, the control unit <b>11</b> generates information about the trained second classification model <b>6</b>A as second learning result data <b>123</b>A. The second learning result data <b>123</b>A may be the same as the second learning result data <b>123</b>. Similarly to the embodiment, the generated second learning result data <b>123</b>A may be provided for the diagnostic assistance apparatus <b>2</b> in a given method.</p><p id="p-0256" num="0255">Additionally, similarly to the learning data sets <b>33</b>A, tuning data sets <b>35</b>A each further include learning attribute information <b>359</b> indicating attributes of a subject, as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>. The learning attribute information <b>359</b> is the same as the learning attribute information <b>339</b>. Except for this point, each tuning data set <b>35</b>A may be the same as the tuning data set <b>35</b>. In step S<b>131</b> described above, the control unit <b>11</b> acquires the plurality of tuning data sets <b>35</b>A, each being a combination of the third learning medical image <b>351</b>, the correct label <b>353</b>, and the learning attribute information <b>359</b>.</p><p id="p-0257" num="0256">In step S<b>132</b>, similarly to the embodiment, the control unit <b>11</b> feeds the third learning medical image <b>351</b> of each tuning data set <b>35</b>A to the trained first classification model <b>5</b> and performs the arithmetic operation of the trained first classification model <b>5</b>. As a result, the control unit <b>11</b> acquires the first result of each third learning medical image <b>351</b> from the trained first classification model <b>5</b>. The control unit <b>11</b> also feeds the third learning medical image <b>351</b> and the learning attribute information <b>359</b> of each tuning data set <b>35</b>A to the trained second classification model <b>6</b>A and performs the arithmetic operation of the trained second classification model <b>6</b>A. As a result, the control unit <b>11</b> acquires the second result of each third learning medical image <b>351</b> from the trained second classification model <b>6</b>A. The control unit <b>11</b> subsequently feeds the acquired first and second results to the connector <b>7</b> and performs the arithmetic operation of the connector <b>7</b>. As a result, the control unit <b>11</b> acquires from the connector <b>7</b> a determination value or an output value indicating a determination result of each third learning medical image <b>351</b>. In step S<b>133</b>, similarly to the embodiment, the control unit <b>11</b> tunes the parameters of the connector <b>7</b> to reduce errors between a determination result derived from a determination value or represented by an output value and a correct result indicated by the correct label <b>353</b>, calculated on the respective tuning data sets <b>35</b>A. As a result, the control unit <b>11</b> can tune the parameters of the connector <b>7</b> to optimize the accuracy of determination about the third learning medical images <b>351</b> of the tuning data sets <b>35</b>A.</p><p id="p-0258" num="0257">As illustrated in <figref idref="DRAWINGS">FIG. <b>14</b>C</figref>, the diagnostic assistance apparatus <b>2</b> (the second model computation unit <b>213</b>) according to this modification obtains the second learning result data <b>123</b>A, thereby having the trained second classification model <b>6</b>A. By the time when the operation in step S<b>203</b> described above is performed, the control unit <b>21</b> operates as the data acquisition unit <b>211</b> and acquires target attribute information <b>229</b> indicating attributes of a target examinee. This means that in this modification the data acquisition unit <b>211</b> is configured to further acquire the target attribute information <b>229</b>. The target attribute information <b>229</b> is attribute information used for the operation of diagnostic assistance. In step S<b>203</b> described above, the control unit <b>21</b> further feeds the acquired target attribute information <b>229</b> to the trained second classification model <b>6</b>A (specifically, input the target medical image <b>221</b> and the target attribute information <b>229</b> to the trained second classification model <b>6</b>A) and performs an arithmetic operation of the trained second classification model <b>6</b>A. By performing this arithmetic operation, the control unit <b>21</b> acquires an output value corresponding to the second result from the trained second classification model <b>6</b>A. Except for these points, the diagnostic assistance apparatus <b>2</b> according to this modification performs the same process as the embodiment, so that the diagnostic assistance apparatus <b>2</b> according to this modification can determine whether a body part captured in the target medical image <b>221</b> is normal and output the determination result.</p><p id="p-0259" num="0258">With this modification, the determination of whether a body part is normal can be made with additional reference to person's attributes. Thus, further improvement of the determination accuracy can be expected. The attributes relate to some kinds of person's characteristics such as age, sex, height, weight, waist circumference, and chest measurement. The attribute information (learning attribute information, target attribute information) may be acquired as appropriate; for example, the attribute information may be inputted by the operator with the input device. The first classification model <b>5</b> may also be configured to further accept an input of the attribute information. For example, the trained first classification model <b>5</b> may be generated for individual classes based on attributes indicated by the attribute information. In this case, the attribute classification may be carried out by, for example, designating each class by the operator or clustering.</p><p id="p-0260" num="0259">&#x3c;4.3&#x3e;</p><p id="p-0261" num="0260">In the embodiment, the operations in steps S<b>205</b> to S<b>208</b> may be omitted. When the trained decoder <b>53</b> is not used in the diagnostic assistance apparatus <b>2</b>, information about the trained decoder <b>53</b> may be excluded from the first learning result data <b>121</b>.</p><p id="p-0262" num="0261">Furthermore, in the embodiment, the one-class classifier <b>55</b> may be excluded from the configuration of the first classification model <b>5</b>. As described above, a portion with high probability of abnormality is not accurately reconstructed in the target decoded image <b>225</b>, and thus, the portion is greatly different from a corresponding portion in the target medical image <b>221</b>. For this reason, in the above case, the trained first classification model <b>5</b> may use a difference value between the target medical image <b>221</b> and the target decoded image <b>225</b> (for example, the sum of values of differences between corresponding picture elements) to evaluate the degree of normality of a body part of a target examinee.</p><p id="p-0263" num="0262">In the embodiment, the first classification model <b>5</b> includes the encoder <b>51</b>, the decoder <b>53</b>, and the one-class classifier <b>55</b>. The configuration of the first classification model <b>5</b> is, however, not limited to this example when the first classification model <b>5</b> can acquire by unsupervised learning a capability to evaluate the degree of normality of a body part captured in a medical image, and the configuration of the first classification model <b>5</b> may be determined as appropriate to the embodiment.</p><p id="p-0264" num="0263"><figref idref="DRAWINGS">FIG. <b>15</b></figref> schematically illustrates an example of an unsupervised learning process of a first classification model according to this modification. The first classification model according to this modification includes an estimator <b>51</b>B, a generator <b>53</b>B, and a discriminator <b>57</b>. The generator <b>53</b>B is configured to accept an input of noises (latent variables) and output a pseudo-image generated based on the inputted noises. The discriminator <b>57</b> is configured to accept an input of an image and identify the origin of the input image, in other words, discriminate whether the input image is an image of learning data or a pseudo-image generated by the generator <b>53</b>B. The estimator <b>51</b>B is configured to accept an input of a medical image and estimate input values (noises) to be fed to the generator <b>53</b>B so that the generator <b>53</b>B can generate the input medical image; in other words, the estimator <b>51</b>B is configured to estimate inputs (noises) in accordance with an output (a pseudo-image) of the generator <b>53</b>B. The estimator <b>51</b>B corresponds to the encoder <b>51</b>, and the generator <b>53</b>B corresponds to the decoder <b>53</b>. The estimator <b>51</b>B, the generator <b>53</b>B, and the discriminator <b>57</b> each have a plurality of operational parameters. The generator <b>53</b>B and the discriminator <b>57</b> may be implemented by neural networks, similarly to, for example, the encoder <b>51</b>. The estimator <b>51</b>B may be implemented by a neural network or a function expression such as regression model.</p><p id="p-0265" num="0264">In this modification, unsupervised learning of the first classification model is adversarial learning of the generator <b>53</b>B and the discriminator <b>57</b>. Specifically, in step S<b>112</b>, the control unit <b>11</b> acquires a plurality of noises <b>371</b> from a predetermined probability distribution (for example, a Gaussian distribution). The control unit <b>11</b> inputs each acquired noise <b>371</b> to the generator <b>53</b>B and performs an arithmetic operation of the generator <b>53</b>B. As a result, the control unit <b>11</b> acquires a pseudo-medical image <b>373</b> generated based on the noise <b>371</b> from the generator <b>53</b>B. The control unit <b>11</b> inputs each pseudo-medical image <b>373</b> to the discriminator <b>57</b> and performs an arithmetic operation of the discriminator <b>57</b>, and as a result, the control unit <b>11</b> acquires an output value corresponding to the discrimination result of the pseudo-medical image <b>373</b> from the discriminator <b>57</b>. In this case, the correct result is identifying the origin of the medical image as the generator <b>53</b>B (corresponding to &#x201c;false&#x201d; in the drawing). The control unit <b>11</b> calculates an error between the output value acquired from the discriminator <b>57</b> and this correct result. In the same manner, the control unit <b>11</b> inputs each first learning medical image <b>31</b> to the discriminator <b>57</b> and performs an arithmetic operation of the discriminator <b>57</b>, and as a result, the control unit <b>11</b> acquires an output value corresponding to the discrimination result of the first learning medical image <b>31</b> from the discriminator <b>57</b>. In this case, the correct result is identifying the origin of the medical image as the first learning medical images <b>31</b> (corresponding to &#x201c;true&#x201d; in the drawing). The control unit <b>11</b> calculates an error between the output value acquired from the discriminator <b>57</b> and this correct result. While the operational parameters of the generator <b>53</b>B are fixed, the control unit <b>11</b> tunes the operational parameters of the discriminator <b>57</b> by, for example, backpropagation to reduce the sum of errors calculated. As such, the control unit <b>11</b> trains the discriminator <b>57</b> to discriminate whether the generator <b>53</b>B or the first learning medical images <b>31</b> is the origin of an input medical image (in other words, whether the input medical image is the pseudo-medical image <b>373</b> or the first learning medical image <b>31</b>).</p><p id="p-0266" num="0265">The control unit <b>11</b> also trains the generator <b>53</b>B to generate the pseudo-medical image <b>373</b> in the manner in which the pseudo-medical image <b>373</b> degrades the discrimination performance of the discriminator <b>57</b>. Specifically, the control unit <b>11</b> inputs each pseudo-medical image <b>373</b> to the discriminator <b>57</b> and performs an arithmetic operation of the discriminator <b>57</b>, and as a result, the control unit <b>11</b> acquires an output value corresponding to the discrimination result of the pseudo-medical image <b>373</b> from the discriminator <b>57</b>. In the case of training the generator <b>53</b>B, the correct result is wrongly identifying the origin of the medical image as the first learning medical images <b>31</b> (corresponding to &#x201c;true&#x201d; in the drawing). The control unit <b>11</b> calculates an error between the output value acquired from the discriminator <b>57</b> and this correct result. While the operational parameters of the discriminator <b>57</b> are fixed, the control unit <b>11</b> tunes the operational parameters of the generator <b>53</b>B by, for example, backpropagation to reduce the sum of errors calculated. In this manner, the control unit <b>11</b> can train the generator <b>53</b>B to generate the pseudo-medical image <b>373</b> in the manner in which the pseudo-medical image <b>373</b> degrades the discrimination performance of the discriminator <b>57</b>.</p><p id="p-0267" num="0266">The control unit <b>11</b> alternately repeats the training of the discriminator <b>57</b> and the training of the generator <b>53</b>B. How many times the trainings are repeated may be determined as appropriate to the embodiment. As such, the trained generator <b>53</b>B and the trained discriminator <b>57</b> can be generated. The training of the discriminator <b>57</b> can impart to the discriminator <b>57</b> a capability to identify the origin of an input medical image, depending on the performance of the generator <b>53</b>B. The training of the generator <b>53</b>B can impart to the generator <b>53</b>B a capability to generate the pseudo-medical image <b>373</b> that causes the discriminator <b>57</b> to misidentify the first learning medical image <b>31</b>, depending on the discrimination performance of the discriminator <b>57</b>. As a result, by alternately repeating the training of the discriminator <b>57</b> and the training of the generator <b>53</b>B, the generator <b>53</b>B can acquire a capability to generate the pseudo-medical image <b>373</b> similar to the first learning medical images <b>31</b> (in other words, capturing a normal body part), along with improvements of the discrimination performance of the discriminator <b>57</b>.</p><p id="p-0268" num="0267">Next, the control unit <b>11</b> generates the estimator <b>51</b>B configured to estimate an input from an output of the trained generator <b>53</b>B. The method for generating the estimator <b>51</b>B may be selected as appropriate to the embodiment. For example, the control unit <b>11</b> acquires the plurality of noises <b>371</b> from the predetermined probability distribution. The control unit <b>11</b> generates the pseudo-medical image <b>373</b> from each noise <b>371</b> by using the trained generator <b>53</b>B. As a result, the control unit <b>11</b> generates a plurality of data sets each being a combination of the noise <b>371</b> and the pseudo-medical image <b>373</b>. The control unit <b>11</b> may derive as the estimator <b>51</b>B a function expression for inversely calculating the noise <b>371</b> from the pseudo-medical image <b>373</b> by using a given method.</p><p id="p-0269" num="0268">As another example, when the estimator <b>51</b>B is implemented by a machine learning model such as a neural network, the control unit <b>11</b> may train the estimator <b>51</b>B such that, with respect to each data set, when the estimator <b>51</b>B acquires an estimation result in response to inputting the pseudo-medical image <b>373</b>, the estimation result matches the corresponding noise <b>371</b>. In this manner, it is possible to generate the trained estimator <b>51</b>B capable of properly estimating an input from an output of the trained generator <b>53</b>B.</p><p id="p-0270" num="0269">As still another example, the control unit <b>11</b> may generate the trained estimator <b>51</b>B with the use of at least one (hereinafter simply referred to as a &#x201c;learning medical image&#x201d;) of the first learning medical image <b>31</b> and the second learning medical image <b>331</b>. Specifically, the control unit <b>11</b> inputs a learning medical image to the estimator <b>51</b>B and performs an arithmetic operation of the estimator <b>51</b>B. By performing this arithmetic operation, the control unit <b>11</b> acquires from the estimator <b>51</b>B an output value corresponding to the result of estimating a noise corresponding to the learning medical image. The control unit <b>11</b> subsequently inputs the noise estimated by the estimator <b>51</b>B to the trained generator <b>53</b>B and performs the arithmetic operation of the trained generator <b>53</b>B. By performing this arithmetic operation, the control unit <b>11</b> acquires from the trained generator <b>53</b>B a pseudo-medical image generated from the estimated noise. The control unit <b>11</b> then calculates errors (reconstruction errors) between the generated pseudo-medical image and the learning medical image. At this time, the control unit <b>11</b> may input the learning medical image to the trained discriminator <b>57</b> and perform the arithmetic operation of the trained discriminator <b>57</b>; and the control unit <b>11</b> may further calculate an error of the discrimination result acquired from the trained discriminator <b>57</b>. The control unit <b>11</b> tunes the operational parameters of the estimator <b>51</b>B by, for example, backpropagation to decrease the sum of errors calculated. As a result, the control unit <b>11</b> trains the estimator <b>51</b>B to minimize the difference between a learning medical image and a pseudo-medical image generated by the trained generator <b>53</b>B from an estimation value estimated by the estimator <b>51</b>B on the learning medical image and also minimize the error of the discrimination result acquired by the trained discriminator <b>57</b> on the learning medical image. As the result of this training process, it is possible to generate the trained estimator <b>51</b>B capable of properly estimating an input from an output of the trained generator <b>53</b>B.</p><p id="p-0271" num="0270">In the manner described above, the trained first classification model can be generated. In this modification, the diagnostic assistance apparatus <b>2</b> uses the trained estimator <b>51</b>B, the trained generator <b>53</b>B, and the trained discriminator <b>57</b> to evaluate the degree of normality of a body part of a target examinee captured in the target medical image <b>221</b>. Specifically, in step S<b>202</b>, the control unit <b>21</b> inputs the target medical image <b>221</b> to the trained estimator <b>51</b>B and performs the arithmetic operation of the trained estimator <b>51</b>B. By performing this arithmetic operation, the control unit <b>21</b> acquires from the trained estimator <b>51</b>B the result of estimating a noise corresponding to the target medical image <b>221</b>. The control unit <b>21</b> subsequently inputs the estimated noise to the trained generator <b>53</b>B and performs the arithmetic operation of the trained generator <b>53</b>B. By performing this arithmetic operation, the control unit <b>21</b> acquires from the trained generator <b>53</b>B the result of generating a pseudo-medical image corresponding to the target medical image <b>221</b>. The control unit <b>21</b> also inputs the target medical image <b>221</b> to the trained discriminator <b>57</b> and performs the arithmetic operation of the trained discriminator <b>57</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires a discrimination result on the target medical image <b>221</b>.</p><p id="p-0272" num="0271">Because unsupervised learning of the first classification model uses the first learning medical images <b>31</b> capturing normal body parts, the higher the possibility in which the body part of the target examinee captured in the target medical image <b>221</b> is normal is, the more similar to the target medical image <b>221</b> the pseudo-medical image generated by the trained generator <b>53</b>B is; in other words, the smaller the difference value between the pseudo-medical image and the target medical image <b>221</b> is. Furthermore, the higher the possibility in which the body part of the target examinee captured in the target medical image <b>221</b> is normal is, the higher the possibility in which the trained discriminator <b>57</b> identifies the first learning medical image <b>31</b> as the origin of the target medical image <b>221</b> is. As a result, the control unit <b>21</b> can evaluate the degree of normality of the body part of the target examinee captured in the target medical image <b>221</b> in accordance with the difference value between the pseudo-medical image and the target medical image <b>221</b> and the output value (discrimination result) of the trained discriminator <b>57</b>. With this modification, similarly to the embodiment, it is possible to provide the first classification model that can evaluate the degree of normality of a body part captured in a fed medical image by one-class classification.</p><p id="p-0273" num="0272">It should be noted that this modification can perform the classification operation with the use of only either one of the two indicators described above. Thus, either one of the two indicators may be omitted. When the discrimination result by the trained discriminator <b>57</b> is omitted, the trained first classification model may be constituted by the trained estimator <b>51</b>B and the generator <b>53</b>B. This means that the discriminator <b>57</b> may be used only to train the generator <b>53</b>B and may be excluded from the configuration of the first classification model. When the difference value between the pseudo-medical image and the target medical image <b>221</b> is omitted, the trained first classification model may be constituted by only the trained discriminator <b>57</b>. This means that the trained discriminator <b>57</b> may be used as a one-class classifier.</p><p id="p-0274" num="0273">&#x3c;4.4&#x3e;</p><p id="p-0275" num="0274">In the embodiment, the connector <b>7</b> is implemented by a neural network. The configuration of the connector <b>7</b>, however, is not necessarily limited to this example. The connector <b>7</b> may be implemented by, for example, a simple function expression including the first parameter and the second parameter. Connecting the weighted first result and the weighted second result may be simply summing or averaging the weighted first result and the weighted second result. In the embodiment, when tuning the first parameter and the second parameter, the control unit <b>11</b> may tune the thresholds as appropriate.</p><p id="p-0276" num="0275">In the embodiment, the parameters of the connector <b>7</b> are tuned by machine learning. The method for tuning the parameters of the connector <b>7</b>, however, is not necessarily limited to this example. For example, the first parameter and the second parameter may be optimized by a known optimization method such as differential evolution or Bayesian optimization. As another example, the model generation apparatus <b>1</b> may assign suitable candidate values to the first parameter and the second parameter and perform the determination operation on the third learning medical images <b>351</b>. The model generation apparatus <b>1</b> may accordingly accept particular candidate values with the highest determination accuracy on the third learning medical images <b>351</b> as the parameters. In this case, the candidate values to be assigned may be predetermined. Alternatively, the candidate values to be assigned may be determined mechanically in, for example, a random manner. As another example, at least one of the first parameter, the second parameter, and the thresholds may be specified by inputs by the operator. The operator may be, for example, a medical doctor or user who operates the model generation apparatus <b>1</b> or the diagnostic assistance apparatus <b>2</b> directly or indirectly via a user terminal. The operator may tune the first parameter, the second parameter, and the thresholds as appropriate while checking the determination result of the third learning medical images <b>351</b>. In the case of this example, the first parameter, the second parameter, and the thresholds can be easily tuned.</p><p id="p-0277" num="0276">Further, in the embodiment, the diagnostic assistance apparatus <b>2</b> may tune the parameters of the connector <b>7</b>. In this case, the third acquisition unit <b>114</b>, the determination unit <b>115</b>, and the tuning unit <b>116</b> may be excluded from the software configuration of the model generation apparatus <b>1</b>. The software configuration of the diagnostic assistance apparatus <b>2</b> may further include the third acquisition unit <b>114</b> and the tuning unit <b>116</b>. The first model computation unit <b>212</b>, the second model computation unit <b>213</b>, and the determination unit <b>214</b> may be configured to perform the same operations as the determination unit <b>115</b>. As such, the diagnostic assistance apparatus <b>2</b> may be configured to perform the operations in steps S<b>131</b> to S<b>134</b>.</p><p id="p-0278" num="0277">Moreover, in the embodiment, the connector <b>7</b> may be excluded. In this case, the third acquisition unit <b>114</b>, the determination unit <b>115</b>, and the tuning unit <b>116</b> may be excluded from the software configuration of the model generation apparatus <b>1</b>. Steps S<b>131</b> to S<b>134</b> may be excluded from the procedure of the model generation apparatus <b>1</b>. In step S<b>204</b>, the control unit <b>21</b> may determine whether a body part of a target examinee is normal, as appropriate in accordance with the first and second results. For example, when at least one of the first result and the second result is evaluated as abnormal in view of avoiding unnoticed abnormalities, the control unit <b>21</b> may determine that the body part of the target examinee is non-normal. As another example, only when both the first result and the second result are evaluated as abnormal, the control unit <b>21</b> may determine that the body part of the target examinee is non-normal. As still another example, the control unit <b>21</b> may connect the first result and the second result without weighting and compare the acquired determination value to a threshold to determine whether the body part of the target examinee is normal.</p><p id="p-0279" num="0278">&#x3c;4.5&#x3e;</p><p id="p-0280" num="0279">In the embodiment, the first classification model <b>5</b> and the second classification model <b>6</b> are arranged in parallel. The configuration of the first classification model <b>5</b> and the second classification model <b>6</b>, however, is not necessarily limited to this example. For example, the second classification model <b>6</b> may be configured to accept inputs of a medical image and a result of evaluating the medical image by the first classification model <b>5</b> and evaluate the degree of normality of a body part captured in the input medical image. In this case, in step S<b>122</b> described above, the control unit <b>11</b> may perform supervised learning of the second classification model <b>6</b> by using the acquired learning data sets <b>33</b> and the trained first classification model <b>5</b>. The supervised learning may include training the second classification model such that, with respect to each learning data set <b>33</b>, in response to an input of the second learning medical image <b>331</b> and an input of the result of evaluating the second learning medical image <b>331</b> by the first classification model <b>5</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>. The connector <b>7</b> may be excluded. In this case, instead of steps S<b>202</b> to S<b>204</b>, the control unit <b>21</b> of the diagnostic assistance apparatus <b>2</b> may operate as the determination unit <b>214</b> and determine whether a body part of a target examinee captured in the acquired target medical image <b>221</b> is normal, by using the trained first classification model <b>5</b> and the trained second classification model <b>6</b>. The determination unit <b>214</b> may include the first model computation unit <b>212</b> and the second model computation unit <b>213</b>. Specifically, the control unit <b>21</b> inputs the target medical image <b>221</b> to the trained first classification model <b>5</b> and performs the arithmetic operation of the trained first classification model <b>5</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires an evaluation result of the target medical image <b>221</b> by the trained first classification model <b>5</b>. The control unit <b>21</b> inputs the target medical image <b>221</b> and the evaluation result by the first classification model <b>5</b> to the trained second classification model <b>6</b> and performs the arithmetic operation of the trained second classification model <b>6</b>. By performing this arithmetic operation, the control unit <b>21</b> can acquire from the trained second classification model <b>6</b> a result of evaluating the degree of normality of a body part of a target examinee captured in the target medical image <b>221</b> (in this case, determining whether the body part is normal).</p><p id="p-0281" num="0280"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> schematically illustrates an example of an unsupervised learning process of a first classification model according to this modification. <figref idref="DRAWINGS">FIG. <b>16</b>B</figref> schematically illustrates an example of an unsupervised learning process of a second classification model <b>6</b>C according to this modification. <figref idref="DRAWINGS">FIG. <b>16</b>C</figref> schematically illustrates an example of a process of the diagnostic assistance apparatus <b>2</b> according to this modification.</p><p id="p-0282" num="0281">As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>, the first classification model according to this modification includes, similarly to the modification in <figref idref="DRAWINGS">FIG. <b>15</b></figref> described above, a generator <b>53</b>C and a discriminator <b>59</b>. The generator <b>53</b>C is configured to generate a pseudo-medical image <b>383</b> from a noise (latent variable) <b>381</b>. The discriminator <b>59</b> is configured to accept an input of a medical image and identify the origin of the input medical image. The generator <b>53</b>C may be configured in the same manner as the generator <b>53</b>B, and the discriminator <b>59</b> may be configured in the same manner as the discriminator <b>57</b>.</p><p id="p-0283" num="0282">In step S<b>112</b> described above, training the first classification model includes alternately repeating a first step of training the discriminator <b>59</b> to discriminate whether the generator <b>53</b>C or the first learning medical images <b>31</b> is the origin of an input medical image and a second step of training the generator <b>53</b>C to generate the pseudo-medical image <b>383</b> in the manner in which the pseudo-medical image <b>383</b> degrades the discrimination performance of the discriminator <b>59</b>. The training steps may be the same as in the modification in <figref idref="DRAWINGS">FIG. <b>15</b></figref> described above. In this modification, the classification result by the first classification model is made based on the difference (difference value) between an input medical image and a pseudo-medical image generated by the generator <b>53</b>C, and the discrimination result of the input medical image by the discriminator <b>59</b>.</p><p id="p-0284" num="0283">As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>B</figref>, the first classification model according to this modification further includes an estimator <b>51</b>C configured to accept an input of a medical image and estimate an input value (noise) having been fed to the generator <b>53</b>C to generate the input medical image by the generator <b>53</b>C. The estimator <b>51</b>C may be configured in the same manner as the estimator <b>51</b>B. The estimator <b>51</b>C may also be generated in the same manner as the estimator <b>51</b>B. In this modification, the control unit <b>11</b> inputs the second learning medical image <b>331</b> of each learning data set <b>33</b>C to the estimator <b>51</b>C and performs an arithmetic operation of the estimator <b>51</b>C. The learning data set <b>33</b>C is the same as the learning data set <b>33</b>A. By performing this arithmetic operation, the control unit <b>11</b> acquires from the estimator <b>51</b>C an output value (an estimation value <b>382</b>) corresponding to the result of estimating a noise corresponding to each second learning medical image <b>331</b>. The control unit <b>11</b> subsequently inputs the estimation value <b>382</b> acquired by the estimator <b>51</b>C to the trained generator <b>53</b>C and performs an arithmetic operation of the trained generator <b>53</b>C. By performing this arithmetic operation, the control unit <b>11</b> acquires from the trained generator <b>53</b><i>c </i>a pseudo-medical image <b>384</b> generated from the estimation value <b>382</b> of noise. The control unit <b>11</b> then calculates an error (a difference value <b>387</b>) between the pseudo-medical image <b>384</b> generated and the corresponding second learning medical image <b>331</b>. The control unit <b>11</b> also inputs the second learning medical image <b>331</b> of each learning data set <b>33</b>C to the trained discriminator <b>59</b> and performs an arithmetic operation of the trained discriminator <b>59</b>. By performing this arithmetic operation, the control unit <b>11</b> acquires from the trained discriminator <b>59</b> an output value <b>385</b> corresponding to a discrimination result of each second learning medical image. The control unit <b>11</b> tunes the operational parameters of the estimator <b>51</b>C to decrease the sum of errors between the difference value <b>387</b> and the output value <b>385</b>. As such, the control unit <b>11</b> trains the estimator <b>51</b>C to, with respect to each learning data set <b>33</b>C, minimize the difference between the second learning medical image <b>331</b> and the pseudo-medical image <b>384</b> and also minimize the error of the discrimination result on the second learning medical image <b>331</b> by the trained discriminator <b>59</b>. As the result of this training process, it is possible to generate the trained estimator <b>51</b>C capable of properly estimating an input from an output of the trained generator <b>53</b>C.</p><p id="p-0285" num="0284">A second classification model <b>6</b>C according to this modification has a convolutional layer <b>66</b>, pooling layers <b>67</b>, and fully connected layers (<b>68</b>, <b>69</b>). The convolutional layer <b>66</b>, the pooling layers <b>67</b>, and the fully connected layers (<b>68</b>, <b>69</b>) may be configured in the same manner as the convolutional layer <b>61</b>, the pooling layers <b>62</b>, and the fully connected layers (<b>63</b>, <b>64</b>) of the second classification model <b>6</b>. In this modification, the difference value <b>387</b>, the output value <b>385</b>, and attribute information are inputted to the fully connected layer <b>68</b>. This means that in this modification the fully connected layer <b>68</b> is an input layer for the difference value <b>387</b>, the output value <b>385</b>, and attribute information. The configuration of the second classification model <b>6</b>C, however, is not necessarily limited to this example. Similarly to the above modification, the second classification model <b>6</b>C may further include one or more layers arranged in parallel with the convolutional layer <b>66</b> and the pooling layers <b>67</b>. In this case, at least any of the difference value <b>387</b>, the output value <b>385</b>, and attribute information may be inputted to a layer on the side closest to input.</p><p id="p-0286" num="0285">In step S<b>122</b>, the control unit <b>11</b> calculates the difference value <b>387</b> and the output value <b>385</b> with respect to each learning data set <b>33</b>C, similarly to the training process for the estimator <b>51</b>C. The control unit <b>11</b> subsequently inputs the second learning medical image <b>331</b> of each learning data set <b>33</b>C, the learning attribute information <b>339</b>, the difference value <b>387</b>, and the output value <b>385</b> to the corresponding input layers of the second classification model <b>6</b>C and performs an arithmetic operation of the second classification model <b>6</b>C. As a result, the control unit <b>11</b> acquires an output value corresponding to a classification result of each second learning medical image <b>331</b> by the second classification model <b>6</b>C from the output layer. The control unit <b>11</b> then tunes the operational parameters of the second classification model <b>6</b>C to decrease the sum of calculated errors. In this manner, the control unit <b>11</b> trains the second classification model <b>6</b>C such that, with respect to each learning data set <b>33</b>C, in response to inputs of the second learning medical image <b>331</b>, the learning attribute information <b>339</b>, the difference value <b>387</b>, and the output value <b>385</b>, when evaluating the degree of normality of a body part captured in the inputted second learning medical image <b>331</b>, the evaluation result matches the correct label <b>333</b> corresponding to the second learning medical image <b>331</b>. As a result, the trained second classification model <b>6</b>C can be generated.</p><p id="p-0287" num="0286">As illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>C</figref>, the control unit <b>21</b> of the diagnostic assistance apparatus <b>2</b> according to this modification, instead of steps S<b>202</b> to S<b>204</b>, determines whether a body part of a target examinee captured in the acquired target medical image <b>221</b> is normal, by using the trained first classification model and the trained second classification model <b>6</b>C. Specifically, the control unit <b>21</b> inputs the acquired target medical image <b>221</b> to the trained estimator <b>51</b>C and performs the arithmetic operation of the trained estimator <b>51</b>C. By performing this arithmetic operation, the control unit <b>21</b> acquires the estimation value <b>382</b> of noise from the trained estimator <b>51</b>C. The control unit <b>21</b> inputs the estimation value <b>382</b> of noise to the trained generator <b>53</b>C and performs the arithmetic operation of the trained generator <b>53</b>C. By performing this arithmetic operation, the control unit <b>21</b> acquires from the trained generator <b>53</b><i>c </i>the pseudo-medical image <b>384</b> generated from the estimation value <b>382</b>. The control unit <b>21</b> calculates the difference (the difference value <b>387</b>) between the target medical image <b>221</b> and the pseudo-medical image <b>384</b>. The control unit <b>21</b> also inputs the target medical image <b>221</b> to the trained discriminator <b>59</b> and performs the arithmetic operation of the trained discriminator <b>59</b>. By performing this arithmetic operation, the control unit <b>21</b> acquires from the trained discriminator <b>59</b> an output value <b>385</b> corresponding to a discrimination result of the target medical image <b>221</b>. Similarly to the above modification, the control unit <b>21</b> acquires the target attribute information <b>229</b> indicating attributes of the target examinee as appropriate. The control unit <b>21</b> inputs the target medical image <b>221</b>, the target attribute information <b>229</b>, the difference value <b>387</b>, and the output value <b>385</b> to the corresponding input layers of the trained second classification model <b>6</b>C and performs the arithmetic operation of the trained second classification model <b>6</b>C. By performing this arithmetic operation, the control unit <b>21</b> can acquire an output value corresponding to a result of determining whether the body part of the target examinee captured in the target medical image <b>221</b> is normal, from the output layer (the fully connected layer <b>69</b>) of the trained second classification model <b>6</b>C.</p><p id="p-0288" num="0287">Similarly to the embodiment, this modification can provide the first classification model trained by unsupervised learning and the second classification model trained by supervised learning. This can improve accuracy of classifying medical images with relatively low costs. Furthermore, in this modification, the operations to calculating the difference value <b>387</b> and the output value <b>385</b> can be performed in a common manner between the training process for the estimator <b>51</b>C and the training process for the second classification model <b>6</b>C. Hence, the training process for the estimator <b>51</b>C and the training process for the second classification model <b>6</b>C can be performed simultaneously. This can improve efficiency of machine learning processing.</p><p id="p-0289" num="0288">In this modification, the discrimination result of the input medical image by the discriminator <b>59</b> may be omitted from the evaluation result by the first classification model. In this case, the discriminator <b>59</b> may be used only to train the generator <b>53</b>B and may be excluded from the configuration of the first classification model. Further, attribute information may be omitted from the inputs to the second classification model <b>6</b>C. In this case, the learning attribute information <b>339</b> may be excluded from each learning data set <b>33</b>C. The operations relating to the target attribute information <b>229</b> may be excluded from the procedure of the diagnostic assistance apparatus <b>2</b>. Moreover, the difference value <b>387</b> and the output value <b>385</b> may be inputted to the second classification model <b>6</b>C after summation.</p><p id="p-0290" num="0289">&#x3c;4.6&#x3e;</p><p id="p-0291" num="0290">In the embodiment, the diagnostic assistance system <b>100</b> is formed by the model generation apparatus <b>1</b> and the diagnostic assistance apparatus <b>2</b> that are connected through a network. The configuration of the diagnostic assistance system <b>100</b>, however, is not necessarily limited to this example, and the configuration can be changed as appropriate to the embodiment.</p><p id="p-0292" num="0291"><figref idref="DRAWINGS">FIG. <b>17</b></figref> schematically illustrates an example of a system configuration according to this modification. A system according to this modification includes, as well as the diagnostic assistance system <b>100</b> according to the embodiment, a data server <b>40</b> and a user terminal <b>41</b>. The data server <b>40</b> and the user terminal <b>41</b> may be configured to have processor and memory resources as appropriate, similarly to, for example, the model generation apparatus <b>1</b>. The data server <b>40</b> may be implemented by, for example, a general-purpose server or NAS. The user terminal <b>41</b> may be implemented by, for example, a general-purpose PC, a tablet terminal, or a mobile phone such as a smartphone.</p><p id="p-0293" num="0292">In this modification, the target medical image <b>221</b> and the result information indicating a determination result of the target medical image <b>221</b> by the diagnostic assistance apparatus <b>2</b> are provided for the data server <b>40</b> in a given method. The data server <b>40</b> collects the target medical image <b>221</b> and the result information. Similarly to the embodiment, the result information may be associated or combined with the target medical image <b>221</b>. In this modification, the user such as a medical doctor can access the data server <b>40</b> by using the user terminal <b>41</b> and view the target medical image <b>221</b> and the result information. As in this modification, individual computers may be prepared for these purposes; one computer is used to collect the target medical image <b>221</b> and the result information, and the other to view the target medical image <b>221</b> and the result information. In this modification, the user may directly access the diagnostic assistance apparatus <b>2</b> by using the user terminal <b>41</b> to view the target medical image <b>221</b> and the result information.</p><p id="p-0294" num="0293"><b>1</b> model generation apparatus, <b>11</b> control unit, <b>12</b> storage unit, <b>13</b> communication interface, <b>14</b> input device, output device, <b>16</b> drive, <b>91</b> storage medium, <b>81</b> model generation program, <b>110</b> first acquisition unit, <b>111</b> first learning unit, <b>112</b> second acquisition unit, <b>113</b> second learning unit, <b>114</b> third acquisition unit, <b>115</b> determination unit, <b>116</b> tuning unit, <b>117</b> storage processing unit, <b>118</b> primary data acquisition unit, <b>119</b> enlargement processing unit, <b>121</b> first learning result data, <b>123</b> second learning result data, <b>125</b> tuning result data, <b>2</b> diagnostic assistance apparatus, <b>21</b> control unit, storage unit, <b>23</b> communication interface, <b>24</b> input device, <b>25</b> output device, <b>26</b> drive, <b>92</b> storage medium, <b>82</b> diagnostic assistance program, <b>211</b> data acquisition unit, <b>212</b> first model computation unit, <b>213</b> second model computation unit, <b>214</b> determination unit, <b>215</b> output unit, <b>221</b> target medical image, <b>225</b> target decoded image, <b>251</b> text indication, <b>252</b> outer frame, <b>255</b> box, <b>256</b> emphasized area, <b>257</b> heat map, <b>31</b> first learning medical image, <b>33</b> learning data set, <b>331</b> second learning medical image, <b>333</b> correct label, <b>35</b> tuning data set, <b>351</b> third learning medical image, <b>353</b> correct label, <b>390</b> primary medical image, <b>395</b> new medical image, <b>5</b> first classification model, <b>51</b> encoder, <b>511</b> input layer, <b>512</b> intermediate (hidden) layer, <b>513</b> output layer, <b>53</b> decoder, <b>531</b> input layer, <b>532</b> intermediate (hidden) layer, <b>533</b> output layer, one-class classifier, <b>551</b> input layer, <b>552</b> intermediate (hidden) layer, <b>553</b> output layer, <b>6</b> second classification model, <b>61</b> convolutional layer, <b>62</b> pooling layer, <b>63</b>, <b>64</b> fully connected layer, <b>7</b> connector, <b>71</b> input layer, <b>72</b> output layer</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A diagnostic assistance apparatus comprising:<claim-text>a data acquisition unit configured to acquire a target medical image of a body part of a target examinee;</claim-text><claim-text>at least one processor comprising:<claim-text>a first classification model trained, by unsupervised learning using a plurality of first learning medical images of normal body parts, to provide an evaluation of a degree of normality of the body part in the acquired target medical image by one-class classification; and</claim-text><claim-text>a second classification model trained, by supervised learning using a plurality of learning data sets, each learning data set comprising a combination of a second learning medical image and a correct label indicating whether the body part captured in the second learning medical image is normal, to provide the evaluation of the degree of normality of the body part captured in the acquired target medical image, the second learning medical images of the plurality of learning data sets comprising a normal medical image capturing a normal body part and an abnormal medical image capturing an abnormal body part,</claim-text></claim-text><claim-text>the at least one processor being configured to:<claim-text>by feeding the acquired target medical image to the trained first classification model and performing a first operation of the trained first classification model, acquire as a first result the degree of normality evaluated by the one-class classification on the body part of the target examinee captured in the target medical image;</claim-text><claim-text>by feeding the acquired target medical image to the trained second classification model and performing a second operation of the trained second classification model, acquire as a second result the degree of normality evaluated on the body part of the target examinee captured in the target medical image; and</claim-text><claim-text>in accordance with the first result and the second result, provide a determination of whether the body part of the target; and examinee captured in the target medical image is normal; and</claim-text></claim-text><claim-text>an output device configured to output the determination.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first result and the second result indicate the degree of normality of the body part by a numerical value,</claim-text><claim-text>the at least one processor comprises a connector including a first parameter that determines a first priority level of the first result and a second parameter that determines a second priority level of the second result,</claim-text><claim-text>the at least one processor is configured to provide the determination by:<claim-text>feeding the acquired first result and the acquired second result to the connector,</claim-text><claim-text>weighting the first result and the second result by using the first parameter and the second parameter,</claim-text><claim-text>providing a connection between the weighted first result and the weighted second result,</claim-text><claim-text>comparing the numerical value acquired by the connection to a threshold, and</claim-text><claim-text>determining whether the body part of the target examinee is normal.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the first parameter and the second parameter are tuned to optimize accuracy of the determination on a plurality of third learning medical images,</claim-text><claim-text>wherein body parts captured in the plurality of third learning medical images have been determined to be normal or non-normal.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first parameter, the second parameter, and/or the threshold is specified by an input by an operator.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first classification model includes an encoder that is configured to provide a conversion of the acquired target medical image into a target feature, and a decoder that is configured to decode the acquired target medical image from the target feature, and</claim-text><claim-text>the unsupervised learning includes training the encoder and the decoder such that, when each first learning medical image is fed to the encoder, a decoded image responsively generated by the decoder matches the first learning medical image.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein:<claim-text>the first classification model comprises a one-class classifier that is trained by the unsupervised learning to provide the evaluation by the one-class classification in accordance with the target feature acquired by the encoder, and</claim-text><claim-text>the first operation comprises, by feeding the target feature acquired by the conversion to the trained one-class classifier, acquiring the first result from the trained one-class classifier.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the one-class classifier is implemented by a neural network.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein:<claim-text>the first operation comprises, when the body part of the target examinee is determined to be non-normal:<claim-text>providing, by feeding the acquired target medical image to the trained encoder, the conversion of the target medical image into the target feature,</claim-text><claim-text>generating, by feeding the target feature acquired by the conversion to the trained decoder, a target decoded image from the target feature,</claim-text><claim-text>determining a difference between the target medical image and the generated target decoded image, and</claim-text><claim-text>specifying, in the target medical image, a related area by which the body part of the target examinee is determined to be non-normal, in accordance with the difference, and</claim-text></claim-text><claim-text>the output of a result of the determination includes an output of information indicating the related area specified.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the plurality of learning data sets each further comprise learning attribute information,</claim-text><claim-text>the second classification model is trained, by the supervised learning additionally using the learning attribute information, to provide the evaluation of the degree of normality of the body part captured in the acquired target medical image with reference to fed attribute information,</claim-text><claim-text>the data acquisition unit is further configured to additionally acquire target attribute information indicating an attribute of the target examinee, and</claim-text><claim-text>the at least one processor is further configured to acquire the second result by additionally feeding the acquired target attribute information to the trained second classification model, and performing the second arithmetic operation of the trained second classification model.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the second classification model is implemented by a convolutional neural network.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output comprises associating result information indicating a result of the determination with the target medical image.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the result information is implemented by a Digital Imaging and Communications in Medicine (DICOM) tag.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The diagnostic assistance apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the output comprises combining information indicating a result of the determination with the target medical image.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A model generation apparatus comprising:<claim-text>a first acquisition unit configured to acquire a plurality of first learning medical images of normal body parts;</claim-text><claim-text>a second acquisition unit configured to acquire a plurality of learning data sets, each learning data set comprising a combination of second learning medical images and a correct label indicating whether the body part captured in the second learning medical image is normal, the second learning medical images of the plurality of learning data sets comprising a normal medical image of a normal body part and an abnormal medical image of an abnormal body part;</claim-text><claim-text>a third acquisition unit configured to acquire a plurality of third learning medical images, wherein body parts captured in the plurality of third learning medical images have been determined to be normal or non-normal; and</claim-text><claim-text>at least one processor configured to:<claim-text>perform unsupervised learning of a first classification model by using the plurality of first learning medical images acquired, the first classification model being configured to accept an input medical image and provide an evaluation of a degree of normality of a body part captured in the input medical image by one-class classification, the unsupervised learning comprising training the first classification model such that: when the input medical image belongs to a class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as normal; and when the input medical image does not belong to the class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as non-normal;</claim-text><claim-text>perform supervised learning of a second classification model by using the plurality of learning data sets acquired, the second classification model being configured to accept the input medical image and provide the evaluation of a degree of normality of the body part captured in the input medical image, the supervised learning including training the second classification model such that, with respect to each learning data set, in response to an input of one of the second learning medical images, when providing the evaluation of the degree of normality on the body part captured in the input second learning medical image, a result of the evaluation matches the correct label corresponding to the input second learning medical image;</claim-text><claim-text>by using the trained first classification model and the trained second classification model, provide a determination of whether the body part captured in each third learning medical image acquired is normal;</claim-text><claim-text>by feeding each third learning medical image to the trained first classification model, acquire as a first result the degree of normality evaluated on the body part captured in the third learning medical image by one-class classification;</claim-text><claim-text>by feeding each third learning medical image to the trained second classification model, acquire as a second result the degree of normality evaluated on the body part captured in the third learning medical image, the first result and the second result being configured to indicate the degree of normality of the body part by a numerical value,</claim-text></claim-text><claim-text>the at least one processor comprising a connector including a first parameter that determines a first priority level of the first result and a second parameter that determines a second priority level of the second result, and being further configured to:<claim-text>by feeding the acquired first result and the acquired second result to the connector, weight the first result and the second result by using the first parameter and the second parameter;</claim-text><claim-text>provide a connection between the weighted first result and the weighted second result;</claim-text><claim-text>compare the numerical value acquired by the connection to a threshold,</claim-text><claim-text>provide the determination of whether the body part captured in each third learning medical image is normal; and</claim-text><claim-text>optimize accuracy of the determination on each third learning medical image by tuning the first parameter and the second parameter.</claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The model generation apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the plurality of third learning medical images comprises one or more limit samples, and</claim-text><claim-text>the at least one processor is further configured to tune the first parameter and the second parameter to avoid making the determination incorrect on the one or more limit samples.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The model generation apparatus according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, the at least one processor being further configured to:<claim-text>by subjecting a primary medical image capturing the body part to enlargement processing, generate at least a portion of a collection of the plurality of first learning medical images and the second learning medical images of the plurality of learning data sets.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The model generation apparatus according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the enlargement processing is constituted by parallel translation, rotation, swiveling, flipping or flopping, cropping, contrast change, enlargement, reduction, or any combination thereof, performed on the primary medical image.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A model generation apparatus comprising:<claim-text>a first acquisition unit configured to acquire a plurality of first learning medical images capturing normal body parts;</claim-text><claim-text>a second acquisition unit configured to acquire a plurality of learning data sets, each learning data set comprising a combination of second learning medical images and a correct label indicating whether the body part captured in the second learning medical image is normal, the second learning medical image of the plurality of learning data sets comprising a normal medical image of a normal body part and an abnormal medical image of an abnormal body part; and</claim-text><claim-text>at least one processor configured to:<claim-text>perform unsupervised learning of a first classification model by using the plurality of first learning medical images acquired, the first classification model being configured to accept an input medical image and provide an evaluation of a degree of normality of a body part captured in the input medical image by one-class classification, the unsupervised learning including training the first classification model such that: when the input medical image belongs to a class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as normal; and when the input medical image does not belong to the class of the plurality of first learning medical images, the body part captured in the input medical image is evaluated as non-normal; and</claim-text><claim-text>perform supervised learning of a second classification model by using the plurality of learning data sets acquired and the trained first classification model, the second classification model being configured to accept the input medical image and an input of a result of the evaluation on the medical image by the first classification model and provide the evaluation of the degree of normality of the body part captured in the input medical image, the supervised learning including training the second classification model such that, with respect to each learning data set, in response to an input of the second learning medical image and an input of a result of the evaluation on the second learning medical image by the first classification model, when providing the evaluation of the degree of normality on the body part captured in the input second learning medical image, the result of the evaluation matches the correct label corresponding to the second learning medical image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The model generation apparatus according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein:<claim-text>the first classification model comprises:<claim-text>a generator configured to generate a pseudo-medical image, and</claim-text><claim-text>a discriminator configured to accept the input medical image and identify an origin of the input medical image,</claim-text></claim-text><claim-text>the training of the first classification model includes alternately repeating:<claim-text>a first step of training the discriminator to identify whether the generator or the plurality of first learning medical images is the origin of the input medical image, and</claim-text><claim-text>a second step of training the generator to generate the pseudo-medical image that degrades discrimination performance of the discriminator, and</claim-text></claim-text><claim-text>the result of the evaluation by the first classification model is made based on a difference between the input medical image and the pseudo-medical image generated by the generator.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The model generation apparatus according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein<claim-text>the first classification model further comprises an estimator configured to accept the input medical image and estimate an input value fed to the generator to generate the input medical image by the generator, and</claim-text><claim-text>the at least one processor is further configured to further train the estimator to, with respect to each learning data set, minimize a difference between the second learning medical image and the pseudo-medical image generated by the trained generator from an estimation value estimated by the estimator from the second learning medical image.</claim-text></claim-text></claim></claims></us-patent-application>