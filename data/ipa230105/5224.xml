<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005225A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005225</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17843187</doc-number><date>20220617</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>006</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>014</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>02</class><subclass>B</subclass><main-group>27</main-group><subgroup>0172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Cloud-based Production of High-Quality Virtual And Augmented Reality Video Of User Activities</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/IB2021/050028</doc-number><date>20210105</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17843187</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62959031</doc-number><date>20200109</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Within Unlimited, Inc.</orgname><address><city>Venice</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Milk</last-name><first-name>Chris</first-name><address><city>Los Angeles</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Koblin</last-name><first-name>Aaron</first-name><address><city>Venice</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Within Unlimited, Inc.</orgname><role>02</role><address><city>Venice</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer-implemented method includes obtaining first data, including telemetry data and beatmap synchronization data from a user device such as a virtual reality (VR) headset. The telemetry data relates to actions and/or movements of a person wearing the user device in a real-world environment. The telemetry data and beatmap synchronization data are used to produce one more video segments of a virtual person in a virtual world environment. The video production may take place in the cloud, away from the user device. The video production may include post-production and visual effects. The video production may be higher quality and/or resolution than images displayed in real-time on the user device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="172.97mm" wi="91.36mm" file="US20230005225A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="223.27mm" wi="145.97mm" orientation="landscape" file="US20230005225A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="226.57mm" wi="152.99mm" file="US20230005225A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="175.01mm" wi="96.69mm" file="US20230005225A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="181.10mm" wi="148.76mm" file="US20230005225A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of PCT/IB2021/050028, filed Jan. 9, 2020, which claims the benefit of U.S. patent application No. 62/959,031, filed Jan. 9, 2020, the entire contents of both of which are hereby fully incorporated herein by reference for all purposes.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">COPYRIGHT NOTICE</heading><p id="p-0003" num="0002">A portion of the disclosure of this patent document contains material which is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent file or records, but otherwise reserves all copyright rights whatsoever.</p><heading id="h-0003" level="1">FIELD OF THE INVENTION</heading><p id="p-0004" num="0003">This invention relates generally to virtual reality (VR), and, more particularly, to methods, systems and devices supporting production of video of user interactions in a VR environment.</p><heading id="h-0004" level="1">BACKGROUND</heading><p id="p-0005" num="0004">Virtual and augmented reality devices allow a user to view and interact with virtual environments. A user may, effectively, immerse themselves in a non-real environment and interact with that environment. For example, a user may interact (e.g., play a game) in a virtual environment, where the user's real-world movements are translated to acts in the virtual world. Thus, e.g., a user may simulate tennis play or bike riding or the like in a virtual environment by their real-world movements.</p><p id="p-0006" num="0005">A user may see a view of their virtual environment with a wearable VR/AR device such as a virtual reality (VR) headset or augmented reality (AR) glasses or the like (generally referred to as a head-mounted display (HMD)). A representation of the VR user (e.g., an avatar) may be shown in the virtual environment to correspond to the VR user's location and/or movements.</p><p id="p-0007" num="0006">While interacting in a VR environment, a user may see themselves or a representation (e.g., an avatar) in the VR environment. However, the user (and others) cannot see themselves from the viewpoint or perspective of a third person in the VR environment.</p><p id="p-0008" num="0007">It is desirable, and an object of this invention, to provide users of VR/AR-based activities with videos or other images or the like showing their movements and activities in their VR environment.</p><p id="p-0009" num="0008">It is further desirable and a further object of this invention to provide such videos and/or images from an arbitrary viewpoint or perspective in the VR environment.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0010" num="0009">The present invention is specified in the claims as well as in the below description. Preferred embodiments are particularly specified in the dependent claims and the description of various embodiments.</p><p id="p-0011" num="0010">A system of one or more computers can be configured to perform particular operations or actions by virtue of having software, firmware, hardware, or a combination of them installed on the system that in operation causes or cause the system to perform the actions. One or more computer programs can be configured to perform particular operations or actions by virtue of including instructions that, when executed by data processing apparatus, cause the apparatus to perform the actions.</p><p id="p-0012" num="0011">One general aspect includes a computer-implemented including (a) obtaining first data, said first data including: (i) telemetry data from a user device, the telemetry data relating to actions and/or movements of a person wearing the user device in a real-world environment, wherein the user device comprises a virtual reality (VR) headset, (iii) synchronization data, and (iii) object data relating to one or more virtual objects in a virtual-world environment.</p><p id="p-0013" num="0012">The method also includes (b) analyzing the first data to recognize the actions and/or movements of the person in the real-world environment. The method also includes (c) mapping the actions and/or movements recognized in (b) of the person in the real-world environment to corresponding virtual actions and/or movements of a virtual person in the virtual world environment and relative to said virtual objects in the virtual world environment, the mapping using the synchronization data. The method also includes (d) rendering at least some of the virtual actions and/or movements of the virtual person in the virtual world environment relative to the virtual objects. The method also includes (e) producing one or more video segments of the virtual person in the virtual world environment as rendered in (d). Other embodiments of this aspect include corresponding computer systems, apparatus, and computer programs recorded on one or more computer storage devices, each configured to perform the actions of the methods.</p><p id="p-0014" num="0013">Implementations may include one or more of the following features, alone or in combination(s):<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0014">the method where the one or more video segments may include a video; and/or</li>        <li id="ul0002-0002" num="0015">the method where the one or more video segments are produced from a corresponding one or more arbitrary viewpoints in the virtual world environment; and/or</li>        <li id="ul0002-0003" num="0016">the method where the one or more video segments may include a 360-video; and/or</li>        <li id="ul0002-0004" num="0017">the method where the first data are obtained in (a) while a first view of the virtual world environment is being displayed on a display of the VR headset; and/or</li>        <li id="ul0002-0005" num="0018">the method where the first view is produced at a first resolution, and where the one or more video segments are at a resolution higher than the first resolution; and/or</li>        <li id="ul0002-0006" num="0019">the method where the telemetry data are produced by and sent from the VR headset; and/or</li>        <li id="ul0002-0007" num="0020">the method where the user device also may include at least one VR handheld controller being worn by the person, and where the first data also includes data from the at least one VR handheld controller; and/or</li>        <li id="ul0002-0008" num="0021">the method where the virtual objects may include objects in a game or activity; and/or</li>        <li id="ul0002-0009" num="0022">the method where the virtual objects may include one or more of: targets, portals, and/or gameplay objects; and/or</li>        <li id="ul0002-0010" num="0023">the method where the object data includes data on user interactions with the virtual objects in the virtual-world environment; and/or</li>        <li id="ul0002-0011" num="0024">the method where the object data includes, for at least one object, data on whether the at least one object has been hit by the user in the virtual-world environment; and/or</li>        <li id="ul0002-0012" num="0025">the method where the first data also includes sensor data from at least one sensor being worn by the person, and where movements of the person are recognized in (b) also using the sensor data; and/or</li>        <li id="ul0002-0013" num="0026">the method where the sensor data may include physiological data of the person; and/or</li>        <li id="ul0002-0014" num="0027">the method where acts (a)-(e) are performed on a computer system remote from the user device; and/or</li>        <li id="ul0002-0015" num="0028">the method where the first data were sent to the computer system via a network; and/or</li>        <li id="ul0002-0016" num="0029">the method where the first data were sent to the computer system in real time, while the person was performing the actions and/or movements.</li>    </ul>    </li></ul></p><p id="p-0015" num="0030">Implementations of the described techniques may include hardware, a method or process, or computer software on a computer-accessible medium.</p><p id="p-0016" num="0031">A skilled reader will understand that any method described above or below and/or claimed and described as a sequence of steps or acts is not restrictive in the sense of the order of steps or acts.</p><p id="p-0017" num="0032">Below is a list of method or process embodiments. Those will be indicated with a letter &#x201c;P&#x201d;. Whenever such embodiments are referred to, this will be done by referring to &#x201c;P&#x201d; embodiments.</p><p id="p-0018" num="0000">P<b>1</b>. A computer-implemented method comprising:</p><p id="p-0019" num="0033">(A) obtaining first data, said first data including:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0034">(i) telemetry data from a user device, the telemetry data relating to actions and/or movements of a person wearing the user device in a real-world environment, wherein the user device comprises a virtual reality (VR) headset,</li>        <li id="ul0004-0002" num="0035">(iii) synchronization data, and</li>        <li id="ul0004-0003" num="0036">(iii) object data relating to one or more virtual objects in a virtual-world environment;</li>    </ul>    </li></ul></p><p id="p-0020" num="0037">(B) analyzing the first data to recognize the actions and/or movements of the person in the real-world environment;</p><p id="p-0021" num="0038">(C) mapping the actions and/or movements recognized in (B) of the person in the real-world environment to corresponding virtual actions and/or movements of a virtual person in the virtual world environment and relative to said virtual objects in the virtual world environment, said mapping using said synchronization data;</p><p id="p-0022" num="0039">(D) rendering at least some of the virtual actions and/or movements of the virtual person in the virtual world environment relative to the virtual objects; and</p><p id="p-0023" num="0040">(E) producing one or more video segments of the virtual person in the virtual world environment as rendered in (D).</p><p id="p-0024" num="0000">P<b>2</b>. The method of embodiment P<b>1</b>, wherein the synchronization data correspond to beatmap data that was previously provided to the user device.<br/>P<b>3</b>. The method of embodiments P<b>1</b> or P<b>2</b>, wherein, in (C), the mapping uses the synchronization data to map the actions and/or movements to the beatmap data.<br/>P<b>4</b>. The method of any of the preceding embodiments, wherein the one or more video segments comprise a video.<br/>P<b>5</b>. The method of any of the preceding embodiments, wherein the one or more video segments are produced from a corresponding one or more arbitrary viewpoints in the virtual world environment.<br/>P<b>6</b>. The method of any of the preceding embodiments, wherein the one or more video segments comprise a 360-video.<br/>P<b>7</b>. The method of any of the preceding embodiments, wherein the first data are obtained in (A) while a first view of the virtual world environment is being displayed on a display of the VR headset.<br/>P<b>8</b>. The method of embodiment(s) P<b>7</b>, wherein the first view is produced at a first resolution, and wherein the one or more video segments are at a resolution higher than the first resolution.<br/>P<b>9</b>. The method of any of the preceding embodiments, wherein the telemetry data are produced by and sent from the VR headset.<br/>P<b>10</b>. The method of any of the preceding embodiments, wherein the user device also comprises at least one VR handheld controller being worn by the person, and wherein the first data also includes data from the at least one VR handheld controller.<br/>P<b>11</b>. The method of any of the preceding embodiments, wherein the virtual objects comprise objects in a game or activity.<br/>P<b>12</b>. The method of any of the preceding embodiments, wherein the virtual objects comprise one or more of: targets, portals, and/or gameplay objects.<br/>P<b>13</b>. The method of any of the preceding embodiments, wherein the object data includes data on user interactions with the virtual objects in the virtual-world environment.<br/>P<b>14</b>. The method of any of the preceding embodiments, wherein the object data includes, for at least one object, data on whether the at least one object has been hit by the user in the virtual-world environment.<br/>P<b>15</b>. The method of any of the preceding embodiments, wherein the first data also includes sensor data from at least one sensor being worn by the person, and wherein movements of the person are recognized in (B) also using the sensor data.<br/>P<b>16</b>. The method of embodiment(s) P<b>15</b>, wherein the sensor data comprise physiological data of the person.<br/>P<b>17</b>. The method of any of the preceding embodiments, wherein acts (A)-(E) are performed on a computer system remote from the user device.<br/>P<b>18</b>. The method of any of the preceding embodiments, wherein the first data were sent to the computer system via a network.<br/>P<b>19</b>. The method of any of the preceding embodiments, wherein the first data were sent to the computer system in real time, while the person was performing the actions and/or movements.</p><p id="p-0025" num="0041">Below are device embodiments, indicated with a letter &#x201c;D&#x201d;.</p><p id="p-0026" num="0042">D20. A device, comprising:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0043">(a) hardware including memory and at least one processor, and</li>        <li id="ul0006-0002" num="0044">(b) a service running on the hardware, wherein the service is configured to:</li>    </ul>    </li></ul></p><p id="p-0027" num="0045">perform the method of any of the method embodiments P<b>1</b>-P<b>19</b>.</p><p id="p-0028" num="0046">Below is an article of manufacture embodiment, indicated with a letter &#x201c;M&#x201d;.</p><p id="p-0029" num="0047">M<b>21</b>. An article of manufacture comprising non-transitory computer-readable media having computer-readable instructions stored thereon, the computer readable instructions including instructions for implementing a computer-implemented method, the method operable on a device comprising hardware including memory and at least one processor and running a service on the hardware, the method comprising the method of any one of the preceding method embodiments P<b>1</b>-P<b>19</b>.</p><p id="p-0030" num="0048">Below is computer-readable recording medium embodiment, indicated with a letter &#x201c;R&#x201d;.<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0049">R<b>22</b>. A non-transitory computer-readable recording medium storing one or more programs, which, when executed, cause one or more processors to, at least: perform the method of any one of the preceding method embodiments P<b>1</b>-P<b>19</b>.</li>    </ul>    </li></ul></p><p id="p-0031" num="0050">The above features, along with additional details of the invention, are described further in the examples herein, which are intended to further illustrate the invention but are not intended to limit its scope in any way.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0032" num="0051">Objects, features, and characteristics of the present invention as well as the methods of operation and functions of the related elements of structure, and the combination of parts and economies of manufacture, will become more apparent upon consideration of the following description and the appended claims with reference to the accompanying drawings, all of which form a part of this specification.</p><p id="p-0033" num="0052"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts aspects of a virtual reality system according to exemplary embodiments hereof;</p><p id="p-0034" num="0053"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts aspects of a video production system according to exemplary embodiments hereof;</p><p id="p-0035" num="0054"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts aspects of a mapping and transforming telemetry data according to exemplary embodiments hereof;</p><p id="p-0036" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of exemplary aspects hereof; and</p><p id="p-0037" num="0056"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a logical block diagram depicting aspects of a computer system.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION OF THE PRESENTLY PREFERRED</heading><heading id="h-0008" level="1">Exemplary Embodiments</heading><p id="p-0038" num="0057">Glossary and Abbreviations</p><p id="p-0039" num="0058">As used herein, unless used otherwise, the following terms or abbreviations have the following meanings:</p><p id="p-0040" num="0059">&#x201c;AR&#x201d; means augmented reality.</p><p id="p-0041" num="0060">&#x201c;VR&#x201d; means virtual reality.</p><p id="p-0042" num="0061">A &#x201c;mechanism&#x201d; refers to any device(s), process(es), routine(s), service(s), or combination thereof. A mechanism may be implemented in hardware, software, firmware, using a special-purpose device, or any combination thereof. A mechanism may be integrated into a single device or it may be distributed over multiple devices. The various components of a mechanism may be co-located or distributed. The mechanism may be formed from other mechanisms. In general, as used herein, the term &#x201c;mechanism&#x201d; may thus be considered to be shorthand for the term device(s) and/or process(es) and/or service(s).</p><heading id="h-0009" level="1">DESCRIPTION</heading><p id="p-0043" num="0062">In the following, exemplary embodiments of the invention will be described, referring to the figures. These examples are provided to provide further understanding of the invention, without limiting its scope.</p><p id="p-0044" num="0063">In the following description, a series of features and/or steps are described. The skilled person will appreciate that unless required by the context, the order of features and steps is not critical for the resulting configuration and its effect. Further, it will be apparent to the skilled person that irrespective of the order of features and steps, the presence or absence of time delay between steps, can be present between some or all of the described steps.</p><p id="p-0045" num="0064">It will be appreciated that variations to the foregoing embodiments of the invention can be made while still falling within the scope of the invention. Alternative features serving the same, equivalent, or similar purpose can replace features disclosed in the specification, unless stated otherwise. Thus, unless stated otherwise, each feature disclosed represents one example of a generic series of equivalent or similar features.</p><p id="p-0046" num="0065">A system supporting a real-time virtual reality environment <b>100</b> is described now with reference now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in which a person (VR user) <b>102</b> in a real-world environment or space <b>112</b> uses a VR device or headset <b>104</b> to view and interact with/in a virtual environment. The VR headset <b>104</b> may be connected (wired and/or wirelessly) to a video production system <b>106</b>, e.g., via an access point <b>108</b> (e.g., a Wi-Fi access point or the like). Since the user's activity may include a lot of movement, the VR headset <b>104</b> is preferably wirelessly connected to the access point <b>108</b>. In some cases, the VR headset <b>104</b> may connect to the video production system <b>106</b> via a user device or computer system (not shown). While shown as a separate component, in some embodiments the access point <b>108</b> may be incorporated into the VR headset <b>104</b>.</p><p id="p-0047" num="0066">As used herein, the term &#x201c;activity&#x201d; may include any activity including, without limitation, any exercise or game, yoga, running, cycling, fencing, tennis, meditation, etc. An activity may or may not require movement, sound (e.g., speech), etc. An activity may include movement (or not) of the user's head, arms, hands, legs, feet, etc. The scope hereof is not limited by the kind of activity.</p><p id="p-0048" num="0067">The video production system <b>106</b> may be part of backend/cloud framework <b>107</b>.</p><p id="p-0049" num="0068">Sensors (not shown in the drawings) in the VR headset <b>104</b> and/or other sensors <b>110</b> in the user's environment may track the VR user's actual movements (e.g., head movements, etc.) and other information. The VR headset <b>104</b> preferably provides user tracking without external sensors. In a presently preferred implementation, the VR headset <b>104</b> is an Oculus Quest headset made by Facebook Technologies, LLC.</p><p id="p-0050" num="0069">Tracking or telemetry data from the VR headset <b>104</b> may be provided in real-time (as all or part of data <b>118</b>) to the video production system <b>106</b>.</p><p id="p-0051" num="0070">Similarly, data from the sensor(s) <b>110</b> may also be provided to the video production system <b>106</b> (e.g., via the access point <b>108</b>).</p><p id="p-0052" num="0071">The user <b>102</b> may also have one or two handheld devices <b>114</b>-<b>1</b>, <b>114</b>-<b>2</b> (collectively handheld device(s) and/or controller(s) <b>114</b>) (e.g., Oculus Touch Controllers). Hand movement information from the handheld controller(s) <b>114</b> may be provided with the data <b>118</b> to the video production system <b>106</b> (e.g., via the access point <b>108</b>).</p><p id="p-0053" num="0072">In some embodiments, hand movement information from the handheld controller(s) <b>114</b> may be provided to the VR headset <b>104</b> or to another computing device which may then provide that information to the video production system <b>106</b>. In such cases, the handheld controller(s) <b>114</b> may communicate wirelessly with the VR headset <b>104</b>.</p><p id="p-0054" num="0073">The VR headset <b>104</b> presents the VR user <b>102</b> with a view <b>124</b> corresponding to that VR user's virtual or augmented environment.</p><p id="p-0055" num="0074">Preferably, the view <b>124</b> of the VR user's virtual environment is shown as if seen from the location, perspective, and orientation of the VR user <b>102</b>. The VR user's view <b>124</b> may be provided as a VR view or as an augmented view (e.g., an AR view).</p><p id="p-0056" num="0075">In some embodiments, the user <b>102</b> may perform an activity such as a game or the like in the VR user's virtual environment. The backend/cloud framework <b>107</b> may include an activity system <b>126</b> that may provide game information to the VR headset <b>104</b>. In presently preferred embodiments, the activity system <b>126</b> may provide so-called beat map and/or other information <b>128</b> to the headset (e.g., via the network <b>119</b> and the access point <b>108</b>).</p><p id="p-0057" num="0076">As the user progresses through an activity, the VR headset <b>104</b> may store information about the position and orientation of VR headset <b>104</b> and of the controllers <b>114</b> for the user's left and right hands. In a present implementation, the user's activity (and the beatmap) is divided into sections (e.g., 20 second sections), and the information is collected and stored at a high frequency (e.g., 72 Hz) within a section. The VR headset <b>104</b> may also store information about the location of targets, portals and all gameplay objects that are temporally variant, where they are in space, whether any have been hit, etc. at the same or similar frequency. This collected information allows the video production system to recreate a gameplay scene at any moment in time in the space of that section.</p><p id="p-0058" num="0077">This collected information may then be sent to the video production system <b>106</b>, preferably in the background, as all or part of data <b>118</b>, as the user's activity/workout continues, and several of these sections may be sent to the video production system <b>106</b> over the course of an activity/workout. The data <b>118</b> that are provided to the video production system <b>106</b> preferably include beatmap information.</p><p id="p-0059" num="0078">The Video Production System</p><p id="p-0060" num="0079">With reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the video production system <b>106</b> is a computer system (as discussed below), e.g., one or more servers, with processor(s) <b>202</b>, memory <b>204</b>, communication mechanisms <b>206</b>, etc. One or more video creation programs <b>210</b> run on the video production system <b>106</b>. The video creation programs <b>210</b> may store data in and retrieve data from one or more databases (not shown).</p><p id="p-0061" num="0080">Although only one user <b>102</b> is shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, it should be appreciated that the video production system <b>106</b> may interact with multiple users at the same time. It should also be appreciated that the following description of the operation of the video production system <b>106</b> with one user extends to multiple users.</p><p id="p-0062" num="0081">The video creation programs <b>210</b> of the video production system <b>106</b> may include data collection mechanism(s) <b>212</b>, movement/tracking mechanism(s) <b>214</b>, mapping and transformation mechanism(s) <b>216</b>, and rendering and production mechanism(s) <b>218</b>.</p><p id="p-0063" num="0082">In operation, the data collection mechanism(s) <b>212</b> obtain data <b>118</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) from a user (e.g., user <b>102</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The data <b>118</b> may include at least some of user movement/telemetry data, information about the location of targets, portals and all gameplay objects that are temporally variant, where they are in space, whether any have been hit, etc.</p><p id="p-0064" num="0083">The video production system <b>106</b> may make decisions on whether a given section should be rendered to video based on various criteria (e.g., did the player hit a lot of targets in that capture, etc.).</p><p id="p-0065" num="0084">The movement/tracking mechanism(s) <b>214</b> determines or approximates, from that data, the user's actual movements in the user's real-world space <b>112</b>. The user's movements may be given relative to a 3-D coordinate system <b>116</b> the user's real-world space <b>112</b>. If the data <b>118</b> includes data from the user's handheld controller(s) <b>114</b>, the movement/tracking mechanism(s) <b>214</b> may also determine movement of one or both of the user's hands in the user's real-world space <b>112</b>. In some cases, the user's headset <b>104</b> may provide the user's actual 3-D coordinates in the real-world space <b>112</b>.</p><p id="p-0066" num="0085">The movement/tracking mechanism(s) <b>214</b> may determine or extrapolate aspects of the user's movement based on machine learning (ML) or other models of user movement. For example, a machine learning mechanism may be trained to recognize certain movements and/or types of movements and may then be used to recognize those movements based on the data <b>118</b> provided by the user <b>102</b>.</p><p id="p-0067" num="0086">The movement/tracking mechanism(s) <b>214</b> may use information provided to the video production system <b>106</b> with the data <b>118</b> (e.g., beatmap information) to synchronize the user's movements with beatmap information <b>128</b> that was sent to the user <b>102</b>.</p><p id="p-0068" num="0087">With reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the mapping and transformation mechanism(s) <b>216</b> may take the movement/tracking data (as determined by the movement/tracking mechanism(s) <b>214</b>), and transform those data from the real-world coordinate system <b>116</b> in the user's real-world space <b>112</b> to corresponding 3-D coordinates in a virtual-world coordinate system <b>314</b> in a virtual world <b>312</b>.</p><p id="p-0069" num="0088">Those of skill in the art will understand, upon reading this description, that the mapping and transformation mechanism(s) <b>216</b> may operate prior to or in conjunction with the movement/tracking mechanism(s) <b>214</b>. As with all mechanisms described herein, the logical boundaries are used to aid the description and are not intended to limit the scope hereof.</p><p id="p-0070" num="0089">For the sake of this description, the user's movement data in the real-world space <b>112</b> are referred to as the user's real-world movement data, and the user's movement data in the virtual-world space <b>312</b> are referred to as the user's virtual movement data.</p><p id="p-0071" num="0090">As should be appreciated, the video production system <b>106</b> may use the provided information and improve the representation in various ways. For example, the video production system <b>106</b> may use the user's head and/or hand positions to infer the whole body position of the player.</p><p id="p-0072" num="0091">The rendering and production mechanism(s) <b>218</b> use the user's virtual movement data (produced as described above) and then renders or produces one or more video segments or sequences <b>120</b> of the user (or an avatar or the like corresponding to the user) in the virtual world <b>312</b>. The video sequences may be from any arbitrary viewpoint(s) <b>316</b> in the virtual world <b>312</b>.</p><p id="p-0073" num="0092">In some embodiments, the video production system <b>106</b> may also receive or have other user data (e.g., physiological data or the like) and may use some of the physiological data (e.g., heartrate, temperature, sweat level, breathing rate, etc.) to determine the user's movements and actions in the virtual space.</p><p id="p-0074" num="0093">The one or more video sequences <b>120</b> produced by the rendering and production mechanism(s) <b>218</b> may be provided to the user or to other users. The one or more video sequences <b>120</b> may also be referred to as video sequence(s) <b>120</b> or video <b>122</b>.</p><p id="p-0075" num="0094">As should be appreciated, the compute power that may be used by the video production system <b>106</b> may far exceed that of the headset <b>104</b> or of a computing device that the user may have. Thus, the resolution and overall quality of the video sequence(s) <b>120</b> may exceed that produced by the headset <b>104</b> of the view <b>124</b> corresponding to that VR user's virtual or augmented environment.</p><p id="p-0076" num="0095">As noted, the video sequence(s) <b>120</b> may be from any arbitrary viewpoint <b>316</b> in the virtual world <b>312</b>. Multiple distinct and arbitrary viewpoints <b>316</b> may be used, and, in some embodiments, the video production system <b>106</b> may produce a 360-degree video of the user's actions in the VR user's virtual or augmented environment. Those of skill in the art will understand, upon reading this description, that the more complex the user's virtual or augmented environment, and the more viewpoints included, the longer it will take to produce the video sequence(s) <b>120</b>.</p><p id="p-0077" num="0096">The rendering and production mechanism(s) <b>218</b> may also include various post-processing operations, e.g., to provide visual effects to the rendered video. For example, the rendering and production mechanism(s) <b>218</b> may show some aspects of the rendered video at a different speed (e.g., in slow motion) and/or enhanced.</p><p id="p-0078" num="0097">The video production system <b>106</b> may be co-located with the user (e.g., in the same room), or it may be fully or wholly located elsewhere. For example, the video production system <b>106</b> may be located at a location distinct from the user, in which case the user's data <b>118</b> may be sent to the video production system <b>106</b> via a network <b>119</b> (e.g., the Internet). Although in preferred cases the user's data <b>118</b> are provided to the video production system <b>106</b> as the data are generated (i.e., in real-time), in some cases, the user's data <b>118</b> may be collected and stored at the user's location, and then sent to the video production system <b>106</b>. When located apart from the user, and accessed via a network, the video production system <b>106</b> may be considered to be a cloud-based system.</p><p id="p-0079" num="0098"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of exemplary aspects of the video production system <b>106</b>.</p><p id="p-0080" num="0099">The example described here excludes the setup of the user's device and linking the headset to the user device to the video production system <b>106</b>. These may be done in any suitable manner and may depend on the kind of headset, device, network connections, etc. For the sake of this example description, it may be assumed that the user's headset and other devices are connected to the video production system <b>106</b>.</p><p id="p-0081" num="0100">With reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>4</b></figref>, the user begins their activity, and their headset <b>104</b> (and handheld device(s)/controller(s) <b>114</b>, if used) provide movement data as user data <b>118</b> to the data collection mechanism(s) <b>212</b>. Other data, e.g., from external sensors <b>110</b> and from the user's physiological sensors (not shown) may also be provided as part of the movement/telemetry data <b>118</b>.</p><p id="p-0082" num="0101">While the activity is going on at the user's end, and the user's movement data <b>118</b> are being sent to the video production system <b>106</b>, the data collection mechanism(s) <b>212</b> on the video production system <b>106</b> continuously receive and collect user data <b>118</b> from the user (at <b>402</b>).</p><p id="p-0083" num="0102">The collected data are analyzed (at <b>406</b>, by the movement/tracking mechanism(s) <b>214</b>) to try to recognize, track, and analyze the user's movement. The movement data determined (at <b>404</b>) by the movement/tracking mechanism(s) <b>214</b> are then mapped and/or transformed (at <b>406</b>) by the mapping and transformation mechanism(s) <b>216</b> to map from the coordinate system <b>116</b> of user's real world <b>112</b> to the virtual coordinate system <b>314</b> of the virtual world <b>312</b>. As noted above, the transformation to the virtual coordinate system <b>314</b> may take place as part of or before the recognition and analysis. The system may synchronize with beatmap data (provided to the video production system <b>106</b> as part of user data <b>118</b>).</p><p id="p-0084" num="0103">The user movement data in the virtual coordinate system <b>314</b> are then rendered (at <b>408</b>) by rendering and production mechanism(s) <b>218</b> to produce (at <b>410</b>) a video <b>122</b> comprising one or more video segments or sequences <b>120</b>. The video <b>122</b> may be from one or more arbitrary viewpoints <b>316</b>, or it may be or include a 360-video. The video may use the beatmap data.</p><p id="p-0085" num="0104">In some cases, the system may defer the rendering until the user stops sending data.</p><p id="p-0086" num="0105">While the user is sending movement and other data to the video production system <b>106</b>, the user's headset <b>104</b> is preferably showing the user a view <b>124</b> of the user in the virtual world <b>312</b>. However, as noted above, the view <b>124</b> that the user sees in real-time is from their point of view, and the view <b>124</b> shown in the virtual world will have a lower resolution and possibly less accurate movement that the view(s) produced by the video production system <b>106</b>. Those of skill in the art will understand, upon reading this description, that the increased compute power of the video production system <b>106</b>, means that it can produce higher quality videos. The higher quality video sequences <b>120</b> may have higher video resolution, more accurate movement depiction, and more VR or AR features and/or interactions.</p><p id="p-0087" num="0106">In some cases, the video production system <b>106</b> produces the video sequences <b>120</b> in real-time. In other cases, the video production system <b>106</b> may collect data for subsequent production of the video sequences <b>120</b>.</p><p id="p-0088" num="0107">In some embodiments the video production system <b>106</b> may use inverse kinematics (IK) techniques and an IK character rig to solve the users position and recreate the scene. Those of skill in the art will understand, upon reading this description, that the use of IK may provide a more accurate and/or convincing rendering of the user in the virtual space. It should be appreciated, however, that any representation of the data and the user may be used.</p><p id="p-0089" num="0108">Real Time</p><p id="p-0090" num="0109">Those of ordinary skill in the art will realize and understand, upon reading this description, that, as used herein, the term &#x201c;real time&#x201d; means near real time or sufficiently real time. It should be appreciated that there are inherent delays in electronic components and in network-based communication (e.g., based on network traffic and distances), and these delays may cause delays in data reaching various components. Inherent delays in the system do not change the real time nature of the data. In some cases, the term &#x201c;real time data&#x201d; may refer to data obtained in sufficient time to make the data useful for its intended purpose.</p><p id="p-0091" num="0110">Although the term &#x201c;real time&#x201d; may be used here, it should be appreciated that the system is not limited by this term or by how much time is actually taken. In some cases, real-time computation may refer to an online computation, i.e., a computation that produces its answer(s) as data arrive, and generally keeps up with continuously arriving data. The term &#x201c;online&#x201d; computation is compared to an &#x201c;offline&#x201d; or &#x201c;batch&#x201d; computation.</p><p id="p-0092" num="0111">Computing</p><p id="p-0093" num="0112">The applications, services, mechanisms, operations, and acts shown and described above are implemented, at least in part, by software running on one or more computers.</p><p id="p-0094" num="0113">Programs that implement such methods (as well as other types of data) may be stored and transmitted using a variety of media (e.g., computer readable media) in a number of manners. Hard-wired circuitry or custom hardware may be used in place of, or in combination with, some or all of the software instructions that can implement the processes of various embodiments. Thus, various combinations of hardware and software may be used instead of software only.</p><p id="p-0095" num="0114">One of ordinary skill in the art will readily appreciate and understand, upon reading this description, that the various processes described herein may be implemented by, e.g., appropriately programmed general purpose computers, special purpose computers and computing devices. One or more such computers or computing devices may be referred to as a computer system.</p><p id="p-0096" num="0115"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of a computer system <b>500</b> upon which embodiments of the present disclosure may be implemented and carried out.</p><p id="p-0097" num="0116">According to the present example, the computer system <b>500</b> includes a bus <b>502</b> (i.e., interconnect), one or more processors <b>504</b>, a main memory <b>506</b>, read-only memory <b>508</b>, removable storage media <b>510</b>, mass storage <b>512</b>, and one or more communications ports <b>514</b>. Communication port(s) <b>514</b> may be connected to one or more networks (not shown) by way of which the computer system <b>500</b> may receive and/or transmit data.</p><p id="p-0098" num="0117">As used herein, a &#x201c;processor&#x201d; means one or more microprocessors, central processing units (CPUs), computing devices, microcontrollers, digital signal processors, or like devices or any combination thereof, regardless of their architecture. An apparatus that performs a process can include, e.g., a processor and those devices such as input devices and output devices that are appropriate to perform the process.</p><p id="p-0099" num="0118">Processor(s) <b>504</b> can be any known processor, such as, but not limited to, an Intel&#xae; Itanium&#xae; or Itanium 2&#xae; processor(s), AMD&#xae; Opteron&#xae; or Athlon MP&#xae; processor(s), or Motorola&#xae; lines of processors, and the like. Communications port(s) <b>514</b> can be any of an Ethernet port, a Gigabit port using copper or fiber, or a USB port, and the like. Communications port(s) <b>514</b> may be chosen depending on a network such as a Local Area Network (LAN), a Wide Area Network (WAN), or any network to which the computer system <b>500</b> connects. The computer system <b>500</b> may be in communication with peripheral devices (e.g., display screen <b>516</b>, input device(s) <b>518</b>) via Input/Output (I/O) port <b>520</b>.</p><p id="p-0100" num="0119">Main memory <b>506</b> can be Random Access Memory (RAM), or any other dynamic storage device(s) commonly known in the art. Read-only memory (ROM) <b>508</b> can be any static storage device(s) such as Programmable Read-Only Memory (PROM) chips for storing static information such as instructions for processor(s) <b>504</b>. Mass storage <b>512</b> can be used to store information and instructions. For example, hard disk drives, an optical disc, an array of disks such as Redundant Array of Independent Disks (RAID), or any other mass storage devices may be used.</p><p id="p-0101" num="0120">Bus <b>502</b> communicatively couples processor(s) <b>504</b> with the other memory, storage, and communications blocks. Bus <b>502</b> can be a PCI/PCI-X, SCSI, a Universal Serial Bus (USB) based system bus (or other) depending on the storage devices used, and the like. Removable storage media <b>510</b> can be any kind of external storage, including hard-drives, floppy drives, USB drives, Compact Disc-Read Only Memory (CD-ROM), Compact Disc&#x2014;Re-Writable (CD-RW), Digital Versatile Disk-Read Only Memory (DVD-ROM), etc.</p><p id="p-0102" num="0121">Embodiments herein may be provided as one or more computer program products, which may include a machine-readable medium having stored thereon instructions, which may be used to program a computer (or other electronic devices) to perform a process. As used herein, the term &#x201c;machine-readable medium&#x201d; refers to any medium, a plurality of the same, or a combination of different media, which participate in providing data (e.g., instructions, data structures) which may be read by a computer, a processor or a like device. Such a medium may take many forms, including but not limited to, non-volatile media, volatile media, and transmission media. Non-volatile media include, for example, optical or magnetic disks and other persistent memory. Volatile media include dynamic random-access memory, which typically constitutes the main memory of the computer. Transmission media include coaxial cables, copper wire and fiber optics, including the wires that comprise a system bus coupled to the processor. Transmission media may include or convey acoustic waves, light waves, and electromagnetic emissions, such as those generated during radfrequency (RF) and infrared (IR) data communications.</p><p id="p-0103" num="0122">The machine-readable medium may include, but is not limited to, floppy diskettes, optical discs, CD-ROMs, magneto-optical disks, ROMs, RAMs, erasable programmable read-only memories (EPROMs), electrically erasable programmable read-only memories (EEPROMs), magnetic or optical cards, flash memory, or other type of media/machine-readable medium suitable for storing electronic instructions. Moreover, embodiments herein may also be downloaded as a computer program product, wherein the program may be transferred from a remote computer to a requesting computer by way of data signals embodied in a carrier wave or other propagation medium via a communication link (e.g., modem or network connection).</p><p id="p-0104" num="0123">Various forms of computer readable media may be involved in carrying data (e.g. sequences of instructions) to a processor. For example, data may be (i) delivered from RAM to a processor; (ii) carried over a wireless transmission medium; (iii) formatted and/or transmitted according to numerous formats, standards or protocols; and/or (iv) encrypted in any of a variety of ways well known in the art.</p><p id="p-0105" num="0124">A computer-readable medium can store (in any appropriate format) those program elements which are appropriate to perform the methods.</p><p id="p-0106" num="0125">As shown, main memory <b>506</b> is encoded with application(s) <b>522</b> that support(s) the functionality as discussed herein (the application(s) <b>522</b> may be an application(s) that provides some or all of the functionality of the services/mechanisms described herein, e.g., VR sharing application <b>230</b>, <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Application(s) <b>522</b> (and/or other resources as described herein) can be embodied as software code such as data and/or logic instructions (e.g., code stored in the memory or on another computer readable medium such as a disk) that supports processing functionality according to different embodiments described herein.</p><p id="p-0107" num="0126">During operation of one embodiment, processor(s) <b>504</b> accesses main memory <b>506</b> via the use of bus <b>502</b> in order to launch, run, execute, interpret, or otherwise perform the logic instructions of the application(s) <b>522</b>. Execution of application(s) <b>522</b> produces processing functionality of the service related to the application(s). In other words, the process(es) <b>524</b> represent one or more portions of the application(s) <b>522</b> performing within or upon the processor(s) <b>504</b> in the computer system <b>500</b>.</p><p id="p-0108" num="0127">For example, process(es) <b>524</b> may include an AR application process corresponding to VR sharing application <b>230</b>.</p><p id="p-0109" num="0128">It should be noted that, in addition to the process(es) <b>524</b> that carries(carry) out operations as discussed herein, other embodiments herein include the application(s) <b>522</b> itself (i.e., the un-executed or non-performing logic instructions and/or data). The application(s) <b>522</b> may be stored on a computer readable medium (e.g., a repository) such as a disk or in an optical medium. According to other embodiments, the application(s) <b>522</b> can also be stored in a memory type system such as in firmware, read only memory (ROM), or, as in this example, as executable code within the main memory <b>506</b> (e.g., within Random Access Memory or RAM). For example, application(s) <b>522</b> may also be stored in removable storage media <b>510</b>, read-only memory <b>508</b>, and/or mass storage device <b>512</b>.</p><p id="p-0110" num="0129">Those skilled in the art will understand that the computer system <b>500</b> can include other processes and/or software and hardware components, such as an operating system that controls allocation and use of hardware resources.</p><p id="p-0111" num="0130">As discussed herein, embodiments of the present invention include various steps or acts or operations. A variety of these steps may be performed by hardware components or may be embodied in machine-executable instructions, which may be used to cause a general-purpose or special-purpose processor programmed with the instructions to perform the operations. Alternatively, the steps may be performed by a combination of hardware, software, and/or firmware. The term &#x201c;module&#x201d; refers to a self-contained functional component, which can include hardware, software, firmware, or any combination thereof.</p><p id="p-0112" num="0131">One of ordinary skill in the art will readily appreciate and understand, upon reading this description, that embodiments of an apparatus may include a computer/computing device operable to perform some (but not necessarily all) of the described process.</p><p id="p-0113" num="0132">Embodiments of a computer-readable medium storing a program or data structure include a computer-readable medium storing a program that, when executed, can cause a processor to perform some (but not necessarily all) of the described process.</p><p id="p-0114" num="0133">Where a process is described herein, those of ordinary skill in the art will appreciate that the process may operate without any user intervention. In another embodiment, the process includes some human intervention (e.g., a step is performed by or with the assistance of a human).</p><p id="p-0115" num="0134">Although embodiments hereof are described using an integrated device (e.g., a smartphone), those of ordinary skill in the art will appreciate and understand, upon reading this description, that the approaches described herein may be used on any computing device that includes a display and at least one camera that can capture a real-time video image of a user. For example, the system may be integrated into a heads-up display of a car or the like. In such cases, the rear camera may be omitted.</p><heading id="h-0010" level="1">CONCLUSION</heading><p id="p-0116" num="0135">As used herein, including in the claims, the phrase &#x201c;at least some&#x201d; means &#x201c;one or more,&#x201d; and includes the case of only one. Thus, e.g., the phrase &#x201c;at least some ABCs&#x201d; means &#x201c;one or more ABCs,&#x201d; and includes the case of only one ABC.</p><p id="p-0117" num="0136">The term &#x201c;at least one&#x201d; should be understood as meaning &#x201c;one or more,&#x201d; and therefore includes both embodiments that include one or multiple components. Furthermore, dependent claims that refer to independent claims that describe features with &#x201c;at least one&#x201d; have the same meaning, both when the feature is referred to as &#x201c;the&#x201d; and &#x201c;the at least one.&#x201d;</p><p id="p-0118" num="0137">As used in this description, the term &#x201c;portion&#x201d; means some or all. So, for example, &#x201c;A portion of X&#x201d; may include some of &#x201c;X&#x201d; or all of &#x201c;X.&#x201d; In the context of a conversation, the term &#x201c;portion&#x201d; means some or all of the conversation.</p><p id="p-0119" num="0138">As used herein, including in the claims, the phrase &#x201c;based on&#x201d; means &#x201c;based in part on&#x201d; or &#x201c;based, at least in part, on,&#x201d; and is not exclusive. Thus, e.g., the phrase &#x201c;based on factor X&#x201d; means &#x201c;based in part on factor X&#x201d; or &#x201c;based, at least in part, on factor X.&#x201d; Unless specifically stated by use of the word &#x201c;only,&#x201d; the phrase &#x201c;based on X&#x201d; does not mean &#x201c;based only on X.&#x201d;</p><p id="p-0120" num="0139">As used herein, including in the claims, the phrase &#x201c;using&#x201d; means &#x201c;using at least,&#x201d; and is not exclusive. Thus, e.g., the phrase &#x201c;using X&#x201d; means &#x201c;using at least X.&#x201d; Unless specifically stated by use of the word &#x201c;only,&#x201d; the phrase &#x201c;using X&#x201d; does not mean &#x201c;using only X.&#x201d;</p><p id="p-0121" num="0140">As used herein, including in the claims, the phrase &#x201c;corresponds to&#x201d; means &#x201c;corresponds in part to&#x201d; or &#x201c;corresponds, at least in part, to,&#x201d; and is not exclusive. Thus, e.g., the phrase &#x201c;corresponds to factor X&#x201d; means &#x201c;corresponds in part to factor X&#x201d; or &#x201c;corresponds, at least in part, to factor X.&#x201d; Unless specifically stated by use of the word &#x201c;only,&#x201d; the phrase &#x201c;corresponds to X&#x201d; does not mean &#x201c;corresponds only to X.&#x201d;</p><p id="p-0122" num="0141">In general, as used herein, including in the claims, unless the word &#x201c;only&#x201d; is specifically used in a phrase, it should not be read into that phrase.</p><p id="p-0123" num="0142">As used herein, including in the claims, the phrase &#x201c;distinct&#x201d; means &#x201c;at least partially distinct.&#x201d; Unless specifically stated, distinct does not mean fully distinct. Thus, e.g., the phrase, &#x201c;X is distinct from Y&#x201d; means that &#x201c;X is at least partially distinct from Y,&#x201d; and does not mean that &#x201c;X is fully distinct from Y.&#x201d; Thus, as used herein, including in the claims, the phrase &#x201c;X is distinct from Y&#x201d; means that X differs from Y in at least some way.</p><p id="p-0124" num="0143">It should be appreciated that the words &#x201c;first&#x201d; and &#x201c;second&#x201d; in the description and claims are used to distinguish or identify, and not to show a serial or numerical limitation. Similarly, the use of letter or numerical labels (such as &#x201c;(a),&#x201d; &#x201c;(b),&#x201d; and the like) are used to help distinguish and/or identify, and not to show any serial or numerical limitation or ordering.</p><p id="p-0125" num="0144">No ordering is implied by any of the labeled boxes in any of the flow diagrams unless specifically shown and stated. When disconnected boxes are shown in a diagram the activities associated with those boxes may be performed in any order, including fully or partially in parallel.</p><p id="p-0126" num="0145">As used herein, including in the claims, singular forms of terms are to be construed as also including the plural form and vice versa, unless the context indicates otherwise. Thus, it should be noted that as used herein, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural references unless the context clearly dictates otherwise.</p><p id="p-0127" num="0146">Throughout the description and claims, the terms &#x201c;comprise,&#x201d; &#x201c;including&#x201d;, &#x201c;having&#x201d;, and &#x201c;contain&#x201d; and their variations should be understood as meaning &#x201c;including but not limited to&#x201d; and are not intended to exclude other components.</p><p id="p-0128" num="0147">The present invention also covers the exact terms, features, values and ranges etc. in case these terms, features, values and ranges etc. are used in conjunction with terms such as about, around, generally, substantially, essentially, at least etc. (i.e., &#x201c;about 3&#x201d; shall also cover exactly 3 or &#x201c;substantially constant&#x201d; shall also cover exactly constant).</p><p id="p-0129" num="0148">Use of exemplary language, such as &#x201c;for instance&#x201d;, &#x201c;such as&#x201d;, &#x201c;for example&#x201d; and the like, is merely intended to better illustrate the invention and does not indicate a limitation on the scope of the invention unless so claimed. Any steps described in the specification may be performed in any order or simultaneously, unless the context clearly indicates otherwise.</p><p id="p-0130" num="0149">All of the features and/or steps disclosed in the specification can be combined in any combination, except for combinations where at least some of the features and/or steps are mutually exclusive. In particular, preferred features of the invention are applicable to all aspects of the invention and may be used in any combination.</p><p id="p-0131" num="0150">Reference numerals have just been referred to for reasons of quicker understanding and are not intended to limit the scope of the present invention in any manner.</p><p id="p-0132" num="0151">While the invention has been described in connection with what is presently considered to be the most practical and preferred embodiments, it is to be understood that the invention is not to be limited to the disclosed embodiment, but on the contrary, is intended to cover various modifications and equivalent arrangements included within the spirit and scope of the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>We claim:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method comprising:<claim-text>(A) obtaining first data, said first data including:<claim-text>(i) telemetry data from a user device, the telemetry data relating to actions and/or movements of a person wearing the user device in a real-world environment, wherein the user device comprises a virtual reality (VR) headset,</claim-text><claim-text>(iii) synchronization data, and</claim-text><claim-text>(iii) object data relating to one or more virtual objects in a virtual-world environment;</claim-text></claim-text><claim-text>(B) analyzing the first data to recognize the actions and/or movements of the person in the real-world environment;</claim-text><claim-text>(C) mapping the actions and/or movements recognized in (B) of the person in the real-world environment to corresponding virtual actions and/or movements of a virtual person in the virtual world environment and relative to said virtual objects in the virtual world environment, said mapping using said synchronization data;</claim-text><claim-text>(D) rendering at least some of the virtual actions and/or movements of the virtual person in the virtual world environment relative to the virtual objects; and</claim-text><claim-text>(E) producing one or more video segments of the virtual person in the virtual world environment as rendered in (D).</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the synchronization data correspond to beatmap data that was previously provided to the user device.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein, in (C), the mapping uses the synchronization data to map the actions and/or movements to the beatmap data.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more video segments comprise a video.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more video segments are produced from a corresponding one or more arbitrary viewpoints in the virtual world environment.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more video segments comprise a 360-video.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of any <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first data are obtained in (A) while a first view of the virtual world environment is being displayed on a display of the VR headset.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the first view is produced at a first resolution, and wherein the one or more video segments are at a resolution higher than the first resolution.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the telemetry data are produced by and sent from the VR headset.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the user device also comprises at least one VR handheld controller being worn by the person, and wherein the first data also includes data from the at least one VR handheld controller.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the virtual objects comprise objects in a game or activity.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the virtual objects comprise one or more of: targets, portals, and/or gameplay objects.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object data includes data on user interactions with the virtual objects in the virtual-world environment.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the object data includes, for at least one object, data on whether the at least one object has been hit by the user in the virtual-world environment.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first data also includes sensor data from at least one sensor being worn by the person, and wherein movements of the person are recognized in (B) also using the sensor data.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the sensor data comprise physiological data of the person.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein acts (A)-(E) are performed on a computer system remote from the user device.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the first data were sent to the computer system via a network.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the first data were sent to the computer system in real time, while the person was performing the actions and/or movements.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A device, comprising:<claim-text>(a) hardware including memory and at least one processor, and</claim-text><claim-text>(b) a service running on said hardware, wherein said service is configured to perform the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. An article of manufacture comprising non-transitory computer-readable media having computer-readable instructions stored thereon, the computer-readable instructions including instructions for implementing a computer-implemented method, said method operable on a device comprising hardware including memory and at least one processor and running a service on said hardware, said method comprising the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. A non-transitory computer-readable recording medium storing one or more programs, which, when executed, cause one or more processors to, at least: perform the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>