<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230006851A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230006851</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17534378</doc-number><date>20211123</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>TW</country><doc-number>110124469</doc-number><date>20210702</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>18</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>15</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>78</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>783</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>12</main-group><subgroup>1831</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>7</main-group><subgroup>15</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>7867</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>7834</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND DEVICE FOR VIEWING CONFERENCE</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ASPEED Technology Inc.</orgname><address><city>Hsinchu City</city><country>TW</country></address></addressbook><residence><country>TW</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Chou</last-name><first-name>Chen-Wei</first-name><address><city>Hsinchu City</city><country>TW</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>ASPEED Technology Inc.</orgname><role>03</role><address><city>Hsinchu City</city><country>TW</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method and a device for viewing a conference are provided. In the method, after a wide-view video of a specific conference, related conference event data, and speech content of each participant are obtained, a highlight video of the specific conference is correspondingly generated. Accordingly, the efficiency of conference viewing is improved.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="49.02mm" wi="91.52mm" file="US20230006851A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="79.93mm" wi="93.56mm" file="US20230006851A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="205.40mm" wi="162.48mm" orientation="landscape" file="US20230006851A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="244.60mm" wi="110.41mm" orientation="landscape" file="US20230006851A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="243.33mm" wi="121.67mm" file="US20230006851A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="230.21mm" wi="161.54mm" file="US20230006851A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="166.54mm" wi="159.85mm" file="US20230006851A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="160.36mm" wi="159.85mm" file="US20230006851A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="232.92mm" wi="156.29mm" orientation="landscape" file="US20230006851A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="111.59mm" wi="159.94mm" file="US20230006851A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims the priority benefit of Taiwanese application no. 110124469, filed on Jul. 2, 2021. The entirety of the above-mentioned patent application is hereby incorporated by reference herein and made a part of this specification.</p><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">Technical Field</heading><p id="p-0003" num="0002">The disclosure relates to an information recording technique, and particularly relates to a method and a device for viewing a conference.</p><heading id="h-0004" level="1">Description of Related Art</heading><p id="p-0004" num="0003">In the related art, most commonly used video conference recording software directly records a video of an entire video conference to generate a conference record. However, in addition to causing the recorded content to be too lengthy, such recording method is also difficult to find a key part of the conference afterwards. In addition, the above method is usually not necessarily able to record the part of the conference that a viewer wants to see, and such problem is more severe when a viewing angle of an adopted camera is greater than 180 degrees.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0005" num="0004">The disclosure is directed to a method and a device for viewing a conference, which are adapted to resolve the aforementioned technical problem.</p><p id="p-0006" num="0005">The disclosure provides a method for viewing a conference, and the method includes the following steps. A wide-view video of a specific conference and a plurality of conference event data associated with the wide-view video are obtained. Each conference event data corresponds to a time point of the specific conference, and each conference event data records a sound source direction of a speaker at the corresponding time point and an image range of the speaker in the wide-view video. Individual speech content of a plurality of participants of the specific conference is obtained. A plurality of specific time sections are obtained in the wide-view video, and at least one discussant is found out in each of the specific time sections according to the plurality of conference event data corresponding to each of the specific time sections. Speech content of each discussant is obtained in each of the specific time sections. A discussion image and the speech content of each discussant in each of the specific time sections are arranged into a corresponding discussion video clip. The discussion video clip corresponding to each of the specific time sections is organized into a conference recording video corresponding to the specific conference.</p><p id="p-0007" num="0006">The disclosure provides a conference viewing device including a storage circuit and a processor. The storage circuit stores a program code. The processor is coupled to the storage circuit and accesses the program code for the following operations. The processor obtains a wide-view video of a specific conference and a plurality of conference event data associated with the wide-view video. Each conference event data corresponds to a time point of the specific conference, and each conference event data records a sound source direction of a speaker at the corresponding time point and an image range of the speaker in the wide-view video. The processor obtains individual speech content of a plurality of participants of the specific conference. The processor determines a plurality of specific time sections in the wide-view video and finds out at least one discussant in each of the specific time sections according to the plurality of conference event data corresponding to each of the specific time sections. The processor obtains speech content of each discussant in each of the specific time sections. The processor arranges a discussion image and the speech content of each discussant in each of the specific time sections into a corresponding discussion video clip. The processor organizes the discussion video clip corresponding to each of the specific time sections into a conference recording video corresponding to the specific conference.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007">The accompanying drawings are included to provide a further understanding of the disclosure, and are incorporated in and constitute a part of this specification. The drawings illustrate embodiments of the disclosure and, together with the description, serve to explain the principles of the disclosure.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of a conference recording device according to an embodiment of the disclosure.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram of wide-view conference images according to an embodiment of the disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of conference event data according to an embodiment of the disclosure.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a conference viewing device according to an embodiment of the disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart of a method for viewing a conference according to an embodiment of the disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a schematic diagram of determining specific time sections according to a first embodiment of the disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a schematic diagram of determining specific time sections according to a second embodiment of the disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of determining specific time sections according to a third embodiment of the disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of generating a discussion video clip according to an embodiment of the disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of setting an anchor point according to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of generating a conference recording video according to an embodiment of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0020" num="0019">Generally, most commonly used conference recording systems include following technologies: (1) beamforming: estimating a location of a sound source through a microphone array; (2) object tracking: tracking a specific object in a conference image; (3) people finding: finding out a location of a specific person in a conference image; (4) speaker view: automatically pointing a camera at the sound source for shooting; (5) participant mode: automatically cutting a large image of the entire conference to an image only including the participants; (6) saving conference records: directly recording the entire video conference; (7) voice to text; (8) language processing: including translation, abstract, etc.</p><p id="p-0021" num="0020">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of a conference recording device according to an embodiment of the disclosure. In different embodiments, a conference recording device <b>100</b> may be implemented by various smart devices and/or computer devices.</p><p id="p-0022" num="0021">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, the conference recording device <b>100</b> may include a storage circuit <b>102</b> and a processor <b>104</b>. The storage circuit <b>102</b> is, for example, any type of a fixed or removable random access memory (RAM), a read-only memory (ROM), a flash memory, a hard disk or other similar devices or a combination of these devices that may be used to record a plurality of program codes or modules.</p><p id="p-0023" num="0022">The processor <b>104</b> is coupled to the storage circuit <b>102</b>, and may be a general purpose processor, a special purpose processor, a conventional processor, a digital signal processor, a plurality of microprocessors, one or a plurality of microprocessors, controllers, microcontrollers, application specific integrated circuits (ASIC), field programmable gate arrays (FPGA), any other types of integrated circuits, state machines, processors based on advanced RISC machine (ARM) and similar products combined with digital signal processor kernels.</p><p id="p-0024" num="0023">In the embodiment of the disclosure, the processor <b>104</b> may access the modules and program codes recorded in the storage circuit <b>102</b> to implement a method for viewing a conference proposed by the disclosure, and details thereof are described below.</p><p id="p-0025" num="0024">In an embodiment, in order to make the concept of the disclosure easier to understand, <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is provided below for further explanation, and <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram of wide-view conference images according to an embodiment of the disclosure.</p><p id="p-0026" num="0025">In <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, a conference image <b>110</b> is, for example, a wide-view conference image taken by a wide-view camera (with a viewing angle greater than or equal to 180 degrees) set in a conference room, but the disclosure is not limited thereto.</p><p id="p-0027" num="0026">In an embodiment, after the conference image <b>110</b> is obtained, a user of the conference recording device <b>100</b> may frame persons of interest and/or objects of interest (for example, a whiteboard) with corresponding rectangular boxes. In addition, the processor <b>104</b> may also automatically perform person detection on the conference image <b>110</b> to frame detected persons in the conference image <b>110</b> with the corresponding rectangular boxes to generate content as shown in a conference image <b>120</b>. After the above operation, the conference image <b>110</b> may be correspondingly changed to the conference image <b>120</b> including a plurality of rectangular boxes (for example, rectangular boxes <b>120</b><i>a</i>, <b>120</b><i>b</i>).</p><p id="p-0028" num="0027">In some embodiments, the user may edit a corresponding tag (such as a name of the person/object, etc.) on each rectangular box in the conference image <b>120</b>. For example, it is assumed that the rectangular boxes <b>120</b><i>a </i>and <b>120</b><i>b </i>respectively correspond to a plurality of participants of the conference, the user may write a name of the corresponding participant into the tag of each of the rectangular boxes <b>120</b><i>a </i>and <b>120</b><i>b</i>, but the disclosure is not limited thereto. In addition, the processor <b>104</b> may also display the tags of one or more rectangular boxes in the conference image <b>120</b> with some predetermined names. For example, corresponding to the rectangular boxes of some participants, the processor <b>104</b> may edit the tags thereof with predetermined names such as &#x201c;person <b>1</b>&#x201d;, &#x201c;person <b>2</b>&#x201d;, etc. Similarly, corresponding to the rectangular boxes of some objects, the processor <b>104</b> may edit the tags thereof with predetermined names such as &#x201c;object <b>1</b>&#x201d;, &#x201c;object <b>2</b>&#x201d;, etc., but the disclosure is not limited thereto.</p><p id="p-0029" num="0028">After the above operations, the conference image <b>120</b> may be correspondingly changed to a conference image <b>130</b>, where each rectangular box may have a corresponding tag, for example, a tag <b>130</b><i>a </i>corresponding to the rectangular box <b>120</b><i>a</i>, and a tag <b>130</b><i>b </i>corresponding to the rectangular box <b>120</b><i>b</i>, but the disclosure is not limited thereto.</p><p id="p-0030" num="0029">In some embodiments, the user may select one or a plurality of rectangular boxes to be tracked in the conference image <b>130</b> (for example, a rectangular box corresponding to a speaker), and the conference image <b>130</b> may be correspondingly changed to the content shown in a conference image <b>140</b>. In this case, the processor <b>104</b> may continuously track the person/object located in the selected one or a plurality of rectangular boxes (for example, a rectangular box <b>120</b><i>c</i>), and accordingly let the rectangular box to move along with movement the person/object.</p><p id="p-0031" num="0030">In an embodiment, the processor <b>104</b> may also detect a sound source direction of a sound in the conference, and find out the corresponding rectangular box in the conference image <b>140</b> based on the sound source direction. In this way, the processor <b>104</b> may learn which person corresponding to the rectangular box makes the sound. In an embodiment, the processor <b>104</b> may mark the rectangular box corresponding to the sound source direction in a specific manner (for example, to change it to a specific color) to highlight which person in the rectangular box is making the sound at the moment, as shown by a rectangular box <b>120</b><i>d </i>in a conference image <b>150</b>. In an embodiment, the processor <b>104</b> may also perform voice recognition on the aforementioned sound to obtain speech content (for example, a transcript) of the person in the rectangular box <b>120</b><i>d. </i></p><p id="p-0032" num="0031">In some embodiments, the processor <b>104</b> may determine the sound source direction of the aforementioned sound and the corresponding rectangular box thereof based on techniques such as beamforming, direction of arrival (DOA), sound localization, lip detection (lip detection), face recognition, etc., but the disclosure is not limited thereto.</p><p id="p-0033" num="0032">Based on the aforementioned concept, the processor <b>104</b> may record an image range (i.e., the corresponding rectangular box), related speech content, and a related time point of each participant in each conference image in the entire conference, but the disclosure is not limited thereto.</p><p id="p-0034" num="0033">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of conference event data according to an embodiment of the disclosure. In the embodiment, the processor <b>104</b> may record each conference event that occurs in the conference with corresponding conference event data, where each conference event corresponds to, for example, a time point when a speaking state of one or more participants in the conference changes (such as changing from never speaking to speaking, or changing from speaking to stopping speaking), but the disclosure is not limited thereto.</p><p id="p-0035" num="0034">In the embodiment, the conference event data related to each conference event may be, for example, recorded by adopting a format <b>200</b>. For example, it is assumed that a 1<sup>st </sup>conference event in the conference is occurred at a time point T<b>1</b>, the processor <b>104</b> may record related information into conference event data <b>210</b> based on the format <b>200</b>. In an embodiment, the conference event data <b>210</b> may record, for example, an index value of the 1<sup>st </sup>conference event (i.e., &#x201c;1&#x201d;), the time point T<b>1</b>, the event content (i.e., &#x201c;participants A and C start speaking&#x201d;), a sound source direction and an image range of the participant A (i.e., a rectangular box range currently corresponding to the participant A), a sound source direction and an image range of the participant C (i.e., a rectangular box range currently corresponding to the participant C), but the disclosure is not limited thereto. In addition, while generating conference event data <b>221</b>, the processor <b>104</b> may further record the related speech content of the participants A and C based on the previous teaching.</p><p id="p-0036" num="0035">Moreover, it is assumed that a 2<sup>nd </sup>conference event in the conference is occurred at a time point T<b>2</b>, the processor <b>104</b> may record related information into the conference event data <b>221</b> based on the format <b>200</b>. In an embodiment, the conference event data <b>221</b> may record, for example, an index value of the 2<sup>nd </sup>conference event (i.e., &#x201c;2&#x201d;), the time point T<b>2</b>, the event content (i.e., &#x201c;the participant A stops speaking&#x201d;), a sound source direction and an image range of the participant A.</p><p id="p-0037" num="0036">In other embodiments, since the 2<sup>nd </sup>conference event may also be construed as &#x201c;the participant C keeps speaking&#x201d;, the processor <b>104</b> may also record information related to the 2<sup>nd </sup>conference event as conference event data <b>222</b> based on the format <b>200</b>. In an embodiment, the conference event data <b>222</b> may record, for example, an index value of the 2<sup>nd </sup>conference event (i.e., &#x201c;2&#x201d;), the time point T<b>2</b>, the event content (i.e., &#x201c;the participant C keeps speaking&#x201d;), a sound source direction and an image range of the participant C. In other words, the conference event data <b>221</b> and <b>222</b> are different recording methods for the same conference event, but the disclosure is not limited thereto. Moreover, while generating the conference event data <b>221</b> and <b>222</b>, the processor <b>104</b> may further record the related speech content of the participant C based on the previous teaching.</p><p id="p-0038" num="0037">For other conference events (for example, &#x201c;the participant C stops speaking&#x201d;, &#x201c;the participant C starts speaking&#x201d;, etc.) the processor <b>104</b> may also record related information as corresponding conference event data based on the previous teaching, and details thereof are not repeated.</p><p id="p-0039" num="0038">In some embodiments, it is assumed that a certain conference is synchronized participated by participants located in different conference places, the processor <b>104</b> may still generate related conference event data based on the above teachings. For example, it is assumed that wide-view cameras C and D are respectively set in conference rooms A and B, and people in the conference rooms A and B participate in a same online conference. In this case, the processor <b>104</b> may execute the mechanism taught in the previous embodiment when receiving the conference images taken by the wide-view cameras C and D for this online conference, and generate conference event data related to the above online conference by synthesizing the conference images of the wide-view cameras C and D, but the disclosure is not limited thereto.</p><p id="p-0040" num="0039">In some embodiments, the disclosure further provides a method for viewing a conference, which is adapted to present a relatively lengthy wide-view video in a relatively simplified conference recording video (which may be construed as a highlight video of the aforementioned specific conference) after obtaining a wide-view video of a specific conference (which includes a plurality of conference images of the specific conference) and related conference event data. In this way, users may understand the general content of the above specific conference only based on this conference recording video. The related details are described below.</p><p id="p-0041" num="0040">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of a conference viewing device according to an embodiment of the disclosure. In different embodiments, a conference viewing device may be implemented as various smart devices and/or computer devices.</p><p id="p-0042" num="0041">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the conference viewing device <b>300</b> may include a storage circuit <b>302</b> and a processor <b>304</b>, where the storage circuit <b>302</b> and the processor <b>304</b> may be individually implemented with reference to the storage circuit <b>102</b> and the processor <b>104</b>, and details thereof are not repeated.</p><p id="p-0043" num="0042">In some embodiments, the conference recording device <b>100</b> and the conference viewing device <b>300</b> may be implemented as a same device or implemented as different devices.</p><p id="p-0044" num="0043">In an embodiment of the disclosure, the processor <b>304</b> may access modules and program codes recorded in the storage circuit <b>302</b> to implement a method for viewing a conference proposed by the disclosure, and the details thereof are described as follows.</p><p id="p-0045" num="0044">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart of a method for viewing a conference according to an embodiment of the disclosure. In the embodiment, the method in <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be executed by the conference viewing device <b>300</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, and details of each step of <figref idref="DRAWINGS">FIG. <b>4</b></figref> are described below with reference to the components shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0046" num="0045">First, in step S<b>410</b>, the processor <b>304</b> may obtain a wide-view video of a specific conference and a plurality of conference event data associated with the wide-view video. In one embodiment, it is assumed that the conference recording device <b>100</b> is used to record the wide-view video of the aforementioned specific conference and generate related conference event data, the processor <b>304</b> may, for example, receive the wide-view video of the aforementioned specific conference and the related conference event data produced by the processor <b>104</b> from the conference recording device <b>100</b>, but the disclosure is not limited thereto.</p><p id="p-0047" num="0046">Thereafter, in step S<b>420</b>, the processor <b>304</b> obtains individual speech content of a plurality of participants of the specific conference. In an embodiment, the processor <b>304</b> may also receive the speech content recorded by the processor <b>104</b> for each participant of the above specific conference from the conference recording device <b>100</b>, but the disclosure is not limited thereto.</p><p id="p-0048" num="0047">In step S<b>430</b>, the processor <b>304</b> determines a plurality of specific time sections in the wide-view video, and finds out a discussant in each specific time section according to the plurality of conference event data corresponding to each specific time section.</p><p id="p-0049" num="0048">In different embodiments, the processor <b>304</b> may determine the aforementioned specific time sections in the wide-view video based on different methods, which will be further described below.</p><p id="p-0050" num="0049">Referring to <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a schematic diagram of determining the specific time sections according to a first embodiment of the disclosure. In the embodiment, the processor <b>304</b> may, for example, provide a conference timeline <b>50</b> of the wide-view video, and the user may mark one or a plurality of required time sections on the conference timeline <b>50</b> by himself. It is assumed that the user marks a plurality of designated time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>on the conference timeline <b>50</b>, the processor <b>304</b> may take the designated time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>as the plurality of specific time sections in step S<b>430</b>.</p><p id="p-0051" num="0050">Referring to <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a schematic diagram of determining the specific time sections according to a second embodiment of the disclosure. In the embodiment, the processor <b>304</b> may, for example, provide the conference timeline <b>50</b> of the wide-view video for the user's reference. In addition, the processor <b>304</b> may also provide a discussion-participating timeline of each participant of the aforementioned specific conference in the specific conference, where the discussion-participating timeline of each participant may be marked with discussion-participating sections of each participant in the specific conference.</p><p id="p-0052" num="0051">For example, it is assumed that the aforementioned specific conference includes a participant <b>1</b> to a participant K (where K is a positive integer), the processor <b>304</b> may provide individual discussion-participating timelines <b>501</b>-<b>50</b>K of the participant <b>1</b> to the participant K based on the related conference recording data of the aforementioned specific conference. In the discussion-participating timeline <b>501</b> of the participant <b>1</b>, the processor <b>304</b> may mark discussion-participating sections <b>501</b><i>a</i>-<b>501</b><i>c </i>of the participant <b>1</b> in the specific conference, where each of the discussion-participating sections <b>501</b><i>a</i>-<b>501</b><i>c </i>is, for example, a time section in which the participant <b>1</b> has participated in a discussion (for example, make a speech), but the disclosure is not limited thereto. Similarly, in the discussion-participating timeline <b>50</b>K of the participant K, the processor <b>304</b> may mark discussion-participating sections <b>50</b>Ka-<b>50</b>Kc of the participant K in the specific conference, where each of the discussion-participating sections <b>50</b>Ka-<b>50</b>Kc is, for example, a time section in which the participant K has participated in a discussion (for example, make a speech), but the disclosure is not limited thereto.</p><p id="p-0053" num="0052">Based on the discussion-participating timelines <b>501</b>-<b>50</b>K, the user may learn which participant has participated in the discussion during which time section, and then decide one or a plurality of time sections to be marked on the conference timeline <b>50</b>. It is assumed that the user marks the designated time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> on the conference timeline <b>50</b>, the processor <b>304</b> may take the designated time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>as the plurality of specific time sections in step S<b>430</b>, but the disclosure is not limited thereto.</p><p id="p-0054" num="0053">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of determining the specific time sections according to a third embodiment of the disclosure. In the embodiment, the processor <b>304</b> may provide participants of the aforementioned specific conference for selection. In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the processor <b>304</b> may display icons <b>60</b><i>a</i>-<b>60</b><i>c </i>corresponding to the aforementioned participants, and the user may select one or a plurality of participants to be followed.</p><p id="p-0055" num="0054">For example, it is assumed that when the user wants to find out the discussion-participating sections in which the participants Claire and Benny are respectively presenters, the user may select the icons <b>60</b><i>a </i>and <b>60</b><i>c </i>accordingly. In this case, the processor <b>304</b> may mark discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c </i>of Claire and Benny in the aforementioned specific conference on the conference timeline <b>60</b> of the wide-view video.</p><p id="p-0056" num="0055">In <figref idref="DRAWINGS">FIG. <b>6</b></figref>, based on the aforementioned conference event data, the processor <b>304</b> may also provide corresponding discussant lists <b>62</b><i>a</i>-<b>62</b><i>c </i>for each of the discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c</i>, and the user may learn about the discussants participating in the discussions in each of the discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c</i>. In addition, the processor <b>304</b> may also emphasize a current presenter in a specific manner in the discussant lists <b>62</b><i>a</i>-<b>62</b><i>c</i>, so that the user may learn the presenters in each of the discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c. </i></p><p id="p-0057" num="0056">For example, it is assumed that Claire is the presenter in the discussion-participating sections <b>61</b><i>a </i>and <b>61</b><i>b</i>, the processor <b>304</b> may display the icon of Claire in a larger size in the discussant lists <b>62</b><i>a </i>and <b>62</b><i>b </i>accordingly. For another example, it is assumed that Benny is the presenter in the discussion-participating section <b>61</b><i>c</i>, the processor <b>304</b> may correspondingly display the icon of Benny in the larger size in the discussant list <b>62</b><i>c</i>. In this way, the user may learn that Claire is the presenter in the discussion-participating sections <b>61</b><i>a </i>and <b>61</b><i>b</i>, and Benny is the presenter in the discussion-participating section <b>61</b><i>c</i>, but the disclosure is not limited thereto.</p><p id="p-0058" num="0057">In addition, after providing the discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c</i>, the user may adjust a size of each of the discussion-participating section <b>61</b><i>a</i>-<b>61</b><i>c </i>according to an actual need. For example, it is assumed that an original start time and an end time of the discussion-participating section <b>61</b><i>a </i>are 0:15:00 to 0:25:00 as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the user may respectively adjust the above start time and end time to required values, such as 0:14:00 and 0:26:00 shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, but the disclosure is not limited thereto. Thereafter, the processor <b>304</b> may use the (adjusted) discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c </i>as the multiple specific time sections in step S<b>430</b>, but the disclosure is not limited thereto.</p><p id="p-0059" num="0058">In the above situation, although it is assumed that the user wants to follow the discussion-participating sections in which some participants are the presenters, in other embodiments, the user may also determine the characteristics of the participants to be followed based on other principles. For example, in some embodiments, it is assumed that when the user wants to find out the discussion-participating sections in which the participants Claire and Benny are respectively the discussants, the user may correspondingly select the icons <b>60</b><i>a </i>and <b>60</b><i>c </i>after triggering the processor <b>304</b> to provide a related search function/interface. In this case, the processor <b>304</b> may also mark the discussion-participating sections <b>61</b><i>a</i>-<b>61</b><i>c </i>in which Claire and Benny are discussants in the aforementioned specific conference on the conference timeline <b>60</b> of the wide-view video, but the disclosure is not limited thereto.</p><p id="p-0060" num="0059">In other embodiments, the processor <b>304</b> may also determine the aforementioned specific time sections in the wide-view video based on other principles. For example, in a fourth embodiment, the processor <b>304</b> may first provide the conference timeline <b>50</b> of the wide-view video as shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. Thereafter, the processor <b>304</b> may obtain a designated number of persons set by the user.</p><p id="p-0061" num="0060">In an embodiment, the processor <b>304</b> may mark a plurality of first reference time sections on the conference timeline <b>50</b> according to the aforementioned designated number of persons. In an embodiment, a number of participants corresponding to each first reference time section may not be less than the aforementioned designated number of persons. To be specific, since some participants may leave or join the conference for some reasons in the middle of the conference, after the user sets the designated number of persons, the processor <b>304</b> may find out the time sections in which the number of participants in the aforementioned specific conference is not less than the designated number of persons to serve as the aforementioned first reference time sections. In this way, certain time sections in which the number of participants is too less (for example, a break time) may be excluded.</p><p id="p-0062" num="0061">In another embodiment, the number of discussants corresponding to each first reference time section may not be less than the aforementioned designated number of persons. Specifically, since there may be some parts of the conference that more people actively participate in the discussion, and there may also be some parts of the conference that only a few people participate in the discussion, in this case, after the user set the designated number of persons, the processor <b>304</b> may find out the time sections in which more people participate in the discussion during the specific conference to serve as the first reference time sections. In this way, more enthusiastic discussion sections may be found.</p><p id="p-0063" num="0062">After the first reference time sections are provided for the user's reference, the user may select one or more designated time sections (for example, the designated time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>). Thereafter, the processor <b>304</b> may accordingly use the aforementioned designated time sections as the multiple specific time sections in step S<b>430</b>.</p><p id="p-0064" num="0063">In a fifth embodiment, in the process of determining the aforementioned specific time sections in the wide-view video, the processor <b>304</b> may first provide the conference timeline <b>50</b> of the wide-view video as shown in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. Thereafter, the processor <b>304</b> may provide a keyword search box for the user to input keywords of interest.</p><p id="p-0065" num="0064">In an embodiment, after obtaining a search keyword entered by the user in the keyword search box, the processor <b>304</b> may accordingly mark a plurality of second reference time sections on the conference timeline <b>50</b>, where speech content of the participants of the specific conference in each second reference time section may include the aforementioned search keyword. In brief, after obtaining the search keyword, the processor <b>304</b> may find out the time sections in which the search keyword was mentioned by the participants, and extract these time sections to serve as the aforementioned second reference time sections.</p><p id="p-0066" num="0065">After the second reference time sections are provided for the user's reference, the user may select one or more designated time sections (for example, the designated time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>). Thereafter, the processor <b>304</b> may accordingly use the aforementioned designated time sections as the multiple specific time sections in step S<b>430</b>, but the disclosure is not limited thereto.</p><p id="p-0067" num="0066">After determining the aforementioned specific time sections in the wide-view video according to the above teaching, the processor <b>304</b> may find out the discussants in each specific time section according to the plurality of conference event data corresponding to each specific time section.</p><p id="p-0068" num="0067">Taking <figref idref="DRAWINGS">FIG. <b>2</b></figref> as an example, it is assumed that a specific time section D<b>1</b> covers the time points T<b>1</b> and T<b>2</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the processor <b>304</b> may learn that the discussants in the specific time section D<b>1</b> include the participants A and C based on the conference event data <b>210</b>, <b>221</b> (or <b>222</b>) corresponding to the time points T<b>1</b> and T<b>2</b>, but the disclosure is not limited thereto.</p><p id="p-0069" num="0068">In other embodiments, in addition to determining the plurality of specific time sections based on one or more designated time sections selected by the user, the processor <b>304</b> may also automatically determine the plurality of specific time sections according to an appropriate mechanism.</p><p id="p-0070" num="0069">For example, in the fourth embodiment, after the processor <b>304</b> marks the plurality of first reference time sections on the conference timeline <b>50</b> according to the aforementioned designated number of persons, the processor <b>304</b> may directly use these first reference time sections as the plurality of specific time sections in step S<b>430</b>, but the disclosure is not limited thereto.</p><p id="p-0071" num="0070">In an embodiment, the processor <b>304</b> may, for example, identify a specific participant of a specific conference from the participants (for example, a presenter of the entire specific conference or other participants that the user pays attention to), and provide a discussion-participating timeline of the specific participant in the above specific conference. In an embodiment, the discussion-participating timeline of the specific participant may be marked with one or more discussion-participating sections of the specific participant in the above specific conference. Thereafter, the processor <b>304</b> may find out a plurality of designated time sections from the aforementioned discussion-participating sections of the specific participant, and take these designated time sections as the plurality of specific time sections in step S<b>430</b>. In an embodiment, a time difference between the designated time sections found by the processor <b>304</b> according to the aforementioned method may be greater than a time threshold. In this way, a situation that the content of the entire specific conference cannot be better characterized due to that the designated time sections identified by the processor <b>304</b> are too close is avoided, but the disclosure is not limited thereto.</p><p id="p-0072" num="0071">In some embodiments, the processor <b>304</b> may also extend the designated time sections of the previous embodiments forward/backward by a period of time, and then use the extended designated time sections as the plurality of specific time sections in step S<b>430</b>, but the disclosure is not limited thereto.</p><p id="p-0073" num="0072">Then, in step S<b>440</b>, the processor <b>304</b> obtains speech content of each discussant in each specific time section. Taking the aforementioned specific time section D<b>1</b> as an example, after learning that the discussants therein include the participants A and C, the processor <b>304</b> may find out the speech content of the participants A and C in the specific time section D<b>1</b> according to each time point in the specific time section D<b>1</b>, but the disclosure is not limited thereto.</p><p id="p-0074" num="0073">In step S<b>450</b>, the processor <b>304</b> arranges a discussion image and the speech content of each discussant in each specific time section into a corresponding discussion video clip.</p><p id="p-0075" num="0074">In an embodiment, the processor <b>304</b> performs roughly the same mechanism for generating corresponding discussion video clips for each specific time section. Therefore, an i<sup>th </sup>(i is an index value and is a positive integer) specific time section in the above specific time sections is temporarily treated as an example for description, but the disclosure is not limited thereto.</p><p id="p-0076" num="0075">In general, the processor <b>304</b> may find out one or a plurality of discussants belonging to the i<sup>th </sup>specific time section, in order to distinguish from the aforementioned discussants, the one or plurality of discussants of the i<sup>th </sup>specific time section are referred to as first discussants, and a first discussion image and first speech content of each first discussant in the i<sup>th </sup>specific time section are found. Thereafter, the processor <b>304</b> may arrange the first discussion image and the first speech content of each first discussant into a discussion video clip corresponding to the i<sup>th </sup>specific time section according to a designated image layout. In an embodiment, the aforementioned designated image layout may include a speech content frame and a first image frame corresponding to each first discussant, where each first image frame may present the first discussion image of each first discussant, the speech content frame may include the first speech content of each first discussant, and the first speech content of each first discussant may be sorted in the speech content frame according to a speech time of each first discussant. In order to make the above concepts easier to understand, <figref idref="DRAWINGS">FIG. <b>7</b></figref> is provided below for further description.</p><p id="p-0077" num="0076">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of generating a discussion video clip according to an embodiment of the disclosure. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, it is assumed that the processor <b>304</b> determines a specific time section <b>71</b> on a conference timeline <b>70</b> according to the teaching of the previous embodiments. In this case, the processor <b>304</b> may find out the discussants belonging to the specific time section <b>71</b>. For ease of description, it is assumed below that the discussants in the specific time section <b>71</b> are Claire and Benny in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, but the disclosure is not limited thereto.</p><p id="p-0078" num="0077">Correspondingly, the processor <b>304</b> may find out the discussion images and speech content of Claire and Benny in the specific time section <b>71</b>, and organize the discussion images and speech content of Claire and Benny in the specific time section <b>71</b> into discussion video clips corresponding to the specific time section <b>71</b> according to a designated image layout <b>72</b>.</p><p id="p-0079" num="0078">In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the designated image layout <b>72</b> may include a speech content frame <b>72</b><i>c </i>and image frames <b>72</b><i>a </i>and <b>72</b><i>b </i>corresponding to each discussant. In an embodiment, the designated image layout <b>72</b> may, for example, display the discussion image of the presenter (such as Claire) in the specific time section <b>71</b> in the larger image frame <b>72</b><i>a</i>, and display the discussion image of other discussants (such as Benny) in the smaller image frame <b>72</b><i>b</i>. In addition, the speech content frame <b>72</b><i>c </i>may include the speech content of Claire and Benny in the specific time section <b>71</b>, and the speech content may be sorted in the speech content frame <b>72</b><i>c </i>according to a speech time of each of Claire and Benny.</p><p id="p-0080" num="0079">In this case, when the discussion video clip corresponding to the specific time section <b>71</b> is played, the discussion images when Claire and Benny make related speeches in the specific time section <b>71</b> may be seen, but the disclosure is not limited thereto.</p><p id="p-0081" num="0080">In different embodiments, the aforementioned designated image layout used by the processor <b>304</b> may also be adjusted to other layout forms according to the needs of the user, which is not limited to the picture-in-picture pattern shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In some embodiments, the processor <b>304</b> may determine the pattern of the designated image layout by itself, or the user may select a desired pattern as the designated image layout, but the disclosure is not limited thereto.</p><p id="p-0082" num="0081">In some embodiments, since one or a plurality of first discussants in the i<sup>th </sup>specific time section may not have the corresponding first discussion image in a certain sub-time section of the i<sup>th </sup>specific time section for some reason, the processor <b>304</b> may correspondingly adjust the content of the first image frame of the first discussant in the sub-time section.</p><p id="p-0083" num="0082">Taking <figref idref="DRAWINGS">FIG. <b>7</b></figref> as an example, it is assumed that Benny does not have the corresponding discussion image in a certain sub-time section in the specific time section <b>71</b>. In this case, the processor <b>304</b> may find out the image frame <b>72</b><i>b </i>corresponding to Benny, and display the image frame <b>72</b><i>b </i>as a predetermined image (for example, a portrait image) or a blank image in this sub-time section, or adjust this sub-time section to an image layout not including the image frame <b>72</b><i>b </i>(for example, an image layout that only includes the image frame <b>72</b><i>a</i>), but the disclosure is not limited thereto.</p><p id="p-0084" num="0083">In some embodiments, regarding the i<sup>th </sup>specific time section, the user may further set an anchor point therein to sequentially divide the i<sup>th </sup>specific time section into a plurality of sub-time sections, and set a different image layout for each sub-time section.</p><p id="p-0085" num="0084">Referring to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, <figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of setting an anchor point according to <figref idref="DRAWINGS">FIG. <b>7</b></figref>. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, after determining the specific time section <b>71</b>, it is assumed that the user additionally sets an anchor point <b>81</b> (a corresponding time point thereof is, for example, &#x201c;0:20:00&#x201d;) in the specific time section <b>71</b>, the processor <b>304</b> may divide the specific time section <b>71</b> into sub-time sections <b>71</b><i>a </i>and <b>71</b><i>b </i>accordingly.</p><p id="p-0086" num="0085">In the embodiment, the processor <b>304</b> may, for example, arrange the discussion images and speech content of Claire and Benny in the sub-time section <b>71</b><i>a </i>into a first sub-discussion video clip corresponding to the sub-time section <b>71</b><i>a </i>according to a first image layout <b>82</b> (which may be the same as the designated image layout <b>72</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). In addition, the processor <b>304</b> may also arrange the discussion images and speech content of Claire and Benny in the sub-time section <b>71</b><i>b </i>into a second sub-discussion video clip corresponding to the sub-time section <b>71</b><i>b </i>according to a second image layout <b>83</b>.</p><p id="p-0087" num="0086">In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the second image layout <b>83</b> may include a speech content frame <b>83</b><i>c </i>and image frames <b>83</b><i>a </i>and <b>83</b><i>b </i>corresponding to each discussant. In an embodiment, the second image layout <b>83</b> may, for example, present the discussants (such as Benny and Claire) in the sub-time section <b>71</b><i>b </i>in the image frames <b>83</b><i>a </i>and <b>83</b><i>b </i>of the same size. In addition, the speech content frame <b>83</b><i>c </i>may include speech content of Claire and Benny in the sub-time section <b>71</b><i>b</i>, and the speech content may be sorted in the speech content frame <b>83</b><i>c </i>according to the speech time of each of Claire and Benny, but the disclosure is not limited thereto.</p><p id="p-0088" num="0087">Thereafter, the processor <b>304</b> may sequentially splice the first sub-discussion video clip and the second sub-discussion video clip into a discussion video clip corresponding to the specific time section <b>71</b>. In this case, when the discussion video clip corresponding to the specific time section <b>71</b> is played, the first sub-discussion video clip corresponding to the first image layout <b>82</b> and the second sub-discussion video clip corresponding to the second image layout <b>83</b> may be seen in sequence, but the disclosure is not limited thereto.</p><p id="p-0089" num="0088">In some embodiments, after dividing the specific time section <b>71</b> into the sub-time sections <b>71</b><i>a </i>and <b>71</b><i>b</i>, the processor <b>304</b> may further insert a transition animation at an ending segment (for example, the last one to several seconds) of the sub-time section <b>71</b><i>a </i>sorted in the front, where the transition animation may be used to convert the first image layout <b>82</b> to the second image layout <b>83</b>. In this case, when the discussion video clip corresponding to the specific time section <b>71</b> is played, the first sub-discussion video clip corresponding to the first image layout <b>82</b>, the above transition animation, and the second sub-discussion video clip corresponding to the second image layout <b>83</b> may be seen in sequence, but the disclosure is not limited thereto.</p><p id="p-0090" num="0089">In other embodiments, the user may set a required number of anchor points in the required specific time section according to an actual requirement, and the corresponding operations performed by the processor <b>304</b> may be learned by referring to the above teachings, and details thereof are not repeated.</p><p id="p-0091" num="0090">After obtaining the discussion video clip corresponding to each specific time section, in step S<b>460</b>, the processor <b>304</b> may organize the discussion video clip corresponding to each specific time section into a conference recording video corresponding to the specific conference (which may be understood as a highlight clip of the aforementioned specific conference).</p><p id="p-0092" num="0091">Referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of generating a conference recording video according to an embodiment of the disclosure. In the embodiment, it is assumed that the processor <b>304</b> has generated discussion video clips <b>90</b><i>a</i>-<b>90</b><i>c </i>respectively corresponding to the specific time sections <b>50</b><i>a</i>-<b>50</b><i>c </i>according to the above teachings after determining the specific time sections <b>50</b><i>a</i>-<b>50</b><i>c</i>. In this case, the processor <b>304</b> may, for example, sequentially combine the discussion video clips <b>90</b><i>a</i>-<b>90</b><i>c </i>into a conference recording video <b>910</b> corresponding to the aforementioned specific conference, but the disclosure is not limited thereto.</p><p id="p-0093" num="0092">In some embodiments, the discussion video clip corresponding to each specific time section may also be set with different frame rates, so as to achieve a time-reducing/slow playback effect when playing the conference recording video.</p><p id="p-0094" num="0093">Taking <figref idref="DRAWINGS">FIG. <b>9</b></figref> as an example, it is assumed that the user wants the discussion video clips <b>90</b><i>b </i>and <b>90</b><i>c </i>to produce a time-reducing/fast playback effect when being played, the user may, for example, set the discussion video clip <b>90</b><i>a </i>to a first frame rate (for example, 30 frames per second), and set the discussion video clips <b>90</b><i>b</i>, <b>90</b><i>c </i>to a second frame rate (for example, 60 frames per second) higher than the first frame rate. Thereafter, the processor <b>304</b> may then sequentially combine the discussion video clips <b>90</b><i>a</i>-<b>90</b><i>c </i>into the conference recording video <b>910</b> corresponding to the aforementioned specific conference. In this way, in the process of playing the conference recording video <b>910</b>, when the parts of the discussion video clips <b>90</b><i>b</i>, <b>90</b><i>c </i>are played, the time-reducing/fast playback effect of the discussion video clips <b>90</b><i>b</i>, <b>90</b><i>c </i>is presented due to the change of the frame rate, but the disclosure is not limited thereto.</p><p id="p-0095" num="0094">Further, compared to the conventional method of abandoning some frames to achieve the time-reducing/fast playback effect, the method of the disclosure may preserve all the images more completely.</p><p id="p-0096" num="0095">On the other hand, it is assumed that the user wants the discussion video clips <b>90</b><i>b </i>and <b>90</b><i>c </i>to have a slow playback effect when being played, the user may, for example, set the discussion video clip <b>90</b><i>a </i>to a first frame rate (for example, 30 frames per second), and set the discussion video clips <b>90</b><i>b </i>and <b>90</b><i>c </i>to a second frame rate (for example, 15 frames per second) lower than the first frame rate. Thereafter, the processor <b>304</b> may then sequentially combine the discussion video clips <b>90</b><i>a</i>-<b>90</b><i>c </i>into the conference recording video <b>910</b> corresponding to the aforementioned specific conference. In this way, in the process of playing the conference recording video <b>910</b>, when the parts of the discussion video clips <b>90</b><i>b</i>, <b>90</b><i>c </i>are played, the slow playback effect of the discussion video clips <b>90</b><i>b</i>, <b>90</b><i>c </i>is presented due to the change of the frame rate, but the disclosure is not limited thereto.</p><p id="p-0097" num="0096">In some embodiments, when adjusting the time-reducing/slow playback effect, the processor <b>304</b> may also adjust a sound part accordingly while maintaining a pitch, but the disclosure is not limited thereto.</p><p id="p-0098" num="0097">In some embodiments, the processor <b>304</b> may also arrange the speech content of each participant into an editable verbatim script according to the aforementioned conference event data, where the editable verbatim script may sort the speech content of each participant according to the corresponding speech time. In some embodiments, the processor <b>304</b> may also provide a function of translating/summarizing the above editable verbatim script. In addition, the user may also select one or a plurality of paragraphs from the above editable verbatim script, and the processor <b>304</b> may find out the presenter (for example, the participant with the most speech content) from the one or plurality of paragraphs selected by the user. In addition, the user may manually edit the content of the aforementioned editable verbatim script by himself. In some embodiments, the user may also select one of the participants to follow, and the processor <b>304</b> may correspondingly find out a verbatim script of the related speech content of the participant for the user's reference/editing, but the disclosure is not limited thereto.</p><p id="p-0099" num="0098">In some embodiments, the processor <b>304</b> may find one or a plurality of text fragments from the aforementioned editable verbatim script according to certain principles, and then combine the corresponding images to generate the corresponding conference recording video. For example, the processor <b>304</b> may score each text segment according to factors such as corresponding enthusiasm of interaction, a degree of intonation fluctuation, and an interval time between sentences, etc., where the score of each text segment may be positively correlated with the corresponding enthusiasm of interaction, the degree of intonation fluctuation, and is negatively related to the interval time between sentences, but the disclosure is not limited thereto.</p><p id="p-0100" num="0099">In some embodiment, it is assumed that when the aforementioned specific conference is in progress, a related person has switched his speaking mode to a presenter mode at some time points (i.e., only the speaker is making a speech), the processor <b>304</b> may take out the time sections corresponding to the presenter mode to serve as the aforementioned specific time sections, but the disclosure is not limited thereto.</p><p id="p-0101" num="0100">In some embodiments, it is assumed that when the aforementioned wide-view film further includes the region of interest (ROI) mentioned in Taiwan patent application No. 109145738 (the full text of which is incorporated herein by reference), the processor <b>304</b> may find out time points when an attribute of each ROI changes, and take out the time sections related to these time points to serve as the aforementioned specific time sections, but the disclosure is not limited thereto.</p><p id="p-0102" num="0101">In view of the foregoing, in the disclosure, after obtaining the conference event data and the speech content of each participant related to the wide-view video of the specific conference, the user may manually determine multiple specific time sections in the wide-view video. Alternatively, the conference viewing device of the disclosure may automatically determine the aforementioned specific time sections according to certain principles. Next, in the method of the disclosure, the discussion image and speech content of each discussant are organized in each specific time section into corresponding discussion video clip, where the discussion video clip corresponding to each specific time section may adopt the corresponding image layout to present the discussion image and the speech content of each discussant. Then, in the method of the disclosure, the discussion video clips corresponding to each specific time section may be organized into the conference recording video corresponding to the specific conference. In this way, the method of the disclosure may intelligently condense the lengthy and difficult-to-view wide-view video into condensed highlight clips, and the efficiency of conference viewing is thereby improved.</p><p id="p-0103" num="0102">It will be apparent to those skilled in the art that various modifications and variations can be made to the disclosed embodiments without departing from the scope or spirit of the disclosure. In view of the foregoing, it is intended that the disclosure covers modifications and variations provided they fall within the scope of the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for viewing a conference, comprising:<claim-text>obtaining a wide-view video of a specific conference and a plurality of conference event data associated with the wide-view video, wherein each conference event data corresponds to a time point of the specific conference, and each conference event data records a sound source direction of a speaker at the corresponding time point and an image range of the speaker in the wide-view video;</claim-text><claim-text>obtaining individual speech content of a plurality of participants of the specific conference;</claim-text><claim-text>determining a plurality of specific time sections in the wide-view video, finding out at least one discussant in each of the specific time sections according to the plurality of conference event data corresponding to each of the specific time sections;</claim-text><claim-text>arranging a discussion image of each of the at least one discussant and the speech content of each of the at least one discussant in each of the specific time sections into a corresponding discussion video clip; and</claim-text><claim-text>organizing the discussion video clip corresponding to each of the specific time sections into a conference recording video corresponding to the specific conference.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the plurality of specific time sections in the wide-view video comprises:<claim-text>providing a conference timeline of the wide-view video; and</claim-text><claim-text>in response to determining that the conference timeline is marked with a plurality of designated time sections by a user, treating the designated time sections as the specific time sections.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising:<claim-text>providing a discussion-participating timeline of each participant in the specific conference, wherein the discussion-participating timeline of each participant is marked with at least one discussion-participating section of each participant in the specific conference.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the plurality of specific time sections in the wide-view video comprises:<claim-text>providing the participants of the specific conference for selection;</claim-text><claim-text>in response to determining that a first participant among the participants is selected by the user, marking at least one discussion-participating section of the first participant in the specific conference on a conference timeline of the wide-view video; and</claim-text><claim-text>treating the at least one discussion-participating section of the first participant as the specific time sections.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising:<claim-text>finding out the at least one discussant in the at least one discussion-participating section of the first participant based on the conference event data, and presenting each of the at least one discussant corresponding to each of the discussion-participating sections by a discussant list corresponding to each of the discussion-participating sections.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the plurality of specific time sections in the wide-view video comprises:<claim-text>providing a conference timeline of the wide-view video;</claim-text><claim-text>obtaining a designated number of persons, marking a plurality of first reference time sections on the conference timeline accordingly, wherein a number of the participants or a number of the at least one discussant corresponding to each of the first reference time sections is not less than the designated number of persons; and</claim-text><claim-text>in response to determining that the first reference time sections have a plurality of designated time sections selected by the user, treating the designated time sections as the specific time sections.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the plurality of specific time sections in the wide-view video comprises:<claim-text>providing a conference timeline of the wide-view video;</claim-text><claim-text>obtaining a search keyword, marking a plurality of second reference time sections on the conference timeline accordingly, wherein the speech content of the participants in each of the second reference time sections comprises the search keyword; and</claim-text><claim-text>in response to determining that the second reference time sections have a plurality of designated time sections selected by the user, treating the designated time sections as the specific time sections.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the specific time sections comprise an i<sup>th </sup>specific time section, i is a positive integer, and the step of arranging the discussion image and the speech content of each of the at least one discussant in each of the specific time sections into the corresponding discussion video clip comprises:<claim-text>finding out at least one first discussant belonging to the i<sup>th </sup>specific time section, finding out a first discussion image and first speech content of each of the at least one first discussant in the i<sup>th </sup>specific time section; and</claim-text><claim-text>arranging the first discussion image and the first speech content of each of the at least one first discussant into the discussion video clip corresponding to the i<sup>th </sup>specific time section according to a designated image layout, wherein the designated image layout comprises a speech content frame and at least one first image frame corresponding to the at least one first discussant, wherein each of the at least one first image frame presents the first discussion image of each of the at least one first discussant, and the speech content frame comprises the first speech content of each of the at least one first discussant, and the first speech content of each of the at least one first discussant is sorted in the speech content frame according to a speech time of each of the at least one first discussant.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein in response to determining that a second discussant in the at least one first discussant does not have the corresponding first discussion image in a sub-time section in the i<sup>th </sup>specific time section, the method further comprises:<claim-text>finding out a third image frame corresponding to the second discussant from at least one second image frame; and</claim-text><claim-text>displaying the third image frame as a predetermined image or a blank image in the sub-time section or adjusting the sub-time section to another image layout not comprising the third image frame.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the specific time sections comprise an i<sup>th </sup>specific time section, i is a positive integer, and the method further comprises:<claim-text>finding out at least one third discussant belonging to the i<sup>th </sup>specific time section, finding out a first discussion image and first speech content of each of the at least one third discussant in the i<sup>th </sup>specific time section;</claim-text><claim-text>in response to determining that an anchor point is inserted in the i<sup>th </sup>specific time section, sequentially dividing the i<sup>th </sup>specific time section into a first sub-time section and a second sub-time section according to the anchor point;</claim-text><claim-text>arranging the first discussion image and the first speech content of each of the at least one third discussant in the first sub-time section into a first sub-discussion video clip corresponding to the first sub-time section according to a first image layout;</claim-text><claim-text>arranging the first discussion image and the first speech content of each of the at least one third discussant in the second sub-time section into a section sub-discussion video clip corresponding to the second sub-time section according to a second image layout; and</claim-text><claim-text>sequentially splicing the first sub-discussion video clip and the second sub-discussion video clip into the discussion video clip corresponding to the i<sup>th </sup>specific time section.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising:<claim-text>inserting a transition animation at an ending segment of the first sub-discussion video clip, wherein the transition animation is used to convert the first image layout to the second image layout.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>arranging the speech content of each of the participants into an editable verbatim script according to the conference event data.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the specific time sections comprise an i<sup>th </sup>specific time section and an (i+1)<sup>th </sup>specific time section, i is a positive integer, and the step of organizing the discussion video clip corresponding to each of the specific time sections into the conference recording video corresponding to the specific conference comprises:<claim-text>setting the discussion video clip corresponding to the i<sup>th </sup>specific time section to a first frame rate, and setting the discussion video clip corresponding to the (i+1)<sup>th </sup>specific time section to a second frame rate, wherein the first frame rate is different from the second frame rate.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the plurality of specific time sections in the wide-view video comprises:<claim-text>identifying a specific participant of the specific conference from the participants, providing a discussion-participating timeline of the specific participant in the specific conference, wherein the discussion-participating timeline of the specific participant is marked with at least one discussion-participating section of the specific participation in the specific conference; and</claim-text><claim-text>finding out a plurality of designated time sections from the at least one discussion-participating section of the specific participant, treating the designated time sections as the specific time sections, wherein a time difference between each of the designated time sections is greater than a time threshold.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the step of determining the plurality of specific time sections in the wide-view video comprises:<claim-text>providing a conference timeline of the wide-view video;</claim-text><claim-text>obtaining a designated number of persons, marking a plurality of first reference time sections on the conference timeline accordingly, wherein a number of the participants or a number of the at least one discussant corresponding to each of the first reference time sections is not less than the designated number of persons; and</claim-text><claim-text>treating the first reference time sections as the specific time sections.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a viewing angle of the wide-view video is greater than or equal to 180 degrees.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>marking a plurality of persons of interest and/or objects of interest in the wide-view video, wherein the plurality of persons of interest and/or objects of interest at least partially correspond to the plurality of participants;</claim-text><claim-text>selectively labeling at least a part of the plurality of persons of interest and/or objects of interest with a plurality of tags; and</claim-text><claim-text>selectively enabling a user to select the at least a part of the plurality of persons of interest and/or objects of interest corresponding to the plurality of participants.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. A conference viewing device, comprising:<claim-text>a storage circuit, storing a program code; and</claim-text><claim-text>a processor, coupled to the storage circuit, accessing the program code to<claim-text>obtain a wide-view video of a specific conference and a plurality of conference event data associated with the wide-view video, wherein each conference event data corresponds to a time point of the specific conference, and each conference event data records a sound source direction of a speaker at the corresponding time point and an image range of the speaker in the wide-view video;</claim-text><claim-text>obtain individual speech content of a plurality of participants of the specific conference;</claim-text><claim-text>determine a plurality of specific time sections in the wide-view video, find out at least one discussant in each of the specific time sections according to the plurality of conference event data corresponding to each of the specific time sections;</claim-text><claim-text>arrange a discussion image of each of the at least one discussant and the speech content of each of the at least one discussant in each specific time section into a corresponding discussion video clip; and</claim-text><claim-text>organize the discussion video clip corresponding to each of the specific time sections into a conference recording video corresponding to the specific conference.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>