<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004743A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004743</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17305261</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>187</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00791</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>187</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30252</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10044</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10132</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUTOMOTIVE LOCALIZATION AND MAPPING IN LOW-LIGHT ENVIRONMENT TECHNICAL FIELD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Continental Automotive Systems, Inc.</orgname><address><city>Auburn Hills</city><state>MI</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Ramirez Llanos</last-name><first-name>Eduardo Jose</first-name><address><city>Rochester</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Bolduc</last-name><first-name>Andrew Phillip</first-name><address><city>Rochester Hills</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ip</last-name><first-name>Julien</first-name><address><city>Madison Heights</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Verma</last-name><first-name>Dhiren</first-name><address><city>Farmington Hills</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Yu</last-name><first-name>Xin</first-name><address><city>Rochester Hills</city><state>MI</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Continental Automotive Systems, Inc.</orgname><role>02</role><address><city>Auburn Hills</city><state>MI</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A localization and mapping system and method for a motor vehicle is disclosed and includes at least one camera configured to obtain images of an environment surrounding the motor vehicle, at least one sensor configured to obtain location information for objects surrounding the motor vehicle and a controller configured to receive the images captured by the at least one camera and the location information obtained by the at least one sensor. The controller enhances the captured images utilizing a neural network and combines the enhanced images with the location information to localize the vehicle within the mapped environment.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.10mm" wi="158.75mm" file="US20230004743A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="231.99mm" wi="121.84mm" orientation="landscape" file="US20230004743A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="237.15mm" wi="166.37mm" orientation="landscape" file="US20230004743A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="245.70mm" wi="143.51mm" orientation="landscape" file="US20230004743A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="253.41mm" wi="158.24mm" orientation="landscape" file="US20230004743A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to a system and method for localizing and mapping an environment surrounding a vehicle.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Autonomous and semi-autonomous vehicles continually gather and update information to determine vehicle position and orientation. In some instances, the vehicle is operated in unfamiliar, unknown and low-light environments. An unknown, low-light environment presents challenges to vehicle sensors and localization algorithms. Images taken in low-light environments may not provide a clear picture of the surrounding environment that is useful for locating and orientating a vehicle.</p><p id="p-0004" num="0003">The background description provided herein is for the purpose of generally presenting a context of this disclosure. Work of the presently named inventors, to the extent it is described in this background section, as well as aspects of the description that may not otherwise qualify as prior art at the time of filing, are neither expressly nor impliedly admitted as prior art against the present disclosure.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">A localization and mapping system for a motor vehicle according to a disclosed example embodiment includes, among other possible things, at least one camera configured to obtain images of an environment surrounding the motor vehicle, at least one sensor configured to obtain location information for objects surrounding the motor vehicle and a controller configured to receive the images captured by the at least one camera and the location information obtained by the at least one sensor, to enhance the captured images utilizing a neural network to produce an enhanced image, combine the enhanced images with the location information and localize the vehicle based on the combined enhanced images and location information.</p><p id="p-0006" num="0005">In another exemplary embodiment of the foregoing localization and mapping system the neural network comprises a database with low-light images and corresponding ground truth images.</p><p id="p-0007" num="0006">In another exemplary embodiment of any of the foregoing localization and mapping systems the controller is configured to apply a weighting factor to a loss function applied to a sequence of images for enhancing the images.</p><p id="p-0008" num="0007">In another exemplary embodiment of any of the foregoing localization and mapping systems, the weighting factor is applied to a pixel-wise mean squared error loss for enhancement of the captured images.</p><p id="p-0009" num="0008">In another exemplary embodiment of any of the foregoing localization and mapping systems, the weighting factor is biased toward application of enhancements based on more recent comparisons between captured images and a ground truth image.</p><p id="p-0010" num="0009">In another exemplary embodiment of any of the foregoing localization and mapping systems, an output enhanced image is combined with the localization information to generate a map of an environment surrounding the motor vehicle.</p><p id="p-0011" num="0010">In another exemplary embodiment of any of the foregoing localization and mapping systems, the at least one sensor comprises at least one radar sensing device mounted on the motor vehicle.</p><p id="p-0012" num="0011">In another exemplary embodiment of any of the foregoing localization and mapping systems, the at least one sensor comprises at least one ultrasonic sensor mounted on the vehicle.</p><p id="p-0013" num="0012">Another exemplary embodiment of any of the foregoing localization and mapping systems further includes at least one sensor generating information regarding a vehicle operating parameter and the controller is further configured to combine information regarding the vehicle operating parameters and the enhanced images to localize the vehicle.</p><p id="p-0014" num="0013">In another exemplary embodiment of any of the foregoing localization and mapping systems, the controller is configured to implement a simultaneous localization and mapping algorithm utilizing the enhanced images, information on the location of objects surrounding the vehicle and information regarding vehicle operating parameters to localize and map an environment around the motor vehicle.</p><p id="p-0015" num="0014">A method for localization and mapping for a vehicle in a low-light environment according to another disclosed example embodiment includes, among other possible things, obtaining a sequence of images of an environment surrounding a vehicle, enhancing low-light images with a low-light image enhancement model to generate a sequence of enhanced images, and generating a map of an environment surrounding the vehicle based on the sequence of enhanced images.</p><p id="p-0016" num="0015">Another example embodiment of the foregoing method further includes generating the low-light image enhancement model for enhancing low-light images with neural network receiving comparing low-light images and ground truth images.</p><p id="p-0017" num="0016">Another example embodiment of any of the foregoing methods further includes applying a weight factor that biases application of correction factors to those formed from more current images.</p><p id="p-0018" num="0017">In another example embodiment of any of the foregoing methods, the weight factor applies more importance to current images and disregards past images.</p><p id="p-0019" num="0018">Another example embodiment of any of the foregoing methods further includes obtaining information from at least one sensor configured to obtain location information for objects surrounding the motor vehicle and combining the location information with the enhanced images to generate the map of the environment surrounding the vehicle.</p><p id="p-0020" num="0019">Another example embodiment of any of the foregoing methods further includes receiving at least one sensor generating information regarding a vehicle operating parameter and combining information regarding the vehicle operating parameters and the enhanced images to localize the vehicle.</p><p id="p-0021" num="0020">Although the different examples have the specific components shown in the illustrations, embodiments of this disclosure are not limited to those particular combinations. It is possible to use some of the components or features from one of the examples in combination with features or components from another one of the examples.</p><p id="p-0022" num="0021">These and other features disclosed herein can be best understood from the following specification and drawings, the following of which is a brief description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic representation of a vehicle with a localization and mapping system.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is schematic view of captured input images and enhanced output images.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram illustrating enhancement of input images with a neural network.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram of an example method of generating a model for enhancing low-light images.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram of an example method of localizing and mapping an environment surrounding a vehicle utilizing an enhanced sequence of images.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a vehicle <b>20</b> is shown schematically and includes a system <b>25</b> for localizing and mapping the surrounding environment and the vehicles position and orientation in that environment. Vehicles include an increasing number of autonomous and/or semi-autonomous driver assist features. Driver assist features utilize environmental information to operate. The example disclosed system <b>25</b> embodiment enhances sequences of images obtained in low-light settings to provide useful information for localizing and mapping the environment surrounding the vehicle without significantly increasing required processing capabilities.</p><p id="p-0029" num="0028">The example vehicle <b>20</b> includes cameras <b>24</b> and at least one other sensing device. In the disclosed example, the sensing device includes radar devices <b>22</b> that are disposed at different locations around the vehicle. The cameras <b>24</b> and radar devices <b>22</b> provide information to a controller <b>28</b>. Although radar devices 22 are shown by way of example, other sensing devices may also be utilized within the contemplation and scope of this disclosure.</p><p id="p-0030" num="0029">The images captured by the cameras <b>24</b> may not provide as much useful information as possible in low-light environments. The example controller <b>28</b> includes an algorithm that enhances captured low-light image sequences. The enhanced images are then utilized by location and mapping algorithms such as for example a simultaneous localization and mapping algorithm (SLAM) <b>34</b>. The SLAM <b>34</b> uses the enhanced images to generate mapping information along with the information captured from other vehicle sensors and information gathering systems and devices.</p><p id="p-0031" num="0030">The controller <b>28</b> may be a portion of an overall vehicle controller and/or a dedicated controller for the example system <b>25</b>. The controller <b>28</b> is configured to process information received from the radar devices <b>22</b>, cameras <b>24</b>, global positioning system device <b>30</b> and information from various vehicle operating systems indicated schematically at <b>26</b> and determine a location and orientation of the vehicle <b>20</b> within the surrounding environment.</p><p id="p-0032" num="0031">The SLAM <b>25</b> algorithm executed by the controller <b>28</b> of the vehicle <b>20</b>. The controller <b>28</b> is schematically shown and includes at least a processing device and a memory device <b>36</b>. The controller <b>28</b> may be a hardware device for executing software, particularly software stored in the memory <b>36</b>. The processing device can be a custom made or commercially available processor, a central processing unit (CPU), an auxiliary processor among several processors associated with the computing device, a semiconductor based microprocessor (in the form of a microchip or chip set) or generally any device for executing software instructions.</p><p id="p-0033" num="0032">The memory <b>36</b> can include any one or combination of volatile memory elements (e.g., random access memory (RAM, such as DRAM, SRAM, SDRAM, VRAM, etc.)) and/or nonvolatile memory elements. Moreover, the memory <b>36</b> may incorporate electronic, magnetic, optical, and/or other types of storage media. Note that the memory can also have a distributed architecture, where various components are situated remotely from one another, but can be accessed by the processor.</p><p id="p-0034" num="0033">The software in the memory <b>36</b> may include one or more separate programs, each of which includes an ordered listing of executable instructions for implementing disclosed logical functions and operation. A system component embodied as software may also be construed as a source program, executable program (object code), script, or any other entity comprising a set of instructions to be performed. When constructed as a source program, the program is translated via a compiler, assembler, interpreter, or the like, which may or may not be included within the memory.</p><p id="p-0035" num="0034">Input/Output devices (not shown) that may be coupled to system I/O Interface(s) may include input devices, for example but not limited to, a keyboard, mouse, scanner, microphone, camera, proximity device, etc. Further, the Input/Output devices may also include output devices, for example but not limited to, a printer, display, etc. Finally, the Input/Output devices may further include devices that communicate both as inputs and outputs, for instance but not limited to, a modulator/demodulator (modem; for accessing another device, system, or network), a radio frequency (RF) or other transceiver, a telephonic interface, a bridge, a router, etc.</p><p id="p-0036" num="0035">The controller <b>28</b> may control vehicle systems to either autonomously control the vehicle <b>20</b> or provide driver assist functions to aid an operator of the vehicle <b>20</b>.</p><p id="p-0037" num="0036">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref> with continued reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, an example low-light images <b>38</b> is shown and input into a neural network model <b>32</b> for enhancement. The neural network model <b>32</b> utilizes visual enhancement techniques to adjust the image, pixel by pixel. The resulting output enhanced image <b>40</b> provides more contrast and clarity of the objects within the images and proximate the vehicle <b>20</b>. The neural network model <b>32</b> is built utilizing past and current images along with corresponding ground truth images. The function and features required to enhance the low-light images are learned through the use of matched pairs of low-light and ground truth images and then applied to current images to obtain output enhanced images.</p><p id="p-0038" num="0037">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, with continued reference to <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, a convolutional neural network <b>42</b> is used to generate the model <b>32</b> for enhancement of the captured low-light images. The input into the convolutional neural network <b>42</b> includes matched pairs of low-light images <b>44</b> and ground truth images <b>46</b>. The disclosed image enhancement operates according to the following terms:</p><p id="p-0039" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>L</i><sub>total</sub><sup>k</sup>=<i>L</i><sub>MSE</sub><sup>k</sup>+<i>&#x3b1;L</i><sub>c</sub><sup>k </sup><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0040" num="0038">In the above equation, L<sub>total </sub>is the loss function to be minimized by the convolutional neural network. The term is the pixel-wise mean squared error loss that is utilized for image enhancement. It minimizes the error between the training data (e.g., low-light image) and the ground truth. This term is given by</p><p id="p-0041" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <msubsup>   <mi>L</mi>   <mi>MSE</mi>   <mi>k</mi>  </msubsup>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mi>WH</mi>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <msubsup>     <mo>&#x2211;</mo>     <mrow>      <mi>x</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>W</mi>    </msubsup>    <mrow>     <msubsup>      <mo>&#x2211;</mo>      <mrow>       <mi>y</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>H</mi>     </msubsup>     <mrow>      <msup>       <mrow>        <mo>(</mo>        <mrow>         <msubsup>          <mi>I</mi>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mrow>           <mi>k</mi>           <mo>,</mo>           <mi>G</mi>          </mrow>         </msubsup>         <mo>-</mo>         <msubsup>          <mi>I</mi>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>          <mi>k</mi>         </msubsup>        </mrow>        <mo>)</mo>       </mrow>       <mn>2</mn>      </msup>      <mo>.</mo>     </mrow>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0042" num="0039">In the above equation, Wand H are the width and height of the input images. I<sub>x,y</sub><sup>k </sup>I<sub>x,y</sub><sup>k,G </sup>and are the pixel value of the output image and the ground truth images at time k. The term L<sub>c</sub><sup>k </sup>is the loss function that captures higher level features to improve the quality of the prediction, which is given by</p><p id="p-0043" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msubsup>    <mi>L</mi>    <mi>c</mi>    <mi>k</mi>   </msubsup>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mrow>      <msub>       <mi>W</mi>       <mi>&#x3d5;</mi>      </msub>      <mo>&#x2062;</mo>      <msub>       <mi>H</mi>       <mi>&#x3d5;</mi>      </msub>     </mrow>    </mfrac>    <mo>&#x2062;</mo>    <mrow>     <munderover>      <mo>&#x2211;</mo>      <mrow>       <mi>x</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <msub>       <mi>W</mi>       <mi>&#x3d5;</mi>      </msub>     </munderover>     <mtext> </mtext>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>y</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <msub>        <mi>H</mi>        <mi>&#x3d5;</mi>       </msub>      </munderover>      <mtext> </mtext>      <msup>       <mrow>        <mo>(</mo>        <mrow>         <msub>          <mrow>           <mi>&#x3d5;</mi>           <mo>(</mo>           <msup>            <mi>I</mi>            <mrow>             <mi>k</mi>             <mo>,</mo>             <mi>G</mi>            </mrow>           </msup>           <mo>)</mo>          </mrow>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>         </msub>         <mo>-</mo>         <msub>          <mrow>           <mi>&#x3d5;</mi>           <mo>(</mo>           <msup>            <mi>I</mi>            <mi>k</mi>           </msup>           <mo>)</mo>          </mrow>          <mrow>           <mi>x</mi>           <mo>,</mo>           <mi>y</mi>          </mrow>         </msub>        </mrow>        <mo>)</mo>       </mrow>       <mn>2</mn>      </msup>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0044" num="0000">where &#x3d5; is the feature map produced by a convolutional layer, W<sub>&#x3d5;</sub>and H<sub>99</sub> are the width and height of the feature map for a given image l<sup>k</sup>. Notice that the term L<sub>c</sub><sup>k </sup>penalizes the error in regions instead of penalizing the error in pixels as done by the term L<sub>MSE</sub><sup>k</sup>.</p><p id="p-0045" num="0040">The loss function above does not weight any of the images. The disclosed example enhancement weights the images according to the below equation.</p><p id="p-0046" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mi>L</mi>  <mo>=</mo>  <mrow>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>k</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mrow>      <mi>k</mi>      <mo>-</mo>      <mi>n</mi>     </mrow>    </munderover>    <mtext> </mtext>    <mrow>     <msub>      <mi>w</mi>      <mi>k</mi>     </msub>     <mo>&#x2062;</mo>     <mrow>      <mo>(</mo>      <mrow>       <msubsup>        <mi>L</mi>        <mi>MSE</mi>        <mi>k</mi>       </msubsup>       <mo>+</mo>       <msubsup>        <mi>aL</mi>        <mi>c</mi>        <mi>k</mi>       </msubsup>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mrow>   <mo>=</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>k</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mrow>      <mi>k</mi>      <mo>-</mo>      <mi>n</mi>     </mrow>    </munderover>    <mtext> </mtext>    <mrow>     <msub>      <mi>w</mi>      <mi>k</mi>     </msub>     <mo>&#x2062;</mo>     <msubsup>      <mi>L</mi>      <mi>total</mi>      <mi>k</mi>     </msubsup>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0047" num="0041">The Lisa weighted sum of L<sub>total</sub><sup>k </sup>over the last number of images n.</p><p id="p-0048" num="0042">The weighted sum of the last number of images smooths out variations in pixel intensities for the sequence of images from k to k-n. The weight factor w<sub>k </sub>is a scaler value chosen to give more importance to current images and to disregard much older images. The weighting of more current images and the disregard and/or disposal of older images reduces the processor system requirements and thereby costs.</p><p id="p-0049" num="0043">Referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref> with continued reference to <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>3</b></figref>, generation of the neural network model <b>32</b> is schematically shown. Low-light images <b>44</b> and corresponding ground truth images <b>46</b> are input into the convolutional neural network <b>42</b> for training and development of the neural network model for low-light image enhancement shown at <b>32</b>. The convolutional neural network applies the lost function along with the weight factor to continually update and improve the model <b>32</b>. The continual improvement of the model may occur during an initial training process and continue throughout operation of the vehicle.</p><p id="p-0050" num="0044">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref> with continued reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the example disclosed method fuses images from the cameras <b>24</b> with information from the GPS <b>30</b>, radar devise and vehicle sensors <b>26</b> to generate and localize the vehicle <b>20</b> in a self-generated map. During low-light conditions, low-light images <b>58</b> from the cameras <b>24</b> are processed through the neural network model <b>32</b> to produce enhanced images <b>60</b>. The enhanced images <b>60</b> are communicated to the SLAM algorithm <b>34</b> for generation of the localized map <b>64</b> and an orientation and position of the vehicle within that map <b>64</b> as indicated at <b>62</b>. The vehicle orientation and position and heading along with any other positional and dynamic vehicle directional information needed for navigation and operation of driver assist features.</p><p id="p-0051" num="0045">Accordingly, the disclosed example localization and mapping system enhances low-light images to improve useful information to orientate and position a vehicle in unfamiliar, low-light environments.</p><p id="p-0052" num="0046">Although the different non-limiting embodiments are illustrated as having specific components or steps, the embodiments of this disclosure are not limited to those particular combinations. It is possible to use some of the components or features from any of the non-limiting embodiments in combination with features or components from any of the other non-limiting embodiments.</p><p id="p-0053" num="0047">It should be understood that like reference numerals identify corresponding or similar elements throughout the several drawings. It should be understood that although a particular component arrangement is disclosed and illustrated in these exemplary embodiments, other arrangements could also benefit from the teachings of this disclosure.</p><p id="p-0054" num="0048">The foregoing description shall be interpreted as illustrative and not in any limiting sense. A worker of ordinary skill in the art would understand that certain modifications could come within the scope of this disclosure. For these reasons, the following claims should be studied to determine the true scope and content of this disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004743A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230004743A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004743A1-20230105-M00002.NB"><img id="EMI-M00002" he="9.14mm" wi="76.20mm" file="US20230004743A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004743A1-20230105-M00003.NB"><img id="EMI-M00003" he="8.13mm" wi="76.20mm" file="US20230004743A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A localization and mapping system for a motor vehicle comprising:<claim-text>at least one camera configured to obtain images of an environment surrounding the motor vehicle;</claim-text><claim-text>at least one sensor configured to obtain location information for objects surrounding the motor vehicle; and</claim-text><claim-text>a controller configured to receive the images captured by the at least one camera and the location information obtained by the at least one sensor, to enhance the captured images utilizing a neural network to produce an enhanced image, combine the enhanced images with the location information and localize the vehicle based on the combined enhanced images and location information.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The localization and mapping system as recited in clam <b>1</b>, wherein the neural network comprises a database with low-light images and corresponding ground truth images.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The localization and mapping system as recited in clam <b>2</b>, wherein the controller is configured to apply a weighting factor to a loss function applied to a sequence of images for enhancing the images.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the weighting factor is applied to a pixel-wise mean squared error loss for enhancement of the captured images.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the weighting factor is biased toward application of enhancements based on more recent comparisons between captured images and a ground truth image.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein an output enhanced image is combined with the localization information to generate a map of an environment surrounding the motor vehicle.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one sensor comprises at least one radar sensing device mounted on the motor vehicle.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one sensor comprises at least one ultrasonic sensor mounted on the vehicle.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00001">claim 1</claim-ref>, including at least one sensor generating information regarding a vehicle operating parameter and the controller is further configured to combine information regarding the vehicle operating parameters and the enhanced images to localize the vehicle.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The localization and mapping system as recited in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the controller is configured to implement a simultaneous localization and mapping algorithm utilizing the enhanced images, information on the location of objects surrounding the vehicle and information regarding vehicle operating parameters to localize and map an environment around the motor vehicle.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A localization and mapping method for a vehicle in a low-light environment, the method comprising:<claim-text>obtaining a sequence of images of an environment surrounding a vehicle;</claim-text><claim-text>enhancing low-light images with a low-light image enhancement model to generate a sequence of enhanced images; and</claim-text><claim-text>generating a map of an environment surrounding the vehicle based on the sequence of enhanced images.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method as recited in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising generating the low-light image enhancement model for enhancing low-light images with neural network receiving comparing low-light images and ground truth images.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method as recited in <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising applying a weight factor that biases application of correction factors to those formed from more current images.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as recited in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the weight factor applies more importance to current images and disregards past images.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as recited in <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising obtaining information from at least one sensor configured to obtain location information for objects surrounding the motor vehicle and combining the location information with the enhanced images to generate the map of the environment surrounding the vehicle.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method as recited in <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising receiving at least one sensor generating information regarding a vehicle operating parameter and combining information regarding the vehicle operating parameters and the enhanced images to localize the vehicle.</claim-text></claim></claims></us-patent-application>