<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001587A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001587</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782317</doc-number><date>20201204</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>13</main-group><subgroup>08</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>13</main-group><subgroup>084</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>1676</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>13</main-group><subgroup>086</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>1697</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">METHOD AND SETUP FOR FENCELESS ROBOTICS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62944029</doc-number><date>20191205</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><last-name>ZAK</last-name><first-name>Alexander</first-name><address><city>Troy</city><state>MI</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only"><addressbook><last-name>HARTLIEB</last-name><first-name>Johannes</first-name><address><city>Semriach</city><country>AT</country></address></addressbook><residence><country>AT</country></residence></us-applicant><us-applicant sequence="02" app-type="applicant" designation="us-only"><addressbook><orgname>MAGNA INERNATIONAL INC.</orgname><address><city>Aurira, Ontario</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZAK</last-name><first-name>Alexander</first-name><address><city>Troy</city><state>MI</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>HARTLIEB</last-name><first-name>Johannes</first-name><address><city>Semriach</city><country>AT</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US2020/063234</doc-number><date>20201204</date></document-id><us-371c12-date><date>20220603</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A fenceless system and method for automatically moving one or more items between a structure at a source location and a destination using a robot is provided. The system comprises a robot having an end effector to selectively grasp an item. A trajectory planning controller directs the robot to move the item between a source location and a destination. A touch sensor detects a contact between an external object and a surface of the robot or a surface surrounding the end effector; and a proximity sensor detects a person in proximity to the robot. A vision sensor detects a location and orientation of items to be moved. The robot moves in proximity to a person without a safety fence preventing the person from contacting the robot. The system adjusts a speed of the robot in response to detecting a person in one of a plurality of zones around the robot.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="82.13mm" wi="158.75mm" file="US20230001587A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="246.63mm" wi="128.61mm" orientation="landscape" file="US20230001587A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="249.51mm" wi="129.20mm" orientation="landscape" file="US20230001587A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="228.94mm" wi="154.01mm" file="US20230001587A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="250.27mm" wi="129.29mm" orientation="landscape" file="US20230001587A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="240.11mm" wi="154.26mm" orientation="landscape" file="US20230001587A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="256.46mm" wi="164.85mm" orientation="landscape" file="US20230001587A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="227.92mm" wi="170.10mm" orientation="landscape" file="US20230001587A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="219.46mm" wi="167.13mm" orientation="landscape" file="US20230001587A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="263.99mm" wi="155.11mm" orientation="landscape" file="US20230001587A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="258.66mm" wi="170.43mm" orientation="landscape" file="US20230001587A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="257.13mm" wi="156.21mm" orientation="landscape" file="US20230001587A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="245.87mm" wi="175.01mm" orientation="landscape" file="US20230001587A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="185.25mm" wi="177.46mm" orientation="landscape" file="US20230001587A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="212.77mm" wi="171.70mm" orientation="landscape" file="US20230001587A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This PCT International Patent Application claims the benefit of U.S. Provisional Patent Application Ser. No. 62/944,029 filed on Dec. 5, 2019, and titled &#x201c;Method And Setup For Fenceless Robotics&#x201d;, the entire disclosure of which is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Assembly processes (or lines) are implemented to produce a finished good. The finished goods are a combination of various parts that are attached together through various techniques. A finished good may be any sort of object or product, for example, those sold in commerce. An automobile or vehicle, or a part of an automobile or vehicle, may be a finished good produced via an assembly process. Many aspects of a manufacturing process involve moving items, such as individual parts, assemblies, or carriers holding one or more individual parts or assemblies for transport or for processing together.</p><p id="p-0004" num="0003">Many finished goods include parts from a variety of sources, which are transported to and within manufacturing locations to be combined into finished goods or into assemblies or sub-assemblies thereof. These parts are frequently transported in bins, where they may be loose, having random locations and orientations. Parts must be transferred from the bins to a destination to facilitate the use of the part in the manufacturing process. Parts are also frequently processed in batches using specialized carriers such as baskets, bins, or paint bucks, which must be loaded and unloaded with individual parts or assemblies. Also, in some cases, movement of the carriers requires special handling due to the requirements of the manufacturing process and/or other considerations such as weight and/or size of the loaded or unloaded carriers.</p><p id="p-0005" num="0004">For many portions of a manufacturing process, the current technique of moving items is a manual process. Special considerations are required in moving large and/or heavy items. For example, stamped metal parts are commonly picked by a human from a bin and placed into a fixture for further processing (e.g., cutting, welding, adhesion, or painting). After processing, the human may pick the parts from a fixture and place them into a bin, which may be a new bin or the same bin from which the parts originated. During the pick/place operations, the human may also perform quality checks on the part.</p><p id="p-0006" num="0005">Robots have been used to automate pick and place operations, especially in places where the items are too heavy for a person to easily lift or where the item or the area presents hazards, such as high temperatures or high noises. Conventionally, fences, such as electronic fences have been used to delineate a robot cell where robots can operate without risk of hitting a person or another object. However, fenced robot cells create a number of costs including costs in maintaining safety and monitoring equipment and costs in production floor space.</p><p id="p-0007" num="0006">Some human-safe robot technologies have been developed to allow robots to move in the presence of humans and without needing a fenced robot cell. Such human-safe technologies may include, for example, touch sensors that stop the robot in response to contact with an external object, such as a person. Conventional human-safe robots generally require relatively low speed operation (e.g., about 150-200 mm/s) and relatively low payload capacities (e.g., about 30 Kg or less). These restrictions on speeds and payload capacities limit the kinetic energies produced by the movement of payloads in order to minimize risks and types of injuries to a human in the event of inadvertent contact.</p><p id="p-0008" num="0007">However, those restrictions on speed and payload capacities make conventional human-safe robot technologies unsuitable for many robotic applications that require higher payloads and/or higher speeds in order to meet production requirements.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0009" num="0008">A fenceless conveyance system and method for automatically moving one or more items between a source location and a destination using a robot is provided.</p><p id="p-0010" num="0009">The fenceless conveyance system comprises a robot having an end effector configured to selectively grasp an item. The system also comprises a trajectory planning controller configured to direct the robot to move the item between a source location and a destination. The system also comprises a touch sensor configured to detect a contact between an external object and a surface of the robot or a surface surrounding the end effector; and a proximity sensor configured to detect a person in proximity to the robot. The robot is configured to move in proximity to a person without a safety fence preventing the person from contacting the robot.</p><p id="p-0011" num="0010">The method for moving an item comprises picking an item from a source location by an end effector on a robot; moving the item along a path to a destination by the robot; placing the item at the destination by the end effector on the robot; and stopping the robot from moving in response to detecting contact between an external object and a surface of the robot or a surface surrounding the end effector. The robot is configured to move in proximity to a person without a safety fence preventing the person from contacting the robot.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011">The detailed description refers to the following drawings, in which like numerals refer to like items, and in which:</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a system for automatically moving one or more parts between a bin at a source location and a destination;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is another schematic diagram of a system for automatically moving one or more parts between a bin at a source location and a destination;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a top view of a machine and bins around a robot for automated loading and unloading of the machine, with each bin being located within a window and without a locating fixture;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of an automated process for loading and unloading of baskets of parts to and from a machine according to some embodiments of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is another schematic diagram of the automated process for loading and unloading of baskets of parts according to some embodiments of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a fenceless robot system according to some embodiments of the present disclosure;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a plan view diagram of the fenceless robot system according to some embodiments of the present disclosure;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is another plan view diagram of the fenceless robot system according to some embodiments of the present disclosure;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>9</b>-<b>10</b></figref> show perspective views of the fenceless robot system according to some embodiments of the present disclosure;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a plan view diagram of a washing machine configured for manual loading and unloading operation;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows a plan view diagram of a washing machine configured for automated loading and unloading using the fenceless robot system of the present disclosure;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows an image of components of the fenceless robot system according to some embodiments of the present disclosure;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b></figref> shows an image of components within the fenceless robot system according to some embodiments of the present disclosure; and</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b></figref> shows an image of the fenceless robot system according to some embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF THE EXAMPLE EMBODIMENTS</heading><p id="p-0027" num="0026">The invention is described more fully hereinafter with references to the accompanying drawings, in which exemplary embodiments of the invention are shown. This invention may, however, be embodied in many different forms and should not be construed as limited to the embodiments set forth herein. Rather, these exemplary embodiments are provided so that this disclosure is thorough, and will fully convey the scope of the invention to those skilled in the art. It will be understood that for the purposes of this disclosure, &#x201c;at least one of each&#x201d; will be interpreted to mean any combination the enumerated elements following the respective language, including combination of multiples of the enumerated elements. For example, &#x201c;at least one of X, Y, and Z&#x201d; will be construed to mean X only, Y only, Z only, or any combination of two or more items X, Y, and Z (e.g., XYZ, XZ, YZ, X). Throughout the drawings and the detailed description, unless otherwise described, the same drawing reference numerals are understood to refer to the same elements, features, and structures. The relative size and depiction of these elements may be exaggerated for clarity, illustration, and convenience.</p><p id="p-0028" num="0027">Referring to the Figures, wherein like numerals indicate corresponding parts throughout the several views, a conveyance system <b>20</b> and method for automatically moving one or more items <b>21</b> between a structure at a source location <b>26</b> and a destination <b>28</b> using a robot <b>30</b> with an end effector <b>32</b> are disclosed.</p><p id="p-0029" num="0028">One example of the conveyance system <b>20</b> of the present disclosure is shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> for automatically moving one or more parts <b>22</b> between a bin <b>24</b> at a source location <b>26</b> and a destination <b>28</b> using a robot <b>30</b> with an end effector <b>32</b>, and where the parts <b>22</b> may be loose or not fixed in specific locations in the bin <b>24</b>. As used in this disclosure, a bin <b>24</b> may include any box, rack, tray, or other carrier for holding parts <b>22</b>. It should be appreciated that the term &#x201c;part&#x201d; <b>22</b> as discussed throughout the subject disclosure, including the claims, may encompass various types of objects including, but not limited to, raw materials, housings, and component pieces in any stage of manufacture, assemblies or sub-assemblies in any stage of construction, and finished pieces or assemblies. A variety of different items <b>21</b> may be accommodated and moved by the same conveyance system <b>20</b>, using the same or different end effectors <b>32</b>. It should also be appreciated that the term &#x201c;item&#x201d; <b>21</b> may refer to a part <b>22</b>, a bin <b>24</b>, or any other physical item including, but not limited to a tool, part, fixture, raw material, housing, component piece in any stage of manufacture, assembly or sub-assembly in any stage of construction, finished pieces or assemblies, a box, rack, tray, or other carrier.</p><p id="p-0030" num="0029">As best shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a first vision system <b>34</b> may identify a part <b>22</b> within the bin <b>24</b> at a source location <b>26</b> and determine a pick location, and a pick orientation of the part <b>22</b>. A second vision system <b>38</b> may determine the location and orientation of a destination <b>28</b>, which may be inside or outside of the bin <b>24</b>. The destination <b>28</b> may be any place where one or more parts are to be moved, including, for example: fixtures or carriers for manufacturing or inspection, shipment, etc.; racks or packages for storage or conveyance; conveyors; fixtures or assemblies in any stage of manufacture. The destination <b>28</b> may be fixed in position and orientation. The destination <b>28</b> may be variable in position and/or orientation, such as for parts being placed on an assembly as it moves along an assembly line. Additionally, the destination <b>28</b> for each of a series of parts <b>22</b> may be different, for example in cases where a rack, or other such assembly is loaded with a plurality of parts <b>22</b>, with each part <b>22</b> in a separate compartment or location on the rack.</p><p id="p-0031" num="0030">Each of the vision systems <b>34</b>, <b>38</b> may be any type of machine vision system, including one or more cameras <b>36</b> or other imaging devices and including but not limited to 2D, 2.5D, and 3D systems capable of identifying and locating a part <b>22</b> in 3-dimensional space, having x, y, and z coordinates, as well as a 3-dimensional orientation of roll, pitch, and yaw. One example of such a machine vision system is the camera system manufactured by Cognex. Such identifying and locating may be done using direct observations and measurements, through comparisons with one or more reference images, through any other method or combination of methods.</p><p id="p-0032" num="0031">The conveyance system <b>20</b> includes a robot <b>30</b> having an end effector <b>32</b> to pick the part <b>22</b> from the bin <b>24</b>, move the part <b>22</b> along a path <b>40</b>, and place the part <b>22</b> at the destination <b>28</b>. The end effector <b>32</b> may be an advanced effector (e.g., tooling), or any other effector capable of moving an item <b>21</b> including, but not limited to, a grasp, clamp, and a suction device. The system also includes a trajectory planning controller <b>42</b> for planning a best path <b>40</b> for the robot <b>30</b> to follow in moving the item <b>21</b> between the source location <b>26</b> and the destination <b>28</b>.</p><p id="p-0033" num="0032">Each of the vision systems <b>34</b>, <b>38</b> may include one or more cameras <b>36</b> located at fixed positions, as shown on <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Alternatively or additionally, the first vision system <b>34</b> may include a camera <b>36</b> that is located on the robot <b>30</b>, as shown on <figref idref="DRAWINGS">FIG. <b>2</b></figref>. More specifically, the camera <b>36</b> may be located on or near a free or distal end of the robot <b>30</b>. The camera <b>36</b> may be located on the end effector <b>32</b> of the robot <b>30</b> or on another part of the robot <b>30</b>, such as a joint or a structural component near the end effector <b>32</b>. Such a robot-mounted camera <b>36</b> may be used instead of or in addition to one or more cameras <b>36</b> at fixed positions. Alternatively or additionally, the second vision system <b>38</b> may include a camera <b>36</b> that is located on the robot <b>30</b>. In one embodiment, the vision systems <b>34</b>, <b>38</b> may share one or more cameras <b>36</b> that are mounted on the robot <b>30</b>. In other words, the vision systems <b>34</b>, <b>38</b> may each be configured to use a shared camera mounted on the robot. Such a configuration may include one of the vision systems <b>34</b>, <b>38</b> passing an image signal from the shared camera to the other one the vision systems <b>34</b>, <b>38</b>. Alternatively, an image from the shared camera may be provided to each of the vision systems <b>34</b>, <b>38</b> from the shared camera or from another device, such as a signal splitter.</p><p id="p-0034" num="0033">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, an example application of the conveyance system <b>20</b> of the present invention may be to replace a manual operation of loading and unloading vehicle fascias onto paint bucks, or racks used in paint processes. For example, the process of loading the paint bucks may require a crew, with persons alternating between picking from the walk-in bin (at floor level) and placing the parts in a buck (at hip level) and by transferring the part to each other in order to relieve the ergonomic stressors. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the conveyance system <b>20</b> of the present invention may replace the manual loading and unloading of vehicle fascias to and from paint bucks and may allow the combined operation to be performed with fewer persons per shift. The bins <b>24</b> may be located within a general window area, which may be a predetermined tolerance value from a predetermined nominal position or boundary. The bins <b>24</b> may not need to be secured or placed in an exact location, and therefore may not require locating fixtures. The parts <b>22</b> may be fixed within the bins <b>24</b>, such as by fixtures formed in the bins <b>24</b>, and the number of parts within a bin <b>24</b> may vary. The conveyance system <b>20</b> of the present disclosure may accommodate several different types of parts, such as for different vehicle models. For example, a conveyance system <b>20</b> may accommodate <b>17</b> or more different types of parts. According to an aspect, the conveyance system <b>20</b> may require both the source and the destination to be stationary. Alternatively, the conveyance system <b>20</b> may allow the loading and unloading of bucks which are moving in up to two different directions simultaneously, such as may result from being moved along a curved segment of conveyor track. The conveyance system <b>20</b> of the present disclosure may provide for faster and/or more consistent cycle times in loading or unloading parts <b>22</b> when compared to the manual loading and unloading operations of the prior art and may allow for a direct labor reduction from 5 persons per shift to 1 person per shift.</p><p id="p-0035" num="0034">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the conveyance system <b>20</b> may be used to control a robot <b>30</b> to move one or more parts <b>22</b> into and out of a machine <b>44</b>. The robot <b>30</b> may pick one or more parts <b>22</b> from a source location <b>26</b>, which may be, for example, a first bin <b>24</b> holding raw, or unfinished parts <b>22</b>, and carry the parts <b>22</b> to the machine <b>44</b> for processing, after which the robot <b>30</b> may remove pick the finished parts <b>22</b> to a destination <b>28</b>, which may be, for example, a second bin <b>24</b> for transporting the finished parts <b>22</b> to another area. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the robot <b>30</b> may load and unload right-hand (RH) and left-hand (LH) parts <b>22</b> for simultaneous processing by the machine <b>44</b>. The conveyance system <b>20</b> may accommodate some variation in the placement of the bins <b>24</b> used for the source location <b>26</b> and the destination <b>28</b>. Such variation may allow the source and destination bins <b>26</b>, <b>28</b> to be located anywhere within a window of space in each direction from a nominal position. Therefore, the bins <b>24</b> do not need to be secured in a precise location and may not require a locating fixture. The robot <b>30</b> may accommodate for variations in the location and tolerances of the parts <b>22</b>. According to an aspect, the conveyance system <b>20</b> may inspect the finished parts <b>22</b>, to ensure that the finished parts <b>22</b> were properly processed by the machine <b>44</b> before the parts <b>22</b> are allowed to be processed further. Such an inspection may be, for example, a hole inspection to verify that holes are properly made in the parts <b>22</b>. According to a further aspect, conveyance system <b>20</b> may accommodate a variation in the number of parts <b>22</b> or bins <b>24</b> located at the source location <b>26</b> and/or the destination <b>28</b>, such as variations in the stack height, and may automatically pick or place parts <b>22</b> from the top of a stack of bins <b>24</b>. Such a system <b>20</b> may replace a current manual loading and unloading operation and may occupy the same or a smaller square footage footprint on a building floor. The example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> may allow a reduction from <b>1</b> to <b>0</b> direct labor on each shift to perform the loading and unloading of parts <b>22</b> from the machine <b>44</b>.</p><p id="p-0036" num="0035">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, an example application of the conveyance system <b>20</b> of the present invention may be to replace a manual operation of loading and unloading baskets <b>46</b> of parts <b>22</b> into and out of a machine <b>44</b>. In the example shown, the machine <b>44</b> is a washer with an input belt <b>48</b> for receiving baskets <b>46</b> of dirty parts <b>22</b> and an output belt <b>50</b> for removal of baskets <b>46</b> cleaned parts <b>22</b>. Parts <b>22</b> to be washed arrive in a basket <b>46</b> and are placed in a defined position on a rack inside the basket <b>46</b>. The operator may load a basket <b>46</b> from a source location <b>26</b> onto the input belt <b>48</b> and may then unload the basket <b>46</b> from the output belt <b>50</b> by moving the basket <b>46</b> to a destination <b>28</b>. The baskets <b>46</b>, with parts <b>22</b> may require the use of a mechanical aid such as a crane <b>52</b> to lift in order to be compliant with health and safety regulations. The use of a crane <b>52</b> may be difficult and/or cumbersome to use and may not be embraced by staff. The operator loading and unloading the machine <b>44</b> may perform quality, quantity, and data logging tasks on the parts <b>22</b>.</p><p id="p-0037" num="0036">The process of manually loading and unloading a washing machine <b>44</b> may involve the following steps:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0037">Step 1: The operator takes the basket <b>46</b> off a standing/fixed delivery carriage <b>54</b> and places the basket <b>46</b> on the input belt <b>48</b> on the right side of the washing machine <b>44</b>.</li>        <li id="ul0002-0002" num="0038">Step 2: The operator takes the basket <b>46</b> off the output belt <b>50</b> on the left side of the washing machine <b>44</b> and places the basket <b>46</b> on a stationary carriage <b>56</b> in a defined area with. Stack height may vary depending on how many baskets <b>46</b> are already in place on the carriage <b>56</b>.</li>    </ul>    </li></ul></p><p id="p-0038" num="0039">As illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the carriage <b>56</b> may be fixed, being stationary and located in a general predetermined location window, shown as a taped rectangle on the floor, but the carriages <b>56</b> do not need to be located or secured in a precise position such as by using a mechanical carriage fixing device. The loading and unloading operations are not time-critical. The cycle time of the machine may allow for some variation in when the baskets <b>46</b> are loaded and unloaded. The loading and loading operations may require careful handling. The baskets <b>46</b> may vary in weight and height to accommodate different numbers and types of parts <b>22</b>. Due to the physical constraints of the machine <b>44</b> and the carriages <b>54</b>, <b>56</b>, fencing for a traditional robot cell around the machine <b>44</b> may not be feasible.</p><p id="p-0039" num="0040">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the conveyance system <b>20</b> including a robot <b>30</b> may be used to load and unload baskets <b>46</b> of parts <b>22</b> from the washing machine <b>44</b>. The system <b>20</b> may locate a basket <b>46</b> from the standing/fixed delivery carriage <b>54</b> and may pick the basket <b>46</b> with an end effector <b>32</b> on the robot <b>30</b>, which may place the basket <b>46</b> on the input belt <b>48</b> (dirty parts) of the washing machine <b>44</b>. The system <b>20</b> may detect and locate a basket <b>46</b> on the output belt <b>50</b> of the washing machine <b>44</b> and may move the basket <b>46</b> onto stack on a carriage <b>56</b>. The conveyance system <b>20</b> may use cameras <b>36</b> accommodate baskets <b>46</b> that vary in size and weight and which are not fixed in a specific location. The conveyance system <b>20</b> may perform quantity, quality inspection, and data logging tasks. The conveyance system <b>20</b> may allow baskets to be stacked at different positions on a carriage <b>56</b> which may vary according to the existing load on that carriage <b>56</b>. The system <b>20</b> may provide for loading and unloading cycle times of less than <b>80</b>s to prevent any bottleneck at the loading or unloading steps. The robot <b>30</b> may have TUV certified skin technology and may recognize and/or inform humans in the working area. In this way, the robot <b>30</b> may be able to operate without protective fencing.</p><p id="p-0040" num="0041"><figref idref="DRAWINGS">FIGS. <b>4</b> and <b>5</b></figref> provide schematic views of the conveyance system <b>20</b> of the present disclosure as used for the example application of loading and unloading baskets <b>46</b> of parts <b>22</b> from a washing machine <b>44</b>.</p><p id="p-0041" num="0042">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a first vision system <b>34</b> including at least one camera <b>36</b> may identify a basket <b>46</b> including its precise pick location and pick orientation in the source location <b>26</b>, which may be delivery carriage <b>54</b> holding a stack of one or more baskets <b>46</b>. The robot <b>30</b> may pick the basket <b>46</b> from the source location <b>26</b> and move the basket <b>46</b> to the input belt <b>48</b> of the washing machine <b>44</b>. A camera <b>36</b> may not be required to cover the input and/or output belts <b>48</b>, <b>50</b>, as those locations may be fixed, and their status as empty or loaded with a basket <b>46</b> may be communicated to the conveyance system <b>20</b> from the machine <b>44</b>. The conveyance system <b>20</b> may also perform the step of unloading the washing machine <b>44</b> by picking up a basket <b>46</b> from the output belt <b>50</b> and placing that basket at a destination location <b>28</b>, which may be the top of a stack of other baskets <b>46</b> upon a carriage <b>56</b>. The precise location and orientation of the destination <b>28</b> may vary according to the exact location of the carriage and/or the height of the stack of baskets <b>46</b> and may be determined by the second vision system <b>38</b> using one or more cameras <b>36</b>. The system <b>20</b> may provide adaptive trajectory planning to determine the best path to move the baskets <b>46</b>.</p><p id="p-0042" num="0043">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the system <b>20</b> may include a trajectory planning controller <b>42</b> for planning a best path <b>40</b> for the robot <b>30</b> to follow in moving an item <b>21</b>, which may be a basket <b>46</b>, between the source location <b>26</b> and the destination <b>28</b>. One or more cameras <b>36</b> may provide a <b>3</b>-dimensional view to detect the exact position of a basket <b>46</b>. The system <b>20</b> may also detect the number and shape of individual parts <b>22</b> in the basket <b>46</b>. The trajectory planning controller <b>42</b> may perform several functions in the system <b>20</b>, which may include, for example, 2D inspection of a basket <b>46</b> and parts <b>22</b> therein; 3D perception and localization; perception and load &#x26; force measurement; production process sequencing, providing a production graphical user interface (GUI); calibration and configuration software; and Production process specific motion planning and control, including the control of the end effector <b>32</b>, also called end-of-arm-tooling (EOAT). The robot <b>30</b> may be a standard type used in industry for automation tasks, and the end effector <b>32</b> may be configured to grasp the standard basket <b>46</b> of different weights.</p><p id="p-0043" num="0044">The trajectory planning controller <b>42</b> may provide adaptive trajectory planning using the information provided by the vision systems (pick part and place part), as well as predetermined or fixed locations to calculate the best trajectory for the robot to follow in picking and placing the item <b>21</b>. The robot <b>30</b> may be directly controlled by a robot controller <b>60</b> which may handle safety functions, movement control, and control of the end effector <b>32</b>. The trajectory planning controller <b>42</b> may coordinate and communicate with a robot operating system (ROS) driver <b>62</b>. The trajectory planning controller <b>42</b> may also be operatively connected to a machine controller <b>64</b>, such as the controller of the washing machine <b>44</b>. This connection to the machine controller <b>64</b> may allow the system <b>20</b> to know when items may be loaded onto or removed from the machine <b>44</b>. The operative connections between devices may include electrical, radio, optical, light-based, and/or mechanical interconnections and may be wired or wireless.</p><p id="p-0044" num="0045">The robot <b>30</b> may be equipped with touch sensors <b>65</b>, which may include pressurized air pads on the end effector <b>32</b> or gripper, which may allow the robot <b>30</b> to be used without the need for fencing. A touch sensor controller <b>66</b>, such as an air skin controller, may be used to monitor the status of one or more touch sensors <b>65</b> on the robot <b>30</b>, and may be operatively connected to a safety circuit of the robot controller <b>60</b>. In order to allow the robot <b>30</b> to operate without traditional safety fencing, such a touch sensor configuration may require safety performance level E and may require the robot <b>30</b> to be able to react to all humans on the shop floor including operators, visitors, supplier staff, etc.. The touch sensor controller <b>66</b> may also be operatively connected to the adaptive system controller <b>42</b>.</p><p id="p-0045" num="0046">After processing by the washing machine <b>44</b>, a camera <b>36</b> may identify a destination location <b>28</b> being a desired stack of baskets <b>46</b> upon a carriage <b>56</b> and which may vary in height as additional baskets <b>46</b> are added to the stack.</p><p id="p-0046" num="0047">The conveyance system <b>20</b> of the present disclosure may provide the following functions: transporting a basket <b>46</b> accounting for variations in the precise special arrangement (x, y, z, roll, pitch, yaw) of both the pick and the place operations; identifying the general source and destination location <b>26</b>, <b>28</b> (x, y, z, yaw) from a stack of one or more baskets <b>46</b> at each location <b>26</b>, <b>28</b>; type identification of baskets <b>46</b> (height, weight, insert feature &#x26; geometry); identifying interactions between baskets <b>46</b> (tangled or various other interactions matching predetermined criteria, such as being caught upon another item <b>21</b> and which may be known as easy to take apart); recognizing and reporting a damaged basket <b>46</b>.</p><p id="p-0047" num="0048">The conveyance system <b>20</b> may also provide for identification and information sharing regarding the items <b>21</b> being moved, such as, for example by reading a bar code on the baskets <b>46</b>, and may also identify individual parts <b>22</b> within a basket <b>46</b>, such as by their shape and size in 3-D space, and/or by their positioning within an insert in the basket <b>46</b>. It may provide a mode in which the robot <b>30</b> drains fluid from the carriage <b>56</b>, such as, for example, by moving the carriage <b>56</b> to a designated dumping location and opening a drain valve or by tilting the carriage <b>56</b>.</p><p id="p-0048" num="0049">The conveyance system <b>20</b> may automatically calibrate to account to changes in the environment, such as temperature and/or lighting, and may provide for environmental awareness, such as for crash detection and awareness. In other words, the cameras <b>36</b> of the conveyance system <b>20</b> may detect persons or other hazards, and may direct the robot <b>30</b> to avoid any such hazards. The conveyance system <b>20</b> may provide for increased system reliability and may allow for different sequencing or sorting baskets <b>46</b>, such as, for example, in normal or special operation modes.</p><p id="p-0049" num="0050">The present disclosure also provides a method for automatically moving one or more items <b>21</b> between a structure at a source location <b>26</b> and a destination <b>28</b> using a robot <b>30</b> with an end effector <b>32</b>. The items <b>21</b> may be individual parts <b>22</b> or assemblies of parts <b>22</b> or other things such as a basket <b>46</b> for holding several parts <b>22</b>. The structure may be a bin <b>24</b> for holding parts <b>22</b>. The structure may also be, for example, a cart or a stack or a conveyor for holding or moving parts <b>22</b> or baskets <b>46</b>. The method includes the steps of identifying a part <b>22</b> having a non-fixed location and orientation upon the structure at the source location <b>26</b> using a first vision system <b>34</b>; determining the precise pick location and pick orientation of the part <b>22</b> upon the structure using the first vision system <b>34</b>; and determining the location and orientation of a destination <b>28</b> using a second vision system <b>38</b>. The first and second vision systems <b>34</b>, <b>38</b> may be a combined vision system and may use one or more of the same cameras <b>36</b>. The method also includes the step of performing adaptive trajectory planning to determine the best path <b>40</b> between the source location <b>26</b> and the destination <b>28</b>. According to an aspect, the step of performing adaptive trajectory planning may include the sub-steps of planning a plurality of possible paths <b>40</b> between the source location <b>26</b> and the destination incorporating geometrical information of the robot and source location <b>26</b> and the pick orientation and the destination <b>28</b> which may include the target location and the target orientation; and determining a best path <b>40</b> between the source location <b>26</b> and the destination <b>28</b> by simulating the plurality of possible paths <b>40</b> between the source location <b>26</b> and the destination <b>28</b>. One example of such an active trajectory planning is ROS (Robotic Operating System).</p><p id="p-0050" num="0051">The method proceeds with the steps of picking the item <b>21</b> from the source location <b>26</b> by the end effector <b>32</b> on the robot <b>30</b>; moving the item <b>21</b> along the best path <b>40</b> by the robot <b>30</b>; placing the item <b>21</b> at the destination <b>28</b> by the end effector <b>32</b> on the robot <b>30</b>. The method may also include the step of checking the item <b>21</b> for quality and/or other characteristics by one or more of the first vision system <b>34</b> and the second vision system <b>38</b>.</p><p id="p-0051" num="0052">According to an aspect, the destination <b>28</b> may have a fixed position and orientation. According to another aspect, the destination <b>28</b> may have a varying position, and/or orientation or one which is not fixed in space. According to another aspect, the item <b>21</b> may be disposed loosely or in a fixed position within a bin <b>24</b> at the source location <b>26</b>.</p><p id="p-0052" num="0053">According to an aspect, the first vision <b>34</b> system may be a 2D vision system and the method may further comprise the step of comparing by the first vision system <b>34</b> an image of the item <b>21</b> to a reference image to determine the source location <b>26</b> and the orientation of the item <b>21</b> at the source location <b>26</b>, also called the pick orientation. According to another aspect, the first vision system <b>34</b> may be a 3D vision system, which may be capable of directly determining the source location <b>26</b> and pick orientation. According to an aspect, the system <b>20</b> may be used for two or more distinct pick-and-place operations such as, for example loading and unloading a machine <b>44</b> as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0053" num="0054">According to an aspect, the second vision system <b>38</b> may be a 2D vision system and the method may further comprise the step of comparing by the second vision system <b>38</b> an image of the item <b>21</b> to a reference image to determine the location and orientation of the destination <b>28</b>. According to another aspect, the second vision system <b>38</b> may be a 3D vision system, which may directly determine the location orientation of the destination <b>28</b>.</p><p id="p-0054" num="0055"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows a fenceless robot system <b>100</b> in accordance with some embodiments, and in which the robot <b>30</b> is configured to move in proximity to a person without a safety fence preventing the person from contacting the robot <b>30</b>. The fenceless robot system <b>100</b> may be similar to the conveyance system <b>20</b> shown in <figref idref="DRAWINGS">FIGS. <b>1</b>-<b>5</b></figref> and may include many of the same components. The example fenceless robot system <b>100</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> includes a vision controller <b>70</b> receiving signals from one or more cameras <b>36</b> for sensing a position and orientation of an item <b>21</b>, such as a basket <b>46</b> in a stack upon a carriage <b>56</b> or a precise location of a carriage <b>56</b> as a destination for the robot <b>30</b> to place the basket <b>46</b>. The input belt <b>48</b> may be a predetermined destination <b>28</b> for the robot to place baskets <b>46</b> to be processed by the washing machine <b>44</b>. The output belt <b>50</b> may be a predetermined source location <b>26</b> for the robot <b>30</b> to pick and remove baskets <b>46</b> after processing by the washing machine <b>44</b>. In some embodiments, and as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the input belt <b>48</b> and the output belt <b>50</b> of the washing machine <b>44</b> may be integrated at a single, combined location. In some embodiments, one or more of the stacks of baskets <b>46</b> may function as a source location <b>26</b> for the robot <b>30</b> to pick baskets <b>46</b> of dirty parts to be loaded into the washing machine <b>44</b>, and one or more other stacks of baskets <b>46</b> may be a destination <b>26</b> for the robot <b>30</b> to place baskets <b>46</b> of clean parts coming out of the washing machine <b>44</b>. In some embodiments, the stack may include up to ten (<b>10</b>) baskets <b>46</b>. It should be appreciated that the washing machine <b>44</b> is merely an example application and that the fenceless robot system <b>100</b> may be used for many other different applications.</p><p id="p-0055" num="0056">The fenceless robot system <b>100</b> includes a trajectory planning controller <b>42</b>, configured to plan a path for the robot <b>30</b> to follow in moving an item <b>21</b>, such as a basket <b>46</b>, between the source location <b>26</b> and the destination <b>28</b>. The trajectory planning controller <b>42</b> may include a ROS (Robotic Operating System). Alternatively or additionally, the trajectory planning controller <b>42</b> may include another proprietary or open hardware and/or software system configured to plan the path or paths for the robot <b>30</b> to follow.</p><p id="p-0056" num="0057">A proximity sensor <b>74</b> is configured to detect a person in proximity to the robot <b>30</b>. The proximity sensor <b>74</b> may include, for example, a laser scanner, a LIDAR sensor, or one or more optical cameras, which may be combined with a machine vision processor configured to detect presence and /or location of a person. In some embodiments, and as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the proximity sensor <b>74</b> may be configured to sense a person within an inner zone <b>80</b> containing the robot <b>30</b> or an outer zone <b>82</b> outside of the reach of the robot <b>30</b>. In some embodiments, such as the arrangement shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the proximity sensor <b>74</b> may be located adjacent to the robot <b>30</b>. In other embodiments, the proximity sensor <b>74</b> may be spaced away from the robot <b>30</b>.</p><p id="p-0057" num="0058">As also shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, one or more touch sensors <b>65</b> are disposed on surfaces of the robot <b>30</b> and on surfaces surrounding the end effector <b>32</b>. The touch sensors <b>65</b> are configured to detect a contact between an external object, such as a person, and the surface of the robot <b>30</b> or the surface surrounding the end effector <b>32</b>. A touch sensor controller <b>66</b> is configured to monitor the touch sensors <b>65</b> and to report the detection of a contact with an external object. In some embodiments, one or more of the touch sensors <b>65</b> include a bladder configured to deform and to generate a change in fluid pressure in response to contacting the external object. For example, each of a plurality of bladders (not shown) may cover a region of the surface of the robot <b>30</b> or the surface surrounding the end effector <b>32</b>. Alternatively or additionally, one or more of the touch sensors <b>65</b> may include a rigid or semi-rigid panel configured to be displaced by contact with an external object, and the displacement of the panel may be cause a corresponding bladder to be squeezed, generating a corresponding increase in fluid pressure, which may be sensed. The touch sensors <b>65</b> may be, for example, Airskin product from Blue Danube Robotics.</p><p id="p-0058" num="0059">In some embodiments, a visual indicator <b>68</b>, such as a multi-colored light, is configured to indicate a contact between the external object and the surface of the robot <b>30</b> or the surface surrounding the end effector <b>32</b>. The visual indicator <b>68</b> may be disposed within or projected upon a region of the surface of the robot <b>30</b> or the surface surrounding the end effector contacted by the external object. In some embodiments, each of the touch sensors <b>65</b> may include a corresponding visual indicator <b>68</b> which may change between different appearances to indicate various status conditions regarding the touch sensor <b>65</b>. For example, a solid blue light may indicate that the touch sensor <b>65</b> is in working condition and is actively waiting to detect a contact with an external object; a flashing red light may indicate that the touch sensor <b>65</b> is currently detecting contact with an external object; and a solid red light may indicate that the touch sensor <b>65</b> previously detected contact with an external object and is holding its state until it receives a reset signal.</p><p id="p-0059" num="0060">A safety Programmable Logic Controller (safety PLC) <b>80</b> is configured to monitor the touch sensor and the proximity sensor and to stop the robot <b>30</b> in response to an error condition by the touch sensor <b>65</b>, the touch sensor controller <b>66</b>, and/or the proximity sensor <b>74</b>. The error condition may include sensing a person within one of the predefined zones <b>76</b>, <b>78</b>, detecting a contact between the robot <b>30</b> or the end effector <b>32</b> and an external object, or an internal or external failure of any hardware or software of the touch sensor <b>65</b>, the touch sensor controller <b>66</b>, and/or the proximity sensor <b>74</b>. The safety PLC <b>80</b> may include integrated safety functions, safety-certified hardware and software, redundant and/or self-checking circuits to verify proper operation of the touch sensor <b>65</b>, the touch sensor controller <b>66</b>, and/or the proximity sensor <b>74</b>. The safety PLC <b>80</b> may include, for example, a safety-rated PLC by Siemens or Allen-Bradley.</p><p id="p-0060" num="0061">In some embodiments, and as also shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the fenceless robot system <b>100</b> includes a processing status indicator <b>86</b> adjacent to a processing location <b>88</b>, such as a combined source location <b>26</b> and destination <b>28</b>. The processing status indicators <b>86</b> are configured to indicate a condition of one or more items <b>21</b> located at the processing location <b>88</b>. For example, the processing status indicators <b>86</b> may indicate one of a plurality of different condition states of the one or more items <b>21</b> located at the processing location <b>88</b>. In some embodiments, the processing status indicators <b>86</b> each comprise a lighted indicator having one of a plurality of different colors or patterns, with each of the different colors or patterns indicating a corresponding one of the different condition states.</p><p id="p-0061" num="0062">For example, the processing status indicator <b>86</b> may be illuminated red to indicate the processing location <b>88</b> being in an active state corresponding to the robot <b>30</b> actively using the processing location <b>88</b> as a source location <b>26</b> or as a destination <b>28</b>. The processing status indicator <b>86</b> may be illuminated purple to indicate the processing location <b>88</b> being in a ready state corresponding to items <b>21</b> at the processing location <b>88</b> being in a queue to be used by the robot <b>30</b> in the future. The processing status indicator <b>86</b> may be illuminated blue to indicate the processing location <b>88</b> being in an inactive state corresponding to items <b>21</b> at the processing location <b>88</b> that are not yet processed for use by the robot <b>30</b>. The processing status indicator <b>86</b> may be illuminated green to indicate the processing location <b>88</b> being in a completed state corresponding to the processing location <b>88</b> holding items that have finished being processed by the robot <b>30</b> and which are ready to be taken away from the fenceless robot system <b>100</b>. The colors and states of the processing status indicators <b>86</b> are merely examples, and other colors and/or state condition may be used or indicated by the status indicators <b>86</b>.</p><p id="p-0062" num="0063">One or more of the processing locations <b>88</b> may be dedicated source locations <b>26</b> where items <b>21</b> are picked and removed by the robot <b>30</b>. Alternatively or additionally, one or more of the processing locations <b>88</b> may be dedicated destinations <b>28</b> where items <b>21</b> are placed by the robot <b>30</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, several different processing locations <b>88</b> are provided as regions or fixtures for placement of a carriage <b>56</b> holding a stack of one or more baskets <b>46</b>.</p><p id="p-0063" num="0064"><figref idref="DRAWINGS">FIGS. <b>6</b>-<b>7</b></figref> show a schematic diagrams of the fenceless robot system <b>100</b> according to some embodiments of the present disclosure. Specifically, <figref idref="DRAWINGS">FIGS. <b>6</b>-<b>7</b></figref> show a plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> concentrically surrounding the robot <b>30</b>. The plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> includes a restricted access zone <b>92</b> which may be designated for exclusive operation of the robot <b>30</b>. In some embodiments, and as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the restricted access zone <b>92</b> may extend around the robot <b>30</b> and have a radius of 733 mm. The plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> also includes an inner safety zone <b>94</b> surrounding the restricted access zone <b>92</b> and extending between 733-1466 mm from the center of the robot <b>30</b>. The plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> also includes an outer safety zone <b>96</b> surrounding the inner safety zone <b>94</b> and extending between 1466-2200 mm from the center of the robot <b>30</b>. The plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> also includes a limited access zone <b>98</b> outside of the outer safety zone <b>96</b> having limited access or use restrictions. For example, the limited access zone <b>98</b> may be designated as a &#x201c;no-storage zone&#x201d; where storage of carriages <b>56</b> or other items is prohibited in order to ensure a clear line of sight between the proximity sensor <b>74</b> and the other ones of the plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b>.</p><p id="p-0064" num="0065">In some embodiments, and as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the inner zone <b>76</b> is subdivided into the restricted access zone <b>92</b> and the inner and outer safety zones <b>94</b>, <b>96</b>, and the peripheral zone <b>78</b> defines the limited access zone <b>98</b>. In some embodiments, and as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the robot <b>30</b> is configured to limit the speed of the end effector <b>32</b> in one or more of the plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> in response to detecting a human within or in proximity to the inner zone <b>76</b>. For example, the robot <b>30</b> may be configured to move the end effector <b>32</b> at a speed of up to 500 mm/s within the restricted access zone <b>92</b> if no humans are present, and the robot <b>30</b> may limit the speed of the end effector <b>32</b> within the restricted access zone <b>92</b> to 350 mm/s in response to detecting a human within or in proximity to the inner zone <b>76</b>. The robot <b>30</b> may be configured to move the end effector <b>32</b> at a speed of up to 350 mm/s within the inner safety zone <b>94</b> regardless of whether a human is present within or in proximity to the inner zone <b>76</b>. The robot <b>30</b> may be configured to move the end effector <b>32</b> at a speed of up to 250 mm/s within the outer safety zone <b>96</b> if no humans are present, and the robot <b>30</b> may limit the speed of the end effector <b>32</b> within the outer safety zone <b>96</b> to 50 mm/s in response to detecting a human within or in proximity to the inner zone <b>76</b>.</p><p id="p-0065" num="0066">In some embodiments, the end effector <b>32</b> may not be limited to the lower limited speed for some motions which have a lower risk of presenting a hazard to a human in proximity to the robot <b>30</b>, such as motions toward the center of the robot <b>30</b>.</p><p id="p-0066" num="0067"><figref idref="DRAWINGS">FIGS. <b>9</b>-<b>10</b></figref> show perspective views of the fenceless robot system <b>100</b> according to some embodiments of the present disclosure. <figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a plan view of a washing machine <b>44</b> configured for manual loading and loading operation. <figref idref="DRAWINGS">FIG. <b>12</b></figref> shows a plan view of the same washing machine <b>44</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref> configured to be loaded and unloaded by the fenceless robot system <b>100</b>. <figref idref="DRAWINGS">FIGS. <b>13</b>-<b>15</b></figref> show images of components of the fenceless robot system <b>100</b> according to some embodiments of the present disclosure. <figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a frame structure <b>104</b> that surrounds the end effector (not shown) and which is used for mounting the touch sensors <b>65</b> surrounding the end effector.</p><p id="p-0067" num="0068">In some embodiments, and as shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a plurality of first panels <b>110</b> are disposed upon a part of the robot <b>30</b>, such as an arm. Each of the first panels <b>110</b> may be configured to detect a contact with an external object, such as a person, with a surface of the robot. In some embodiments, and as also shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a plurality of second panels <b>112</b> are disposed upon or around a joint between the robot <b>30</b> and an end effector thereof. Each of the second panels <b>112</b> may be configured to detect a contact with an external object, such as a person, with a surface of the joint. In some embodiments, and as also shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a plurality of third panels <b>114</b> are disposed around an end effector of the robot <b>30</b>. The third panels <b>114</b> may define a box shape with an open bottom for receiving the objects to be moved by the robot <b>30</b>. Each of the first panels <b>110</b> may be configured to detect a contact with an external object, such as a person, with a surface of the end effector. The system may include a visual indicator <b>120</b> showing a location of the contact between the external object and the surface of the robot <b>30</b> or the surface surrounding the end effector corresponding indicator. For example, the visual indicator <b>120</b> may take the form of a warning light on each of the panels <b>110</b>, <b>112</b>, <b>114</b> that illuminates in response to the corresponding one of the panels <b>110</b>, <b>112</b>, <b>114</b> detecting a contact. The visual indicator <b>120</b> may take other forms, such as a light or other indicator that is projected upon the surface at or near the location of a detected contact with an external object.</p><p id="p-0068" num="0069">A method for moving an item <b>21</b> using a fenceless robot system <b>100</b> is also provided. The method comprises: picking an item from a source location <b>26</b> by an end effector <b>32</b> on a robot <b>30</b>; moving the item along a path to a destination <b>28</b> by the robot <b>30</b>; placing the item at the destination <b>28</b> by the end effector on the robot <b>30</b>; and stopping the robot <b>30</b> from moving in response to detecting contact between an external object and a surface of the robot <b>30</b> or a surface surrounding the end effector <b>32</b>. The robot <b>30</b> is configured to move in proximity to a person without a safety fence preventing the person from contacting the robot <b>30</b>.</p><p id="p-0069" num="0070">In some embodiments, the method for moving an item <b>21</b> using a fenceless robot system <b>100</b> also includes indicating the contact between the external object and the surface of the robot <b>30</b> or the surface surrounding the end effector <b>32</b> by a visual indicator <b>68</b> in a region of the surface of the robot <b>30</b> or the surface surrounding the end effector <b>32</b> contacted by the external object.</p><p id="p-0070" num="0071">In some embodiments, the robot <b>30</b> is capable of moving a payload, including the end effector <b>30</b> and the item <b>21</b>, having a mass of at least 100 kg. In some example embodiments, the robot <b>30</b> may be capable of moving an item <b>21</b>, such as a basket <b>46</b> full of parts having a combined mass of about 40 kg, however, the robot <b>30</b> may be configure to move items <b>21</b> having greater mass. In some embodiments, the robot <b>30</b> moves the item <b>21</b> at a speed greater than 200 mm/s. In some embodiments, the robot <b>30</b> moves the item <b>21</b> at a speed of up to 500 mm/s. In some embodiments, the robot <b>30</b> moves the item <b>21</b> at a speed of up to 700 mm/s.</p><p id="p-0071" num="0072">In some embodiments, the method for moving an item <b>21</b> using a fenceless robot system <b>100</b> also includes detecting the person in one of a plurality of zones around the robot; and adjusting a speed of the robot <b>30</b> in response to detecting the person in one of the plurality of zones around the robot. In some embodiments, adjusting a speed of the robot <b>30</b> in response to detecting the person in one of the plurality of zones around the robot further comprises limiting the speed of the robot to a predetermined threshold speed; and the predetermined threshold speed is one of a plurality of different speeds depending on a location of the end effector <b>32</b> of the robot <b>30</b>.</p><p id="p-0072" num="0073">In some embodiments, the method for moving an item <b>21</b> using a fenceless robot system <b>100</b> also includes detecting the person in one of a plurality of zones <b>92</b>, <b>94</b>, <b>96</b>, <b>98</b> around the robot <b>30</b> being a restricted access zone <b>92</b>; and immediately stopping the robot <b>30</b> from moving in response to detecting the person in the restricted access zone <b>92</b>.</p><p id="p-0073" num="0074">In some embodiments, the method for moving an item <b>21</b> using a fenceless robot system <b>100</b> also includes detecting the person in one of a plurality of zones around the robot being an outer safety zone; and limiting a speed of the robot <b>30</b> in response to detecting the person in outer safety zone. In some embodiments, limiting the speed of the robot <b>30</b> in response to detecting the person in outer safety zone may comprise limiting the speed of the robot <b>30</b> in motions toward or across the outer safety zone while not limiting the speed of the robot <b>30</b> in motions away from the outer safety zone.</p><p id="p-0074" num="0075">In some embodiments, the method for moving an item <b>21</b> using a fenceless robot system <b>100</b> also includes detecting the person in one of a plurality of zones around the robot being an inner safety zone located between the robot and the outer safety zone; and immediately stopping the robot from moving in response to detecting the person in the inner safety zone if the robot is moving toward or across the outer safety zone at a speed in excess of a predetermined threshold.</p><p id="p-0075" num="0076">The foregoing description of the embodiments has been provided for purposes of illustration and description. It is not intended to be exhaustive or to limit the disclosure. Individual elements or features of a particular embodiment are generally not limited to that particular embodiment, but, where applicable, are interchangeable and can be used in a selected embodiment, even if not specifically shown or described. The same may also be varied in many ways. Such variations are not to be regarded as a departure from the disclosure, and all such modifications are intended to be included within the scope of the disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for moving an item, the method comprising:<claim-text>picking an item from a source location by an end effector on a robot;</claim-text><claim-text>moving the item along a path to a destination by the robot;</claim-text><claim-text>placing the item at the destination by the end effector on the robot;</claim-text><claim-text>stopping the robot from moving in response to detecting contact between an external object and one of: a surface of the robot, or a surface surrounding the end effector;</claim-text><claim-text>indicating the contact between the external object and the one of the surface of the robot or the surface surrounding the end effector by a visual indicator in a region of the surface of the robot or the surface surrounding the end effector contacted by the external object and wherein the robot is configured to move in proximity to a person without a safety fence preventing the person from contacting the robot.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. (canceled)</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>detecting the person in one of a plurality of zones around the robot; and</claim-text><claim-text>adjusting a speed of the robot in response to detecting the person in one of the plurality of zones around the robot.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein adjusting a speed of the robot in response to detecting the person in one of the plurality of zones around the robot further comprises limiting the speed of the robot to a predetermined threshold speed; and<claim-text>wherein the predetermined threshold speed is one of a plurality of different speeds depending on a location of the end effector of the robot.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the one of the plurality of zones around the robot is a restricted access zone; and<claim-text>the method further comprising: immediately stopping the robot from moving in response to detecting the person in the restricted access zone.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the one of the plurality of zones around the robot is an outer safety zone; and<claim-text>the method further comprising: limiting a speed of the robot in response to detecting the person in outer safety zone.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein limiting the speed of the robot in response to detecting the person in the outer safety zone comprises limiting the speed of the robot in motions toward or across the outer safety zone while not limiting the speed of the robot in motions away from the outer safety zone.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the one of the plurality of zones around the robot is an inner safety zone located between the robot and the outer safety zone; and<claim-text>the method further comprising: immediately stopping the robot from moving in response to detecting the person in the inner safety zone if the robot is moving toward or across the outer safety zone at a speed in excess of a predetermined threshold.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A fenceless conveyance system comprising:<claim-text>a robot having an end effector configured to selectively grasp an item;</claim-text><claim-text>a trajectory planning controller configured to direct the robot to move the item between a source location and a destination;</claim-text><claim-text>a touch sensor configured to detect a contact between an external object and a surface of the robot or a surface surrounding the end effector, wherein the touch sensor comprises a plurality of bladders configured to deform and to generate a change in fluid pressure in response to contacting the external object, with each of the plurality of bladders covering a region of the surface of the robot or the surface surrounding the end effector;</claim-text><claim-text>a proximity sensor configured to detect a person in proximity to the robot; and</claim-text><claim-text>wherein the robot is configured to move in proximity to a person without a safety fence preventing the person from contacting the robot.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The fenceless conveyance system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the proximity sensor comprises one of a laser scanner, one or more cameras, or a LIDAR sensor.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The fenceless conveyance system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising a safety Programmable Logic Controller (safety PLC) configured to monitor the touch sensor and the proximity sensor and to stop the robot in response to an error condition by either of the touch sensor or the proximity sensor.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. (canceled)</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The fenceless conveyance system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising a visual indicator showing a location of the contact between the external object and the surface of the robot or the surface surrounding the end effector.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The fenceless conveyance system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising: a processing status indicator adjacent to a processing location for indicating a condition of one or more items located at the processing location, the processing status indicator indicating one of a plurality of different condition states of the one or more items located at the processing location.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The fenceless conveyance system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the plurality of different condition states of the one or more items at the processing location comprise:<claim-text>an active state corresponding to the robot actively using the processing location as a source location or as a destination; and</claim-text><claim-text>a ready state corresponding to items at the processing location being in a queue to be used by the robot in the future;</claim-text><claim-text>an inactive state corresponding to items at the processing location that are not yet processed for use by the robot; and</claim-text><claim-text>a completed state corresponding to the processing location holding items that have finished being processed by the robot.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one of the surface of the robot or the surface surrounding the end effector includes the surface surrounding the end effector.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein detecting the contact between the external object and the surface surrounding the end effector includes detecting a change in fluid pressure as a result of a deformation of a bladder covering the surface surrounding the end effector.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the robot moves a payload, including the end effector and the item, having a mass of at least 100 kg.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The fenceless conveyance system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the touch sensor is configured to detect the contact between the external object and the surface surrounding the end effector.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The fenceless conveyance system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the visual indicator includes a multi-colored light configured to illuminate with a predetermined color to indicate the contact between the external object and the surface of the robot or the surface surrounding the end effector.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The fenceless conveyance system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the visual indicator is disposed within or adjacent to the surface of the robot or the surface surrounding the end effector.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The fenceless conveyance system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the visual indicator projected upon the surface of the robot or the surface surrounding the end effector.</claim-text></claim></claims></us-patent-application>