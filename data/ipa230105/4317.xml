<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004318A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004318</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17469192</doc-number><date>20210908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>34</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0649</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>11</main-group><subgroup>3409</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0611</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0679</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0629</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR REORDERING DATA IN A STORAGE DEVICE BASED ON DATA ACCESS PATTERNS</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217697</doc-number><date>20210701</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Samsung Electronics Co., Ltd.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Zhang</last-name><first-name>Tong</first-name><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Zongwang</first-name><address><city>Dublin</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Pitchumani</last-name><first-name>Rekha</first-name><address><city>Oak Hill</city><state>VA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Ki</last-name><first-name>Yang Seok</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for reordering data for storage includes detecting a data access pattern, associated with an application, for accessing a data, generating a remapping function based on a data access pattern information, the remapping function including operations to determine a reordering of the data based on address information for the data, receiving the data at a storage device, the data being ordered according to a first layout sequence, reordering the data, by the storage device, based on the remapping function, and storing the data, at the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="203.20mm" wi="123.87mm" file="US20230004318A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="146.05mm" wi="117.77mm" file="US20230004318A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="233.68mm" wi="170.01mm" orientation="landscape" file="US20230004318A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.81mm" wi="175.01mm" orientation="landscape" file="US20230004318A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="139.53mm" wi="156.55mm" file="US20230004318A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="234.95mm" wi="173.23mm" file="US20230004318A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="220.90mm" wi="131.49mm" orientation="landscape" file="US20230004318A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="216.24mm" wi="125.90mm" file="US20230004318A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">The present application claims priority to, and the benefit of, U.S. Provisional Application Ser. No. 63/217,697, filed Jul. 1, 2021, entitled &#x201c;TILING/SWIZZLING-AWARE PLACEMENT AND PREFETCHING FOR CACHE LINE-ADDRESSABLE MEMORY/STORAGE DEVICES,&#x201d; the entire content of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">One or more aspects of embodiments according to the present disclosure relate to systems and methods for reordering data in a storage device based on data access patterns.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The present background section is intended to provide context only, and the disclosure of any embodiment or concept in this section does not constitute an admission that said embodiment or concept is prior art.</p><p id="p-0005" num="0004">In the field of computer storage, a storage device may be used to store data to be accessed by a program (or application). The program (or application) may perform various operations with (or on) the data. For example, the program may access the data in smaller portions than accessing all of the data at once, or the program may access a given region of the data and ignore other regions. Thus, the program (or an algorithm implemented in the program) may have a particular access pattern for accessing the data (e.g., a data access pattern) that deviates from a different access pattern for the data.</p><p id="p-0006" num="0005">Accordingly, there is a need for methods, devices, and systems for reordering data for storage in a storage device based on data access patterns.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">Aspects of embodiments of the present disclosure relate to computer storage systems, and provide improvements to the ordering of data within a storage device based on data access patterns.</p><p id="p-0008" num="0007">According to some embodiments of the present disclosure, there is provided a method for reordering data for storage, the method including detecting a data access pattern, associated with an application, for accessing a data, generating a remapping function based on a data access pattern information, the remapping function including operations to determine a reordering of the data based on address information for the data, receiving the data at a storage device, the data being ordered according to a first layout sequence, reordering the data, by the storage device, based on the remapping function, and storing the data, at the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</p><p id="p-0009" num="0008">The reordering the data based on the remapping function may include receiving, at the storage device, the remapping function or a mapping table.</p><p id="p-0010" num="0009">The data access pattern may be detected by determining the data access pattern information from an application source code of the application.</p><p id="p-0011" num="0010">The data access pattern may include a tiling access pattern or a swizzling access pattern.</p><p id="p-0012" num="0011">The data may include an image data, the tiling access pattern corresponds to a partitioning of the image data by a graphics processing unit (GPU) or a central processing unit (CPU), the second layout sequence includes a tile sequence corresponding to the tiling access pattern, and the first layout sequence may include a linear sequence corresponding to an ordering of rows associated with the image data.</p><p id="p-0013" num="0012">The second layout sequence may further include an ordering of sub-tiles within an ordering of tiles, the ordering of sub-tiles corresponding to the swizzling access pattern.</p><p id="p-0014" num="0013">The data access pattern information may include tile dimension information, tile order information, image dimension information, or sub-tile order information.</p><p id="p-0015" num="0014">The detecting the data access pattern or the generating the remapping function may be performed by a compiler.</p><p id="p-0016" num="0015">The method may further include storing, at the storage device a first mapping table associated with accessing the data according to the first layout sequence, and a second mapping table associated with accessing the data according to the second layout sequence, receiving, by the storage device, a request to access the data according to the first layout sequence or the second layout sequence, and returning the data, by the storage device, in the first layout sequence based on the first mapping table or in the second layout sequence based on the second mapping table.</p><p id="p-0017" num="0016">The first mapping table may include a linear access mapping table, the second mapping table may include a tiling access mapping table, and the returning the data in the second layout sequence may include returning a first tile and prefetching a second tile based on a next tile prefetch.</p><p id="p-0018" num="0017">According to other embodiments of the present disclosure, there is provided a storage device for reordering data for storage, the storage device being configured to receive a data that is ordered according to a first layout sequence, reorder the data based on a remapping function, the remapping function being generated based on a data access pattern, associated with an application, for accessing the data, and store the data, at a nonvolatile memory of the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</p><p id="p-0019" num="0018">The reordering the data based on the remapping function may include receiving, at the storage device, the remapping function or a mapping table.</p><p id="p-0020" num="0019">The data access pattern may be detected by determining a data access pattern information from an application source code of the application.</p><p id="p-0021" num="0020">The storage device may be configured to store a first mapping table associated with accessing the data according to the first layout sequence, store a second mapping table associated with accessing the data according to the second layout sequence, receive a request to access the data according to the first layout sequence or the second layout sequence, and return the data in the first layout sequence based on the first mapping table or in the second layout sequence based on the second mapping table.</p><p id="p-0022" num="0021">The first mapping table may include a linear access mapping table, the second mapping table may include a tiling access mapping table, and the returning the data in the second layout sequence may include returning a first tile and prefetching a second tile based on a next tile prefetch.</p><p id="p-0023" num="0022">According to other embodiments of the present disclosure, there is provided a system for reordering data for storage, the system including a host, and a storage device, wherein the storage device is configured to receive a data that is ordered according to a first layout sequence, reorder the data based on a remapping function, the remapping function being generated based on a data access pattern, associated with an application, for accessing the data, and store the data, at a nonvolatile memory of the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</p><p id="p-0024" num="0023">The reordering the data based on the remapping function may include receiving, at the storage device, the remapping function or a mapping table.</p><p id="p-0025" num="0024">The data access pattern may be detected by determining a data access pattern information from an application source code of the application.</p><p id="p-0026" num="0025">The storage device may be configured to store a first mapping table associated with accessing the data according to the first layout sequence, store a second mapping table associated with accessing the data according to the second layout sequence, receive a request to access the data according to the first layout sequence or the second layout sequence, and return the data in the first layout sequence based on the first mapping table or in the second layout sequence based on the second mapping table.</p><p id="p-0027" num="0026">The first mapping table may include a linear access mapping table, the second mapping table may include a tiling access mapping table, and the returning the data in the second layout sequence may include returning a first tile and prefetching a second tile based on a next tile prefetch.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0028" num="0027">Non-limiting and non-exhaustive embodiments of the present disclosure are described with reference to the following figures, wherein like reference numerals refer to like parts throughout the various views unless otherwise specified.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a system diagram depicting a system for reordering data for storage, according to some embodiments of the present disclosure,</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a conceptual diagram depicting a system and method for reordering data for storage, according to some embodiments of the present disclosure,</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a conceptual diagram depicting a method for generating a remapping function for reordering data for storage, according to some embodiments of the present disclosure,</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a conceptual diagram depicting a use of vectors to represent tiles and data access patterns within tiles for reordering data for storage, according to some embodiments of the present disclosure,</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a conceptual diagram depicting a use of mapping tables for reordering data for storage, according to some embodiments of the present disclosure,</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a conceptual diagram depicting an improved processing time associated with the reordering of data for storage, according to some embodiments of the present disclosure, and</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart depicting example operations of methods for reordering data for storage, according to some embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0036" num="0035">Corresponding reference characters indicate corresponding components throughout the several views of the drawings. Skilled artisans will appreciate that elements in the figures are illustrated for simplicity and clarity, and have not necessarily been drawn to scale. For example, the dimensions of some of the elements, layers, and regions in the figures may be exaggerated relative to other elements, layers, and regions to help to improve clarity and understanding of various embodiments. Also, common but well-understood elements and parts not related to the description of the embodiments might not be shown in order to facilitate a less obstructed view of these various embodiments and to make the description clear.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0037" num="0036">Features of the inventive concept and methods of accomplishing the same may be understood more readily by reference to the detailed description of embodiments and the accompanying drawings. Hereinafter, embodiments will be described in more detail with reference to the accompanying drawings. The described embodiments, however, may be embodied in various different forms, and should not be construed as being limited to only the illustrated embodiments herein. Rather, these embodiments are provided as examples so that this disclosure will be thorough and complete, and will fully convey the aspects and features of the present inventive concept to those skilled in the art. Accordingly, processes, elements, and techniques that are not necessary to those having ordinary skill in the art for a complete understanding of the aspects and features of the present inventive concept may not be described.</p><p id="p-0038" num="0037">Unless otherwise noted, like reference numerals, characters, or combinations thereof denote like elements throughout the attached drawings and the written description, and thus, descriptions thereof will not be repeated. Further, parts not related to the description of the embodiments might not be shown to make the description clear. In the drawings, the relative sizes of elements, layers, and regions may be exaggerated for clarity.</p><p id="p-0039" num="0038">In the detailed description, for the purposes of explanation, numerous specific details are set forth to provide a thorough understanding of various embodiments. It is apparent, however, that various embodiments may be practiced without these specific details or with one or more equivalent arrangements.</p><p id="p-0040" num="0039">It will be understood that, although the terms &#x201c;zeroth,&#x201d; &#x201c;first,&#x201d; &#x201c;second,&#x201d; &#x201c;third,&#x201d; etc., may be used herein to describe various elements, components, regions, layers and/or sections, these elements, components, regions, layers and/or sections should not be limited by these terms. These terms are used to distinguish one element, component, region, layer or section from another element, component, region, layer or section. Thus, a first element, component, region, layer or section described below could be termed a second element, component, region, layer or section, without departing from the spirit and scope of the present disclosure.</p><p id="p-0041" num="0040">It will be understood that when an element or component is referred to as being &#x201c;on,&#x201d; &#x201c;connected to,&#x201d; or &#x201c;coupled to&#x201d; another element or component, it can be directly on, connected to, or coupled to the other element or component, or one or more intervening elements or components may be present. However, &#x201c;directly connected/directly coupled&#x201d; refers to one component directly connecting or coupling another component without an intermediate component. Meanwhile, other expressions describing relationships between components such as &#x201c;between,&#x201d; &#x201c;immediately between&#x201d; or &#x201c;adjacent to&#x201d; and &#x201c;directly adjacent to&#x201d; may be construed similarly. In addition, it will also be understood that when an element or component is referred to as being &#x201c;between&#x201d; two elements or components, it can be the only element or component between the two elements or components, or one or more intervening elements or components may also be present.</p><p id="p-0042" num="0041">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the present disclosure. As used herein, the singular forms &#x201c;a&#x201d; and &#x201c;an&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises,&#x201d; &#x201c;comprising,&#x201d; &#x201c;have,&#x201d; &#x201c;having,&#x201d; &#x201c;includes,&#x201d; and &#x201c;including,&#x201d; when used in this specification, specify the presence of the stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof. As used herein, the term &#x201c;and/or&#x201d; includes any and all combinations of one or more of the associated listed items.</p><p id="p-0043" num="0042">As used herein, the term &#x201c;substantially,&#x201d; &#x201c;about,&#x201d; &#x201c;approximately,&#x201d; and similar terms are used as terms of approximation and not as terms of degree, and are intended to account for the inherent deviations in measured or calculated values that would be recognized by those of ordinary skill in the art. &#x201c;About&#x201d; or &#x201c;approximately,&#x201d; as used herein, is inclusive of the stated value and means within an acceptable range of deviation for the particular value as determined by one of ordinary skill in the art, considering the measurement in question and the error associated with measurement of the particular quantity (i.e., the limitations of the measurement system). For example, &#x201c;about&#x201d; may mean within one or more standard deviations, or within &#xb1;30%, 20%, 10%, 5% of the stated value. Further, the use of &#x201c;may&#x201d; when describing embodiments of the present disclosure refers to &#x201c;one or more embodiments of the present disclosure.&#x201d;</p><p id="p-0044" num="0043">When one or more embodiments may be implemented differently, a specific process order may be performed differently from the described order. For example, two consecutively described processes may be performed substantially at the same time or performed in an order opposite to the described order.</p><p id="p-0045" num="0044">Any of the components or any combination of the components described (e.g., in any system diagrams included herein) may be used to perform one or more of the operations of any flow chart included herein. Further, (i) the operations are example operations, and may involve various additional operations not explicitly covered, and (ii) the temporal order of the operations may be varied.</p><p id="p-0046" num="0045">The electronic or electric devices and/or any other relevant devices or components according to embodiments of the present disclosure described herein may be implemented utilizing any suitable hardware, firmware (e.g. an application-specific integrated circuit), software, or a combination of software, firmware, and hardware. For example, the various components of these devices may be formed on one integrated circuit (IC) chip or on separate IC chips. Further, the various components of these devices may be implemented on a flexible printed circuit film, a tape carrier package (TCP), a printed circuit board (PCB), or formed on one substrate.</p><p id="p-0047" num="0046">Further, the various components of these devices may be a process or thread, running on one or more processors, in one or more computing devices, executing computer program instructions and interacting with other system components for performing the various functionalities described herein. The computer program instructions are stored in a memory which may be implemented in a computing device using a standard memory device, such as, for example, a random access memory (RAM). The computer program instructions may also be stored in other non-transitory computer readable media such as, for example, a CD-ROM, flash drive, or the like. Also, a person of skill in the art should recognize that the functionality of various computing devices may be combined or integrated into a single computing device, or the functionality of a particular computing device may be distributed across one or more other computing devices without departing from the spirit and scope of the embodiments of the present disclosure.</p><p id="p-0048" num="0047">Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which the present inventive concept belongs. It will be further understood that terms, such as those defined in commonly used dictionaries, should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and/or the present specification, and should not be interpreted in an idealized or overly formal sense, unless expressly so defined herein.</p><p id="p-0049" num="0048">As mentioned above, in the field of computer storage, a storage device may be used to store data that may be associated with a matrix (e.g., a bitmap representing pixels or tiles of an image). A program (or application) may perform matrix manipulation with the data. For example, an application may access the data in smaller portions than the whole matrix at once, or the application may access a given region of the matrix and ignore other regions. Thus, the application may have a particular data access pattern that deviates from a different data access pattern. For example, the particular data access pattern may be a tiling access pattern and/or a swizzling access pattern, while the different data access pattern may be a row by row, or line by line, linear access pattern.</p><p id="p-0050" num="0049">A &#x201c;tile&#x201d; may be a contiguous block of pixels that spans more than one row. For example, tiles may be created by breaking down a matrix (e.g., an image, which is a special type of matrix) into smaller pieces. Tiles are usually in a square or rectangular shape. A &#x201c;sub-tile&#x201d; may be a smaller contiguous block of pixels within a tile. A &#x201c;tiling access pattern&#x201d; may be an access pattern where data is accessed according to tiles and/or sub-tiles corresponding to a matrix or an image, rather than being accessed according to the whole matrix or whole image (or according to a linear sequence associated with the whole matrix or whole image). A &#x201c;swizzling access pattern&#x201d; may be an access pattern where data may be accessed according to a given access pattern (e.g., a predetermined access pattern, like a Z-order or any other pattern).</p><p id="p-0051" num="0050">The data may be stored in the storage device according to the different data access pattern. Accordingly, a processing time (e.g., a kernel execution time) for accessing the data according to the particular data access pattern may be delayed due to the data being scattered within the storage device in relation to the particular data access pattern.</p><p id="p-0052" num="0051">A storage device that takes into consideration aspects of computer architecture (e.g., DRAM access, CPU cache access, etc.) when providing placement and prefetching of data, may improve overall performance of a computer system (e.g., there may be a multiple order of magnitude performance difference compared to native, or conventional, approaches).</p><p id="p-0053" num="0052">Aspects of embodiments of the present disclosure provide a compiler-assisted approach that may generate a data layout (e.g., an optimized data layout) that may be accessed more efficiently by many applications (e.g., applications involving matrix manipulation, such as machine learning applications).</p><p id="p-0054" num="0053">Conventional graphics processing unit (GPU) based image processing methods may frequently have tiling/swizzling access patterns. Additionally, deep learning applications (or workloads) (e.g., image recognition, instance segmentation, other image processing, etc.) may also apply tiling/swizzling techniques. For example, convolutional neural networks and other image processing techniques may apply filters or convolutional kernels that compute outputs for regions or patches of input bitmaps that span multiple rows of the input bitmaps. In addition, filters or convolutional kernels may span multiple channels of a multi-channel bitmap.</p><p id="p-0055" num="0054">Depending on a workload's algorithm, a GPU kernel might want to access data (e.g., an image, etc.) in a smaller chunk at a time. The smaller chunk may be called a tile. The tile may fit into a cache line so that the kernel ends up having good cache efficiency. Thus, data may be reorganized into tiles (e.g., tiles corresponding to a data access pattern associated with an algorithm to be applied to the data) to improve performance.</p><p id="p-0056" num="0055">Data may be partitioned by a program for tiling, and the data may be stored in a linear 2D layout (e.g., in consecutive logical pages) in a storage device (e.g., the program may have a tiling access pattern, while the data is stored in a linear layout). In many cases, an application may request data access by tiling partition, rather than in consecutive logical pages. However, a storage device may not consider (e.g., may not be optimized for) GPU tiling and swizzling access patterns. For example, a storage device may store data in a linear format that may be less efficient (or less than optimal) for a given GPU workload. For example, data processing inefficiencies may occur when data is stored without consideration of data access patterns because the data may be scattered for a particular data access pattern.</p><p id="p-0057" num="0056">Accordingly, system performance may be negatively affected as a result of an inefficient data layout. For example, a GPU kernel execution time and a number of accesses may be significantly increased.</p><p id="p-0058" num="0057">To improve system or device performance, a device controller may be configured to detect a data access pattern and remap the data accordingly.</p><p id="p-0059" num="0058">In some embodiments, data may be reordered according to a tiling/swizzling access pattern for cache line addressable storage while still allowing for linear access.</p><p id="p-0060" num="0059">Many improvements and advantages may be obtained by a system for reordering data, according to aspects of embodiments of the present disclosure. For example, an overall load latency may be reduced due to data being packed together (e.g., in a tile corresponding to the access pattern of the algorithm that is processing the data), such that the data of a tile may be read all at once, instead of being read and transferred from multiple locations. Bandwidth use may be improved due to storing the data in a packed manner because the data may be transferred more efficiently and with a fewer number of transfers. At a cache line level, more efficiently packed data may ensure that all the data in a cache line is useful. At a device level (e.g., a NAND chip level), more efficiently packed data may ensure that all the data in a NAND page is useful. Thus, for transferring the same amount of useful data, bandwidth may be improved. Efficiency may be improved because the storage device may be more likely to pre-fetch the next data that will be used by the algorithm. Power consumption may be indirectly improved based on a more efficient use of bandwidth because less time and energy may be consumed while transferring data, and execution may be performed more quickly. Memory space may be more efficiently used for certain workloads (e.g., general-purpose graphics processing unit applications (GPGPU), artificial intelligence/machine learning, and image and graph processing) because an application may consume less cache line for the same amount of read when all of the data in the cache line is useful.</p><p id="p-0061" num="0060">A mechanism for reordering data for storage and prefetch based on a data access pattern may improve a system performance by providing a storage device that is capable of storing data according to a layout sequence that may be accessed more efficiently by a host computer.</p><p id="p-0062" num="0061">For example, an application running on a host computer (or a remote computer from the host) may send image data to a GPU (or a CPU) of the host computer. A kernel (e.g., a computation kernel) associated with the GPU (or the CPU) may partition the image data, row by row (or line by line), and save the data (e.g., in a cache of the host computer) in a linear sequence. For example, an image from somewhere may be stored to a device (e.g., the host computer) without partitioning (e.g., as-is, or in a linear sequence). A compiler of the host computer (or the remote computer) may analyze an application source code (e.g., a computation kernel) of the application and extract (e.g., determine) a data access pattern from the application source code. The data access pattern may be described using a remapping function. For example, the compiler may generate a remapping function for reordering the data by extracting data access pattern information (e.g., tile dimension information, image dimension information, tile order information, and/or sub-tile order information) from the source code.</p><p id="p-0063" num="0062">The compiler may provide the remapping function to a computation kernel running on the GPU or the CPU. The GPU (or the CPU) may then send the data (arranged according to a linear sequence) and the remapping function (or information corresponding to the mapping function, e.g., a lookup or mapping table) to the storage device.</p><p id="p-0064" num="0063">The storage device may receive the data (e.g., at a cache of the storage device) in the linear sequence. A storage controller of the storage device may then (1) use the remapping function (or table) to reorder the data based on the access pattern associated with the data, and (2) store the data in a memory (e.g., a nonvolatile memory) of the storage device in a layout that may be more efficiently accessed, based on the access pattern.</p><p id="p-0065" num="0064">The storage controller may maintain mapping tables associated with the data (e.g., a tiling access mapping table and/or a linear access mapping table). When the host computer sends a read request to access the data according to a tiling access pattern, the storage controller may provide the first tile to the host computer based on the tiling access mapping table information and quickly prefetch the subsequent tiles using a simple next tile prefetch. When the host computer sends a read request to access the data according to a linear access pattern, the storage controller may provide the data to the host based on the linear access mapping table. Thus, the storage device may provide the data to the host in an efficient and flexible manner.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a system diagram depicting a system for reordering data for storage, according to some embodiments of the present disclosure.</p><p id="p-0067" num="0066">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a system for reordering data for storage <b>1</b>, may include a storage device <b>100</b> (e.g., a solid state drive (SSD)) and a host <b>200</b> (e.g., a host computer).</p><p id="p-0068" num="0067">The host <b>200</b> may include a graphics processing unit (GPU) and/or a central processing unit (CPU), hereinafter &#x201c;GPU <b>210</b>&#x201d;. (That is, throughout the present disclosure, operations described as being related to or performed by a GPU or components of the GPU may, likewise, be related to or performed by a CPU or components of the CPU.) A kernel <b>211</b> (e.g., a computation kernel loaded to the GPU's device memory) and a GPU cache <b>212</b> may be associated with the GPU <b>210</b>. For a CPU case, the kernel <b>211</b> may be loaded into a host memory <b>250</b> (e.g., a system memory). The GPU cache <b>212</b> may have a given size (e.g., a fixed size), which may be a relatively small size. The host <b>200</b> may include the host memory <b>250</b> (e.g., a random access memory (RAM)). Computer programs may be located in (e.g., run on) the host memory <b>250</b>. For example, a compiler <b>230</b> and an application <b>300</b> (or workload) may be located in the host memory <b>250</b>.</p><p id="p-0069" num="0068">Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts the compiler <b>230</b> and the application <b>300</b> as being located at the host <b>200</b>, it should be understood that the present disclosure is not limited thereto. For example, the system <b>1</b> may have a variety of configurations. In some embodiments, the compiler <b>230</b> may be a compiler (e.g., a just-in-time compiler) running on the same machine as the application <b>300</b>. (In some embodiments, an interpreter may be used alongside, in place of, or in combination with a just-in-time compiler.) In some embodiments, the compiler <b>230</b> and the application <b>300</b> may run on a different host than the host <b>200</b>. In some embodiments, the compiler <b>230</b> and the application <b>300</b> may run on different hosts from each other. For example, the compilation may be done on a different machine (e.g., a different host), and once a program binary code is generated (e.g., by the compiler <b>230</b>), the program binary code may run on any compatible machine.</p><p id="p-0070" num="0069">The storage device <b>100</b> may be connected to the host <b>200</b> via a data bus <b>20</b>. The data bus may include (e.g., may be implemented by way of) a variety of technologies (e.g., peripheral component interconnect express (PCIe), Compute Express Link (CXL), etc.). The storage device <b>100</b> may include a device controller <b>110</b>, a storage device cache <b>120</b> (e.g., a volatile memory), and a nonvolatile memory <b>130</b>. The device controller <b>110</b> may include (e.g., may be implemented by way of) a processor and memory with embedded logic (e.g., a microcontroller, an FPGA, etc.) for handling requests from the host <b>200</b> to write data to or to read data from the storage device <b>100</b>. The storage device cache <b>120</b> may include a cache line <b>121</b> (e.g., a portion of the storage device cache <b>120</b> that is the smallest unit of access in the storage device cache <b>120</b>). The cache line <b>121</b> may have a given size (e.g., a fixed size), which may be a relatively small size. The nonvolatile memory <b>130</b> may include pages (e.g., a portion of the nonvolatile memory <b>130</b> that is the smallest unit of access in the nonvolatile memory <b>130</b>) having a given size (e.g., a fixed size). The pages may be grouped into blocks, and the blocks may be grouped into sectors (e.g., a sector <b>131</b>). The storage device <b>100</b> may be a cache-line addressable and/or block addressable storage device.</p><p id="p-0071" num="0070">Accordingly, a layout sequence of data stored (e.g., temporarily stored) in the storage device cache <b>120</b>, the GPU cache <b>212</b>, and/or the nonvolatile memory <b>130</b> may significantly impact their respective efficiencies. For example, scattered data and/or large portions of data may negatively impact efficiency because it may be less efficiently laid out within the given cache or nonvolatile memory spaces.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a simplified conceptual diagram depicting a system and method for reordering data for storage, according to some embodiments of the present disclosure.</p><p id="p-0073" num="0072">Referring to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the application <b>300</b> may send a data <b>400</b> (e.g., an image data, or some other data that may correspond to a matrix for matrix manipulation) to the GPU <b>210</b>. The GPU <b>210</b> (e.g., via the kernel <b>211</b>) may partition the data <b>400</b> into smaller portions (e.g., tiles) for processing the data <b>400</b>. For example, the GPU <b>210</b> may partition the data <b>400</b> into a tile zero T<b>0</b> (&#x201c;T<b>0</b>&#x201d;), a tile one T<b>1</b> (&#x201c;T<b>1</b>&#x201d;), a tile two T<b>2</b> (&#x201c;T<b>2</b>&#x201d;), and a tile three T<b>3</b> (&#x201c;T<b>3</b>&#x201d;). The GPU <b>210</b> may partition each tile further, e.g., into sub-tiles ST. Each sub-tile ST may be partitioned further (e.g., each tile or sub-tile ST may be partitioned down to a pixel level). As discussed in further detail below with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, each tile may be represented by a vector. The tiles T<b>0</b> to T<b>3</b> may be provided (e.g., ordered) according to a tile sequence associated with the partitioning of the data <b>400</b>.</p><p id="p-0074" num="0073">The GPU <b>210</b> may store (e.g., temporarily store) the data <b>400</b> in a cache (e.g., in the GPU cache <b>212</b> or in the storage device cache <b>120</b>, see <figref idref="DRAWINGS">FIG. <b>1</b></figref>) according to a first layout sequence <b>501</b>. For example, the GPU <b>210</b> may process the data <b>400</b> in a row by row (or line by line) fashion and store the data in a linear sequence, as depicted by a partitioned data having a linear data access pattern <b>401</b>. Accordingly, the first layout sequence <b>501</b> may include (e.g., may be) a linear sequence having one or more tiles corresponding to different logical page addresses LPAs (e.g., an LPA zero LPA<b>0</b> (&#x201c;LPA<b>0</b>&#x201d;), an LPA one LPA<b>1</b> (&#x201c;LPA<b>1</b>&#x201d;), an LPA two LPA<b>2</b> (&#x201c;LPA<b>2</b>&#x201d;), and an LPA three LPA<b>3</b> (&#x201c;LPA<b>3</b>&#x201d;). For example, two sub-tiles ST corresponding to T<b>0</b> (and respectively corresponding to positions 0 and 1 in the partitioned data having a linear data access pattern <b>401</b>) may be located in LPA<b>0</b>, while another two sub-tiles ST corresponding to T<b>0</b> (and respectively corresponding to positions 4 and 5 in the partitioned data having the linear data access pattern <b>401</b>) may be located in LPA<b>1</b>. For example, the sub-tiles ST associated with TO may be scattered across different logical page addresses.</p><p id="p-0075" num="0074">In some embodiments, the storage device <b>100</b> may store the data <b>400</b> in the nonvolatile memory <b>130</b> according to the first layout sequence <b>501</b> (e.g., for later access by the application <b>300</b>). In such embodiments, the first layout sequence <b>501</b> may be suitable for processing, e.g., by the GPU <b>210</b> if the application <b>300</b> were to request the data <b>400</b> according to a data access pattern DAP corresponding to the first layout sequence <b>501</b> (e.g., in the linear sequence).</p><p id="p-0076" num="0075">An application <b>300</b> may, however, access the data <b>400</b> according to a different data access pattern DAP. For example, the application <b>300</b> may request the data <b>400</b> according to a tiling access pattern, as depicted by a partitioned data having a tiling or swizzling data access pattern <b>402</b> (e.g., because the data <b>400</b> may be more easily processed in a tiling or swizzling sequence).</p><p id="p-0077" num="0076">An application <b>300</b> may also access the data <b>400</b> according to a swizzling access pattern (e.g., within each tile). For example, as depicted in the partitioned data having the tiling or swizzling data access pattern <b>402</b>, the swizzling data access pattern DAP may be a Z-order (or Morton-coded order) swizzling data access pattern DAP (e.g., in the shape of a &#x201c;z&#x201d;).</p><p id="p-0078" num="0077">A given data access pattern DAP may depend on the given application <b>300</b>. A different swizzling data access pattern DAP (e.g., other than a Z-order) may be used by a given application <b>300</b>. The application <b>300</b> may access the sub-tiles ST according to any swizzling access pattern. For example, the application <b>300</b> may request access to T<b>0</b>, such that the sub-tile ST corresponding to position 0 (in the partitioned data having the tiling or swizzling data access pattern <b>402</b>) is accessed three times in a row and the sub-tiles ST corresponding to positions 1, 2, and/or 3 (in the partitioned data having the tiling or swizzling data access pattern <b>402</b>) are not accessed (e.g., are ignored).</p><p id="p-0079" num="0078">Thus, in cases where the application <b>300</b> does not access the data <b>400</b> according to the first layout sequence (e.g., in the linear sequence), the processing of the data <b>400</b> by the GPU <b>210</b> may be stalled (or slowed) while the GPU <b>210</b> waits for a next sub-tile ST associated with a given data access pattern DAP of the application <b>300</b> to be located and retrieved (e.g., by a prefetcher of the storage device <b>100</b>, implemented by the device controller <b>110</b>).</p><p id="p-0080" num="0079">In some embodiments, and as discussed in further detail below in reference to <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>6</b></figref>, the compiler <b>230</b> may detect a data access pattern DAP in an application source code ASC associated with the application <b>300</b> (see <figref idref="DRAWINGS">FIG. <b>3</b></figref>). For example, data access pattern information DAPI may be annotated (e.g., by a programmer of the application <b>300</b>) for extraction by the compiler <b>230</b>, or the compiler may extract high-level data access pattern information DAPI automatically (e.g., by using an algorithm, such as a loop optimization algorithm). For example, a compiler may automatically extract high-level data access pattern information DAPI by analyzing a loop in the application source code (e.g., a function in the application source code) to determine stride, length, and other information. The compiler <b>230</b> may generate a remapping function RMF based on the data access pattern information DAPI. The remapping function RMF may be used to reorder the data <b>400</b> from the first layout sequence <b>501</b> to a second layout sequence <b>502</b> that may be more efficiently accessed by the application <b>300</b> according to a data access pattern DAP corresponding to the second layout sequence <b>502</b>.</p><p id="p-0081" num="0080">In some embodiments, the compiler <b>230</b> or a device controller <b>110</b> may use the remapping function to generate one or more mapping tables. For example, a first mapping table M<b>1</b> may be a linear access mapping table, and a second mapping table M<b>2</b> may be a tiling access mapping table (e.g., for a tiling and/or swizzling data access pattern DAP).</p><p id="p-0082" num="0081">In some embodiments, the compiler <b>230</b> may provide the remapping function RMF to the kernel <b>211</b>. The GPU <b>210</b> may send the remapping function RMF and/or the first mapping table M<b>1</b> (or a portion thereof) and/or the second mapping table M<b>2</b> (or a portion thereof) to the storage device <b>100</b>, and may send the data <b>400</b> (e.g., arranged according to the first layout sequence <b>501</b>) to the storage device <b>100</b>. The storage device <b>100</b> may receive the data <b>400</b> (e.g., at the storage device cache <b>120</b>), and the device controller <b>110</b> may use the remapping function RMF and/or the first mapping table M<b>1</b> (or a portion thereof) and/or the second mapping table M<b>2</b> (or a portion thereof) to reorder the data <b>400</b> from the first layout sequence <b>501</b> to the second layout sequence <b>502</b>. Accordingly, the device controller <b>110</b> may store the data <b>400</b> at physical page addresses PPA of the nonvolatile memory <b>130</b> according to the second layout sequence <b>502</b>. For example, T<b>0</b> may be stored in in a physical page address zero PPA<b>0</b> (&#x201c;PPA<b>0</b>&#x201d;), T<b>1</b> may be stored in in a physical page address one PPA<b>1</b> (&#x201c;PPA<b>1</b>&#x201d;), T<b>2</b> may be stored in in a physical page address two PPA<b>2</b> (&#x201c;PPA<b>2</b>&#x201d;), and T<b>3</b> may be stored in in a physical page address three PPA<b>3</b> (&#x201c;PPA<b>3</b>&#x201d;). Thus, the sub-tiles ST associated with each tile T<b>0</b> to T<b>3</b> may be located adjacently to one another, such that, for example, the sub-tiles ST corresponding to T<b>0</b> are not scattered (e.g., not separated by sub-tiles ST of other tiles, e.g., T<b>1</b> to T<b>3</b>).</p><p id="p-0083" num="0082">In some embodiments, the storage controller <b>110</b> may maintain (e.g., store) the first mapping table M<b>1</b> (e.g., the linear access mapping table) and the second mapping table M<b>2</b> (e.g., the tiling and/or swizzling access mapping table). When the host <b>200</b> sends a read request RD_REQ (see <figref idref="DRAWINGS">FIG. <b>5</b></figref>) to access the data <b>400</b> according to a tiling and/or swizzling data access pattern DAP, the device controller <b>110</b> may provide a first tile, for example, TO based on the second mapping table M<b>2</b> and may prefetch (e.g., quickly or efficiently prefetch) the subsequent tiles T<b>1</b> to T<b>3</b> using a simple next tile prefetch NP. When the host <b>200</b> sends a read request RD_REQ to access the data <b>400</b> according to a linear data access pattern DAP, the device controller <b>110</b> may provide the data <b>400</b> (e.g., the sub-tiles ST) based on the first mapping table M<b>1</b>. In some embodiments, the device controller <b>110</b> may control a prefetcher to prefetch a next page based on the data access pattern DAP currently being used (e.g., based on a which mapping table is currently being used). Accordingly, the storage device <b>100</b> may provide the data <b>400</b> to the host <b>200</b> in an efficient and flexible manner.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a simplified conceptual diagram depicting a method for generating a remapping function for reordering data for storage, according to some embodiments of the present disclosure.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a simplified conceptual diagram depicting a use of vectors to represent tiles and data access patterns within tiles for reordering data for storage, according to some embodiments of the present disclosure.</p><p id="p-0086" num="0085">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the compiler <b>230</b> may extract data access pattern information DAPI from the application source code ASC of the application <b>300</b>. For example, the data access pattern information DAPI may include tile dimension information, tile order information, image dimension information, and/or sub-tile order information. For example, tile dimension information may include a tile width, a tile height, and an image dimension. For example, each tile T<b>0</b> to T<b>3</b> may have a width of two (for example, may include two sub-tiles ST) and a height of two (for example, may include two sub-tiles. The image dimension may be 4&#xd7;4 (e.g., the overall image data <b>400</b> may have dimensions of four sub-tiles ST in width by four sub-tiles ST in height.</p><p id="p-0087" num="0086">Furthermore, the application source code ASC may provide at least some of the data access pattern information DAPI as vector information.</p><p id="p-0088" num="0087">For example, and referring to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, vectors may be used to represent tile order information and sub-tile order information. In image processing, a GPU may process data (e.g., image data) according to single instruction multiple data (SIMD) instructions that operate on vectors.</p><p id="p-0089" num="0088">For example, a vector may represent a tile, and each vector may identify how the data corresponding to the vector may be ordered. For example, a vector zero V<b>0</b> (&#x201c;V<b>0</b>&#x201d;) may represent TO of the partitioned data having the tiling or swizzling data access pattern <b>402</b>. Accordingly, the sub-tiles ST corresponding to positions 0, 1, 2, and 3 may be represented by a vector V<b>0</b>={0, 1, 2, 3}. The vector may be reordered, for example, by altering the vector to V<b>0</b>={0, 2, 1, 3}, or any other arrangement (e.g., any other predetermined arrangement) based on an SIMD instruction.</p><p id="p-0090" num="0089">For example, a vector may be pre-rearranged according to a swizzling configuration and/or a tiling configuration. For example, a given vector may be A={x, y, z, w} and the application <b>300</b> may want to access the vector in the order A.w A.w, A.x, A.y. This data access pattern DAP order may be written as A.wwxy, and may effectively be used to access vector {w, w, x, y} instead of the original {x, y, z, w}. In this example, the application <b>300</b> would ignore z completely. Accordingly, the compiler <b>230</b> may generate a more efficient storage order (e.g., an optimal storage order) {w, x, y, z} by pre-rearranging vectors to correspond to the data access pattern DAP.</p><p id="p-0091" num="0090">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the compiler <b>230</b> may extract the data access pattern information DAPI to generate the remapping function RMF. For example, a linear address LA of 7 (in the partitioned data having a linear data access pattern <b>401</b>) may be plugged into the remapping function RMF to be reordered to position 11 in the partitioned data having the tiling or swizzling data access pattern <b>402</b>, corresponding to T<b>2</b> and having an Index X in T<b>2</b> of 1 and an Index Y in T<b>2</b> of 1 (see also <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0092" num="0091">In some embodiments, the compiler <b>230</b> and/or the device controller <b>110</b> may use the remapping function RMF to generate (e.g., to maintain) the first mapping table M<b>1</b> (e.g., a linear access mapping table) and the second mapping table M<b>2</b> (e.g., the tiling and/or swizzling access pattern table). The device controller <b>110</b> may then reorder the data <b>400</b> according to the given data access pattern DAP. For example, the first mapping table M<b>1</b> may list logical page addresses LPAs (for a linear sequence of the data <b>400</b>) and cross-reference the logical page addresses LPAs with corresponding physical page addresses PPAs (associated with the second layout sequence <b>502</b>) in the nonvolatile memory <b>130</b> (and corresponding bitmap locations of the appropriate sub-tiles ST corresponding to the logical page addresses LPAs). The second mapping table M<b>2</b> may list the tile addresses TA and cross-reference the tile addresses TA with corresponding physical page addresses PPAs. The tile addresses TA may have a one-to-one correspondence with the physical page addresses PPA when the data has been written to the nonvolatile memory <b>130</b> according to the second layout sequence <b>502</b>. Accordingly, a data write sequence may be ordered according to tile order (instead of linear address order).</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a simplified conceptual diagram depicting a use of mapping tables for reordering data for storage, according to some embodiments of the present disclosure.</p><p id="p-0094" num="0093">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the storage controller <b>110</b> may process a read request RD_REQ or a write request WR_REQ by referring to the first mapping table M<b>1</b> when the requests correspond to a linear data access pattern DAP, or by referring to the second mapping table M<b>2</b> when the request correspond to a tiling and/or swizzling access pattern. The storage device <b>100</b> (e.g., a prefetcher of the storage device) may return the data <b>400</b> to a cache (e.g., the storage device cache <b>120</b>) according to the data access pattern DAP for the data <b>400</b>.</p><p id="p-0095" num="0094">When the data has been written to (e.g., stored in) the nonvolatile memory <b>130</b> according to the second layout sequence <b>502</b> and a read request RD_REQ corresponds to the second layout sequence <b>502</b>, the storage device <b>100</b> may return (e.g., efficiently return) the data <b>400</b> corresponding to, for example, PPA<b>0</b> and then return (e.g., efficiently return) the subsequent data <b>400</b> corresponding to, for example, PPA<b>1</b> to PPA<b>3</b> based on (e.g., according to or using) an efficient prefetch (e.g., a next tile prefetch NP).</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a simplified conceptual diagram depicting an improved processing time associated with the reordering of data for storage, according to some embodiments of the present disclosure.</p><p id="p-0097" num="0096">Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, each tile in the second layout sequence <b>502</b> may be efficiently located and returned. In comparison to storing the data <b>400</b> according to a sequence that deviates from the data access pattern DAP, a kernel execution time of the kernel <b>211</b> may be reduced when storing the data <b>400</b> according to the data access pattern DAP.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart depicting example operations of methods for reordering data for storage, according to some embodiments of the present disclosure.</p><p id="p-0099" num="0098">Referring to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a method <b>7000</b> for reordering data for storage may include detecting a data access pattern (operation <b>7100</b>), generating a remapping function based on data access pattern information (operation <b>7200</b>), receiving the data at a storage device, the data being ordered according to a first layout sequence (operation <b>7300</b>), reordering the data based on the remapping function (operation <b>7400</b>), storing the data according to a second layout sequence corresponding to the data access pattern (operation <b>7500</b>), and storing the data, in a nonvolatile memory of the storage device, according to a second layout sequence corresponding to the data access pattern (operation <b>7600</b>).</p><p id="p-0100" num="0099">Accordingly, embodiments of the present disclosure provide improvements to placement and prefetching of data within a storage device for more efficient access to the data. By using a compiler to detect a data access pattern for the data and using data access pattern information to generate a remapping function, a storage device may store the data and return the data such that an application requesting to access the data may do so efficiently, and system performance may be improved.</p><p id="p-0101" num="0100">While embodiments of the present disclosure have been particularly shown and described with reference to the embodiments described herein, it will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present invention as set forth in the following claims and their equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for reordering data for storage, the method comprising<claim-text>detecting a data access pattern, associated with an application, for accessing a data,</claim-text><claim-text>generating a remapping function based on a data access pattern information, the remapping function including operations to determine a reordering of the data based on address information for the data,</claim-text><claim-text>receiving the data at a storage device, the data being ordered according to a first layout sequence,</claim-text><claim-text>reordering the data, by the storage device, based on the remapping function, and</claim-text><claim-text>storing the data, at the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the reordering the data based on the remapping function comprises receiving, at the storage device, the remapping function or a mapping table.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data access pattern is detected by determining the data access pattern information from an application source code of the application.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data access pattern comprises a tiling access pattern or a swizzling access pattern.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein<claim-text>the data comprises an image data,</claim-text><claim-text>the tiling access pattern corresponds to a partitioning of the image data by a graphics processing unit (GPU) or a central processing unit (CPU),</claim-text><claim-text>the second layout sequence comprises a tile sequence corresponding to the tiling access pattern, and</claim-text><claim-text>the first layout sequence comprises a linear sequence corresponding to an ordering of rows associated with the image data.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the second layout sequence further comprises an ordering of sub-tiles within an ordering of tiles, the ordering of sub-tiles corresponding to the swizzling access pattern.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein<claim-text>the data access pattern information comprises tile dimension information, tile order information, image dimension information, or sub-tile order information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detecting the data access pattern or the generating the remapping function are performed by a compiler.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising<claim-text>storing, at the storage device<claim-text>a first mapping table associated with accessing the data according to the first layout sequence, and</claim-text><claim-text>a second mapping table associated with accessing the data according to the second layout sequence,</claim-text></claim-text><claim-text>receiving, by the storage device, a request to access the data according to the first layout sequence or the second layout sequence, and</claim-text><claim-text>returning the data, by the storage device, in the first layout sequence based on the first mapping table or in the second layout sequence based on the second mapping table.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the first mapping table comprises a linear access mapping table,</claim-text><claim-text>the second mapping table comprising a tiling access mapping table, and</claim-text><claim-text>the returning the data in the second layout sequence comprises returning a first tile and prefetching a second tile based on a next tile prefetch.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A storage device for reordering data for storage, the storage device being configured to<claim-text>receive a data that is ordered according to a first layout sequence,</claim-text><claim-text>reorder the data based on a remapping function, the remapping function being generated based on a data access pattern, associated with an application, for accessing the data, and</claim-text><claim-text>store the data, at a nonvolatile memory of the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The storage device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the reordering the data based on the remapping function comprises receiving, at the storage device, the remapping function or a mapping table.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The storage device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the data access pattern is detected by determining a data access pattern information from an application source code of the application.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The storage device of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the storage device is configured to<claim-text>store a first mapping table associated with accessing the data according to the first layout sequence,</claim-text><claim-text>store a second mapping table associated with accessing the data according to the second layout sequence,</claim-text><claim-text>receive a request to access the data according to the first layout sequence or the second layout sequence, and</claim-text><claim-text>return the data in the first layout sequence based on the first mapping table or in the second layout sequence based on the second mapping table.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The storage device of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein<claim-text>the first mapping table comprises a linear access mapping table,</claim-text><claim-text>the second mapping table comprising a tiling access mapping table, and</claim-text><claim-text>the returning the data in the second layout sequence comprises returning a first tile and prefetching a second tile based on a next tile prefetch.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A system for reordering data for storage, the system comprising<claim-text>a host, and</claim-text><claim-text>a storage device,</claim-text><claim-text>wherein the storage device is configured to<claim-text>receive a data that is ordered according to a first layout sequence,</claim-text><claim-text>reorder the data based on a remapping function, the remapping function being generated based on a data access pattern, associated with an application, for accessing the data, and</claim-text><claim-text>store the data, at a nonvolatile memory of the storage device, according to a second layout sequence corresponding to the data access pattern, the second layout sequence being different than the first layout sequence.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the reordering the data based on the remapping function comprises receiving, at the storage device, the remapping function or a mapping table.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the data access pattern is detected by determining a data access pattern information from an application source code of the application.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the storage device is configured to<claim-text>store a first mapping table associated with accessing the data according to the first layout sequence,</claim-text><claim-text>store a second mapping table associated with accessing the data according to the second layout sequence,</claim-text><claim-text>receive a request to access the data according to the first layout sequence or the second layout sequence, and</claim-text><claim-text>return the data in the first layout sequence based on the first mapping table or in the second layout sequence based on the second mapping table.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein<claim-text>the first mapping table comprises a linear access mapping table,</claim-text><claim-text>the second mapping table comprising a tiling access mapping table, and</claim-text><claim-text>the returning the data in the second layout sequence comprises returning a first tile and prefetching a second tile based on a next tile prefetch.</claim-text></claim-text></claim></claims></us-patent-application>