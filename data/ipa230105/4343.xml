<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004344A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004344</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364023</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0488</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>041</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>167</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>165</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>1118</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>016</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0488</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190501</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04164</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Activity-Dependent Audio Feedback Themes For Touch Gesture Inputs</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Giles</last-name><first-name>Daniel Lee</first-name><address><city>Fremont</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Nassar</last-name><first-name>Abidshan Jeffrey</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods that provide audio feedback in response to gesture validity can provide a more intuitive interface that can train users to correctly complete gestures. Moreover, systems and methods that provide line-specific audio feedback can provide more specific feedback that can allow a user to better understand what sensing line is being contacted. The systems and methods can further include basing the audio feedback based at least in part on obtained activity data, such that invalid and valid feedbacks can provide different sounds dependent on the determined activity state.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="186.44mm" wi="124.12mm" file="US20230004344A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.35mm" wi="136.23mm" file="US20230004344A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="191.94mm" wi="162.31mm" orientation="landscape" file="US20230004344A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="222.08mm" wi="122.94mm" file="US20230004344A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="160.53mm" wi="140.21mm" orientation="landscape" file="US20230004344A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="206.93mm" wi="175.85mm" orientation="landscape" file="US20230004344A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="175.09mm" wi="136.31mm" file="US20230004344A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="219.96mm" wi="162.48mm" orientation="landscape" file="US20230004344A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="219.96mm" wi="162.48mm" orientation="landscape" file="US20230004344A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="213.44mm" wi="134.62mm" file="US20230004344A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="218.61mm" wi="187.20mm" orientation="landscape" file="US20230004344A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="209.47mm" wi="102.36mm" file="US20230004344A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="138.26mm" wi="123.95mm" orientation="landscape" file="US20230004344A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="232.83mm" wi="126.15mm" file="US20230004344A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="229.11mm" wi="126.15mm" file="US20230004344A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="241.89mm" wi="182.20mm" file="US20230004344A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="223.77mm" wi="125.48mm" file="US20230004344A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="207.86mm" wi="159.51mm" orientation="landscape" file="US20230004344A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="238.93mm" wi="182.03mm" orientation="landscape" file="US20230004344A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="239.18mm" wi="181.95mm" orientation="landscape" file="US20230004344A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD</heading><p id="p-0002" num="0001">The present disclosure relates generally to providing audio feedback in response to a touch input. More particularly, the present disclosure relates to receiving activity data and touch gesture input data and determining an audio feedback based on the activity data and the gesture's validity.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Touch based ambient devices which use gestures as an input mechanism require accurate motions from users to ensure accurate gesture recognition. Gestures performed by users however, have a great degree of variability, which can make it very difficult for the machine learning algorithms to recognize the user's intent. Even with visual or haptic feedback users may still be confused as to what is an acceptable versus what is an unacceptable gesture. This leads to reduced recognition rates during runtime, and higher frustration by users.</p><p id="p-0004" num="0003">Moreover, inputs can be limited for some devices, and devices may fail to leverage activity dependent data for different inputs, which can further restrict the corpus of inputs available.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">Aspects and advantages of embodiments of the present disclosure will be set forth in part in the following description, or can be learned from the description, or can be learned through practice of the embodiments.</p><p id="p-0006" num="0005">One example aspect of the present disclosure is directed to a computing system for an interactive object. The system can include an input sensor, one or more activity sensors configured to generate activity data associated with user activity, and one or more control circuits configured to perform operations. The operations can include obtain input data associated with at least one input detected by the input sensor. The operations can include determine an activity state of the user based at least in part on the activity data generated by the one or more activity sensors. In some implementations, the operations can include determine a gesture validity associated with the at least one input detected by the input sensor. The operations can include determine an audio feedback based at least in part on the activity state and the gesture validity. The system can include one or more audio output devices configured to provide the audio feedback based at least in part on the activity state and the gesture validity.</p><p id="p-0007" num="0006">Another example aspect of the present disclosure is directed to a computer-implemented method. The method can include detecting, by a computing system including one or more processors, a respective response associated with each of a plurality of parallel sensing lines of a touch sensor in response to at least one touch input. The method can include determining, by the computing system, a line-specific audio feedback for each sensing line associated with the touch input. The method can include sending, by the computing system, a first set of instructions to an audio output device to provide the line-specific audio feedback. The method can include determining, by the computing system, whether the touch input can be associated with at least one of a plurality of actions. In some implementations, the method can include sending, by the computing system, a second set of instructions to the audio output device to provide a gesture audio feedback if the touch input can be associated with at least one of the plurality of actions.</p><p id="p-0008" num="0007">Another example aspect of the present disclosure is directed to an interactive object. The interactive object can include an article having one or more touch sensors integrated therein for recognizing a user gestural input and an electronic device associated with the article, the electronic device can be communicatively coupled to the one or more touch sensors of the article. The electronic device can have a processor and an audio output device capable of providing audio feedback to the user. The interactive object can include one or more contextual activity sensors associated with at least one of the article or the electronic device. The one or more contextual activity sensors can include at least one of an inertial motion sensor, a location sensor, a proximity sensor, or other type of sensor. In some implementations, the electronic device can be configured and programmed to perform operations. The operations can include process readings from the one or more contextual activity sensors to determine which of a predetermined plurality of user activities the user is performing. During a first time interval in which the user is determined to be performing a first activity, the operations can include receiving a first touch gesture from the user; determining whether the first touch gesture is valid or invalid; and providing first audio feedback of a first sound category to the user. In some implementations, the first audio feedback can include a first sound if the first touch gesture is valid and can include a second sound if the first touch gesture is invalid. During a second time interval in which the user is determined to be performing a second activity different than the first activity, the operations can include receiving a second touch gesture from the user; determining whether the second touch gesture is valid or invalid; and providing second audio feedback of a second sound category to the user. The second sound category can be different than said first sound category. In some implementations, the second audio feedback can include a third sound if the second touch gesture is valid and can include a fourth sound if the second touch gesture is invalid. In some implementations, the user can be provided with audio feedback sounds that, in addition to confirming whether their touch gestures are valid, also provide confirmation of which activity the user has been determined to be performing.</p><p id="p-0009" num="0008">Other aspects of the present disclosure are directed to various systems, apparatuses, non-transitory computer-readable media, user interfaces, and electronic devices.</p><p id="p-0010" num="0009">These and other features, aspects, and advantages of various embodiments of the present disclosure will become better understood with reference to the following description and appended claims. The accompanying drawings, which are incorporated in and constitute a part of this specification, illustrate example embodiments of the present disclosure and, together with the description, serve to explain the related principles.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010">Detailed discussion of embodiments directed to one of ordinary skill in the art is set forth in the specification, which makes reference to the appended figures, in which:</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example computing environment including an interactive object having a touch sensor in accordance with example embodiments of the present disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example computing environment that includes an interactive object having a touch sensor in accordance with example embodiments of the present disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of a sensor system, such as can be integrated with an interactive object in accordance with example embodiments of the present disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an example of a touch sensor integrated with an interactive object in accordance with example embodiments of the present disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a block diagram of an example line-specific audio feedback system according to example embodiments of the present disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref> are top and bottom views, respectively, depicting an example of a touch sensor including individual subsets of sensing lines coupled to opposite sides of a flexible substrate in accordance with example embodiments of the present disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts a block diagram of an example activity state tree according to example embodiments of the present disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts a block diagram of an example activity state determination system according to example embodiments of the present disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram depicting an example computing environment <b>1000</b>, illustrating the detection of gestures based on an identified input location of a touch sensor in accordance with example embodiments of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts a block diagram of an example gesture tree according to example embodiments of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts a block diagram of an example thread visualizer application according to example embodiments of the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram depicting a context switching system for touch inputs in accordance with example embodiments of the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts a flow chart diagram of an example method to perform providing audio feedback in response to an activity state and user input according to example embodiments of the present disclosure.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts a flow chart diagram of an example method to perform providing line-specific audio feedbacks according to example embodiments of the present disclosure.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>16</b></figref> depicts a flow chart diagram of an example method to perform providing an audio feedback based on gesture validity according to example embodiments of the present disclosure.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>17</b></figref> depicts a flow chart diagram of an example method to perform determining an audio feedback based on activity data and input data according to example embodiments of the present disclosure.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> depicts a block diagram of an example computing system that performs determining an audio feedback based on activity data and input data according to example embodiments of the present disclosure.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> depicts a block diagram of an example computing device that performs determining an audio feedback based on activity data and input data according to example embodiments of the present disclosure.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>18</b>C</figref> depicts a block diagram of an example computing device that performs determining an audio feedback based on activity data and input data according to example embodiments of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0031" num="0030">Reference numerals that are repeated across plural figures are intended to identify the same features in various implementations.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0032" num="0031">Generally, the present disclosure relates to systems and methods for providing feedback in response to inputs. For example, a computing system can include a plurality of sensors, which may include one or more activity sensors and one or more input sensors (e.g., touch sensors). The one or more activity sensors can be configured to generate activity data descriptive of a user's activity state, and the one or more touch sensors can include a plurality of sensing lines configured to receive touch inputs. The activity data can be processed to determine the activity state (e.g., a user is at a high activity level, a user is at a low activity level, a user is at home, a user is at work, a user is listening to music, a user is commuting, etc.), which can then be used to determine a plurality of functions, or actions, associated with that activity state. The plurality of functions, or actions (e.g., next song, volume up, call another device, turn on, turn off, etc.), can be associated with a plurality of gestures for selection (e.g., a swipe up, a swipe down, touching a first line and a third line, etc.). The systems and methods can then process the touch input to determine if the touch input is associated with any of the plurality of gestures for the plurality of functions. An audio feedback can then be determined and provided based on the activity state and whether the touch input matches one or more of the plurality of gestures. For example, if a user is determined to be at a high activity level the sound provided can be different than the possible sounds provided when a user is at a low activity level. Moreover, if the touch input is determined to match one or more of the plurality of gestures, the sound provided can be different than the sound provided if the touch input does not match one of the plurality of gestures. In some implementations, if there is not a match, no sound may be played or may cause haptic feedback to occur.</p><p id="p-0033" num="0032">The systems and methods in accordance with example implementations can provide for interface training and input acknowledgement. For example, the systems and methods can further include line-specific audio feedback for training a user to input gestures for selecting functions. More specifically, the line-specific audio feedback can provide a user with a notification to inform which sensing lines are being contacted. After completion of the gesture, a notification can be provided to inform the user if the gesture is valid or invalid, such that a valid gesture is a gesture that matches one or more of the plurality of gestures associated with the plurality of functions. In some implementations, the notifications can differ based on which activity state the user is determined to be in. Additionally and/or alternatively, an audio notification can be provided to indicate the activity state is determined to have changed (e.g., a change in activity level or a change in location).</p><p id="p-0034" num="0033">Generally, the present disclosure is directed to audio feedback in response to touch gesture inputs. More specifically, the systems and methods disclosed herein can receive a gesture input, process the gesture input to determine if the gesture input is associated with any actions, and provide audio feedback based at least in part on the determination. For example, a user can be wearing a smart wearable with a plurality of conductive lines that form one or more capacitive-array touch sensors. The user can swipe down on the lines to input a down swipe gesture. The systems and methods can process the touch gesture and determine if there is a programmed action associated with the down gesture. If there is a determined associated action, a first sound can be played, and if there is not, a second sound can play. If there is a determined associated action with a gesture or touch input, the gesture or touch input may be determined to be valid. If there is not a determined action associated with a gesture or touch input, the gesture or touch input may be determined to be invalid. In some examples, a gesture may be determined to be valid if the gesture is part of a set of predefined gestures. If a gesture is not part of a set of predefined gestures, the gesture may be determined to be invalid.</p><p id="p-0035" num="0034">Moreover, determining the action can be based at least in part on a user's activity or situation (e.g., a commute bundle, a workout bundle, a work bundle, etc.). The smart wearable or soft goods item can include different corpus of actions for different user activities or situations. For example, the system can include one or more inertial sensors, which can be used to determine movement by a user. If a user is determined to be in an active state, the system can have a set of actions that can differ from if the user is determined to be in an inactive state. For example, a lateral swipe during an active state can be associated with a burnt calories function that can provide feedback on a determined amount of calories burnt, while a lateral function during an inactive state can be associated with a weather function that can provide feedback on the weather forecast for the day.</p><p id="p-0036" num="0035">Alternatively and/or additionally, the systems and methods can have different sounds dependent on the user activity or situation. For example, if a user is determined to be working out, the audio feedback may be a first sound for a valid gesture, and the audio feedback can be a second sound for an invalid gesture. However, if the user is determined to be in a relaxation state or low activity state, the audio feedback may be a third sound for a valid gesture, and the audio feedback may be a fourth sound or no sound at all for an invalid gesture.</p><p id="p-0037" num="0036">The valid-invalid audio feedback can provide a more intuitive input interface. Alternatively and/or additionally, the system can provide different sounds for each sensor or sensor set. For example, one sensor set can provide a chime feedback, while another sensor set can provide a ding feedback. The varying sounds for each sensor or sensor set can allow a user to better understand which sensor or sensor set they are coming into contact with, which can provide a more intuitive interface. The different sounds can provide quicker feedback to in turn potentially lead to quicker training of successful gestures. In this implementation, the system can point the user more accurately to exactly where their gesture fell out of recognition. In some implementations, the touch sensors can be arranged in a similar way to strings on a guitar with thin parallel lines of sensor lines. In some implementations, the system can have multiple layers of &#x201c;string sensors&#x201d; for layered input or for cancelling out input noise. Moreover, in some implementations, the system can have one or more perpendicular lines of touch sensors for receiving more complex gesture inputs.</p><p id="p-0038" num="0037">In some implementations, each string can provide a separate sound but may be complementary to one another, similar to strings on a guitar, such that a full swipe can provide a combination of complementary sounds to provide a pleasing combined tone.</p><p id="p-0039" num="0038">Moreover, in some implementations, the system can provide an audio feedback for each string separately and at the end of the gesture can provide an output indicating a successful gesture if the gesture was determined to be valid or no sound at all if the gesture was determined to be invalid.</p><p id="p-0040" num="0039">In some implementations, the audio feedback can be a form of gesture training for a user to help a user get familiar with different gesture inputs. The valid sounds and invalid sounds can reinforce valid gestures and deter invalid gestures and can therefore train the user to more accurately complete gestures. After learning how to make the valid inputs, the user may turn off the audio feedback.</p><p id="p-0041" num="0040">In some implementations, the systems and methods can include a computing system. The computing system can include an input sensor, one or more activity sensors, one or more control circuits, and one or more audio output devices. In some implementations, the input sensor can include one or more touch sensors. The touch sensor can include a plurality of conductive sensing elements integrated with a flexible substrate. In some implementations, the plurality of conductive sensing elements can be of cylindrical shape. The one or more activity sensors can be configured to generate activity data descriptive of an activity state of a user. In some implementations, the one or more activity sensors can include at least one of an inertial sensor, a location sensor, or a proximity sensor. The one or more control circuits can be configured to obtain input data associated with an input to the input sensor. In some implementations, the input data can be touch data associated with a touch input to a touch sensor. The touch data being based at least in part on a respective response to the touch input by the plurality of conductive sensing elements. Additionally and/or alternatively, the one or more control circuits can be configured to determine the activity state of the user based at least in part on activity data generated by the one or more activity sensors. Moreover, the one or more control circuits can be configured to determine a gesture validity based on whether the touch input is associated with an action and determine an audio feedback based at least in part on the activity state and the gesture validity. In some implementations, determining the gesture validity can include determining a plurality of actions associated with the activity state and determining the gesture validity based on whether the touch input is associated with at least one action of the plurality of actions. In some implementations, the one or more audio output devices can be configured to provide a plurality of sounds based on the activity state and the gesture validity. The one or more audio output devices can include at least one of a mobile computing device or communicatively connected to a mobile computing device. The plurality of sounds can include a plurality of musical sounds descriptive of sounds output by a musical instrument.</p><p id="p-0042" num="0041">In some implementations, the one or more control circuits can be configured to determine an action associated with the touch input, and the sensor system can include one or more communication components configured to send instructions to a computing device to complete the action. Additionally, in some implementations, determining a gesture validity can include determining whether the touch input is indicative of one or more predefined gestures.</p><p id="p-0043" num="0042">Alternatively and/or additionally, the systems and methods for training a user to provide inputs to the interface can include an interactive object wearable by a user. The interactive object can include a touch sensor and one or more control circuits. The touch sensor can be implemented into the sleeve of a wearable garment. In some implementations, the touch sensor can include a plurality of parallel sensing lines elongated in one or more directions with a separation therebetween. The touch sensor can include an input surface configured to receive inputs from the user. The one or more control circuits can be in communication with the touch sensor, and the one or more control circuits can be configured to perform one or more operations. The operations can include detecting a respective response associated with each of the plurality of parallel sensing lines in response to a touch input to the touch sensor. The operations can further include determining a line-specific audio feedback for each sensing line associated with the touch input. A first set of instructions can then be sent. The first set of instructions can include instructions to an audio output device to play the line-specific audio feedback. In some implementations, the line-specific audio feedback for each sensing line can include a different sound frequency but be of a common musical note. The operations can include determining whether the touch input is associated with at least one of a plurality of actions. A second set of instructions can then be sent. The second set of instructions can include instructions to the audio output device to play a first sound if the touch input is associated with at least one of the plurality of actions. The second set of instructions can include at least one of play a second sound if the touch input is not associated with at least one of the plurality of actions or provide haptic feedback if the touch input is not associated with at least one of the plurality of actions.</p><p id="p-0044" num="0043">In some implementations, the interactive object can include a second plurality of parallel sensing lines elongated in perpendicular direction to the plurality of parallel sensing lines. Moreover, in some implementations, in response to determining the touch input is associated with at least one of the plurality of actions, action instructions can be sent to a computing device to complete the action. The audio output device and the interactive object can be communicatively paired via Bluetooth. In some implementations, the audio output device can be a mobile computing device or may be communicatively connected to a mobile computing device.</p><p id="p-0045" num="0044">Additionally, the operations can include determining a user activity state changes from a first activity state to a second activity state; determining an activity state sound associated with the second activity state; and sending activity state instructions to the audio output device to play the activity state sound.</p><p id="p-0046" num="0045">In some implementations, the interactive object wearable can include a smart wearable device. In some implementations, the one or more activity sensors can be configured to generate activity data descriptive of an activity state. Moreover, the smart wearable device can include one or more touch sensors configured to generate gesture data in response to receiving touch input. The smart wearable device may include one or more processors. The one or more processors can be configured to process the activity data generated by the one or more activity sensors to determine the activity state and determine a plurality of actions associated with that particular activity state. Moreover, the operations can include processing the gesture data to determine a touch input validity. Determining the touch input validity can include determining whether the touch inputs are valid or invalid. Valid touch inputs can include a valid gesture associated with at least one of the plurality of actions. Invalid touch inputs can include an invalid gesture that is not associated with any of the plurality of actions. The one or more processors can be configured to determine an audio feedback based at least in part on the activity state and touch input validity.</p><p id="p-0047" num="0046">The smart wearable device can further include one or more communication components configured to send instructions to one or more computing devices based at least in part on the gesture data and the touch input validity.</p><p id="p-0048" num="0047">The one or more activity sensors can include various sensor types for intaking various data types associated with various possible activities. For example, the one or more activity sensors can include a clock and the activity state can be based on a time of day. Alternatively and/or additionally, the one or more activity sensors can include an inertial sensor, and the activity state can be based on a user's determined activity level. In some implementations, the one or more activity sensors can include a proximity sensor, and the activity state can be based at least in part on a proximity to one or more other devices or objects. The one or more activity sensors may include a location sensor, and the activity state can be based at least in part on a location of the smart wearable device.</p><p id="p-0049" num="0048">In some implementations, the systems and methods disclosed herein can be implemented in a computing system. The computing system can include an article having one or more touch sensors integrated therein for recognizing a user gestural input. The computing system may further include an electronic device associated with the article. The electronic device can be communicatively coupled to the gesture sensor of the article. Moreover, the electronic device can have a processor and an audio output device capable of providing audio feedback to the user. In some implementations, the electronic device can include a visual display for displaying a visualization of the one or more touch sensors.</p><p id="p-0050" num="0049">The computing system can further include one or more contextual activity sensors associated with at least one of the article or the electronic device. The one or more contextual activity sensors can include at least one of an inertial motion sensor, a location sensor, a proximity sensor, or another type of sensor.</p><p id="p-0051" num="0050">In some implementations, the electronic device can be configured and programmed to perform operations stored in one or more non-transitory computer readable media. The operations can include processing readings from the one or more contextual activity sensors to determine which of a predetermined plurality of user activities the user is performing. The predetermined plurality of user activities can include a relaxing activity and a working out activity.</p><p id="p-0052" num="0051">During a first time interval in which the user is determined to be performing a first activity, the operations can include receiving a first touch gesture from the user and determining whether the first touch gesture is valid or invalid. The first touch gesture can include one or more directions and one or more magnitudes for each respective direction. A first audio feedback of a first sound category can then be provided to the user. The first audio feedback can include a first sound if the first touch gesture is valid and can include a second sound if the first touch gesture is invalid. In some implementations, the first activity can be at least one of the relaxing activity or working out activity. The visual display on the electronic device may display the visualization with a representation of the first touch gesture in response to receiving the first touch gesture from the user.</p><p id="p-0053" num="0052">During a second time interval in which the user is determined to be performing a second activity different than the first activity, the operations can include receiving a second touch gesture from the user and determining whether the second touch gesture is valid or invalid. The second touch gesture can include one or more directions and one or more magnitudes for each respective direction. A second audio feedback of a second sound category can then be provided to the user. The second sound category can be different than said first sound category. In some implementations, the second audio feedback can include a third sound if the second touch gesture is valid and can include a fourth sound if the second touch gesture is invalid.</p><p id="p-0054" num="0053">The audio feedback sounds can be provided to confirm whether the touch gestures are valid and provide confirmation of which activity the user has been determined to be performing.</p><p id="p-0055" num="0054">The article can be a variety of clothing types, accessories, or other objects. For example, the article can be a jacket, and the one or more touch sensors can be in the sleeve of the jacket. Alternatively, the article can be a carrying device, and the one or more touch sensors can be in a strap of the carrying device.</p><p id="p-0056" num="0055">Alternatively and/or additionally, the systems and methods can include generating activity data descriptive of a user activity with one or more activity sensors. The systems and methods can include generating input data descriptive of one or more user inputs with one or more touch sensors. In some implementations, the systems and methods can include determining the user activity based at least in part on the activity data. In some implementations, the user activity can be determined based at least in part on one of a user's location, a user's physical activity level, and/or a time of day. A plurality of actions associated with the user activity can be determined. In some implementations, the plurality of actions associated with the user activity can differ from a second plurality of actions associated with a second user activity. The input data and the plurality of actions can be processed to determine an audio feedback. If the input data is associated with at least one of the plurality of actions, the audio feedback can include a first sound, and if the input data is not associated with at least one of the plurality of actions, the audio feedback can include a second sound.</p><p id="p-0057" num="0056">The systems and methods can further include sending, by one or more wireless transmission components, instructions to play the audio feedback. Alternatively and/or additionally, the systems and methods can include generating, by the one or more activity sensors, second activity data descriptive of a second user activity and generating, by the one or more touch sensors, second input data descriptive of one or more second user inputs. The systems and methods can include determining the second user activity based at least in part on the second activity data. A second plurality of actions associated with the second user activity can be determined. Moreover, a second audio feedback can be determined based on the second input data and the second plurality of actions. If the second input data is associated with at least one of the second plurality of actions, the second audio feedback can include a third sound.</p><p id="p-0058" num="0057">In some implementations, the systems and methods can include processing the input data to determine one or more string sensors contacted during user input. The systems and methods can then include determining an audio response for each respective string sensor of the one or more string sensors and sending, by one or more wireless transmission components, instructions to play the one or more audio responses.</p><p id="p-0059" num="0058">The one or more user inputs can be one or more gesture inputs descriptive of one or more interactions with one or more string sensors.</p><p id="p-0060" num="0059">The systems and methods can further include determining an activity feedback associated with the user activity and sending, by one or more communication components, instructions to provide the activity feedback in response to determining the user activity.</p><p id="p-0061" num="0060">In some implementations, the systems and methods disclosed herein can utilize one or more machine-learned models for activity state determination. More specifically, one or more machine-learned models may be trained to learn when different action sets and their respective gesture sets should be used. For example, one or more machine-learned models can be used to determine when a specific user has gone from an inactive state to an active state. Another example can involve training a machine-learned model to determine what actions are commonly taken at certain times of day or at certain locations. The action sets can then be associated with certain gestures and can be associated with an activity state and an activity state audio feedback set.</p><p id="p-0062" num="0061">One example implementation of systems and methods disclosed herein can include a user wearing a smart jacket. A user can be wearing a jacket, which can contain a touch gesture sensor on its sleeve, and which can be coupled to a smartphone. The smartphone can be playing music to the user over headphones. During a first time interval, the user can be sitting quietly in a library, and they make a down-swipe gesture on the sleeve of their jacket that is intended to make the smartphone advance to the next song. If the gesture was done properly, a nice brief soft harmonious sound of a classical violin can be played. If the gesture was not done properly, the classical violin can play a soft, discordant, uncomfortable-sounding violin sound. During a second time interval, the user can be jogging or lifting weights, and the user can make a down-swipe gesture that can be intended to advance the music to the next song. If the gesture was done properly, a brief loud but harmonious sound of an aggressive rock-n-roll electric guitar can be played. If the gesture was not done properly, then a loud, discordant, uncomfortable aggressive sound of a rock-n-roll electric guitar can be played. Thus, the kind of sound which the user hears for gestural input feedback can be customized to the sensed activity of the user (e.g., soft and classy for when the user is in a library reading, and loud and aggressive if the user is jogging or lifting weights).</p><p id="p-0063" num="0062">The systems and methods disclosed herein can enable for a large corpus of actions to be selected despite limited inputs being available. The systems and methods can leverage determining an activity state to provide gesture sets associated with action sets dependent on a determined activity, such that a user can be provided access to actions they commonly select during certain activity states. For example, the systems and methods may determine that a user uses a certain set of actions often at home and a different set of actions at work; therefore, the systems and methods can associate action sets with their respective activity state (i.e., respective locations), such that gestures at each location result in different action selections.</p><p id="p-0064" num="0063">Additionally and/or alternatively, a user may manually define activity states, action sets for activity states, and/or gestures for action sets for specific activity states. The manually defined action sets can provide a tailored input experience that leverages activity state determination to add more accessibility to actions for selection despite limited input possibilities.</p><p id="p-0065" num="0064">The touch inputs can include a variety of inputs. Additionally, the sensing lines of the one or more touch sensors can be configured in a variety of layouts. For example, the touch inputs can be one or more compressions of the sensing lines individually or in combination. Alternatively, a touch input can include swiping across multiple sensing lines in one or more directions. In some implementations, some sensing lines may be configured to receive gestures in a parallel gesture to at least a portion of the sensing lines. This implementation can involve perpendicular sensing lines or may involve having a portion of a set of sensing lines being disposed for input in a staggered manner. In some implementations, the duration of the compressions or gestures can indicate differing intents and, therefore, can be interpreted as different inputs. Moreover, in some implementations, the pressure provided can indicate an intent of the touch input. In some implementations, a gesture may be restricted to a time period such that if a user does not complete the touch input, or gesture, in a defined amount of time, the gesture will be determined invalid.</p><p id="p-0066" num="0065">The systems and methods disclosed herein can be used for providing audio feedback for a variety of input types. For example, in some implementations, the systems and methods can include obtaining input data descriptive of motion from one or more motion sensors (e.g., one or more inertial sensors). The motion data can be processed to determine if the motion is indicative of an input (e.g., an arm extension, a leg motion, a pointing pose, etc.) matching at least one of the plurality of gestures associated with the plurality of actions.</p><p id="p-0067" num="0066">Alternatively and/or additionally, the input can be an input detected using one or more image sensors. For example, the input may be a wave, an eye motion, blinking, etc. Therefore, the input data can comprise image data that can be analyzed to determine if the input is associated with one or more gestures from the plurality of gestures associated with the plurality of actions for a specific activity state.</p><p id="p-0068" num="0067">In some implementations, the input can be a speech input detected using one or more audio sensors. For example, the systems and methods disclosed herein can include one or more models for processing the audio data to generate speech recognition outputs, which can then be analyzed to determine if the input is associated with one or more gestures from the plurality of gestures.</p><p id="p-0069" num="0068">In some implementations, the systems and methods disclosed herein can be provided as part of a game used for training or reinforcement of gesture inputs, such that a user may select a tutorial or game in order to implement the systems and methods disclosed herein.</p><p id="p-0070" num="0069">In some implementations, the audio feedbacks can be determined based on sound mapping in a database of sounds, such that each line, each gesture, and each activity state can be mapped to one or more sounds in a database of sounds. The sounds can be synthetically created sounds, recorded sounds, or sounds downloaded from another database. The database can include a plurality of harmonious sounds and a plurality of discordant sounds for providing harmonious sounds with a valid input and a discordant sound for invalid inputs.</p><p id="p-0071" num="0070">The systems and methods disclosed herein can be implemented in a variety of methods and systems, including, but not limited to: computer-implemented methods, computing systems, sensor systems, computing devices, interactive objects, and/or wearable devices.</p><p id="p-0072" num="0071">The systems and methods of the present disclosure provide a number of technical effects and benefits. As one example, the system and methods can provide a more intuitive interface by providing audio feedback with respect to an activity state and a gesture validity. The systems and methods can be used to train a user how to correctly complete gestures. The systems and methods can further be used to inform a user what specific sensing lines are contacted and in what order. Furthermore, the systems and methods can enable activity state and gesture validity audio feedback that can inform a user what action set is currently active and whether the gesture was correctly performed.</p><p id="p-0073" num="0072">Another technical benefit of the systems and methods of the present disclosure is changing the action set and the audio feedback based on the activity state of the user. The systems and methods disclosed herein can be used to enable the dynamic changing of action sets with respective gestures based on a determined activity state. The dynamic changing can allow for a wide-variety of inputs to be accessible to a user with limited touch sensors. For example, a user may use different inputs at work versus at home; therefore, the same gesture may be associated with different actions based on a user's location. A certain sound can be used to notify the user the work action set and associated gestures are currently active.</p><p id="p-0074" num="0073">In some implementations, the systems and methods disclosed herein can help with solving cognitive load and textual ambiguity issues.</p><p id="p-0075" num="0074">With reference now to the Figures, example embodiments of the present disclosure will be discussed in further detail.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an illustration of an example environment <b>100</b> in which an interactive object including a touch sensor can be implemented. Environment <b>100</b> includes a touch sensor <b>102</b> (e.g., capacitive or resistive touch sensor), or other sensor. Touch sensor <b>102</b> is shown as being integrated within various interactive objects <b>104</b>. Touch sensor <b>102</b> may include one or more sensing elements such as conductive threads or other sensing lines that are configured to detect a touch input. In some examples, the conductive threads or other sensing lines are arranged to extend parallel to each other along the length of the touch sensor in a longitudinal direction. The conductive threads or other sensing lines can be laterally spaced from each other.</p><p id="p-0077" num="0076">In some examples, a capacitive touch sensor can be formed from an interactive textile which is a textile that is configured to sense multi-touch-input. As described herein, a textile corresponds to any type of flexible woven material consisting of a network of natural or artificial fibers, often referred to as thread or yarn. Textiles may be formed by weaving, knitting, crocheting, knotting, pressing threads together or consolidating fibers or filaments together in a nonwoven manner. A capacitive touch sensor can be formed from any suitable conductive material and in other manners, such as by using flexible conductive lines including metal lines, filaments, etc. attached to a non-woven substrate.</p><p id="p-0078" num="0077">In environment <b>100</b>, interactive objects <b>104</b> include &#x201c;flexible&#x201d; objects, such as a shirt <b>104</b>-<b>1</b>, a hat <b>104</b>-<b>2</b>, a handbag <b>104</b>-<b>3</b> and a shoe <b>104</b>-<b>6</b>. It is to be noted, however, that touch sensor <b>102</b> may be integrated within any type of flexible object made from fabric or a similar flexible material, such as garments or articles of clothing, garment accessories, garment containers, blankets, shower curtains, towels, sheets, bedspreads, or fabric casings of furniture, to name just a few. Examples of garment accessories may include sweat-wicking elastic bands to be worn around the head, wrist, or bicep. Other examples of garment accessories may be found in various wrist, arm, shoulder, knee, leg, and hip braces or compression sleeves. Headwear is another example of a garment accessory, e.g., sun visors, caps, and thermal balaclavas. Examples of garment containers may include waist or hip pouches, backpacks, handbags, satchels, hanging garment bags, and totes. Garment containers may be worn or carried by a user, as in the case of a backpack, or may hold their own weight, as in rolling luggage. Touch sensor <b>102</b> may be integrated within flexible objects <b>104</b> in a variety of different ways, including weaving, sewing, gluing, and so forth. Flexible objects may also be referred to as &#x201c;soft&#x201d; objects.</p><p id="p-0079" num="0078">In this example, objects <b>104</b> further include &#x201c;hard&#x201d; objects, such as a plastic cup <b>104</b>-<b>4</b> and a hard smart phone casing <b>104</b>-<b>5</b>. It is to be noted, however, that hard objects <b>104</b> may include any type of &#x201c;hard&#x201d; or &#x201c;rigid&#x201d; object made from non-flexible or semi-flexible materials, such as plastic, metal, aluminum, and so on. For example, hard objects <b>104</b> may also include plastic chairs, water bottles, plastic balls, or car parts, to name just a few. In another example, hard objects <b>104</b> may also include garment accessories such as chest plates, helmets, goggles, shin guards, and elbow guards. Alternatively, the hard or semi-flexible garment accessory may be embodied by a shoe, cleat, boot, or sandal. Touch sensor <b>102</b> may be integrated within hard objects <b>104</b> using a variety of different manufacturing processes. In one or more implementations, injection molding is used to integrate touch sensors into hard objects <b>104</b>.</p><p id="p-0080" num="0079">Touch sensor <b>102</b> enables a user to control an object <b>104</b> with which the touch sensor <b>102</b> is integrated, or to control a variety of other computing devices <b>106</b> via a network <b>108</b>. Computing devices <b>106</b> are illustrated with various non-limiting example devices: server <b>106</b>-<b>1</b>, smartphone <b>106</b>-<b>2</b>, laptop <b>106</b>-<b>3</b>, computing spectacles <b>106</b>-<b>4</b>, television <b>106</b>-<b>5</b>, camera <b>106</b>-<b>6</b>, tablet <b>106</b>-<b>7</b>, desktop <b>106</b>-<b>8</b>, and smart watch <b>106</b>-<b>9</b>, though other devices may also be used, such as home automation and control systems, sound or entertainment systems, home appliances, security systems, netbooks, and e-readers. Note that computing device <b>106</b> can be wearable (e.g., computing spectacles and smart watches), non-wearable but mobile (e.g., laptops and tablets), or relatively immobile (e.g., desktops and servers). Computing device <b>106</b> may be a local computing device, such as a computing device that can be accessed over a Bluetooth connection, near-field communication connection, or other local-network connection. Computing device <b>106</b> may be a remote computing device, such as a computing device of a cloud computing system.</p><p id="p-0081" num="0080">Network <b>108</b> includes one or more of many types of wireless or partly wireless communication networks, such as a local-area-network (LAN), a wireless local-area-network (WLAN), a personal-area-network (PAN), a wide-area-network (WAN), an intranet, the Internet, a peer-to-peer network, point-to-point network, a mesh network, and so forth.</p><p id="p-0082" num="0081">A touch sensor <b>102</b> can interact with computing devices <b>106</b> by transmitting touch data or other sensor data through network <b>108</b>. Additionally or alternatively, the touch sensor <b>102</b> may transmit gesture data, movement data, or other data derived from sensor data generated by the touch sensor <b>102</b>. Computing device <b>106</b> can use the touch data to control computing device <b>106</b> or applications at computing device <b>106</b>. As an example, consider that the touch sensor <b>102</b> integrated at shirt <b>104</b>-<b>1</b> may be configured to control the user's smartphone <b>106</b>-<b>2</b> in the user's pocket, television <b>106</b>-<b>5</b> in the user's home, smart watch <b>106</b>-<b>9</b> on the user's wrist, or various other appliances in the user's house, such as thermostats, lights, music, and so forth. For example, the user may be able to swipe up or down on touch sensor <b>102</b> integrated within the user's shirt <b>104</b>-<b>1</b> to cause the volume on television <b>106</b>-<b>5</b> to go up or down, to cause the temperature controlled by a thermostat in the user's house to increase or decrease, or to turn on and off lights in the user's house. Note that any type of touch, tap, swipe, hold, or stroke gesture may be recognized by touch sensor <b>102</b>.</p><p id="p-0083" num="0082">In more detail, consider <figref idref="DRAWINGS">FIG. <b>2</b></figref> which illustrates an example system <b>190</b> that includes an interactive object <b>104</b>, a removable electronics module <b>150</b>, and a computing device <b>106</b>. In system <b>190</b>, touch sensor <b>102</b> is integrated in an object <b>104</b>, which may be implemented as a flexible object (e.g., shirt <b>104</b>-<b>1</b>, hat <b>104</b>-<b>2</b>, or handbag <b>104</b>-<b>3</b>) or a hard object (e.g., plastic cup <b>104</b>-<b>4</b> or smart phone casing <b>104</b>-<b>5</b>).</p><p id="p-0084" num="0083">Touch sensor <b>102</b> is configured to sense touch-input from a user when one or more fingers of the user's hand touch or approach touch sensor <b>102</b>. Touch sensor <b>102</b> may be configured as a capacitive touch sensor or resistive touch sensor to sense single-touch, multi-touch, and/or full-hand touch-input from a user. To enable the detection of touch-input, touch sensor <b>102</b> includes sensing lines <b>110</b>. Sensing elements may include various shapes and geometries. In some examples, sensing lines <b>110</b> can be formed as a grid, array, or parallel pattern of sensing lines so as to detect touch input. In some implementations, the sensing lines <b>110</b> do not alter the flexibility of touch sensor <b>102</b>, which enables touch sensor <b>102</b> to be easily integrated within interactive objects <b>104</b>.</p><p id="p-0085" num="0084">Interactive object <b>104</b> includes an internal electronics module <b>124</b> (also referred to as internal electronics device) that is embedded within interactive object <b>104</b> and is directly coupled to sensing lines <b>110</b>. Internal electronics module <b>124</b> can be communicatively coupled to a removable electronics module <b>150</b> (also referred to as a removable electronics device) via a communication interface <b>162</b>. Internal electronics module <b>124</b> contains a first subset of electronic circuits or components for the interactive object <b>104</b>, and removable electronics module <b>150</b> contains a second, different, subset of electronic circuits or components for the interactive object <b>104</b>. As described herein, the internal electronics module <b>124</b> may be physically and permanently embedded within interactive object <b>104</b>, whereas the removable electronics module <b>150</b> may be removably coupled to interactive object <b>104</b>.</p><p id="p-0086" num="0085">In system <b>190</b>, the electronic components contained within the internal electronics module <b>124</b> include sensing circuitry <b>126</b> that is coupled to sensing lines <b>110</b> that form the touch sensor <b>102</b>. In some examples, the internal electronics module includes a flexible printed circuit board (PCB). The printed circuit board can include a set of contact pads for attaching to the conductive lines. In some examples, the printed circuit board includes a microprocessor. For example, wires from conductive threads may be connected to sensing circuitry <b>126</b> using flexible PCB, creping, gluing with conductive glue, soldering, and so forth. In one embodiment, the sensing circuitry <b>126</b> can be configured to detect a user-inputted touch-input on the conductive threads that is pre-programmed to indicate a certain request. In one embodiment, when the conductive threads form a tunneled pattern as described herein, sensing circuitry <b>126</b> can be configured to also detect the location of the touch-input on sensing line <b>110</b>, as well as motion of the touch-input. For example, when an object, such as a user's finger, touches sensing lines <b>110</b>, the position of the touch can be determined by sensing circuitry <b>126</b> by detecting a change in capacitance on the sensing lines <b>110</b>. The touch-input may then be used to generate touch data usable to control a computing device <b>106</b>. For example, the touch-input can be used to determine various gestures, such as single-finger touches (e.g., touches, taps, and holds), multi-finger touches (e.g., two-finger touches, two-finger taps, two-finger holds, and pinches), single-finger and multi-finger swipes (e.g., swipe up, swipe down, swipe left, swipe right), and full-hand interactions (e.g., touching the textile with a user's entire hand, covering textile with the user's entire hand, pressing the textile with the user's entire hand, palm touches, and rolling, twisting, or rotating the user's hand while touching the textile).</p><p id="p-0087" num="0086">Internal electronics module <b>124</b> can include various types of electronics, such as sensing circuitry <b>126</b>, sensors (e.g., capacitive touch sensors woven into the garment, microphones, or accelerometers), output devices (e.g., LEDs, speakers, or micro-displays), electrical circuitry, and so forth. Removable electronics module <b>150</b> can include various electronics that are configured to connect and/or interface with the electronics of internal electronics module <b>124</b>. Generally, the electronics contained within removable electronics module <b>150</b> are different than those contained within internal electronics module <b>124</b>, and may include electronics such as microprocessor <b>152</b>, power source <b>154</b> (e.g., a battery), memory <b>155</b>, network interface <b>156</b> (e.g., Bluetooth, WiFi, USB), sensors (e.g., accelerometers, heart rate monitors, pedometers, IMUs), output devices (e.g., speakers, LEDs), and so forth.</p><p id="p-0088" num="0087">In some examples, removable electronics module <b>150</b> is implemented as a strap or tag that contains the various electronics. The strap or tag, for example, can be formed from a material such as rubber, nylon, plastic, metal, or any other type of fabric. Notably, however, removable electronics module <b>150</b> may take any type of form. For example, rather than being a strap, removable electronics module <b>150</b> could resemble a circular or square piece of material (e.g., rubber or nylon).</p><p id="p-0089" num="0088">The inertial measurement unit(s) (IMU(s)) <b>158</b> can generate sensor data indicative of a position, velocity, and/or an acceleration of the interactive object. The IMU(s) <b>158</b> may generate one or more outputs describing one or more three-dimensional motions of the interactive object <b>104</b>. The IMU(s) may be secured to the internal electronics module <b>124</b>, for example, with zero degrees of freedom, either removably or irremovably, such that the inertial measurement unit translates and is reoriented as the interactive object <b>104</b> is translated and is reoriented. In some embodiments, the inertial measurement unit(s) <b>158</b> may include a gyroscope or an accelerometer (e.g., a combination of a gyroscope and an accelerometer), such as a three-axis gyroscope or accelerometer configured to sense rotation and acceleration along and about three, generally orthogonal axes. In some embodiments, the inertial measurement unit(s) may include a sensor configured to detect changes in velocity or changes in rotational velocity of the interactive object and an integrator configured to integrate signals from the sensor such that a net movement may be calculated, for instance by a processor of the inertial measurement unit, based on an integrated movement about or along each of a plurality of axes.</p><p id="p-0090" num="0089">Communication interface <b>162</b> enables the transfer of power and data (e.g., the touch-input detected by sensing circuitry <b>126</b>) between the internal electronics module <b>124</b> and the removable electronics module <b>260</b>. In some implementations, communication interface <b>162</b> may be implemented as a connector that includes a connector plug and a connector receptacle. The connector plug may be implemented at the removable electronics module <b>150</b> and is configured to connect to the connector receptacle, which may be implemented at the interactive object <b>104</b>. One or more communication interface(s) may be included in some examples. For instance, a first communication interface may physically couple the removable electronics module <b>150</b> to one or more computing devices <b>106</b>, and a second communication interface may physically couple the removable electronics module <b>150</b> to interactive object <b>104</b>.</p><p id="p-0091" num="0090">In system <b>190</b>, the removable electronics module <b>150</b> includes a microprocessor <b>152</b>, a power source <b>154</b>, memory <b>155</b>, one or more output devices <b>159</b>, an IMU <b>158</b>, a gesture manager <b>161</b>, and network interface <b>156</b>. Power source <b>154</b> may be coupled, via communication interface <b>162</b>, to sensing circuitry <b>126</b> to provide power to sensing circuitry <b>126</b> to enable the detection of touch-input and may be implemented as a small battery. When touch-input is detected by sensing circuitry <b>126</b> of the internal electronics module <b>124</b>, data representative of the touch-input may be communicated, via communication interface <b>162</b>, to microprocessor <b>152</b> of the removable electronics module <b>150</b>. Microprocessor <b>152</b> may then analyze the touch-input data to generate one or more control signals, which may then be communicated to a computing device <b>106</b> (e.g., a smart phone, server, cloud computing infrastructure, etc.) via the network interface <b>156</b> to cause the computing device to initiate a particular functionality. Generally, network interfaces <b>156</b> are configured to communicate data, such as touch data, over wired, wireless, or optical networks to computing devices. By way of example and not limitation, network interfaces <b>156</b> may communicate data over a local-area-network (LAN), a wireless local-area-network (WLAN), a personal-area-network (PAN) (e.g., Bluetooth&#x2122;), a wide-area-network (WAN), an intranet, the Internet, a peer-to-peer network, point-to-point network, a mesh network, and the like (e.g., through network <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0092" num="0091">Object <b>104</b> may also include one or more output devices <b>127</b> configured to provide a haptic response, a tactical response, an audio response, a visual response, or some combination thereof. Similarly, removable electronics module <b>150</b> may include one or more output devices <b>159</b> configured to provide a haptic response, tactical response, and audio response, a visual response, or some combination thereof. Output devices <b>127</b> may include visual output devices, such as one or more light-emitting diodes (LEDs), audio output devices such as one or more speakers, one or more tactile output devices, and/or one or more haptic output devices. In some examples, the one or more output devices <b>159</b> are formed as part of removable electronics module <b>150</b>, although this is not required.</p><p id="p-0093" num="0092">In one example, an output device <b>127</b> can include one or more LEDs configured to provide different types of output signals. For example, the one or more LEDs can be configured to generate a circular pattern of light, such as by controlling the order and/or timing of individual LED activations. Other lights and techniques may be used to generate visual patterns including circular patterns. In some examples, one or more LEDs may produce different colored light to provide different types of visual indications. Output devices <b>127</b> may include a haptic or tactile output device that provides different types of output signals in the form of different vibrations and/or vibration patterns. In yet another example, output devices <b>127</b> may include a haptic output device such as may tighten or loosen an interactive garment with respect to a user. For example, a clamp, clasp, cuff, pleat, pleat actuator, band (e.g., contraction band), or other device may be used to adjust the fit of a garment on a user (e.g., tighten and/or loosen). In some examples, an interactive textile may be configured to tighten a garment such as by actuating conductive threads within the touch sensor <b>102</b>.</p><p id="p-0094" num="0093">A gesture manager <b>161</b> can be capable of interacting with applications at computing devices <b>106</b> and touch sensor <b>102</b> effective to aid, in some cases, control of applications through touch-input received by touch sensor <b>102</b>. For example, gesture manager <b>161</b> can interact with applications. In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, gesture manager <b>161</b> is illustrated as implemented at removable electronics module <b>150</b>. It will be appreciated, however, that gesture manager <b>161</b> may be implemented at internal electronics module <b>124</b>, a computing device <b>106</b> remote from the interactive object, or some combination thereof. A gesture manager <b>161</b> may be implemented as a standalone application in some embodiments. In other embodiments, a gesture manager <b>161</b> may be incorporated with one or more applications at a computing device.</p><p id="p-0095" num="0094">A gesture or other predetermined motion can be determined based on touch data detected by the touch sensor <b>102</b> and/or an inertial measurement unit <b>158</b> or other sensor. For example, gesture manager <b>161</b> can determine a gesture based on touch data, such as single-finger touch gesture, a double-tap gesture, a two-finger touch gesture, a swipe gesture, and so forth. As another example, gesture manager <b>161</b> can determine a gesture based on movement data such as a velocity, acceleration, etc. as can be determined by inertial measurement unit <b>158</b>.</p><p id="p-0096" num="0095">A functionality associated with a gesture can be determined by gesture manager <b>161</b> and/or an application at a computing device <b>106</b>. In some examples, it is determined whether the touch data corresponds to a request to perform a particular functionality. For example, the gesture manager <b>161</b> can determine whether touch data corresponds to a user input or gesture that is mapped to a particular functionality, such as initiating a vehicle service, triggering a text message or other notification, answering a phone call, creating a journal entry, and so forth. As described throughout, any type of user input or gesture may be used to trigger the functionality, such as swiping, tapping, or holding touch sensor <b>102</b>. In one or more implementations, a gesture manager <b>161</b> can enable application developers or users to configure the types of user input or gestures that can be used to trigger various different types of functionalities. For example, a gesture manager <b>161</b> can cause a particular functionality to be performed, such as by sending a text message or other communication, answering a phone call, creating a journal entry, increase the volume on a television, turn on lights in the user's house, open the automatic garage door of the user's house, and so forth.</p><p id="p-0097" num="0096">While internal electronics module <b>124</b> and removable electronics module <b>150</b> are illustrated and described as including specific electronic components, it is to be appreciated that these modules may be configured in a variety of different ways. For example, in some cases, electronic components described as being contained within internal electronics module <b>124</b> may be at least partially implemented at the removable electronics module <b>150</b>, and vice versa. Furthermore, internal electronics module <b>124</b> and removable electronics module <b>150</b> may include electronic components other than those illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, such as sensors, light sources (e.g., LED's), displays, speakers, and so forth.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of a sensor system <b>200</b>, such as can be integrated with an interactive object <b>204</b> in accordance with one or more implementations. In this example, the sensing elements (e.g., sensing lines <b>110</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) are implemented as conductive threads <b>210</b> on or within a substrate <b>215</b>. Touch sensor <b>202</b> includes non-conductive threads <b>212</b> woven with conductive threads <b>210</b> to form a capacitive touch sensor <b>202</b> (e.g., interactive textile). It is noted that a similar arrangement may be used to form a resistive touch sensor. Non-conductive threads <b>212</b> may correspond to any type of non-conductive thread, fiber, or fabric, such as cotton, wool, silk, nylon, polyester, and so forth.</p><p id="p-0099" num="0098">At <b>220</b>, a zoomed-in view of conductive thread <b>210</b> is illustrated. Conductive thread <b>210</b> includes a conductive wire <b>230</b> or a plurality of conductive filaments that are twisted, braided, or wrapped with a flexible thread <b>232</b>. As shown, the conductive thread <b>210</b> can be woven with or otherwise integrated with the non-conductive threads <b>212</b> to form a fabric or a textile. Although a conductive thread and textile is illustrated, it will be appreciated that other types of sensing elements and substrates may be used, such as flexible metal lines formed on a plastic substrate.</p><p id="p-0100" num="0099">In one or more implementations, conductive wire <b>230</b> is a thin copper wire. It is to be noted, however, that the conductive wire <b>230</b> may also be implemented using other materials, such as silver, gold, or other materials coated with a conductive polymer. The conductive wire <b>230</b> may include an outer cover layer formed by braiding together non-conductive threads. The flexible thread <b>232</b> may be implemented as any type of flexible thread or fiber, such as cotton, wool, silk, nylon, polyester, and so forth.</p><p id="p-0101" num="0100">Capacitive touch sensor <b>202</b> can be formed cost-effectively and efficiently, using any conventional weaving process (e.g., jacquard weaving or 3D-weaving), which involves interlacing a set of longer threads (called the warp) with a set of crossing threads (called the weft). Weaving may be implemented on a frame or machine known as a loom, of which there are a number of types. Thus, a loom can weave non-conductive threads <b>212</b> with conductive threads <b>210</b> to create a capacitive touch sensor <b>202</b>. In another example, capacitive touch sensor <b>202</b> can be formed using a predefined arrangement of sensing lines formed from a conductive fabric such as an electro-magnetic fabric including one or more metal layers.</p><p id="p-0102" num="0101">The conductive threads <b>210</b> can be formed into the touch sensor <b>202</b> in any suitable pattern or array. In one embodiment, for instance, the conductive threads <b>210</b> may form a single series of parallel threads. For instance, in one embodiment, the capacitive touch sensor may comprise a single plurality of parallel conductive threads conveniently located on the interactive object, such as on the sleeve of a jacket.</p><p id="p-0103" num="0102">In example system <b>200</b>, sensing circuitry <b>126</b> is shown as being integrated within object <b>104</b>, and is directly connected to conductive threads <b>210</b>. During operation, sensing circuitry <b>126</b> can determine positions of touch-input on the conductive threads <b>210</b> using self-capacitance sensing or mutual capacitive sensing.</p><p id="p-0104" num="0103">For example, when configured as a self-capacitance sensor, sensing circuitry <b>126</b> charges can charge a selected conductive thread <b>210</b> by applying a control signal (e.g., a sine signal) to the selected conductive thread <b>210</b>. The control signal may be referred to as a scanning voltage in some examples and the processing of determining the capacitance of a selected conductive thread may be referred to as scanning. In some examples, the control signal can be applied to a selected conductive thread while grounding or applying a low-level voltage to the other conductive threads. When an object, such as the user's finger, touches the sensor of conductive thread <b>210</b>, the capacitive coupling between the conductive thread <b>210</b> that is being scanned and system ground may be increased, which changes the capacitance sensed by the touched conductive thread <b>210</b>. This process can be repeated by applying the scanning voltage to each selected conductive thread while grounding the remaining non-selected conductive threads. In some examples, the conductive threads can be scanned individually, proceeding through the set of conductive threads in sequence. In other examples, more than one conductive thread may be scanned simultaneously.</p><p id="p-0105" num="0104">Sensing circuitry <b>126</b> uses the change in capacitance to identify the presence of the object (e.g., user's finger, stylus, etc.). When an object, such as the user's finger, touches the sensor of conductive threads, the capacitance changes on the conductive threads (e.g., increases or decreases). Sensing circuitry <b>126</b> uses the change in capacitance on conductive threads to identify the presence of the object. To do so, sensing circuitry <b>126</b> detects a position of the touch-input by scanning the conductive threads to detect changes in capacitance. Sensing circuitry <b>126</b> determines the position of the touch-input based on conductive threads having a changed capacitance. Other sensing techniques such as mutual capacitive sensing may be used in example embodiments.</p><p id="p-0106" num="0105">The conductive thread <b>210</b> and sensing circuitry <b>126</b> is configured to communicate the touch data that is representative of the detected touch-input to gesture manager <b>161</b> (e.g., at removable electronics module <b>150</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). The microprocessor <b>152</b> may then cause communication of the touch data, via network interface <b>156</b>, to computing device <b>106</b> to enable the device to determine gestures based on the touch data, which can be used to control object <b>104</b>, computing device <b>106</b>, or applications implemented at computing device <b>106</b>. In some implementations, a predefined motion may be determined by the internal electronics module and/or the removable electronics module and data indicative of the predefined motion can be communicated to a computing device <b>106</b> to control object <b>104</b>, computing device <b>106</b>, or applications implemented at computing device <b>106</b>.</p><p id="p-0107" num="0106">In accordance with some embodiments, a plurality of sensing lines can be formed from a multilayered flexible film to facilitate a flexible sensing line. For example, the multilayered film may include one or more flexible base layers such as a flexible textile, plastic, or other flexible material. One or more metal layers may extend over the flexible base layer(s). Optionally, one or more passivation layers can extend over the one or more flexible base layers and the one or more metal layer(s) to promote adhesion between the metal layer(s) and the base layer(s). In accordance with some examples, a multilayered sheet including one or more flexible base layers, one or more metal layers, and optionally one or more passivation layers can be formed and then cut, etched, or otherwise divided into individual sensing lines. Each sensing line can include a line of the one or more metal layers formed over a line of the one or more flexible base layers. Optionally, a sensing line can include a line of one or more passivation layers overlying the one or more flexible base layers. An electromagnetic field shielding fabric can be used to form the sensing lines in some examples.</p><p id="p-0108" num="0107">The plurality of conductive threads <b>210</b> forming touch sensor <b>202</b> are integrated with non-conductive threads <b>212</b> to form flexible substrate <b>215</b> having a first surface <b>217</b>. opposite a second side (or surface) in a direction orthogonal to the first side and the second side. Any number of conductive threads may be used to form a touch sensor. Moreover, any number of conductive threads may be used to form the plurality of first conductive threads and the plurality of second conductive threads. Additionally, the flexible substrate may be formed from one or more layers. For instance, the conductive threads may be woven with multiple layers of non-conductive threads. In this example, the conductive threads are formed on the first surface only. In other examples, a first set of conductive threads can be formed on the first surface and a second set of conductive threads at least partially formed on the second surface.</p><p id="p-0109" num="0108">One or more control circuits of the sensor system <b>200</b> can obtain touch data associated with a touch input to touch sensor <b>202</b>. The one or more control circuits can include sensing circuitry <b>126</b> and/or a computing device such as a microprocessor <b>128</b> at the internal electronics module, microprocessor <b>152</b> at the removable electronics module <b>150</b>, and/or a remote computing device <b>106</b>. The one or more control circuits can implement gesture manager <b>161</b> in example embodiments. The touch data can include data associated with a respective response by each of the plurality of conductive threads <b>210</b>. The touch data can include, for example, a capacitance associated with conductive threads <b>210</b>-<b>1</b>, <b>210</b>-<b>2</b>, <b>210</b>-<b>3</b>, and <b>210</b>-<b>4</b>. In some examples, the control circuit(s) can determine whether the touch input is associated with a first subset of conductive threads exposed on the first surface or a second subset of conductive threads exposed on the second surface. The control circuit(s) can classify the touch input as associated with a particular subset based at least in part on the respective response to the touch input by the plurality of conductive sensing elements.</p><p id="p-0110" num="0109">The control circuits(s) can be configured to detect a surface of the touch sensor at which a touch input is received, detect one or more gestures or other user movements in response to touch data associated with a touch input, and/or initiate one or more actions in response to detecting the gesture or other user movement. By way of example, control circuit(s) can obtain touch data that is generated in response to a touch input to touch sensor <b>202</b>. The touch data can be based at least in part on a response (e.g., resistance or capacitance) associated with sensing elements from each subset of sensing elements. The control circuit(s) can determine whether the touch input is associated with a first surface of the touch sensor or a second surface of the touch sensor based at least in part on the response associated with the first sensing element and the response associated with the second sensing element. The control circuit(s) can selectively determine whether a touch input corresponds to a particular input gesture based at least in part on whether the touch input is determined to have been received at first surface of the touch sensor or a second surface of the touch sensor. Notably, the control circuit(s) can analyze the touch data from each subset of sensing elements to determine whether a particular gesture has been performed. In this regard, the control circuits can utilize the individual subsets of elements to identify the particular surface of the touch sensor. However, the control circuits can utilize the full set of sensing elements to identify whether a gesture has been performed.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a perspective view of an example of an interactive object including a capacitive touch sensor <b>302</b> in accordance with example embodiments of the present disclosure. In this example, the interactive object is an interactive garment <b>304</b> having a capacitive touch sensor <b>302</b> integrated in the cuff of a sleeve. By way of example, the user can perform a gesture by brushing in on the cuff of the interactive object <b>104</b> cuff where the capacitive touch sensor <b>302</b> is placed. Gesture manager <b>161</b> can be configured to initiate and/or implement one or more functionalities in response to the brush in gesture. For example, a user may perform a brush in gesture in order to receive a notification related to an application at the remote computing device (e.g., having a text message converted into an audible output at the remote computing device). Note that any type of touch, tap, swipe, hold, or stroke gesture may be recognized by capacitive touch sensor <b>302</b>. In an alternate example, a resistive touch sensor may be used rather than a capacitive touch sensor.</p><p id="p-0112" num="0111">The conductive sensing elements <b>310</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are positioned at a touch sensor area <b>305</b>. The sensing elements <b>310</b> can be partitioned into subsets of sensing elements to enable a sensor system to distinguish between touch inputs at an intended touch input surface <b>320</b> and touch inputs at an unintended second surface <b>322</b>. The second surface can be positioned toward a user's body when the interactive garment is worn. In traditional designs, it may be difficult to distinguish a touch between the intended touch input surface <b>320</b> and the unintended second surface <b>322</b>. Typically, thick padding is provided between the user's body and the touch sensor to attenuate the capacitive signal generated in response to the user's body at the unintended second surface <b>322</b>.</p><p id="p-0113" num="0112">In accordance with example embodiments, the first subset of conductive sensing elements can be coupled to a first side of a flexible substrate and the second subset of conductive sensing elements can be coupled to a second side of the flexible substrate. The first side can be positioned closer to the intended touch input surface <b>320</b> and the second side can be positioned further from the intended touch input surface <b>320</b> and closer to the second surface <b>322</b> which will be adjacent to the user when worn. For example, the first side can be adjacent to the intended touch input surface and the second side can be adjacent to one or more portions of the user's body when the interactive garment is worn by the user. In some embodiments, the first side of the flexible substrate and the second side of the flexible substrate can be separated in the direction orthogonal to the first side and the second side of the flexible substrate. By positioning subsets of sensing elements on opposite sides of the substrate, the capacitive signal generated by the user's body at the second surface can be distinguished from the capacitive signal generated by the touch input at the intended touch input surface.</p><p id="p-0114" num="0113">In accordance with example embodiments, the first and second subsets of sensing elements can be separated in a vertical direction to provide signal differentiation to distinguish inputs provided at the intended input surface from inputs provided at the unintended second surface. For example, the sensing elements can be arranged as parallel sensing lines as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The sensing lines can be elongated in a first direction with a separation therebetween in a second direction orthogonal to the first direction. A first subset of the parallel sensing lines and a second subset of the parallel sensing lines can be separated in a direction orthogonal to the first direction and the second direction. Notably, this separation can be provided with or without the use of a flexible substrate to which the lines are coupled. A touch input provided at the intended touch input surface (e.g., + vertical axis <b>205</b>) can generate a stronger signal (e.g., capacitance) on the first subset of lines than the second subset of lines. A touch input provided at the unintended second surface (e.g., &#x2212;vertical axis) can generate a stronger signal (e.g., capacitance) on the second subset of lines than the first subset of lines.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example sensing line system <b>400</b> in which each sensing line is associated with a respective line-specific audio feedback. The sensing lines <b>420</b> can be a plurality of sensing lines <b>420</b> attached to a touch sensor or may be separately attached to a plurality of touch sensors. The sensing line system <b>400</b> can further include gesture validity determination which can cause a gesture validity audio feedback <b>410</b> to be determined and provided.</p><p id="p-0116" num="0115">The sensing line system <b>400</b> depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref> can be utilized for training a user to correctly complete gestures. For example, the sensing lines <b>420</b> can be implemented into a smart wearable with one or more touch sensors, and one or more processors in a jacket, backpack, etc. The first sensing line <b>422</b> can be associated with a first sound <b>402</b>, the second sensing line <b>424</b> can be associated with a second sound <b>404</b>, the third sensing line <b>426</b> can be associated with a third sound <b>406</b>, and the fourth sensing line <b>428</b> can be associated with a fourth sound <b>408</b>. Therefore, when a user contacts a second sensing line <b>424</b>, a second sound <b>404</b> can be provided in real-time. The user can then hear real-time feedback of which of the sensing lines <b>420</b> are being contacted.</p><p id="p-0117" num="0116">At the completion of the gesture, a gesture validity can be determined, which can cause a gesture validity audio feedback <b>410</b> to be determined and provided. The line-specific audio feedback paired with the gesture validity audio feedback <b>410</b> can allow for user gesture training with an intuitive interface for inputs.</p><p id="p-0118" num="0117">In some implementations the line-specific audio feedbacks can include sounds with a common musical key but different respective notes. For example, the first sound <b>402</b>, the second sound <b>404</b>, the third sound <b>406</b>, and the fourth sound <b>408</b> can all share a common musical key, but the first sound <b>402</b> can be differentiated from the other sounds based on the musical note. Moreover, in some implementations, the gesture validity audio feedback <b>410</b> can be an on-key sound if the gesture is valid or an off-key sound if the gesture is invalid. Additionally and/or alternatively, the gesture validity audio feedback <b>410</b> can include the note of at least one of the selected sensing lines <b>420</b> in response to the gesture being valid. In some implementations, the gesture validity audio feedback <b>410</b> can include a playback of each of the respective line-specific audio feedbacks of the associated sensing lines in the gesture if the gesture is determined to be valid.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>6</b></figref> and <figref idref="DRAWINGS">FIG. <b>7</b></figref> are top and bottom views, respectively, depicting an example of a touch sensor including individual subsets of sensing lines coupled to opposite sides of a flexible substrate in accordance with example embodiments of the present disclosure. A sensor assembly <b>500</b> includes a touch sensor <b>502</b> and an internal electronics module <b>524</b>. Internal electronics module <b>524</b> is one example of an internal electronics module <b>124</b> as depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Touch sensor <b>502</b> is one example of a touch sensor <b>102</b> as illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref>, and can be configured as a capacitive touch sensor or resistive touch sensor in example embodiments.</p><p id="p-0120" num="0119">Touch sensor <b>502</b> is formed from a plurality of conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b>. Conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b> are one example of sensing elements <b>110</b>. Conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b> are an example of parallel sensing lines coupled to a substrate and extending in a longitudinal direction to receive touch input. Internal electronics module <b>524</b> may include sensing circuitry (not shown) in electrical communication with the plurality of conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b>. Internal electronics module <b>524</b> may include one or more communication ports that can couple to a communications cable to provide communication with a removable electronics module. A communication port can additionally or alternatively be coupled to a communication cable to provide communication with various input and/or output devices. Output devices may include an audible output device such as a speaker, a visual output device such as a light (e.g., LED), or a haptic output device such as a haptic motor. Any suitable type of output device may be provided as an output device.</p><p id="p-0121" num="0120">The set of conductive threads <b>510</b> can be woven or otherwise integrated with a plurality of non-conductive threads to form an interactive textile substrate <b>515</b>. More particularly, the conductive threads <b>510</b> can be formed on opposite sides of substrate <b>515</b>. A first subset of conductive threads <b>510</b>-<b>0</b>, <b>510</b>-<b>2</b>, <b>510</b>-<b>4</b>, <b>510</b>-<b>6</b>, and <b>510</b>-<b>8</b> can be woven with or otherwise coupled to the first side <b>517</b> of the interactive textile and the second subset of conductive threads <b>510</b>-<b>1</b>, <b>510</b>-<b>3</b>, <b>510</b>-<b>5</b>, <b>510</b>-<b>7</b>, and <b>510</b>-<b>9</b> can be woven with or otherwise coupled to the second side <b>519</b> of the interactive textile. The first subset of conductive threads can be formed on the first side <b>517</b> adjacent to a first surface of the touch sensor and the second subset of conductive threads can be formed on the opposite side <b>519</b> adjacent to a second surface of the touch sensor. A touch input to the touch sensor can be detected by the plurality of conductive threads using sensing circuitry of internal electronics module <b>524</b> connected to the one or more conductive threads. The sensing circuitry can generate touch data (e.g., raw sensor data or data derived from the raw sensor data) based on the touch input. The sensing circuitry and/or other control circuitry (e.g., a local or remote processor) can analyze the touch data to determine a surface of the touch sensor associated with the touch input. In some examples, the control circuitry can ignore touch inputs associated with the second surface, while analyzing touch inputs associated with the first surface to detect one or more predetermined gestures. As another example, the control circuitry can analyze touch inputs associated with the first surface for a first set of predetermined gestures and can analyze touch inputs associated with the second surface for a second set of predetermined gestures.</p><p id="p-0122" num="0121">With reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the first side <b>517</b> of flexible substrate <b>515</b> can correspond to an intended touch input surface for the touch sensor when integrated with an interactive object. For example, sensor assembly <b>500</b> can be integrated with an object to form an interactive object having a touch input surface adjacent to the first side <b>517</b> of substrate <b>515</b>. The second side <b>519</b> depicted in <figref idref="DRAWINGS">FIG. <b>7</b></figref> can be adjacent to a user's body when an interactive garment incorporating the sensor assembly is worn by the user.</p><p id="p-0123" num="0122">Conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b> can be formed on or within the textile-based substrate <b>515</b>. By way of example, textile-based substrate <b>515</b> may be formed by weaving, embroidering, stitching, or otherwise integrating conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b> with a set of nonconductive threads.</p><p id="p-0124" num="0123">The conductive threads are coupled to a connecting ribbon <b>514</b> in some examples, which can be utilized to position the conductive lines for connection to a plurality of electrical contact pads (not shown) of internal electronics module <b>524</b>. The plurality of conductive threads <b>510</b>-<b>0</b> to <b>510</b>-<b>9</b> can be collected and organized using a ribbon with a pitch that matches a corresponding pitch of connection points of an electronic component such as a component of internal electronics module <b>524</b>. It is noted, however, that a connecting ribbon is not required.</p><p id="p-0125" num="0124">The first subset of conductive threads <b>510</b>-<b>0</b>, <b>510</b>-<b>2</b>, <b>510</b>-<b>4</b>, <b>510</b>-<b>6</b>, and <b>510</b>-<b>8</b> extend from the connecting ribbon <b>514</b> in the direction of longitudinal axis <b>201</b> with a spacing therebetween in the lateral direction. More particularly, the first subset of conductive threads is coupled to the first side <b>517</b> of flexible substrate <b>515</b>. The first subset of conductive threads can be coupled to the first side of the flexible substrate using any suitable technique. For example, flexible substrate <b>515</b> can be formed by weaving nonconductive threads with conductive threads <b>510</b>. In another example, the first subset of conductive threads <b>510</b> can be embroidered to the first side of flexible substrate <b>515</b>. Other techniques such as gluing, heat pressing, or other fastening techniques may be used.</p><p id="p-0126" num="0125">The second subset of conductive threads <b>510</b>-<b>1</b>, <b>510</b>-<b>3</b>, <b>510</b>-<b>5</b>, <b>510</b>-<b>7</b>, and <b>510</b>-<b>9</b> extend from the connecting ribbon <b>514</b> on the first side <b>517</b> of the flexible substrate for a limited distance. The second subset of conductive threads extend through the flexible substrate such that they are at least partially exposed on the second side <b>519</b> of the flexible substrate. In some examples, the portion of the second subset of conductive threads at the first side can be loose or otherwise not coupled to the flexible substrate <b>515</b>. In other examples the second subset of conductive threads can be attached to the first side of flexible substrate <b>515</b> before passing through the flexible substrate to the second side <b>519</b>. At the second side <b>519</b>, conductive threads <b>510</b>-<b>1</b>, <b>510</b>-<b>3</b>, <b>510</b>-<b>5</b>, <b>510</b>-<b>7</b>, and <b>510</b>-<b>9</b> extend in the longitudinal direction. The second subset of conductive threads can be coupled to the second side of the flexible substrate using any suitable technique such as those earlier described with the first subset of conductive threads.</p><p id="p-0127" num="0126">A similar pre-fabricated sensor assembly may additionally or alternatively include other types of sensors. For example, a capacitive touch sensor utilizing a thin film of conductive materials may be utilized in place of conductive thread. In some examples, resistive touch sensors can be formed in a similar manner to capacitive touch sensors as described.</p><p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. <b>8</b></figref> depicts an example action set determination system <b>800</b>. The user device <b>802</b>, or user computing system, can include one or more sensors for receiving touch inputs and may be part of a user computing system that includes one or more activity sensors for obtaining or generating activity data. The activity data can be descriptive and/or indicative of a particular activity state. The user device <b>802</b>, or user computing system, can obtain the activity data, process the activity data, determine an activity state based on the activity data, and determine an action set associated with the activity state. For example, the one or more activity sensors can obtain movement data from one or more inertial sensors. The user computing system can process the movement data to determine an activity level. If the activity level is a low activity level, a first action set <b>814</b> may be determined, and if the activity level is a high activity level, a second action set <b>816</b> may be determined. In some implementations, a high activity level may be determined by movement data, motion data, or accelerometer data being descriptive of activity above a threshold. In some implementations, a low activity level may be determined by movement data, motion data, or accelerometer data being descriptive of activity below a threshold. The threshold may be an active versus non-active threshold. In some implementations, the threshold may be predefined, manually defined, or may be a machine-learned threshold (e.g., a user's average heart rate or average activity level may be determined and used to determine a threshold indicative of a high activity level). The threshold may be user specific or may be based on a globally defined threshold.</p><p id="p-0129" num="0128">Another example can involve a time of day or time of week. For example, a user may complete a set of actions repeatedly at a certain time of day, or a user may manually generate an action set for a certain time of day or a certain time of week, month, year, etc. The user computing system can generate or obtain activity data descriptive of a time. In some implementations, the time can be determined to be the time of the day associated with a certain set of actions. For example, during a user's commute <b>808</b>, the user may select a certain set of actions. Therefore, if the user computing system determines the time is the commuting time <b>808</b>, the third action set <b>818</b> may be determined.</p><p id="p-0130" num="0129">Another example can involve obtaining activity data descriptive of a user device <b>802</b> location. The activity data can include GPS data that can be processed to determine the user is at work <b>810</b>. In response to the user device being at the work location <b>810</b>, the fourth action set <b>820</b> can be determined. In some implementations, the activity data can include both movement data and location data to provide more tailored action sets. For example, an activity level can be determined along with a location being determined, such that a non-active home activity state <b>804</b> can be determined or an active home activity state <b>806</b> can be determined. The non-active home activity state <b>804</b> can be associated with the first action set <b>814</b>, and the active home activity state <b>806</b> can be associated with the second action set <b>816</b>.</p><p id="p-0131" num="0130">In some implementations, the corpus of possible activity states can include a plurality of other activity states <b>812</b> associated with a plurality of other action sets, such as fifth action set <b>822</b>. The activity data can include motion data, location data, time data, weather data, device-specific data, etc. The one or more activity sensors can include inertial sensors, location sensors, internal clocks, thermometer sensors, pressure sensors, transmitters, etc. For example, in some implementations, metadata can be sent to the user device indicative of a device being communicatively connected to a certain type of device. Based on the determined type of device, a certain action set can be determined. For example, a music playing device can be associated with a music player action set including volume control, song/channel control, and/or play/pause control. Moreover, for devices with a plurality of functions, an action set may be determined based on an open application or a most recently used function.</p><p id="p-0132" num="0131">Each of the action sets can be associated with the same or partially overlapping gestures, such that a single gesture can cause different actions dependent on the determined activity state.</p><p id="p-0133" num="0132"><figref idref="DRAWINGS">FIG. <b>9</b></figref> depicts an example activity state determination system <b>900</b> according to the systems and methods disclosed herein. The system <b>900</b> can include a user computing system <b>802</b> that includes a plurality of sensors. The plurality of sensors can include one or more touch sensors and one or more activity sensors. The activity sensors can include time sensors (e.g., an internal clock) <b>904</b>, location sensors (e.g., a GPS locator) <b>906</b>, motion sensors (e.g., inertial sensors) <b>908</b>, wireless transmitters (e.g., a Bluetooth transmitter) <b>910</b>, and/or other sensors (e.g., image sensors, audio sensors, etc.) <b>912</b>. The one or more activity sensors can be used to determine an activity state <b>914</b>.</p><p id="p-0134" num="0133">For example, the time sensors <b>904</b> can generate time data that can be processed to determine a time (e.g., the time of day in which the user usually listens to music), which may cause a certain action set (e.g., a plurality of music player actions) to be selected based on the time activity state. Another example can involve location data. The location data may be processed to determine the user is at work. Based on the user being at work a certain action set can be determined, such that the work action set is different from the home action set.</p><p id="p-0135" num="0134">Moreover, in some implementations, the activity state can be determined based on activity data that includes data generated by multiple sensors of multiple sensor types. For example, an active home activity state can be determined based on location data indicative of the user's home and motion data that is descriptive of motion above a determined threshold.</p><p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram depicting an example computing environment <b>1000</b>, illustrating the detection of gestures based on an identified input location of a touch sensor in accordance with example embodiments of the present disclosure.</p><p id="p-0137" num="0136">Interactive object <b>104</b> and/or one or more computing devices in communication with interactive object <b>104</b> can detect a user gesture based at least in part on capacitive touch sensor <b>102</b>. For example, interactive object <b>104</b> and/or the one or more computing devices can implement a gesture manager <b>161</b> that can identify one or more gestures in response to touch input <b>1002</b> to the capacitive touch sensor <b>102</b>.</p><p id="p-0138" num="0137">Interactive object <b>104</b> can detect touch input <b>1002</b> to capacitive touch sensor <b>102</b> based on a change in capacitance associated with a set of conductive threads <b>210</b>. For example, a user can move an object (e.g., finger, conductive stylus, etc.) proximate to or touch capacitive touch sensor <b>102</b>, causing a response by the individual sensing elements. By way of example, the capacitance associated with each sensing element can change when an object touches or comes in proximity to the sensing element. As shown at (<b>1004</b>), sensing circuitry <b>126</b> can detect a change in capacitance associated with one or more of the sensing elements. Sensing circuitry <b>126</b> can generate touch data at (<b>1006</b>) that is indicative of the response (e.g., change in capacitance) of the sensing elements to the touch input. The touch data can include one or more touch input features associated with touch input <b>1002</b>. In some examples, the touch data may identify a particular element, and an associated response such as a change in capacitance. In some examples, the touch data may indicate a time associated with an element response.</p><p id="p-0139" num="0138">Gesture manager <b>161</b> can analyze the touch data to identify the one or more touch input features associated with touch input <b>1002</b>. Gesture manager <b>161</b> can be implemented at interactive object <b>104</b> (e.g., by one or more processors of internal electronics module <b>124</b> and/or removable electronics module <b>206</b>) and/or one or more computing devices remote from the interactive object <b>104</b>.</p><p id="p-0140" num="0139">Gesture manager <b>161</b> can analyze the touch data at (<b>1008</b>) to identify one or more sensor responses. For example, a lateral sensor response, longitudinal sensor response, or combination of sensor responses can be determined in example embodiments. By way of example, gesture manager <b>161</b> can determine at least one signal difference associated with a sensing line in a plurality of first conductive sensing elements coupled to a first surface of the flexible substrate and one signal difference associated with a sensing line in a plurality of second conductive sensing elements coupled to either a first or second surface of the flexible substrate. If the signal difference of the sensing line in a plurality of second conductive sensing elements is within a predetermined threshold range of the signal difference of the sensing line in a plurality of first conductive sensing elements, the gesture manager <b>161</b> can determine that the touch input is associated with a first sensing subregion of the flexible substrate rather than a second sensing subregion. However, if the signal difference exceeds a predetermined threshold value, the gesture manager <b>161</b> can determine that the touch input was received at the second sensing subregion.</p><p id="p-0141" num="0140">In accordance with some implementations, the one or more control circuits can analyze a respective response such as a resistance or capacitance of each sensing element of the touch sensor to determine whether a touch input is associated with a first sensing subregion of the touch sensor or a second subregion of the touch sensor.</p><p id="p-0142" num="0141">Gesture manager <b>161</b> can determine a gesture based at least in part on the touch data. In some examples gesture manager <b>161</b> may identify a particular gesture based on the surface of which the touch input is received. For example, a first gesture may be identified in response to a touch input at a first surface while a second gesture may be identified in response to a touch input at a second surface.</p><p id="p-0143" num="0142">In some examples, gesture manager <b>161</b> can identify at least one gesture based on reference data <b>1020</b>. Reference data <b>1020</b> can include data indicative of one or more predefined parameters associated with a particular input gesture. The reference data <b>1020</b> can be stored in a reference database <b>1015</b> in association with data indicative of one or more gestures. Reference database <b>1015</b> can be stored at interactive object <b>104</b> (e.g., internal electronics module <b>124</b> and/or removable electronics module <b>206</b>) and/or at one or more remote computing devices in communication with the interactive object <b>104</b>. In such a case, interactive object <b>104</b> can access reference database <b>1015</b> via one or more communication interfaces (e.g., network interface <b>162</b>).</p><p id="p-0144" num="0143">Gesture manager <b>161</b> can compare the touch data indicative of the touch input <b>1002</b> with reference data <b>1020</b> corresponding to at least one gesture. For example, gesture manager <b>161</b> can compare touch input features associated with touch input <b>1002</b> to reference data <b>1020</b> indicative of one or more pre-defined parameters associated with a gesture. Gesture manager <b>161</b> can determine a correspondence between at least one touch input feature and at least one parameter. Gesture manager <b>161</b> can detect a correspondence between touch input <b>1002</b> and at least one line gesture identified in reference database <b>1015</b> based on the determined correspondence between at least one touch input feature and at least one parameter. For example, a similarity between the touch input <b>1002</b> and a respective gesture can be determined based on a correspondence of touch input features and gesture parameters.</p><p id="p-0145" num="0144">In some examples, gesture manager <b>161</b> can input touch data into one or more machine learned gesture models <b>1025</b>. A machine-learned gesture model <b>1025</b> can be configured to output a detection of at least one gesture based on touch data and/or an identification of a surface or subset of sensing element associated with a touch input. Machine-learned gesture models <b>1025</b> can generate an output including data indicative of a gesture detection. For example, machine learned gesture model <b>1025</b> can be trained, via one or more machine learning techniques, using training data to detect particular gestures based on touch data. Similarly, a machine learned gesture model <b>1025</b> can be trained, via one or more machine learning techniques, using training data to detect a surface associated with a touch input.</p><p id="p-0146" num="0145">Gesture manager <b>161</b> can input touch data indicative of touch input <b>1002</b> and/or into machine learned gesture model <b>1025</b>. One or more gesture models <b>1025</b> can be configured to determine whether the touch input is associated with a first subset of sensing elements or a second subset of sensing elements. Additionally or alternatively, one or more gesture models <b>1025</b> can be configured to generate one or more outputs indicative of whether the touch data corresponds to one or more input gestures. Gesture model <b>1025</b> can output data indicative of a particular gesture associated with the touch data. Additionally, or alternatively, gesture model <b>1025</b> can output data indicative of a surface associated with the touch data. Gesture model <b>1025</b> can be configured to output data indicative of an inference or detection of a respective gesture based on a similarity between touch data indicative of touch input <b>1002</b> and one or more parameters associated with the gesture.</p><p id="p-0147" num="0146">In accordance with examples embodiments, a sensor system can selectively determine whether a touch input corresponds to a particular input gesture based at least in part on whether the touch input is determined to have been received at a first sensing subregion of the flexible substrate or a second subregion of the flexible substrate. In response to determining that the touch input is associated with the first subregion, the sensor system can determine whether the touch data corresponds to one or more gestures or other predetermined movements. For example, the sensor system can compare the touch data with reference data representing one or more predefined parameters to determine if the touch data corresponds to one or more gestures. In response to detecting the first input gesture, the sensor system can initiate a functionality at a computing device. In response to determining that the touch input is associated with the second surface, however, the sensor system can automatically determine that the touch input is not indicative of the particular input gesture such that a functionality is not initiated.</p><p id="p-0148" num="0147">In some implementations, a touch input at the first surface of the touch sensor can be associated with a first input gesture while the same or a similar touch input at a second surface of the touch sensor can be associated with a second input gesture. For example, the sensor system can determine whether touch data generated in response to a touch input is associated with the first surface of the touch sensor or the second surface of the touch sensor. Additionally, the sensor system can determine whether the touch data corresponds to one or more predefined parameters. If the touch data corresponds to the one or more predefined parameters and is associated with the first sensing subregion the sensor system can determine that a first input gesture has been performed. If, however, the touch data corresponds to the one or more predefined parameters and is associated with the second sensing subregion, the sensor system can determine that a second input gesture has been performed. By differentiating the sensing subregion at which an input is received, the sensor system can detect a larger number of input gestures in example embodiments.</p><p id="p-0149" num="0148">Interactive object <b>104</b> and/or a remote computing device in communication with interactive object <b>104</b> can initiate one or more actions based on a detected gesture. For example, the detected gesture can be associated with a navigation command (e.g., scrolling up/down/side, flipping a page, etc.) in one or more user interfaces coupled to interactive object <b>104</b> (e.g., via the capacitive touch sensor <b>102</b>, the controller, or both) and/or any of the one or more remote computing devices. In addition, or alternatively, the respective gesture can initiate one or more predefined actions utilizing one or more computing devices, such as, for example, dialing a number, sending a text message, playing a sound recording etc.</p><p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts an example audio feedback determination tree <b>1100</b> according to systems and methods disclosed herein. The determination tree <b>1100</b> can be user <b>1102</b> dependent such that the activity states, gesture sets, and/or the action sets can be user-specific. Alternatively, in some implementations, the activity states, gesture sets, and/or the action sets may be the same for a plurality of users. The user computing system <b>1102</b> can include one or more activity sensors to determine activity states <b>1104</b>. For example, a first activity state <b>1104</b>A can be determined based on the activity data. The first activity state <b>1104</b>A can be associated with an action set associated with a first gesture <b>1106</b>A and a second gesture <b>1106</b>B. A plurality of activity states <b>1104</b> associated with a plurality of action sets may be determined, including a second activity state <b>1104</b>B and an nth activity state <b>1104</b>C. The second activity state <b>1104</b>B can be associated with a plurality of actions associated with a plurality of gestures <b>1106</b> including a third gesture <b>1106</b>C, a fourth gesture <b>1106</b>D, and an nth gesture <b>1106</b>E. Additionally and/or alternatively, the second activity state <b>1104</b>B can include an action associated with either the first gesture <b>1106</b>A or the second gesture <b>1106</b>B.</p><p id="p-0151" num="0150">The systems and methods disclosed herein can include an audio notification being determined and provided in response to a change in activity state. For example, when a first activity state <b>1104</b>A is determined, a specific sound can play, and a different sound may be played in response to a second activity state <b>1104</b>B being determined.</p><p id="p-0152" num="0151">Each gesture <b>1106</b> can include a different valid audio feedback <b>1108</b>. For example, the system may process the input data to determine if the gesture is a valid gesture associated with the activity state <b>1104</b>. For example, when a successful first gesture <b>1106</b>A is completed, a first sound <b>1108</b>A may be played with the valid determination. If the gesture is determined to be invalid, a second sound <b>1110</b>A may be provided. Similarly, when a successful second gesture <b>1106</b>B is completed, a third sound <b>1108</b>B may be played with the valid determination. If the gesture is determined to be invalid, a fourth sound <b>1110</b>B may be provided. In some implementations, the invalid feedback <b>1110</b> can be the same for all gestures <b>1106</b> associated with a certain activity state <b>1104</b>. However, the valid feedback <b>1108</b> for each gesture can be different regardless of the activity state <b>1104</b>.</p><p id="p-0153" num="0152">In some implementations, the valid feedback <b>1108</b>C for the third gesture <b>1106</b>C can include a harmonious sound, and the invalid feedback <b>1110</b>C for the third gesture <b>1106</b>C can include a discordant sound. Additionally and/or alternatively, the invalid feedback <b>1110</b>D for the fourth gesture <b>1106</b>D can include a haptic feedback. In some implementations, the valid feedback <b>1108</b>D for the fourth gesture <b>1106</b>D can include a sound in the same musical key as the valid feedback <b>1108</b>C for the third gesture <b>1106</b>C.</p><p id="p-0154" num="0153">In some implementations, the valid feedback <b>1108</b>E and/or the invalid feedback <b>1110</b>E for the nth gesture <b>1106</b>E may be based at least in part on the one or more sensing lines contacted during the gesture. For example, the systems and methods can include providing line-specific audio feedback, and the valid feedback <b>1108</b>E can include a repetition of at least one of the line-specific audio feedbacks, while the invalid feedback <b>1110</b>E may include an off-key sound when compared to the line-specific audio feedback.</p><p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts an example thread visualizer application <b>1200</b>. The thread visualizer application <b>1200</b> can be utilized to further aid in training a user to provide valid gestures. The thread visualizer application <b>1200</b> can be downloaded and accessed on a mobile computing device or any computing device. The thread visualizer application <b>1200</b> can include a visual interface <b>1250</b> that can include a plurality of bars representing a plurality of sensing lines. A first set of bars can be superimposed over a second set of bars. The first set of bars can be indicative of at least one of a pressure level or a capacitance level for each respective sensing line during a period of time (e.g., half a second, two seconds, ten seconds, etc.). The second set of bars can represent a normalized maximum for each respective sensing line. In some implementations, the second set of bars can include bars of equal size or different sizes. Additionally and/or alternatively, a threshold line <b>1214</b> can be displayed on each bar of the second set of bars to display the threshold pressure for which a touch input would be registered. In some implementations, the threshold <b>1214</b> can be the same for each sensing line. However, in some implementations, the threshold <b>1214</b> may vary for each sensing line.</p><p id="p-0156" num="0155">The depicted visual display <b>1250</b> in <figref idref="DRAWINGS">FIG. <b>12</b></figref> represents six sensing lines. In the depicted example, the visual display <b>1250</b> indicates that a touch input is registered for two sensing lines (i.e., <b>1204</b> and <b>1208</b>). The other four sensing lines (i.e., <b>1202</b>, <b>1206</b>, <b>1210</b>, and <b>1212</b>) do not register a touch input despite some pressure being registered. The visual display <b>1250</b> can update in real-time to provide real-time data on registered inputs. The audio feedback determination system along with the thread visualizer application <b>1200</b> can provide real-time feedback that can help train or reinforce how gestures should be input for certain actions.</p><p id="p-0157" num="0156"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram depicting a context switching system for touch inputs in accordance with example embodiments of the present disclosure. In this example, a context switching system <b>1300</b> can have a number of associated gestures that it can detect. For example, the gestures can include swiping in the +x direction <b>1308</b>, swiping in the &#x2212;x direction <b>1310</b>, double tapping <b>1302</b>, swiping in <b>1304</b>, and swiping out <b>1306</b>. Each gesture can be associated with a particular action. However, given that the number of actions that may be desired can exceed the number of gestures that can be detected, the context switching system <b>1300</b> can enable several different contexts. Each context can associate one or more gestures with different actions based on the current context. Example contexts can include a navigation context <b>1312</b>, a music context <b>1314</b>, and a party context <b>1316</b>.</p><p id="p-0158" num="0157">In some examples, one or more gestures can be associated with switching contexts. In this example, the swipe +x <b>1308</b> and the swipe &#x2212;x <b>1310</b> gestures can be associated with switching from one context to another. Once a context is selected, the specific actions associated with particular gestures can be based on the associated context. For example, in the navigation context <b>1312</b>, the double tap gestures <b>1302</b> can be associated with a drop pin action, the swipe in gesture <b>1304</b> can be associated with a next direction action, the swipe out gesture <b>1306</b> is associated with an eta action.</p><p id="p-0159" num="0158"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts a flow chart diagram of an example method to perform according to example embodiments of the present disclosure. Although <figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method <b>1400</b> can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.</p><p id="p-0160" num="0159">At <b>1472</b>, a computing system can generate activity data descriptive of an activity state of a user and obtain input data associated with an input. The activity data can be generated by one or more activity sensors in a computing system. The one or more activity sensors can include one or more motion sensors, one or more location sensors, one or more time sensors, and/or one or more proximity sensors. In some implementations, the input data can include touch data associated with one or more touch inputs. Moreover, the touch data can be obtained in response to one or more touch inputs being obtained by one or more touch sensors in a computing system. The one or more touch sensors can include a plurality of sensing lines. The sensing lines can be included in a smart wearable and may be cylindrical in shape. Alternatively and/or additionally, the input data can be descriptive of inputs detected in a variety of manners with a variety of sensors. For example, in some implementations, the input can be a motion gesture that may be detected using one or more inertial sensors. The input data may be of an input detected by processing image data from an image sensor to detect the input. The input can be a hand wave, eye movement, blinking, finger movement, or mouth movements. In some implementations, the input data can include audio data descriptive of human speech generated using one or more audio sensors.</p><p id="p-0161" num="0160">At <b>1474</b>, the computing system can determine the activity state of the user based at least in part on activity data generated by the one or more sensors. The activity state can be an activity level (e.g., active, non-active, high activity level, or low activity level) in which the activity level is determined based on a user's pulse or based on motion data generated with an inertial sensor. Alternatively and/or additionally, the activity state can be a time or a location determined using activity data generated by one or more activity sensors.</p><p id="p-0162" num="0161">At <b>1476</b>, the computing system can determine a gesture validity associated with the touch input. Determining gesture validity can involve determining an action set is associated with the activity state, and then determining if the input is associated with any of the gestures associated with the action set. If the input is associated with any of the gestures, the gesture is valid. If they are not associated or matching, then the gesture is invalid.</p><p id="p-0163" num="0162">At <b>1478</b>, the computing system can determine an audio feedback based at least in part on the activity state and the gesture validity. In some implementations, an audio feedback for a valid gesture can be harmonious, while an audio feedback for an invalid gesture can be discordant. The audio feedback may be different for each activity state.</p><p id="p-0164" num="0163">At <b>1480</b>, the computing system can provide the audio feedback based on the activity state and the gesture validity. The audio feedback can be provided with one or more audio output devices in the computing system.</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts a flow chart diagram of an example method to perform according to example embodiments of the present disclosure. Although <figref idref="DRAWINGS">FIG. <b>15</b></figref> depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method <b>1500</b> can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.</p><p id="p-0166" num="0165">At <b>1502</b>, a computing system can detect a respective response associated with each of the plurality of parallel sensing lines in response to a touch input to the touch sensor.</p><p id="p-0167" num="0166">At <b>1504</b>, the computing system can determine a line-specific audio feedback for each sensing line associated with the touch input. The line-specific audio feedback may be determined by accessing a directory or database of sounds associated with the specific sensing line. In some implementations, the line-specific audio feedback may be based at least in part on a determined activity state. Additionally, in some implementations, each line-specific audio feedback for the plurality of sensing lines may have a common musical key but a different musical note.</p><p id="p-0168" num="0167">At <b>1506</b>, the computing system can send a first set of instructions to an audio output device to play the line-specific audio feedback. The first set of instructions can be sent to have the line-specific audio feedback provided in real-time to provide real-time feedback to a user. In some implementations, the line-specific audio feedback may mimic sounds from a musical instrument (e.g., a guitar, a banjo, a ukulele, a piano, etc.).</p><p id="p-0169" num="0168">At <b>1508</b>, the computing system can determine whether the touch input is associated with at least one of a plurality of actions. The plurality of actions may be a plurality of actions associated with a determined activity state (e.g., a high activity level or a user being at home). The plurality of actions can be associated with a plurality of gestures, and determining whether the touch input is associated with at least one of a plurality of actions can include determining whether the touch input is associated with at least one of the plurality of gestures.</p><p id="p-0170" num="0169">At <b>1510</b>, the computing system can send a second set of instructions to the audio output device to play a first sound if the touch input is associated with at least one of the plurality of actions. If the touch input is not associated with at least one of the plurality of actions, a second set of instructions can be sent to the audio output device to provide a second sound and/or provide haptic feedback. In some implementations, the first sound may include at least a portion of the line-specific audio feedback.</p><p id="p-0171" num="0170"><figref idref="DRAWINGS">FIG. <b>16</b></figref> depicts a flow chart diagram of an example method to perform according to example embodiments of the present disclosure. Although <figref idref="DRAWINGS">FIG. <b>16</b></figref> depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method <b>1600</b> can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.</p><p id="p-0172" num="0171">At <b>1602</b>, a computing system can process readings from the one or more contextual activity sensors to determine which of a predetermined plurality of user activities the user is performing. The one or more contextual activity sensors can include a plurality of sensor types and may be used to determine a plurality of user activity types (e.g., an activity level, a time of day, a location, or a connected device).</p><p id="p-0173" num="0172">During a first time interval in which the user is determined to be performing a first activity, the computing system can complete a first set of operations.</p><p id="p-0174" num="0173">At <b>1604</b>, the computing system can receive a first touch gesture from the user. The first touch gesture can include individual sensing line compressions, a swipe gesture, and/or a timed compression.</p><p id="p-0175" num="0174">At <b>1606</b>, the computing system can determine whether the first touch gesture is valid or invalid. Determination of the validity of the gesture can include determining the gesture set associated with the action set for the first activity. The first touch gesture can then be compared to the gesture set to determine if the first touch gesture matches any of the touch gestures in the gesture set. If the first touch gesture matches one or more gestures in the gesture set, the first touch gesture is determined to be valid.</p><p id="p-0176" num="0175">At <b>1608</b>, the computing system can provide first audio feedback of a first sound category to the user. The first audio feedback can include a first sound if the first gesture is valid or can include a second sound if the first gesture is invalid. The first sound can be harmonious, while the second sound can be discordant.</p><p id="p-0177" num="0176">During a second time interval in which the user is determined to be performing a second activity different than the first activity, the computing system can complete a second set of operations.</p><p id="p-0178" num="0177">At <b>1610</b>, the computing system can receive a second touch gesture from the user. The second touch gesture can include individual sensing line compressions, a swipe gesture, and/or a timed compression.</p><p id="p-0179" num="0178">At <b>1612</b>, the computing system can determine whether the second touch gesture is valid or invalid. Determination of the validity of the gesture can include determining the gesture set associated with the action set for the second activity. The second touch gesture can then be compared to the gesture set to determine if the second touch gesture matches any of the touch gestures in the gesture set. If the second touch gesture matches one or more gestures in the gesture set, the second touch gesture is determined to be valid.</p><p id="p-0180" num="0179">At <b>1614</b>, the computing system can provide second audio feedback of a second sound category to the user. The second audio feedback can include a third sound if the second gesture is valid or can include a fourth sound if the second gesture is invalid. The third sound can be harmonious, while the fourth sound can be discordant. The first sound, the second sound, the third sound, and the fourth sound can all be different sounds.</p><p id="p-0181" num="0180"><figref idref="DRAWINGS">FIG. <b>17</b></figref> depicts a flow chart diagram of an example method to perform according to example embodiments of the present disclosure. Although <figref idref="DRAWINGS">FIG. <b>17</b></figref> depicts steps performed in a particular order for purposes of illustration and discussion, the methods of the present disclosure are not limited to the particularly illustrated order or arrangement. The various steps of the method <b>1700</b> can be omitted, rearranged, combined, and/or adapted in various ways without deviating from the scope of the present disclosure.</p><p id="p-0182" num="0181">At <b>1702</b>, a computing system can generate activity data descriptive of a user activity. The activity data can be generated continuously and stored for a brief period of time or may be generated in response to a trigger event (e.g., a speech input, a device pairing, a touch input, etc.).</p><p id="p-0183" num="0182">At <b>1704</b>, the computing system can generate input data descriptive of one or more user inputs. The one or more user inputs can include a speech input, a touch input, and/or a visual input.</p><p id="p-0184" num="0183">At <b>1706</b>, the computing system can determine the user activity based at least in part on the activity data. The determination of the user activity can include using a machine-learned model trained to determine user activities.</p><p id="p-0185" num="0184">At <b>1708</b>, the computing system can determine a plurality of actions associated with the user activity. The plurality of actions can be actions manually associated with the user activity via a predefined user setting or via user inputs. In some implementations, the plurality of actions can be associated with the user activity based on historical data. The historical data can include data indicative of the actions most commonly selected during a respective user activity. The plurality of actions can be stored with associated sounds and associated activity states.</p><p id="p-0186" num="0185">At <b>1710</b>, the computing system can determine an audio feedback based on the input data and the plurality of actions. The audio feedback can be different for each respective action and for each respective input type.</p><p id="p-0187" num="0186"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> depicts a block diagram of an example computing environment <b>1400</b> that can be used to implement any type of computing device as described herein. The system environment includes a remote computing system <b>1402</b>, an interactive computing system <b>1420</b>, and a training computing system <b>1440</b> that are communicatively coupled over a network <b>1460</b>. The interactive computing system <b>1420</b> can be used to implement an interactive object in some examples.</p><p id="p-0188" num="0187">The remote computing system <b>1402</b> can include any type of computing device, such as, for example, a personal computing device (e.g., laptop or desktop), a mobile computing device (e.g., smartphone or tablet), a gaming console or controller, an embedded computing device, a server computing device, or any other type of computing device.</p><p id="p-0189" num="0188">The remote computing system <b>1402</b> includes one or more processors <b>1404</b> and a memory <b>1406</b>. The one or more processors <b>1404</b> can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. The memory <b>1406</b> can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. The memory <b>1406</b> can store data <b>1408</b> and instructions <b>1410</b> which are executed by the processor <b>1404</b> to cause the remote computing system <b>1402</b> to perform operations.</p><p id="p-0190" num="0189">The remote computing system <b>1402</b> can also include one or more input devices <b>1412</b> that can be configured to receive user input. By way of example, the one or more input devices <b>1412</b> can include one or more soft buttons, hard buttons, microphones, scanners, cameras, etc. configured to receive data from a user of the remote computing system <b>1402</b>. For example, the one or more input devices <b>1412</b> can serve to implement a virtual keyboard and/or a virtual number pad. Other example user input devices <b>1412</b> include a microphone, a traditional keyboard, or other means by which a user can provide user input.</p><p id="p-0191" num="0190">The remote computing system <b>1402</b> can also include one or more output devices <b>1414</b> that can be configured to provide data to one or more users. By way of example, the one or more output device(s) <b>1414</b> can include a user interface configured to display data to a user of the remote computing system <b>1402</b>. Other example output device(s) <b>1414</b> include one or more visual, tactile, and/or audio devices configured to provide information to a user of the remote computing system <b>1402</b>.</p><p id="p-0192" num="0191">The interactive computing system <b>1420</b> can be used to implement any type of interactive object such as, for example, a wearable computing device. The interactive computing system <b>1420</b> includes one or more processors <b>1422</b> and a memory <b>1424</b>. The one or more processors <b>1422</b> can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. The memory <b>1424</b> can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. The memory <b>1424</b> can store data <b>1426</b> and instructions <b>1428</b> which are executed by the processor <b>1422</b> to cause the interactive computing system <b>1420</b> to perform operations.</p><p id="p-0193" num="0192">The interactive computing system <b>1420</b> can also include one or more input devices <b>1430</b> that can be configured to receive user input. For example, the user input device <b>1430</b> can be a touch-sensitive component (e.g., a touch sensor <b>102</b>) that is sensitive to the touch of a user input object (e.g., a finger or a stylus). As another example, the user input device <b>1430</b> can be an inertial component (e.g., inertial measurement unit <b>158</b>) that is sensitive to the movement of a user. Other example user input components include a microphone, a traditional keyboard, or other means by which a user can provide user input. The interactive computing system <b>1420</b> can also include one or more output devices <b>1432</b> configured to provide data to a user. For example, the one or more output devices <b>1432</b> can include one or more visual, tactile, and/or audio devices configured to provide the information to a user of the interactive computing system <b>1420</b>.</p><p id="p-0194" num="0193">The training computing system <b>1440</b> includes one or more processors <b>1442</b> and a memory <b>1444</b>. The one or more processors <b>1442</b> can be any suitable processing device (e.g., a processor core, a microprocessor, an ASIC, a FPGA, a controller, a microcontroller, etc.) and can be one processor or a plurality of processors that are operatively connected. The memory <b>1444</b> can include one or more non-transitory computer-readable storage mediums, such as RAM, ROM, EEPROM, EPROM, flash memory devices, magnetic disks, etc., and combinations thereof. The memory <b>1444</b> can store data <b>1446</b> and instructions <b>1448</b> which are executed by the processor <b>1442</b> to cause the training computing system <b>1440</b> to perform operations. In some implementations, the training computing system <b>1440</b> includes or is otherwise implemented by one or more server computing devices.</p><p id="p-0195" num="0194">The training computing system <b>1440</b> can include a model trainer <b>1452</b> that trains a machine-learned classification model <b>1450</b> using various training or learning techniques, such as, for example, backwards propagation of errors. In other examples as described herein, training computing system <b>1440</b> can train machine-learned classification model <b>1450</b> using training data <b>1454</b>. For example, the training data <b>1454</b> can include labeled sensor data generated by interactive computing system <b>1420</b>. The training computing system <b>1440</b> can receive the training data <b>1454</b> from the interactive computing system <b>1420</b>, via network <b>1460</b>, and store the training data <b>1454</b> at training computing system <b>1440</b>. The machine-learned classification model <b>1450</b> can be stored at training computing system <b>1440</b> for training and then deployed to remote computing system <b>1402</b> and/or the interactive computing system <b>1420</b>. In some implementations, performing backwards propagation of errors can include performing truncated backpropagation through time. The model trainer <b>1452</b> can perform a number of generalization techniques (e.g., weight decays, dropouts, etc.) to improve the generalization capability of the classification model <b>1450</b>.</p><p id="p-0196" num="0195">In particular, the training data <b>1454</b> can include a plurality of instances of sensor data, where each instance of sensor data has been labeled with ground truth inferences such as one or more predefined movement recognitions. For example, the label(s) for each instance of sensor data can describe the position and/or movement (e.g., velocity or acceleration) of an object movement. In some implementations, the labels can be manually applied to the training data by humans. In some implementations, the machine-learned classification model <b>1450</b> can be trained using a loss function that measures a difference between a predicted inference and a ground-truth inference.</p><p id="p-0197" num="0196">The model trainer <b>1452</b> includes computer logic utilized to provide desired functionality. The model trainer <b>1452</b> can be implemented in hardware, firmware, and/or software controlling a general purpose processor. For example, in some implementations, the model trainer <b>1452</b> includes program files stored on a storage device, loaded into a memory and executed by one or more processors. In other implementations, the model trainer <b>1452</b> includes one or more sets of computer-executable instructions that are stored in a tangible computer-readable storage medium such as RAM hard disk or optical or magnetic media.</p><p id="p-0198" num="0197">In some examples, a training database <b>1456</b> can be stored in memory on an interactive object, removable electronics module, user device, and/or a remote computing device. For example, in some embodiments, a training database <b>1456</b> can be stored on one or more remote computing devices such as one or more remote servers. The machine-learned classification model <b>1450</b> can be trained based on the training data in the training database <b>1456</b>. For example, the machine-learned classification model <b>1450</b> can be learned using various training or learning techniques, such as, for example, backwards propagation of errors based on the training data from training database <b>1456</b>.</p><p id="p-0199" num="0198">In this manner, the machine-learned classification model <b>1450</b> can be trained to determine at least one of a plurality of predefined movement(s) associated with the interactive object based on movement data.</p><p id="p-0200" num="0199">The machine-learned classification model <b>1450</b> can be trained, via one or more machine learning techniques using training data. For example, the training data can include movement data previously collected by one or more interactive objects. By way of example, one or more interactive objects can generate sensor data based on one or more movements associated with the one or more interactive objects. The previously generated sensor data can be labeled to identify at least one predefined movement associated with the touch and/or the inertial input corresponding to the sensor data. The resulting training data can be collected and stored in a training database <b>1456</b>.</p><p id="p-0201" num="0200">The network <b>1460</b> can be any type of communications network, such as a local area network (e.g., intranet), wide area network (e.g., Internet), or some combination thereof and can include any number of wired or wireless links. In general, communication over the network <b>1460</b> can be carried via any type of wired and/or wireless connection, using a wide variety of communication protocols (e.g., TCP/IP, HTTP, SMTP, FTP), encodings or formats (e.g., HTML, XML), and/or protection schemes (e.g., VPN, secure HTTP, SSL).</p><p id="p-0202" num="0201"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> illustrates one example computing system that can be used to implement the present disclosure. Other computing systems can be used as well. For example, in some implementations, the remote computing system <b>1402</b> can include the model trainer <b>1452</b> and the training data <b>1454</b>. In such implementations, the classification model <b>1450</b> can be trained and used locally at the remote computing system <b>1402</b>. In some of such implementations, the remote computing system <b>1402</b> can implement the model trainer <b>1452</b> to personalize the classification model <b>1450</b> based on user-specific movements.</p><p id="p-0203" num="0202"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> depicts a block diagram of an example computing device <b>10</b> that performs according to example embodiments of the present disclosure. The computing device <b>10</b> can be a user computing device or a server computing device.</p><p id="p-0204" num="0203">The computing device <b>10</b> includes a number of applications (e.g., applications <b>1</b> through N). Each application contains its own machine learning library and machine-learned model(s). For example, each application can include a machine-learned model. Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc.</p><p id="p-0205" num="0204">As illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref>, each application can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, each application can communicate with each device component using an API (e.g., a public API). In some implementations, the API used by each application is specific to that application.</p><p id="p-0206" num="0205"><figref idref="DRAWINGS">FIG. <b>18</b>C</figref> depicts a block diagram of an example computing device <b>50</b> that performs according to example embodiments of the present disclosure. The computing device <b>50</b> can be a user computing device or a server computing device.</p><p id="p-0207" num="0206">The computing device <b>50</b> includes a number of applications (e.g., applications <b>1</b> through N). Each application is in communication with a central intelligence layer. Example applications include a text messaging application, an email application, a dictation application, a virtual keyboard application, a browser application, etc. In some implementations, each application can communicate with the central intelligence layer (and model(s) stored therein) using an API (e.g., a common API across all applications).</p><p id="p-0208" num="0207">The central intelligence layer includes a number of machine-learned models. For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>C</figref>, a respective machine-learned model (e.g., a model) can be provided for each application and managed by the central intelligence layer. In other implementations, two or more applications can share a single machine-learned model. For example, in some implementations, the central intelligence layer can provide a single model (e.g., a single model) for all of the applications. In some implementations, the central intelligence layer is included within or otherwise implemented by an operating system of the computing device <b>50</b>.</p><p id="p-0209" num="0208">The central intelligence layer can communicate with a central device data layer. The central device data layer can be a centralized repository of data for the computing device <b>50</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>18</b>C</figref>, the central device data layer can communicate with a number of other components of the computing device, such as, for example, one or more sensors, a context manager, a device state component, and/or additional components. In some implementations, the central device data layer can communicate with each device component using an API (e.g., a private API).</p><p id="p-0210" num="0209">The technology discussed herein makes reference to servers, databases, software applications, and other computer-based systems, as well as actions taken and information sent to and from such systems. The inherent flexibility of computer-based systems allows for a great variety of possible configurations, combinations, and divisions of tasks and functionality between and among components. For instance, processes discussed herein can be implemented using a single device or component or multiple devices or components working in combination. Databases and applications can be implemented on a single system or distributed across multiple systems. Distributed components can operate sequentially or in parallel.</p><p id="p-0211" num="0210">While the present subject matter has been described in detail with respect to various specific example embodiments thereof, each example is provided by way of explanation, not limitation of the disclosure. Those skilled in the art, upon attaining an understanding of the foregoing, can readily produce alterations to, variations of, and equivalents to such embodiments. Accordingly, the subject disclosure does not preclude inclusion of such modifications, variations and/or additions to the present subject matter as would be readily apparent to one of ordinary skill in the art. For instance, features illustrated or described as part of one embodiment can be used with another embodiment to yield a still further embodiment. Thus, it is intended that the present disclosure cover such alterations, variations, and equivalents.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computing system for an interactive object, comprising:<claim-text>an input sensor;</claim-text><claim-text>one or more activity sensors configured to generate activity data associated with user activity;</claim-text><claim-text>one or more control circuits configured to:<claim-text>obtain input data associated with at least one input detected by the input sensor wherein the input sensor comprises a touch sensor comprising a plurality of conductive lines, and wherein the input data comprises touch data associated with at least one touch input to the touch sensor, the touch data based at least in part on a respective response to the at least one touch input by the plurality of conductive lines;</claim-text><claim-text>in response to obtaining the input data:<claim-text>process the touch data to determine each of the plurality of conductive lines associated with the touch input;</claim-text><claim-text>determine a line-specific audio feedback for each of the plurality of conductive lines associated with the touch input; and</claim-text><claim-text>provide the line-specific audio feedback for each of the plurality of conductive lines via one or more audio output devices;</claim-text></claim-text><claim-text>determine an activity state of a user based at least in part on the activity data generated by the one or more activity sensors;</claim-text><claim-text>determine a gesture validity associated with the at least one input detected by the input sensor;</claim-text><claim-text>determine a validity audio feedback based at least in part on the activity state and the gesture validity; and</claim-text></claim-text><claim-text>the one or more audio output devices configured to provide the validity audio feedback based at least in part on the activity state and the gesture validity.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computing system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more control circuits are configured to:<claim-text>in response to determining that the user is engaged in a first user activity during a first time interval based at least in part on the activity data;<claim-text>determine the gesture validity by determining whether a first input during the first time interval corresponds to at least one predefined gesture of a first set of predefined gestures;</claim-text><claim-text>determine and provide a first audio feedback response in response to determining that the user is engaged in the first user activity and the first input corresponds to at least one predefined gesture of the first set of predefined gestures;</claim-text></claim-text><claim-text>in response to determining that the user is engaged in a second user activity during a second time interval based at least in part on the activity data;<claim-text>determine the gesture validity by determining whether a second input during the second time interval corresponds to at least one predefined gesture of a second set of predefined gestures;</claim-text><claim-text>determine and provide a second audio feedback response in response to determining that the user is engaged in the second user activity and the second input corresponds to at least one predefined gesture of the second set of predefined gestures, the second audio feedback response being different than the first audio feedback response.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computing system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the one or more control circuits are configured to:<claim-text>in response to determining that the user is engaged in the first user activity during the first time interval based at least in part on the activity data;<claim-text>determine and provide a third audio feedback response in response to determining that the user is engaged in the first user activity and the first input does not correspond to at least one predefined gesture of the first set of predefined gestures, the third audio feedback response being different than the first audio feedback response;</claim-text></claim-text><claim-text>in response to determining that the user is engaged in the second user activity during a second time interval based at least in part on the activity data;<claim-text>determine and provide a fourth audio feedback response in response to determining that the user is engaged in the second user activity and the second input does not correspond to at least one predefined gesture of the second set of predefined gestures, the fourth audio feedback response being different than the third audio feedback response.</claim-text></claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computing system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the one or more control circuits are configured to:<claim-text>in response to determining that the user is engaged in the first user activity during a first time interval based at least in part on the activity data;<claim-text>determine and provide no gesture validity audio feedback in response to determining that the user is engaged in the first user activity and the first input does not correspond to at least one predefined gesture of the first set of predefined gestures;</claim-text></claim-text><claim-text>in response to determining that the user is engaged in the second user activity during a second time interval based at least in part on the activity data;<claim-text>determine and provide no audio feedback in response to determining that the user is engaged in the second user activity and the second input does not correspond to at least one predefined gesture of the second set of predefined gestures.</claim-text></claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computing system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the first set of predefined gestures and the second set of predefined gestures comprise the same gestures.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computing system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more activity sensors comprise at least one of an inertial sensor, a location sensor, or a proximity sensor.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computing system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the gesture validity associated with the at least one input comprises:<claim-text>determining a plurality of gestures associated with the activity state of the user; and</claim-text><claim-text>determining whether the input is associated with at least one gesture of the plurality of gestures.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computing system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the one or more control circuits are configured to determine an action associated with the input; and</claim-text><claim-text>the computing system further comprises one or more communication components configured to send instructions to a computing device to complete the action.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computing system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the input sensor comprises the touch sensor, wherein the touch sensor comprises a plurality of conductive sensing elements integrated with a flexible substrate, wherein the plurality of conductive sensing elements comprise the plurality of conductive lines.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. (canceled)</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein each of the plurality of conductive lines are associated with a different pitch.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A computer-implemented method, comprising:<claim-text>detecting, by a computing system comprising one or more processors, a respective response associated with each of a plurality of parallel sensing lines of a touch sensor in response to at least one touch input;</claim-text><claim-text>obtaining, by the computing system, activity data generated with one or more activity sensors;</claim-text><claim-text>determining, by the computing system, an activity state based on the activity data;</claim-text><claim-text>determining, by the computing system, a line-specific audio feedback for each sensing line associated with the touch input;</claim-text><claim-text>sending, by the computing system, a first set of instructions to an audio output device to provide the line-specific audio feedback for each of the plurality of parallel sensing lines associated with the at least one touch input;</claim-text><claim-text>determining, by the computing system, whether the touch input is associated with at least one of a plurality of actions; and</claim-text><claim-text>sending, by the computing system, a second set of instructions to the audio output device to provide a gesture audio feedback if the touch input is associated with at least one of the plurality of actions, wherein the gesture audio feedback is determined based at least in part on the activity state.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the line-specific audio feedback for each sensing line is of a different sound frequency.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>in response to determining the touch input is associated with at least one of the plurality of actions, sending action instructions to a computing device to complete an action.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>determining that a user activity state changes from a first activity state to a second activity state;</claim-text><claim-text>determining an activity state sound associated with the second activity state; and</claim-text><claim-text>sending activity state instructions to the audio output device to play the activity state sound.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the second set of instructions comprise at least one of an instruction to play a second sound if the touch input is not associated with at least one of the plurality of actions or an instruction to provide haptic feedback if the touch input is not associated with at least one of the plurality of actions.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the gesture audio feedback comprises at least one of the line-specific audio feedbacks associated with at least one of the sensing lines associated with the touch input.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The computer-implemented method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein:<claim-text>the line-specific audio feedback is a different note for each sensing line;</claim-text><claim-text>the line-specific audio feedback for each sensing line comprise a common musical key; and</claim-text><claim-text>in response to the touch input not being associated with at least one of the plurality of actions, sending, by the computing system, a third set of instructions to the audio output device to provide an off-key sound.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. An interactive object, comprising:<claim-text>an article having one or more touch sensors integrated therein for recognizing a user gestural input, wherein the one or more touch sensors comprise a plurality of conductive lines;</claim-text><claim-text>an electronic device associated with the article, the electronic device being communicatively coupled to the one or more touch sensors of the article, the electronic device having a processor and an audio output device capable of providing audio feedback to the user; and</claim-text><claim-text>one or more contextual activity sensors associated with at least one of the article or the electronic device, the one or more contextual activity sensors including at least one of an inertial motion sensor, a location sensor, a proximity sensor, or other type of sensor;</claim-text><claim-text>wherein the electronic device is configured and programmed to:<claim-text>process readings from the one or more contextual activity sensors to determine which of a predetermined plurality of user activities the user is performing;</claim-text><claim-text>during a first time interval in which the user is determined to be performing a first activity:<claim-text>receiving a first touch gesture from the user;</claim-text><claim-text>processing the first touch gesture to determine each of the plurality of conductive lines associated with the first touch gesture;</claim-text><claim-text>determining a line-specific audio feedback for each of the plurality of conductive lines associated with the first touch gesture;</claim-text><claim-text>providing the line-specific audio feedback via the audio output device;</claim-text><claim-text>determining whether the first touch gesture is valid or invalid; and</claim-text><claim-text>providing first validity audio feedback of a first sound category to the user, wherein the first validity audio feedback comprises a first sound if the first touch gesture is valid and comprises a second sound if the first touch gesture is invalid; and</claim-text></claim-text><claim-text>during a second time interval in which the user is determined to be performing a second activity different than the first activity:<claim-text>receiving a second touch gesture from the user;</claim-text><claim-text>determining whether the second touch gesture is valid or invalid; and</claim-text><claim-text>providing second validity audio feedback of a second sound category to the user, the second sound category being different than said first sound category, wherein the second validity audio feedback comprises a third sound if the second touch gesture is valid and comprises a fourth sound if the second touch gesture is invalid;</claim-text></claim-text><claim-text>whereby the user is provided with audio feedback sounds that, in addition to confirming whether their touch gestures are valid, also provide confirmation of which activity the user has been determined to be performing.</claim-text></claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The computing system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more activity sensors comprise a location sensor;<claim-text>wherein the activity state is determined based on location data generated by the location sensor, wherein the activity state is associated with a location, wherein the location is associated with a sound category; and</claim-text><claim-text>wherein the validity audio feedback is associated with the sound category.</claim-text></claim-text></claim></claims></us-patent-application>