<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230000337A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230000337</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17770878</doc-number><date>20191025</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>15</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>0025</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>102</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>152</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">IMAGE PROCESSING METHOD, IMAGE PROCESSING DEVICE, AND PROGRAM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>NIKON CORPORATION</orgname><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HIROKAWA</last-name><first-name>Mariko</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TANABE</last-name><first-name>Yasushi</first-name><address><city>Fujisawa-shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/041982</doc-number><date>20191025</date></document-id><us-371c12-date><date>20220830</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Image processing performed by a processor and including acquiring a two-dimensional fundus image, acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image, and creating data to represent a process to move the first point to the second point.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.16mm" wi="158.75mm" file="US20230000337A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="117.35mm" wi="125.98mm" file="US20230000337A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.80mm" wi="153.08mm" orientation="landscape" file="US20230000337A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="123.27mm" wi="128.27mm" file="US20230000337A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="149.52mm" wi="113.62mm" file="US20230000337A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="116.84mm" wi="68.66mm" file="US20230000337A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="171.79mm" wi="91.86mm" file="US20230000337A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="186.86mm" wi="97.79mm" file="US20230000337A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="96.18mm" wi="104.82mm" file="US20230000337A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="95.84mm" wi="105.58mm" file="US20230000337A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="87.29mm" wi="47.84mm" file="US20230000337A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="213.53mm" wi="139.11mm" orientation="landscape" file="US20230000337A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="187.88mm" wi="138.01mm" orientation="landscape" file="US20230000337A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="223.86mm" wi="145.80mm" orientation="landscape" file="US20230000337A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="187.28mm" wi="137.58mm" orientation="landscape" file="US20230000337A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="187.88mm" wi="141.22mm" orientation="landscape" file="US20230000337A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="163.32mm" wi="113.62mm" file="US20230000337A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="203.45mm" wi="93.22mm" file="US20230000337A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="176.11mm" wi="119.97mm" file="US20230000337A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="182.12mm" wi="109.73mm" file="US20230000337A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="223.69mm" wi="146.56mm" orientation="landscape" file="US20230000337A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="187.20mm" wi="149.10mm" orientation="landscape" file="US20230000337A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="187.20mm" wi="139.78mm" orientation="landscape" file="US20230000337A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="187.71mm" wi="143.51mm" orientation="landscape" file="US20230000337A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="128.95mm" wi="78.49mm" file="US20230000337A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="134.37mm" wi="78.40mm" file="US20230000337A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="137.41mm" wi="78.40mm" file="US20230000337A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">The present invention relates to an image processing method, an image processing device, and a program.</p><heading id="h-0001" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">A panorama fundus image combining device and method are disclosed in United States Patent Application Publication No. 2009/0136100, and there is a desire for an appropriate image display method for analyzing and examining a fundus.</p><heading id="h-0002" level="1">SUMMARY OF INVENTION</heading><p id="p-0004" num="0003">A first aspect of technology disclosed herein is image processing performed by a processor and including acquiring a two-dimensional fundus image, acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image, and creating data to represent a process to move the first point to the second point.</p><p id="p-0005" num="0004">An image processing device of a second aspect of technology disclosed herein including a memory, and a processor coupled to the memory. The processor is configured to acquire a two-dimensional fundus image, acquire a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image, and create data to represent a process to move the first point to the second point.</p><p id="p-0006" num="0005">A program of a third aspect of technology disclosed herein causes a computer to execute processing including acquiring a two-dimensional fundus image, acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image, and creating data to represent a process to move the first point to the second point.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an ophthalmic system <b>100</b>.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic configuration diagram illustrating an overall configuration of an ophthalmic device <b>110</b>.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of configuration of an electrical system of a server <b>140</b>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating functionality of a CPU <b>262</b> of the server <b>140</b> of a first exemplary embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of image processing by the server <b>140</b> of the first exemplary embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of 3D rendering processing of step <b>504</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a diagram illustrating a UWF fundus image G<b>1</b>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a diagram illustrating an eyeball and an imaging angle.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating a point g<b>1</b> of a UWF fundus image G<b>1</b> and a point m<b>1</b> on an eyeball model M corresponding to the point g<b>1</b>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram illustrating a path L<b>1</b> along which a pixel at a point mg<b>1</b> of an UWF fundus image G<b>1</b> moves to a point m<b>1</b> on an eyeball model M corresponding to a point g<b>1</b>.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a way in which a pixel at a point mg<b>1</b> of an UWF fundus image G<b>1</b> moves at a constant speed V<b>0</b> along the path L<b>1</b> to points md, . . . , mh, . . . mm, . . . m<b>1</b>.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram illustrating respective frames of video data.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating a first display pattern of a display screen <b>400</b>A.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a diagram illustrating images displayed on a video data display section <b>455</b>A.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating a second display pattern of the display screen <b>400</b>A.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a diagram illustrating a third display pattern of the display screen <b>400</b>A.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram illustrating functionality of a CPU <b>262</b> of a server <b>140</b> of a second exemplary embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a flowchart of image processing by the server <b>140</b> of the server <b>140</b> of the second exemplary embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart of positional alignment processing between images of step <b>324</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>19</b>A</figref> is a diagram illustrating a manner in which a line segment LGU is set on a UWF upward looking fundus image GU.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>19</b>B</figref> is a diagram illustrating a manner in which a line segment LGD is set on a UWF downward looking fundus image GDC.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is diagram for explaining generation of a montage image GM.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram illustrating a first display format of a display screen <b>400</b>B.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a diagram illustrating a second display format of the display screen <b>400</b>B.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a diagram illustrating a third display format of the display screen <b>400</b>B.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flowchart of image processing by a server <b>140</b> of a first modified example.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a flowchart of image processing by a server <b>140</b> of a second modified example.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flowchart of image processing by a server <b>140</b> of a third modified example.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DESCRIPTION OF EMBODIMENTS</heading><heading id="h-0005" level="1">First Exemplary Embodiment</heading><p id="p-0035" num="0034">Detailed explanation follows regarding a first exemplary embodiment of technology disclosed herein, with reference to the drawings.</p><p id="p-0036" num="0035">Explanation follows regarding a configuration of an ophthalmic system <b>100</b>, with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the ophthalmic system <b>100</b> includes an ophthalmic device <b>110</b>, an eye axial length measurement device <b>120</b>, a management server device (referred to hereafter as &#x201c;server&#x201d;) <b>140</b>, and an image display device (referred to hereafter as &#x201c;viewer&#x201d;) <b>150</b>. The ophthalmic device <b>110</b> acquires an image of the fundus. The eye axial length measurement device <b>120</b> measures the axial length of the eye of a patient. The server <b>140</b> stores fundus images that were obtained by imaging the fundus of patients using the ophthalmic device <b>110</b> in association with patient IDs. The viewer <b>150</b> displays medical information such as fundus images acquired from the server <b>140</b>.</p><p id="p-0037" num="0036">The ophthalmic device <b>110</b>, the. eye axial length measurement device <b>120</b>, the server <b>140</b>, and the viewer <b>150</b> are connected together through a network <b>130</b>.</p><p id="p-0038" num="0037">Next, explanation follows regarding a configuration of the ophthalmic device <b>110</b>, with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0039" num="0038">For ease of explanation, scanning laser ophthalmoscope is abbreviated to SLO. Optical coherence tomography is also abbreviated to OCT.</p><p id="p-0040" num="0039">With the ophthalmic device <b>110</b> installed on a horizontal plane and a horizontal direction taken as an X direction, a direction perpendicular to the horizontal plane is denoted a Y direction, and a direction connecting the center of the pupil at the anterior eye portion of the examined eye <b>12</b> and the center of the eyeball is denoted a Z direction. The X direction, the Y direction, and the Z direction are thus mutually perpendicular directions.</p><p id="p-0041" num="0040">The ophthalmic device <b>110</b> includes an imaging device <b>14</b> and a control device <b>16</b>. The imaging device <b>14</b> is provided with an SLO unit <b>18</b>, an OCT unit <b>20</b>, and an imaging optical system <b>19</b>, and acquires a fundus image of the fundus of the examined eye <b>12</b>. Two-dimensional fundus images that have been acquired by the SLO unit <b>18</b> are referred to hereafter as SLO images. Tomographic images, face-on images (en-face images) and the like of the retina created based on OCT data acquired by the OCT unit <b>20</b> are referred to hereafter as OCT images.</p><p id="p-0042" num="0041">The control device <b>16</b> includes a computer provided with a Central Processing Unit (CPU) <b>16</b>A, Random Access Memory (RAM) <b>16</b>B, Read-Only Memory (ROM) <b>16</b>C, and an input/output (I/O) port <b>16</b>D.</p><p id="p-0043" num="0042">The control device <b>16</b> is provided with an input/display device <b>16</b>E connected to the CPU <b>16</b>A through the I/O port <b>16</b>D. The input/display device <b>16</b>E includes a graphical user interface to display images of the examined eye <b>12</b> and to receive various instructions from a user. An example of the graphical user interface is a touch panel display.</p><p id="p-0044" num="0043">The control device <b>16</b> is also provided with an image processing device <b>16</b>G connected to the I/O port <b>16</b>D. The image processing device <b>16</b>G generates images of the examined eye <b>12</b> based on data acquired by the imaging device <b>14</b>. The control device <b>16</b> is provided with a communication interface (I/F) <b>16</b>F connected to the I/O port <b>16</b>D. The ophthalmic device <b>110</b> is connected to the eye axial length measurement device <b>120</b>, the server <b>140</b>, and the viewer <b>150</b> through the communication interface (I/F) <b>16</b>F and the network <b>130</b>.</p><p id="p-0045" num="0044">Although the control device <b>16</b> of the ophthalmic device <b>110</b> is provided with the input/display device <b>16</b>E as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the technology disclosed herein is not limited thereto. For example, a configuration may adopted in which the control device <b>16</b> of the ophthalmic device <b>110</b> is not provided with the input/display device <b>16</b>E, and instead a separate input/display device is provided that is physically independent of the ophthalmic device <b>110</b>. In such cases, the display device is provided with an image processing processor unit that operates under the control of the CPU <b>16</b>A in the control device <b>16</b>. Such an image processing processor unit may display SLO images and the like based on an image signal output as an instruction by the CPU <b>16</b>A.</p><p id="p-0046" num="0045">The imaging device <b>14</b> operates under the control of the CPU <b>16</b>A of the control device <b>16</b>. The imaging device <b>14</b> includes the SLO unit <b>18</b>, the imaging optical system <b>19</b>, and the OCT unit <b>20</b>. The imaging optical system <b>19</b> includes a first optical scanner <b>22</b>, a second optical scanner <b>24</b>, and a wide-angle optical system <b>30</b>.</p><p id="p-0047" num="0046">The first optical scanner <b>22</b> scans light emitted from the SLO unit <b>18</b> two dimensionally in the X direction and the Y direction. The second optical scanner <b>24</b> scans light emitted from the OCT unit <b>20</b> two dimensionally in the X direction and the Y direction. As long as the first optical scanner <b>22</b> and the second optical scanner <b>24</b> are optical elements capable of deflecting light beams, they may be configured by any out of, for example, polygon mirrors, mirror galvanometers, or the like. A combination thereof may also be employed.</p><p id="p-0048" num="0047">The wide-angle optical system <b>30</b> includes an objective optical system (not illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) provided with a common optical system <b>28</b>, and a combining section <b>26</b> that combines light from the SLO unit <b>18</b> with light from the OCT unit <b>20</b>.</p><p id="p-0049" num="0048">The objective optical system of the common optical system <b>28</b> may be a reflection optical system employing a concave mirror such as an elliptical mirror, a refraction optical system employing a wide-angle lens, or may be a reflection-refraction optical system employing a combination of a concave mirror and a lens. Employing a wide-angle optical system that utilizes an elliptical mirror, wide-angle lens, or the like enables imaging to be performed not only of a central portion of the fundus where the optic nerve head and macular are present, but also of the retina at a fundus peripheral portion where an equatorial portion of the eyeball and vortex veins are present.</p><p id="p-0050" num="0049">For a system including an elliptical mirror, a configuration may be adopted that utilizes an elliptical mirror system as disclosed in International Publication (WO) Nos. 2016/103484 or 2016/103489. The disclosures of WO Nos. 2016/103484 and 2016/103489 are incorporated in their entirety by reference herein.</p><p id="p-0051" num="0050">Observation of the fundus over a wide field of view (FOV) <b>12</b>A is implemented by employing the wide-angle optical system <b>30</b>. The FOV <b>12</b>A refers to a range capable of being imaged by the imaging device <b>14</b>. The FOV <b>12</b>A may be expressed as a viewing angle. In the present exemplary embodiment the viewing angle may be defined in terms of an internal illumination angle and an external illumination angle. The external illumination angle is the angle of illumination by a light beam shone from the ophthalmic device <b>110</b> toward the examined eye <b>12</b>, and is an angle of illumination defined with respect to the pupil <b>27</b>. The internal illumination angle is the angle of illumination of a light beam shone onto the fundus, and is an angle of illumination defined with respect to the eyeball center O. A correspondence relationship exists between the external illumination angle and the internal illumination angle. For example, an external illumination angle of 120&#xb0; is equivalent to an internal illumination angle of approximately 160&#xb0;. The internal illumination angle in the present exemplary embodiment is 200&#xb0;.</p><p id="p-0052" num="0051">An angle of 200&#xb0; for the internal illumination angle is an example of a &#x201c;specific value&#x201d; of technology disclosed herein.</p><p id="p-0053" num="0052">SLO fundus images obtained by imaging at an imaging angle having an internal illumination angle of 160&#xb0; or greater are referred to as UWF-SLO fundus images (see FIG. <b>7</b>A). UWF is an abbreviation of ultra-wide field. Obviously an SLO image that is not UWF can be acquired by imaging the fundus at an imaging angle that is an internal illumination angle of less than 160&#xb0;.</p><p id="p-0054" num="0053">The UWF-SLO fundus images imaged by the wide-angle optical system of the ophthalmic device <b>110</b> are images resulting from stereographic projection transformation of retinal images of the eyeball onto a two-dimensional flat plane. The fundus peripheral portion suffers from a greater distortion of distances and surface area under stereographic projection transformation than a fundus center portion. Thus there is an issue that as a result of the surface area and shape of structures and pathological lesions present at the fundus peripheral portion and the fundus equatorial portion, such as for example fundus structures of vortex veins and detached retina, appearing distorted, ascertaining the position of these subjects on the eyeball in relation to the periphery is difficult in the UWF-SLO fundus image, which is a two-dimensional fundus image.</p><p id="p-0055" num="0054">Explanation follows regarding a fundus equatorial portion <b>174</b>, with reference to <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>. The eyeball (examined eye <b>12</b>) is a spherical structure with a diameter of approximately 24 mm and an eyeball center <b>170</b>. A straight line joining an anterior pole <b>175</b> to a posterior pole <b>176</b> is referred to as an ocular axis <b>172</b>, a line running along an intersection between a plane orthogonal to the ocular axis <b>172</b> and the eyeball surface is referred to as a line of latitude, and the equator <b>174</b> corresponds to the line of latitude with the greatest length. Portions of the retina and the choroid coinciding with the position of the equator <b>174</b> configure an equatorial portion <b>178</b>. The equatorial portion <b>178</b> corresponds to one part of a fundus peripheral portion.</p><p id="p-0056" num="0055">An SLO system is realized by the control device <b>16</b>, the SLO unit <b>18</b>, and the imaging optical system <b>19</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The SLO system is provided with the wide-angle optical system <b>30</b>, enabling fundus imaging over the wide FOV <b>12</b>A.</p><p id="p-0057" num="0056">The SLO unit <b>18</b> is provided with plural light sources such as, for example, a blue (B) light source <b>40</b>, a green (G) light source <b>42</b>, a red (R) light source <b>44</b>, an infrared (for example near infrared) (IR) light source <b>46</b>, and optical systems <b>48</b>, <b>50</b>, <b>52</b>, <b>54</b>, <b>56</b> to guide the light from the light sources <b>40</b>, <b>42</b>, <b>44</b>, <b>46</b> onto a single optical path using reflection or transmission. The optical systems <b>48</b>, <b>50</b>, <b>56</b> are configured by mirrors, and the optical systems <b>52</b>, <b>54</b> are configured by beam splitters. B light is reflected by the optical system <b>48</b>, is transmitted through the optical system <b>50</b>, and is reflected by the optical system <b>54</b>. G light is reflected by the optical systems <b>50</b>, <b>54</b>, R light is transmitted through the optical systems <b>52</b>, <b>54</b>, and IR light is reflected by the optical systems <b>56</b>, <b>52</b>. The respective lights are thereby guided onto a single optical path.</p><p id="p-0058" num="0057">The SLO unit <b>18</b> is configured so as to be capable of switching between the light source or the combination of light sources employed for emitting laser light of different wavelengths, such as a mode in which G light, R light and B light are emitted, a mode in which infrared light is emitted, etc. Although the example in <figref idref="DRAWINGS">FIG. <b>2</b></figref> includes four light sources, i.e. the B light source <b>40</b>, the G light source <b>42</b>, the R light source <b>44</b>, and the IR light source <b>46</b>, the technology disclosed herein is not limited thereto. For example, the SLO unit <b>18</b> may, furthermore, also include a white light source, in a configuration in which light is emitted in various modes, such as a mode in which white light is emitted alone.</p><p id="p-0059" num="0058">Light introduced to the imaging optical system <b>19</b> from the SLO unit <b>18</b> is scanned in the X direction and the Y direction by the first optical scanner <b>22</b>. The scanning light passes through the wide-angle optical system <b>30</b> and the pupil <b>27</b> and is shone onto the posterior eye portion of the examined eye <b>12</b>. Reflected light that has been reflected by the fundus passes through the wide-angle optical system <b>30</b> and the first optical scanner <b>22</b> and is introduced into the SLO unit <b>18</b>.</p><p id="p-0060" num="0059">The SLO unit <b>18</b> is provided with a beam splitter <b>64</b> that, from out of the light coming from the posterior eye portion (e.g. fundus) of the examined eye <b>12</b>, reflects the B light therein and transmits light other than B light therein, and a beam splitter <b>58</b> that, from out of the light transmitted by the beam splitter <b>64</b>, reflects the G light therein and transmits light other than G light therein. The SLO unit <b>18</b> is further provided with a beam splitter <b>60</b> that, from out of the light transmitted through the beam splitter <b>58</b>, reflects R light therein and transmits light other than R light therein. The SLO unit <b>18</b> is further provided with a beam splitter <b>62</b> that reflects IR light from out of the light transmitted through the beam splitter <b>60</b>.</p><p id="p-0061" num="0060">The SLO unit <b>18</b> is provided with plural light detectors corresponding to the plural light sources. The SLO unit <b>18</b> includes a B light detector <b>70</b> for detecting B light reflected by the beam splitter <b>64</b>, and a G light detector <b>72</b> for detecting G light reflected by the beam splitter <b>58</b>. The SLO unit <b>18</b> includes an R light detector <b>74</b> for detecting R light reflected by the beam splitter <b>60</b> and an IR light detector <b>76</b> for detecting IR light reflected by the beam splitter <b>62</b>.</p><p id="p-0062" num="0061">Light that has passed through the wide-angle optical system <b>30</b> and the first optical scanner <b>22</b> and been introduced into the SLO unit <b>18</b> (i.e. reflected light that has been reflected by the fundus) is reflected by the beam splitter <b>64</b> and photo-detected by the B light detector <b>70</b> when B light, and is transmitted through the beam splitter <b>64</b> and reflected by the beam splitter <b>58</b> and photo-detected by the G light detector <b>72</b> when G light. When R light, the incident light is transmitted through the beam splitters <b>64</b>, <b>58</b>, reflected by the beam splitter <b>60</b>, and photo-detected by the R light detector <b>74</b>. When IR light, the incident light is transmitted through the beam splitters <b>64</b>, <b>58</b>, <b>60</b>, reflected by the beam splitter <b>62</b>, and photo-detected by the IR light detector <b>76</b>. The image processing device <b>16</b>G that operates under the control of the CPU <b>16</b>A employs signals detected by the B light detector <b>70</b>, the G light detector <b>72</b>, the R light detector <b>74</b>, and the IR light detector <b>76</b> to generate UWF-SLO images.</p><p id="p-0063" num="0062">The UWF-SLO image (hereafter sometimes referred to as a UWF fundus image or an original fundus image) encompasses a UWF-SLO image (green fundus image) obtained by imaging the fundus in green, and a UWF-SLO image (red fundus image) obtained by imaging the fundus in red. The UWF-SLO image further encompasses a UWF-SLO image (blue fundus image) obtained by imaging the fundus in blue, and a UWF-SLO image (IR fundus image) obtained by imaging the fundus in IR.</p><p id="p-0064" num="0063">The control device <b>16</b> also controls the light sources <b>40</b>, <b>42</b>, <b>44</b> so as to emit light at the same time. A green fundus image, a red fundus image, and a blue fundus image are obtained with mutually corresponding positions by imaging the fundus of the examined eye <b>12</b> at the same time with the B light, G light, and R light. An RGB color fundus image is obtained from the green fundus image, the red fundus image, and the blue fundus image. The control device <b>16</b> obtains a green fundus image and a red fundus image with mutually corresponding positions by controlling the light sources <b>42</b>, <b>44</b> so as to emit light at the same time and by imaging the fundus of the examined eye <b>12</b> at the same time with the G light and R light. A RG color fundus image is obtained from the green fundus image and the red fundus image.</p><p id="p-0065" num="0064">Specific examples of the UWF-SLO image include a blue fundus image, a green fundus image, a red fundus image, an IR fundus image, an RGB color fundus image, and an RG color fundus image. The image data for the respective UWF-SLO images are transmitted from the ophthalmic device <b>110</b> to the server <b>140</b> through the communication interface (I/F) <b>16</b>F, together with patient information input through the input/display device <b>16</b>E. The respective image data of the UWF-SLO image and the patient information are stored associated with each other in a storage device <b>254</b>. The patient information includes, for example, patient ID, name, age, visual acuity, right eye/left eye discriminator, and the like. The patient information is input by an operator through the input/display device <b>16</b>E.</p><p id="p-0066" num="0065">An OCT system is realized by the control device <b>16</b>, the OCT unit <b>20</b>, and the imaging optical system <b>19</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The OCT system is provided with the wide-angle optical system <b>30</b>. This enables fundus imaging to be performed over the wide FOV <b>12</b>A similarly to when imaging the SLO fundus images as described above. The OCT unit <b>20</b> includes a light source <b>20</b>A, a sensor (detector) <b>20</b>B, a first light coupler <b>20</b>C, a reference optical system <b>20</b>D, a collimator lens <b>20</b>E, and a second light coupler <b>20</b>F.</p><p id="p-0067" num="0066">Light emitted from the light source <b>20</b>A is split by the first light coupler <b>20</b>C. After one part of the split light has been collimated by the collimator lens <b>20</b>E into parallel light, to serve as measurement light, the parallel light is introduced into the imaging optical system <b>19</b>. The measurement light is scanned in the X direction and the Y direction by the second optical scanner <b>24</b>. The scanning light is shone onto the fundus through the wide-angle optical system <b>30</b> and the pupil <b>27</b>. Measurement light that has been reflected by the fundus passes through the wide-angle optical system <b>30</b> and the second optical scanner <b>24</b> so as to be introduced into the OCT unit <b>20</b>. The measurement light then passes through the collimator lens <b>20</b>E and the first light coupler <b>20</b>C before being incident to the second light coupler <b>20</b>F.</p><p id="p-0068" num="0067">The other part of the light emitted from the light source <b>20</b>A and split by the first light coupler <b>20</b>C is introduced into the reference optical system <b>20</b>D as reference light, and is made incident to the second light coupler <b>20</b>F through the reference optical system <b>20</b>D.</p><p id="p-0069" num="0068">The respective lights that are incident to the second light coupler <b>20</b>F, namely the measurement light reflected by the fundus and the reference light, interfere with each other in the second light coupler <b>20</b>F so as to generate interference light. The interference light is photo-detected by the sensor <b>20</b>B. The image processing device <b>16</b>G operating under the control of the CPU <b>16</b>A generates OCT images, such as tomographic images and en-face images, based on OCT data detected by the sensor <b>20</b>B.</p><p id="p-0070" num="0069">OCT fundus images obtained by imaging at an imaging angle having an internal illumination angle of 160&#xb0; or greater are referred to as UWF-OCT images. Obviously OCT data can be acquired at an imaging angle having an internal illumination angle of less than 160&#xb0;.</p><p id="p-0071" num="0070">The image data of the UWF-OCT images is transmitted, together with the patient information, from the ophthalmic device <b>110</b> to the server <b>140</b> though the communication interface (I/F) <b>16</b>F. The image data of the UWF-OCT images and the patient information are stored associated with each other in the storage device <b>254</b>.</p><p id="p-0072" num="0071">Note that although in the present exemplary embodiment an example is given in which the light source <b>20</b>A is a swept-source OCT (SS-OCT), the light source <b>20</b>A may be from various types of OCT system, such as a spectral-domain OCT (SD-OCT) or a time-domain OCT (TD-OCT) system.</p><p id="p-0073" num="0072">Next, explanation follows regarding the eye axial length measurement device <b>120</b>. The eye axial length measurement device <b>120</b> has two modes, i.e. a first mode and a second mode, for measuring eye axial length, this being the length of the examined eye <b>12</b> in an eye axial direction. In the first mode light from a non-illustrated light source is guided into the examined eye <b>12</b>. Interference light between light reflected from the fundus and light reflected from the cornea is photo-detected, and the eye axial length is measured based on an interference signal representing the photo-detected interference light. The second mode is a mode to measure the eye axial length by employing non-illustrated ultrasound waves.</p><p id="p-0074" num="0073">The eye axial length measurement device <b>120</b> transmits the eye axial length as measured using either the first mode or the second mode to the server <b>140</b>. The eye axial length may be measured using both the first mode and the second mode, and in such cases, an average of the eye axial lengths as measured using the two modes is transmitted to the server <b>140</b> as the eye axial length. The server <b>140</b> stores the eye axial length of the patient in association with the patient ID.</p><p id="p-0075" num="0074">Explanation follows regarding a configuration of an electrical system of the server <b>140</b>, with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. As illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the server <b>140</b> is provided with a computer body <b>252</b>. The computer body <b>252</b> includes a CPU <b>262</b>, RAM <b>266</b>, ROM <b>264</b>, and an input/output (I/O) port <b>268</b> connected together by a bus <b>270</b>. The storage device <b>254</b>, a display <b>256</b>, a mouse <b>255</b>M, a keyboard <b>255</b>K, and a communication interface (I/F) <b>258</b> are connected to the input/output (I/O) port <b>268</b>. The storage device <b>254</b> is, for example, configured by non-volatile memory. The input/output (I/O) port <b>268</b> is connected to the network <b>130</b> through the communication interface (I/F) <b>258</b>. The server <b>140</b> is thus capable of communicating with the ophthalmic device <b>110</b> and the viewer <b>150</b>. The storage device <b>254</b> is stored with an image processing program, described later. Note that the image processing program may be stored in the ROM <b>264</b>.</p><p id="p-0076" num="0075">The image processing program is an example of a &#x201c;program&#x201d; of technology disclosed herein. The storage device <b>254</b> and the ROM <b>264</b> are examples of &#x201c;memory&#x201d; and a &#x201c;computer readable storage medium&#x201d; of technology disclosed herein. The CPU <b>262</b> is an example of a &#x201c;processor&#x201d; of technology disclosed herein.</p><p id="p-0077" num="0076">A processing section <b>208</b>, described later, of the server <b>140</b> (see also <figref idref="DRAWINGS">FIG. <b>5</b></figref>) stores various data received from the ophthalmic device <b>110</b> in the storage device <b>254</b>. More specifically, the processing section <b>208</b> stores respective image data of the UWF-SLO images and image data of the UWF-OCT images in the storage device <b>254</b> associated with the patient information (such as the patient ID as described above). Moreover, in cases in which there is a pathological change in the examined eye of the patient and cases in which surgery has been performed on a pathological lesion, pathology information is input through the input/display device <b>16</b>E of the ophthalmic device <b>110</b>, transmitted to the server <b>140</b>, and stored in the storage device <b>254</b>. The pathology information is stored in the storage device <b>254</b> associated with the patient information. The pathology information includes information about the position of the pathological lesion, name of the pathological change, and type of surgery and date/time of surgery etc. when surgery was performed on the pathological lesion.</p><p id="p-0078" num="0077">The viewer <b>150</b> is provided with a computer equipped with a CPU, RAM, ROM and the like, and a display. The image processing program is installed in the ROM, and based on an instruction from a user the computer controls the display so as to display the medical information such as fundus images acquired from the server <b>140</b>.</p><p id="p-0079" num="0078">Explanation follows regarding various functions implemented by the CPU <b>262</b> of the server <b>140</b> of the first exemplary embodiment executing the image processing program, with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The image processing program includes an image acquisition function, image processing function (3D video generation function, 3D eyeball image generation function, save function, and feature portion acquisition function), display control function, and output function. By the CPU <b>262</b> executing the image processing program including these functions, the CPU <b>262</b> functions as an image acquisition section <b>1410</b>, an image processing section <b>1420</b> (a 3D video generation section <b>1421</b>, a 3D eyeball image generation section <b>1422</b>, a save section <b>1423</b>, and a feature portion acquisition section <b>1424</b>), a display control section <b>1430</b>, and an output section <b>1440</b>, as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0080" num="0079">Next detailed description follows regarding image processing by the server <b>140</b>, with reference to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The image processing illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is implemented by the CPU <b>262</b> of the server <b>140</b> executing the image processing program. This image processing is started when a UWF fundus image (UWF-SLO image) is acquired by the ophthalmic device <b>110</b> and transmitted together with the patient ID to the server <b>140</b>, and the server <b>140</b> has received the patient ID and the UWF fundus image.</p><p id="p-0081" num="0080">At step <b>502</b> the image acquisition section <b>1410</b> acquires a UWF fundus image G<b>1</b> corresponding to the patient ID from the storage device <b>254</b>. The UWF fundus image G<b>1</b> is, for example, a RGB color fundus image (see <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>).</p><p id="p-0082" num="0081">At step <b>504</b> the image processing section <b>1420</b> executes 3D rendering processing (see also <figref idref="DRAWINGS">FIG. <b>6</b></figref>), described in detail later, on the UWF fundus image G<b>1</b>.</p><p id="p-0083" num="0082">At step <b>506</b> the display control section <b>1430</b> generates a display screen (see also <figref idref="DRAWINGS">FIG. <b>12</b></figref>, <figref idref="DRAWINGS">FIG. <b>14</b></figref>, and <figref idref="DRAWINGS">FIG. <b>15</b></figref>), described in detail later.</p><p id="p-0084" num="0083">Next, description follows regarding the 3D rendering processing of step <b>504</b>, with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0085" num="0084">At step <b>602</b> the 3D video generation section <b>1421</b> acquires respective points on an eyeball model M corresponding to each of the points of the UWF fundus image G<b>1</b>.</p><p id="p-0086" num="0085">More specifically, the 3D video generation section <b>1421</b> generates video data configured from plural frames to represent a process of projecting the UWF fundus image G<b>1</b> onto the eyeball model M, as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>. A method of projecting a spherical surface onto a flat plane is stereographic projection, however in the present exemplary embodiment the 3D video generation section <b>1421</b> generates video data to represent a process of reverse stereographic projection from a flat plane onto a spherical surface.</p><p id="p-0087" num="0086">More specifically, the 3D video generation section <b>1421</b> first performs positional alignment between the UWF fundus image G<b>1</b> and the eyeball model M. Specifically the 3D video generation section <b>1421</b> arranges such that a projection center of the UWF fundus image G<b>1</b> is aligned with a projection center of the eyeball model M, and a projection plane from the center of the eyeball model M is horizontal. The XY directions taking a center gc of the UWF fundus image G<b>1</b> as the center are aligned with the XY directions taking a pupil center me of the eyeball model M as the center.</p><p id="p-0088" num="0087">After positional alignment between the eyeball model M and the UWF fundus image G<b>1</b> is finished, the 3D video generation section <b>1421</b> sets zero for a variable g related to a time step to identify each of the points of the UWF fundus image G<b>1</b>, increments the variable g by one, and transforms a point on the UWF fundus image G<b>1</b> as identified by the variable g into a point on the eyeball model M according to the following transformation equation. A point on the eyeball model M is accordingly acquired that corresponds to the point on the UWF fundus image G<b>1</b> identified by the variable g. In the present Example the UWF fundus image G<b>1</b> is arranged in a flat plane of Z=&#x2212;1. The following transformation equation is employed, wherein points on the UWF fundus image G<b>1</b> are denoted by (Xg, Yg, &#x2212;1), points on the eyeball model M corresponding to the UWF fundus image G<b>1</b> points (Xg, Yg, &#x2212;1) are denoted by (Xmg, Ymg, Zmg).</p><p id="p-0089" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Xmg=</i>4<i>Xg</i>/(4+<i>Xg</i><sup>2</sup><i>+Yg</i><sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0090" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Ymg=</i>4<i>Yg</i>/(4+<i>Xg</i><sup>2</sup><i>+Yg</i><sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0091" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>Zmg</i>=(&#x2212;4+<i>Xg</i><sup>2</sup><i>+Yg</i><sup>2</sup>)/(4+<i>Xg</i><sup>2</sup><i>+Yg</i><sup>2</sup>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0092" num="0088">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a point g<b>1</b> (X1, Y1, &#x2212;1) of the UWF fundus image G<b>1</b> identified by variable g=1 is transformed into a point m<b>1</b> (Xm<b>1</b>, Ym<b>1</b>, Zm<b>1</b>) on the eyeball model M. The above transformation is executed for all of the points on the UWF fundus image G<b>1</b>. The point g<b>1</b> is set for each of the pixels of the UWF fundus image G<b>1</b>. The above transformation is executed for all of the pixels of the UWF fundus image G<b>1</b>.</p><p id="p-0093" num="0089">Note that instead of the above transformation being performed for each point (pixel) of the UWF fundus image G<b>1</b>, a user may perform the above transformation on a region selected by the user to include a pathological change of the UWF fundus image G<b>1</b>, such as a detached retina, or a region including a vortex vein.</p><p id="p-0094" num="0090">A default eyeball model may be employed as the eyeball model employed at step <b>602</b>. However, in the present exemplary embodiment the 3D video generation section <b>1421</b> reads the eye axial length corresponding to the patient ID from the storage device <b>254</b>, and pre-corrects the size of the default eyeball model according to the read eye axial length.</p><p id="p-0095" num="0091">Moreover, in the present exemplary embodiment the 3D video generation section <b>1421</b> uses the transformation equation to acquire the points on the eyeball model M corresponding to each of the points of the UWF fundus image G<b>1</b>. The technology disclosed herein is not limited thereto. A transformation lookup table may be prepared in advance for the eyeball model, and the 3D video generation section <b>1421</b> may acquire points on the eyeball model M corresponding to each of the points of the UWF fundus image G<b>1</b> by reference to this table.</p><p id="p-0096" num="0092">At step <b>604</b> the 3D video generation section <b>1421</b> acquires a trace for a pixel at each of the points on the UWF fundus image to move to each of the corresponding points on the eyeball model.</p><p id="p-0097" num="0093">More specifically, the 3D video generation section <b>1421</b> sets variable g to zero, increments variable g by one, and sets as a path a line connecting the point on the UWF fundus image G<b>1</b> identified by the variable g to the point on the eyeball model M corresponding to this point. The 3D video generation section <b>1421</b> computes a position at each specific time of a pixel of the point on the UWF fundus image G<b>1</b> identified by the variable g. Note that the specific time is a time according to a frame rate of video data as described later.</p><p id="p-0098" num="0094">For a pattern of the above movement there are plural patterns differing by at least one out of path of movement of the pixel at each point on the UWF fundus image G<b>1</b> or movement speed.</p><p id="p-0099" num="0095">The path referred to above may be a straight line or a curve, however a straight line is employed in the present exemplary embodiment. Note that a user may be given a selection from out of a straight line or a curve (for example, plural upward convex curves of different curvature, plural downward convex curves of different curvature).</p><p id="p-0100" num="0096">Moreover, the speed may be fixed or varied during movement. In the present exemplary embodiment the speed is fixed at a predetermined speed V<b>0</b> during movement. For the speed too, an operator may be given a selection from out of a constant speed (plural speeds), plural patterns gradually getting faster, plural patterns gradually getting slower, and the like.</p><p id="p-0101" num="0097">More specifically, for example, for variable g=1, the 3D video generation section <b>1421</b> may transform a point g<b>1</b> on the UWF fundus image G<b>1</b> identified by variable g=1 to a point mg<b>1</b> in an XYZ coordinate system of the eyeball model M, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, by transforming point g<b>1</b> coordinate (X1, Y1) to point mg<b>1</b> coordinate (X1, Y1, &#x2212;1). Generally in a stereographic projection transformation the UWF fundus image G<b>1</b> is arranged at Z=0, however in this case the UWF fundus image G<b>1</b> is intentionally arranged at Z=&#x2212;1 to generate a transformation video while leaving the UWF fundus image G<b>1</b> untouched, as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0102" num="0098">The 3D video generation section <b>1421</b> sets a line segment L<b>1</b> for point mg<b>1</b> (X1, Y1, &#x2212;1) and point m<b>1</b> (Xm<b>1</b>, Ym<b>1</b>, Zm<b>1</b>) in the XYZ coordinate system.</p><p id="p-0103" num="0099">As described later, in the present exemplary embodiment video data is created to represent a process to move the pixel of point mg<b>1</b> (X1, Y1, &#x2212;1) to point m<b>1</b> (Xm<b>1</b>, Ym<b>1</b>, Zm<b>1</b>). When the video data is replayed, the pixel at point mg<b>1</b> (X1, Y1, &#x2212;1) is displayed as moving along the line segment L<b>1</b> to the point m<b>1</b> (Xm<b>1</b>, Ym<b>1</b>, Zm<b>1</b>). Thus the line segment L<b>1</b> is a trace of movement of the pixel of point mg<b>1</b> (X1, Y1, &#x2212;1). Note that the line segment L<b>1</b> is also called path L<b>1</b>.</p><p id="p-0104" num="0100">In the present exemplary embodiment, a pattern of constant speed straight line movement is predetermined for pixels at all of the points on the UWF fundus image G<b>1</b> to move along the corresponding paths to respective points on the eyeball model M corresponding to each pixel point. This pattern is, for example, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, moving a pixel at point mg<b>1</b> (X1, Y1, 0) at the constant speed V<b>0</b> along path L<b>1</b> to the points md, . . . , mh, . . . mm, . . . m<b>1</b>. The 3D video generation section <b>1421</b> computes the position (XYZ coordinate system) at each specific time of the pixel at point mg<b>1</b> (X1, Y1, &#x2212;1) from the path L<b>1</b> and the speed V<b>0</b>.</p><p id="p-0105" num="0101">The 3D video generation section <b>1421</b> executes the above processing until variable g reaches the total number of points on the UWF fundus image G<b>1</b>. Positions along a movement path are thereby acquired for pixels at all points on the UWF fundus image G<b>1</b>.</p><p id="p-0106" num="0102">Note that the movement pattern may be different for every pixel at each point of the UWF fundus image G<b>1</b>. For example, a pixel at a given point on the UWF fundus image G<b>1</b> may have a movement of a constant speed in a straight line, whereas a pixel at another point may move along an upward convex curved path at a gradually slowing speed.</p><p id="p-0107" num="0103">At step <b>606</b> the 3D video generation section <b>1421</b> generates video data to represent a process to move pixels at all points on the UWF fundus image G<b>1</b> to the respective points on the eyeball model M corresponding to these pixel points.</p><p id="p-0108" num="0104">Specifically, first using each pixel of the UWF fundus image G<b>1</b> as a data point, an initial frame is generated with these data points arranged in an XY plane Z=&#x2212;1 in a three-dimensional coordinate system (processing corresponding to f<b>1</b> in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, wherein <figref idref="DRAWINGS">FIG. <b>11</b></figref> is a diagram to explain plural frames of video data).</p><p id="p-0109" num="0105">Next these data points are arranged at a positions after a specific time (&#x394;t) has elapsed from the trace data set at step <b>604</b>. A flat plane (curved plane) is defined by arranging these data points for all of the data points of the UWF fundus image G<b>1</b>. Pixel data (color and brightness data) is input for data points in this flat plane, and intermediate frames are generated for after specific times (&#x394;t) have elapsed (corresponding to fd, fh, fm of <figref idref="DRAWINGS">FIG. <b>11</b></figref>).</p><p id="p-0110" num="0106">The generation of intermediate frames continues and frame creation is repeated in a similar manner from specific times &#x394;t<b>1</b> to &#x394;tn (wherein n is a frame number for generating frames up to a final position n). fn is the final frame in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, and is a frame of a state in which all the data points have moved to their respective points on the eyeball model M.</p><p id="p-0111" num="0107">After completing frames <b>1</b> to n, frame numbers, a video file number, and the like are appended, and video data is generated.</p><p id="p-0112" num="0108">A linear interpolation, spline interpolation, or the like may be employed as the method of computing a flat plane. A surface model, a spatial lattice model, or the like may be employed as a flat plane geometric model. Without computing a flat plane, the data points may be given a size and displayed as a scatter diagram.</p><p id="p-0113" num="0109">In the present exemplary embodiment as described above, the pixel at all points on the UWF fundus image G<b>1</b> are moved along path L at constant speed V<b>0</b> in a predetermined constant speed straight line movement pattern to the respective points on the eyeball model M corresponding to these pixel points. Thus at step <b>606</b>, as video data, plural frames (still images) are each generated for each of the specific times to represent the constant speed straight line movement of the pixels for all points on the UWF fundus image G<b>1</b> to the points on the eyeball model M corresponding to these pixel points.</p><p id="p-0114" num="0110">For example, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a first frame F<b>1</b> matches the UWF fundus image G<b>1</b>. The 3D video generation section <b>1421</b> generates frames for specific times afterwards by arranging each of the corresponding pixels at its position after the calculated movement. By repeating the above processing, the 3D video generation section <b>1421</b> generates frames fd, . . . , fh, . . . , fm, . . . fr at each of the specific times until the pixels at all points on the UWF fundus image G<b>1</b> have moved to the points on the eyeball model M corresponding to these pixel points, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0115" num="0111">At step <b>608</b> the 3D eyeball image generation section <b>1422</b> generates 3D eyeball image data.</p><p id="p-0116" num="0112">3D fundus image data is image data of an eyeball image obtained by transforming the UWF fundus image G<b>1</b> so as to be stuck onto a specific eyeball model, and is obtained by combining an anterior eye portion image, such as of the iris, lens body, and the like, with the transformed UWF fundus image G<b>1</b>. The 3D eyeball image generation section <b>1422</b> may take the image after the pixels at all points on the UWF fundus image G<b>1</b> have been moved to the points on the eyeball model M corresponding to these pixel points (the final frame of the video) as the 3D eyeball image.</p><p id="p-0117" num="0113">In separate processing to the image processing of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the feature portion acquisition section <b>1424</b> acquires information about a feature portion. Examples of a feature portion include, as fundus structures, blood vessels (for example, retinal blood vessels, choroidal blood vessels, etc.), an optic nerve head, a macular, a vortex vein, or the like. The 3D eyeball image generation section <b>1422</b> generates 3D fundus image data by taking an image after the pixels at all points on the UWF fundus image G<b>1</b> have been moved to the points on the eyeball model M corresponding to these pixel points, and emphasizing an image of feature portions acquired by the feature portion acquisition section <b>1424</b>. The feature portion may also encompass a pathological lesion. Pathological lesions are identified by examination by an ophthalmologist. For example, a mark indicating a region including a detached retina is displayed overlaid on the UWF fundus image G<b>1</b>.</p><p id="p-0118" num="0114">Coordinate data on the UWF fundus image G<b>1</b> is taken for the positions and regions of such feature objects, and names of pathological changes and names of feature objects are combined with the coordinate data and saved in the storage device <b>254</b>.</p><p id="p-0119" num="0115">When creating the video data, video data may be created at step <b>606</b> from the UWF fundus image G<b>1</b> marked with the positions of feature objects and pathological changes.</p><p id="p-0120" num="0116">At step <b>610</b>, the save section <b>1423</b> saves (stores) each data type (video data and 3D fundus image data) in the storage device <b>2554</b>.</p><p id="p-0121" num="0117">After the saving processing at step <b>610</b> has finished for each data type, the 3D rendering processing of step <b>504</b> of <figref idref="DRAWINGS">FIG. <b>5</b></figref> is ended, and image processing proceeds to step <b>506</b> where the display control section <b>1430</b> generates a display screen (see also <figref idref="DRAWINGS">FIG. <b>12</b></figref>, <figref idref="DRAWINGS">FIG. <b>14</b></figref>, and <figref idref="DRAWINGS">FIG. <b>15</b></figref>), and the image processing is then ended.</p><p id="p-0122" num="0118">A user (such as an ophthalmologist) inputs the patient ID into the viewer <b>150</b> when examining the examined eye of the patient. The viewer <b>150</b> instructs the server <b>140</b> to transmit examined eye image data corresponding to the patient ID, and so on. The output section <b>1440</b> of the server <b>140</b> transmits the patient name, patient age, patient visual acuity, left eye/right eye information, eye axial length, imaging date, image data, and so on corresponding to the patient ID to the viewer <b>150</b> together with the patient ID.</p><p id="p-0123" num="0119">The image data etc. includes UWF fundus images, video data, 3D fundus image data, and information about feature portions.</p><p id="p-0124" num="0120">On receiving the patient ID, patient name, patient age, patient visual acuity, left eye/right eye information, eye axial length, imaging date, and image data, the viewer <b>150</b> displays a display screen <b>400</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> on the display.</p><p id="p-0125" num="0121">As illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the display screen <b>400</b>A includes an information display area <b>402</b> and an image display area <b>404</b>A.</p><p id="p-0126" num="0122">The information display area <b>402</b> includes a patient ID display field <b>412</b>, a patient name display field <b>414</b>, an age display field <b>416</b>, a left eye/right eye display field <b>418</b>, an eye axial length display field <b>420</b>, a visual acuity display field <b>422</b>, and an imaging date and time display field <b>424</b>. The viewer <b>150</b> displays various information in the respective display fields from the patient ID display field <b>412</b> to the imaging date and time display field <b>424</b> based on the received information.</p><p id="p-0127" num="0123">The image display area <b>404</b>A includes an SLO image display field <b>452</b>A and a video data display field <b>454</b>A<b>1</b>. The video data display field <b>454</b>A<b>1</b> includes a video data display section <b>455</b>A, a replay button <b>456</b>, and a display progress display section <b>458</b>.</p><p id="p-0128" num="0124">The UWF fundus image is displayed in the SLO image display field <b>452</b>A. The video data is displayed on the video data display section <b>455</b>A when the replay button <b>456</b> is operated, and a portion corresponding to the proportion already displayed from out of the entire video data is infilled in a specific color in the display progress display section <b>458</b>.</p><p id="p-0129" num="0125">Explanation follows regarding replay of the video data on the video data display section <b>455</b>A. The UWF fundus image G<b>1</b> such as illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> and <figref idref="DRAWINGS">FIG. <b>13</b></figref> is displayed prior to operating the replay button <b>456</b>.</p><p id="p-0130" num="0126">When the replay button <b>456</b> is operated, first a face-on view of the UWF fundus image G<b>1</b> is displayed (picture a), then an image Gd of the UWF fundus image G<b>1</b> tilted by a specific angle is displayed (picture b), then a perspective view image Gh of the UWF fundus image G<b>1</b> further tilted is displayed (picture c), as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. The process during which the picture a, picture b, and the picture c are displayed is a first process to transform a face-on view of a two-dimensional fundus image into a perspective view.</p><p id="p-0131" num="0127">Then while the image Gh is left as is, a picture d is displayed in which a three-dimensional fundus image fh exhibiting an initial state of reverse stereographic projection transformation is displayed overlaid on the image Gh. Pictures are displayed to represent the manner of reverse stereographic projection transformation, more specifically the first frame f<b>1</b>, . . . , frame fh, . . . , frame fr are displayed. Then a picture e is displayed, in which a three-dimensional image fr exhibiting the completed state of reverse stereographic projection transformation is displayed overlaid on the image Gh. The process during which picture d to picture e are displayed is a second process a three-dimensional fundus image is displayed changing as it is subjected to reverse stereographic projection transformation on a two-dimensional fundus image in horizontal view.</p><p id="p-0132" num="0128">Finally, the picture e is rotated, and an image Gz is displayed with a face-on view of the three-dimensional fundus image fr overlaid on the face-on view of the UWF fundus image G<b>1</b> (picture f). The process of displaying from picture e to picture f is a third process of rotating an integrated image of the horizontal view of the two-dimensional fundus image together with a three-dimensional fundus image after transformation completion.</p><p id="p-0133" num="0129">Note that the video data may, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, be a video merely of the process of reverse stereographic projection transformation on the UWF fundus image G<b>1</b> (i.e. not a video in which the image Gh remains as is as in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, but a video in which the image Gh does not remain).</p><p id="p-0134" num="0130">In cases in which the video data is displayed, the paths of the points are displayed from predetermined points, which in the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> are four pre-selected points outermost in the upward and downward directions (L<b>1</b>, L<b>2</b>, L<b>3</b>, L<b>4</b> in the diagram). Moreover, the paths of positions of the above mentioned feature portions, such as vortex veins, detached retina, or the like, may be displayed.</p><p id="p-0135" num="0131">When the video data is displayed in the video data display section <b>455</b>A and the image Gz is displayed, as illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, a 3D eyeball image switch button <b>460</b> is displayed in the video data display field <b>454</b>A<b>1</b>.</p><p id="p-0136" num="0132">When the 3D eyeball image switch button <b>460</b> is operated, as illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, the video data display field <b>454</b>A<b>1</b> is switched to a 3D fundus image display field <b>454</b>A<b>2</b>. The 3D eyeball image is displayed in the 3D fundus image display field <b>454</b>A<b>2</b>. As described above, the image data is of an image of an eyeball obtained by transforming the UWF fundus image G<b>1</b> so as to be stuck onto a specific eyeball model, and by combining an anterior eye portion image, such as of the iris, lens body, and the like, with the transformed UWF fundus image G<b>1</b>.</p><p id="p-0137" num="0133">A 3D video switch button <b>462</b> is displayed in the 3D fundus image display field <b>454</b>A<b>2</b>. A switch is made to the display screen <b>400</b>A of <figref idref="DRAWINGS">FIG. <b>12</b></figref> when the 3D video switch button <b>462</b> is operated.</p><p id="p-0138" num="0134">As explained above, the first exemplary embodiment creates video data to represent a process to move pixels at each point of the UWF fundus image to respective corresponding points on the eyeball model. This enables a user to know the manner in which each point on the UWF fundus image moves to each of the corresponding points on the eyeball model. A user is accordingly able to know which point of the corresponding points on the eyeball model is which point on the UWF fundus image.</p><p id="p-0139" num="0135">Thus by a user (ophthalmologist) viewing the video data generated from the UWF fundus image, the user is able to directly ascertain a position and size of a pathological change (surface area of a pathological lesion) on the eyeball at fundus peripheral portions and fundus equatorial portions. Moreover, the ophthalmologist is able to easily explain the examination results, such as the position and size of a pathological change, to the examinee (patient).</p><p id="p-0140" num="0136">Moreover, the first exemplary embodiment illustrates a case in which a video is generated of a process of transforming a two-dimensional UWF fundus image into a three-dimensional fundus image based on an eyeball model. However, there is no limitation thereto, and a video may be generated of a process of transforming an image of an anterior eye portion imaged in two dimensions into a three-dimensional anterior eye portion image based on an eyeball model. For example, a video may be generated for transformation of a two-dimensional flat plane image of a lens body based on a three-dimensional lens body model. This can be used to easily ascertain sites of the lens body such as the pigmentation and the like in a three-dimensional manner, and can be used for planning of a procedure for glaucoma surgery.</p><heading id="h-0006" level="1">Second Exemplary Embodiment</heading><p id="p-0141" num="0137">Next, description follows regarding a second exemplary embodiment of the technology disclosed herein. The configuration of the second exemplary embodiment is substantially similar to the configuration of the first exemplary embodiment, and so only differing parts (a fixation function of the ophthalmic device <b>110</b> and montage image generation of the server <b>140</b>) will be described.</p><p id="p-0142" num="0138">First explanation follows regarding the fixation function of the ophthalmic device <b>110</b> of the second exemplary embodiment.</p><p id="p-0143" num="0139">The imaging device <b>14</b> of the ophthalmic device <b>110</b> includes a fixation target control device to illuminate a non-illustrated upper fixation lamp and lower fixation lamp (further including a non-illustrated central fixation lamp) under control of the control device <b>16</b>. The orientation (gaze direction) of the examined eye <b>12</b> can be changed by illuminating one or other out of the central fixation lamp, the upper fixation lamp, or the lower fixation lamp.</p><p id="p-0144" num="0140">Next, description follows regarding a montage image generation function implemented by the CPU <b>262</b> of the server <b>140</b> of the second exemplary embodiment executing an image processing program.</p><p id="p-0145" num="0141"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram illustrating various functions implemented by the CPU <b>262</b> of the server <b>140</b> of the second exemplary embodiment executing the image processing program. The image processing program of the second exemplary embodiment has, in comparison to the first exemplary embodiment, a montage image generation function added to the image processing function. Thus by the CPU <b>262</b> executing the image processing program including these functions, the CPU <b>262</b> further functions as a montage image generation section <b>1425</b>.</p><p id="p-0146" num="0142">The ophthalmic device <b>110</b> controls the fixation target control device to illuminate the upper fixation lamp in order to shift the gaze of the patient to face diagonally upward. The gaze of the patient is thereby facing diagonally upward, namely, facing in a direction from the center of the eyeball toward the upper fixation lamp. Rather than by illuminating the upper fixation lamp, the gaze of the examined eye may also be placed in a diagonally upward facing state by the operator of the ophthalmic device <b>110</b> giving an instruction for the patient to gaze diagonally upward, such as &#x201c;please look up&#x201d;.</p><p id="p-0147" num="0143">The ophthalmic device <b>110</b> images the fundus in the upward looking state in which the gaze of the patient is facing diagonally upward. An UWF upward looking fundus image GU is obtained (see also <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>).</p><p id="p-0148" num="0144">The ophthalmic device <b>110</b> controls the fixation target control device to illuminate the lower fixation lamp in order to shift the gaze of the patient to face diagonally downward. The gaze of the patient is thereby facing diagonally downward, namely, facing in a direction from the center of the eyeball toward the lower fixation lamp. Rather than by illuminating the upper fixation lamp, the gaze of the examined eye may also be placed in a diagonally downward facing state by the operator of the ophthalmic device <b>110</b> giving an instruction for the patient to gaze diagonally downward, such as &#x201c;please look down&#x201d;.</p><p id="p-0149" num="0145">The ophthalmic device <b>110</b> images the fundus in the downward looking state in which the gaze of the patient is facing diagonally downward. An UWF downward looking fundus image GD is obtained thereby. Note that <figref idref="DRAWINGS">FIG. <b>19</b>B</figref> illustrates a UWF downward looking fundus image GDC that has been positionally aligned, described later.</p><p id="p-0150" num="0146">Examples of the UWF upward looking fundus image GU include, similarly to the first exemplary embodiment, a blue fundus image (UWF upward looking fundus image B), a green fundus image (UWF upward looking fundus image G), a red fundus image (UWF upward looking fundus image R), an IR fundus image (UWF upward looking fundus image IR), an RGB color fundus image (UWF upward looking fundus image RGB), and an RG color fundus image (UWF upward looking fundus image RG).</p><p id="p-0151" num="0147">Similarly with the UWF downward looking fundus image GD, there is a blue fundus image (UWF downward looking fundus image B), a green fundus image (UWF downward looking fundus image G), a red fundus image (UWF downward looking fundus image R), an IR fundus image (UWF downward looking fundus image IR), an RGB color fundus image (UWF downward looking fundus image RGB), and an RG color fundus image (UWF downward looking fundus image RG).</p><p id="p-0152" num="0148">The UWF upward looking fundus image GU and the UWF downward looking fundus image GD are transmitted to the server <b>140</b> by the ophthalmic device <b>110</b> and stored in the storage device <b>254</b>.</p><p id="p-0153" num="0149"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates a flowchart of image processing of the second exemplary embodiment. The image processing of the second exemplary embodiment is started when the server <b>140</b> has received the UWF upward looking fundus image GU and the UWF downward looking fundus image GD.</p><p id="p-0154" num="0150">At step <b>320</b> the image acquisition section <b>1410</b> acquires the UWF upward looking fundus image and the UWF downward looking fundus image from the storage device <b>254</b>. At step <b>322</b> the montage image generation section <b>1425</b> performs processing to emphasize blood vessels in the UWF upward looking fundus image and in the UWF downward looking fundus image. Then binarization processing is executed so as to binarize with respect to a specific threshold. The blood vessels of the fundus are emphasized in white by this binarization processing.</p><p id="p-0155" num="0151">At step <b>324</b> the montage image generation section <b>1425</b> performs positional alignment of the UWF upward looking fundus image and the UWF downward looking fundus image. Explanation follows regarding the positional alignment processing of step <b>324</b>, with reference to the flowchart of <figref idref="DRAWINGS">FIG. <b>18</b></figref>. Explanation follows taking an example of a case in which the UWF downward looking fundus image is transformed while the UWF upward looking fundus image is used for reference (namely, a case in which the UWF upward looking fundus image is not transformed and transformation is only performed on the UWF downward looking fundus image).</p><p id="p-0156" num="0152">At step <b>340</b> of <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the montage image generation section <b>1425</b> extracts a feature point group <b>1</b> to perform the above positional alignment by performing image processing on the UWF upward looking fundus image GU.</p><p id="p-0157" num="0153">The feature point group <b>1</b> is configured by plural feature points on the fundus image, and as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b>A</figref> includes an optic nerve head ONHU, a macular MU, and branch points of the retinal blood vessels. Note that branch points of the choroidal blood vessels may also be extracted as feature points.</p><p id="p-0158" num="0154">The feature points are pixels of maximum brightness of an optic nerve head ONHU region, pixels of minimum brightness of a macular MU region, pixels at positions of branch points of the retinal blood vessels and the choroidal blood vessels, and the coordinates of these pixels are extracted as feature point data. Alternatively, not only branch points in the retinal blood vessels and the choroidal blood vessels, but regions including a characteristic blood vessel running pattern may be extracted, and center points of the regions including these patterns taken as feature points.</p><p id="p-0159" num="0155">Note that terminal points, curve points, or meander points of the retinal blood vessels and the choroidal blood vessels may be extracted as feature points.</p><p id="p-0160" num="0156">The feature points may use a scale intevariant feature transform (SIFT), a speed upped robust feature (SURF), or the like as an algorithm to perform feature point detection.</p><p id="p-0161" num="0157">In order to perform the positional alignment with high accuracy, preferably there is a plural number of four or more of the extracted feature points. There is only one of the optic nerve head and the macular present in the UWF upward looking fundus image GU for the examined eye. Thus four or more of the feature points <b>1</b> can be extracted from the UWF upward looking fundus image GU by extracting branch points of the retinal blood vessels or choroidal blood vessels merely at two or more locations.</p><p id="p-0162" num="0158">The optic nerve head, macular, retinal blood vessels, and choroidal blood vessels present at a fundus central portion are preferably selection targets for feature points to use in the positional alignment since they are imaged in both the UWF upward looking fundus image GU and the UWF downward looking fundus image GD. Namely, the feature points are preferably selected from a fundus central portion that is a common region to both the UWF upward looking fundus image GU and the UWF downward looking fundus image GD.</p><p id="p-0163" num="0159">Thus at step <b>340</b>, the montage image generation section <b>1425</b> extracts the feature point group <b>1</b> by performing image processing on the fundus central portion that is a region on the lower side of the center of the UWF upward looking fundus image GU.</p><p id="p-0164" num="0160">A vortex vein present at a fundus peripheral portion in the UWF upward looking fundus image GU is excluded from being a target for selection of feature points used in the positional alignment described above. The fundus peripheral portion is not a common region to the WF upward looking fundus image GU and the UWF downward looking fundus image GD, and so structures in the fundus peripheral portion are excluded from being a target for selection of feature points.</p><p id="p-0165" num="0161">At step <b>342</b> the montage image generation section <b>1425</b> extracts a feature point group <b>2</b> from the UWF downward looking fundus image GD corresponding to the feature point group <b>1</b>. The feature point group <b>2</b> is, as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>, configured by the optic nerve head ONHD, the macular MD, and branch points in the retinal blood vessels. Due to being in the same eye, the optic nerve head ONHD corresponds to the optic nerve head ONHU, and the macular MD corresponds to the macular MU. The branch points described above in the UWF upward looking fundus image GU correspond to the branch points of the blood vessels in the UWF downward looking fundus image GD. Branch locations having the same branching pattern as the branching pattern of branch points in the UWF upward looking fundus image GU are extracted from the UWF downward looking fundus image GD using image recognition processing or the like.</p><p id="p-0166" num="0162">At step <b>344</b> the montage image generation section <b>1425</b> employs the feature point group <b>1</b> and the feature point group <b>2</b> to generate a projection transformation matrix for geometric transformation of the UWF downward looking fundus image GD. The projection transformation matrix is a matrix to transform the UWF downward looking fundus image GD so as to conform to the UWF upward looking fundus image GU. The projection transformation matrix is defined by at least four feature points.</p><p id="p-0167" num="0163">At step <b>346</b>, the generated projection transformation matrix is employed to transform the UWF downward looking fundus image GD so as to obtain the UWF downward looking fundus image GDC after transformation (see <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>). The feature point group <b>1</b> and the feature point group <b>2</b> both arrive at the same position after transformation using the projection transformation matrix, and positional alignment processing has been executed. Due to the transformation, the UWF downward looking fundus image GDC is larger than the UWF downward looking fundus image GD (there is an increase in the surface area).</p><p id="p-0168" num="0164">In the above description the projection transformation matrix is generated to make the UWF downward looking fundus image GD conform to the UWF upward looking fundus image GU, and the UWF downward looking fundus image GD is transformed. On the other hand, a projection transformation matrix may be generated to make the UWF upward looking fundus image GU conform to the UWF downward looking fundus image GD, and the UWF upward looking fundus image GU may be transformed.</p><p id="p-0169" num="0165">The inter-image positional alignment processing is performed thereby, completing step <b>324</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>, and the image processing proceeds to step <b>326</b>.</p><p id="p-0170" num="0166">At step <b>326</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the montage image generation section <b>1425</b> combines the UWF upward looking fundus image GU and the UWF downward looking fundus image GDC after transformation to generate a montage image GM.</p><p id="p-0171" num="0167">More specifically, first the montage image generation section <b>1425</b> sets a line segment LGU on the UWF upward looking fundus image GU to pass through the optic nerve head ONHU and the macular MU, as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>. Similarly, the montage image generation section <b>1425</b> sets a line segment LGD on the UWF downward looking fundus image GDC after transformation to pass through the optic nerve head ONHD and the macular MD, as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>.</p><p id="p-0172" num="0168">Next, the montage image generation section <b>1425</b> performs processing to assign weights to overlapping regions of the UWF upward looking fundus image GU and the UWF downward looking fundus image GDC. As illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the montage image generation section <b>1425</b> assigns a weight of &#x201c;1&#x201d; to an upper side UWF upward looking fundus image GUx region that is a region on the upper side of the line segment LGU of the UWF upward looking fundus image GU. The montage image generation section <b>1425</b> assigns a weight of &#x201c;0&#x201d; to a region on the lower side of the line segment LGU. The montage image generation section <b>1425</b> assigns a weight of &#x201c;1&#x201d; to a lower side UWF downward looking fundus image GDCx that is a region on the lower side of the line segment LGD of the UWF downward looking fundus image after transformation, and assigns a weight of &#x201c;0&#x201d; in a region on the upper side of the line segment LGD.</p><p id="p-0173" num="0169">The montage image generation section <b>1425</b> generates a montage image GM combining the UWF upward looking fundus image GUx and the UWF downward looking fundus image GDCx by performing the weighting processing in this manner on the UWF upward looking fundus image GU and the UWF downward looking fundus image GDC. As illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the line segment LG is a line segment connecting the optic nerve head OMH to the macular M, the upper side of the line segment LG is the UWF upward looking fundus image GUx and the lower side of the line segment LG is the UWF downward looking fundus image GDCx. The montage image is an example of a &#x201c;montage image&#x201d; of the technology disclosed herein.</p><p id="p-0174" num="0170">Note that weighting related to the overlapping portions of the UWF upward looking fundus image GU and the UWF downward looking fundus image GDC is not limited the example described above, and various values may be set for a mixing ratio between the UWF upward looking fundus image GU and the UWF downward looking fundus image GDC.</p><p id="p-0175" num="0171">Positional alignment is performed on the UWF upward looking fundus image GU and the UWF downward looking fundus image GDC in this manner and they are combined. The combining enables a fundus image to be obtained to analyze vortex veins positioned in the fundus peripheral portion or the fundus equatorial portion and choroidal blood vessels at the vortex vein periphery without blood vessels of the fundus becoming non-contiguous, or for analyzing abnormal portions, pathological lesions and the like.</p><p id="p-0176" num="0172">Next at step <b>328</b> the feature portion acquisition section <b>1424</b> uses the montage image GM to analyze the position of the vortex veins and the blood vessel diameter of blood vessels in the vicinity of the vortex veins. Examples of vortex vein information obtained by such analysis include information related to the number of the vortex veins, positions of the vortex veins, the number of blood vessels connected to the vortex veins, and the blood vessel diameter of blood vessels at the periphery of the vortex shaped film.</p><p id="p-0177" num="0173">After the processing of step <b>328</b>, the 3D rendering processing is executed at step <b>504</b>S. At step <b>504</b> of the first exemplary embodiment, the image processing section <b>1420</b> executes the 3D rendering processing (see also <figref idref="DRAWINGS">FIG. <b>6</b></figref>) on the UWF fundus image G<b>1</b>. In contrast thereto, at step <b>504</b> in the second exemplary embodiment the image processing section <b>1420</b> executes the 3D rendering processing (see also <figref idref="DRAWINGS">FIG. <b>6</b></figref>) on the montage image GM. Thus the only difference therebetween is in the image subjected to processing, and so explanation of the 3D rendering processing of step <b>504</b>S will be omitted.</p><p id="p-0178" num="0174">After step <b>504</b>S, display screen generation processing is executed at step <b>506</b>S. Details are given later regarding the display screen of the second exemplary embodiment.</p><p id="p-0179" num="0175">At step <b>332</b>, the output section <b>1440</b> outputs the montage image GM and the vortex vein analysis information obtained by vortex vein analysis to the storage device <b>254</b> of the server <b>140</b>. The montage image GM and the vortex vein analysis information obtained by the vortex vein analysis is stored in the storage device <b>254</b> of the server <b>140</b>.</p><p id="p-0180" num="0176">At step <b>332</b>, the output section <b>1440</b> also outputs image data corresponding to a display screen <b>400</b>B to the viewer <b>150</b>.</p><p id="p-0181" num="0177">Note that the display control section <b>1430</b> may output an instruction to the display <b>256</b> to display the montage image GM.</p><p id="p-0182" num="0178">Next, description follows regarding the display screen <b>400</b>B of the second exemplary embodiment. The display screen <b>400</b>B of the second exemplary embodiment is substantially similar to the display screen <b>400</b>A of the first exemplary embodiment, and so only differing parts will be explained.</p><p id="p-0183" num="0179">The display screen <b>400</b>B includes an image display area <b>404</b>B instead of the image display area <b>404</b>A. The image display area <b>404</b>B includes a montage image display field <b>452</b>B and a video data display field <b>454</b>B<b>1</b>.</p><p id="p-0184" num="0180">The montage image is displayed in the montage image display field <b>452</b>B.</p><p id="p-0185" num="0181">When the replay button <b>456</b> is operated, the video data is displayed on a video data display section <b>455</b>B, and a bar graph of the proportion already displayed from out of the entire video data is displayed in the display progress display section <b>458</b>.</p><p id="p-0186" num="0182">The replay of video data on the video data display section <b>455</b>B is similar to that of the first exemplary embodiment and so explanation thereof will be omitted.</p><p id="p-0187" num="0183">When the video data is displayed on the video data display section <b>455</b> and an image Gz is displayed, as illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>, an 3D eyeball image switch button <b>460</b> is displayed in a video data display field <b>454</b>B<b>1</b>. When the 3D eyeball image switch button <b>460</b> is operated, as illustrated in <figref idref="DRAWINGS">FIG. <b>23</b></figref>, a switch is made to a 3D fundus image display field <b>454</b>B<b>2</b> instead of the video data display field <b>454</b>B<b>1</b>. A 3D fundus image is displayed in the 3D fundus image display field <b>454</b>B<b>2</b>. As described above, the 3D fundus image displayed has feature portions emphasized on an image after the pixels of all points on the montage image have been moved to points on the eyeball model M corresponding to these pixel points.</p><p id="p-0188" num="0184">In the first exemplary embodiment as described above, video data is created to represent the process to move each of the points of the montage image to the corresponding point on the eyeball model. Thus the way in which each of the points of the montage image moves to the corresponding point on the eyeball model can be known. This enables which point each of the corresponding points on the eyeball model is on the montage image to be known.</p><p id="p-0189" num="0185">Moreover, the montage image includes a wider area than the UWF fundus image of the first exemplary embodiment, and so which point on the montage image of the wider area than the UWF fundus image is related to each of the corresponding points on the eyeball model can be known. Thus a user is able to directly ascertain a position on the eyeball of vortex veins and pathological changes such as a detached retina simply by the user viewing the video data generated from the montage image. The ophthalmologist is able to explain to the examinee (patient) the results of examination, such as the position and size of pathological change in the fundus peripheral portion, in a manner that is easier to understand.</p><p id="p-0190" num="0186">A method described in International Publication (WO) No. PCT/JP2019/021868 may be employed as the method to acquire the UWF upward looking fundus image and the UWF downward looking fundus image, and the method to create the montage image. The entire disclosure of WO No. PCT/JP2019/021868 filed on May 31, 2019 is incorporated in the present specification by reference herein.</p><p id="p-0191" num="0187">Next explanation follows regarding various modified examples of the technology disclosed herein.</p><p id="p-0192" num="0188">The configuration of each of the modified examples is similar to the configuration of the first exemplary embodiment, and so explanation thereof will be omitted below. The operation of each of the modified examples is substantially similar to the operation of the first exemplary embodiment, and so explanation will mainly be of differing parts. In the first exemplary embodiment video data is created to represent the process to move pixels at each of the points on the UWF fundus image to respective corresponding points on the eyeball model. However, the technology disclosed herein is not limited thereto.</p><heading id="h-0007" level="1">First Modified Example</heading><p id="p-0193" num="0189">Explanation follows regarding image processing of a first modified example, with reference to <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0194" num="0190">At step <b>2402</b>, the image acquisition section <b>1410</b> acquires a two-dimensional fundus image (UWF fundus image G<b>1</b> or montage image) corresponding to the patient ID from the storage device <b>254</b>.</p><p id="p-0195" num="0191">At step <b>2404</b>, the 3D video generation section <b>1421</b> acquires each of second points on the eyeball model M, which has been corrected according to eye axial length, corresponding to each of first points of the two-dimensional fundus image.</p><p id="p-0196" num="0192">At step <b>2406</b>, the 3D video generation section <b>1421</b> creates data (video data) representing a process to move each of the first points from a corresponding second point to this first point.</p><p id="p-0197" num="0193">This enables a user to know the way in which of each of the first points of the UWF fundus image moves from a respective corresponding second point on the eyeball model to the first point.</p><heading id="h-0008" level="1">Second Modified Example</heading><p id="p-0198" num="0194">Explanation follows regarding image processing of the second modified example, with reference to <figref idref="DRAWINGS">FIG. <b>25</b></figref>.</p><p id="p-0199" num="0195">In the second modified example, data for a three-dimensional fundus image is generated in advance by reverse stereographic projection of a two-dimensional fundus image (UWF fundus image G<b>1</b> or montage image) onto an eyeball model corrected according to eye axial length, and the data is stored in the storage device <b>245</b> associated with the patient ID.</p><p id="p-0200" num="0196">At step <b>2502</b>, the image acquisition section <b>1410</b> acquires three-dimensional fundus image data corresponding to the patient ID from the storage device <b>254</b>.</p><p id="p-0201" num="0197">At step <b>2504</b>, the 3D video generation section <b>1421</b> acquires each of the second points on the two-dimensional fundus image corresponding to each of the first points of the three-dimensional fundus image.</p><p id="p-0202" num="0198">At step <b>2506</b>, the 3D video generation section <b>1421</b> creates data (video data) representing a process to move each of the first points to the corresponding second point.</p><p id="p-0203" num="0199">This enables a user to know the way in which each of the first points of the three-dimensional fundus image moves to each of the corresponding second points on the two-dimensional fundus image.</p><heading id="h-0009" level="1">Third Modified Example</heading><p id="p-0204" num="0200">Explanation follows regarding image processing of a third modified example, with reference to <figref idref="DRAWINGS">FIG. <b>26</b></figref>.</p><p id="p-0205" num="0201">In the third modified example, data of a three-dimensional fundus image is generated in advance similarly to in the second modified example, and the data is stored in the storage device <b>245</b> associated with the patient ID.</p><p id="p-0206" num="0202">At step <b>2602</b>, the image acquisition section <b>1410</b> acquires data of the three-dimensional fundus image corresponding to the patient ID from the storage device <b>254</b>.</p><p id="p-0207" num="0203">At step <b>2604</b>, the 3D video generation section <b>1421</b> acquires each of the second points on the two-dimensional fundus image corresponding to each of the first points on the three-dimensional fundus image.</p><p id="p-0208" num="0204">At step <b>2606</b>, the 3D video generation section <b>1421</b> creates data (video data) representing a process to move each of the first points from the corresponding second point to the first point.</p><p id="p-0209" num="0205">This enables a user to know the way in which each of the first points of the three-dimensional fundus image moves from each of the corresponding second points on the two-dimensional fundus image to the first point.</p><heading id="h-0010" level="1">Other Modified Examples</heading><p id="p-0210" num="0206">The data of the three-dimensional fundus image in each of the examples described above may be obtained by reverse stereographic projection of a two-dimensional fundus image (UWF fundus image G<b>1</b> or montage image) onto an eyeball model. However the technology disclosed herein is not limited thereto. For example, OCT volume data, OCT angiograph data, three-dimensional fundus image data obtained by MRI or ultrasound, or three-dimensional image data such as of an anterior eye portion, may be employed.</p><p id="p-0211" num="0207">In the first exemplary embodiment video data is created representing a process to move pixels at all points of a UWF fundus image to corresponding points on an eyeball model. In the second exemplary embodiment video data is created representing a process to move all the pixels of a montage image to corresponding points on the eyeball model. However the technology disclosed herein is not limited thereto.</p><p id="p-0212" num="0208">Video data may be created for the movement described above of a pixel at least at one point selected by a user.</p><p id="p-0213" num="0209">In such cases the user may select plural points in an area by stipulating the area of the UWF fundus image or the montage image.</p><p id="p-0214" num="0210">The user may also stipulate the above-mentioned feature portion of the eyeball as the area. More specifically, the user may stipulate, for example, a lens body, a fundus structure, a pathological lesion, or the like as the feature portion.</p><p id="p-0215" num="0211">In the first exemplary embodiment video data is created to represent a process to move pixels at each point of the UWF fundus image (UWF-SLO fundus image) to each of the corresponding points on the eyeball model. In the second exemplary embodiment video data is created to represent a process to move pixels at each point of the montage image (combined image of the UWF-SLO fundus image) to each of the corresponding points on the eyeball model. However, the technology disclosed herein is not limited thereto.</p><p id="p-0216" num="0212">Video data may be created to represent a process to move pixels at each point of a retinal vascular image or a choroidal vascular image obtained from the UWF-SLO image or the montage image (combined image of the UWF-SLO fundus image) to each of the corresponding points on the eyeball model.</p><p id="p-0217" num="0213">Moreover, instead of a UWF-SLO image, for example, video data may be created to represent a process to move pixels at each point of a fundus image obtained by a fundus camera to each of the corresponding points on the eyeball model.</p><p id="p-0218" num="0214">Video data is created in the first exemplary embodiment and the second exemplary embodiment. However, technology disclosed herein is not limited thereto. For example, at least one image may be created to represent a process to move pixels at each point of a UWF fundus image to each of the corresponding points on the eyeball model. For example, an image may be created of a position where a pixel at a point of the UWF fundus image is positioned at an intermediate position between at least one point of the UWF fundus image and the corresponding point on the eyeball model.</p><p id="p-0219" num="0215">In the respective examples described above, the image processing of <figref idref="DRAWINGS">FIG. <b>5</b></figref> and <figref idref="DRAWINGS">FIG. <b>17</b></figref> is executed by the server <b>140</b>, however the technology disclosed herein is not limited thereto. For example, this processing may be executed by the ophthalmic device <b>110</b> or the viewer <b>150</b>, or a separate other image processing device may be connected to the network <b>130</b> and this processing executed by this image processing device.</p><p id="p-0220" num="0216">Although explanation has been given in the respective examples described above regarding an example in which a computer is employed to implement image processing using a software configuration, the technology disclosed herein is not limited thereto. For example, instead of a software configuration employing a computer, the image processing may be executed solely by a hardware configuration such as a field programmable gate array (FPGA) or an application specific integrated circuit (ASIC). Alternatively, a configuration may be adopted in which some processing out of the image processing is executed by a software configuration, and the remaining processing is executed by a hardware configuration.</p><p id="p-0221" num="0217">Such technology disclosed herein encompasses cases in which the image processing is implemented by a software configuration utilizing a computer, and also cases in which the image processing is implemented by a configuration that is not a software configuration utilizing a computer, and encompasses the following first technology and second technology.</p><p id="p-0222" num="0218">First Technology</p><p id="p-0223" num="0219">An image processing device image including:</p><p id="p-0224" num="0220">an image acquisition section configured to acquire a two-dimensional fundus image;</p><p id="p-0225" num="0221">a point acquisition section configured to acquire a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image; and</p><p id="p-0226" num="0222">a creation section configured to create data to represent a process to move a pixel at the first point to the second point.</p><p id="p-0227" num="0223">Note that the 3D video generation section <b>1421</b> is an example of a &#x201c;point acquisition section&#x201d; and a &#x201c;creation section&#x201d;.</p><p id="p-0228" num="0224">Second Technology</p><p id="p-0229" num="0225">An image processing method including:</p><p id="p-0230" num="0226">an image acquisition section acquiring a two-dimensional fundus image;</p><p id="p-0231" num="0227">a point acquisition section acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image; and</p><p id="p-0232" num="0228">a creation section creating data to represent a process to move a pixel at the first point to the second point.</p><p id="p-0233" num="0229">The following third technology is proposed from the content disclosed above.</p><p id="p-0234" num="0230">Third Technology</p><p id="p-0235" num="0231">A computer program product for image processing, the computer program product including a computer-readable storage medium that is not itself a transitory signal, with a program stored on the computer-readable storage medium, the program causing a computer to execute processing including:</p><p id="p-0236" num="0232">acquiring a two-dimensional fundus image;</p><p id="p-0237" num="0233">acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image; and</p><p id="p-0238" num="0234">creating data to represent a process to move a pixel at the first point to the second point.</p><p id="p-0239" num="0235">It must be understood that the image processing described above is merely an example thereof. Obviously redundant steps may be omitted, new steps may be added, and the processing sequence may be swapped around within a range not departing from the spirit of the present disclosure.</p><p id="p-0240" num="0236">All publications, patent applications and technical standards mentioned in the present specification are incorporated by reference in the present specification to the same extent as if each individual publication, patent application, or technical standard was specifically and individually indicated to be incorporated by reference.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image processing method in which image processing is performed by a processor, the image processing method comprising:<claim-text>acquiring a two-dimensional fundus image;</claim-text><claim-text>acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image; and</claim-text><claim-text>creating data to represent a process to move the at least one first point to the second point.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the data is video data configured from a plurality of frames.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image processing method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein:<claim-text>the process is set for each of a specific interval of time;</claim-text><claim-text>a frame is generated for each specific interval of time for an intermediate position for moving a pixel at the at least one first point; and</claim-text><claim-text>the video data includes a frame at the at least one first point, frames at a plurality of intermediate positions set between the at least one first point and the second point, and a frame at the second point.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>a plurality of movement patterns are predetermined for the movement; and</claim-text><claim-text>the data is data representing a process of moving according to a movement pattern selected from among the plurality of movement patterns.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image processing method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the movement pattern is a path connecting the at least one first point and the second point in three-dimensional space.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image processing method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the path is a straight line path connecting the at least one first point and the second point.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image processing method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the movement pattern includes data related to a movement speed for the at least one first point to move to the second point.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one first point is a point selected from among a plurality of points on the two-dimensional fundus image.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one first point is respective points in a stipulated area of the two-dimensional fundus image.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one first point is respective points in the entire two-dimensional fundus image.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one first point is a point corresponding to a single pixel configuring the two-dimensional fundus image.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the eyeball model is a model resulting from a default eyeball model being corrected according to an eye axial length.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The image processing method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the two-dimensional fundus image is a montage image resulting from combining a first direction fundus image imaged in a state in which a gaze of an examined eye is facing in a first direction together with a second direction fundus image imaged in a state in which the examined eye is facing in a second direction different from the first direction.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. An image processing device comprising:<claim-text>a memory, and</claim-text><claim-text>a processor coupled to the memory,</claim-text><claim-text>wherein the processor is configured to:</claim-text><claim-text>acquire a two-dimensional fundus image;</claim-text><claim-text>acquire a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image; and</claim-text><claim-text>create data to represent a process to move the at least one first point to the second point.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory storage medium storing a program to cause a computer to execute processing comprising:<claim-text>acquiring a two-dimensional fundus image;</claim-text><claim-text>acquiring a second point on an eyeball model corresponding to at least one first point of the two-dimensional fundus image; and</claim-text><claim-text>creating data to represent a process to move the at least one first point to the second point.</claim-text></claim-text></claim><claim id="CLM-16-27" num="16-27"><claim-text><b>16</b>-<b>27</b>. (canceled)</claim-text></claim></claims></us-patent-application>