<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004289A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004289</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942388</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2016-185477</doc-number><date>20160923</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>061</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>068</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>065</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0656</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0679</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0253</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>7204</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>7206</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>7201</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>7205</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">STORAGE DEVICE THAT WRITES DATA FROM A HOST DURING GARBAGE COLLECTION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16940269</doc-number><date>20200727</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11474702</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942388</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15441068</doc-number><date>20170223</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10761733</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16940269</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KIOXIA CORPORATION</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>KANNO</last-name><first-name>Shinichi</first-name><address><city>Ota Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A memory system includes a controller, a buffer, and a nonvolatile memory including a plurality of blocks, wherein each of the blocks includes a plurality of pages and each of the pages includes a plurality of unit data portions. The controller is configured to carry out garbage collection by reading data from one or more pages of a target block of the garbage collection and selectively copying valid unit data portions included in the read data to another block, count a number of invalid unit data portions included in the read data, and accept, in the buffer, unit data portions from a host as write data, up to a number determined based on the counted number, during the garbage collection.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="103.46mm" wi="158.75mm" file="US20230004289A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="208.20mm" wi="151.05mm" orientation="landscape" file="US20230004289A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="235.20mm" wi="119.89mm" orientation="landscape" file="US20230004289A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="207.94mm" wi="128.86mm" orientation="landscape" file="US20230004289A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="212.68mm" wi="156.04mm" orientation="landscape" file="US20230004289A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="226.82mm" wi="136.48mm" file="US20230004289A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="119.21mm" wi="133.86mm" file="US20230004289A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="237.49mm" wi="146.64mm" file="US20230004289A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="229.53mm" wi="145.80mm" orientation="landscape" file="US20230004289A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="230.12mm" wi="145.63mm" file="US20230004289A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="118.45mm" wi="146.90mm" file="US20230004289A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="229.53mm" wi="154.52mm" file="US20230004289A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="240.45mm" wi="150.45mm" orientation="landscape" file="US20230004289A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="158.24mm" wi="148.51mm" orientation="landscape" file="US20230004289A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="178.22mm" wi="143.43mm" orientation="landscape" file="US20230004289A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/940,269, filed Jul. 27, 2020, which is a continuation of U.S. patent application Ser. No. 15/441,068, filed Feb. 23, 2017, now U.S. Pat. No. 10,761,733, issued Sep. 1, 2020, which application is based upon and claims the benefit of priority from Japanese Patent Application No. 2016-185477, filed Sep. 23, 2016, the entire contents of each of which are incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">Embodiments described herein relate generally to a technology of controlling a nonvolatile memory.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Today, a memory system including a nonvolatile memory is widely used. An example of such a memory system includes a solid state drive (SSD) based on a NAND flash technology. The SSD is characteristic in low power consumption and high-speed performance, and is typically used as a main storage of various computers. Improved performance on data input/output between a host and a memory system is desired.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an information processing system including a memory system according to an embodiment.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a data write operation and a garbage collection operation carried out in the memory system according to the embodiment.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of a data flow during the data write operation of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a data flow during the data write operation and the garbage collection operation of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart of a write process performed by the memory system according to the embodiment.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart of a host write destination block allocation process performed by the memory system according to the embodiment.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of a garbage collection process performed by the memory system according to the embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example of the data write operation and the garbage collection operation carried out in parallel for a plurality of namespace functions.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flowchart illustrating another procedure of the write process performed by the memory system according to the embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating another procedure of the host write destination block allocation process performed by the memory system according to the embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating another procedure of the garbage collection process performed by the memory system according to the embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates another example of the data write operation and the garbage collection operation carried out in parallel for the plurality of namespace functions.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram of a host.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates a configuration of a computer which includes the memory system according to the embodiment and the host.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">In order to improve performance of data input/output between a host and a memory system, a new function for reducing variation in write latency may be needed.</p><p id="p-0020" num="0019">An embodiment provides a memory system and a control method directed to reducing variation in write latency.</p><p id="p-0021" num="0020">According to an embodiment, a memory system includes a controller, a buffer, and a nonvolatile memory including a plurality of blocks, wherein each of the blocks includes a plurality of pages and each of the pages includes a plurality of unit data portions. The controller is configured to carry out garbage collection by reading data from one or more pages of a target block of the garbage collection and selectively copying valid unit data portions included in the read data to another block, count a number of invalid unit data portions included in the read data, and accept, in the buffer, unit data portions from a host as write data, up to a number determined based on the counted number, during the garbage collection.</p><p id="p-0022" num="0021">Hereinafter, embodiments will be described with reference to the drawings.</p><p id="p-0023" num="0022">First, the description will be made about a configuration of an information processing system <b>1</b> which includes the memory system according to an embodiment with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The memory system is a semiconductor storage device which is configured to write data in the nonvolatile memory and to read data from the nonvolatile memory. The memory system is achieved by a solid state drive (SSD) <b>3</b> based on a NAND flash technology, for example.</p><p id="p-0024" num="0023">The information processing system <b>1</b> includes a host calculator (hereinafter, also referred to as host) <b>2</b> and the SSD <b>3</b>. The host <b>2</b> is an information processing device (computing device) which is configured to store data in the SSD <b>3</b>. Examples of the information processing device include a server computer and a personal computer.</p><p id="p-0025" num="0024">The SSD <b>3</b> may be included in the information processing device which serves as the host <b>2</b>, or may be externally connected to the information processing device through a cable or a network.</p><p id="p-0026" num="0025">An interface to connect the host <b>2</b> and the SSD <b>3</b> may be used SCSI, Serial Attached SCSI(SAS), ATA, Serial ATA(SATA), PCI Express (PCIe), NVM Express (NVMe), the Ethernet (registered trademark), and Fibre channel.</p><p id="p-0027" num="0026">The SSD <b>3</b> may include a controller <b>4</b>, a nonvolatile memory (NAND memory) <b>5</b>, and a DRAM <b>6</b>. The NAND memory <b>5</b> may contain a plurality of NAND flash memory chips while not limited in number.</p><p id="p-0028" num="0027">The NAND memory <b>5</b> includes a memory cell array. The memory cell array includes a number of NAND blocks (blocks) BC to Bm&#x2212;1. One block corresponds to a unit of erasing. The block may be called a &#x201c;physical block&#x201d; or an &#x201c;erase block&#x201d;.</p><p id="p-0029" num="0028">Each of the blocks BC to Bm&#x2212;1 includes a number of pages (physical pages). In other words, each of the blocks BC to Bm&#x2212;1 includes pages P<b>0</b> to Pn&#x2212;1. A plurality of memory cells connected to the same word line are organized as one page. In the NAND memory <b>5</b>, data read and data write are performed in units of one page. The data erase is performed in units of one block.</p><p id="p-0030" num="0029">The controller <b>4</b> is electrically connected to the NAND memory (nonvolatile memory) <b>5</b> through a NAND interface <b>13</b> such as Toggle and ONFI. The controller <b>4</b> may serve as a flash translation layer (FTL) which is configured to perform data management of the NAND memory <b>5</b> and block management of the NAND memory <b>5</b>.</p><p id="p-0031" num="0030">The data management includes (1) management of mapping information which indicates a correspondence between a logical block address (LBA) and a physical address and (2) a process of hiding reading/writing in units of page and erasing in units of block. The mapping management between the LBA and the physical address is performed using a lookup table (LUT) <b>33</b> which serves as a logical/physical address conversion table. The lookup table (LUT) <b>33</b> is created in units of predetermined management size, and manages the mapping between the LBA and the physical address in units of the management size. Most of write commands from the host <b>2</b> require writing of 4 KB data.</p><p id="p-0032" num="0031">For that reason, the lookup table (LUT) <b>33</b> may manage the mapping between the LBA and the physical address in units of 4 KB for example. The physical address corresponding to a certain LBA indicates a physical memory location of the NAND memory <b>5</b> in which the data of the LBA are written. The physical address includes a physical block address and a physical page address. The physical page address is allocated to all pages, and the physical block address is allocated to all blocks. The data writing to a page can be made only one time per one erase cycle.</p><p id="p-0033" num="0032">For that reason, the controller <b>4</b> maps rewritten data associated with the same LBA to a page of the NAND memory <b>5</b> different from a page in which original (old) data are written. In other words, the controller <b>4</b> writes the written data in another (new) page. The controller <b>4</b> updates the lookup table (LUT) <b>33</b> to associate the LBA with the new page, and invalidates the original page (that is, the old data).</p><p id="p-0034" num="0033">The block management includes a bad-block management, a wear leveling, and a garbage collection operation. The garbage collection operation is an operation of creating a free space in the NAND memory <b>5</b>. During the garbage collection operation, all valid data in one or more blocks, in which valid data and invalid data are stored are selectively copied to another block (copy destination free block) in order to increase the number of free blocks of the NAND memory <b>5</b>. Then, the lookup table (LUT) <b>33</b> is updated, such that each LBA of the copied valid data is mapped to a correct physical address. The block containing only invalid data is re-categorized as a free block after copying the valid data to another block. As a result, the block can be reused after erasing the data.</p><p id="p-0035" num="0034">The host <b>2</b> sends a write command to the SSD <b>3</b>. The write command includes a logical address (start logical address) of write data (that is, write data to be written) and a transfer length. In the present embodiment, the LBA is used as the logical address, but, in other embodiments, an object ID may be used as the logical address.</p><p id="p-0036" num="0035">The controller <b>4</b> of the SSD <b>3</b> writes the write data designated by the start logical address and the transfer length designated by the write command into a page of a block of the NAND memory <b>5</b>. Furthermore, the controller <b>4</b> updates the lookup table (LUT) <b>33</b> to map the LBA corresponding to the written data to the physical address indicating a physical storage location to which the data are written.</p><p id="p-0037" num="0036">More specifically, the controller <b>4</b> allocates one of the free blocks in the NAND memory <b>5</b> as a write destination block to write the data from the host <b>2</b>. The write destination block is a block to which the data from the host <b>2</b> is written, and called &#x201c;write target block&#x201d; or &#x201c;input block&#x201d;. The controller <b>4</b> writes the write data received from the host <b>2</b> into an available page in the write destination block while updating the lookup table (LUT) <b>33</b> irrespective of the LBA of the write data received from the host <b>2</b>. In a case where there is no available page in the write destination block, the controller <b>4</b> allocates a new free block as the write destination block.</p><p id="p-0038" num="0037">Next, the description will be made about a configuration of the controller <b>4</b>.</p><p id="p-0039" num="0038">The controller <b>4</b> includes a host interface <b>11</b>, a CPU <b>12</b>, the NAND interface <b>13</b>, and a DRAM interface <b>14</b>. The CPU <b>12</b>, the NAND interface <b>13</b>, and the DRAM interface <b>14</b> are mutually connected through a bus <b>10</b>.</p><p id="p-0040" num="0039">The host interface <b>11</b> receives various commands from the host <b>2</b>. The commands include, for example, a write command, a read command, and an erase command.</p><p id="p-0041" num="0040">The CPU <b>12</b> is a processor which is configured to control the host interface <b>11</b>, the NAND interface <b>13</b>, and the DRAM interface <b>14</b>. The CPU <b>12</b> performs physical resource management processing to manage the NAND memory <b>5</b>, and command processing to process various commands received from the host <b>2</b>. The physical resource management processing and the command processing may be controlled by firmware executed by the CPU <b>12</b>.</p><p id="p-0042" num="0041">The firmware causes the CPU <b>12</b> to serve as a garbage collection control section <b>21</b>, a write control section <b>22</b>, a read control section <b>23</b>, and a namespace management section <b>24</b>.</p><p id="p-0043" num="0042">The garbage collection control section <b>21</b> controls the garbage collection operation. As described above, the garbage collection operation is an operation of creating the free space in the NAND memory <b>5</b>. Through the garbage collection operation, all valid data in some blocks storing both valid data and invalid data are selectively copied to the other blocks. The garbage collection control section <b>21</b> includes a block select section <b>211</b>, a data copy control section <b>212</b>, and an accumulated invalid data count measurement section <b>213</b>. The block select section <b>211</b> selects a target block of the garbage collection from a number of blocks in the NAND memory <b>5</b>.</p><p id="p-0044" num="0043">The data copy control section <b>212</b> performs a data copy operation during the garbage collection. The data copy control section <b>212</b> controls an operation of copying the valid data in the target block of the garbage collection to a copy destination block for the garbage collection. The copy destination block for the garbage collection is also called a write destination block for the garbage collection.</p><p id="p-0045" num="0044">The accumulated invalid data count measurement section <b>213</b> counts up the number of invalid data portions found out during the garbage collection operation to calculate the number of accumulated invalid data portions. The number of invalid data portions may be measured (counted) in units of the management size (for example, 4 KB as described above). The number of accumulated invalid data indicates the number (total number) of invalid data portions found out during the garbage collection operation. Furthermore, the accumulated invalid data count measurement section <b>213</b> decreases the number of accumulated invalid data by the number of write data portions received from the host <b>2</b>. In other words, when a write request is received from the host <b>2</b>, the accumulated invalid data count measurement section <b>213</b> subtracts the number corresponding to the data size of the write data designated by the write request from the number of accumulated invalid data portions. For example, when the write request to write 16 KB of write data is received, i.e., four write data portions (4&#xd7;4 KB) are received, &#x201c;4&#x201d; is subtracted from the number of accumulated invalid data portions.</p><p id="p-0046" num="0045">The write control section <b>22</b> controls a write operation according to the write request (write command) from the host <b>2</b>. The write control section <b>22</b> allows a reception of the write data portion from the host <b>2</b> within the range of the number of accumulated invalid data. In other words, the write control section <b>22</b> can receive the write data portions as many as the upper limit (the number of accumulated invalid data portions) from the host <b>2</b>. The write control section <b>22</b> writes the received write data portion in the write destination block of the host <b>2</b>.</p><p id="p-0047" num="0046">The read control section <b>23</b> controls a read operation according to a read request (read command) from the host <b>2</b>. The read control section <b>23</b> reads, for example, the data at LBA designated by the read request from the NAND memory <b>5</b>, and transmits the read data to the host <b>2</b>.</p><p id="p-0048" num="0047">The namespace management section <b>24</b> manages a plurality of logical address spaces (LBA spaces) corresponding to a plurality of namespaces in order to handle the SSD <b>3</b> as if there are a plurality of drives. A designated number of blocks are allocated (reserved) in each namespace. The number of allocated blocks may be different in every namespace. The namespace management section <b>24</b> logically divides the NAND memory <b>5</b> into a plurality of areas associated with the plurality of namespaces. For example, in a case where a first namespace and a second namespace are created, the namespace management section <b>24</b> logically divides the NAND memory <b>5</b> into a first area for writing write data portions associated with the first namespace and a second area for writing write data portions associated with the second namespace.</p><p id="p-0049" num="0000">The write data (that is, each write data portion associated with the first namespace) designated by the write request associated with an identifier (NSID #1) of the first namespace are written in a block allocated as the write destination block in the first area (first namespace). When the write destination block is full of data, a new block is allocated as a new write destination block in the first area (first namespace).</p><p id="p-0050" num="0048">The write data (that is, each write data portion associated with the second namespace) designated by the write request associated with an identifier (NSID #2) of the second namespace are written in a block allocated as the write destination block in the second area (second namespace). When the write destination block is full of data, a new block is allocated as a new write destination block in the second area (second namespace).</p><p id="p-0051" num="0049">With reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the garbage collection operation carried out by the garbage collection control section <b>21</b> and the write operation carried out by the write control section <b>22</b> will be described.</p><p id="p-0052" num="0050">First, the description will be made about the garbage collection operation.</p><p id="p-0053" num="0051">The block select section <b>211</b> selects a target block <b>551</b> (hereinafter, also referred to as GC target block) of the garbage collection from an active block group <b>55</b>. In the active block group <b>55</b>, a block in use (that is, an active block) is contained. The active block is a block which contains the data from the host <b>2</b>. An invalid data count storage unit <b>34</b> may store the number of invalid data stored in each block. The block select section <b>211</b> may select a block having a high rate of invalid data as the target block <b>551</b> of the garbage collection with reference to the invalid data count storage unit <b>34</b>.</p><p id="p-0054" num="0052">The data copy control section <b>212</b> sequentially selects the data portion in the GC target block <b>551</b>. The data copy control section <b>212</b> determines whether the selected data portion is valid or invalid each time a certain data portion is selected. The invalid data are data which is no longer used (that is, data not associated with any LBA). The valid data are data in use (that is, data associated with an LBA). In a case where the selected data portion is valid, the data copy control section <b>212</b> temporally writes the data portion in a GC buffer <b>32</b>. In a case where the selected data portion is invalid, the data copy control section <b>212</b> skips writing the data portion in the GC buffer <b>32</b>. Furthermore, the accumulated invalid data count measurement section <b>213</b> adds &#x201c;1&#x201d; to the number of accumulated invalid data portions each time it is determined that the selected data portion is invalid by the data copy control section <b>212</b>.</p><p id="p-0055" num="0053">The data copy control section <b>212</b> writes the data written in the GC buffer <b>32</b>, for example, page by page in a copy destination block <b>512</b> (hereinafter, also referred to as GC copy destination block) of the garbage collection. The GC copy destination block <b>512</b> is a block which is allocated from a free block group <b>51</b>. The free block group <b>51</b> includes a block which is not currently used (that is, a free block). In a case where the GC copy destination block is not yet allocated, the data copy control section <b>212</b> allocates one free block in the free block group <b>51</b> as a new GC copy destination block <b>512</b>. In a case where the data is written up to the end of the allocated GC copy destination block <b>512</b>, the data copy control section <b>212</b> re-categorizes the GC copy destination block <b>512</b> to the active block group <b>55</b>.</p><p id="p-0056" num="0054">In a case where all valid data portions in the GC target block <b>551</b> are written (copied) in the GC copy destination block <b>512</b>, the data copy control section <b>212</b> re-categorizes the GC target block <b>551</b> to the free block group <b>51</b>. The data copy control section <b>212</b> may notify the block select section <b>211</b> of the fact that the GC target block <b>551</b> is re-categorized as a free block. The block select section <b>211</b> may select a new GC target block as needed.</p><p id="p-0057" num="0055">Next, the description will be made about the write operation according to the write request from the host <b>2</b>. The write control section <b>22</b> receives the write request from the host <b>2</b>. The write request includes information (transfer length) for designating a data size of the data to be written.</p><p id="p-0058" num="0056">The write control section <b>22</b> allows a reception of the write data portion from the host <b>2</b> within the range of the number of accumulated invalid data. For example, if the current number of accumulated invalid data is &#x201c;4&#x201d;, the write control section <b>22</b> can receive four write data portions (16 KB write data=4&#xd7;4 KB) in maximum. For that reason, in a case where the number of data portions corresponding to the data size designated by the write request is equal to or less than the number of accumulated invalid data portions, the write control section <b>22</b> determines that the write data of the data size designated by the write request is receivable, and returns a response to the host <b>2</b>.</p><p id="p-0059" num="0057">Since the response shows that the writing is allowed, the host <b>2</b> transmits the write data to the SSD <b>3</b> according to the response.</p><p id="p-0060" num="0058">The write control section <b>22</b> receives the respective write data portions from the host <b>2</b>, and temporally writes the received write data portions in a write buffer <b>31</b>. The accumulated invalid data count measurement section <b>213</b> subtracts the number of the received write data portions from the number of accumulated invalid data portions, i.e., decrease the number of accumulated invalid data portions by the number of write data portions received from the host <b>2</b>. The write control section <b>22</b> writes the respective write data portions written in the write buffer <b>31</b> into a write destination block <b>511</b> for the host <b>2</b> in units of a page, for example.</p><p id="p-0061" num="0059">The write control section <b>22</b> allocates one free block in the free block group <b>51</b> as a new write destination block <b>511</b> in a case where the write destination block <b>511</b> of the host <b>2</b> is not yet allocated, in a case where the data are written up to the end of the allocated write destination block <b>511</b>, or in a case where an error occurs in the write destination block <b>511</b>. In the case where the data are written up to the end of the allocated write destination block <b>511</b>, the write control section <b>22</b> re-categorizes the write destination block <b>511</b> as an active block in the active block group <b>55</b>.</p><p id="p-0062" num="0060">The accumulated invalid data count measurement section <b>213</b> determines whether the number of free blocks remaining in the free block group <b>51</b> is equal to or more than a threshold when the write destination block <b>511</b> of the host <b>2</b> is newly allocated. In a case where the number of remaining blocks is equal to or more than the threshold, the accumulated invalid data count measurement section <b>213</b> adds the number of data portions writable to the newly-allocated write destination block <b>511</b> to the number of accumulated invalid data portions. The number of data portions writable to the write destination block <b>511</b> is the number of data portions corresponding to the capacity of the blocks. Furthermore, in a case where there is a currently-accumulated data portion in the write buffer <b>31</b>, the accumulated invalid data count measurement section <b>213</b> may subtract the number of data portions from the number of accumulated invalid data portions. In a case where the number of remaining free blocks in the free block group <b>51</b> is less than the threshold, the accumulated invalid data count measurement section <b>213</b> sets the number of accumulated invalid data portions to &#x201c;0&#x201d;. Furthermore, in a case where there is a currently-accumulated data portion in the write buffer <b>31</b>, the accumulated invalid data count measurement section <b>213</b> may subtract the number of data portions from the number of accumulated invalid data portions.</p><p id="p-0063" num="0061">In a case where it is not possible to receive the write data of the data size designated by the write request, the write control section <b>22</b> does not return a response to the host <b>2</b> until the write data become receivable. In a case where the number of data portions corresponding to the data size designated by the write request exceeds the number of accumulated invalid data portions, the write control section <b>22</b> determines that the write data of the data size designated by the write request are not receivable, and does not return the response to the host <b>2</b>. Then, in a case where the number of accumulated invalid data portions increased through the garbage collection operation described above becomes equal to or more than the number of data portions corresponding to the data size designated by the write request, the write control section <b>22</b> determines that the write data of the data size designated by the write request are receivable, and returns the response to the host <b>2</b>.</p><p id="p-0064" num="0062">The description will be made with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref> about a data flow on the NAND memory <b>5</b> and the write buffer <b>31</b> in a case where a data write operation is performed. Herein, it is assumed that the free blocks equal to or more than the threshold are remaining in the free block group <b>51</b>.</p><p id="p-0065" num="0063">(1) First, a write destination block <b>511</b>A of the host <b>2</b> is allocated from the free block group <b>51</b>, and the number of data portions corresponding to the block size of the write destination block <b>511</b>A is set to the number of accumulated invalid data portions. For example, in a case where a management size is 4 KB, a page size is 16 KB, and the number of pages per block is 256, the number of data portions corresponding to the block size is 1024 (=16 KB+4 KB&#xd7;256) for example.</p><p id="p-0066" num="0064">(2) The write data portions A<b>1</b>, A<b>2</b>, . . . , Ax as many as the number of accumulated invalid data portions are received from the host <b>2</b>, and the write data portions A<b>1</b>, A<b>2</b>, . . . , Ax are temporally written in the write buffer <b>31</b>. At that time, the number of accumulated invalid data portions is subtracted by &#x201c;1&#x201d; for each write data portion.</p><p id="p-0067" num="0065">(3) The write data portions A<b>1</b>, A<b>2</b>, . . . , Ax in the write buffer <b>31</b> are written in the write destination block <b>511</b>A in units of a page.</p><p id="p-0068" num="0066">(4) In a case where the data are written up to the end of the write destination block <b>511</b>A, a new write destination block <b>511</b>B is allocated from the free block group <b>51</b>. Then, the number of data portions (for example, 1024) corresponding to the block size is set to the number of accumulated invalid data portions.</p><p id="p-0069" num="0067">(5) Next, the write data portions B<b>1</b>, B<b>2</b>, . . . , Bx as many as the number of accumulated invalid data portions are received from the host <b>2</b>, and the write data portions B<b>1</b>, B<b>2</b>, . . . , Bx are temporally written in the write buffer <b>31</b>. At that time, the number of accumulated invalid data portions is decreased by &#x201c;1&#x201d; for each write data portion.</p><p id="p-0070" num="0068">(6) The write data portions B<b>1</b>, B<b>2</b>, . . . , Bx in the write buffer <b>31</b> are written in the write destination block <b>511</b>B in units of a page.</p><p id="p-0071" num="0069">In this way, during a period when the free blocks equal to or more than the threshold are contained in the free block group <b>51</b>, the allocation of the write destination block of the host <b>2</b> and the writing of the data to the block are repeatedly performed.</p><p id="p-0072" num="0070">Next, the description will be made with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref> about a data flow on the NAND memory <b>5</b>, the write buffer <b>31</b>, and the GC buffer <b>32</b> in a case where the garbage collection operation and the data write operation are concurrently performed. Herein, it is assumed that the free blocks less than the threshold are remaining in the free block group <b>51</b>.</p><p id="p-0073" num="0071">(1) First, unprocessed data portions are sequentially selected from the head of the target block <b>551</b> of the garbage collection. In a case where a certain data portion is selected, it is determined whether the selected data portion is valid or invalid. In a case where the data portion is invalid, the number of accumulated invalid data portions is incremented by &#x201c;1&#x201d;. In a case where the data portion is valid, the data portion is copied to the GC buffer <b>32</b>.</p><p id="p-0074" num="0072">For example, in a case where four data portions D<b>1</b>, D<b>2</b>, D<b>3</b>, and D<b>4</b> on a first page are invalid data, invalid data, valid data, and valid data, respectively, the number of accumulated invalid data portion is incremented by &#x201c;1&#x201d; for the invalid data portion D<b>1</b>. Similarly, the number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data portion D<b>2</b>. Then, the valid data portions D<b>3</b> and D<b>4</b> are copied to the GC buffer <b>32</b>. In practice, since data reading is performed in units of a page, four data portions D<b>1</b>, D<b>2</b>, D<b>3</b>, and D<b>4</b> are read out at the same time. Only the valid data portions D<b>3</b> and D<b>4</b> in these data portions may be accumulated in the GC buffer <b>32</b>. In a case where four data portions D<b>5</b>, D<b>6</b>, D<b>7</b>, and D<b>8</b> on a second page are invalid data, invalid data, invalid data, and valid data, respectively, the number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data portion D<b>5</b>. Similarly, the number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data portion D<b>6</b>. The number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data portion D<b>7</b>. The valid data D<b>8</b> is copied to the GC buffer <b>32</b>.</p><p id="p-0075" num="0073">As described above, since the data reading is performed in units of a page, four data portions D<b>5</b>, D<b>6</b>, D<b>7</b>, and D<b>8</b> are read out at the same time. Only the valid data portion D<b>8</b> in these data portions may be accumulated in the GC buffer <b>32</b>. In a case where four data portions D<b>9</b>, D<b>10</b>, D<b>11</b>, and D<b>12</b> on a third page are valid data, valid data, invalid data, and invalid data, respectively, the valid data portions D<b>9</b> and D<b>10</b> are copied to the GC buffer <b>32</b>. The number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data D<b>11</b>. Similarly, the number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data D<b>12</b>. As described above, since the data reading is performed in units of a page, four data portions D<b>9</b>, D<b>10</b>, D<b>11</b>, and D<b>12</b> are read out at the same time. Only the valid data portions D<b>9</b> and D<b>10</b> in these data portions may be accumulated in the GC buffer <b>32</b>. As described above, the data are processed from the first page to the third page, and the valid data portions D<b>3</b>, D<b>4</b>, D<b>8</b>, D<b>9</b> and D<b>10</b> equal to or more than one page size are accumulated in the GC buffer <b>32</b>. The number of accumulated invalid data portions becomes &#x201c;7&#x201d;.</p><p id="p-0076" num="0074">(2) The valid data portions D<b>3</b>, D<b>4</b>, D<b>8</b>, and D<b>9</b> as many as one page in the valid data portions D<b>3</b>, D<b>4</b>, D<b>8</b>, D<b>9</b>, and D<b>10</b> accumulated in the GC buffer <b>32</b> are copied to the copy destination block <b>512</b> of the garbage collection. In this viewpoint, the garbage collection operation may be temporally stopped, and the data write operation may be performed in the meantime.</p><p id="p-0077" num="0075">(3) During the data write operation, the write data portion is allowed to be received within the range of the number of accumulated invalid data portions. For example, the write data portions W<b>1</b>, W<b>2</b>, . . . , W<b>7</b> as many as the number of accumulated invalid data portions are receivable from the host <b>2</b>. The write data portions W<b>1</b>, W<b>2</b>, . . . , W<b>7</b> are temporally written in the write buffer <b>31</b>. At that time, the number of accumulated invalid data portions is decreased by &#x201c;1&#x201d; for each write data portion that is received. Therefore, when seven write data portions W<b>1</b>, W<b>2</b>, . . . , W<b>7</b> are received and then the number of accumulated invalid data portions would become &#x201c;0&#x201d;.</p><p id="p-0078" num="0076">In a case where the number of accumulated invalid data portion becomes &#x201c;0&#x201d;, new write data are not received from the host <b>2</b>. For example, at this time point, the write operation may be temporally stopped, and the garbage collection operation may be resumed.</p><p id="p-0079" num="0077">(4) In the subsequent garbage collection operation, four data portions D<b>13</b>, D<b>14</b>, D<b>15</b>, and D<b>16</b> on a fourth page are invalid data, valid data, valid data, and valid data, respectively, the number of accumulated invalid data portions is incremented by &#x201c;1&#x201d; for the invalid data portion D<b>13</b>. The valid data portions D<b>14</b>, D<b>15</b>, and D<b>16</b> are copied to the GC buffer <b>32</b>. As described above, since the data reading is performed in units of a page, four data portions D<b>13</b>, D<b>14</b>, D<b>15</b>, and D<b>16</b> are read out at the same time. Only the valid data portions D<b>14</b>, D<b>15</b>, and D<b>16</b> in these data portions may be accumulated in the GC buffer <b>32</b>.</p><p id="p-0080" num="0078">As a result, the valid data portions D<b>10</b>, D<b>14</b>, D<b>15</b>, and D<b>16</b> equal to or more than one page size are accumulated in the GC buffer <b>32</b>. The number of accumulated invalid data portions becomes &#x201c;1&#x201d;.</p><p id="p-0081" num="0079">(5) The valid data portions D<b>10</b>, D<b>14</b>, D<b>15</b>, and D<b>16</b> as many as one page accumulated in the GC buffer <b>32</b> are copied to the copy destination block <b>512</b> of the garbage collection. In this viewpoint, the garbage collection operation may be temporally stopped, and the data write operation may be performed in the meantime.</p><p id="p-0082" num="0080">(6) During the data write operation, the write data portions W<b>1</b>, W<b>2</b>, W<b>3</b>, and W<b>4</b> as many as one page in the write buffer <b>31</b> are written in a write destination block <b>511</b>C.</p><p id="p-0083" num="0081">(7) The write data portion W<b>8</b> as many as the number of accumulated invalid data portions is received from the host <b>2</b>. The write data portion W<b>8</b> is temporally written in the write buffer <b>31</b>. The number of accumulated invalid data portions is decreased by &#x201c;1&#x201d; for the write data portion W<b>8</b>. Therefore, after one write data portion W<b>8</b> is received, the number of accumulated invalid data portion becomes &#x201c;0&#x201d;. For example, at this time point, the write operation may be temporally stopped, or the garbage collection operation may be resumed.</p><p id="p-0084" num="0082">The description will be made with reference to the flowcharts of <figref idref="DRAWINGS">FIGS. <b>5</b> and <b>6</b></figref> about an exemplary procedure of a write process which is performed by the SSD <b>3</b>.</p><p id="p-0085" num="0083">As illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, first, the controller <b>4</b> of the SSD <b>3</b> determines a state of a write destination block, i.e., if the state is any one of a state where an error occurs in the write destination block of the host <b>2</b>, a state where a writing location is at the end of the write destination block, and a state where the write destination block is not yet allocated (Step S<b>101</b>). In a case where it is one of these three states (YES of Step S<b>101</b>), the controller <b>4</b> performs a process of allocating the write destination block of the host <b>2</b> (Step S<b>102</b>).</p><p id="p-0086" num="0084">The description will be made about a procedure of a host write destination block allocation process of Step S<b>102</b> with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>. First, the controller <b>4</b> allocates a free block in the free block group <b>51</b> as the write destination block of the host <b>2</b> (Step S<b>21</b>). The controller <b>4</b> determines whether the number of free blocks remaining in the free block group <b>51</b> is equal to or more than the threshold (Step S<b>22</b>). The threshold is a value equal to or more than &#x201c;1&#x201d; for example. In a case where the number of free blocks remaining in the free block group <b>51</b> is equal to or more than the threshold (YES of Step S<b>22</b>), the controller <b>4</b> sets the number of accumulated invalid data with a value obtained by subtracting the number of data portions accumulated in the write buffer <b>31</b> from the block size which is the number of data portions corresponding to one block (that is, block size&#x2212;the number of accumulated data) (Step S<b>23</b>). The controller <b>4</b> sets &#x201c;1&#x201d; to the number of insufficient blocks (Step S<b>24</b>).</p><p id="p-0087" num="0085">On the other hand, in a case where the number of free blocks remaining in the free block group <b>51</b> is less than the threshold (NO of Step S<b>22</b>), the controller <b>4</b> sets the number of accumulated invalid data portions with a negative value corresponding to the number of data portions accumulated in the write buffer <b>31</b> (that is, &#x2212;(the number of accumulated data portions)) (Step S<b>25</b>). The controller <b>4</b> calculates the number of insufficient blocks (Step S<b>26</b>). The controller <b>4</b> calculates, for example, a value obtained by adding &#x201c;1&#x201d; to the number of bad blocks as the number of insufficient blocks in consideration of the number of bad blocks which are unusable and the number of blocks which are used next time for the writing of the host <b>2</b>. For example, when one bad block is newly generated, the number of insufficient blocks is set to &#x201c;2&#x201d;. When two bad blocks are newly generated at the same time, the number of insufficient blocks is set to &#x201c;3&#x201d;.</p><p id="p-0088" num="0086">As illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, after the procedure of the host write destination block allocation process of Step S<b>102</b> is completed, alternatively, in a case where the state of the write destination block is not any one of a state where an error occurs in the write destination block of the host <b>2</b>, a state where the writing location is at the end of the write destination block, and a state where the write destination block is not yet allocated (NO of Step S<b>101</b>), the controller <b>4</b> determines whether the write data from the host <b>2</b> are accumulated up to the threshold (Step S<b>103</b>). The write data are, for example, data stored in the write buffer <b>31</b>. In a case where the write data are accumulated as much as the threshold (that is, a case where the write data portions equal to or more than the threshold are accumulated in the write buffer <b>31</b>) (YES of Step S<b>103</b>), the controller <b>4</b> writes the accumulated data to the write destination block of the host <b>2</b> (Step S<b>104</b>). The controller <b>4</b> writes, for example, data in units of a page with respect to the write destination block. The controller <b>4</b> determines whether an error in writing occurs (Step S<b>105</b>). In a case where an error occurs in writing (YES of Step S<b>105</b>), the controller <b>4</b> performs a process of allocating the write destination block of the host <b>2</b> (Step S<b>102</b>). The procedure of the host write destination block allocation process is as described above with reference to <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0089" num="0087">In a case where there is no error in writing (NO of Step S<b>105</b>), the controller <b>4</b> updates the LUT <b>33</b> according to the written data (Step S<b>106</b>), and the process proceeds to Step S<b>107</b>. The controller <b>4</b> maps each LBA corresponding to the written data to each physical address on the NAND memory <b>5</b> through the updating of the LUT <b>33</b>.</p><p id="p-0090" num="0088">In a case where the write data are not accumulated as much as the threshold (NO of Step S<b>103</b>), the process proceeds to Step S<b>107</b> without the data accumulated in the write buffer <b>31</b> being written.</p><p id="p-0091" num="0089">In Step S<b>107</b>, the controller <b>4</b> determines whether the number of accumulated invalid data portions is equal to or more than &#x201c;1&#x201d;. In a case where the number of accumulated invalid data portions is not equal to or more than &#x201c;1&#x201d; (NO of Step S<b>107</b>), the controller <b>4</b> bypasses the write process, and performs a garbage collection process. The procedure of the garbage collection process will be described with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0092" num="0090">In a case where the number of accumulated invalid data portions is equal to or more than &#x201c;1&#x201d; (YES of Step S<b>107</b>), the controller <b>4</b> allows reception of data from the host <b>2</b> up to a value obtained by dividing the number of accumulated invalid data portions by the number of insufficient blocks (Step S<b>108</b>). The decimal numbers of the value obtained by dividing the number of accumulated invalid data portions by the number of insufficient blocks is rounded off. Next, the controller <b>4</b> determines whether the host <b>2</b> issues a write request (write command) in which the number of data portions (or the amount of data) within the range of reception allowance is designated (Step S<b>109</b>).</p><p id="p-0093" num="0091">In a case where there is no write request to which the number of data portions (or the amount of data) within the range of reception allowance is designated (NO of Step S<b>109</b>), the controller <b>4</b> bypasses the write process and performs the garbage collection process. Therefore, for example, the garbage collection process is progressed even in a case where the host <b>2</b> is in a host idle state and thus does not issue the write request.</p><p id="p-0094" num="0092">In a case where the host <b>2</b> issues the write request to which the number of data portions (or the amount of data) within the range of reception allowance is designated (YES of Step S<b>109</b>), the controller <b>4</b> returns a response to the write request from the host <b>2</b>, and receives the write data from the host <b>2</b> (Step S<b>110</b>). The received write data are temporally written in the write buffer <b>31</b>. The controller <b>4</b> subtracts a value obtained by multiplying the number of insufficient blocks and the number of received write data from the number of accumulated invalid data (Step S<b>111</b>). The controller <b>4</b> updates the LUT <b>33</b> according to the data temporally written in the write buffer <b>31</b> (Step S<b>112</b>), and the process proceeds to Step S<b>107</b>. The controller <b>4</b> maps the LBA corresponding to the data temporally written in the write buffer <b>31</b> to the physical address on the write buffer <b>31</b> (that is, the DRAM <b>6</b>) by updating the LUT <b>33</b>.</p><p id="p-0095" num="0093">Next, the description will be made with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref> about a procedure of the garbage collection process which is performed by the SSD <b>3</b>.</p><p id="p-0096" num="0094">First, the controller <b>4</b> of the SSD <b>3</b> determines whether the number of free blocks remaining in the free block group <b>51</b> is equal to or more than a threshold (Step S<b>301</b>). The threshold is a value equal to or more than &#x201c;1&#x201d; for example. In a case where the number of free blocks remaining in the free block group <b>51</b> is equal to or more than the threshold (YES of Step S<b>301</b>), the controller <b>4</b> completes the garbage collection process, and performs the write process described above. In a case where the number of free blocks remaining in the free block group <b>51</b> is equal to or more than the threshold, the garbage collection process is not performed. As a result, it is possible to prevent the garbage collection process from being excessively performed.</p><p id="p-0097" num="0095">In a case where the number of free blocks remaining in the free block group <b>51</b> is less than the threshold (NO of Step S<b>301</b>), the controller <b>4</b> determines whether there is a target block of the garbage collection where the non-copied valid data is remaining (Step S<b>302</b>). In a case where there is no target block of the garbage collection where the non-copied valid data is remaining (NO of Step S<b>302</b>), the controller <b>4</b> selects a target block of a new garbage collection (Step S<b>303</b>). In a case where there is the target block of the garbage collection where the non-copied valid data is remaining (YES of Step S<b>302</b>), Step S<b>303</b> is skipped.</p><p id="p-0098" num="0096">Next, the controller <b>4</b> selects one piece of unprocessed data from the target block of the garbage collection (Step S<b>304</b>). The selection of data piece is performed in units of minimum data having the same size as the management size (for example, 4 KB). The controller <b>4</b> determines whether the unprocessed data portion that is selected is valid or invalid (Step S<b>305</b>). In a case where the unprocessed data portion that is selected is not valid (that is, invalid) (NO of Step S<b>305</b>), the controller <b>4</b> adds &#x201c;1&#x201d; to the number of accumulated invalid data portions (Step S<b>306</b>).</p><p id="p-0099" num="0097">On the other hand, in a case where the unprocessed data portion that is selected is the valid data (YES of Step S<b>305</b>), the controller <b>4</b> accumulates the data portion as the data for copying (Step S<b>307</b>). The controller <b>4</b> temporally writes the data portion in a copy buffer <b>32</b> for example. The controller <b>4</b> determines whether the number of accumulated data portions for copying is equal to or more than the amount of one page (Step S<b>308</b>). In a case where the number of accumulated data portions for copying is less than the amount of one page (NO of Step S<b>308</b>), the process returns to Step S<b>302</b>.</p><p id="p-0100" num="0098">In a case where the number of accumulated data portions for copying is equal to or more than the amount of one page (YES of Step S<b>308</b>), the controller <b>4</b> determines whether the copy destination block of the garbage collection is allocated (Step S<b>309</b>). In a case where the copy destination block of the garbage collection is not allocated (NO of Step S<b>309</b>), the controller <b>4</b> allocates one free block in the free block group <b>51</b> as the copy destination block of the garbage collection (Step S<b>310</b>). In a case where the copy destination block of the garbage collection is already allocated (YES of Step S<b>309</b>), Step S<b>310</b> is skipped.</p><p id="p-0101" num="0099">The controller <b>4</b> copies one page of data among the accumulated data for copying into the copy destination block of the garbage collection (Step S<b>311</b>). The controller <b>4</b> updates the LUT <b>33</b> corresponding to the copied data (Step S<b>312</b>). The controller <b>4</b> maps the LBA corresponding to the copied data to the physical address on the copy destination block by updating the LUT <b>33</b>.</p><p id="p-0102" num="0100">Next, the controller <b>4</b> determines whether the update of the LUT <b>33</b> corresponding to all valid data in the target block of the garbage collection is ended (Step S<b>313</b>). In a case where the update of the LUT <b>33</b> corresponding to all the valid data is not ended (NO of Step S<b>313</b>), the controller <b>4</b> starts the write process. In a case where the update of the LUT <b>33</b> corresponding to all valid data is ended (YES of Step S<b>313</b>), the controller <b>4</b> sets the copied target block of the garbage collection to be the free block (Step S<b>314</b>), and starts the write process.</p><p id="p-0103" num="0101">As described above, in the present embodiment, the data portions in the target block of the garbage collection are sequentially selected one by one. When the selected data portion is valid, the data portion is copied to the copy destination block of the GC. When the selected data portion is invalid, &#x201c;1&#x201d; is added to the number of accumulated invalid data portions. The reception of the write data portion from the host <b>2</b> is allowed within the range of the number of accumulated invalid data portions. The number of accumulated invalid data portions is decreased by the number of received write data portions. Therefore, the host write operation and the garbage collection operation can be progressed at the same time only by a simple control of counting the number of invalid data portions which is found out during the garbage collection operation. Therefore, the variation in write latency can be decreased.</p><p id="p-0104" num="0102">The write latency indicates a response time required in writing data. Recently, the amount of data required to be copied for the garbage collection increases as the block size increases. In the present embodiment, a ratio of the host write operation to the garbage collection operation can be controlled in units of data size not in units of a block or a page. In other words, in the present embodiment, the mapping between the LBA and the physical address of the NAND memory <b>5</b> is managed in units of the management size using the LUT <b>33</b> as an address conversion table. The number of accumulated invalid data portions indicates the number of invalid data portions which are found out during the garbage collection operation in units of the management size. As a result, the write latency can be controlled in units of minimum data size (that is, minimum granularity) such as the management size not in units of a block or a page. The host write operation can be performed at a stable latency.</p><p id="p-0105" num="0103">Since the parameters (for example, overprovisioning size and the like) other than the number of left free blocks and the number of accumulated invalid data are not required, a stable operation can be made even when the size of overprovisioning is dynamically changed.</p><p id="p-0106" num="0104">Furthermore, even in a case where the blocks are different in size, the write latency can be stably controlled in units of the minimum data size (that is, minimum granularity) such as the management size, for example.</p><p id="p-0107" num="0105"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example of the data write operation and the garbage collection operation that are concurrently carried out in a case where the SSD <b>3</b> has a plurality of namespaces managed therein. The namespace management section <b>24</b> can manage a plurality of logical address spaces (LBA spaces) corresponding to the plurality of namespaces in order to handle the SSD <b>3</b> as a plurality of drives. In the respective namespaces, LBA ranges (LBA<b>0</b> to LBAn&#x2212;1) are allocated. The size of the LBA range (that is, the number of LBAs) may vary depending on the namespace. Each LBA range starts from LBA<b>0</b>. A designated number of blocks are allocated for each namespace. The number of allocated blocks may be different depending on the namespaces. The namespace management section <b>24</b> may separately manage the mapping between the LBA and the physical address in each namespace using a different LUT for each namespace. For example, in the case of the namespace (first area) of NSID #1, the controller <b>4</b> manages the mapping between the LBA of the namespace of NSID #1 and the physical address of the first area in units of the management size using the LUT for the namespace of NSID #1. In the case of the namespace (n-th area) of NSID #n, the controller <b>4</b> manages the mapping between the LBA of the namespace of NSID #n and the physical address of the n-th area in units of the management size using the LUT for the namespace of NSID #n.</p><p id="p-0108" num="0106">The free blocks of the NAND memory <b>5</b> are managed by a common free block pool <b>51</b>, and some blocks are allocated to the namespace of NSID #1 from the common free block pool (free block group) <b>51</b>. These allocated blocks are used to store the data associated with the namespace of NSID #1. In other words, these blocks are allocated to the namespace of NSID #1 as blocks for storing the data associated with the namespace of NSID #1. The namespace of NSID #n is also similarly allocated with the blocks. In this way, the NAND memory <b>5</b> is logically divided into a plurality of areas corresponding to the namespaces of NSID #1 to NSID #n.</p><p id="p-0109" num="0107">The SSD <b>3</b> includes a separate virtual flash pool for each namespace. A virtual flash pool <b>6</b>-<b>1</b> is used for managing a physical resource amount (total number of allocated blocks) allocated (reserved) for the namespace of NSID #1. Similarly, a virtual flash pool <b>6</b>-<i>n </i>is used for managing a physical resource amount (total number of allocated blocks) allocated (reserved) for the namespace of NSID #n. In this case, there is no need to consider which block should be allocated (reserved), but only the number of allocating (reserving) blocks are managed by each virtual flash pool.</p><p id="p-0110" num="0108">Each free block is managed by the common free block pool <b>51</b> which is shared by the plurality of namespaces. A block returned from the virtual flash pool of each namespace is managed by the common free block pool <b>51</b>.</p><p id="p-0111" num="0109">For each of the plurality of namespaces, the data write operation and the garbage collection operation are performed as described above with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0112" num="0110">First, the garbage collection operation for the namespace of NSID #1 will be described.</p><p id="p-0113" num="0111">The block select section <b>211</b> selects a target block (GC target block) <b>551</b>-<b>1</b> of the garbage collection from an active block pool (active block group) <b>55</b>-<b>1</b>. The data copy control section <b>212</b> sequentially selects the data portions in the GC target block <b>551</b>-<b>1</b>, and writes the selected data portion in a GC copy destination block <b>512</b>-<b>1</b> when the selected data portion is the valid data. When the selected data portion is the invalid data, the accumulated invalid data count measurement section <b>213</b> adds &#x201c;1&#x201d; to the number of accumulated invalid data of the namespace of NSID #1.</p><p id="p-0114" num="0112">The GC copy destination block <b>512</b>-<b>1</b> is a block allocated from the common free block pool <b>51</b>. In a case where the data are written up to the end of the allocated GC copy destination block <b>512</b>-<b>1</b>, the data copy control section <b>212</b> moves the GC copy destination block <b>512</b>-<b>1</b> to the active block pool <b>55</b>-<b>1</b>.</p><p id="p-0115" num="0113">In a case where all valid data in the GC target block <b>551</b>-<b>1</b> are written in the GC copy destination block <b>512</b>-<b>1</b>, the data copy control section <b>212</b> re-categorizes the GC target block <b>551</b>-<b>1</b> as a free block in the common free block pool <b>51</b>.</p><p id="p-0116" num="0000">Next, the write operation according to the write request designating the namespace of NSID #1 from the host <b>2</b> will be described.</p><p id="p-0117" num="0114">The write request (write command) containing NSID #1 is sent to the write control section <b>22</b> for the namespace of NSID #1. The write control section <b>22</b> allows the reception of the write data portion associated with the namespace of NSID #1 within the range of the number of accumulated invalid data portion of the namespace of NSID #1. In other words, the write control section <b>22</b> can receive the write data portion associated with the namespace of NSID #1 from the host <b>2</b> up to the number of accumulated invalid data portions. The write control section <b>22</b> temporally writes the received write data in a write buffer <b>31</b>-<b>1</b>. The accumulated invalid data count measurement section <b>213</b> subtracts the number of write data portions received from the host <b>2</b> from the number of accumulated invalid data portions for the namespace of NSID #1. The write control section <b>22</b> writes the data written in the write buffer <b>31</b>-<b>1</b> into a write destination block <b>511</b>-<b>1</b> of the host <b>2</b> for the namespace of NSID #1 in units of a page for example.</p><p id="p-0118" num="0115">The write control section <b>22</b> allocates one free block in the free block pool <b>51</b> as a new write destination block <b>511</b>-<b>1</b> in a case where the write destination block <b>511</b>-<b>1</b> of the host <b>2</b> for the namespace of NSID #1 is not yet allocated, in a case where the data are written up to the end of the allocated write destination block <b>511</b>-<b>1</b>, or in a case where an error occurs in the write destination block <b>511</b>-<b>1</b>. In a case where the data are written up to the end of the allocated write destination block <b>511</b>-<b>1</b>, the write control section <b>22</b> re-categorizes the write destination block <b>511</b>-<b>1</b> as an active block in the active block pool <b>55</b>-<b>1</b>.</p><p id="p-0119" num="0116">The accumulated invalid data count measurement section <b>213</b> determines whether the number of remaining blocks allocable for the namespace of NSID #1 is equal to or more than the threshold when the write destination block <b>511</b>-<b>1</b> of the host <b>2</b> is newly allocated. The number of remaining blocks is obtained by subtracting the number of used blocks (the number of blocks which are used for the namespace of NSID #1) from a total number of blocks allocated (reserved) for the namespace of NSID #1. In a case where the number of remaining blocks is equal to or more than the threshold, the accumulated invalid data count measurement section <b>213</b> adds the number of data portions writable to the newly-allocated write destination block <b>511</b>-<b>1</b> to the number of accumulated invalid data portions for the namespace of NSID #1. The number of data portions writable to the write destination block <b>511</b>-<b>1</b> is the number of data portions corresponding to the capacity of the block. Furthermore, in a case where there are the data portions which are currently accumulated in the write buffer <b>31</b>-<b>1</b>, the accumulated invalid data count measurement section <b>213</b> may subtract the number of data portions from the number of accumulated invalid data portions for the namespace of NSID #1. In a case where the number of remaining blocks allocable for the namespace of NSID #1 is less than the threshold, the accumulated invalid data count measurement section <b>213</b> sets &#x201c;0&#x201d; to the number of accumulated invalid data portions for the namespace of NSID #1. Furthermore, in a case where there are data portions which are currently accumulated in the write buffer <b>31</b>-<b>1</b>, the accumulated invalid data count measurement section <b>213</b> may subtract the number of data portions from the number of accumulated invalid data portions for the namespace of NSID #1.</p><p id="p-0120" num="0117">Similarly, the garbage collection operation for the namespace of NSID #n and the write operation according to the write request for the namespace of NSID #n from the host <b>2</b> are performed.</p><p id="p-0121" num="0118">The description will be made with reference to the flowcharts of <figref idref="DRAWINGS">FIGS. <b>9</b> and <b>10</b></figref> about an exemplary procedure of the write process performed by the SSD <b>3</b> in which a plurality of areas is set for the plurality of namespaces. In the following, the description will be made about the write process to one of the plurality of namespaces in order to help with understanding. The controller <b>4</b> can perform the similar write processes in parallel with respect to the plurality of namespaces.</p><p id="p-0122" num="0119">As illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, first, the controller <b>4</b> of the SSD <b>3</b> determines the state of the write destination block, i.e., if the state is any one of a state where an error occurs in the write destination block from the host <b>2</b> to a certain namespace, a state where the writing location is at the end of the write destination block, and a state where the write destination block is not yet allocated (Step S<b>401</b>). In a case where the state is any one of these three states (YES of Step S<b>401</b>), the controller <b>4</b> performs a process of allocating the write destination block from the host <b>2</b> to the namespace (Step S<b>402</b>).</p><p id="p-0123" num="0120">The procedure of the host write destination block allocation process of Step S<b>402</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. First, the controller <b>4</b> allocates the free block in the free block group <b>51</b> as the write destination block to the namespace of the host <b>2</b> (Step S<b>51</b>). The controller <b>4</b> determines whether the number of remaining blocks obtained by subtracting the number of used blocks indicating the number of blocks used in the namespace from a total number of blocks allocated for the namespace is equal to or more than the threshold (Step S<b>52</b>). The threshold is a value equal to or more than &#x201c;1&#x201d; for example. In a case where the number of remaining blocks is equal to or more than the threshold (YES of Step S<b>52</b>), the controller <b>4</b> sets the number of accumulated invalid data portions for the namespace to a value obtained by subtracting the number of data portions accumulated in the write buffer <b>31</b> from the block size (block size&#x2212;the number of accumulated data portions), that is, the number of data portions corresponding to one block (Step S<b>53</b>). The controller <b>4</b> sets &#x201c;1&#x201d; to the number of insufficient blocks of the namespace (Step S<b>54</b>).</p><p id="p-0124" num="0121">On the other hand, in a case where the number of free blocks remaining in the free block group <b>51</b> is less than the threshold (NO of Step S<b>52</b>), the controller <b>4</b> sets the number of accumulated invalid data portions for the namespace to a negative value (that is, &#x2212;(the number of accumulated data portions)) corresponding to the number of data portions accumulated in the write buffer <b>31</b> (Step S<b>55</b>). The controller <b>4</b> calculates the number of insufficient blocks of the namespace (Step S<b>56</b>). The controller <b>4</b> calculates, for example, a value obtained by adding &#x201c;1&#x201d; to the number of bad blocks as the number of insufficient blocks in consideration of the number of bad blocks which are unusable and the number of blocks which are used next time for the writing of the host <b>2</b>. For example, when one bad block is newly generated, the number of insufficient blocks is set to &#x201c;2&#x201d;. When two bad blocks are newly generated at the same time, the number of insufficient blocks is set to &#x201c;3&#x201d;.</p><p id="p-0125" num="0122">As illustrated in the flowchart of <figref idref="DRAWINGS">FIG. <b>9</b></figref>, after the procedure of the host write destination block allocation process of Step S<b>402</b> is completed, or in a case where the state of the write destination block does not correspond to any one of the state where an error occurs in the write destination block of the host <b>2</b> in a certain namespace, the state where the writing location is at the end of the write destination block, and the state where the write destination block is not yet allocated (NO of Step S<b>401</b>), the controller <b>4</b> determines whether or not the write data from the host <b>2</b> to the namespace is accumulated as much as the threshold (Step S<b>403</b>). The write data are, for example, data stored in the write buffer <b>31</b>. In a case where the write data are accumulated as much as the threshold (that is, a case where the write data portions equal to or more than the threshold are accumulated in the write buffer <b>31</b> (YES of Step S<b>403</b>), the controller <b>4</b> writes the accumulated data in the write destination block of the host <b>2</b> for the namespace (Step S<b>404</b>). The controller <b>4</b> writes, for example, data in units of a page with respect to the write destination block. The controller <b>4</b> determines whether an error occurs during writing data (Step S<b>405</b>). In a case where an error occurs (YES of Step S<b>405</b>), the controller <b>4</b> performs a process of allocating the write destination block of the host <b>2</b> (Step S<b>402</b>). The procedure of the host write destination block allocation process is the same as described above with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0126" num="0123">In a case where an error does not occur (NO of Step S<b>405</b>), the controller <b>4</b> updates the LUT <b>33</b> of the namespace according to the written data (Step S<b>406</b>), and the process proceeds to Step S<b>407</b>. The controller <b>4</b> maps the LBA corresponding to the data written in the area of the namespace to the physical address on the NAND memory <b>5</b> by updating the LUT <b>33</b> for the namespace.</p><p id="p-0127" num="0124">In a case where the write data are not accumulated as much as the threshold (NO of Step S<b>403</b>), the process of writing the data accumulated in the write buffer <b>31</b> is not performed, and the process proceeds to Step S<b>407</b>.</p><p id="p-0128" num="0000">In Step S<b>407</b>, the controller <b>4</b> determines whether or not the number of accumulated invalid data portions for the namespace is equal to or more than &#x201c;1&#x201d;. In a case where the number of accumulated invalid data portions for the namespace is not equal to or more than &#x201c;1&#x201d; (NO of Step S<b>407</b>), the controller <b>4</b> bypasses the write process and performs the garbage collection process. The procedure of the garbage collection process will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0129" num="0125">In a case where the number of accumulated invalid data portions for the namespace is equal to or more than &#x201c;1&#x201d; (YES of Step S<b>407</b>), the controller <b>4</b> allows reception of data from the host <b>2</b> up to a value obtained by dividing the number of accumulated invalid data portions for the namespace by the number of insufficient blocks (Step S<b>408</b>). The decimal numbers of the value obtained by dividing the number of accumulated invalid data portions by the number of insufficient blocks is rounded off. Next, the controller <b>4</b> determines whether there is a write request (write command) from the host <b>2</b> in which the number of data (or the quantity of data) portions within the range of reception allowance is designated with respect to the namespace (Step S<b>409</b>).</p><p id="p-0130" num="0126">In a case where there is no write request in which the number of data portions (or the quantity of data) within the range of reception allowance is designated with respect to the namespace (NO of Step S<b>409</b>), the controller <b>4</b> bypasses the write process and performs the garbage collection process. Therefore, for example, the garbage collection process is progressed even in a case where the host <b>2</b> is in a host idle state and thus does not issue the write request.</p><p id="p-0131" num="0127">In a case where there is a write request from the host <b>2</b> in which the number of data portions (or the quantity of data) within the range of reception allowance is designated with respect to the namespace (YES of Step S<b>409</b>), the controller <b>4</b> returns a response to the write request from the host <b>2</b>, and receives the write data with respect to the namespace from the host <b>2</b> (Step S<b>410</b>). The received write data are temporally written in the write buffer <b>31</b> for every namespace. The controller <b>4</b> subtracts a value obtained by multiplying the number of insufficient blocks to the number of received write data portions from the number of accumulated invalid data portions for the namespace (Step S<b>411</b>). The controller <b>4</b> updates the LUT <b>33</b> for the namespace according to the data temporally written in the write buffer <b>31</b> (Step S<b>412</b>), and the process proceeds to Step S<b>407</b>. The controller <b>4</b> maps the LBA corresponding to the data temporally written in the write buffer <b>31</b> to the physical address on the write buffer <b>31</b> (that is, the DRAM <b>6</b>) by updating the LUT <b>33</b> of the namespace. Next, the description will be made with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref> about an exemplary procedure of the garbage collection process performed by the SSD <b>3</b>.</p><p id="p-0132" num="0128">First, the controller <b>4</b> of the SSD <b>3</b> determines whether or not the number of remaining blocks, which is obtained by subtracting the number of used blocks, i.e., the number of blocks used in the namespace, from a total number of blocks allocated for the namespace is equal to or more than the threshold (Step S<b>601</b>). The threshold is a value equal to or more than &#x201c;1&#x201d; for example. In a case where the number of remaining blocks is equal to or more than the threshold (YES of Step S<b>601</b>), the controller <b>4</b> ends the garbage collection process and performs the write process described above. In a case where the number of remaining blocks is equal to or more than the threshold, the garbage collection process is not performed. As a result, it is possible to prevent the garbage collection process from being excessively performed.</p><p id="p-0133" num="0129">In a case where the number of remaining blocks is less than the threshold (NO of Step S<b>601</b>), the controller <b>4</b> determines whether or not there is a target block of the garbage collection for the namespace where non-copied valid data are remaining (Step S<b>602</b>). In a case where there is no target block of the garbage collection for the namespace where the non-copied valid data is remaining (NO of Step S<b>602</b>), the controller <b>4</b> selects a target block of a new garbage collection for the namespace (Step S<b>603</b>). In a case where there is a target block of the garbage collection for the namespace where the non-copied valid data is remaining (YES of Step S<b>602</b>), the procedure of Step S<b>603</b> is skipped.</p><p id="p-0134" num="0130">Next, the controller <b>4</b> selects one piece of the non-processed data from the target block of the garbage collection (Step S<b>604</b>). The selection of data is performed in units of a minimum data size that is the same as the management size (for example, 4 KB). The controller <b>4</b> determines whether or not the non-processed data portion that is selected is valid or invalid (Step S<b>605</b>). In a case where the non-processed data portion that is selected is not valid (that is, invalid) (NO of Step S<b>605</b>), the controller <b>4</b> adds &#x201c;1&#x201d; to the number of accumulated invalid data portions for the namespace (Step S<b>606</b>).</p><p id="p-0135" num="0131">On the other hand, in a case where the non-processed data portion that is selected is valid (YES of Step S<b>605</b>), the controller <b>4</b> accumulates the data portion as the data for copying (Step S<b>607</b>). The controller <b>4</b> temporally writes the data portion in the copy buffer <b>32</b> for example. The controller <b>4</b> determines whether or not the number of accumulated data portions for copying is equal to or more than one page in the namespace (Step S<b>608</b>). In a case where the number of accumulated data portions for copying is less than one page (NO of Step S<b>608</b>), the process returns to Step S<b>602</b>.</p><p id="p-0136" num="0132">In a case where the number of accumulated data portions for copying is equal to or more than one page (YES of Step S<b>608</b>), the controller <b>4</b> determines whether or not the copy destination block of the garbage collection for the namespace is allocated (Step S<b>609</b>). In a case where the copy destination block of the garbage collection is not allocated (NO of Step S<b>609</b>), the controller <b>4</b> allocates one free block of the free block group <b>51</b> as the copy destination block of the garbage collection for the namespace (Step S<b>610</b>). In a case where the copy destination block of the garbage collection is already allocated (YES of Step S<b>609</b>), the procedure of Step S<b>610</b> is skipped.</p><p id="p-0137" num="0133">The controller <b>4</b> copies data as much as one page among the accumulated data portions for copying to the copy destination block of the garbage collection for the namespace (Step S<b>611</b>). The controller <b>4</b> updates the LUT <b>33</b> corresponding to the copied data for the namespace (Step S<b>612</b>). The controller <b>4</b> maps the LBA corresponding to the copied data to the physical address on the copy destination block in the namespace by updating the LUT <b>33</b> for the namespace.</p><p id="p-0138" num="0134">Next, the controller <b>4</b> determines whether update of the LUT <b>33</b> corresponding to all valid data in the target block of the garbage collection is ended (Step S<b>613</b>). In a case where the update of the LUT <b>33</b> corresponding to all valid data is not ended (NO of Step S<b>613</b>), the controller <b>4</b> starts the write process. In a case where the update of the LUT <b>33</b> corresponding to all valid data is ended (YES of Step S<b>613</b>), the controller <b>4</b> sets the copied target block of the garbage collection to the free block (Step S<b>614</b>), and starts the write process.</p><p id="p-0139" num="0135">As described above, also in a case where a plurality of areas is provided for the plurality of namespaces in the SSD <b>3</b>, the host write operation and the garbage collection operation can be processed at the same time by a simple control of counting the number of invalid data portions found out during the garbage collection operation for each namespace. As a result, it is possible to decrease a variation in write latency.</p><p id="p-0140" num="0136"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates another example of the data write operation and the garbage collection operation in a case where the SSD <b>3</b> supports the plurality of namespaces. For each of the plurality of namespaces, the garbage collection operation is performed as described above with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. That is, the number of accumulated invalid data portions is calculated for each namespace.</p><p id="p-0141" num="0137">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the write control section <b>22</b> calculates a total sum of the numbers of accumulated invalid data portions for the plurality of namespaces, and allows reception of the write data portion from the host <b>2</b> within the range of the calculated total sum. In other words, the write control section <b>22</b> can receive the write data portions from the host <b>2</b> by the total sum (upper limit) of the numbers of accumulated invalid data portions for the plurality of namespaces. The write control section <b>22</b> temporally writes the received write data from the host <b>2</b> in the write buffer <b>31</b>. The accumulated invalid data count measurement section <b>213</b> may subtract the number of data portions received from the host <b>2</b> from the total sum of the number of accumulated invalid data portions. The write control section <b>22</b> writes the data written in the write buffer <b>31</b> in the write destination blocks <b>511</b>-<b>1</b> to <b>511</b>-<i>n </i>of the host <b>2</b> for the namespace designated by the write request, for example, in units of a page.</p><p id="p-0142" num="0138">In a case where the number of free blocks insufficient in the memory system is equal to or more than two, the write control section <b>22</b> may allow reception of the write data portion from the host <b>2</b> within the range of a value obtained by dividing the total sum of the numbers of accumulated invalid data portions by the number of insufficient free blocks.</p><p id="p-0143" num="0000"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an exemplary hardware configuration of the information processing device serving as the host <b>2</b>.<br/>The information processing device is formed as a server computer or a personal computer. The information processing device includes a processor (CPU) <b>101</b>, a main memory <b>102</b>, a BIOS-ROM <b>103</b>, a network controller <b>105</b>, a peripheral interface controller <b>106</b>, a controller <b>107</b>, an embedded controller (EC) <b>108</b>, and the like.</p><p id="p-0144" num="0139">The processor <b>101</b> is a CPU configured to control operations of the respective components of the information processing device. The processor <b>101</b> executes various software programs loaded from any one of the plurality of SSDs <b>3</b> to the main memory <b>102</b>. The main memory <b>102</b> is configured with a random access memory such as a DRAM. The software program executed by the processor <b>101</b> includes an application software layer <b>41</b> described above, an OS <b>42</b>, and a file system <b>43</b>.</p><p id="p-0145" num="0140">The processor <b>101</b> executes also a basic input/output system (BIOS) stored in the BIOS-ROM (nonvolatile memory) <b>103</b>. The BIOS is a system program for a hardware control.</p><p id="p-0146" num="0000">The network controller <b>105</b> is a communication device such as a wired LAN controller and a wireless LAN controller. The peripheral interface controller <b>106</b> is configured to perform communication with the peripheral devices such as a USB device.</p><p id="p-0147" num="0141">The controller <b>107</b> is configured to perform the communication with the devices connected to the plurality of connectors <b>107</b>A. In the present embodiment, the plurality of SSDs <b>3</b> are connected to the plurality of connectors <b>107</b>A. Examples of the controller <b>107</b> include an SAS expander, a PCIe Switch, a PCIe expander, a flash array controller, or a RAID controller.</p><p id="p-0148" num="0142">An EC <b>108</b> serves as a system controller configured to perform power management of the information processing device. The EC <b>108</b> turns on and off the power of the information processing device according to a user's operation on a power switch. The EC <b>108</b> is formed as a processing circuit such as a one-chip micro controller. The EC <b>108</b> may include a keyboard controller which controls an input device such as a keyboard (KB).</p><p id="p-0149" num="0143"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates an exemplary configuration of the information processing device which includes the plurality of SSDs <b>3</b> and the host <b>2</b>.</p><p id="p-0150" num="0000">The information processing device includes a housing <b>201</b> of a thin box shape which is stored in a rack. A number of SSDs <b>3</b> may be disposed in the housing <b>201</b>. In this case, the SSDs <b>3</b> may be detachably inserted into slots provided in a front surface <b>201</b>A of the housing <b>201</b>.</p><p id="p-0151" num="0144">A system board (mother board) <b>202</b> is disposed in the housing <b>201</b>. On the system board (mother board) <b>202</b>, various electronic components including the CPU <b>101</b>, the memory <b>102</b>, the network controller <b>105</b>, and the controller <b>107</b> are mounted. These electronic components serve as the host <b>2</b>.</p><p id="p-0152" num="0145">As described above, a variation in write latency can be reduced according to the present embodiment. The memory system according to the present embodiment includes the NAND memory (nonvolatile memory) <b>5</b>, and the controller <b>4</b> which is electrically connected to the NAND memory <b>5</b>. The controller <b>4</b> sequentially selects data portions in the GC target block <b>551</b> which is selected as the target block of the garbage collection from the NAND memory <b>5</b>. When the selected data portion is valid, the controller <b>4</b> copies the data portion from the free block group <b>51</b> in the NAND memory <b>5</b> to the GC copy destination block <b>512</b> allocated as the copy destination block for the garbage collection. When the selected data portion is invalid, the controller <b>4</b> performs the garbage collection operation in which the number of accumulated invalid data portions, which indicates the number of invalid data portions found out during the garbage collection operation, is incremented by &#x201c;1&#x201d;. The controller <b>4</b> allows reception of the write data portion from the host <b>2</b> within the range of the number of accumulated invalid data portions, and decreases the number of accumulated invalid data portions by the number of received write data portions.</p><p id="p-0153" num="0146">As a result, the host write operation and the garbage collection operation can be processed at the same time by a simple control of counting the number of invalid data portions which is found out during the garbage collection operation. Therefore, the variation in write latency can be decreased.</p><p id="p-0154" num="0147">In the above embodiment, the NAND memory is exemplified as the nonvolatile memory. However, the function of this embodiment is applicable to other various nonvolatile memories such as an MRAM (Magnetoresistive Random Access Memory), a PRAM (Phase change Random Access Memory), a ReRAM (Resistive Random Access Memory), and a FeRAM (Ferroelectric Random Access Memory).</p><p id="p-0155" num="0148">While certain embodiments have been described, these embodiments have been presented by way of example only, and are not intended to limit the scope of the inventions. Indeed, the novel embodiments described herein may be embodied in a variety of other forms; furthermore, various omissions, substitutions and changes in the form of the embodiments described herein may be made without departing from the spirit of the inventions. The accompanying claims and their equivalents are intended to cover such forms or modifications as would fall within the scope and spirit of the inventions.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A memory system comprising:<claim-text>a buffer;</claim-text><claim-text>a nonvolatile memory that includes a plurality of blocks, each of the plurality of blocks being a unit of a data erase operation, the plurality of blocks including at least a first block and a second block; and</claim-text><claim-text>a controller electrically connected to the nonvolatile memory and configured to,</claim-text><claim-text>in executing a garbage collection operation on the nonvolatile memory,<claim-text>sequentially select data portions stored in the first block,</claim-text><claim-text>in a case where a first data portion that is one of the sequentially selected data portions includes valid data, copy the first data portion from the first block to the second block, and</claim-text><claim-text>in a case where a second data portion that is one of the sequentially selected data portions includes invalid data, increment an invalid data counter by an amount of the invalid data included in the second data portion that is detected during the garbage collection operation, and</claim-text></claim-text><claim-text>in parallel with the garbage collection operation,<claim-text>allow write data from a host to be received in the buffer up to an amount indicated by the invalid data counter, and</claim-text><claim-text>decrease the invalid data counter by an amount of the received write data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The memory system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the controller is further configured to manage mapping between a logical block address and a physical address of the nonvolatile memory in a unit of management size, and</claim-text><claim-text>the amount of the invalid data tracked by the invalid data counter is indicated in a unit of the management size.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The memory system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the controller is further configured to:</claim-text><claim-text>manage, by using a free block pool, a plurality of free blocks among the plurality of blocks;</claim-text><claim-text>allocate one or more write destination blocks from the free block pool to write the write data received from the host; and</claim-text><claim-text>in a case where the number of the allocated one or more write destination blocks is equal to or greater than a threshold, increment the invalid data counter by an amount of data which is writable to the allocated one or more write destination blocks.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The memory system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the controller is configured to execute the garbage collection operation in a case where the number of free blocks managed by using the free block pool is less than the threshold.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The memory system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein<claim-text>the controller is further configured to:</claim-text><claim-text>invalidate the first data portion after the first data portion is copied from the first block to the second block; and</claim-text><claim-text>return the first block to the free block pool in a case where the first block stores no valid data.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The memory system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the plurality of blocks further includes a third block, a fourth block, a fifth block, and a sixth block, and<claim-text>the controller is further configured to,</claim-text><claim-text>logically divide the nonvolatile memory into a first area for writing data associated with a first namespace and a second area for writing data associated with a second namespace, the first area including at least the third block and the fourth block, the second including at least the fifth block and the sixth block,</claim-text><claim-text>in executing a first garbage collection operation for the garbage collection operation on the first area,<claim-text>sequentially select data portions stored in the third block,</claim-text><claim-text>in a case where a third data portion that is one of the sequentially selected data portions includes valid data, copy the third data portion from the third block to the fourth block, and</claim-text><claim-text>in a case where a fourth data portion that is one of the sequentially selected data portions includes invalid data, increment a first invalid data counter by an amount of the invalid data included in the fourth data portion that is detected during the first garbage collection operation,</claim-text></claim-text><claim-text>in parallel with the first garbage collection operation,<claim-text>allow write data associated with the first namespace from the host to be received in the buffer up to an amount indicated by the first invalid data counter, and</claim-text><claim-text>decrease the first invalid data counter by an amount of the received write data associated with the first namespace,</claim-text></claim-text><claim-text>in executing a second garbage collection operation for the garbage collection operation on the second area,<claim-text>sequentially select data portions stored in the fifth block,</claim-text><claim-text>in a case where a fifth data portion that is one of the sequentially selected data portions includes valid data, copy the fifth data portion from the fifth block to the sixth block, and</claim-text><claim-text>in a case where a sixth data portion that is one of the sequentially selected data portions includes invalid data, increment a second invalid data counter by an amount of the invalid data included in the sixth data portion that is detected during the second garbage collection operation, and</claim-text></claim-text><claim-text>in parallel with the second garbage collection operation,<claim-text>allow write data associated with the second namespace from the host to be received in the buffer up to an amount indicated by the second invalid data counter, and</claim-text><claim-text>decrease the second invalid data counter by an amount of the received write data associated with the second namespace.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The memory system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the plurality of blocks further includes a third block, a fourth block, a fifth block, and a sixth block, and<claim-text>the controller is further configured to:</claim-text><claim-text>logically divide the nonvolatile memory into a first area for writing data associated with a first namespace and a second area for writing data associated with a second namespace, the first area including at least the third block and the fourth block, the second including at least the fifth block and the sixth block,</claim-text><claim-text>in executing a first garbage collection operation for the garbage collection operation on the first area,<claim-text>sequentially select data portions stored in the third block,</claim-text><claim-text>in a case where a third data portion that is one of the sequentially selected data portions includes valid data, copy the third data portion from the third block to the fourth block, and</claim-text><claim-text>in a case where a fourth data portion that is one of the sequentially selected data portions includes invalid data, increment a first invalid data counter by an amount of the invalid data included in the fourth data portion that is detected during the first garbage collection operation,</claim-text></claim-text><claim-text>in executing a second garbage collection operation for the garbage collection operation on the second area,<claim-text>sequentially select data portions stored in the fifth block,</claim-text><claim-text>in a case where a fifth data portion that is one of the sequentially selected data portions includes valid data, copy the fifth data portion from the fifth block to the sixth block, and</claim-text><claim-text>in a case where a sixth data portion that is one of the sequentially selected data portions includes invalid data, increment a second invalid data counter by an amount of the invalid data included in the sixth data portion that is detected during the second garbage collection operation, and</claim-text></claim-text><claim-text>in parallel with the first garbage collection operation and the second garbage collection operation,<claim-text>allow write data from the host to be received in the buffer up to an amount indicated by a sum of the first invalid data counter and the second invalid data counter.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The memory system according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the controller is further configured to, in a case where the number of insufficient free blocks in the nonvolatile memory is equal to or more than two, allow the write data portion from the host to be received in the buffer up to an amount obtained by dividing the amount indicated by the sum of the first invalid data counter and the second invalid data counter by the number of the insufficient free blocks.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method of controlling a nonvolatile memory, the nonvolatile memory including a plurality of blocks, each of the plurality of blocks being a unit of a data erase operation, the plurality of blocks including at least a first block and a second block, said method comprising:<claim-text>in executing a garbage collection operation on the nonvolatile memory,<claim-text>sequentially selecting data portions stored in the first block,</claim-text><claim-text>in a case where a first data portion that is one of the sequentially selected data portions includes valid data,</claim-text><claim-text>copying the first data portion from the first block to the second block, and</claim-text><claim-text>in a case where a second data portion that is one of the sequentially selected data portions includes invalid data, incrementing an invalid data counter by an amount of the invalid data included in the second data portion that is detected during the garbage collection operation; and</claim-text></claim-text><claim-text>in parallel with the garbage collection operation,<claim-text>allowing write data from a host to be received in a buffer up to an amount indicated by the invalid data counter, and</claim-text><claim-text>decreasing the invalid data counter by an amount of the received write data.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>managing mapping between a logical block address and a physical address of the nonvolatile memory in a unit of management size, wherein</claim-text><claim-text>the amount of the invalid data tracked by the invalid data counter is indicated in a unit of the management size.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, further comprising:<claim-text>managing, by using a free block pool, a plurality of free blocks among the plurality of blocks;</claim-text><claim-text>allocating one or more write destination blocks from the free block pool to write the write data received from the host; and</claim-text><claim-text>in a case where the number of the allocated one or more write destination blocks is equal to or greater than a threshold, incrementing the invalid data counter by an amount of data which is writable to the allocated one or more write destination blocks.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising;<claim-text>executing the garbage collection operation in a case where the number of free blocks managed by using the free block pool is less than the threshold.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, further comprising:<claim-text>invalidating the first data portion after the first data portion is copied from the first block to the second block; and</claim-text><claim-text>returning the first block to the free block pool in a case where the first block stores no valid data.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the plurality of blocks further includes a third block, a fourth block, a fifth block, and a sixth block, and<claim-text>the method further comprises:</claim-text><claim-text>logically dividing the nonvolatile memory into a first area for writing data associated with a first namespace and a second area for writing data associated with a second namespace, the first area including at least the third block and the fourth block, the second including at least the fifth block and the sixth block,</claim-text><claim-text>in executing a first garbage collection operation for the garbage collection operation on the first area,<claim-text>sequentially selecting data portions stored in the third block,</claim-text><claim-text>in a case where a third data portion that is one of the sequentially selected data portions includes valid data, copying the third data portion from the third block to the fourth block, and</claim-text><claim-text>in a case where a fourth data portion that is one of the sequentially selected data portions includes invalid data, incrementing a first invalid data counter by an amount of the invalid data included in the fourth data portion that is detected during the first garbage collection operation,</claim-text></claim-text><claim-text>in parallel with the first garbage collection operation,<claim-text>allowing write data associated with the first namespace from the host to be received in the buffer up to an amount indicated by the first invalid data counter, and</claim-text><claim-text>decreasing the first invalid data counter by an amount of the received write data associated with the first namespace,</claim-text></claim-text><claim-text>in executing a second garbage collection operation for the garbage collection operation on the second area,<claim-text>sequentially selecting data portions stored in the fifth block,</claim-text><claim-text>in a case where a fifth data portion that is one of the sequentially selected data portions includes valid data, copying the fifth data portion from the fifth block to the sixth block, and</claim-text><claim-text>in a case where a sixth data portion that is one of the sequentially selected data portions includes invalid data, incrementing a second invalid data counter by an amount of the invalid data included in the sixth data portion that is detected during the second garbage collection operation, and</claim-text></claim-text><claim-text>in parallel with the second garbage collection operation,<claim-text>allowing write data associated with the second namespace from the host to be received in the buffer up to an amount indicated by the second invalid data counter, and</claim-text><claim-text>decreasing the second invalid data counter by an amount of the received write data associated with the second namespace.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein<claim-text>the plurality of blocks further includes a third block, a fourth block, a fifth block, and a sixth block, and<claim-text>the method further comprises:</claim-text><claim-text>logically dividing the nonvolatile memory into a first area for writing data associated with a first namespace and a second area for writing data associated with a second namespace, the first area including at least the third block and the fourth block, the second including at least the fifth block and the sixth block,</claim-text><claim-text>in executing a first garbage collection operation for the garbage collection operation on the first area,<claim-text>sequentially selecting data portions stored in the third block,</claim-text><claim-text>in a case where a third data portion that is one of the sequentially selected data portions includes valid data, copying the third data portion from the third block to the fourth block, and</claim-text><claim-text>in a case where a fourth data portion that is one of the sequentially selected data portions includes invalid data, incrementing a first invalid data counter by an amount of the invalid data included in the fourth data portion that is detected during the first garbage collection operation,</claim-text></claim-text><claim-text>in executing a second garbage collection operation for the garbage collection operation on the second area,<claim-text>sequentially selecting data portions stored in the fifth block,</claim-text><claim-text>in a case where a fifth data portion that is one of the sequentially selected data portions includes valid data, copying the fifth data portion from the fifth block to the sixth block, and</claim-text><claim-text>in a case where a sixth data portion that is one of the sequentially selected data portions includes invalid data, incrementing a second invalid data counter by an amount of the invalid data included in the sixth data portion that is detected during the second garbage collection operation, and</claim-text></claim-text><claim-text>in parallel with the first garbage collection operation and the second garbage collection operation,<claim-text>allowing write data from the host to be received in the buffer up to an amount indicated by a sum of the first invalid data counter and the second invalid data counter.</claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>in a case where the number of insufficient free blocks in the nonvolatile memory is equal to or more than two, allowing the write data portion from the host to be received in the buffer up to an amount obtained by dividing the amount indicated by the sum of the first invalid data counter and the second invalid data counter by the number of the insufficient free blocks.</claim-text></claim-text></claim></claims></us-patent-application>