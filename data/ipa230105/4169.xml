<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004170A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004170</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17566102</doc-number><date>20211230</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>C</subclass><main-group>21</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0276</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0225</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0274</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20200801</date></cpc-version-indicator><section>G</section><class>01</class><subclass>C</subclass><main-group>21</main-group><subgroup>3807</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0231</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>027</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6289</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>443</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6201</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>56</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>2201</main-group><subgroup>0216</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">MODULAR CONTROL SYSTEM AND METHOD FOR CONTROLLING AUTOMATED GUIDED VEHICLE</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63217118</doc-number><date>20210630</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Delta Electronics Int'l (Singapore) Pte Ltd</orgname><address><city>Singapore</city><country>SG</country></address></addressbook><residence><country>SG</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Nanyang Technological University</orgname><address><city>Singapore</city><country>SG</country></address></addressbook><residence><country>SG</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Chen</last-name><first-name>Chun-Lin</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wee</last-name><first-name>Yongjun</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Li</last-name><first-name>Maoxun</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Xie</last-name><first-name>Lihua</first-name><address><city>Singapore</city><country>SG</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Huang</last-name><first-name>Po-Kai</first-name><address><city>Taipei</city><country>TW</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Hung</last-name><first-name>Jui-Yang</first-name><address><city>Taipei</city><country>TW</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A modular control system for controlling an AGV includes an interface, a processor, a memory, and a plurality of programs. The plurality of programs include a task scheduling module, a sensor fusion module, a mapping module, and a localization module. The interface receives a command signal from an AGV management system and sensor signals from a plurality of sensors. The memory stores a surrounding map and the plurality of programs to be executed by the processor. The task scheduling module converts the command signal to generate an enabling signal. The sensor fusion module processes the received sensor signals according to the enabling signal and generates an organized sensor data. The mapping module processes the organized sensor data and the surrounding map to generate an updated surrounding map. The localization module processes the organized sensor data and the updated surrounding map to generate a location and pose signal.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="184.57mm" wi="136.31mm" file="US20230004170A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="217.93mm" wi="144.02mm" file="US20230004170A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="168.74mm" wi="136.23mm" orientation="landscape" file="US20230004170A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="141.14mm" wi="100.25mm" orientation="landscape" file="US20230004170A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="222.76mm" wi="116.59mm" file="US20230004170A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="168.57mm" wi="141.82mm" orientation="landscape" file="US20230004170A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="198.04mm" wi="136.57mm" orientation="landscape" file="US20230004170A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="87.55mm" wi="99.40mm" orientation="landscape" file="US20230004170A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="113.28mm" wi="113.37mm" orientation="landscape" file="US20230004170A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="86.36mm" wi="89.15mm" orientation="landscape" file="US20230004170A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="201.17mm" wi="129.62mm" orientation="landscape" file="US20230004170A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="214.12mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="214.21mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="223.18mm" wi="141.39mm" orientation="landscape" file="US20230004170A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="214.21mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="223.18mm" wi="143.17mm" orientation="landscape" file="US20230004170A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="209.80mm" wi="138.35mm" file="US20230004170A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="214.21mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="242.99mm" wi="143.17mm" orientation="landscape" file="US20230004170A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="221.66mm" wi="147.40mm" orientation="landscape" file="US20230004170A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="214.21mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="223.44mm" wi="180.85mm" orientation="landscape" file="US20230004170A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="212.51mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="274.07mm" wi="114.13mm" orientation="landscape" file="US20230004170A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="212.51mm" wi="139.11mm" orientation="landscape" file="US20230004170A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="140.72mm" wi="112.69mm" orientation="landscape" file="US20230004170A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="137.92mm" wi="114.13mm" orientation="landscape" file="US20230004170A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="129.54mm" wi="112.78mm" orientation="landscape" file="US20230004170A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="141.90mm" wi="121.33mm" orientation="landscape" file="US20230004170A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="109.64mm" wi="80.69mm" orientation="landscape" file="US20230004170A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="169.42mm" wi="90.42mm" orientation="landscape" file="US20230004170A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="103.97mm" wi="131.66mm" orientation="landscape" file="US20230004170A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="119.30mm" wi="132.59mm" orientation="landscape" file="US20230004170A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="145.12mm" wi="93.39mm" orientation="landscape" file="US20230004170A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="82.89mm" wi="123.70mm" orientation="landscape" file="US20230004170A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="90.25mm" wi="129.88mm" orientation="landscape" file="US20230004170A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Application Ser. No. 63/217,118 filed on Jun. 30, 2021, the disclosure of which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The present disclosure relates to a modular control system, and more particularly relates to a modular control system and method for controlling an automated guided vehicle (hereinafter &#x201c;AGV&#x201d;).</p><heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0004" num="0003">Nowadays the AGVs are playing an important role in factory/warehouse automation field and the enhancement in technology has been increasing autonomy of AGV that requires little human intervention to complete the functional task of AGV. The mature sensing and perception technology allow navigation in complex environments and the intelligent control algorithm allows AGVs to conduct the more complex missions or functional tasks.</p><p id="p-0005" num="0004">However the AGVs have been designed to handle a variety of functional tasks such as mapping, localization, navigation, automatic mapping, docking, and safety operation, which needs high variance in size, weight, power, mobility, maximum payload, payload type, and navigation type. As such the AGVs require high degree of customization to be used in different applications. The expert input is required to customize the AGV solution according to the required mission complexity, environment difficulty and human independence. The technical problem needs to be resolved is to quickly adapt the AGV solution to be used in different applications.</p><p id="p-0006" num="0005">Please refer to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates the systematic architecture of the AGV in U.S. Pat. No. 9,476,730 B2. A generic framework for the AGV control exists but requires much customization in different applications due to the different sensing and computational hardware, software, and algorithms used. Furthermore, the conventional architecture is difficult to upgrade with new hardware, software, or algorithm introduced, and the completed development lifecycle needs to be repeated with rigorous testing. In comparison, a modular control system (both hardware and software) with distinct and independent units, which every unit has a defined functional task operation and interface is preferred and proposed.</p><p id="p-0007" num="0006">It should be noted that the information of the disclosure in the Background above is only used to enhance the understanding of the background of the present invention, and therefore may include information that does not constitute the prior art known to those of ordinary skill in the art.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0008" num="0007">The present disclosure provides a modular control system and method for controlling an AGV in order to overcome at least one of the above-mentioned drawbacks.</p><p id="p-0009" num="0008">An object of the present disclosure is to provide a modular control system and method for controlling the AGV, so as to control the AGV to generate a map of its surrounding, locate its own position within the map, plan a path to a target position, and move to a target position, and the proposed architecture facilitates upgrading with new hardware, software, or algorithm introduced.</p><p id="p-0010" num="0009">According to the present disclosure, a modular control system for controlling an AGV includes an interface, a processor, and a memory. The interface is used for receiving a command signal from an AGV management system and sensor signals from a plurality of sensors. The memory is used for storing a surrounding map and a plurality of programs to be executed by the processor. The plurality of programs include a task scheduling module, a sensor fusion module, a mapping module, and a localization module. The task scheduling module receives the command signal from the interface for converting the command signal to generate an enabling signal corresponding to the received command signal. The sensor fusion module receives the sensor signals and the enabling signal for processing the received sensor signals according to the enabling signal and generates an organized sensor data. The mapping module, according to the enabling signal, processes the organized sensor data and the surrounding map to generate an updated surrounding map, and stores the updated surrounding map into the memory. The localization module, according to the enabling signal, processes the organized sensor data and the updated surrounding map to generate a location and pose signal.</p><p id="p-0011" num="0010">In yet another embodiment of the present disclosure, a method for controlling the AGV is provided. The method includes the steps of: (a) providing a modular control system comprising an interface, a processor, and a memory, wherein the memory stores a surrounding map and a plurality of programs to be executed by the processor, and the plurality of programs comprises a task scheduling module, a sensor fusion module, a mapping module, and a localization module; (b) the modular control system communicating through the interface to an AGV management system for receiving a command signal; (c) the modular control system communicating through the interface to a plurality of sensors for receiving sensor signals; (d) the task scheduling module receiving the command signal from the interface, and converting the received command signal to generate an enabling signal corresponding to the received command signal; (e) the sensor fusion module receiving the sensor signals and the enabling signal, processing the received sensor signals according to the enabling signal, and generating an organized sensor data; (f) the mapping module, according to the enabling signal, processing the organized sensor data and the surrounding map to generate an updated surrounding map, and storing the updated surrounding map into the memory; and (g) the localization module, according to the enabling signal, processing the organized sensor data and the updated surrounding map to generate a location and pose signal.</p><p id="p-0012" num="0011">The above contents of the present disclosure will become more readily apparent to those ordinarily skilled in the art after reviewing the following detailed description and accompanying drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates the systematic architecture of an AGV of the prior art;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically illustrates the proposed architecture of a modular control system for controlling the AGV according to a first embodiment of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates the operations of the plurality of programs shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram showing the method for controlling the AGV according to the first embodiment of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates the proposed architecture of the modular control system for controlling the AGV according to a second embodiment of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> schematically illustrates the operations of the plurality of programs shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> schematically illustrates the proposed architecture of the mapping module;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>8</b></figref> schematically illustrates the detailed process flow diagram of the parallel fusion policy;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>9</b></figref> schematically illustrates the detailed process flow diagram of the central fusion policy;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically illustrates the proposed architecture of the modular control system for controlling the AGV according to a third embodiment of the present disclosure;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>11</b></figref> schematically illustrates the proposed architecture of <figref idref="DRAWINGS">FIG. <b>10</b></figref> in more detail;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>12</b></figref> schematically illustrates the mapping operation flow of the AGV;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>13</b></figref> schematically illustrates the detailed process flow diagram of the AGV mapping;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>14</b></figref> schematically illustrates the localization operation flow of the AGV;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>15</b></figref> schematically illustrates the detailed process flow diagram of the AGV localization;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>16</b></figref> schematically illustrates the flow chart of repositioning when positioning is lost;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>17</b></figref> schematically illustrates the navigation operation flow of the AGV;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>18</b></figref> schematically illustrates the detailed process flow diagram of the AGV navigation;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>19</b></figref> schematically illustrates some examples of the navigation process being executed during path planning;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>20</b></figref> schematically illustrates the auto-mapping operation flow of the AGV;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>21</b></figref> schematically illustrates the example images of the AGV auto-mapping;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>22</b></figref> schematically illustrates the docking operation flow of the AGV;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>23</b></figref> schematically illustrates the example images of the AGV docking;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>24</b></figref> schematically illustrates the safety operation flow of the AGV;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>25</b></figref> schematically illustrates the structure of a navigation sensor unit;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>26</b></figref> schematically illustrates the structure of a docking sensor unit;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>27</b></figref> schematically illustrates the structure of a core computing unit;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>28</b></figref> schematically illustrates an example of a conveyor AGV;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>29</b></figref> schematically illustrates an example of a one-way tunnel AGV;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>30</b></figref> schematically illustrates an example of a two-way tunnel AGV;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIGS. <b>31</b> and <b>32</b></figref> schematically illustrate an example of a forklift AGV in different views;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>33</b></figref> schematically illustrates an example of a lifting AGV;</p><p id="p-0045" num="0044">and</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIGS. <b>34</b> and <b>35</b></figref> schematically illustrate an example of a unit load AGV in different views.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE PREFERRED EMBODIMENT</heading><p id="p-0047" num="0046">The present disclosure will now be described more specifically with reference to the following embodiments. It is to be noted that the following descriptions of preferred embodiments of this disclosure are presented herein for purpose of illustration and description only. It is not intended to be exhaustive or to be limited to the precise form disclosed.</p><p id="p-0048" num="0047">Comparing with the conventional AGV framework, the modular control system and method for controlling an AGV provided in the present disclosure adopts an open software architecture and standardized hardware modules with multiple and possible combinations to achieve the following advantages: 1) design and implement a new AGV or upgrade an existing AGV quickly and easily; 2) re-use software and hardware modules to achieve minimally essential AGV functional tasks; 3) adapt to different types of AGV vehicle platform; 4) open to improve in combination with new sensors or perception devices; and 5) open interface to high level of AGV management systems (e.g., Fleet management system).</p><p id="p-0049" num="0048">Please refer to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, which schematically illustrates the proposed architecture of a modular control system for controlling the AGV according to a first embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the modular control system <b>200</b> for controlling the AGV <b>202</b> includes an interface <b>204</b>, a processor <b>206</b>, a memory <b>208</b>, and a plurality of programs <b>210</b> to support the AGV for the essential functional tasks of: (a) mapping; (b) localization; (c) navigation; (d) automatic mapping; (e) docking; and (f) safety (optionally). The plurality of programs <b>210</b> include a task scheduling module <b>212</b>, a sensor fusion module <b>214</b>, a mapping module <b>216</b>, and a localization module <b>218</b>. The interface <b>204</b> receives a command signal S<b>1</b> from an AGV management system <b>220</b> and sensor signals S<b>2</b> from a plurality of sensors <b>222</b>. The memory <b>208</b> stores a surrounding map <b>224</b> and the plurality of programs <b>210</b> to be executed by the processor <b>206</b>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> further schematically illustrates the operations of the plurality of programs shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The task scheduling module <b>212</b> receives the command signal S<b>1</b> from the interface <b>204</b> for converting the received command signal S<b>1</b> to generate an enabling signal S<b>3</b> corresponding to the received command signal S<b>1</b>. The sensor fusion module <b>214</b> receives the sensor signals S<b>2</b> and the enabling signal S<b>3</b> for processing the received sensor signals S<b>2</b> according to the enabling signal S<b>3</b>, and generates an organized sensor data <b>226</b>. The mapping module <b>216</b>, according to the enabling signal S<b>3</b>, processes the organized sensor data <b>226</b> and the surrounding map <b>224</b> to generate an updated surrounding map <b>228</b>, and stores the updated surrounding map <b>228</b> into the memory <b>208</b>. The localization module <b>218</b>, according to the enabling signal S<b>3</b>, processes the organized sensor data <b>226</b> and the updated surrounding map <b>228</b> to generate a location and pose signal <b>230</b>.</p><p id="p-0050" num="0049">On the other hand, the present disclosure also provides a method for controlling the AGV. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow diagram showing the method for controlling the AGV according to an embodiment of the present disclosure. The method includes the following steps.</p><p id="p-0051" num="0050">In step S<b>302</b>, the modular control system <b>200</b> including the interface <b>204</b>, the processor <b>206</b>, and the memory <b>208</b> is provided, wherein the memory <b>208</b> stores the surrounding map <b>224</b> and the plurality of programs <b>210</b> to be executed by the processor <b>206</b>, and the plurality of programs <b>210</b> include the task scheduling module <b>212</b>, the sensor fusion module <b>214</b>, the mapping module <b>216</b>, and the localization module <b>218</b>.</p><p id="p-0052" num="0051">In step S<b>304</b>, the modular control system <b>200</b> communicates with the AGV management system <b>220</b> through the interface <b>204</b> for receiving a command signal S<b>1</b>.</p><p id="p-0053" num="0052">In step S<b>306</b>, the modular control system <b>200</b> communicates with the plurality of sensors <b>222</b> through the interface <b>204</b> for receiving the sensor signals S<b>2</b>.</p><p id="p-0054" num="0053">In step S<b>308</b>, the task scheduling module <b>212</b> receives the command signal S<b>1</b> from the interface <b>204</b>, and processes the received command signal S<b>1</b> to generate the enabling signal S<b>3</b> corresponding to the received command signal S<b>1</b>.</p><p id="p-0055" num="0054">In step S<b>310</b>, the sensor fusion module <b>214</b> receives the sensor signals S<b>2</b> and the enabling signal S<b>3</b>, processes the received sensor signals S<b>2</b> according to the enabling signal S<b>3</b>, and generates the organized sensor data <b>226</b>.</p><p id="p-0056" num="0055">In step S<b>312</b>, the mapping module <b>216</b> processes the organized sensor data <b>226</b> and the surrounding map <b>224</b> according to the enable signal S<b>3</b> to generate the updated surrounding map <b>228</b>, and stores the updated surrounding map <b>228</b> into the memory <b>208</b>.</p><p id="p-0057" num="0056">In step S<b>314</b>, the localization module <b>218</b> processes the organized sensor data <b>226</b> and the updated surrounding map <b>228</b> according to the enable signal S<b>3</b> to generate the location and pose signal <b>230</b>.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>5</b></figref> schematically illustrates the proposed architecture of the modular control system for controlling the AGV according to a second embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>6</b></figref> further schematically illustrates the operations of the plurality of programs shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In one embodiment, the plurality of programs <b>210</b> further include a navigation module <b>232</b> and a robot coordination module <b>234</b>. According to the enabling signal S<b>3</b>, the navigation module <b>232</b> processes the location and pose signal <b>230</b> and the updated surrounding map <b>228</b> to generate a target path signal and motion control parameters <b>236</b>. According to the enabling signal S<b>3</b>, the robot coordination module <b>234</b> processes the target path signal and the motion control parameters <b>236</b> to generate a robotic control signal <b>238</b> for controlling the motion of the AGV.</p><p id="p-0059" num="0058">In one embodiment, the interface <b>204</b> includes a north bound interface for communicating with the AGV management system <b>220</b> to receive the command signal S<b>1</b> and to transmit the updated surrounding map <b>228</b>, the location and pose signal <b>230</b>, or the target path signal and the motion control parameters <b>236</b> to the AGV management system <b>220</b>.</p><p id="p-0060" num="0059">In another embodiment, the interface <b>204</b> includes a vehicle command interface which transmits the robotic control signal <b>238</b> to motors or actuators of the AGV <b>202</b> for controlling the motion of the AGV <b>202</b>.</p><p id="p-0061" num="0060">In another embodiment, the interface <b>204</b> includes a material handling command interface which transmits the robotic control signal <b>238</b> to motors or actuators of a robot attached to the AGV <b>202</b> for controlling the motion or position of the robot.</p><p id="p-0062" num="0061">In another embodiment, the interface <b>204</b> includes a sensor interface for receiving the sensor signals S<b>2</b> from various sensors <b>222</b> including 2D or 3D vision sensor, LIDAR (Light Detection And Ranging) sensor, IMU (Inertial Measurement Unit) sensor, or robot odometry sensor. The sensor interface pre-processes the sensor signals S<b>2</b> by filtering out error or irrelevant sensor data and formatting the sensor data into predefined format to generate pre-processed sensor signals.</p><p id="p-0063" num="0062">In one embodiment, according to a pre-defined fusion policy or a dynamic fusion policy, the sensor fusion module <b>214</b> synchronizes or aggregates by weightage the pre-processed sensor signals to generate the organized sensor data <b>226</b>, and the fusion policy includes a parallel fusion policy or a central fusion policy.</p><p id="p-0064" num="0063">In another embodiment of the present disclosure, the plurality of programs <b>210</b> further include a docking module <b>240</b>. According to the enabling signal S<b>3</b>, the docking module <b>240</b> processes the organized sensor data <b>226</b> and the surrounding map <b>224</b> to generate a docking path signal and motion control parameters <b>242</b>. In a further embodiment, according to the enabling signal S<b>3</b>, the robot coordination module <b>234</b> processes the docking path signal and the motion control parameters <b>242</b> to generate the robotic control signal <b>238</b> for controlling the motion of the AGV <b>202</b>.</p><p id="p-0065" num="0064">In another embodiment, the interface <b>204</b> further includes a vehicle command interface which transmits the robotic control signal <b>238</b> to motors or actuators of the AGV <b>202</b> for controlling the motion of the AGV <b>202</b> to a docket position.</p><p id="p-0066" num="0065">In another embodiment, the interface <b>204</b> further includes a material handling command interface which transmits the robotic control signal <b>238</b> to motors or actuators of a robot attached to the AGV <b>202</b> for controlling the motion or position of the robot.</p><p id="p-0067" num="0066">Please refer to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, which schematically illustrates the proposed architecture of the mapping module. The mapping module <b>216</b> includes a feature extraction module <b>244</b>, a matching module <b>246</b>, and a combination module <b>248</b>. The feature extraction module <b>244</b> extracts spatial features from the organized sensor data <b>226</b> to generate extracted features. The matching module <b>246</b> matches the extracted features with the surrounding map <b>224</b> to obtain a matching result. The combination module <b>248</b>, according to the extracted features, the location and pose signal <b>230</b> and the matching result, to generate the updated surrounding map <b>228</b>.</p><p id="p-0068" num="0067">The AGV management system with high-level software applications includes fleet management system, Manufacturing Execution Systems (MES), and manual operator request of functional task. The AGV management system will be communicated with relevant parameters (e.g., mapping command, estimated target pose, localization command, localization mode, target pose, target speed, target acceleration, navigation command, auto-mapping command, region of interest, estimated target pose, docking command, docking mode, docking target, estimated start pose etc.) input for functional tasks and transmit the command signal to the task scheduling module via the north bound interface.</p><p id="p-0069" num="0068">In the task scheduling module operation, the task scheduling module plays the role for converting the command signal to generate the enabling signal and issue the enabling signal to the plurality of programs with variety of functional task modules. The functional task modules include the mapping module, the localization module, the navigation module, and the robot coordination module.</p><p id="p-0070" num="0069">In the sensor fusion module operation, the sensor fusion module is the program for combining data from multiple physical sensors in real-time, while also adding information from mathematical models, to create an accurate picture of the local environment. The fusion policy includes the parallel fusion policy or the central fusion policy.</p><p id="p-0071" num="0070">Please refer to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, which schematically illustrates the detailed process flow diagram of the parallel fusion policy. According to the parallel fusion policy, the original data are obtained by each independent sensor and processed locally, and then the results are sent to the information fusion center for intelligent optimization and combination to obtain the final result. The distributed (parallel) fusion method has low demand for communication bandwidth, fast calculation speed, good reliability and continuity, but the accuracy of tracking is far less than that of the centralized fusion method.</p><p id="p-0072" num="0071">Please refer to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, which schematically illustrates the detailed process flow diagram of the central fusion policy. According to the central fusion policy, the centralized raw data obtained by each sensor is directly sent to the central processing unit for fusion processing. Its data processing precision is high and the algorithm is flexible. However the disadvantage is high processor requirement, low reliability, and large data volume, which makes it difficult to implement.</p><p id="p-0073" num="0072">In the mapping module operation, the organized sensor data is passed to the mapping module with feature extraction module for extracting various spatial features (e.g., edges, planes, static or dynamic object, etc.) and the matching module for performing the extracted features matching. A combination of organized sensor data, extracted features, and AGV's pose estimation data is processed by the combination module to generate or update the 2D or 3D surrounding map of the AGV. The latest surrounding map will be updated to the updated surrounding map and stored to the memory.</p><p id="p-0074" num="0073">In the localization module operation, the organized sensor data is passed to the localization module for determining and estimating the AGV's relative position with reference to the latest surrounding map (2D costmap or 3D point cloud). If there is no existing map of the environment, the latest organized sensor data based on the AGV's immediate surrounding will be used to form the first map which is stored to memory.</p><p id="p-0075" num="0074">In the navigation module operation, the AGV is equipped with multiple sensors including robot odometry sensor, 2D or/and 3D sensor (e.g., LIDAR or/and VISION) or/and the IMU sensor (optional). These sensors are used to construct 2D/3D maps of the environment. A navigation module on the AGV precisely locates and orients the AGV in geo-spatial coordinates using sensor signal from 2D or/and 3D sensor (e.g., LIDAR or/and VISION) or/and the IMU sensor, odometer, compass and camera based sensors.</p><p id="p-0076" num="0075">In the docking module operation, the first stage is the same functional task as the navigation module in order to get close to the target object. The second stage is for a new functional task to identify the target object and control the docking operation by the docking module.</p><p id="p-0077" num="0076">In one embodiment, the interfaces include the north bound interface, the vehicle command interface, the material handling command interface, and the sensor interface. The sensor signal with raw data from the 2D or/and 3D sensor (e.g., LIDAR or/and VISION) or/and the IMU sensor (optional) is transmitted to the sensor interface by a unified communication interface (e.g., serial or ethernet-based communication).</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>10</b></figref> schematically illustrates the proposed architecture of the modular control system for controlling the AGV according to a third embodiment of the present disclosure, and <figref idref="DRAWINGS">FIG. <b>11</b></figref> schematically illustrates the proposed architecture of <figref idref="DRAWINGS">FIG. <b>10</b></figref> in more detail. As shown in <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>, the modular control system <b>200</b>&#x2032; is examplified by a core computing unit, the AGV management system <b>220</b>&#x2032; is examplified by including high-level software applications (e.g., fleet management, user application or MES) and manual operator request, the sensors <b>222</b>&#x2032; are examplified by including a navigation sensor unit and a docking sensor unit, and the AGV <b>202</b>&#x2032; is examplified by including a robot unit, a robot arm or a material handling unit, and a safety unit (optional), which are external hardware for robot control and monitoring.</p><p id="p-0079" num="0078">In the core computing unit, the processor and the memory are examplified by the orchestrator storing and executing the plurality of programs. The plurality of programs include the task scheduling module, the sensor fusion module, the mapping module, the localization module, the navigation module, the robot coordination module, and the docking module. Optionally, the plurality of programs further include a safety client module and an event management module. Moreover, the interface is examplified by including the north bound interface with the north bound communication module, the sensor interface with the sensor communication module, and the robot interface, which includes the vehicle command interface with the vehicle communication module, and the material handling command interface with the material handling communication module.</p><p id="p-0080" num="0079">Each of the navigation sensor unit and the docking sensor unit includes multiple sensors, and communicates with the core computing unit through the sensor interface. For example, the navigation sensor unit includes the 2D sensor, the 3D sensor, and the IMU sensor, and the docking sensor unit includes the 3D senor, the proximity sensor, and the docking feedback sensor. The sensor signals received from the navigation sensor unit and the docking sensor unit may be pre-processed by the sensor interface or by the sensor fusion module.</p><p id="p-0081" num="0080">The robot unit includes the mobile robot or vehicle and the bumper or emergency sensor, and the robot unit communicates with the core computing unit through the vehicle command interface. The robot unit further includes a robot odometry sensor which communicates with the core computing unit through the sensor interface to transmit odometry information (e.g. odometer data). The robot arm or the material handling unit communicates with the core computing unit through the material handling command interface. The safety unit includes the proximity sensor and the blind zone detection sensor, and the safety unit communicates with the core computing unit through the sensor interface.</p><p id="p-0082" num="0081">The proposed modular control system supports the communication with higher-level external application software and the lower-level external hardware robot control. The orchestrator in the core computing unit includes all the essential functions that support AGV and mobile robot platform: (a) mapping, (b) localization, (c) navigation, (d) automatic mapping, (e) docking, and (f) safety.</p><p id="p-0083" num="0082">The following paragraphs and figures show the process and data flow for each of the essential AGV functional task across the hardware and software modules/subsystems.</p><p id="p-0084" num="0083">(a) Mapping Operation Flow</p><p id="p-0085" num="0084">Please refer to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, which schematically illustrates the mapping operation flow of the AGV. As shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>:</p><p id="p-0086" num="0085">Step 0: The AGV management system with high-level software applications (e.g., Fleet management or MES) or manual operator requesting of AGV mapping operations (map request) will be communicated with relevant parameters (e.g., mapping command, estimated target pose, etc.) to the task scheduling module via the north bound interface. The task scheduling module then issues a mapping request by the enabling signal to the mapping module. The mapping module will calculate a map representation of the AGV's surrounding following these steps.</p><p id="p-0087" num="0086">Step 1: The sensor signal with raw data from the 2D or/and 3D sensor (e.g., LIDAR or/and VISION) or/and the IMU sensor (optional) in the navigation sensor unit is transmitted to the sensor interface of the core computing unit through a unified communication interface (e.g., serial or ethernet-based communication). Pre-processing of sensor data is performed to filter bad or irrelevant data, format into required format and transform into derived values.</p><p id="p-0088" num="0087">Step 2: The sensor interface of the core computing unit obtains the odometry information/odometer data (optional) from the robot unit, and processes this data (filter, format and transform) as required by the sensor fusion module.</p><p id="p-0089" num="0088">Step 3: The input sensor data (digital signal from LIDAR or camera) from steps 1 and 2 will next be transmitted to the sensor fusion module in the orchestrator after pre-processing. Here the sensor data will be synchronized and aggregated with varying weightage by the sensor fusion module based on pre-defined or dynamic sensor fusion policies to generate the organized sensor data. This process works for different type of sensor fusion methods (e.g., parallel fusion method, centralized fusion method, etc.).</p><p id="p-0090" num="0089">Step 4: The organized sensor data (also called sensor fusion data) from step 3 is then passed to the localization module to determinate/estimate the AGV's relative position with reference to the latest local or surrounding map (2D costmap or 3D point cloud). If there is no existing map of the environment, the latest organized sensor data based on the AGV's immediate surrounding will be used to form the first surrounding map which is stored to memory.</p><p id="p-0091" num="0090">Step 5: At the same time the organized sensor data from step 3 is passed to the mapping module which extracts various spatial features (e.g., edges, planes, static or dynamic object, etc.) and performs features matching. A combination of organized sensor data, extracted features, and AGV's pose estimation data is processed to generate or update the 2D or 3D surrounding map of the AGV as the updated surrounding map. This latest local map will be updated to the AGV's map and stored to memory.</p><p id="p-0092" num="0091">Step 6: In the final step, the mapping module sends the newly generated or updated map data to the AGV management system and ends the map request service.</p><p id="p-0093" num="0092">Please refer to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, which schematically illustrates the detailed process flow diagram of AGV mapping. In the AGV mapping operation, the sensor fusion module processes raw sensor data (steps 1-3) of the sensor signals to generate the organized sensor data (sensor fusion data). Thereafter the localization module processes the sensor fusion data and the local map estimation (steps 4-5) to generate the position/pose data. Then the mapping module processes the sensor fusion data and the position/pose data (step 6) to generate the map data, which is further transmitted to the AGV management system through the north bound interface.</p><p id="p-0094" num="0093">(b) Localization Operation Flow</p><p id="p-0095" num="0094">Please refer to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, which schematically illustrates the localization operation flow of the AGV. As shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>:</p><p id="p-0096" num="0095">Step 0: A localization service request (pose request) is triggered by the AGV management system with high-level software applications (e.g., Fleet management or MES) or manual operator with relevant parameters (e.g., localization command, localization mode, etc.) to the task scheduling module via the north bound interface. The task scheduling module then issues an AGV's pose/position request by the enabling signal to the localization module. The localization module will calculate the AGV's current 2D/3D pose with reference to its local 2D/3D map with the following steps.</p><p id="p-0097" num="0096">Steps 1-4: Steps 1-4 of the localization operation are identical to those of the mapping operation's steps 1-4.</p><p id="p-0098" num="0097">Step 5: At the same time (as steps 3 and 4) the organized sensor data from step 3 is passed to the mapping module which extracts various spatial features (e.g., edges, planes, static or dynamic object, etc.) and performs features matching. A combination of organized sensor data and extracted features is processed to generate or update the 2D or 3D surrounding map of the AGV as the updated surrounding map. The mapping module then sends the 2D or 3D surrounding map to the localization module for position/pose calculation.</p><p id="p-0099" num="0098">Step 6: The localization module provides the position/pose information of robot in 2D/3D map coordinate system to the north bound interface and ends the service request.</p><p id="p-0100" num="0099">Please refer to <figref idref="DRAWINGS">FIG. <b>15</b></figref>, which schematically illustrates the detailed process flow diagram of AGV localization. In the localization operation, the sensor fusion module processes raw sensor data (steps 1-3) of the sensor signals to generate the organized sensor data (sensor fusion data). Thereafter the localization module processes the sensor fusion data and the local map estimation (steps 4-5) to generate the position/pose data. Particularly, the pull local cost map or point cloud submodule has a position/pose signal input which is given by the localization service request submodule. Then the AGV pose transformation and estimation submodule can generate a new position/pose signal and pass it to the localization service request submodule after obtaining the organized sensor data and the updated surrounding map.</p><p id="p-0101" num="0100">Please refer to <figref idref="DRAWINGS">FIG. <b>16</b></figref>, which schematically illustrates the flow chart of repositioning when positioning is lost. In the event of positioning loss, the process as shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref> will be executed independently to relocate and retrieve the correct position and attitude of the AGV. This will be an event-triggered process. This occurs in the AGV pose transformation and estimation function in the localization module, whereby the pose estimation value is collected over multiple (N1) iterations and the covariance is calculated and compared against a pre-defined or dynamic value. If the covariance of the pose estimation is higher than the standard value, a countermeasure (e.g., extend the search scan area and rotates the AGV slightly) may be performed over a brief duration (T2).</p><p id="p-0102" num="0101">(c) Navigation Operation Flow</p><p id="p-0103" num="0102">Please refer to <figref idref="DRAWINGS">FIG. <b>17</b></figref>, which schematically illustrates the navigation operation flow of the AGV. As shown in <figref idref="DRAWINGS">FIG. <b>17</b></figref>:</p><p id="p-0104" num="0103">Step 0: The AGV management system with high-level software applications (e.g., Fleet management or MES) or manual operator requesting of AGV navigation operations will be communicated with relevant parameters (e.g., target pose, target speed, target acceleration, navigation command, etc.) to the task scheduling module via the north bound interface. The task scheduling module then issues a navigation request by the enabling signal to the navigation module. The navigation module will calculate the navigation/target path from current pose to target pose using the following steps.</p><p id="p-0105" num="0104">Steps 1-5: Steps 1-5 of the navigation operation are identical to those of the localization operation's steps 1-5.</p><p id="p-0106" num="0105">Step 6: The navigation module will plan an optimal navigation path from current pose to target pose according to the local map information. The navigation module will then send the target path in map coordinate system to the north bound interface for real-time monitoring by the AGV management system. The optimal navigation path may be computed by various optimization methods (e.g., shortest part, lowest energy cost, etc.). The optimal navigation path usually consists of multiple waypoints by which the AGV would travel to reach the target pose.</p><p id="p-0107" num="0106">Step 7: The navigation module sends an optimal navigation path from current pose to target pose together with motion control parameters (e.g., target speed, target acceleration) to the robot coordination module. The robot coordination module will then send a robotic control signal including vehicle control command and parameters (e.g., speed and acceleration, etc.) to the vehicle/robot to move it according to the planned path of motion.</p><p id="p-0108" num="0107">Steps 6 and 7 are iterative steps that are repeated until the target pose is reached or there is an exception event (e.g., collision avoidance, safety event, etc.) that occurred.</p><p id="p-0109" num="0108">Please refer to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, which schematically illustrates the detailed process flow diagram of AGV navigation. In the navigation operation, the sensor fusion module processes raw sensor data (steps 1-3) of the sensor signals to generate the organized sensor data (sensor fusion data), and the localization module processes the sensor fusion data and the local map estimation (steps 4-5) to generate the position/pose data. Thereafter the navigation module processes the position/pose data and the local map estimation (steps 6-7) to generate the target path data, and the mapping module processes the sensor fusion data to generate the map data.</p><p id="p-0110" num="0109">Please refer to <figref idref="DRAWINGS">FIG. <b>19</b></figref>, which schematically illustrates some examples of the navigation process being executed during path planning. Some examples of the navigation process being executed during path planning are provided as shown in <figref idref="DRAWINGS">FIG. <b>19</b></figref>. The examples illustrate how the AGV navigates through a straight corridor with multiple intermediate waypoints/steps from start point to end point.</p><p id="p-0111" num="0110">(d) Auto-Mapping Operation Flow</p><p id="p-0112" num="0111">Please refer to <figref idref="DRAWINGS">FIG. <b>20</b></figref>, which schematically illustrates the auto-mapping operation flow of the AGV. As shown in <figref idref="DRAWINGS">FIG. <b>20</b></figref>:</p><p id="p-0113" num="0112">Step 0: The AGV management system with high-level software applications (e.g., Fleet management or MES) or manual operator requesting of AGV auto-mapping operations will be communicated with relevant parameters (e.g., auto-mapping command, region of interest, estimated target pose, etc.) to the task scheduling module via the north bound interface. The task scheduling module then issues an auto-mapping request by the enabling signal to the mapping module. The mapping module proceeds with map exploration and calculates a map representation of the region of interest following these steps.</p><p id="p-0114" num="0113">Steps 1-5: Steps 1-5 of the auto-mapping operation are identical to those of the localization operation's steps 1-5.</p><p id="p-0115" num="0114">Step 6: Based on the first map that was generated with the AGV stationary in its first position, the mapping module may trigger a rotation of the AGV about its stationary point (optional) by sending a command to the robot coordination module, while repeating Steps 1-5. If not, the mapping module will send an exploratory target pose to the navigation module. Various auto-mapping strategies exist, which direct the AGV towards unexplored space by detecting frontiers. Frontiers are boundaries separating known space from unknown space.</p><p id="p-0116" num="0115">Steps 7-8: Steps 7-8 of the auto-mapping operation are identical to those of the navigation operation's steps 6-7.</p><p id="p-0117" num="0116">The steps 1-8 are repeated in exploration steps (to new frontiers) whereby the AGV identifies areas within the region of interest that is unknown and repeatedly updates the map with new data gathered. This continues until the entire region of interest that is accessible to the AGV is explored.</p><p id="p-0118" num="0117">Step 9: The mapping module sends the updated map data repeatedly to AGV management system with high-level software applications via the north bound interface. This continues until the entire region of interest that is accessible to the AGV is explored, whereby the auto-mapping service is completed.</p><p id="p-0119" num="0118">Please refer to <figref idref="DRAWINGS">FIG. <b>21</b></figref>, which schematically illustrates the example images of AGV auto-mapping. Some example images of the auto-mapping process that occurs through a series of exploration steps are provided as shown in <figref idref="DRAWINGS">FIG. <b>21</b></figref>. The example illustrates how the AGV explores unexplored regions in its environment through multiple exploration paths until the entire region of interest is covered.</p><p id="p-0120" num="0119">(e) Docking Operation Flow</p><p id="p-0121" num="0120">Please refer to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, which schematically illustrates the docking operation flow of the AGV. As shown in <figref idref="DRAWINGS">FIG. <b>22</b></figref>:</p><p id="p-0122" num="0121">Step 0: The AGV management system with high-level software applications (e.g., Fleet management or MES) or manual operator requesting of AGV docking operations will be communicated with relevant parameters (e.g., docking command, docking mode, docking target, estimated start pose, etc.) to the task scheduling module via the north bound interface. The task scheduling module then issues a docking request by the enabling signal to the docking module. The docking module will perform the automatic docking with docking stations (e.g., machines, shelves, trolleys, etc.) using the following steps.</p><p id="p-0123" num="0122">Step 1: The sensor signal with raw data from the 3D sensor (e.g., 3D LIDAR or/and VISION) and the proximity sensor data (e.g., range, presence, etc.) in the docking sensor unit is transmitted to the sensor interface of the core computing unit through a unified communication interface (e.g., serial or ethernet-based communication). Pre-processing of the sensor data is performed to filter bad or irrelevant data, format into required format and transform into derived values.</p><p id="p-0124" num="0123">Steps 2-3: Steps 2-3 of the docking operation are identical to those of the mapping operation's steps 2-3.</p><p id="p-0125" num="0124">Step 4: The organized sensor data from step 3 is then passed to the docking module to determinate/estimate the AGV's relative position with reference to the latest local map (2D costmap or 3D point cloud). This could be the standard 2D/3D map from the mapping module or a standalone one (which is usually of higher resolution) of the docking station in robot body coordinate system.</p><p id="p-0126" num="0125">Steps 5 and 6: The docking module then sends an optimal docking path from current pose to docking pose together with motion control parameters (e.g., target speed, target acceleration) to the robot coordination module. The robot coordination module will then send vehicle control command and parameters (e.g., speed and acceleration, etc.) to the vehicle/robot unit to move it according to the planned path of motion. Steps 1-6 are repeated until the vehicle/robot unit is successfully dock with a feedback signal from the docking sensor unit (optional).</p><p id="p-0127" num="0126">Step 7: (Optional) The docking module notifies the event management module that the docking is completed and a subsequent action for material handling may be triggered by sending a request/command to the robot arm/material handling unit via the material handling communication module.</p><p id="p-0128" num="0127">Step 8: In the final step, the docking module sends docking completion signal and status update to the AGV management system with high-level software applications via the north bound interface and ends the docking request service.</p><p id="p-0129" num="0128">This process flow supports different methods of AGV docking (e.g., marker-based, edge-detection, etc.) for both 2D and 3D mapping. Please refer to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, which schematically illustrates the example images of AGV docking. The examples illustrate a forklift AGV (left) docking itself to an empty pallet and a unit load AGV (right) docking itself to a trolley.</p><p id="p-0130" num="0129">(f) Safety Operation Flow</p><p id="p-0131" num="0130">Please refer to <figref idref="DRAWINGS">FIG. <b>24</b></figref>, which schematically illustrates the safety operation flow of the AGV.</p><p id="p-0132" num="0131">By design, the safety client module will be constantly monitoring safety sensor data and safety trigger. Safety trigger may come from the on-board safety sensors in the robot unit or the safety unit or even from the collision avoidance mechanism within the localization module. All safety triggers will activate the AGV safety operation through the following steps.</p><p id="p-0133" num="0132">Step 1: Safety trigger signals may come from the proximity sensor data (e.g., range) and the blind zone detection sensor data (e.g., range) in the safety unit, and the safety alarms from the bumper and emergency sensor in the robot unit, which are directly transmitted to the safety client module. The communication is through low latency and low complexity protocols (e.g., I/O, IO-Link, etc.) that adhere to safety standard requirements.</p><p id="p-0134" num="0133">Step 2: When the safety client module receives a safety trigger, raises a safety alert event/alarm to the event management module which in turn activates safe stop mechanism.</p><p id="p-0135" num="0134">Steps 3 and 4: The event management module will send emergency commands to the robot coordinate module to make the vehicle/robot unit and the robot arm/material handling unit to perform an emergency stop.</p><p id="p-0136" num="0135">Step 5: At the same time a safety alert alarm is sent to AGV management system with high-level software applications (e.g., fleet management/MES) to notify users of the safety event through the north bound interface.</p><p id="p-0137" num="0136">In terms of hardware, 3 subsystems (navigation sensor unit, docking sensor unit, and core computing unit) may be reused on different AGV in multiple combinations and for various applications.</p><p id="p-0138" num="0137">Please refer to <figref idref="DRAWINGS">FIG. <b>25</b></figref>, <figref idref="DRAWINGS">FIG. <b>26</b></figref>, and <figref idref="DRAWINGS">FIG. <b>27</b></figref>. <figref idref="DRAWINGS">FIG. <b>25</b></figref> schematically illustrates the structure of a navigation sensor unit. <figref idref="DRAWINGS">FIG. <b>26</b></figref> schematically illustrates the structure of a docking sensor unit. <figref idref="DRAWINGS">FIG. <b>27</b></figref> schematically illustrates the structure of a core computing unit.</p><p id="p-0139" num="0138">A navigation sensor unit is a modular subsystem that comprises a 360&#xb0; 2D sensor (e.g., LIDAR), an 180&#xb0; 3D sensor (e.g., depth camera, 3D LIDAR) and a communication interface. The sensor data from the navigation sensor unit provides a 2D and 3D image and range data required for AGV mapping, localization, navigation, auto-mapping operations. The communication interface ensures low latency and robust communication with the core computing unit which then processes the sensor data for precise mapping, pose estimation and collision avoidance in 3D environment.</p><p id="p-0140" num="0139">A docking sensor unit is a modular subsystem that comprises an 180&#xb0; 3D sensor (e.g., depth camera, 3D LIDAR), a proximity sensor (e.g., Infrared sensor) and a communication interface. The sensor data from the docking sensor unit provides a 3D image and range data required for AGV docking operations. The communication interface ensures low latency and robust communication with the core computing unit which then processes the sensor data for precise mapping, pose estimation and collision avoidance in 3D environment.</p><p id="p-0141" num="0140">A core computing unit is a modular subsystem that comprises a computing unit (e.g., embedded system, mini-PC, IPC), a power unit and a communication interface. The computing unit has an operating system with a plurality of programs including all required software modules and system drivers installed. The power unit provides the necessary power conversion from external power or battery, distributes the power to all subsystems and allows manual or auto power on/off and restart. The communication interface ensures low latency and robust communication with the navigation sensor unit, the docking sensor unit and (optional) the safety unit.</p><p id="p-0142" num="0141">A combination of 1 set of navigation sensor unit, 1 set of docking sensor unit and 1 set of core computing unit is the minimal requirement for a one directional (forward) travel, and additional 1 set of navigation sensor unit is required for two directional (forward, backward) and omnidirectional travel. The following paragraphs and figures illustrate the combinations of the 3 subsystems that can be used (but not limited to) for multiple AGV types.</p><p id="p-0143" num="0142">(1) Conveyor AGV</p><p id="p-0144" num="0143">Please refer to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, which schematically illustrates an example of a conveyor AGV. A proposed combination of one navigation sensor unit, one core computing unit, and one docking sensor unit (optional) is shown for example.</p><p id="p-0145" num="0144">(2) One-Way Tunnel AGV</p><p id="p-0146" num="0145">Please refer to <figref idref="DRAWINGS">FIG. <b>29</b></figref>, which schematically illustrates an example of a one-way tunnel AGV. A proposed combination of one navigation sensor unit and one core computing unit is shown for example.</p><p id="p-0147" num="0146">(3) Two-Way Tunnel AGV</p><p id="p-0148" num="0147">Please refer to <figref idref="DRAWINGS">FIG. <b>30</b></figref>, which schematically illustrates an example of a two-way tunnel AGV. A proposed combination of two navigation sensor units and one core computing unit is shown for example.</p><p id="p-0149" num="0148">(4) Forklift AGV</p><p id="p-0150" num="0149">Please refer to <figref idref="DRAWINGS">FIG. <b>31</b></figref> and <figref idref="DRAWINGS">FIG. <b>32</b></figref>, which schematically illustrate an example of a forklift AGV in different views. A proposed combination of two navigation sensor units, one core computing unit, and one docking sensor unit is shown for example.</p><p id="p-0151" num="0150">(5) Lifting AGV</p><p id="p-0152" num="0151">Please refer to <figref idref="DRAWINGS">FIG. <b>33</b></figref>, which schematically illustrates an example of a lifting AGV. A proposed combination of two navigation sensor units and one core computing unit is shown for example.</p><p id="p-0153" num="0152">(6) Unit Load AGV</p><p id="p-0154" num="0153">Please refer to <figref idref="DRAWINGS">FIG. <b>34</b></figref> and <figref idref="DRAWINGS">FIG. <b>35</b></figref>, which schematically illustrate an example of a unit load AGV in different views. A proposed combination of one navigation sensor unit, one core computing unit, and one docking sensor unit is shown for example.</p><p id="p-0155" num="0154">The proposed combinations of the navigation sensor unit, the docking sensor unit, and the core computing unit can be deployed, configured, and tested on different AGV platforms through the following generic steps. The modular hardware and software of the present disclosure can be immediately configured and used in different types of AGV and can even meet certain AGV safety regulations.</p><p id="p-0156" num="0155">The recommended setting/calibration/test steps are described as following:</p><p id="p-0157" num="0156">A. Setting AGV appearance, specifications, and parameter input:</p><p id="p-0158" num="0157">Set the following parameters according to different AGV vehicle movement methods and docking equipment.</p><p id="p-0159" num="0158">1. AGV size, maximum load weight (optional).</p><p id="p-0160" num="0159">2. Driving wheels: type, number, wheel radius, placement, maximum speed.</p><p id="p-0161" num="0160">3. Driven wheel (optional): type, number, wheel radius, placement.</p><p id="p-0162" num="0161">4. Types and quantity of unit (module) used in AGV vehicle.</p><p id="p-0163" num="0162">5. AGV communication interface test.</p><p id="p-0164" num="0163">6. Definition of AGV external safety device.</p><p id="p-0165" num="0164">B. Kits: Navigation/Docking Unit Calibration:</p><p id="p-0166" num="0165">The following steps are recommended calibration methods for the navigation/docking unit.</p><p id="p-0167" num="0166">Step 1: Set the definition of the placement of the navigation/docking unit: For different AGV, the illustration above may be referred to place the definition of the navigation/docking unit and set the configuration distance coordinates (relative to the center between the driving wheels).</p><p id="p-0168" num="0167">Step 2: Navigation/docking unit communication interface test: Use the installed core computing unit to perform the communication connection test with the navigation/docking unit in order to perform the next steps.</p><p id="p-0169" num="0168">Step 3: Sensor range setting: Set the maximum range that can be detected by the 2D/3D sensor in the navigation unit. Set the maximum range that can be detected by the 3D sensor in the docking unit.</p><p id="p-0170" num="0169">Step 4: Mapping/localization function calibration (not necessary): Test the mapping/localization function in the navigation unit and use a known field size for calibration.</p><p id="p-0171" num="0170">Step 5: Navigation function calibration (not necessary): Use the calibration map created in step 4 to set the position from point A to point B for calibration.</p><p id="p-0172" num="0171">Step 6: Calibration of the docking function: Install the docking calibration label on the device to be docked and perform ID recording and docking position/pose calibration.</p><p id="p-0173" num="0172">C. Safety Mechanism/Device/Equipment Verification:</p><p id="p-0174" num="0173">The following are function and safety test and verification steps.</p><p id="p-0175" num="0174">Step 1: Setting the internal safety mechanism of the kit: Set the navigation/docking unit's function of avoiding obstacles (e.g., AGV vehicle movement methods and braking rules far, middle and near from obstacles).</p><p id="p-0176" num="0175">Step 2: AGV external safety contact obstacles buffer performance test (e.g., bumper): Turn off the internal safety mechanism setting of the kit, AGV runs at the rated speed, and place obstacles in the direction of AGV travel (diameter 50 mm, weight 55 kg or less). The AGV in motion stops when it encounters an obstacle. Test the moving distance forced to stop. The test is performed under no load and under load. The braking distance must not exceed the value specified by the AGV vehicle manufacturer.</p><p id="p-0177" num="0176">Step 3: AGV external safety emergency stop performance test (for example: emergency stop button): AGV automatically runs at the rated speed. After pressing the emergency stop button at a pre-marked location on the linear trajectory, the AGV emergency stops and tests from the marked position to the stop. The distance of the position is tested 5 times each in the case of no load and specified load, forward and backward (except without the reverse function), and the braking distance must not exceed the value specified by the AGV vehicle manufacturer.</p><p id="p-0178" num="0177">D. Fully Calibrated AGV Vehicle Motion Test:</p><p id="p-0179" num="0178">The following step tests the motion of the AGV as a whole and is also the last step of the deployment process.</p><p id="p-0180" num="0179">Step 1. Vehicle motion accuracy test: When the AGV is moving on the set path at the specified speed, the tester visually reads the maximum value of deviation from the baseline. The test is performed under no load and specified load, forward and backward (except without back function), and the accuracy of movement must not exceed the value specified by the AGV vehicle manufacturer.</p><p id="p-0181" num="0180">Step 2. Vehicle maximum turning radius test: Automatically run at the set speed on the curve of the minimum rotation radius of the guideline specified by the AGV, and smoothly rotate on the guide trajectory. The transition between the various actions of the AGV is required to be smooth. Test separately under no load and with specified load.</p><p id="p-0182" num="0181">According to the present disclosure, it is provided a multi-sensor modular system and method of real-time 3D mapping, localization, navigation and control of AGVs. The proposed system includes modular hardware and modular software. The modular hardware includes the navigation sensor unit, the docking sensor unit, the core computing unit, and the safety unit (optional). The modular software includes the task scheduling module, the sensor fusion module, the mapping module, the localization module, the navigation module, the robot coordination module, the docking module, the safety client module, the event management module, and the sensor/north bound/robot interface. The proposed system can control/guide different mobile robots or vehicles to generate a map of its surrounding (manually or automatically), locate its own position within the map, plan a path to a target position (given by external control system), move to a target position (given by external control system), detect nearby obstacles and avoid them, and dock to a static object (in a fixed location) for material handling or charging.</p><p id="p-0183" num="0182">From the above descriptions, the present disclosure provides a modular control system and method for controlling an AGV. It is different from the conventional AGV framework, the modular control system and method adopts an open software architecture and standardized hardware modules with multiple possible combinations to achieve the advantages of designing and implementing a new AGV or upgrading an existing AGV quickly and easily, re-using software and hardware modules to achieve minimally essential AGV functional tasks, adapting to different types of AGV vehicle platform, being open to improvement with new sensors or perception devices and/or combinations thereof, and having an open interface to high level of AGV management system.</p><p id="p-0184" num="0183">While the invention has been described in terms of what is presently considered to be the most practical and preferred embodiments, it is to be understood that the invention needs not be limited to the disclosed embodiment. On the contrary, it is intended to cover various modifications and similar arrangements included within the spirit and scope of the appended claims which are to be accorded with the broadest interpretation so as to encompass all such modifications and similar structures.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A modular control system for controlling an automated guided vehicle (AGV), comprising:<claim-text>an interface for receiving a command signal from an AGV management system and sensor signals from a plurality of sensors;</claim-text><claim-text>a processor; and</claim-text><claim-text>a memory for storing a surrounding map and a plurality of programs to be executed by the processor, the plurality of programs comprising:<claim-text>a task scheduling module, receiving the command signal from the interface, for converting the received command signal to generate an enabling signal corresponding to the received command signal;</claim-text><claim-text>a sensor fusion module, receiving the sensor signals and the enabling signal, for processing the received sensor signals according to the enabling signal, and generating an organized sensor data;</claim-text><claim-text>a mapping module, according to the enabling signal, for processing the organized sensor data and the surrounding map to generate an updated surrounding map, and storing the updated surrounding map into the memory; and</claim-text><claim-text>a localization module, according to the enabling signal, for processing the organized sensor data and the updated surrounding map to generate a location and pose signal.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The modular control system as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of programs further comprise:<claim-text>a navigation module, according to the enabling signal, for processing the location and pose signal and the updated surrounding map to generate a target path signal and motion control parameters; and</claim-text><claim-text>a robot coordination module, according to the enabling signal, for processing the target path signal and the motion control parameters to generate a robotic control signal for controlling the motion of the AGV.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The modular control system as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the interface comprises:<claim-text>a north bound interface for communicating with the AGV management system to receive the command signal and to transmit the updated surrounding map, the location and pose signal or the target path signal and the motion control parameters to the AGV management system.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The modular control system as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the interface comprises:<claim-text>a vehicle command interface transmitting the robotic control signal to motors or actuators of the AGV for controlling the motion of the AGV.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The modular control system as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the interface comprises:<claim-text>a material handling command interface transmitting the robotic control signal to motors or actuators of a robot attached to the AGV for controlling the motion or position of the robot.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The modular control system as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the interface comprises:<claim-text>a sensor interface for receiving the sensor signals from the plurality of sensors, including 2D or 3D vision sensor, LIDAR sensor, IMU sensor, or robot odometry sensor, wherein the sensor interface pre-processes the sensor signals by filtering out error or irrelevant sensor data and formatting sensor data into predefined format to generate pre-processed sensor signals.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The modular control system as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the sensor fusion module, according to a pre-defined fusion policy or a dynamic fusion policy, synchronizes or aggregates by weightage the pre-processed sensor signals to generate the organized sensor data, and wherein the fusion policy comprises a parallel fusion policy or a central fusion policy.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The modular control system as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the mapping module comprises:<claim-text>a feature extraction module for extracting spatial features from the organized sensor data to generate extracted features;</claim-text><claim-text>a matching module for matching the extracted features with the surrounding map to obtain a matching result; and</claim-text><claim-text>a combination module, according to the extracted features, the location and pose signal and the matching result, to generate the updated surrounding map.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The modular control system as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the plurality of programs further comprise:<claim-text>a docking module, according to the enabling signal, for processing the organized sensor data and the surrounding map to generate a docking path signal and motion control parameters;</claim-text><claim-text>wherein the robot coordination module, according to the enabling signal, for processing the docking path signal and the motion control parameters to generate the robotic control signal for controlling the motion of the AGV.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The modular control system as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the interface comprises:<claim-text>a vehicle command interface transmitting the robotic control signal to motors or actuators of the AGV for controlling the motion of the AGV to a docket position.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The modular control system as claimed in <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the interface comprises:<claim-text>a material handling command interface transmitting the robotic control signal to motors or actuators of a robot attached to the AGV for controlling the motion or position of the robot.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method for controlling an automated guided vehicle (AGV), the method comprising steps of:<claim-text>(a) providing a modular control system comprising an interface, a processor, and a memory, wherein the memory stores a surrounding map and a plurality of programs to be executed by the processor, and the plurality of programs comprises a task scheduling module, a sensor fusion module, a mapping module, and a localization module;</claim-text><claim-text>(b) the modular control system communicating through the interface to an AGV management system for receiving a command signal;</claim-text><claim-text>(c) the modular control system communicating through the interface to a plurality of sensors for receiving sensor signals;</claim-text><claim-text>(d) the task scheduling module receiving the command signal from the interface, and converting the received command signal to generate an enabling signal corresponding to the received command signal;</claim-text><claim-text>(e) the sensor fusion module receiving the sensor signals and the enabling signal, processing the received sensor signals according to the enabling signal, and generating an organized sensor data;</claim-text><claim-text>(f) the mapping module, according to the enabling signal, processing the organized sensor data and the surrounding map to generate an updated surrounding map, and storing the updated surrounding map into the memory; and</claim-text><claim-text>(g) the localization module, according to the enabling signal, processing the organized sensor data and the updated surrounding map to generate a location and pose signal.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the plurality of programs further comprise a navigation module and a robot coordination module, and the method further comprises steps of:<claim-text>the navigation module, according to the enabling signal, processing the location and pose signal and the updated surrounding map to generate a target path signal and motion control parameters; and</claim-text><claim-text>the robot coordination module, according to the enabling signal, processing the target path signal and the motion control parameters to generate a robotic control signal for controlling the motion of the AGV.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the interface comprises a north bound interface, the modular control system communicates with the AGV management system through the north bound interface to receive the command signal in the step (b), and the method further comprises a step of:<claim-text>the modular control system communicating with the AGV management system through the north bound interface to transmit the updated surrounding map, the location and pose signal or the target path signal to the AGV management system.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the interface comprises a vehicle command interface, and the method further comprises a step of:<claim-text>the modular control system transmitting the robotic control signal to motors or actuators of the AGV through the vehicle command interface for controlling the motion of the AGV.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the interface comprises a material handling command interface, and the method further comprises a step of:<claim-text>the modular control system transmitting the robotic control signal to motors or actuators of a robot attached into the AGV through the material handling command interface for controlling the motion or position of the robot.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the interface comprises a sensor interface, the modular control system receives the sensor signals from the plurality of sensors, including 2D or 3D vision sensor, LIDAR sensor, IMU sensor, or robot odometry sensor, through the sensor interface, and the step (c) further comprises a step of:<claim-text>the sensor interface pre-processing the sensor signals by filtering out error or irrelevant sensor data and formatting the sensor data into predefined format to generate pre-processed sensor signals.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method as claimed in <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising a step of:<claim-text>the sensor fusion module synchronizing or aggregating by weightage the pre-processed sensor signals to generate the organized sensor data according to a pre-defined fusion policy or a dynamic fusion policy, wherein the fusion policy comprises a parallel fusion policy or a central fusion policy.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the mapping module comprises:<claim-text>a feature extraction module for extracting spatial features from the organized sensor data to generate extracted features;</claim-text><claim-text>a matching module for matching the extracted features with the surrounding map to obtain a matching result; and</claim-text><claim-text>a combination module, according to the extracted features, the location and pose signal and the matching result, to generate the updated surrounding map.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the plurality of programs further comprise a docking module, and the method further comprises steps of:<claim-text>the docking module, according to the enabling signal, processing the organized sensor data and the surrounding map to generate a docking path signal and motion control parameters; and</claim-text><claim-text>the robot coordination module, according to the enabling signal, processing the docking path signal and the motion control parameters to generate the robotic control signal for controlling the motion of the AGV.</claim-text></claim-text></claim></claims></us-patent-application>