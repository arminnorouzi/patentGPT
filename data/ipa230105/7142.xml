<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007143A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007143</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941395</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202011306972.5</doc-number><date>20201119</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>1</main-group><subgroup>32</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>1</main-group><subgroup>32272</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>44</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>774</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">PICTURE ENCRYPTION METHOD AND APPARATUS, COMPUTER DEVICE, STORAGE MEDIUM AND PROGRAM PRODUCT</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/CN2021/125894</doc-number><date>20211022</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17941395</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Tencent Technology (Shenzhen) Company Limited</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>YANG</last-name><first-name>Weiming</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>TANG</last-name><first-name>Huizhong</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Shaoming</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>GUO</last-name><first-name>Runzeng</first-name><address><city>Shenzhen</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Tencent Technology (Shenzhen) Company Limited</orgname><role>03</role><address><city>Shenzhen</city><country>CN</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Disclosed are a picture encryption method and apparatus, a computer device, a storage medium and a program product. The method includes: acquiring N pictures having a time sequence, N being an integer equal to or greater than 2; performing feature extraction on the N pictures to acquire a picture feature of each of the N pictures; successively performing target prediction on the N pictures according to the time sequence to obtain prediction information of the each of the N pictures, the target prediction referring to a prediction on the each of the N pictures based on status information, and the status information being information which is updated based on picture features of pictures that have been predicted; and encrypting the N pictures based on the prediction information of the each of the N pictures.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="74.68mm" wi="157.56mm" file="US20230007143A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="202.35mm" wi="159.60mm" file="US20230007143A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="120.73mm" wi="128.44mm" file="US20230007143A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="228.85mm" wi="168.06mm" file="US20230007143A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="220.47mm" wi="161.12mm" file="US20230007143A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="103.12mm" wi="169.25mm" file="US20230007143A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="204.89mm" wi="171.53mm" file="US20230007143A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="103.38mm" wi="73.58mm" file="US20230007143A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="171.03mm" wi="170.10mm" file="US20230007143A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="176.95mm" wi="164.00mm" file="US20230007143A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation application of PCT Patent Application No. PCT/CN2021/125894, filed on Oct. 22, 2021, which claims priority to Chinese Patent Application No. 202011306972.5, filed with the China National Intellectual Property Administration, PRC on Nov. 19, 2020, each of which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE TECHNOLOGY</heading><p id="p-0003" num="0002">Embodiments of this disclosure relate to the technical field of artificial intelligence, and in particular, to a picture encryption method and apparatus, a computer device, a storage medium and a program product.</p><heading id="h-0003" level="1">BACKGROUND OF THE DISCLOSURE</heading><p id="p-0004" num="0003">Nowadays, with increasing development of artificial intelligence, application of the artificial intelligence technology is more and more extensive in life, including application in the face recognition technology. Face recognition can be applied to payment scenarios to realize &#x201c;face swiping&#x201d; payment.</p><p id="p-0005" num="0004">In related art, in order to guarantee the safety of face swiping data acquired by face swiping, it is usually necessary to encrypt the face swiping data. A common encryption method encrypts the face swiping data by adopting an asymmetric algorithm according to a certain encryption format, where the encryption format is usually introduction of a timestamp of the face swiping data or a counter.</p><p id="p-0006" num="0005">However, encryption of the face swiping data by adopting the above-mentioned solution will lead to a condition that the encryption format of the face swiping data is easily cracked, resulting in poor safety of the face swiping data.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">Embodiments of this disclosure provide a picture encryption method and apparatus, a computer device, a storage medium and a program product, so as to improve the safety of picture encryption. The technical solution is as follows:</p><p id="p-0008" num="0007">according to one aspect, a picture encryption method is provided. The method is executed by a computer device, the method including:</p><p id="p-0009" num="0008">acquiring N first pictures having a time sequence, where N is an integer equal to or greater than 2;</p><p id="p-0010" num="0009">performing feature extraction on the N first pictures to acquire picture features of the N first pictures;</p><p id="p-0011" num="0010">successively performing target prediction on the N first pictures according to the time sequence to obtain prediction information of the N first pictures, where the target prediction refers to prediction on the first pictures based on status information, and the status information is information for updating based on the picture features of the first pictures that have been predicted; and</p><p id="p-0012" num="0011">encrypting the N first pictures based on the prediction information of the N first pictures.</p><p id="p-0013" num="0012">According to another aspect, a picture encryption apparatus is provided. The apparatus includes:</p><p id="p-0014" num="0013">a picture acquisition module configured to acquire N first pictures having a time sequence, where N is an integer equal to or greater than 2;</p><p id="p-0015" num="0014">a feature extraction module configured to perform feature extraction on the N first pictures to acquire picture features of the N first pictures;</p><p id="p-0016" num="0015">an information output module configured to successively perform target prediction on the N first pictures according to the time sequence to obtain prediction information of the N first pictures, where the target prediction refers to prediction on the first pictures based on status information, and the status information is information for updating based on the picture features of the first pictures that have been predicted; and</p><p id="p-0017" num="0016">a picture encryption module configured to encrypt the N first pictures based on the prediction information of the N first pictures.</p><p id="p-0018" num="0017">In a possible implementation, the information output module includes:</p><p id="p-0019" num="0018">an information output sub-module configured to successively input the picture features of the N first pictures into a recurrent network in an image prediction model according to the time sequence to obtain the prediction information of the N first pictures outputted by the recurrent network.</p><p id="p-0020" num="0019">In a possible implementation, the apparatus further includes:</p><p id="p-0021" num="0020">an picture segment acquisition module configured to, through a partitioning network in the image prediction model, perform partitioning processing on the N first pictures according to a size of a target window to acquire M first picture segments corresponding to the N first pictures before the performing feature extraction on the N first pictures to acquire picture features corresponding to the N first pictures, where M is an integer equal to or greater than 1, and the size of the target window is a network parameter of the partitioning network; and</p><p id="p-0022" num="0021">the feature extraction module includes:</p><p id="p-0023" num="0022">a feature extraction sub-module configured to perform feature extraction on the M first picture segments of the N first pictures based on attention weights of the M first picture segments to obtain picture features of the N first pictures.</p><p id="p-0024" num="0023">In a possible implementation, the prediction information includes prediction sub-information of the M first picture segments of the corresponding first pictures, the prediction sub-information being used for indicating prediction results of the first picture segments.</p><p id="p-0025" num="0024">In a possible implementation, the feature extraction sub-module includes:</p><p id="p-0026" num="0025">a feature preprocessing unit for preprocessing the M first picture segments of a target picture to obtain preprocessed features of the M first picture segments of the target picture, where the target picture is any one of the N first pictures; and</p><p id="p-0027" num="0026">a target feature acquisition unit for inputting the preprocessed features of the M first picture segments of the target picture into an image feature extraction network in the image prediction model to obtain the picture feature of the target picture outputted by the image feature extraction network,</p><p id="p-0028" num="0027">where the image feature extraction network includes an attention mechanism layer used for processing the preprocessed features of the M first picture segments by taking the attention weights of the M first picture segments as parameters.</p><p id="p-0029" num="0028">In a possible implementation, the feature preprocessing unit is configured to:</p><p id="p-0030" num="0029">acquire a gray value of a target picture segment, where the target picture segment is any one of the M first picture segments corresponding to the target picture;</p><p id="p-0031" num="0030">acquire first type feature information of the target picture segment based on fast Fourier transform of a first order of magnitude; and</p><p id="p-0032" num="0031">combine the gray value of the target picture segment with the first type feature information of the target picture block to obtain a preprocessed feature of the target picture segment.</p><p id="p-0033" num="0032">In a possible implementation, the first type feature information includes:</p><p id="p-0034" num="0033">at least one of energy distribution feature information, high and low frequency distribution feature information, flatness feature information and spectral centroid feature information of an image frequency spectrogram.</p><p id="p-0035" num="0034">In a possible implementation, the apparatus further includes:</p><p id="p-0036" num="0035">a training data acquisition module configured to acquire training data before acquiring N first pictures with a time sequence, where the training data includes K picture samples having a time sequence and annotated information of the K picture samples, K being an integer equal to or greater than 2;</p><p id="p-0037" num="0036">a sample feature extraction module configured to perform feature extraction on the K picture samples to acquire picture features corresponding to the K picture samples;</p><p id="p-0038" num="0037">a sample prediction information acquisition module configured to successively input the picture features corresponding to the K picture samples into the recurrent network in the image prediction model according to the time sequence to obtain the prediction information of the K picture samples outputted by the recurrent network;</p><p id="p-0039" num="0038">a sample prediction result acquisition module configured to acquire prediction results of the K picture samples based on the prediction information of the K picture samples;</p><p id="p-0040" num="0039">a loss function value acquisition module configured to acquire a loss function value based on the prediction results of the K picture samples and the annotated information of the K picture samples; and</p><p id="p-0041" num="0040">a model updating module configured to update the image prediction model based on the loss function value.</p><p id="p-0042" num="0041">In a possible implementation, the model updating module includes:</p><p id="p-0043" num="0042">a parameter updating sub-module configured to perform parameter updating on at least one network of the recurrent network, the partitioning network and the image feature extraction network based on the loss function value.</p><p id="p-0044" num="0043">In a possible implementation, the recurrent network includes:</p><p id="p-0045" num="0044">at least one of a bi-directional gated recurrent unit (BGRU), a gated recurrent unit (GRU) and a long short term memory (LSTM) network.</p><p id="p-0046" num="0045">In a possible implementation, the picture encryption module includes:</p><p id="p-0047" num="0046">a picture encryption sub-module configured to splice the N first pictures with the corresponding prediction information to generate encrypted picture data of the N first pictures.</p><p id="p-0048" num="0047">In a possible implementation, all or part of pictures in the N first pictures include face images.</p><p id="p-0049" num="0048">In a possible implementation, the prediction information is used for indicating whether the first pictures include the face images with image qualities satisfying quality conditions.</p><p id="p-0050" num="0049">According to another aspect, a computer device is provided, including a processor and a memory, the memory storing at least one instruction, at least one program, a code set, or an instruction set, and the at least one instruction, the at least one program, the code set or the instruction set being loaded and executed by the processor to implement the foregoing picture encryption method.</p><p id="p-0051" num="0050">According to another aspect, a non-transitory computer-readable storage medium is provided, the non-transitory storage medium storing at least one instruction, at least one program, a code set or an instruction set, the at least one instruction, the at least one program, the code set or the instruction set being loaded and executed by a processor to implement the foregoing picture encryption method.</p><p id="p-0052" num="0051">According to an aspect of this application, a computer program product or a computer program is provided, the computer program product or the computer program including computer instructions, the computer instructions being stored in a non-transitory computer-readable storage medium. A processor of a computer device reads the computer instructions from the non-transitory computer-readable storage medium, and executes the computer instructions, so that the computer device performs the picture encryption method provided in the various optional implementations of the foregoing aspects.</p><p id="p-0053" num="0052">The technical solutions provided in this application may include the following beneficial effects:</p><p id="p-0054" num="0053">After the first pictures having the time sequence are subjected to feature extraction, the first pictures are subjected to target prediction to obtain the prediction information corresponding to the first pictures, and the first pictures are encrypted based on the prediction information corresponding to the first picture. Prediction is performed through the updated status information of the picture features of the first pictures that has been predicted, so as to obtain current prediction information that predicts the picture features of the first pictures. Through the above-mentioned solution, the prediction information of the first pictures is affected by considering relevance of the picture features of the first pictures in time dimension, so that a condition that a first image is easily cracked for an encryption format only introducing a non-neural network safety factor is avoided, and therefore, the safety of the encrypted first image is also improved while the encryption format that encrypts the first pictures is expanded.</p><p id="p-0055" num="0054">It is to be understood that the above general descriptions and the following detailed descriptions are merely for exemplary and explanatory purposes, and cannot limit this application.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0056" num="0055">The accompanying drawings herein, which are incorporated into the specification and constitute a part of this specification, show embodiments that conform to this disclosure, and are used for describing a principle of this application together with this specification.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a picture encryption system according to an exemplary embodiment.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a face acquisition terminal according to an example embodiment.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a picture encryption method according to an exemplary embodiment.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic flowchart of a picture encryption method according to an exemplary embodiment.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic structural diagram of a BGRU involved in an exemplary embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of continuous time face image acquisition involved in the exemplary embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram of an image prediction module involved in the exemplary embodiment shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic structural diagram of a face picture encryption system according to an exemplary embodiment.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram of a picture encryption apparatus illustrated by an exemplary embodiment.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic structural diagram of a computer device according to an exemplary embodiment.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a structural block diagram of a computer device according to an exemplary embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0068" num="0067">Exemplary embodiments are described in detail herein, and examples thereof are shown in the accompanying drawings. When the following descriptions are made with reference to the accompanying drawings, unless otherwise indicated, the same numbers in different accompanying drawings represent the same or similar elements. The following implementations described in the following exemplary embodiments do not represent all implementations that are consistent with this application. Instead, they are merely examples of apparatuses and methods consistent with aspects related to this application as recited in the appended claims.</p><p id="p-0069" num="0068">&#x201c;Several&#x201d; mentioned herein refers to one or more, and &#x201c;more&#x201d; refers to two or more than two. And/or describes an association relationship for describing associated objects and represents that three relationships may exist. For example, A and/or B may represent the following three cases: Only A exists, both A and B exist, and only B exists. The character &#x201c;/&#x201d; generally indicates an &#x201c;or&#x201d; relationship between the associated objects.</p><p id="p-0070" num="0069">The solutions shown in the embodiments of this disclosure may be implemented in a process of, for example, face swiping payment, by virtue of artificial intelligence (AI).</p><p id="p-0071" num="0070">Face recognition is a bioidentification technology for identity recognition based on feature information of a human face. Face recognition acquires, by using a vidicon or a camera, an image or a video stream that includes a human face, and automatically detects and tracks the human face in the image, so as to further perform a series of related application operations on the detected face image. It technically includes image acquisition, feature localization, identity verification and searching.</p><p id="p-0072" num="0071">After face pictures of a user having the time sequence are acquired, by way of feature extraction, the prediction information corresponding to the face pictures is outputted by the recurrent network, and the face pictures are encrypted based on the prediction information corresponding to the face pictures, so that the safety of the encrypted face image is improved. In one example, each of the face pictures may be encrypted using the prediction information of all the face pictures (of different persons), to at least introduce more randomness.</p><p id="p-0073" num="0072">The solutions provided in the embodiments of this disclosure involve technologies such as face recognition and machine learning of AI, and are specifically described by using the following embodiments.</p><p id="p-0074" num="0073">The solutions of the embodiments of this disclosure include a model training stage and an image processing stage. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a face picture encryption system according to an exemplary embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in the model training stage, a model training device <b>110</b> trains inputted each group of picture samples to obtain a machine learning model, and in the image processing stage, an image processing device <b>120</b> may output the prediction information corresponding to the group of first pictures according to the trained machine learning model and the inputted to-be-encrypted each group of first pictures, and perform data packing on the prediction information and the group of first pictures in a certain data format, so as to complete encryption of the group of first pictures.</p><p id="p-0075" num="0074">The above-mentioned model training device <b>110</b> and the image processing device <b>120</b> may be computer devices with a machine learning capability. For example, the computer device may be a fixed computer device such as a personal computer, a server, and a fixed scientific research device; and alternatively, the computer device may be a mobile computer device such as a tablet computer and an e-book reader.</p><p id="p-0076" num="0075">In a possible implementation, the model training device <b>110</b> and the image processing device <b>120</b> may be the same device; and alternatively, the model training device <b>110</b> and the image processing device <b>120</b> may be different devices. Moreover, when the model training device <b>110</b> and the image processing device <b>120</b> are different devices, the model training device <b>110</b> and the image processing device <b>120</b> may be devices of the same type. For example, the model training device <b>110</b> and the image processing device <b>120</b> may both be personal computers; and alternatively, the model training device <b>110</b> and the image processing device <b>120</b> may also be devices of different types. For example, the model training device <b>110</b> may be a server, and the image processing device <b>120</b> may be a fixed scientific research device, or the like. Specific types of the model training device <b>110</b> and the image processing device <b>120</b> are not limited in the embodiments of this disclosure.</p><p id="p-0077" num="0076">The face picture acquisition terminal <b>130</b> may be a smartphone, a tablet computer, a laptop computer, a desktop computer, a cash device or the like having a face image recognition function, but is not limited thereto.</p><p id="p-0078" num="0077">For example, the picture acquisition terminal <b>130</b> is a face acquisition terminal, and the face acquisition terminal may be a terminal having a face recognition function.</p><p id="p-0079" num="0078">The image processing device <b>120</b> may be an independent physical server, or may be a server cluster or a distributed system formed by a plurality of physical servers, or may be a cloud server that provides a basic cloud computing service such as a cloud service, a cloud database, cloud computing, a cloud function, cloud storage, a network service, cloud communication, a middleware service, a domain name service, a security service, a content delivery network (CDN), big data, and an artificial intelligence platform.</p><p id="p-0080" num="0079">In a possible implementation, the face picture encryption system is applied to an intelligent payment platform, where the picture acquisition terminal <b>130</b> may be a face acquisition terminal, for example, a large screen self-assisted cash device. When the user uses the large screen self-assisted cash device, the large screen self-assisted cash device may acquire face videos of the user within a period of time. The face videos include several face pictures having a time sequence, the face pictures may be pictures including face images and are uploaded to the image processing device <b>120</b> via a transmission network. The image processing device <b>120</b> performs feature extraction and operation on the face pictures having the time sequence through the machine learning model, and finally outputs the prediction information corresponding to the face pictures, and encrypts the face pictures and the corresponding prediction information in an appointed data format, so as to generate the encrypted face image.</p><p id="p-0081" num="0080">The face picture acquisition terminal <b>130</b> and the image processing device <b>120</b> are connected through a communication network. Optionally, the communication network is a wired network or a wireless network.</p><p id="p-0082" num="0081">Optionally, the wireless network or the wired network uses a standard communication technology and/or protocol. The network is usually the Internet, but may alternatively be any other networks, including but not limited to a local area network (LAN), a metropolitan area network (MAN), a wide area network (WAN), a mobile, wired, or wireless network, or any combination of a dedicated network or a virtual dedicated network). In some embodiments, technologies and/or formats, such as HyperText Markup Language (HTML) and extensible markup language (XML), are used for representing data exchanged through a network. In addition, all or some links may be encrypted by using conventional encryption technologies such as a secure socket layer (SSL), transport layer security (TLS), a virtual private network (VPN), and internet protocol security (IPsec). In other embodiments, custom and/or dedicated data communication technologies may also be used in place of or in addition to the foregoing data communication technologies. This is not limited in this application.</p><p id="p-0083" num="0082">The embodiments of this disclosure may be applied to scenarios for encrypting face pictures. The first pictures are acquired by the face acquisition terminal. When the face recognition technology is applied to intelligent payment, the face acquisition terminal acquires the face images. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of a face acquisition terminal according to an exemplary embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the face acquisition terminal <b>20</b> includes a camera component <b>21</b>, a display screen <b>22</b>, a certificate printing area <b>23</b> and a commodity scanning area <b>24</b>.</p><p id="p-0084" num="0083">The camera component <b>21</b> may acquire the face image of the user, and the display screen <b>22</b> may display the acquired face image and an acquisition result.</p><p id="p-0085" num="0084">In the payment process, the user may input commodity information of a to-be-paid commodity in the commodity scanning area <b>24</b> and then start a flow of face recognition payment.</p><p id="p-0086" num="0085">The display screen <b>22</b> may support display of interface information and may be touched by the user to operate.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of a picture encryption method according to an exemplary embodiment. The picture encryption method may be executed by a computer device. The computer device may be either a terminal or a server; and alternatively, the computer device may include the terminal and the server. For example, the above-mentioned computer device may be the image processing device <b>120</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the picture encryption method includes the following steps:</p><p id="p-0088" num="0087">In S<b>301</b>, N first pictures having a time sequence are acquired, where N is an integer equal to or greater than 2.</p><p id="p-0089" num="0088">In the embodiments of this disclosure, the computer device acquires an image video uploaded by the picture acquisition terminal within an appointed period of time and acquires the N first pictures arranged according to the time sequence from the image video. N is an integer greater than or equal to 2.</p><p id="p-0090" num="0089">In a possible implementation, time intervals among the N first pictures having the time sequence are identical.</p><p id="p-0091" num="0090">The computer device may acquire each frame of face picture in the image video as the N first pictures.</p><p id="p-0092" num="0091">In S<b>302</b>, feature extraction is performed on the N first pictures to acquire picture features of the N first pictures.</p><p id="p-0093" num="0092">In the embodiments of this disclosure, the computer device performs feature extraction on the acquired N first pictures to acquire picture features of the N first pictures.</p><p id="p-0094" num="0093">The computer device may perform feature extraction on the N first pictures through an image feature extraction network in the trained image prediction model to acquire picture features corresponding to the N first pictures.</p><p id="p-0095" num="0094">In a possible implementation, the image prediction model is a neural network model trained by a gradient descent algorithm by taking a cross entropy as a loss function based on a TensorFlow machine learning library.</p><p id="p-0096" num="0095">Feature extraction on the N first pictures may be performed by the image feature extraction network in the image predication model. The image feature extraction network includes a convolutional layer and a pooling layer.</p><p id="p-0097" num="0096">In S<b>303</b>, target prediction is successively performed on the N first pictures according to the time sequence to obtain prediction information of the N first pictures, where the target prediction refers to prediction on the first pictures based on status information, and the status information is information which is updated based on the picture features of the first pictures that have been predicted. In example implementations, the status information may be a combination of picture features of pictures that have been predicted. Therefore, the status information is updated with each prediction iteration and is correlated with history prediction results. In example implementations, the already predicted pictures may each correspond to different persons (e.g., each predicted picture include a facial image of a different person). Therefore, the status information may be a mixture/combination of picture features corresponding to different persons.</p><p id="p-0098" num="0097">In the embodiments of this disclosure, the computer device may successively perform target prediction on the N first pictures according to the time sequence to obtain the prediction information of the N first pictures.</p><p id="p-0099" num="0098">The picture features corresponding to the N first pictures obtained by feature extraction are successively inputted into a recurrent network in an image prediction model according to the time sequence of the N first pictures and are calculated by the recurrent network to output the prediction information of the N first pictures.</p><p id="p-0100" num="0099">Optionally, the target prediction refers to prediction on the first pictures based on current status information, and the current status information is information which is updated based on the picture features of pictures that have been predicted.</p><p id="p-0101" num="0100">The recurrent network can be added into the image prediction model behind the image feature extraction network. The status information of the current recurrent network may be determined according to the picture features of face pictures that have been inputted. The computer device may predict the currently inputted picture features according to the status information of the current recurrent network and output a prediction result.</p><p id="p-0102" num="0101">In S<b>304</b>, the N first pictures are encrypted based on the prediction information of the N first pictures.</p><p id="p-0103" num="0102">In the embodiments of this disclosure, the computer device may process the first pictures based on the prediction information of the N first pictures, so as to encrypt the N first pictures.</p><p id="p-0104" num="0103">The computer device may encrypt the N first pictures of a human based on the outputted prediction information after the picture features of the N first pictures are inputted into the recurrent network.</p><p id="p-0105" num="0104">In a possible implementation, the prediction information corresponding to the N first pictures is combined with picture data of the N first pictures according to an appointed data structure, so as to generate the encrypted N first pictures.</p><p id="p-0106" num="0105">In conclusion, after the first pictures having the time sequence are subjected to feature extraction, the first pictures are subjected to target prediction to obtain the prediction information corresponding to the first pictures, and the first pictures are encrypted based on the prediction information corresponding to the first picture. Prediction is performed through the updated status information of the picture features of the pictures that has been predicted, so as to obtain current prediction information that predicts the picture features of the first pictures. Through the above-mentioned solution, the prediction information of the first pictures is affected by considering relevance of the picture features of the first pictures in time dimension, so that a condition that a first image is easily cracked for an encryption format only introducing a non-neural network safety factor is avoided, and therefore, the safety of the encrypted first image is also improved while the encryption format that encrypts the first pictures is expanded.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of a picture encryption method according to an exemplary embodiment. The picture encryption method may be executed by a computer device. The computer device may be either a terminal or a server; and alternatively, the computer device may include the terminal and the server. For example, the above-mentioned computer device may be the image processing device <b>120</b> shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the picture encryption method includes the following steps:</p><p id="p-0108" num="0107">In S<b>401</b>, training data is acquired.</p><p id="p-0109" num="0108">In the embodiments of this disclosure, the model training device acquires the training data for model training.</p><p id="p-0110" num="0109">The training data includes K picture samples having a time sequence and annotated information of the K picture samples.</p><p id="p-0111" num="0110">The annotated information may be used for indicating whether the first pictures include a target object or the first pictures include the target object with the image quality satisfying a quantity condition. The annotated information may further be a label of the picture including the target object.</p><p id="p-0112" num="0111">In a possible implementation, the first pictures are images in each video acquired by the face acquisition device.</p><p id="p-0113" num="0112">For example, with respect to a condition that the sample is the face image, the face acquisition device may acquire multiple segments of face images, and the multiple segments of face images include half of positive sample face images and half of negative sample face images. Each frame of face pictures included in the face images as the positive sample face images is the face picture with the image quality satisfying the quality condition. Each frame of face pictures included in the face images as the negative sample face images is not the face picture or is the face image with the image quality dissatisfying the quality condition. The acquired training data is preprocessed, namely, data cleaning is performed, so as to screen out invalid face pictures in the training data.</p><p id="p-0114" num="0113">The invalid face pictures may be face pictures, where face data in the pictures cannot be extracted due to too dark or black pictures.</p><p id="p-0115" num="0114">In S<b>402</b>, feature extraction is performed on the K picture samples to acquire picture features corresponding to the K picture samples.</p><p id="p-0116" num="0115">In the embodiments of this disclosure, the computer device for model training may perform feature extraction on the K picture samples to acquire picture features corresponding to the K picture samples.</p><p id="p-0117" num="0116">In a case that the first picture sample is a face image sample, the computer device for model training may perform partitioning processing on the K face picture samples to acquire L face picture segment samples corresponding to the K face picture samples, and perform feature extraction on the L face picture segment samples of the K face pictures based on the attention weights corresponding to the L face picture segment samples to acquire the picture features corresponding to the K face pictures.</p><p id="p-0118" num="0117">K is an integer greater than or equal to 2. L is a positive integer equal to or greater than 1.</p><p id="p-0119" num="0118">Exemplarily, the computer device partitions the face picture samples in the face image samples according to various sizes of windows, converts RGB three-colored face picture segment samples into gray face picture segment samples, then performs feature information extraction at different orders of magnitudes of fast Fourier transform to acquire at least one of energy distribution feature information, high and low frequency distribution feature information, flatness feature information and spectral centroid feature information of the image frequency spectrogram corresponding to the face picture segment samples, splices the gray values of the face picture segment samples with the correspondingly acquired appointed type feature information, inputs the spliced gray value and feature information into an input layer of the image prediction model, and performs feature extraction on the face picture segment samples through the convolutional layer, the pooling layer and the attention mechanism layer, so as to acquire the picture features corresponding to the K face pictures.</p><p id="p-0120" num="0119">In S<b>403</b>, the picture features corresponding to the K picture samples are successively inputted into the recurrent network in the image prediction model according to the time sequence to obtain the prediction information of the K picture samples outputted by the recurrent network.</p><p id="p-0121" num="0120">In the embodiments of this disclosure, the computer device successively inputs the picture features corresponding to the K picture samples into the recurrent network in the image prediction model according to the time sequence to obtain the prediction information of the K picture samples outputted by the recurrent network based on the status information at the moment.</p><p id="p-0122" num="0121">In a case that the first picture sample is the face image sample, the computer device successively inputs the picture features corresponding to the K face picture samples outputted by the image feature extraction network into the recurrent network in the image prediction model according to the time sequence to obtain the prediction information of the K face picture samples outputted by the recurrent network.</p><p id="p-0123" num="0122">The prediction information is used for indicating the prediction results of the corresponding face picture samples; and the recurrent network is a network that predicts the inputted picture features through the status information, and the status information is information for updating based on the picture features of the face picture samples that have been inputted.</p><p id="p-0124" num="0123">In a possible implementation, the L face picture segment samples corresponding to the K face picture samples are successively inputted into the recurrent network according to the time sequence, so as to output the prediction information corresponding to the K face picture samples, where the prediction information corresponding to the face picture samples is a vector of the prediction result including L face picture segment samples.</p><p id="p-0125" num="0124">In a possible implementation, the recurrent network includes at least one of a bidirectional gated recurrent unit (BGRU), a gated recurrent unit (GRU) and a long short term memory (LSTM) network.</p><p id="p-0126" num="0125">For example, <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic structural diagram of a BGRU involved in an embodiment of this application. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the BGRU is a common bi-directional gated recurrent neural network, where its input is the picture feature x[t] inputted at the current moment and a hidden status h[t&#x2212;1] at a previous moment, namely, the outputted prediction result y[t] will be affected by the picture feature inputted at the current moment t and the hidden status at the previous moment t&#x2212;1.</p><p id="p-0127" num="0126">In S<b>404</b>, prediction results of the K picture samples are acquired based on the prediction information of the K picture samples.</p><p id="p-0128" num="0127">In the embodiments of this disclosure, the computer device may acquire whether the K picture samples include the target object with the image quality satisfying the quality condition according to the prediction information of the K picture samples.</p><p id="p-0129" num="0128">In a case that the first picture sample is the face image sample, the computer device may determine whether the group of training samples are the target face pictures according to the prediction information of the K face picture samples. The target face pictures may face images with image qualities satisfying quality conditions.</p><p id="p-0130" num="0129">In S<b>405</b>, a loss function value is acquired based on the prediction results of the K picture samples and the annotated information of the K picture samples.</p><p id="p-0131" num="0130">In a possible implementation, the computer device counts the prediction results of the K picture samples, and determines the loss function value of the model according to whether the indicated prediction results include the target object and whether the pre-annotated information of the K picture samples include the target object.</p><p id="p-0132" num="0131">In a case that the first picture sample is the face image sample, the computer device counts the prediction results of the K face picture samples, and determines the loss function value of the model according to whether the prediction results are predicted as the target face and whether the pre-annotated information of the K picture samples is the target face.</p><p id="p-0133" num="0132">In S<b>406</b>, the image prediction model is updated based on the loss function value.</p><p id="p-0134" num="0133">In the embodiments of this disclosure, the computer device updates model parameters in the image prediction model based on the calculated loss function value till the model training is completed.</p><p id="p-0135" num="0134">In a possible implementation, parameter updating is performed on at least one network of the recurrent network, the partitioning network and the image feature extraction network based on the loss function value.</p><p id="p-0136" num="0135">The updated model parameters include, but not limited to, the attention weights and the sizes of the target windows.</p><p id="p-0137" num="0136">In a possible implementation, after the image prediction model is trained, model evaluation on the image prediction model is performed through a test set.</p><p id="p-0138" num="0137">In a case that the first picture sample is the face image sample, the computer device may convert each group of face pictures into each group of gray pictures by acquiring each group of face pictures in the test set, then partition each group of gray pictures, extract feature information of each group of gray pictures, predict the several partitioned face picture segments subjected to feature extraction through the recurrent network, and count the prediction results of face picture segments by adopting a voting method. If the face picture segment is predicted as the target face, a vote is casted to the positive sample, and otherwise, a vote is casted to the negative sample.</p><p id="p-0139" num="0138">In S<b>407</b>, N first pictures having a time sequence are acquired.</p><p id="p-0140" num="0139">In the embodiments of this disclosure, the N first pictures having the time sequence are acquired by an image acquisition device.</p><p id="p-0141" num="0140">In a possible implementation, all or part of pictures in the N first pictures include the face images, and the prediction information is used for indicating whether the first pictures include the face images with image qualities satisfying quality conditions.</p><p id="p-0142" num="0141">That is to say, the image acquisition device may be a face acquisition device, and the first pictures may be face pictures.</p><p id="p-0143" num="0142">In a possible implementation, the face acquisition device acquires the face images within an appointed period of time and uploads them to the server, and the server acquires N face pictures from the face images, where N is an integer equal to or greater than 2.</p><p id="p-0144" num="0143">The appointed period of time may be either a preset duration or a duration determined by the face acquisition terminal according to a face recognition condition.</p><p id="p-0145" num="0144">Exemplarily, a developer may directly set the duration during which the face acquisition terminal acquires the face images to be 5 s. When the face acquisition terminal starts to acquire the face images for 5 s, the acquired 5 s face images are uploaded to the server; and alternatively, the face acquisition terminal determines the acquisition duration according to the qualities of the currently acquired face images, and the qualities of the face images acquired by the face acquisition terminal are negatively correlated to the acquisition duration.</p><p id="p-0146" num="0145">Acquisition of the N face pictures from the face images may be acquisition by taking a frame as a unit, namely, the N face pictures include each frame of face picture in the acquired face images.</p><p id="p-0147" num="0146">In a possible implementation, the N face pictures are face pictures successively acquired according to the time sequence in a process of face recognition at a single time.</p><p id="p-0148" num="0147">For example, <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of continuous time face image acquisition involved in an embodiment of this application. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, when the user faces the face acquisition terminal, the face acquisition terminal displays a first status picture <b>61</b> to remind the user to adjust. When the face acquisition terminal starts to perform face image acquisition, the face acquisition terminal displays a second status picture <b>62</b>. There is a countdown indication in the display area, and when countdown is completed, the N face pictures are acquired completely.</p><p id="p-0149" num="0148">In S<b>408</b>, through a partitioning network in the image prediction model, partitioning processing is performed on the N first pictures according to a size of a target window to acquire M first picture segments corresponding to the N first pictures.</p><p id="p-0150" num="0149">In the embodiments of this disclosure, the computer device may partition the N first pictures according to the target window through the partitioning network by inputting the N first pictures into the partitioning network in the image prediction module to obtain the M first picture segments corresponding to the N first pictures.</p><p id="p-0151" num="0150">M is an integer equal to or greater than 1; and the size of the target window may be a network parameter of the partitioning network.</p><p id="p-0152" num="0151">In a possible implementation, the size of the target window is a model parameter updated in the training stage of the image prediction model.</p><p id="p-0153" num="0152">Exemplarily, by taking a condition that the first picture is the face picture as an example, when the size of the target window is 2&#xd7;2, and if the face picture is a 4&#xd7;4 picture, the face picture subjected to the partitioning network may be divided into 4 face picture segments.</p><p id="p-0154" num="0153">In S<b>409</b>, feature extraction is performed on the M first picture segments corresponding to the N first pictures based on attention weights corresponding to the M first picture segments to acquire picture features of the N first pictures.</p><p id="p-0155" num="0154">In the embodiments of this disclosure, the computer device may perform feature extraction on the M first picture segments corresponding to the N first pictures based on attention weights corresponding to the M first picture segments to acquire picture features of the N first pictures.</p><p id="p-0156" num="0155">In a possible implementation, the computer device preprocesses the M first picture segments corresponding to the target picture to obtain the preprocessed features of the M first picture segments corresponding to the target picture, and then inputs the preprocessed features of the M first picture segments corresponding to the target picture into the image feature extraction network in the image prediction model to obtain the picture features of the target picture outputted by the image feature extraction network.</p><p id="p-0157" num="0156">Exemplarily, the target picture may be the target face picture when being applied to a scenario for encrypting the face picture.</p><p id="p-0158" num="0157">The target face picture is any one of the N face pictures. The image feature extraction network includes the attention mechanism layer. The attention mechanism layer is used for processing the preprocessed features of the M face picture segments by taking the attention weights of the M face picture segments as parameters.</p><p id="p-0159" num="0158">In a possible implementation, the computer device acquires the gray value of the target picture segment, then acquires the first type feature information of the target picture segment based on fast Fourier transform of first order of magnitude, and combines the gray value of the target picture segment with the first type feature information of the target picture segment to obtain the preprocessed features of the target picture segment.</p><p id="p-0160" num="0159">The target picture segment is any one of the M first picture segments corresponding to the target picture.</p><p id="p-0161" num="0160">Exemplarily, the computer device may input the M first picture segments corresponding to the N first pictures into the image feature extraction network, and perform feature extraction on the M first picture segments through the image feature extraction network to obtain the picture segment features corresponding to the M first picture segments, and the image feature extraction network includes the attention mechanism layer. The attention mechanism layer may be used for distributing the attention weight for the first picture segments. The picture features of the N first pictures corresponding to the M first picture segments are acquired based on the picture segment features.</p><p id="p-0162" num="0161">The image feature extraction network may include the convolutional layer and the pooling layer.</p><p id="p-0163" num="0162">Exemplarily, the first layer of the image prediction model is an input layer, the convolutional layer using 64 convolution kernels (the size of the convolution kernels is 3&#xd7;3, the stride is 1 and the padding is 1) is taken as a second layer, the pooling layer with windows of 2&#xd7;2 and stride of 2 is taken as a third layer, the convolutional layer using 128 convolution kernels (the size of the convolution kernels is 3&#xd7;3, the stride is 1 and the padding is 1) is taken as a fourth layer, the pooling layer with windows of 2&#xd7;2 and stride of 2 is taken as a fifth layer, the convolutional layer using 256 convolution kernels (the size of the convolution kernels is 3&#xd7;3, the stride is 1 and the padding is 1) is taken as a sixth layer, the pooling layer with windows of 2&#xd7;2 and stride of 2 is taken as a seventh layer, a batch normalization layer is added into an eighth layer, and a max pooling layer is added into a ninth layer. The attention mechanism layer may be located in front of the convolutional layer or the pooling layer.</p><p id="p-0164" num="0163">In a possible implementation, the first type feature information includes at least one of energy distribution feature information, high and low frequency distribution feature information, flatness feature information and spectral centroid feature information of an image frequency spectrogram (or spectrum graph).</p><p id="p-0165" num="0164">For example, the computer device may extract the energy distribution feature of the image frequency spectrogram in a case that the order of magnitude of fast Fourier transform (FFT Size) is 4096, and normalize the energy distribution feature of the image frequency spectrogram. The computer device may extract the high and low frequency distribution feature in a case that the order of magnitude of fast Fourier transform (FFT Size) is 2048, and normalize the high and low frequency distribution feature. The computer device may extract the flatness feature in a case that the order of magnitude of fast Fourier transform (FFT Size) is 1024, and normalize the flatness feature. The computer device may extract the spectral centroid feature in a case that the order of magnitude of fast Fourier transform (FFT Size) is 1024, and normalize the spectral centroid feature.</p><p id="p-0166" num="0165">In S<b>410</b>, the picture features of the N first pictures are successively inputted into a recurrent network in an image prediction model according to the time sequence to obtain the prediction information of the N first pictures outputted by the recurrent network.</p><p id="p-0167" num="0166">In the embodiments of this disclosure, the computer device successively inputs the picture features corresponding to the N first pictures outputted by the image feature extraction network into the recurrent network in the image prediction model according to the time sequence to obtain the prediction information of the N first pictures outputted by the recurrent network.</p><p id="p-0168" num="0167">In a possible implementation, the M first picture segments are successively inputted into the recurrent network according to the time sequence of the corresponding N first pictures to obtain the prediction results corresponding to the first picture segments outputted by the recurrent network. The prediction information of the N first pictures is acquired based on the prediction results of the M first picture segments.</p><p id="p-0169" num="0168">The prediction information may be used for indicating the prediction results of the first pictures; and the recurrent network is a network that predicts the inputted picture features through the status information, and the status information is information for updating based on the picture features of the face pictures that have been inputted.</p><p id="p-0170" num="0169">In a possible implementation, when the first pictures are the face pictures, the prediction information is information for indicating whether the corresponding face pictures include the faces, or the prediction information is information for indicating whether the corresponding face pictures include the face images with the image qualities satisfying the quality conditions.</p><p id="p-0171" num="0170">In a possible implementation, the recurrent network includes at least one of a bi-directional gated recurrent unit (BGRU), a gated recurrent unit (GRU) and a long short term memory (LSTM) network.</p><p id="p-0172" num="0171">For example, the tenth layer of the image prediction model can be added with the bi-directional gated recurrent unit (BGRU), where a hidden unit is 256. When 2 face pictures respectively correspond to 2 face picture segments, the face pictures are successively inputted into the recurrent network according to the time sequence. The 2 face picture segments corresponding to a first face picture sample are inputted first. If the prediction result of the face picture segment includes face information, the prediction result corresponding to the face picture segment is 1, and otherwise, it is 0. When the 2 face picture segments both are predicted to have the face information, the prediction information corresponding to the first face picture is a vector of (1, 1).</p><p id="p-0173" num="0172">In a possible implementation, the prediction results of the M face picture segments in the N face pictures are outputted by way of a sigmoid function adopted in the last layer of the image prediction model.</p><p id="p-0174" num="0173">The prediction information may be an outputted logits value of M&#xd7;Y, where M is the number of segments of an inputted image of the model, and Y is the prediction result of each segment, which is 0 or 1.</p><p id="p-0175" num="0174">For example, <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic structural diagram of an image prediction model involved in the embodiments of this disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a frequency spectrogram <b>71</b> corresponding to the face picture is inputted into the convolutional layers for feature extraction, then the extracted features are outputted and calculated in time dimension by way of the recurrent network <b>72</b>, and a prediction label <b>73</b> corresponding to the frequency spectrogram <b>71</b> is outputted by way of a fully connected layer.</p><p id="p-0176" num="0175">In a possible implementation, the prediction information includes prediction sub-information of the M first picture segments of the corresponding first pictures, the prediction sub-information being used for indicating prediction results of the corresponding first picture segments.</p><p id="p-0177" num="0176">In S<b>411</b>, the N first pictures are spliced with the corresponding prediction information to generate encrypted picture data of the N first pictures.</p><p id="p-0178" num="0177">In the embodiments of this disclosure, the computer device may splice the N first pictures with the corresponding prediction information to obtain the encrypted picture data of the N first pictures.</p><p id="p-0179" num="0178">The computer device may splice the N face pictures with the corresponding prediction information to obtain the encrypted picture data of the N face pictures.</p><p id="p-0180" num="0179">In a possible implementation, the N face pictures are spliced with the prediction information based on a target data structure to generate the encrypted N face pictures.</p><p id="p-0181" num="0180">Exemplarily, the target data structure may be the encrypted face pictures generated by splicing in format of {prediction information} {payload}, where payload is the face data corresponding to the face pictures.</p><p id="p-0182" num="0181">In another possible implementation, the N face pictures are spliced with the prediction information and attribute information of the face data based on the target data structure to generate the encrypted N face pictures.</p><p id="p-0183" num="0182">In a possible implementation, the attribute information of the face data includes at least one of timestamp, counter information (counter), magic number information (magic_num), device model information (device_info), signature algorithm edition information (sign_version) and random number information (random).</p><p id="p-0184" num="0183">Exemplarily, the target data structure may be the encrypted face pictures generated by splicing in format of {magic_num} {device_info} {sign_version} {timestamp} {counter} {prediction information} {random} {payload}.</p><p id="p-0185" num="0184">In conclusion, after the first pictures having the time sequence are subjected to feature extraction, the first pictures are subjected to target prediction to obtain the prediction information corresponding to the first pictures, and the first pictures are encrypted based on the prediction information corresponding to the first picture. Prediction is performed through the updated status information of the picture features of the first pictures that has been predicted, so as to obtain current prediction information that predicts the picture features of the first pictures. Through the above-mentioned solution, the prediction information of the first pictures is affected by considering relevance of the picture features of the first pictures in time dimension, so that a condition that a first image is easily cracked for an encryption format only introducing a non-neural network safety factor is avoided, and therefore, the safety of the encrypted first image is also improved while the encryption format that encrypts the first pictures is expanded.</p><p id="p-0186" num="0185"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic structural diagram of a face picture encryption system according to an exemplary embodiment. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the face acquisition terminal <b>81</b> acquires the N face pictures of the user having the time sequence, and all or part of the N face pictures include the face images, where the N face pictures are the face picture 1 corresponding to a moment t1, the face picture 2 corresponding to a moment t2, . . . . the face picture N corresponding to a moment tN. The N face pictures are transmitted to the server <b>82</b> and are partitioned through the partitioning network <b>821</b>, the face picture segments have the attention weights correspondingly, and the face picture segments having the attention weights are inputted into the image feature extraction network <b>822</b>. The image feature extraction network <b>822</b> includes the convolutional layer and the pooling layer. The extracted extraction features corresponding to the N face pictures are inputted into the recurrent network <b>823</b> according to the time sequence, the hidden status corresponding to the next moment and the prediction information corresponding to the face picture 1 are outputted based on the extraction feature corresponding to the face picture 1 and the hidden status of the current BGRU unit, and then the hidden status corresponding to the next moment and the prediction information corresponding to the face picture 2 are outputted based on the extraction feature corresponding to the face picture 2 and the hidden status of the processed face picture 1 till the prediction information corresponding to the face picture N is outputted completely, and the picture data of the N face pictures are spliced with the corresponding prediction information to generate the encrypted face pictures.</p><p id="p-0187" num="0186">In conclusion, after the first pictures having the time sequence are subjected to feature extraction, the first pictures are subjected to target prediction to obtain the prediction information corresponding to the first pictures, and the first pictures are encrypted based on the prediction information corresponding to the first picture. Prediction is performed through the updated status information of the picture features of the first pictures that has been predicted, so as to obtain current prediction information that predicts the picture features of the first pictures. Through the above-mentioned solution, the prediction information of the first pictures is affected by considering relevance of the picture features of the first pictures in time dimension, so that a condition that a first image is easily cracked for an encryption format only introducing a non-neural network safety factor is avoided, and therefore, the safety of the encrypted first image is also improved while the encryption format that encrypts the first pictures is expanded.</p><p id="p-0188" num="0187"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram of a picture encryption apparatus according to an exemplary embodiment. In this disclosure, a unit and a module may be hardware such as a combination of electronic circuitries; firmware; or software such as computer instructions. The unit and the module may also be any combination of hardware, firmware, and software. In some implementation, a unit may include at least one module. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the picture encryption apparatus may include:</p><p id="p-0189" num="0188">a picture acquisition module <b>910</b> configured to acquire N first pictures having a time sequence, where N is an integer equal to or greater than 2;</p><p id="p-0190" num="0189">a feature extraction module <b>920</b> configured to perform feature extraction on the N first pictures to acquire picture features of the N first pictures;</p><p id="p-0191" num="0190">an information output module <b>930</b> configured to successively perform target prediction on the N first pictures according to the time sequence to obtain prediction information of the N first pictures, where the target prediction refers to prediction on the first pictures based on status information, and the status information is information for updating based on the picture features of the first pictures that have been predicted; and</p><p id="p-0192" num="0191">a picture encryption module <b>940</b> configured to encrypt the N first pictures based on the prediction information of the N first pictures.</p><p id="p-0193" num="0192">In a possible implementation, the information output module <b>930</b> includes:</p><p id="p-0194" num="0193">an information output sub-module configured to successively input the picture features of the N first pictures into a recurrent network in an image prediction model according to the time sequence to obtain the prediction information of the N first pictures outputted by the recurrent network.</p><p id="p-0195" num="0194">In a possible implementation, the apparatus further includes:</p><p id="p-0196" num="0195">an picture segment acquisition module configured to, through a partitioning network in the image prediction model, perform partitioning processing on the N first pictures according to a size of a target window to acquire M first picture segments corresponding to the N first pictures before the performing feature extraction on the N first pictures to acquire picture features corresponding to the N first pictures, where M is an integer equal to or greater than 1, and the size of the target window is a network parameter of the partitioning network; and</p><p id="p-0197" num="0196">the feature extraction module <b>920</b> includes:</p><p id="p-0198" num="0197">a feature extraction sub-module configured to perform feature extraction on the M first picture segments of the N first pictures based on attention weights of the M first picture segments to obtain picture features of the N first pictures.</p><p id="p-0199" num="0198">In a possible implementation, the prediction information includes prediction sub-information of the M first picture segments of the corresponding first pictures, the prediction sub-information being used for indicating prediction results of the first picture segments.</p><p id="p-0200" num="0199">In a possible implementation, the feature extraction sub-module includes:</p><p id="p-0201" num="0200">a feature preprocessing unit for preprocessing the M first picture segments of a target picture to obtain preprocessed features of the M first picture segments of the target picture, where the target picture is any one of the N first pictures; and</p><p id="p-0202" num="0201">a target feature acquisition unit for inputting the preprocessed features of the M first picture segments of the target picture into an image feature extraction network in the image prediction model to obtain the picture feature of the target picture outputted by the image feature extraction network,</p><p id="p-0203" num="0202">where the image feature extraction network includes an attention mechanism layer used for processing the preprocessed features of the M first picture segments by taking the attention weights of the M first picture segments as parameters.</p><p id="p-0204" num="0203">In a possible implementation, the feature preprocessing unit is configured to:</p><p id="p-0205" num="0204">acquire a gray value of a target picture segment, where the target picture segment is any one of the M first picture segments corresponding to the target picture;</p><p id="p-0206" num="0205">acquire first type feature information of the target picture segment based on fast Fourier transform of a first order of magnitude; and</p><p id="p-0207" num="0206">combine the gray value of the target picture segment with the first type feature information of the target picture block to obtain a preprocessed feature of the target picture segment.</p><p id="p-0208" num="0207">In a possible implementation, the first type feature information includes:</p><p id="p-0209" num="0208">at least one of energy distribution feature information, high and low frequency distribution feature information, flatness feature information and spectral centroid feature information of an image frequency spectrogram.</p><p id="p-0210" num="0209">In a possible implementation, the apparatus further includes:</p><p id="p-0211" num="0210">a training data acquisition module configured to acquire training data before acquiring N first pictures with a time sequence, where the training data includes K picture samples having a time sequence and annotated information of the K picture samples, K being an integer equal to or greater than 2;</p><p id="p-0212" num="0211">a sample feature extraction module configured to perform feature extraction on the K picture samples to acquire picture features corresponding to the K picture samples;</p><p id="p-0213" num="0212">a sample prediction information acquisition module configured to successively input the picture features corresponding to the K picture samples into the recurrent network in the image prediction model according to the time sequence to obtain the prediction information of the K picture samples outputted by the recurrent network;</p><p id="p-0214" num="0213">a sample prediction result acquisition module configured to acquire prediction results of the K picture samples based on the prediction information of the K picture samples;</p><p id="p-0215" num="0214">a loss function value acquisition module configured to acquire a loss function value based on the prediction results of the K picture samples and the annotated information of the K picture samples; and</p><p id="p-0216" num="0215">a model updating module configured to update the image prediction model based on the loss function value.</p><p id="p-0217" num="0216">In a possible implementation, the model updating module includes:</p><p id="p-0218" num="0217">a parameter updating sub-module configured to perform parameter updating on at least one network of the recurrent network, the partitioning network and the image feature extraction network based on the loss function value.</p><p id="p-0219" num="0218">In a possible implementation, the recurrent network includes:</p><p id="p-0220" num="0219">at least one of a bi-directional gated recurrent unit (BGRU), a gated recurrent unit (GRU) and a long short term memory (LSTM) network.</p><p id="p-0221" num="0220">In a possible implementation, the picture encryption module <b>940</b> includes:</p><p id="p-0222" num="0221">a picture encryption sub-module configured to splice the N first pictures with the corresponding prediction information to generate encrypted picture data of the N first pictures.</p><p id="p-0223" num="0222">In a possible implementation, all or part of pictures in the N first pictures include face images.</p><p id="p-0224" num="0223">In a possible implementation, the prediction information is used for indicating whether the first pictures include the face images with image qualities satisfying quality conditions.</p><p id="p-0225" num="0224">In conclusion, after the first pictures having the time sequence are subjected to feature extraction, the first pictures are subjected to target prediction to obtain the prediction information corresponding to the first pictures, and the first pictures are encrypted based on the prediction information corresponding to the first picture. Prediction is performed through the updated status information of the picture features of the first pictures that has been predicted, so as to obtain current prediction information that predicts the picture features of the first pictures. Through the above-mentioned solution, the prediction information of the first pictures is affected by considering relevance of the picture features of the first pictures in time dimension, so that a condition that a first image is easily cracked for an encryption format only introducing a non-neural network safety factor is avoided, and therefore, the safety of the encrypted first image is also improved while the encryption format that encrypts the first pictures is expanded.</p><p id="p-0226" num="0225"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic structural diagram of a computer device according to an exemplary embodiment. The computer device <b>1000</b> includes a central processing unit (CPU) <b>1001</b>, a system memory <b>1004</b> including a random access memory (RAM) <b>1002</b> and a read-only memory (ROM) <b>1003</b>, and a system bus <b>1005</b> connecting the system memory <b>1004</b> to the CPU <b>1001</b>. The computer device <b>1000</b> further includes a basic input/output (I/O) system <b>1006</b> assisting in transmitting information between components in a computer device, and a mass storage device <b>1007</b> configured to store an operating system <b>1013</b>, an application program <b>1014</b>, and another program module <b>1015</b>. Similarly, the input/output controller <b>1010</b> further provides output to a display screen, a printer, or other types of output devices.</p><p id="p-0227" num="0226">According to the embodiments of this disclosure, the computer device <b>1000</b> may further be connected, through a network such as the Internet, to a remote computer device on the network. That is, the computer device <b>1000</b> may be connected to a network <b>1012</b> by using a network interface unit <b>1011</b> connected to the system bus <b>1005</b>, or may be connected to another type of network or a remote computer device system (not shown) by using a network interface unit <b>1011</b>.</p><p id="p-0228" num="0227">The memory further includes one or more programs. The one or more programs are stored in the memory. The CPU <b>1001</b> executes the one or more programs to implement all or some steps of the method shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> or <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0229" num="0228"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a structural block diagram of a computer device <b>1100</b> according to an exemplary embodiment. The computer device <b>1100</b> may be the terminal in the image recognition system shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0230" num="0229">Generally, the computer device <b>1100</b> includes a processor <b>1101</b> and a memory <b>1102</b>.</p><p id="p-0231" num="0230">In some embodiments, the processor <b>1101</b> may further include an artificial intelligence (AI) processor. The AI processor is configured to process computing operations related to machine learning.</p><p id="p-0232" num="0231">The memory <b>1102</b> may include one or more computer-readable storage media that may be non-transitory. In some embodiments, the non-transitory computer-readable storage medium in the memory <b>1102</b> is configured to store at least one instruction, and the at least one instruction being configured to be executed by the processor <b>1101</b> to implement the method provided in the method embodiments of this disclosure.</p><p id="p-0233" num="0232">In some embodiments, the computer device <b>1100</b> further optionally includes a peripheral interface <b>1103</b> and at least one peripheral. The processor <b>1101</b>, the memory <b>1102</b>, and the peripheral interface <b>1103</b> may be connected through a bus or a signal cable. Each peripheral may be connected to the peripheral interface <b>1103</b> through a bus, a signal cable, or a circuit board. Specifically, the peripheral includes at least one of a radio frequency (RF) circuit <b>1104</b>, a display screen <b>1105</b>, a camera component <b>1106</b>, an audio circuit <b>1107</b>, a positioning component <b>1108</b>, and a power supply <b>1109</b>.</p><p id="p-0234" num="0233">In some embodiments, the computer device <b>1100</b> further includes one or more sensors <b>1110</b>. The one or more sensors <b>1110</b> include, but are not limited to, an acceleration sensor <b>1111</b>, a gyroscope sensor <b>1112</b>, a pressure sensor <b>1113</b>, a fingerprint sensor <b>1114</b>, an optical sensor <b>1115</b>, and a proximity sensor <b>1116</b>.</p><p id="p-0235" num="0234">A person skilled in the art may understand that the structure shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref> does not constitute any limitation on the computer device <b>1100</b>, and the computer device may include more components or fewer components than those shown in the figure, or some components may be combined, or a different component deployment may be used.</p><p id="p-0236" num="0235">In an exemplary embodiment, a non-temporary computer-readable storage medium including an instruction is further provided, for example, a memory including at least one instruction, at least one program, a code set, or an instruction set. The at least one instruction, the at least one program, the code set, or the instruction set may be executed by a processor to implement all or some steps of the method shown in any embodiment in <figref idref="DRAWINGS">FIG. <b>3</b></figref> or <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0237" num="0236">A person skilled in the art is to be aware that in the one or more examples, the functions described in the embodiments of this disclosure may be implemented by using hardware, software, firmware, or any combination thereof. When implemented by using software, the functions can be stored in a computer-readable medium or can be used as one or more instructions or code in a computer-readable medium for transmission. The computer-readable medium includes a computer storage medium and a communication medium, where the communication medium includes any medium that enables a computer program to be transmitted from one place to another. The storage medium may be any available medium accessible to a general-purpose or dedicated computer device.</p><p id="p-0238" num="0237">According to an aspect of this application, a computer program product or a computer program is provided, the computer program product or the computer program including computer instructions, the computer instructions being stored in a non-transitory computer-readable storage medium. A processor of a computer device reads the computer instructions from the non-transitory computer-readable storage medium, and executes the computer instructions, so that the computer device performs the picture encryption method provided in the various optional implementations of the foregoing aspects.</p><p id="p-0239" num="0238">Other embodiments of this disclosure will be apparent to a person skilled in the art from consideration of the specification and practice of this application here. This application is intended to cover any variation, use, or adaptive change of this application. These variations, uses, or adaptive changes follow the general principles of this application and include common general knowledge or common technical means in the art that are not disclosed in this application. The specification and the embodiments are considered as merely exemplary, and the scope and spirit of this application are pointed out in the following claims.</p><p id="p-0240" num="0239">It is to be understood that this application is not limited to the precise structures described above and shown in the accompanying drawings, and various modifications and changes can be made without departing from the scope of this application. The scope of this application is subject only to the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for picture encryption, executed by a computer device, the method comprising:<claim-text>acquiring N pictures having a time sequence, N being an integer equal to or greater than 2;</claim-text><claim-text>performing feature extraction on the N pictures to acquire a picture feature of each of the N pictures;</claim-text><claim-text>successively performing target prediction on the N pictures according to the time sequence to obtain prediction information of the each of the N pictures, the target prediction referring to a prediction on the each of the N pictures based on status information, and the status information being information which is updated based on picture features of pictures that have been predicted; and</claim-text><claim-text>encrypting the N pictures based on the prediction information of the each of the N pictures.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein successively performing the target prediction on the N pictures according to the time sequence to obtain the prediction information of the each of the N pictures comprises:<claim-text>successively inputting the picture feature of the each of the N pictures into a recurrent network in an image prediction model according to the time sequence to obtain the prediction information of the each of the N pictures outputted by the recurrent network.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein before performing the feature extraction on the N pictures, the method further comprises:<claim-text>through a partitioning network in the image prediction model, performing partitioning processing on the N pictures according to a size of a target window to acquire M picture segments for each of the N pictures, wherein M is a positive integer, and the size of the target window is a network parameter of the partitioning network; and</claim-text><claim-text>performing the feature extraction on the N pictures to acquire the picture feature of the each of the N pictures comprises:</claim-text><claim-text>for the each of the N pictures, performing feature extraction on the M picture segments based on attention weights each corresponding to one of the M picture segments to obtain the picture feature of the each of the N pictures.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the prediction information comprises prediction sub-information for indicating prediction result of the each of the M picture segments of the each of the N pictures.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein performing the feature extraction on the M picture segments comprises:<claim-text>preprocessing the M picture segments to obtain preprocessed features of the M picture segments; and</claim-text><claim-text>inputting the preprocessed features of the M picture segments into an image feature extraction network in the image prediction model to acquire the picture feature of the each of the N pictures outputted by the image feature extraction network,</claim-text><claim-text>wherein the image feature extraction network comprises an attention mechanism layer used for processing the preprocessed features of the M picture segments by taking the attention weights of the M picture segments as parameters.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein preprocessing the M picture segments to obtain the preprocessed features of the M first picture segments comprises:<claim-text>acquiring a gray value of the each of the M picture segments;</claim-text><claim-text>acquiring first type feature information of the each of the M picture segments based on a fast Fourier transform of a first order of magnitude; and</claim-text><claim-text>combining the gray value with the first type feature information to obtain the preprocessed feature of the each of the M picture segments.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, where the first type feature information comprises at least one of following information of a spectrogram of the each of the M picture segments:<claim-text>an energy distribution feature information;</claim-text><claim-text>a high and low frequency distribution feature information;</claim-text><claim-text>a flatness feature information; or</claim-text><claim-text>a spectral centroid feature information.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein before acquiring the N pictures, the method further comprises:<claim-text>acquiring training data, wherein the training data comprises K picture samples having a time sequence and annotated information of the K picture samples, K being an integer equal to or greater than 2;</claim-text><claim-text>performing feature extraction on the K picture samples to acquire picture features corresponding to the K picture samples;</claim-text><claim-text>successively inputting the picture features corresponding to the K picture samples into a recurrent network in the image prediction model according to the time sequence to obtain prediction information of the K picture samples outputted by the recurrent network;</claim-text><claim-text>acquiring prediction results of the K picture samples based on the prediction information of the K picture samples;</claim-text><claim-text>acquiring a loss function value based on the prediction results of the K picture samples and the annotated information of the K picture samples; and</claim-text><claim-text>updating the image prediction model based on the loss function value.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein updating the image prediction model based on the loss function value comprises:<claim-text>performing parameter updating on at least one of: the recurrent network, the partitioning network, or the image feature extraction network based on the loss function value.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the recurrent network comprises at least one of:<claim-text>a bi-directional gated recurrent unit (BGRU);</claim-text><claim-text>a gated recurrent unit (GRU); or</claim-text><claim-text>a long short term memory (LSTM) network.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein encrypting the N pictures based on the prediction information of the each of the N pictures comprises:<claim-text>splicing the each of the N pictures with the corresponding prediction information to obtain spliced data; and</claim-text><claim-text>encrypting the spliced data to generate encrypted picture data of the N pictures.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein all or part of pictures in the N pictures comprise face images.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the prediction information is used for indicating whether the each of the N pictures comprising a face image meeting a quality requirement.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the N pictures are pictures acquired successively according to the time sequence in a face image based identity recognition process.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A device for picture encryption, the device comprising a memory for storing computer instructions and a processor in communication with the memory, wherein, when the processor executes the computer instructions, the processor is configured to cause the device to:<claim-text>acquire N pictures having a time sequence, N being an integer equal to or greater than 2;</claim-text><claim-text>perform feature extraction on the N pictures to acquire a picture feature of each of the N pictures;</claim-text><claim-text>successively perform target prediction on the N pictures according to the time sequence to obtain prediction information of the each of the N pictures, the target prediction referring to a prediction on the each of the N pictures based on status information, and the status information being information which is updated based on picture features of pictures that have been predicted; and</claim-text><claim-text>encrypt the N pictures based on the prediction information of the each of the N pictures.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein, when the processor is configured to cause the device to successively perform the target prediction on the N pictures according to the time sequence to obtain the prediction information of the each of the N pictures, the processor is configured to cause the device to:<claim-text>successively input the picture feature of the each of the N pictures into a recurrent network in an image prediction model according to the time sequence to obtain the prediction information of the each of the N pictures outputted by the recurrent network.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The device according to <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein, before the processor is configured to cause the device to perform the feature extraction on the N pictures, the processor is configured to further cause the device to:<claim-text>through a partitioning network in the image prediction model, perform partitioning processing on the N pictures according to a size of a target window to acquire M picture segments for each of the N pictures, wherein M is a positive integer, and the size of the target window is a network parameter of the partitioning network; and</claim-text><claim-text>perform the feature extraction on the N pictures to acquire the picture feature of the each of the N pictures comprises:</claim-text><claim-text>for the each of the N pictures, perform feature extraction on the M picture segments based on attention weights each corresponding to one of the M picture segments to obtain the picture feature of the each of the N pictures.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The device according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the prediction information comprises prediction sub-information for indicating prediction result of the each of the M picture segments of the each of the N pictures.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory storage medium for storing computer readable instructions, the computer readable instructions, when executed by a processor, causing the processor to:<claim-text>acquire N pictures having a time sequence, N being an integer equal to or greater than 2;</claim-text><claim-text>perform feature extraction on the N pictures to acquire a picture feature of each of the N pictures;</claim-text><claim-text>successively perform target prediction on the N pictures according to the time sequence to obtain prediction information of the each of the N pictures, the target prediction referring to a prediction on the each of the N pictures based on status information, and the status information being information which is updated based on picture features of pictures that have been predicted; and</claim-text><claim-text>encrypt the N pictures based on the prediction information of the each of the N pictures.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory storage medium according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein, wherein, when the computer readable instructions cause the processor to successively perform the target prediction on the N pictures according to the time sequence to obtain the prediction information of the each of the N pictures, the computer readable instructions cause the processor to:<claim-text>successively input the picture feature of the each of the N pictures into a recurrent network in an image prediction model according to the time sequence to obtain the prediction information of the each of the N pictures outputted by the recurrent network.</claim-text></claim-text></claim></claims></us-patent-application>