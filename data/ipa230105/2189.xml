<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230002190A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230002190</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941510</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>66</class><subclass>B</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>66</class><subclass>B</subclass><main-group>1</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>66</class><subclass>B</subclass><main-group>3</main-group><subgroup>002</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>66</class><subclass>B</subclass><main-group>1</main-group><subgroup>468</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>66</class><subclass>B</subclass><main-group>2201</main-group><subgroup>4638</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>66</class><subclass>B</subclass><main-group>2201</main-group><subgroup>4676</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">INDICATION SYSTEM AND A METHOD FOR GENERATING AN INDICATION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/FI2020/050246</doc-number><date>20200415</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17941510</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>KONE Corporation</orgname><address><city>Helsinki</city><country>FI</country></address></addressbook><residence><country>FI</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>LAURILA</last-name><first-name>Jussi</first-name><address><city>Helsinki</city><country>FI</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>RAUTA</last-name><first-name>Visa</first-name><address><city>Helsinki</city><country>FI</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>KONE Corporation</orgname><role>03</role><address><city>Helsinki</city><country>FI</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An indication system for generating an indication includes at least one indication device, at least one detection device configured to monitor at least one area inside a building to provide monitoring data, and a control unit. The control unit is configured to: detect based on the monitoring data obtained from the at least one detection device at least one predefined gesture of an identified user for which an elevator car (A-D) has been allocated, and control the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user in response to the detection of the at least one predefined gesture of the identified user. A method for generating an indication is also disclosed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="70.44mm" wi="88.65mm" file="US20230002190A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="253.32mm" wi="166.20mm" file="US20230002190A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="187.88mm" wi="151.72mm" file="US20230002190A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The invention concerns in general the technical field of visual indication. Especially the invention concerns systems for generating visual indication.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Typically, when an elevator call is allocated for a user, the elevator call information, e.g. allocated elevator car and/or destination floor, is indicated for the user by means of, e.g. a display. However, the if the elevator call is allocated already when the user is on the way to the elevator, e.g. when the user accesses via an access control gate device, such as a security gate or turnstile, the user may forget the elevator call information before arriving to the elevator.</p><p id="p-0004" num="0003">Thus, there is need to develop further solutions in order to improve the indication of elevator call information.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">The following presents a simplified summary in order to provide basic understanding of some aspects of various invention embodiments. The summary is not an extensive overview of the invention. It is neither intended to identify key or critical elements of the invention nor to delineate the scope of the invention. The following summary merely presents some concepts of the invention in a simplified form as a prelude to a more detailed description of exemplifying embodiments of the invention.</p><p id="p-0006" num="0005">An objective of the invention is to present an indication system and a method for generating an indication. Another objective of the invention is that the indication system and the method for generating an indication enables an on-demand indication of information for a user.</p><p id="p-0007" num="0006">The objectives of the invention are reached by an indication system and a method as defined by the respective independent claims.</p><p id="p-0008" num="0007">According to a first aspect, an indication system for generating an indication is provided, wherein the indication system comprises: at least one indication device, at least one detection device configured to monitor at least one area inside a building to provide monitoring data, and a control unit configured to: detect based on the monitoring data obtained from the at least one detection device at least one predefined gesture of an identified user for which an elevator car has been allocated, and control the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user in response to the detection of the at least one predefined gesture of the identified user.</p><p id="p-0009" num="0008">The operation of the at least one detection device may be based on object recognition or pattern recognition.</p><p id="p-0010" num="0009">The monitored at least one area may comprise a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas on at least one floor of the building.</p><p id="p-0011" num="0010">The visual indication may comprise elevator car allocation information and/or destination guidance information.</p><p id="p-0012" num="0011">Alternatively or in addition, the visual indication may be indicated during a predefined period of time or until a detection of a predefined second gesture of the identified user.</p><p id="p-0013" num="0012">The monitoring may comprise tracking movements and gestures of the identified user within the at least one monitoring area.</p><p id="p-0014" num="0013">Moreover, the control unit may further be configured to: detect based on the tracked movements of the identified user that the identified user exits the building, and generate an instruction to an elevator control system to cancel all existing elevator car allocations for said identified user.</p><p id="p-0015" num="0014">Furthermore, the control unit may further be configured to control the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user, wherein the visual indication may comprise an elevator car allocation cancel information.</p><p id="p-0016" num="0015">According to a second aspect, a method for generating an indication is provided, wherein the method comprising: monitoring, by at least one detection device, at least one area inside a building to provide monitoring data; detecting, by a control unit, based on the monitoring data obtained from the at least one detection device at least one predefined gesture of an identified user for which an elevator car has been allocated; and controlling, by the control unit, at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user in response to a detection of the at least one predefined gesture of the identified user.</p><p id="p-0017" num="0016">The operation of the at least one detection device may be based on object recognition or pattern recognition.</p><p id="p-0018" num="0017">The monitored at least one area may comprise a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas on at least one floor of the building.</p><p id="p-0019" num="0018">The visual indication may comprise elevator car allocation information and/or destination guidance information.</p><p id="p-0020" num="0019">Alternatively or in addition, the visual indication may be indicated during a predefined period of time or until a detection of a predefined second gesture of the identified user.</p><p id="p-0021" num="0020">The monitoring may comprise tracking movements and gestures of the identified user within the at least one monitoring area.</p><p id="p-0022" num="0021">Moreover, the method may further comprise: detecting, by the control unit, based on the tracked movements of the identified user that the identified user exits the building; and generating, by the control unit, an instruction to an elevator control system to cancel all existing elevator car allocations for said identified user.</p><p id="p-0023" num="0022">Furthermore, the method may further comprise controlling, by the control unit, the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user, wherein the visual indication may comprise an elevator car allocation cancel information.</p><p id="p-0024" num="0023">Various exemplifying and non-limiting embodiments of the invention both as to constructions and to methods of operation, together with additional objects and advantages thereof, will be best understood from the following description of specific exemplifying and non-limiting embodiments when read in connection with the accompanying drawings.</p><p id="p-0025" num="0024">The verbs &#x201c;to comprise&#x201d; and &#x201c;to include&#x201d; are used in this document as open limitations that neither exclude nor require the existence of unrecited features. The features recited in dependent claims are mutually freely combinable unless otherwise explicitly stated. Furthermore, it is to be understood that the use of &#x201c;a&#x201d; or &#x201c;an&#x201d;, i.e. a singular form, throughout this document does not exclude a plurality.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF FIGURES</heading><p id="p-0026" num="0025">The embodiments of the invention are illustrated by way of example, and not by way of limitation, in the figures of the accompanying drawings.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates schematically an example environment according to the invention, wherein different embodiments according to the invention may be implemented.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIGS. <b>2</b>A and <b>2</b>B</figref> illustrate schematically example situations according to the invention.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates schematically an example of a method according to the invention.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates schematically an example of components of a control unit according to the invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DESCRIPTION OF THE EXEMPLIFYING EMBODIMENTS</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates schematically an example environment according to the invention, wherein an indication system <b>100</b> according to the invention may be implemented. The example environment is an elevator environment, i.e. an elevator system <b>120</b>. The elevator system <b>120</b> may comprise at least two elevator cars A-D each travelling along a respective elevator shaft, an elevator control system (for sake of clarity not shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), and the indication system <b>100</b> according to the invention. The elevator control system may be configured to control the operations of the elevator system <b>120</b>, e.g. generate elevator call(s) to allocate the elevator cars A-D. The elevator control system may locate in a machine room of the elevator system <b>120</b> or in one of landings.</p><p id="p-0032" num="0031">The indication system <b>100</b> comprises at least one detection device <b>102</b> for providing monitoring data, at least one indication device <b>104</b>, and a control unit <b>106</b>. The control unit <b>106</b> may be external entity or it may be implemented as a part of one or more other entities of the indication system <b>100</b>. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the control unit <b>106</b> is an external entity. The external entity herein means an entity that locates separate from other entities of the indication system <b>100</b>. The implementation of the control unit <b>106</b> may be done as a standalone entity or as a distributed computing environment between a plurality of stand-alone devices, such as a plurality of servers providing distributed computing resource. The control unit <b>106</b> may be configured to control the operations of the indication system <b>100</b>. The control unit <b>106</b> may be communicatively coupled to at least one detection device <b>102</b>, the at least one indication device <b>104</b>, and any other entities of the indication system <b>100</b>. The communication between the control unit <b>106</b> and the other entities of the indication system <b>100</b> may be based on one or more known communication technologies, either wired or wireless.</p><p id="p-0033" num="0032">The at least one detection device <b>102</b> is configured to monitor at least one area inside a building in order to provide the monitoring data. The monitored at least one area may comprise e.g. a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas, e.g. corridors or rooms, on at least one floor of the building. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> the monitored area is a lobby area of the building, wherein the elevators A-D are located. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> the indication system <b>100</b> comprises two detection devices <b>102</b> arranged within the elevator lobby area so that the two detection devices <b>102</b> are capable to monitor the elevator lobby area. However, the invention is not limited to that and the indication system <b>100</b> may comprise any other number of detection devices <b>102</b>. For example, if the at least one monitoring area comprises alternatively or in addition the landing area on at least one floor of the building, the indication system <b>100</b> may comprise at least one detection device <b>102</b> arranged within the landing area on the at least one floor of the building to be able to monitor the landing area on the at least one floor of the building.</p><p id="p-0034" num="0033">The at least one detection device <b>102</b> may comprise at least one optical imaging device, e.g. at least one camera. The at least one detection device <b>102</b> may enable detection, tracking, and/or identification of a user <b>108</b> at a distance away from the at least one detection device <b>102</b>. The distance may be e.g. between 0 to 10 meters from the at least one detection device <b>102</b> and preferably between 1 to 2 meters, 1 to 3 meters or 1 to 5 meters. The at least one detection device <b>102</b> may be arranged to a wall, a ceiling and/or to a separate support device arranged within the at least one monitored area. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the two detection devices <b>102</b> are arranged to opposite walls of the elevator lobby area. According to an example embodiment of the invention the operation of the at least one detection device <b>102</b> may be based on object recognition or pattern recognition.</p><p id="p-0035" num="0034">The at least one detection device <b>102</b> is configured to provide the monitoring data to the control unit <b>106</b>. The control unit <b>106</b> is configured to detect at least one predefined gesture of an identified user <b>108</b> for which an elevator car A-D has been allocated. The allocation of an elevator car A-D for the user <b>108</b> and the identification of the user <b>108</b> may be provided by any known methods. Preferably, the allocation of the elevator car for the user <b>108</b> and the identification of the user <b>108</b> is provided already, when the user <b>108</b> is on the way to the elevator A-D, e.g. when the user <b>108</b> accesses the building or when the user passed through an access control gate device, such as a security gate. The access control gate devices allow access of identified authorized users through the access control gate device. The access control may be based on using keycards; tags; identification codes; e.g. PIN code, ID number, barcodes, QR codes, etc.; and/or biometric technologies, e.g. fingerprint, facial recognition, iris recognition, retinal scan, voice recognition, etc. The access control gate device may be communicatively coupled to the elevator control system enabling the elevator car allocation for the identified user <b>108</b> in response to the identification of an authorized user <b>108</b>. The control unit <b>106</b> of the indication system <b>100</b> may obtain the elevator car allocation information and destination guidance information from the access control gate device, the elevator control system, and/or a database to which the elevator car allocation information and destination guidance information are stored.</p><p id="p-0036" num="0035">The detection of the at least one predefined gesture of the identified user is based on the monitoring data obtained from the at least one detection device <b>102</b>. The control unit <b>106</b> may utilize machine vision in the detection of the at least one predefined gesture. The predefined gestures of the identified user <b>108</b> may e.g. comprise, but is not limited to, lower a look in front of feet, a wave of hand, a toss of head, or any other gesture of the user <b>108</b>.</p><p id="p-0037" num="0036">The control unit <b>106</b> is configured to control the at least one indication device <b>104</b> to generate a visual indication <b>110</b> on a floor of the building in a vicinity of, i.e. close to, the identified user in response to the detection of the at least one predefined gesture of the identified user <b>108</b>. For example, the visual indication <b>110</b> may be generated on the floor in front of the feet of the user <b>108</b> as shown in the example <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The generated visual indication <b>110</b> may comprise elevator car allocation information and/or destination guidance information. The elevator car allocation information may comprise e.g. the allocated elevator car, destination floor, and/or destination place. The destination guidance information may comprise e.g. text- and/or figure-based guidance information. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> the generated visual indication <b>110</b> comprises the allocated elevator car, i.e. the elevator car A, the destination floor, i.e. the floor <b>8</b>, and the destination place, i.e. caf&#xe9;. This enables an on-demand indication of the elevator car allocation information and/or destination guidance information for the user <b>108</b>. Moreover, this enables for the user <b>108</b> an interference-free and hands-free way to check the elevator car allocation information and/or destination guidance information.</p><p id="p-0038" num="0037">The at least one indication device <b>104</b> may comprise one or more projector devices configured to project the generated visual indication <b>110</b> on the floor in a vicinity of the identified user <b>108</b>. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref> the indication system <b>100</b> comprises two indication devices <b>104</b> arranged within the elevator lobby area so that the two indication devices <b>104</b> are capable to generate the visual indication <b>110</b> on the floor within the elevator lobby area. However, the invention is not limited to that and the indication system <b>100</b> may comprise any other number of indication devices <b>104</b>. For example, if the at least one monitoring area comprises alternatively or in addition the landing area on at least one floor of the building, the indication system <b>100</b> may comprise at least one indication device arranged also within the landing area on the at least one floor of the building to be able to generate the visual indication on the floor within the landing area on the at least one floor of the building. The at least one indication device <b>104</b> may be arranged to a wall, a ceiling and/or to a separate support device arranged within the at least one monitored area. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the two detection devices <b>102</b> are arranged to opposite walls of the elevator lobby area.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> illustrates a non-limiting example situation, wherein the control unit <b>106</b> is configured to control the at least one indication device <b>104</b> to generate the visual indication <b>110</b> on the floor in front of the feet of the identified user <b>108</b> in response to the detection of the at least one predefined gesture of the identified user <b>108</b> (for sake of clarity the control unit <b>106</b>, the at least one indication device <b>104</b>, and the at least one detection device <b>102</b> are not shown in <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>). In the example of <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> the identified user <b>108</b> is entering the elevator car B which has been allocated for said identified user <b>108</b>. The generated visual indication <b>110</b> comprises the allocated elevator car, i.e. the elevator car B, the destination floor, i.e. the floor <b>10</b>, and the destination place, i.e. a meeting room in the floor <b>10</b>. <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> illustrates another non-limiting example situation, wherein the same identified user <b>108</b> arrives at the destination floor <b>10</b>, exits the elevator car B, and performs the predefined gesture (for sake of clarity the control unit <b>106</b>, the at least one indication device <b>104</b>, and the at least one detection device <b>102</b> are not shown in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>). The control unit <b>106</b> is configured to control the at least one indication device <b>104</b> arranged to the destination floor <b>10</b> to generate the visual indication <b>110</b> on the floor in front of the feet of the identified user <b>108</b> in response to the detection of the at least one predefined gesture of the identified user <b>108</b>. The generated visual indication <b>110</b> comprises the destination place, i.e. the meeting room, and guidance information to the destination place. The guidance information comprises figure-based guidance information, e.g. the arrow in this example, to the destination place.</p><p id="p-0040" num="0039">The visual indication <b>110</b> may be indicated during a predefined period of time. The predefined period of time may be such that the identified user <b>108</b> has time to see the visual indication, for example, but not limited to, the predefined period of time may be between 5 to 10 seconds. Alternatively, the visual indication <b>110</b> may be indicated until a detection of a predefined second gesture of the identified user <b>108</b>. The predefined second gesture may be dependent on the previously detected predefined gesture and/or a counter gesture to the previously detected predefined gesture. According to an example, of the previously detected predefined gesture is lowering the look on the floor in front of his feet, the predefined second gesture of the identified user <b>108</b> may be raising the look from the floor. Alternatively, if the previously detected predefined gesture is a wave of hand or a toss of head, the predefined second gesture of the identified user <b>108</b> may be a wave of hand or a toss of head into another direction.</p><p id="p-0041" num="0040">According to an example embodiment of the invention, the monitoring may comprise tracking movements and gestures of the identified user <b>108</b> within the at least one monitoring area. This enables tracking the movement of the identified user <b>108</b> within the monitoring area and every time the identified user <b>108</b> performs the predefined gesture, the control unit <b>106</b> may be configured to control the at least one indication device <b>104</b> to generate the visual indication on the floor in a vicinity of the identified user <b>108</b>, e.g. in front of the identified user <b>108</b>, irrespective of the location of the identified user <b>108</b> as long as the identified user <b>108</b> resides within the monitored area. This enables that the indication of the elevator car allocation information and/or destination guidance information may follow the user <b>108</b> to the destination of the user <b>108</b>.</p><p id="p-0042" num="0041">According to an example embodiment of the invention, the control unit <b>106</b> may further be configured to detect if the identified user <b>108</b> exits the building based on the tracked movements of the identified user <b>108</b>. In response to the detection of the exit of the identified user <b>108</b>, the control unit <b>106</b> may be configured to generate an instruction to the elevator control system to cancel all existing elevator car allocations for said identified user <b>108</b>. This reduces amount of unnecessary elevator car allocations and thus improves the operation of the elevator system <b>120</b>.</p><p id="p-0043" num="0042">According to an example embodiment of the invention, in response to the cancelling the existing elevator car allocations for said identified user <b>108</b>, the control unit <b>106</b> may further be configured to control the at least one indication device <b>104</b> to generate the visual indication <b>110</b> on the floor of the building in a vicinity of the identified user <b>108</b>, e.g. in front if the identified user <b>108</b>, wherein the generated visual indication comprises an elevator car allocation cancel information.</p><p id="p-0044" num="0043">Next an example of the method according to the invention is described by referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. <figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically illustrates the invention as a flow chart.</p><p id="p-0045" num="0044">At a step <b>302</b>, the at least one detection device <b>102</b> monitors at least one area inside the building in order to provide the monitoring data. The monitored at least one area may comprise e.g. a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas, e.g. corridors or rooms, on at least one floor of the building. According to an example embodiment of the invention the operation of the at least one detection device <b>102</b> may be based on object recognition or pattern recognition. The at least one detection device <b>102</b> provides the monitoring data to the control unit <b>106</b>.</p><p id="p-0046" num="0045">At a step <b>304</b>, the control unit <b>106</b> detects at least one predefined gesture of an identified user <b>108</b> for which an elevator car A-D has been allocated. The allocation of an elevator car A-D for the user <b>108</b> and the identification of the user <b>108</b> may be provided by any known methods as discussed above. The control unit <b>106</b> of the indication system <b>100</b> may obtain the elevator car allocation information and destination guidance information from the access control gate device, the elevator control system, and/or a database to which the elevator car allocation information and destination guidance information are stored. The detection of the at least one predefined gesture of the identified user is based on the monitoring data obtained from the at least one detection device <b>102</b>. The control unit <b>106</b> may utilize machine vision in the detection of the at least one predefined gesture. The predefined gestures of the identified user <b>108</b> may e.g. comprise, but is not limited to, lower a look in front of feet, a wave of hand, a toss of head, or any other gesture of the user <b>108</b>.</p><p id="p-0047" num="0046">At a step <b>306</b>, the control unit <b>104</b> controls the at least one indication device <b>104</b> to generate a visual indication <b>110</b> on a floor of the building in a vicinity of, i.e. close to, the identified user <b>108</b> in response to the detection of the at least one predefined gesture of the identified user <b>108</b>. For example, the visual indication <b>110</b> may be generated on the floor in front of the feet of the user <b>108</b> as shown in the example <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The generated visual indication <b>110</b> may comprise elevator car allocation information and/or destination guidance information. The elevator car allocation information may comprise e.g. the allocated elevator car, destination floor, and/or destination place. The destination guidance information may comprise e.g. text- and/or figure-based guidance information. This enables a simple way to indicate the elevator car allocation information and/or destination guidance information for the user <b>108</b>. Moreover, this enables for the user <b>108</b> an interference-free and hands-free way to check the elevator car allocation information and/or destination guidance information.</p><p id="p-0048" num="0047">The visual indication <b>110</b> may be indicated during a predefined period of time. The predefined period of time may be such that the identified user <b>108</b> has time to see the visual indication, for example, but not limited to, the predefined period of time may be between 5 to 10 seconds. Alternatively, the visual indication <b>110</b> may be indicated until a detection of a predefined second gesture of the identified user <b>108</b>. The predefined second gesture may be dependent on the previously detected predefined gesture and/or a counter gesture to the previously detected predefined gesture. According to an example, of the previously detected predefined gesture is lowering the look on the floor in front of his feet, the predefined second gesture of the identified user <b>108</b> may be a raising the look from the floor. Alternatively, if the previously detected predefined gesture is a wave of hand or a toss of head, the predefined second gesture of the identified user <b>108</b> may be a wave of hand or a toss of head into another direction.</p><p id="p-0049" num="0048">According to an example embodiment of the invention, the monitoring may comprise tracking movements and gestures of the identified user <b>108</b> within the at least one monitoring area. This enables tracking the movement of the identified user <b>108</b> within the monitoring area and every time the identified user <b>108</b> performs the predefined gesture, the control unit <b>106</b> may control the at least one indication device <b>104</b> to generate the visual indication on the floor in a vicinity of the identified user <b>108</b>, e.g. in front of the identified user <b>108</b>, irrespective of the location of the identified user <b>108</b> as long as the identified user <b>108</b> resides within the monitored area. This enables that the indication of the elevator car allocation information and/or destination guidance information may follow the user <b>108</b> to the destination of the user <b>108</b>.</p><p id="p-0050" num="0049">According to an example embodiment of the invention, the method may further comprise detecting, by the control unit <b>106</b>, based on the tracked movements of the identified user <b>108</b>, if the identified user <b>108</b> exits the building. In response to the detection of the exit of the identified user <b>108</b>, the control unit <b>106</b> may generate an instruction to the elevator control system to cancel all existing elevator car allocations for said identified user <b>108</b>. This reduces amount of unnecessary elevator car allocations and thus improves the operation of the elevator system <b>120</b>.</p><p id="p-0051" num="0050">According to an example embodiment of the invention, in response to the cancelling the existing elevator car allocations for said identified user <b>108</b>, the method may further comprise controlling, by the control unit <b>106</b>, the at least one indication device <b>104</b> to generate the visual indication <b>110</b> on the floor of the building in a vicinity of the identified user <b>108</b>, e.g. in front if the identified user <b>108</b>, wherein the generated visual indication comprises an elevator car allocation cancel information.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>4</b></figref> schematically illustrates an example of components of the control unit <b>106</b> according to the invention. The control unit <b>106</b> may comprise a processing unit <b>410</b> comprising one or more processors, a memory unit <b>420</b> comprising one or more memories, a communication unit <b>430</b> comprising one or more communication devices, and possibly a user interface (UI) unit <b>450</b>. The memory unit <b>420</b> may store portions of computer program code <b>425</b> and any other data, and the processing unit <b>410</b> may cause the control unit <b>106</b> to operate as described by executing at least some portions of the computer program code <b>425</b> stored in the memory unit <b>420</b>. The communication unit <b>430</b> may be based on at least one known communication technologies, either wired or wireless, in order to exchange pieces of information as described earlier. The communication unit <b>430</b> provides an interface for communication with any external unit, such as the at least one indication device <b>104</b>, the at least one detection device <b>102</b>, the elevator control system, database and/or any external entities or systems. The communication unit <b>430</b> may comprise one or more communication devices, e.g. radio transceiver, antenna, etc. The user interface <b>440</b> may comprise I/O devices, such as buttons, keyboard, touch screen, microphone, loudspeaker, display and so on, for receiving input and outputting information. The computer program <b>425</b> may be stored in a non-statutory tangible computer readable medium, e.g. an USB stick or a CD-ROM disc.</p><p id="p-0053" num="0052">The specific examples provided in the description given above should not be construed as limiting the applicability and/or the interpretation of the appended claims. Lists and groups of examples provided in the description given above are not exhaustive unless otherwise explicitly stated.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An indication system for generating an indication, wherein the indication system comprises:<claim-text>at least one indication device,</claim-text><claim-text>at least one detection device configured to monitor at least one area inside a building to provide monitoring data, and</claim-text><claim-text>a control unit configured to:<claim-text>detect based on the monitoring data obtained from the at least one detection device at least one predefined gesture of an identified user for which an elevator car (A-D) has been allocated, and</claim-text><claim-text>control the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user in response to the detection of the at least one predefined gesture of the identified user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the operation of the at least one detection device is based on object recognition or pattern recognition.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the monitored at least one area comprises a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas on at least one floor of the building.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the visual indication comprises elevator car allocation information and/or destination guidance information.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the visual indication is indicated during a predefined period of time or until a detection of a predefined second gesture of the identified user.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the monitoring comprises tracking movements and gestures of the identified user within the at least one monitoring area.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The system according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the control unit is further configured to:<claim-text>detect based on the tracked movements of the identified user that the identified user exits the building, and</claim-text><claim-text>generate an instruction to an elevator control system to cancel all existing elevator car allocations for said identified user.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the control unit is further configured to control the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user, wherein the visual indication comprises an elevator car allocation cancel information.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A method for generating an indication, wherein the method comprising comprises:<claim-text>monitoring, by at least one detection device, at least one area inside a building to provide monitoring data,</claim-text><claim-text>detecting, by a control unit, based on the monitoring data obtained from the at least one detection device at least one predefined gesture of an identified user for which an elevator car (A-D) has been allocated, and</claim-text><claim-text>controlling, by the control unit, at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user in response to a detection of the at least one predefined gesture of the identified user.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the operation of the at least one detection device is based on object recognition or pattern recognition.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the monitored at least one area comprises a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas on at least one floor of the building.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the visual indication comprises elevator car allocation information and/or destination guidance information.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the visual indication is indicated during a predefined period of time or until a detection of a predefined second gesture of the identified user.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method according to <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the monitoring comprises tracking movements and gestures of the identified user within the at least one monitoring area.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method according to <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the method further comprises:<claim-text>detecting, by the control unit, based on the tracked movements of the identified user that the identified user exits the building, and</claim-text><claim-text>generating, by the control unit, an instruction to an elevator control system to cancel all existing elevator car allocations for said identified user.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein method further comprises controlling, by the control unit, the at least one indication device to generate a visual indication on a floor of the building in a vicinity of the identified user, wherein the visual indication comprises an elevator car allocation cancel information.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the monitored at least one area comprises a lobby area of the building, a landing area on at least one floor of the building, and/or one or more other areas on at least one floor of the building.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the visual indication comprises elevator car allocation information and/or destination guidance information.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the visual indication comprises elevator car allocation information and/or destination guidance information.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The system according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the visual indication is indicated during a predefined period of time or until a detection of a predefined second gesture of the identified user.</claim-text></claim></claims></us-patent-application>