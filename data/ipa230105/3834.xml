<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003835A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003835</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17773503</doc-number><date>20201030</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>41</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>H</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>295</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>415</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>H</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>0209</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>295</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>5</main-group><subgroup>05</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">REMOTE RECOVERY OF ACOUSTIC SIGNALS FROM PASSIVE SOURCES</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62929140</doc-number><date>20191101</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ARIZONA BOARD OF REGENTS ON BEHALF OF ARIZONA STATE UNIVERSITY</orgname><address><city>SCOTTSDALE</city><state>AZ</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>RONG</last-name><first-name>YU</first-name><address><city>TEMPE</city><state>AZ</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>BLISS</last-name><first-name>DANIEL W.</first-name><address><city>PHOENIX</city><state>AZ</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SRINIVAS</last-name><first-name>SHARANYA</first-name><address><city>CAMBRIDGE</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>VENKATARAMANI</last-name><first-name>ADARSH</first-name><address><city>TEMPE</city><state>AZ</state><country>US</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/US20/58326</doc-number><date>20201030</date></document-id><us-371c12-date><date>20220429</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Remote recovery of acoustic signals from passive sources is provided. Wideband radars, such as ultra-wideband (UWB) radars can detect minute surface displacements for vibrometry applications. Embodiments described herein remotely sense sound and recover acoustic signals from vibrating sources using radars. Early research in this domain only demonstrated single sound source recovery using narrowband millimeter wave radars in direct line-of-sight scenarios. Instead, by using wideband radars (e.g., X band UWB radars), multiple sources separated in ranges are observed and their signals isolated and recovered. Additionally, the see-through ability of microwave signals is leveraged to extend this technology to surveillance of targets obstructed by barriers. Blind surveillance is achieved by reconstructing audio from a passive object which is merely in proximity of the sound source using clever radar and audio processing techniques.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="45.13mm" wi="145.03mm" file="US20230003835A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="235.03mm" wi="167.13mm" file="US20230003835A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="160.53mm" wi="103.80mm" file="US20230003835A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="199.56mm" wi="167.64mm" file="US20230003835A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="193.80mm" wi="87.21mm" orientation="landscape" file="US20230003835A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="184.23mm" wi="125.65mm" orientation="landscape" file="US20230003835A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="204.89mm" wi="166.03mm" file="US20230003835A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="184.74mm" wi="126.15mm" orientation="landscape" file="US20230003835A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="238.42mm" wi="161.63mm" file="US20230003835A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="242.40mm" wi="165.02mm" file="US20230003835A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="171.96mm" wi="129.71mm" orientation="landscape" file="US20230003835A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="207.01mm" wi="166.71mm" file="US20230003835A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="160.36mm" wi="118.87mm" file="US20230003835A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="190.84mm" wi="118.87mm" file="US20230003835A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims the benefit of provisional patent application Ser. No. 62/929,140, filed Nov. 1, 2019, the disclosure of which is hereby incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD OF THE DISCLOSURE</heading><p id="p-0003" num="0002">The present disclosure relates to acoustic signal detection through radio frequency (RF) signals.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Radars have always been an integral part of industrial automation, surveillance security and health monitoring systems. Previously, their ability to detect minute surface displacements has been exploited for vibrometry applications such as speech recovery. Similarly, surface skin motions have been extracted for vital signs detection and even through-wall cardiac sensing.</p><p id="p-0005" num="0004">Recovery of acoustic signals from surface vibrations using distinct sensors has been a topic of interest for a few decades now. When sound propagates through a medium, it creates pressure waves that induce surface displacements. Feasibility of using radars to remotely sense sound has been illustrated for speech retrieval. Radar sensing has also been illustrated for generic acoustic signals using different millimeter wave radars and several improvements have been proposed to radar receivers. A variety of speech recovery methods have also been proposed to handle noisy backgrounds.</p><p id="p-0006" num="0005">These previous approaches employed narrowband millimeter wave Doppler radars for sound sensing and have been limited to single acoustic source recovery in direct line-of-sight scenarios.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">Remote recovery of acoustic signals from passive sources is provided. Wideband radars, such as ultra-wideband (UWB) radars can detect minute surface displacements for vibrometry applications. Embodiments described herein remotely sense sound and recover acoustic signals from vibrating sources using radars. Early research in this domain only demonstrated single sound source recovery using narrowband millimeter wave radars in direct line-of-sight scenarios. Instead, by using wideband radars (e.g., X band UWB radars), multiple sources separated in ranges are observed and their signals isolated and recovered. Additionally, the see-through ability of microwave signals is leveraged to extend this technology to surveillance of targets obstructed by barriers. Blind surveillance is achieved by reconstructing audio from a passive object which is merely in proximity of the sound source using clever radar and audio processing techniques.</p><p id="p-0008" num="0007">Embodiments disclosed herein use a radio frequency (RF) radar sensor, such as a wideband radar sensor (e.g., X band UWB pulse Doppler radar sensor), to measure surface displacement of a vibrating surface. A radar response signal is received, from which a motion signal is extracted. An acoustic signal is reconstructed from the motion signal using signal processing techniques, such as a fast Fourier transform (FFT)-based time-frequency signal processing technique. In some examples, the vibrating surface can be a passively vibrating source (e.g., one excited indirectly, such as by a non-line-of-sight active acoustic source). Some examples can isolate acoustic signals from multiple active sources. Some examples can also exploit a see-through ability of the radar signal to recover acoustic signals.</p><p id="p-0009" num="0008">An exemplary embodiment provides a method for remote recovery of an acoustic signal. The method includes transmitting a radar signal toward a vibrating surface and receiving an RF response signal corresponding to the radar signal. The method further includes extracting a motion signal from the RF response signal, determining spatial information corresponding to the vibrating surface from the motion signal, and reconstructing an acoustic signal from the motion signal using the spatial information.</p><p id="p-0010" num="0009">Another exemplary embodiment provides an RF device. The RF device includes a radar sensor and a processing circuit coupled to the radar sensor. The radar sensor is configured to receive an RF response signal to a radar signal. The processing circuit is configured to recover an acoustic signal by extracting a motion signal for one or more vibrating surfaces from the RF response signal, determining spatial information corresponding to the one or more vibrating surfaces from the motion signal, and using the spatial information to recover the acoustic signal from the motion signal.</p><p id="p-0011" num="0010">Another exemplary embodiment provides a system for remote recovery of an acoustic signal. The system includes a radar sensor, a database, and a processing circuit coupled to the database. The radar sensor is configured to receive an RF response signal to a radar signal. The database is configured to store the RF response signal. The processing circuit is configured to extract a motion signal for a vibrating surface from the RF response signal, determine spatial information corresponding to the vibrating surface from the motion signal, the spatial information comprising at least one of a range to the vibrating surface, an angle to the vibrating surface, and a Doppler frequency shift associated with the vibrating surface, and reconstruct the acoustic signal from the motion signal using the spatial information.</p><p id="p-0012" num="0011">Those skilled in the art will appreciate the scope of the present disclosure and realize additional aspects thereof after reading the following detailed description of the preferred embodiments in association with the accompanying drawing figures.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWING FIGURES</heading><p id="p-0013" num="0012">The accompanying drawing figures incorporated in and forming a part of this specification illustrate several aspects of the disclosure, and together with the description serve to explain the principles of the disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of an exemplary system for remote recovery of an acoustic signal, illustrating observation of surface displacement of a passive object in the proximity of an active source.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram of the system of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, illustrating detection and isolation of acoustic signals by observing surface displacement of multiple sound sources separated in different ranges to the radar.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a schematic diagram of the system of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, illustrating recovery of an acoustic signal from a sound source obstructed by a dielectric medium.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of the system of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref>, which includes a radio frequency (RF) device for acoustic recovery using wideband radar.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a graphical flowchart of an exemplary process for audio signal extraction from a backscattered radar return.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of a radar data matrix which includes micro-displacements corresponding to an acoustic signal.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an image of a first evaluation setup, which recovers an acoustic signal by observing surface displacement of a passive source.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a spectrogram of the original audio signal.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> is a graphical representation of the radar recovered audio waveform from the radar result of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>6</b>D</figref> is a graphical representation of the original audio waveform.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an image of a second evaluation setup, which recovers multiple acoustic signals by observing and isolating surface displacement of two active sources at different ranges.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal from the closer sound source S<sub>2 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a spectrogram of the original audio signal from sound source S<sub>2</sub>.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is a graphical representation of the radar recovered audio waveform {circumflex over (x)}<sub>2</sub>(t) from sound source S<sub>2</sub>.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>8</b>D</figref> is a graphical representation of the original audio waveform x<sub>2</sub>(t) from sound source S<sub>2</sub>.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>8</b>E</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal from the further sound source S<sub>1 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>8</b>F</figref> is a spectrogram of the original audio signal from sound source S<sub>1</sub>.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>8</b>G</figref> is a graphical representation of the radar recovered audio waveform {circumflex over (x)}<sub>1</sub>(t) from sound source S<sub>1</sub>.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>8</b>H</figref> is a graphical representation of the original audio waveform x<sub>1</sub>(t) from sound source S<sub>1</sub>.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an image of the third evaluation setup, which recovers an acoustic signal through a wall or other barrier.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal from <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is a spectrogram of the original audio signal.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>10</b>C</figref> is a graphical representation of the radar recovered audio waveform.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>10</b>D</figref> is a graphical representation of the original audio waveform.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flow diagram illustrating a process for remote recovery of an acoustic signal.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of a generalized representation of an exemplary computer system that could be used to perform any of the methods or functions described above, such as remote recovery of an acoustic signal.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0041" num="0040">The embodiments set forth below represent the necessary information to enable those skilled in the art to practice the embodiments and illustrate the best mode of practicing the embodiments. Upon reading the following description in light of the accompanying drawing figures, those skilled in the art will understand the concepts of the disclosure and will recognize applications of these concepts not particularly addressed herein. It should be understood that these concepts and applications fall within the scope of the disclosure and the accompanying claims.</p><p id="p-0042" num="0041">It will be understood that, although the terms first, second, etc. may be used herein to describe various elements, these elements should not be limited by these terms. These terms are only used to distinguish one element from another. For example, a first element could be termed a second element, and, similarly, a second element could be termed a first element, without departing from the scope of the present disclosure. As used herein, the term &#x201c;and/or&#x201d; includes any and all combinations of one or more of the associated listed items.</p><p id="p-0043" num="0042">It will be understood that when an element such as a layer, region, or substrate is referred to as being &#x201c;on&#x201d; or extending &#x201c;onto&#x201d; another element, it can be directly on or extend directly onto the other element or intervening elements may also be present. In contrast, when an element is referred to as being &#x201c;directly on&#x201d; or extending &#x201c;directly onto&#x201d; another element, there are no intervening elements present. Likewise, it will be understood that when an element such as a layer, region, or substrate is referred to as being &#x201c;over&#x201d; or extending &#x201c;over&#x201d; another element, it can be directly over or extend directly over the other element or intervening elements may also be present. In contrast, when an element is referred to as being &#x201c;directly over&#x201d; or extending &#x201c;directly over&#x201d; another element, there are no intervening elements present. It will also be understood that when an element is referred to as being &#x201c;connected&#x201d; or &#x201c;coupled&#x201d; to another element, it can be directly connected or coupled to the other element or intervening elements may be present. In contrast, when an element is referred to as being &#x201c;directly connected&#x201d; or &#x201c;directly coupled&#x201d; to another element, there are no intervening elements present.</p><p id="p-0044" num="0043">Relative terms such as &#x201c;below&#x201d; or &#x201c;above&#x201d; or &#x201c;upper&#x201d; or &#x201c;lower&#x201d; or &#x201c;horizontal&#x201d; or &#x201c;vertical&#x201d; may be used herein to describe a relationship of one element, layer, or region to another element, layer, or region as illustrated in the Figures. It will be understood that these terms and those discussed above are intended to encompass different orientations of the device in addition to the orientation depicted in the Figures.</p><p id="p-0045" num="0044">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the disclosure. As used herein, the singular forms &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise. It will be further understood that the terms &#x201c;comprises,&#x201d; &#x201c;comprising,&#x201d; &#x201c;includes,&#x201d; and/or &#x201c;including&#x201d; when used herein specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p><p id="p-0046" num="0045">Unless otherwise defined, all terms (including technical and scientific terms) used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this disclosure belongs. It will be further understood that terms used herein should be interpreted as having a meaning that is consistent with their meaning in the context of this specification and the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.</p><p id="p-0047" num="0046">Remote recovery of acoustic signals from passive sources is provided. Wideband radars, such as ultra-wideband (UWB) radars can detect minute surface displacements for vibrometry applications. Embodiments described herein remotely sense sound and recover acoustic signals from vibrating sources using radars. Early research in this domain only demonstrated single sound source recovery using narrowband millimeter wave radars in direct line-of-sight scenarios. Instead, by using wideband radars (e.g., X band UWB radars), multiple sources separated in ranges are observed and their signals isolated and recovered. Additionally, the see-through ability of microwave signals is leveraged to extend this technology to surveillance of targets obstructed by barriers. Blind surveillance is achieved by reconstructing audio from a passive object which is merely in proximity of the sound source using clever radar and audio processing techniques.</p><p id="p-0048" num="0047">Embodiments disclosed herein use a radio frequency (RF) radar sensor, such as a wideband radar sensor (e.g., X band UWB pulse Doppler radar sensor), to measure surface displacement of a vibrating surface. A radar response signal is received, from which a motion signal is extracted. An acoustic signal is reconstructed from the motion signal using signal processing techniques, such as a fast Fourier transform (FFT)-based time-frequency signal processing technique. In some examples, the vibrating surface can be a passively vibrating source (e.g., one excited indirectly, such as by a non-line-of-sight active acoustic source). Some examples can isolate acoustic signals from multiple active sources. Some examples can also exploit a see-through ability of the radar signal to recover acoustic signals.</p><heading id="h-0007" level="1">I. Introduction</heading><p id="p-0049" num="0048">Previous approaches employed narrowband millimeter wave Doppler radars to single acoustic source recovery in direct line-of-sight scenarios. Embodiments of the present disclosure instead use wideband radar sensors (e.g., a UWB radar operating at X frequency band) as an RF microphone but in much more complex environments.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref> illustrate three unique evaluation setups used to demonstrate the success of the technology described herein. <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of an exemplary system <b>10</b> for remote recovery of an acoustic signal, illustrating observation of surface displacement of a passive object in the proximity of an active source. <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram of the system of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, illustrating detection and isolation of acoustic signals by observing surface displacement of multiple sound sources separated in different ranges to the radar. <figref idref="DRAWINGS">FIG. <b>1</b>C</figref> is a schematic diagram of the system of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, illustrating recovery of an acoustic signal from a sound source obstructed by a dielectric medium. This disclosure analyzes the spectrum and waveform of a reference source and recovered signals to illustrate workings of proposed processing techniques.</p><heading id="h-0008" level="1">II. Radar Processing</heading><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of the system <b>10</b> of <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref>, which includes an RF device <b>12</b> for acoustic recovery using wideband radar. In an exemplary aspect, the RF device <b>12</b> includes a radar sensor <b>14</b> which transmits a radar signal <b>16</b> and receives an RF response signal <b>18</b> to the radar signal <b>16</b> after interaction with (e.g., reflection/refraction from) one or more vibrating surfaces <b>20</b>. The vibrating surfaces <b>20</b> represent any surface which may vibrate with an acoustic signal, such as an active acoustic source or a passive acoustic source, such as in the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref>. The RF device <b>12</b> further includes a processing device <b>22</b> for processing and analyzing the RF response signal <b>18</b> to recover one or more acoustic signals. Generally, the RF device <b>12</b> further includes a database or memory <b>24</b> for storing instructions and/or data, which is coupled to the processing device <b>22</b> (and optionally coupled to the radar sensor <b>14</b>). The RF device <b>12</b> may include additional components, such as discussed in Section VI below.</p><p id="p-0052" num="0051">In an exemplary aspect, the radar sensor is a wideband radar, and the radar signal <b>16</b> is a wideband radar signal, such as an X band UWB impulse signal. It should be understood that other signals may be used (e.g., a non-impulse radar signal) in an appropriate electromagnetic frequency (e.g., terrestrial radio, microwave, mmWave, optical, etc.). In some examples, the radar sensor <b>14</b> receives the RF response signal <b>18</b> without sending the radar signal <b>16</b> (e.g., the radar signal <b>16</b> is sent from another component of the RF device <b>12</b> or from another transmitter device).</p><p id="p-0053" num="0052">In the illustrated embodiment, the system <b>10</b> for remote recovery of an acoustic signal primarily includes the RF device <b>12</b>. In other embodiments, the system <b>10</b> includes the components of the RF device <b>12</b> as two or more separate devices. For example, the processing device <b>22</b> and/or the database/memory <b>24</b> may be incorporated in a remote server, with minimal or no signal processing being performed at the radar sensor <b>14</b>.</p><p id="p-0054" num="0053">This section elaborates on proposed processing steps that make detection and recovery of audio signals using radars (e.g., the radar sensor <b>14</b>) possible. First, models for radar return for an environment are established. Next, how radar processing alters the model analytically is described. Given that Doppler shift is extracted in slow time from the pre-processing stage, exemplary audio recovery methods are elaborated.</p><p id="p-0055" num="0054">A. Signal Model</p><p id="p-0056" num="0055">An impulse radar emits same narrow pulse at every pulse repetition interval. When reflected off a vibrating target (e.g., the vibrating surface <b>20</b>), the received pulse is modulated in phase and magnitude. Inspecting and extracting meaningful information from such backscattered signal is termed radar &#x2018;sensing&#x2019;. The RF response of surface vibrations of the object is modeled as a superposition of responses from discrete, dynamic scattering centers. The i-th scattering center is parameterized by reflectivity coefficient &#x3c1;<sub>i</sub>(t) and radial distance d<sub>i</sub>(t) from the radar sensor, which vary as a function of time t. The received composite signal is modeled as:</p><p id="p-0057" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>y</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>&#x3c4;</mi>       <mo>,</mo>       <mi>t</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <msubsup>       <mo>&#x2211;</mo>       <mi>i</mi>       <mi>N</mi>      </msubsup>      <mrow>       <mrow>        <msub>         <mi>&#x3c1;</mi>         <mi>i</mi>        </msub>        <mo>(</mo>        <mi>t</mi>        <mo>)</mo>       </mrow>       <mo>&#x2062;</mo>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mi>&#x3c4;</mi>         <mo>-</mo>         <mrow>          <msub>           <mi>&#x3c4;</mi>           <msub>            <mi>d</mi>            <mi>i</mi>           </msub>          </msub>          <mo>(</mo>          <mi>t</mi>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Equation</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mn>1</mn>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>=</mo>     <mrow>      <msubsup>       <mo>&#x2211;</mo>       <mi>i</mi>       <mi>N</mi>      </msubsup>      <mrow>       <mrow>        <msub>         <mi>&#x3c1;</mi>         <mi>i</mi>        </msub>        <mo>(</mo>        <mi>t</mi>        <mo>)</mo>       </mrow>       <mo>&#x2062;</mo>       <mrow>        <mi>p</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mi>&#x3c4;</mi>         <mo>-</mo>         <mrow>          <mn>2</mn>          <mo>&#x2062;</mo>          <mfrac>           <mrow>            <msub>             <mi>d</mi>             <mi>i</mi>            </msub>            <mo>(</mo>            <mi>t</mi>            <mo>)</mo>           </mrow>           <mi>c</mi>          </mfrac>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Equation</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mn>2</mn>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0058" num="0000">where N is the number of scattering centers and p(&#x3c4;) is the transmitted pulse, c denotes the speed of light. Note that t and T are two different time scales. The former is often referred as slow-time sampling interval and is related to the pulse repetition interval. The latter time scale is referred as fast-time sampling interval and is often associated with ADC sampling interval providing distance information.</p><p id="p-0059" num="0056">The direct RF sampled signal is then down converted to the complex baseband and is represented as:</p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>y</i><sub>b</sub>(<i>t</i>)=<i>y</i>(<i>t</i>,&#x3c4;)<i>e</i><sup>&#x2212;j2&#x3c0;F</sup><sup><sub2>c</sub2></sup><sup>&#x3c4;</sup>&#x2003;&#x2003;Equation 3<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0000">where F<sub>c </sub>denotes the nominal operating frequency.</p><p id="p-0062" num="0057">B. Radar Processing</p><p id="p-0063" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a graphical flowchart of an exemplary process for audio signal extraction from a backscattered radar return (e.g., the RF response signal <b>18</b>). At a pre-processing stage (block <b>300</b>), motion filtering is implemented to remove a static background. In an exemplary aspect, the received radar return is organized in two-dimensional (2-D) matrix format with one direction corresponding to the slow-time samples t and the other direction to the fast-time/range samples T. By stacking range samples column-wise (block <b>302</b>), vibration profiles are revealed in a range-slow time heatmap. The most significant energy indicates where the surface motion occurs.</p><p id="p-0064" num="0059">In order to capture all the local motions, multiple range bins of interest are spatially combined into a single time series {dot over (y)}<sub>b</sub>(t) (block <b>304</b>) and a composite vibration profile (e.g., a Doppler profile) is obtained (block <b>306</b>). Then, time-varying spectral features are inspected through time frequency analysis. For visualization purposes of this study, a few edited audio sound clips that have very distinct spectral and temporal features are selected as shown in the results section. The radar recovered acoustic wave is constructed (block <b>308</b>) by the proposed audio recovery method in Section III. The goal is to extract good quality audio samples from radar signals and then these audible samples are played out by an audio device so that human can hear and recognize it.</p><heading id="h-0009" level="1">III. Audio Signal Processing</heading><p id="p-0065" num="0060">Sound is produced when molecules inside a medium are exerted by an internal or external force (certain embodiments consider only external forces). More elaborately, kinetic energy generated due to motion of an external object is transferred into surrounding molecules in vicinity of the object. This energy travels through the medium as a longitudinal wave and is heard as sound. In the context of radar processing, rate of object displacement is the Doppler frequency/velocity of object. The Doppler frequency is computed by taking a Fourier transform across slow time. In radar vibrometry, a short-time Fourier transform (STFT) is instead computed across slow time for a selected number of range bins.</p><p id="p-0066" num="0061"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a graphical representation of an analogy of a radar data matrix (which includes micro-displacements corresponding to an acoustic signal) with sound production. Using this analogy, radar vibrometry can be interpreted as hearing Doppler frequency of the object in time.</p><p id="p-0067" num="0062">During radar processing for a given slow time period, the received complex baseband waveform for a closely spaced set of range bins contains all vibration information from detected sources. STFT is then operated on the spatially combined complex baseband waveform {dot over (y)}<sub>b</sub>(t):</p><p id="p-0068" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i><sub>y</sub>(<i>t,f;h</i>)=STFT(<i>{dot over (y)}</i><sub>b</sub>(<i>t</i>))=&#x222b;<i>{dot over (y)}</i><sub>b</sub>(&#x3ba;)<i>h</i>*(&#x3ba;&#x2212;<i>t</i>)<i>e</i><sup>&#x2212;j2&#x3c0;f&#x3ba;</sup><i>d&#x3ba;</i>&#x2003;&#x2003;Equation 4<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0069" num="0000">Equivalently, it may be represented in the frequency spectrum as:</p><p id="p-0070" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>S</i><sub>y</sub>(<i>t,f;h</i>)=<i>S</i><sub>(y,f)</sub>(<i>t,f;h</i>)+<i>S</i><sub>(y,&#x2212;f)</sub>(<i>t,&#x2212;f;h</i>)&#x2003;&#x2003;Equation 5<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0071" num="0000">where S<sub>y</sub>(t, f; h) is the STFT of the signal {dot over (y)}<sub>b </sub>using a window function h(t). It is to be noted that the bandwidth of S<sub>y</sub>(t, f; h) is always band limited by maximum measurable Doppler frequency. The band limited signal inherently reduces ambient noise to give high quality sound. However, the STFT of complex baseband signal {dot over (y)}<sub>b</sub>(t) is generally not symmetric, S<sub>(y,f)</sub>&#x2260;S<sub>(y,&#x2212;f)</sub>. This poses a significant challenge in recovery. This symmetry issue is addressed by emphasizing that vibrations are physical phenomena and are transmitted/acquired as real signals. To this end, a conjugate symmetry is enforced such that S<sub>(y,f)</sub>(t, f; h)=S<sub>(y,&#x2212;f)</sub>*(t, &#x2212;f; h), and the resulting inverse STFT transformed waveform is deemed to be a real-valued signal x(t).</p><p id="p-0072" num="0063">The chosen mirroring spectrum is based on power density comparison, higher power gives better audible sound. Thus, if S<sub>(y,f)</sub>(t,f;h) contains more energy than S<sub>(y,&#x2212;f</sub>)(t,&#x2212;f;h) in the spectral range of interest, such as:</p><p id="p-0073" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3a3;<sub>t</sub>&#x3a3;<sub>f</sub><sub><sub2>1</sub2></sub><sup>f</sup><sup><sub2>2</sub2></sup><i>|S</i><sub>(y,f)</sub>(<i>t,f;h</i>)|<sup>2</sup>&#x3e;&#x3a3;<sub>t</sub>&#x3a3;<sub>f</sub><sub><sub2>1</sub2></sub><sup>f</sup><sup><sub2>2</sub2></sup><i>|S</i><sub>(y,&#x2212;f)</sub>(<i>t,&#x2212;f;h</i>)|<sup>2</sup>&#x2003;&#x2003;Equation 6<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0074" num="0000">then S<sub>(y,f)</sub>(t, f; h) is a better candidate for mirroring and vice versa. Hence Equation 5 reduces to:</p><p id="p-0075" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i>(<i>t</i>)=STFT<sup>&#x2212;1</sup><i>{S</i><sub>(y,f)</sub>(<i>t,f;h</i>)+<i>S</i><sub>(y,&#x2212;f</sub>)*(<i>t,&#x2212;f;h</i>)}&#x2003;&#x2003;Equation 7<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0076" num="0000">where g(t) is a window function used in inverse STFT. Additional processing steps by using filters on S<sub>y</sub>(t, f; h) may be performed to reduce noise, however for clarity those details are omitted. Under the unity energy assumption on window functions h(t) and g(t), Equation 7 simplifies into:</p><p id="p-0077" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>x</i>(<i>t</i>)=2Re{<i>{dot over (y)}</i><sub>b</sub>(<i>t</i>)}&#x2003;&#x2003;Equation 8<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0078" num="0000">specifically when both STFT and inverse STFT (ISTFT) windows are identical the resulting sound waveform generated is the real part of {dot over (y)}<sub>b</sub>(t), x(t)=2Re{{dot over (y)}<sub>b</sub>(t)}. If S<sub>(y,&#x2212;f)</sub>(t, &#x2212;f; h) is used in for mirroring in Equation 7, the resulting sound waveform is the imaginary part of {dot over (y)}<sub>b</sub>(t), x(t)=2Imag{{dot over (y)}<sub>b</sub>(t)}. The resulting range averaged complex baseband waveform contains most of the vibration information for production of sound. Evaluations on passive and active sources as well as discussion of these results are in the next section.</p><heading id="h-0010" level="1">IV. Evaluation Results</heading><p id="p-0079" num="0064">Successful radar vibrometry is demonstrated for a multitude of tests using a UWB radar with 2 gigahertz (GHz) bandwidth operating around 10 GHz with slow-time frame rate 1000 hertz (Hz). Three distinct evaluation setups are constructed&#x2014;1) recover source audio by observing a passive source, 2) detect and isolate signal from two sources separated in space and 3) retrieve audio from a source obstructed by a non-conductive barrier such as a wall (see <figref idref="DRAWINGS">FIGS. <b>1</b>A-<b>1</b>C</figref>).</p><p id="p-0080" num="0065">A. Evaluation Setup 1: Passive Source</p><p id="p-0081" num="0066"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an image of the first evaluation setup, which recovers an acoustic signal by observing surface displacement of a passive source. A passive object is exposed to an audio source in a cluttered environment, and surface vibrations of the object as a result of its proximity to the source are studied via UWB radar. Here, the audio source is a speaker labeled S and the object under observation is an empty aluminum soda can P, both of which are separated by a small distance of about 15 cm.</p><p id="p-0082" num="0067"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a spectrogram of the original audio signal. The micro-displacements dominated by the passive object are collected from the range bins of interest and processed. An improved radar spectrogram is obtained since all the major spectral features (bright dots at higher frequencies and square shapes at lower frequencies) seen in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> are recovered in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>. Additionally, higher order harmonic frequency components and intermodulations are observed.</p><p id="p-0083" num="0068"><figref idref="DRAWINGS">FIG. <b>6</b>C</figref> is a graphical representation of the radar recovered audio waveform from the radar result of <figref idref="DRAWINGS">FIG. <b>5</b></figref>. <figref idref="DRAWINGS">FIG. <b>6</b>D</figref> is a graphical representation of the original audio waveform. The radar recovered waveform is also consistent with the majority of the temporal structure of the original audio waveform.</p><p id="p-0084" num="0069">B. Evaluation Setup 2: Multiple Sources</p><p id="p-0085" num="0070"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an image of the second evaluation setup, which recovers multiple acoustic signals by observing and isolating surface displacement of two active sources at different ranges. This setup demonstrates the ability to isolate audio signals for different vibrating sources from the radar return signal in non-ideal environments by clever radar processing. An evaluation setup was constructed where two audio sources S<sub>1 </sub>and S<sub>2</sub>, separated in space, are being illuminated by a UWB radar. To add interest, two sound sources (two commercially available speakers) are used, playing two pre-edited sound files. The two sources are loudly playing different audio signals x<sub>1</sub>(t) and x<sub>2</sub>(t) respectively, which are recovered using methods outlined earlier.</p><p id="p-0086" num="0071"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal from the closer sound source S<sub>2 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is a spectrogram of the original audio signal from sound source S<sub>2</sub>. <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is a graphical representation of the radar recovered audio waveform {circumflex over (x)}<sub>2</sub>(t) from sound source S<sub>2</sub>. <figref idref="DRAWINGS">FIG. <b>8</b>D</figref> is a graphical representation of the original audio waveform x<sub>2</sub>(t) from sound source S<sub>2</sub>.</p><p id="p-0087" num="0072"><figref idref="DRAWINGS">FIG. <b>8</b>E</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal from the further sound source S<sub>1 </sub>of <figref idref="DRAWINGS">FIG. <b>7</b></figref>. <figref idref="DRAWINGS">FIG. <b>8</b>F</figref> is a spectrogram of the original audio signal from sound source S<sub>1</sub>. <figref idref="DRAWINGS">FIG. <b>8</b>G</figref> is a graphical representation of the radar recovered audio waveform {circumflex over (x)}<sub>1</sub>(t) from sound source S<sub>1</sub>. <figref idref="DRAWINGS">FIG. <b>8</b>H</figref> is a graphical representation of the original audio waveform x<sub>1</sub>(t) from sound source S<sub>1</sub>.</p><p id="p-0088" num="0073">In particular, the closer sound source S<sub>2 </sub>has a triangular temporal-spectral content since the loudness increases over time. More spectral harmonic structures show up at later time. The recovered radar spectrogram (<figref idref="DRAWINGS">FIG. <b>8</b>A</figref>) is very similar to the reference audio spectrogram (<figref idref="DRAWINGS">FIG. <b>8</b>B</figref>). The amplitude of the radar recovered sound wave (<figref idref="DRAWINGS">FIG. <b>8</b>C</figref>) increases over time as expected but not all details are recovered (<figref idref="DRAWINGS">FIG. <b>8</b>D</figref>). While the temporal-spectral content in the sound source S<sub>1 </sub>has a square shape, both the radar spectrogram and the waveform resemble the audio reference. Better recovery of the acoustic waveform of S<sub>1 </sub>is observed because this sound is played by a much larger/louder speaker resulting in a larger surface vibration though it is located at a slightly further distance.</p><p id="p-0089" num="0074">C. Evaluation Setup 3: See-Through Barriers</p><p id="p-0090" num="0075"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an image of the third evaluation setup, which recovers an acoustic signal through a wall or other barrier. Unlike light, microwave signals penetrate dielectric medium, such as clothes, glass, plastic, dry wood, wall, etc. This implies this setup can be utilized to surveil vibrating targets that are beyond line of sight (e.g., not directly observable through sound or light waves), a regime where cameras fail. Here, the ability to reconstruct an audio signal x(t) where the source S is obstructed by a non-conducting barrier B is demonstrated.</p><p id="p-0091" num="0076"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a spectrogram of a radar result for remote recovery of the acoustic signal from <figref idref="DRAWINGS">FIG. <b>9</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b>B</figref> is a spectrogram of the original audio signal. <figref idref="DRAWINGS">FIG. <b>10</b>C</figref> is a graphical representation of the radar recovered audio waveform. <figref idref="DRAWINGS">FIG. <b>10</b>D</figref> is a graphical representation of the original audio waveform.</p><p id="p-0092" num="0077">Despite the non-line-of-sight environment, the major spectral features and the temporal features are recovered in <figref idref="DRAWINGS">FIGS. <b>10</b>A and <b>10</b>C</figref> by comparing to the audio reference in <figref idref="DRAWINGS">FIGS. <b>10</b>B and <b>10</b>D</figref>. There are nine sound symbols corresponding to the nine distinct peaks in the audio waveform (<figref idref="DRAWINGS">FIG. <b>10</b>D</figref>). Similarly, the radar recovered waveform (<figref idref="DRAWINGS">FIG. <b>10</b>C</figref>) has nine spikes with a significantly reduced signal signal-to-noise ratio (SNR) due to energy loss in this blocked environment compared to the previous two evaluations. It is interesting to see that the most significant energy in the radar spectrogram is the low frequency content around 190 Hz while in the audio spectrogram it is 2nd-order harmonics around 380 Hz.</p><heading id="h-0011" level="1">V. Method for Remote Recovery of an Acoustic Signal</heading><p id="p-0093" num="0078"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flow diagram illustrating a process for remote recovery of an acoustic signal. The process begins at operation <b>1100</b>, with transmitting a radar signal toward a vibrating surface. The process continues at operation <b>1102</b>, with receiving an RF response signal corresponding to the radar signal. In an exemplary aspect, the radar sensor <b>14</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> sends a series of wideband radar impulses and receives the RF response signal. The process continues at operation <b>1104</b>, with extracting a motion signal from the RF response signal. The process continues at operation <b>1106</b>, with determining spatial information corresponding to the vibrating surface from the motion signal. The process continues at operation <b>1108</b>, with reconstructing an acoustic signal from the motion signal using the spatial information.</p><p id="p-0094" num="0079">Although the operations of <figref idref="DRAWINGS">FIG. <b>11</b></figref> are illustrated in a series, this is for illustrative purposes and the operations are not necessarily order dependent. Some operations may be performed in a different order than that presented. Further, processes within the scope of this disclosure may include fewer or more steps than those illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><heading id="h-0012" level="1">VI. Computer System</heading><p id="p-0095" num="0080"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of a generalized representation of an exemplary computer system <b>1200</b> that could be used to perform any of the methods or functions described above, such as remote recovery of an acoustic signal. In some examples, the RF device <b>12</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> is implemented as the computer system <b>1200</b> or a component of the computer system <b>1200</b>. In this regard, the computer system <b>1200</b> may be a circuit or circuits included in an electronic board card, such as, a printed circuit board (PCB), a server, a personal computer, a desktop computer, a laptop computer, an array of computers, a personal digital assistant (PDA), a computing pad, a mobile device, or any other device, and may represent, for example, a server or a user's computer.</p><p id="p-0096" num="0081">The exemplary computer system <b>1200</b> in this embodiment includes a processing device <b>1202</b> or processor, a main memory <b>1204</b> (e.g., read-only memory (ROM), flash memory, dynamic random access memory (DRAM), such as synchronous DRAM (SDRAM), etc.), and a static memory <b>1206</b> (e.g., flash memory, static random access memory (SRAM), etc.), which may communicate with each other via a data bus <b>1208</b>. Alternatively, the processing device <b>1202</b> may be connected to the main memory <b>1204</b> and/or static memory <b>1206</b> directly or via some other connectivity means. In an exemplary aspect, the processing device <b>1202</b> could be used to perform any of the methods or functions described above.</p><p id="p-0097" num="0082">The processing device <b>1202</b> represents one or more general-purpose processing devices, such as a microprocessor, central processing unit (CPU), or the like. More particularly, the processing device <b>1202</b> may be a complex instruction set computing (CISC) microprocessor, a reduced instruction set computing (RISC) microprocessor, a very long instruction word (VLIW) microprocessor, a processor implementing other instruction sets, or other processors implementing a combination of instruction sets. The processing device <b>1202</b> is configured to execute processing logic in instructions for performing the operations and steps discussed herein.</p><p id="p-0098" num="0083">The various illustrative logical blocks, modules, and circuits described in connection with the embodiments disclosed herein may be implemented or performed with the processing device <b>1202</b>, which may be a microprocessor, field programmable gate array (FPGA), a digital signal processor (DSP), an application-specific integrated circuit (ASIC), or other programmable logic device, a discrete gate or transistor logic, discrete hardware components, or any combination thereof designed to perform the functions described herein. Furthermore, the processing device <b>1202</b> may be a microprocessor, or may be any conventional processor, controller, microcontroller, or state machine. The processing device <b>1202</b> may also be implemented as a combination of computing devices (e.g., a combination of a DSP and a microprocessor, a plurality of microprocessors, one or more microprocessors in conjunction with a DSP core, or any other such configuration).</p><p id="p-0099" num="0084">The computer system <b>1200</b> may further include a network interface device <b>1210</b>. The computer system <b>1200</b> also may or may not include an input <b>1212</b>, configured to receive input and selections to be communicated to the computer system <b>1200</b> when executing instructions. The input <b>1212</b> may include, but not be limited to, a touch sensor (e.g., a touch display), an alphanumeric input device (e.g., a keyboard), and/or a cursor control device (e.g., a mouse). In an exemplary aspect, the radar sensor of <figref idref="DRAWINGS">FIG. <b>2</b></figref> is an input <b>1212</b> to the computer system <b>1200</b>. The computer system <b>1200</b> also may or may not include an output <b>1214</b>, including but not limited to a display, a video display unit (e.g., a liquid crystal display (LCD) or a cathode ray tube (CRT)), or a printer. In some examples, some or all inputs <b>1212</b> and outputs <b>1214</b> may be combination input/output devices. In an exemplary aspect, the radar sensor of <figref idref="DRAWINGS">FIG. <b>2</b></figref> is also an output <b>1214</b> of the computer system <b>1200</b>.</p><p id="p-0100" num="0085">The computer system <b>1200</b> may or may not include a data storage device that includes instructions <b>1216</b> stored in a computer-readable medium <b>1218</b>. The instructions <b>1216</b> may also reside, completely or at least partially, within the main memory <b>1204</b> and/or within the processing device <b>1202</b> during execution thereof by the computer system <b>1200</b>, the main memory <b>1204</b>, and the processing device <b>1202</b> also constituting computer-readable medium. The instructions <b>1216</b> may further be transmitted or received via the network interface device <b>1210</b>.</p><p id="p-0101" num="0086">While the computer-readable medium <b>1218</b> is shown in an exemplary embodiment to be a single medium, the term &#x201c;computer-readable medium&#x201d; should be taken to include a single medium or multiple media (e.g., a centralized or distributed database, and/or associated caches and servers) that store the one or more sets of instructions <b>1216</b>. The term &#x201c;computer-readable medium&#x201d; shall also be taken to include any medium that is capable of storing, encoding, or carrying a set of instructions for execution by the processing device <b>1202</b> and that causes the processing device <b>1202</b> to perform any one or more of the methodologies of the embodiments disclosed herein. The term &#x201c;computer-readable medium&#x201d; shall accordingly be taken to include, but not be limited to, solid-state memories, optical medium, and magnetic medium.</p><p id="p-0102" num="0087">The operational steps described in any of the exemplary embodiments herein are described to provide examples and discussion. The operations described may be performed in numerous different sequences other than the illustrated sequences. Furthermore, operations described in a single operational step may actually be performed in a number of different steps. Additionally, one or more operational steps discussed in the exemplary embodiments may be combined.</p><p id="p-0103" num="0088">Those skilled in the art will recognize improvements and modifications to the preferred embodiments of the present disclosure. All such improvements and modifications are considered within the scope of the concepts disclosed herein and the claims that follow.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230003835A1-20230105-M00001.NB"><img id="EMI-M00001" he="11.60mm" wi="76.20mm" file="US20230003835A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for remote recovery of an acoustic signal, the method comprising:<claim-text>transmitting a radar signal toward a vibrating surface;</claim-text><claim-text>receiving a radio frequency (RF) response signal corresponding to the radar signal;</claim-text><claim-text>extracting a motion signal from the RF response signal;</claim-text><claim-text>determining spatial information corresponding to the vibrating surface from the motion signal; and</claim-text><claim-text>reconstructing an acoustic signal from the motion signal using the spatial information.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the radar signal comprises a series of wideband radar impulses.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the spatial information comprises at least one of an angle to the vibrating surface, a range to the vibrating surface, and a Doppler frequency shift due to the vibrating surface.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the Doppler frequency shift is a large-scale Doppler frequency shift.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the Doppler frequency shift is a micro-Doppler frequency shift.</claim-text></claim><claim id="CLM-06-15" num="06-15"><claim-text><b>6</b>-<b>15</b>. (canceled)</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A radio frequency (RF) device, comprising:<claim-text>a radar sensor configured to receive a radio frequency (RF) response signal to a radar signal; and</claim-text><claim-text>a processing circuit coupled to the radar sensor and configured to recover an acoustic signal by:<claim-text>extracting a motion signal for one or more vibrating surfaces from the RF response signal;</claim-text><claim-text>determining spatial information corresponding to the one or more vibrating surfaces from the motion signal; and</claim-text><claim-text>using the spatial information to recover the acoustic signal from the motion signal.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The RF device of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the radar sensor is further configured to transmit the radar signal comprising a wideband radar signal.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The RF device of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the spatial information comprises at least one of an angle to each of the one or more vibrating surfaces, a range to each of the one or more vibrating surfaces, and a Doppler frequency shift due to each of the one or more vibrating surfaces.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The RF device of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein:<claim-text>the RF response signal comprises a plurality of acoustic signals each from a different acoustic source; and</claim-text><claim-text>the processing circuit is further configured to recover each one of the plurality of acoustic signals by:<claim-text>determining at least one of a range or an angle to a respective vibrating surface associated with the one of the plurality of acoustic signals;</claim-text><claim-text>determining a Doppler frequency shift at the at least one of the range or the angle to the respective vibrating surface; and</claim-text><claim-text>recovering the one of the plurality of acoustic signals using the Doppler frequency shift.</claim-text></claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A system for remote recovery of an acoustic signal, comprising:<claim-text>a radar sensor configured to receive a radio frequency (RF) response signal to a radar signal;</claim-text><claim-text>a database configured to store the RF response signal; and</claim-text><claim-text>a processing circuit coupled to the database and configured to:<claim-text>extract a motion signal for a vibrating surface from the RF response signal;</claim-text><claim-text>determine spatial information corresponding to the vibrating surface from the motion signal, the spatial information comprising at least one of a range to the vibrating surface, an angle to the vibrating surface, and a Doppler frequency shift associated with the vibrating surface; and</claim-text><claim-text>reconstruct the acoustic signal from the motion signal using the spatial information.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>