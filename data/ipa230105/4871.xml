<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004872A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004872</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17365650</doc-number><date>20210701</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6256</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>0012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30096</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR DEEP LEARNING TECHNIQUES UTILIZING CONTINUOUS FEDERATED LEARNING WITH A DISTRIBUTED DATA GENERATIVE MODEL</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GE PRECISION HEALTHCARE LLC</orgname><address><city>Wauwatosa</city><state>WI</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Ghose</last-name><first-name>Soumya</first-name><address><city>Niskayuna</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Madhavan</last-name><first-name>Radhika</first-name><address><city>Latham</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Bhushan</last-name><first-name>Chitresh</first-name><address><city>Glenville</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Shanbhag</last-name><first-name>Dattesh Dayanand</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Anand</last-name><first-name>Deepa</first-name><address><city>Bangalore</city><country>IN</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Yeo</last-name><first-name>Desmond Teck Beng</first-name><address><city>Clifton Park</city><state>NY</state><country>US</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Foo</last-name><first-name>Thomas Kwok-Fah</first-name><address><city>Clifton Park</city><state>NY</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer implemented method is provided. The method includes establishing, via multiple processors, a continuous federated learning framework including a global model at a global site and respective local models derived from the global model at respective local sites. The method also includes retraining or retuning, via the multiple processors, the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="174.75mm" wi="156.55mm" file="US20230004872A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="189.65mm" wi="149.69mm" file="US20230004872A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="185.17mm" wi="158.58mm" file="US20230004872A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.23mm" wi="127.68mm" file="US20230004872A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="215.65mm" wi="125.22mm" file="US20230004872A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="161.63mm" wi="156.29mm" orientation="landscape" file="US20230004872A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The subject matter disclosed herein relates to deep learning techniques and, more particularly, to systems and methods for deep learning techniques utilizing continuous federated learning with a distributed data generative model.</p><p id="p-0003" num="0002">Deep learning models have been proven successful in addressing problems involving sufficiently large, balanced and labeled datasets that appear in computer vision, speech processing, image processing, and other problems. Ideally, it is desired that these models continuously learn and adapt with new data, but this remains a challenge for neural network models since most of these models are trained with static large batches of data. Retraining with incremental data generally leads to catastrophic forgetting (i.e. training a model with new information interferes with previously learned knowledge).</p><p id="p-0004" num="0003">Ideally, artificial intelligence (AI) learning systems should adapt and learn continuously with new knowledge while refining existing knowledge. Current AI learning schemes assume that all samples are available during the training phase and, therefore, requires retraining of the network parameters on the entire dataset in order to adapt to changes in the data distribution. Although retraining from scratch pragmatically addresses catastrophic forgetting, in many practical scenarios, data privacy concerns do not allow for sharing of training data. In those cases, retraining with incremental new data can lead to significant loss of accuracy (catastrophic forgetting).</p><heading id="h-0002" level="1">BRIEF DESCRIPTION</heading><p id="p-0005" num="0004">A summary of certain embodiments disclosed herein is set forth below. It should be understood that these aspects are presented merely to provide the reader with a brief summary of these certain embodiments and that these aspects are not intended to limit the scope of this disclosure. Indeed, this disclosure may encompass a variety of aspects that may not be set forth below.</p><p id="p-0006" num="0005">In one embodiment, a computer implemented method is provided. The method includes establishing, via multiple processors, a continuous federated learning framework including a global model at a global site and respective local models derived from the global model at respective local sites. The method also includes retraining or retuning, via the multiple processors, the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</p><p id="p-0007" num="0006">In another embodiment, a deep learning-based continuous federated learning network system is provided. The system includes a global site including a global model. The system also includes multiple local sites, wherein each respective local site of the multiple local sites includes a respective local model derived from the global model. The system further includes multiple processors configured to retrain or retune the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</p><p id="p-0008" num="0007">In a further embodiment, a non-transitory computer-readable medium, the computer-readable medium including processor-executable code that when executed by one or more processors, causes the one or more processors to perform actions. The actions include establish a continuous federated learning framework comprising a global model at a global site and respective local models derived from the global model at respective local sites. The actions also include retraining or retuning the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">These and other features, aspects, and advantages of the present invention will become better understood when the following detailed description is read with reference to the accompanying drawings in which like characters represent like parts throughout the drawings, wherein:</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an embodiment of a schematic diagram of a continuous federated learning scheme or scenario, in accordance with aspects of the present disclosure;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an embodiment of a schematic diagram of a continuous federated learning scheme or scenario (e.g., utilizing distributed data generative models), in accordance with aspects of the present disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an embodiment of a schematic diagram of a centralized arrangement for a global site and local sites, in accordance with aspects of the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an embodiment of a schematic diagram of a decentralized arrangement for a global site and local sites, in accordance with aspects of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of a processor-based device or system that may be configured to implement functionality described herein, in accordance with aspects of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an embodiment of a flow chart of a method for retraining local and global models, in accordance with aspects of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an embodiment of a flow chart of a method for retraining local and global models, in accordance with aspects of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrate examples of synthetic medical images (e.g., FLAIR MRI images) generated with a generative model, in accordance with aspects of the present disclosure; and</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrate examples of synthetic medical images (e.g., T2 MRI images) generated with a generative model, in accordance with aspects of the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">One or more specific embodiments will be described below. In an effort to provide a concise description of these embodiments, not all features of an actual implementation are described in the specification. It should be appreciated that in the development of any such actual implementation, as in any engineering or design project, numerous implementation-specific decisions must be made to achieve the developers' specific goals, such as compliance with system-related and business-related constraints, which may vary from one implementation to another. Moreover, it should be appreciated that such a development effort might be complex and time consuming, but would nevertheless be a routine undertaking of design, fabrication, and manufacture for those of ordinary skill having the benefit of this disclosure.</p><p id="p-0020" num="0019">When introducing elements of various embodiments of the present invention, the articles &#x201c;a,&#x201d; &#x201c;an,&#x201d; &#x201c;the,&#x201d; and &#x201c;said&#x201d; are intended to mean that there are one or more of the elements. The terms &#x201c;comprising,&#x201d; &#x201c;including,&#x201d; and &#x201c;having&#x201d; are intended to be inclusive and mean that there may be additional elements other than the listed elements. Furthermore, any numerical examples in the following discussion are intended to be non-limiting, and thus additional numerical values, ranges, and percentages are within the scope of the disclosed embodiments.</p><p id="p-0021" num="0020">Some generalized information is provided to provide both general context for aspects of the present disclosure and to facilitate understanding and explanation of certain of the technical concepts described herein.</p><p id="p-0022" num="0021">Deep-learning (DL) approaches discussed herein may be based on artificial neural networks, and may therefore encompass one or more of deep neural networks, fully connected networks, convolutional neural networks (CNNs), perceptrons, encoders-decoders, recurrent networks, wavelet filter banks, u-nets, generative adversarial networks (GANs), or other neural network architectures. The neural networks may include shortcuts, activations, batch-normalization layers, and/or other features. These techniques are referred to herein as deep-learning techniques, though this terminology may also be used specifically in reference to the use of deep neural networks, which is a neural network having a plurality of layers.</p><p id="p-0023" num="0022">As discussed herein, deep-learning techniques (which may also be known as deep machine learning, hierarchical learning, or deep structured learning) are a branch of machine learning techniques that employ mathematical representations of data and artificial neural networks for learning and processing such representations. By way of example, deep-learning approaches may be characterized by their use of one or more algorithms to extract or model high level abstractions of a type of data-of-interest. This may be accomplished using one or more processing layers, with each layer typically corresponding to a different level of abstraction and, therefore potentially employing or utilizing different aspects of the initial data or outputs of a preceding layer (i.e., a hierarchy or cascade of layers) as the target of the processes or algorithms of a given layer. In an image processing or reconstruction context, this may be characterized as different layers corresponding to the different feature levels or resolution in the data. In general, the processing from one representation space to the next-level representation space can be considered as one &#x2018;stage&#x2019; of the process. Each stage of the process can be performed by separate neural networks or by different parts of one larger neural network.</p><p id="p-0024" num="0023">Deep neural nets combine feature representation learning and classifiers in a unified framework and have proven successful in many a problem involving sufficiently large, balanced and labeled datasets that appear in computer vision, speech processing, and image processing, and other problems. However, problems related to healthcare or in-flight monitoring offer a different set of challenges like limited data, diversity in sample distributions, and limited or no access to training data. Transfer learning is a common framework to retrain models given new incoming data but these set of models suffer from catastrophic forgetting (i.e., catastrophic loss of previously learned responses, whenever an attempt is made to train the network with a single new (additional) response). The challenge is to learn and adapt with new incoming data, while retaining memory of previously learned responses. This is further challenging in scenarios where the data at a site cannot be shared with a global or central site for retraining. In this case, the model should be able to adapt and learn online with data only from the site where it is deployed.</p><p id="p-0025" num="0024">Standard deep learning models are trained on centralized training data. Performance of a deep learning models may be adversely affected from site-specific variabilities like machine make, software versions, patient demographics, and site-specific clinical preferences. Federated learning enables incremental site-specific tuning of the global model to create local versions. Such models are more robust to site specific variabilities. Local models from multiple local sites are then further sent to the cloud using encrypted communication for fine tuning of the global model. During the process performance standard has to be maintained in global and local test dataset to adhere to regulatory authorities.</p><p id="p-0026" num="0025">The present disclosure provides for a data generation framework that enables estimating and generating synthetic or generative samples derived from global and local datasets to resolve the issue of data sharing privacy. Tuning/retraining of the weights and global model updating occurs utilizing the synthetic or generative samples (mitigating the issue of data privacy) from a distribution that closely resembles global and local dataset distribution in a federated-like learning framework. This enables local learning at the site level to account for site-specific preferences while maintaining global performance (mitigating the issue of catastrophic forgetting).</p><p id="p-0027" num="0026">Distributed local incremental learning and fine tuning ensures better performance compared to a global model trained with data from foreign sites. Such a model by design is generalizable across multiple industries including aviation, healthcare, power, additive manufacturing, and robotics. By making the updating of the weights of the global model dependent on the synthetic or generative samples derived from the global dataset, it ensures the licensed/validated global model architecture is maintained and local weights are fine tuned to better fit to local preferences, thus, improving performance over time without catastrophic forgetting.</p><p id="p-0028" num="0027">For example, as described below, a continuous federated learning framework including a global model at a global site and respective local models derived from the global model at respective local sites may be established. The retraining or retuning of the global model and the respective local models occurs without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets. This enables the diversity of the data distribution to be captured at the particular sites (e.g., local sites). Also, the efficiency of training (i.e., retraining/retuning) is increased.</p><p id="p-0029" num="0028">With the preceding in mind, and by way of providing useful context, <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a schematic diagram of a continuous federated learning scheme or scenario <b>10</b>. Standard deep learning models are trained on centralized training data. Performance of a deep learning models may be adversely affected from site-specific variabilities like machine make, software versions, patient demographics and site-specific clinical preferences. As depicted, the continuous federated learning scheme <b>10</b> includes a global site <b>12</b> (e.g., central or main site) and multiple local sites or nodes <b>14</b> (e.g., remote from the global site <b>12</b>). The global site <b>12</b> includes a global model <b>16</b> (e.g., global neural network or machine learning model) trained on a primary dataset <b>17</b> (e.g., global dataset). Federated learning enables incremental site-specific tuning of the global model <b>16</b> (via local incremental learning on local data) to create local versions <b>18</b>. Such models are more robust to site specific variabilities. Local models <b>18</b> (e.g., local neural network or machine learning model) from local sites <b>14</b> are then further sent to the cloud using encrypted communication for fine tuning of the global model <b>16</b>. During the process, a performance standard has to be maintained in global and local test datasets.</p><p id="p-0030" num="0029">In the continuous federated learning scenario <b>10</b>, the global model <b>16</b> is deployed across multiple sites <b>14</b> that cannot export data. A site-specific ground truth is generated using auto-curation models that may use segmentation, registration machine learning, and/or deep learning models. The site-specific ground truth may have to be refined depending on local preferences of the expert. An automatically generated and refined ground truth is then further used for local training of the models. Selective local updates of the weights of the global model <b>16</b> creates a local mutant <b>18</b> of the global model <b>16</b>. The weights of the local models <b>18</b> are then encrypted and sent to the central server for selective updating of the global model <b>16</b> as indicated by block <b>20</b>. These local updates or site-specific preferences (e.g., weights) from the local sites <b>14</b> are combined when updating the global model <b>16</b> at the global site <b>12</b>. The global model update would be strategic and would be dependent on domain and industry specific requirements.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a continuous federated learning scheme or scenario <b>22</b> that utilizes distributed data generative models to resolve issues with data privacy (or high volume data that cannot be stored) and catastrophic forgetting. The continuous federated learning scheme <b>22</b> includes a global site <b>12</b> (e.g., central or main site) and multiple local sites or nodes <b>14</b> (e.g., remote from the global site <b>12</b>). The global site <b>12</b> includes a global model <b>16</b> (e.g., global neural network or machine learning model) trained on a primary dataset <b>17</b> (e.g., global dataset) of actual or true data. During multi-site deployment, each local site <b>14</b> also receives the global model <b>16</b> as initially trained at the global site <b>12</b>. At the global site <b>12</b>, a generative model <b>24</b> (e.g., global generative model) utilizes the primary dataset <b>17</b> to synthesize or generate a synthetic or generated (e.g., generative) dataset <b>26</b> (e.g., global synthetic or generated dataset) similar to the primary dataset <b>17</b>. The synthesized or generated dataset <b>26</b> derived from the primary dataset <b>17</b> reflects the distribution of the actual or true data in the primary dataset <b>17</b>. The generative model <b>24</b> may be created utilizing variational autoencoders, a generative adversarial network, data augmentation, and/or regression methods. The generative model <b>24</b> and the generated dataset <b>26</b> are distributed to each of the local sites <b>14</b> via multi-site deployment.</p><p id="p-0032" num="0031">At the local sites <b>14</b>, the generated dataset <b>26</b> and a local dataset (actual or true local data) are combined for utilization in the local retuning/retraining of the global model <b>16</b> to generate a new local model <b>18</b>. Also, at the local sites <b>14</b>, a local generative model <b>28</b> is created from the generative model <b>24</b> and the local dataset. The local generative model <b>28</b> utilizes the local dataset to synthesize or generate a synthetic or generated (e.g., generative) dataset <b>30</b> (e.g., local synthetic or generated dataset) similar to the primary dataset <b>17</b>. The local synthesized or generated dataset <b>30</b> derived from the local dataset reflects the distribution of the actual or true data in the local dataset <b>17</b>. The new local models <b>18</b>, the local generative models <b>28</b>, and the local generated datasets <b>30</b> from each of the local sites <b>14</b> are then encrypted and sent to the central server for selective updating/retuning/retraining of the global model <b>16</b> as indicated by block <b>32</b>. A retrained global model may then be provided to the local sites <b>14</b>. This process may occur in an iterative manner. Over time, after repeating the cycle iteratively, the respective local generative model <b>28</b> and the generative model <b>24</b> should eventually have the same distribution (i.e., the models <b>24</b>, <b>28</b> will converge at least with regard to mean and variance).</p><p id="p-0033" num="0032">Retraining using synthetic samples similar to global and local datasets ensures data privacy and mitigates catastrophic forgetting. Creating generative models configured to generate synthetic samples similar to those at the global and local sites ensures global and local data distribution is captured enabling training (e.g., retraining) of a neural network in a continuous federated learning framework without data sharing.</p><p id="p-0034" num="0033">The global site <b>12</b> and the local sites <b>14</b> may be arranged in a centralized arrangement as depicted in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. For example, the global site <b>12</b> may be located at one or more central or main servers or computing devices at a central or main site. The local sites or nodes <b>14</b> may be located remotely from the location of the global site <b>12</b>. Each local site or node <b>14</b> may include one or more servers or computing devices. The global site <b>12</b> and the local sites <b>14</b> may be interconnected via the Internet. In certain embodiments, the global site <b>12</b> and the local sites may be interconnected via a cloud or a cloud computing environment. As used herein, the term &#x201c;cloud&#x201d; or &#x201c;cloud computing environment&#x201d; may refer to various evolving arrangements, infrastructure, networks, and the like that will typically be based upon the Internet. The term may refer to any type of cloud, including client clouds, application clouds, platform clouds, infrastructure clouds, server clouds, and so forth.</p><p id="p-0035" num="0034">Alternatively, the global site <b>12</b> and the local sites <b>14</b> may be arranged in a decentralized arrangement as depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. In the decentralized arrangement, the global site <b>12</b> does not truly exist (but is maintained, for example, in a cloud environment) by the local sites <b>14</b> which are configured to coordinate between themselves. For example, a cloud computing environment <b>36</b> includes a plurality of distributed nodes <b>14</b> (e.g., local sites). The computing resources of the nodes <b>14</b> are pooled to serve multiple consumers, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. Examples of resources include storage, processing, memory, network bandwidth, and virtual machines. The nodes <b>14</b> may communicate with one another to distribute resources, and such communication and management of distribution of resources may be controlled by a cloud management module, residing on one or more nodes <b>14</b>. The nodes <b>14</b> may communicate via any suitable arrangement and protocol. Further, the nodes <b>14</b> may include servers associated with one or more providers. For example, certain programs or software platforms may be accessed via a set of nodes <b>14</b> provided by the owner of the programs while other nodes <b>14</b> are provided by data storage companies. Certain nodes <b>14</b> may also be overflow nodes that are used during higher load times.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram of a processor-based device or system <b>38</b> that may be configured to implement functionality described herein in accordance with one embodiment. Various functionality, as described herein, may be performed by, or in conjunction with, a processor-based system <b>38</b>, which is generally depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref> in accordance with one embodiment. For example, the various computing devices or servers (e.g., utilized at the global site and/or local sites) herein may include, or be partially or entirely embodied in, a processor-based system, such as that presently illustrated. The processor-based system <b>38</b> may be a general-purpose computer, such as a personal computer, configured to run a variety of software, including software implementing all or part of the functionality described herein. Alternatively, in other embodiments, the processor-based system <b>38</b> may include, among other things, a distributed computing system, or an application-specific computer or workstation configured to implement all or part of the presently described functionality based on specialized software and/or hardware provided as part of the system. Further, the processor-based system <b>38</b> may include either a single processor or a plurality of processors to facilitate implementation of the presently disclosed functionality.</p><p id="p-0037" num="0036">In one embodiment, the exemplary processor-based system <b>38</b> includes a microcontroller or microprocessor <b>40</b>, such as a central processing unit (CPU), which executes various routines and processing functions of the system <b>38</b>. For example, the microprocessor <b>40</b> may execute various operating system instructions, as well as software routines configured to effect certain processes, stored in or provided by a manufacture including one or more computer readable-media (at least collectively storing the software routines), such as a memory <b>42</b> (e.g., a random access memory (RAM) of a personal computer) or one or more mass storage devices <b>44</b> (e.g., an internal or external hard drive, a solid-state storage device, a CD-ROM, a DVD, or another storage device). In addition, the microprocessor <b>40</b> processes data provided as inputs for various routines or software programs, such as data provided as part of the present subject matter described herein in computer-based implementations.</p><p id="p-0038" num="0037">Such data may be stored in, or provided by, the memory <b>42</b> or mass storage device <b>44</b>. The memory <b>42</b> or the mass storage device may store various datasets (e.g., actual datasets such as the global dataset or local dataset, local synthetic dataset, global synthetic dataset, etc.), various deep learning or machine learning models (e.g., global modes, local models, global generative model, local generative model, etc.), and other information. Alternatively, such data may be provided to the microprocessor <b>40</b> via one or more input devices <b>46</b>. The input devices <b>46</b> may include manual input devices, such as a keyboard, a mouse, touchscreen (e.g., on tablet), or the like. In addition, the input devices <b>46</b> may include a network device, such as a wired or wireless Ethernet card, a wireless network adapter, or any of various ports or devices configured to facilitate communication with other devices via any suitable communications network, such as a local area network or the Internet. Through such a network device, the system <b>38</b> may exchange data and communicate with other networked electronic systems, whether proximate to or remote from the system <b>38</b>.</p><p id="p-0039" num="0038">Results generated by the microprocessor <b>40</b>, such as the results obtained by processing data in accordance with one or more stored routines, may be provided to an operator via one or more output devices <b>48</b> (e.g., a display). Communication between the various components of the processor-based system <b>38</b> may typically be accomplished via a chipset and one or more busses or interconnects which electrically connect the components of the system <b>38</b>.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow chart of a method <b>50</b> for retraining local and global models in a continuous federated learning framework. One or more components (e.g., processor-based devices <b>38</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>) of the global site <b>12</b> and/or the local sites <b>14</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be utilized for performing the method <b>50</b>. One or more steps of the method <b>50</b> may be performed simultaneously or in a different order from that depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. It is assumed in the continuous federated learning framework that global and local datasets are available at the global site and the local sites, respectively, and generated trained models.</p><p id="p-0041" num="0040">The method <b>50</b> includes establishing a continuous federated learning framework including a global model at a global site and respective local models derived from the global model at respective local sites (block <b>52</b>). Establishing the continuous federated learning framework may include generating a trained global model (e.g., utilizing an actual global dataset) and validating the trained global model (e.g., utilizing an actual global test dataset held out from or separate from the actual global dataset) at a global site (e.g., central or main site). Establishing the continuous federated learning framework may also include providing the trained global model to multiple local sites or nodes remote from the global site. This may include at each local site accessing the trained global model from a database or memory available to each local site.</p><p id="p-0042" num="0041">The method <b>50</b> also includes retraining or retuning the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic or generative datasets generated from the actual datasets (block <b>54</b>). Actual datasets (e.g., actual global dataset and respective actual local datasets) are not shared (e.g., due to data privacy or a high volume of data that cannot be stored) between the local sites or between the global site and the local sites.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flow chart of a method <b>56</b> for retraining local and global models in a continuous federated learning framework. One or more components (e.g., processor-based devices <b>38</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>) of the global site <b>12</b> and/or the local sites <b>14</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be utilized for performing the method <b>50</b>. One or more steps of the method <b>56</b> may be performed simultaneously or in a different order from that depicted in <figref idref="DRAWINGS">FIG. <b>7</b></figref>. It is assumed in the continuous federated learning framework that global and local datasets are available at the global site and the local sites, respectively, and generated trained models. It also assumed that each local site includes a trained local model (e.g., trained on an actual local dataset) derived from the trained global model at the global site.</p><p id="p-0044" num="0043">The method <b>56</b> includes, at the global site, creating or generating a generative model configured to generate a synthetic or generated global dataset similar to and based on the actual global dataset utilizing the actual global dataset (block <b>58</b>). In certain embodiments, the generative model may be created utilizing variational autoencoders, a generative adversarial network, data augmentation, and/or regression methods.</p><p id="p-0045" num="0044">The method <b>56</b> also includes, at the global site, providing the generative model and the synthetic global dataset to each of the respective local sites (block <b>60</b>). The method <b>56</b> further includes, at each local site, retraining or retuning each respective local model utilizing both the synthetic global dataset and an actual local dataset at the respective local site to locally retune weights to generate a new respective local model (block <b>62</b>). The method <b>56</b> even further includes, at each local site, validating each new respective local model utilizing an actual local test dataset at the respective local site (without catastrophic forgetting) (block <b>64</b>). The actual local test dataset is held out from or separate from the actual local dataset utilized for training the local model and generating the generative local dataset. The method <b>56</b> still further includes, at each local site, creating or generating a local generative model configured to generate a synthetic or generated local dataset similar to and based on the actual local dataset utilizing the actual local dataset (block <b>66</b>). The global generative model may also be utilized in generating the local generative model. In particular, the global generative model at the local site may be retuned or retrained utilizing the actual local dataset.</p><p id="p-0046" num="0045">The method <b>56</b> includes, at each local site, providing the respective local generative model, the new respective local model, and the respective synthetic local dataset to the global site (block <b>68</b>). The method <b>56</b> also includes, at the global site, validating each new respective local model utilizing an actual global test dataset (block <b>70</b>). The actual global test dataset is held out from or separate from the actual global dataset utilized for training the global model and generating the generative global dataset.</p><p id="p-0047" num="0046">The method <b>56</b> includes, at the global site, retraining or retuning the global model utilizing the respective synthetic local datasets from each of the respective local sites to retune global weights to generate a retrained global model (block <b>72</b>). The method <b>56</b> also includes, at the global site, validating the retrained global model utilizing the actual global test dataset (block <b>74</b>). The method <b>56</b> further includes, at the global site, providing the retrained global model to each of the respective local sites (block <b>76</b>). The steps of the method <b>56</b> may then be repeated in an iterative manner.</p><p id="p-0048" num="0047">The systems and methods described above may be utilized on a variety of types of data in various industries (e.g., healthcare, aviation, etc.). One example of data that may be utilized is imaging data (e.g., medical imaging data) acquired from medical imaging systems. <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref> provide examples of synthetic medical images generated by global or local generative models via simulated data distribution based on actual medical images. <figref idref="DRAWINGS">FIG. <b>8</b></figref> includes synthetic images <b>78</b>, <b>80</b>, and <b>82</b> of magnetic resonance (MR) images of the brain acquired with a fluid-attenuated inversion recovery (FLAIR) MRI sequence. Circles <b>84</b> indicate synthetic lesions generated in the synthetic images <b>78</b>, <b>80</b>, and <b>82</b>. <figref idref="DRAWINGS">FIG. <b>9</b></figref> includes synthetic images <b>86</b>, <b>88</b>, and <b>90</b> of MR images of the brain acquired with a T2 MRI sequence. Circles <b>92</b> indicate synthetic lesions generated in the synthetic images <b>86</b>, <b>88</b>, and <b>90</b>.</p><p id="p-0049" num="0048">Technical effects of the disclosed subject matter include providing systems and methods for utilizing a continuous federated learning framework that utilizes training of local and global models with generated data similar to the actual local and global datasets mitigating the issue of catastrophic forgetting and data sharing concerns. The approach is operator independent and also provides local adaptation and site-specific customization. In addition, the approach reduces time related to retraining/testing, especially in areas (e.g., healthcare) where it is difficult to share training data. Further, the approach provides online learning of site-specific data.</p><p id="p-0050" num="0049">The techniques presented and claimed herein are referenced and applied to material objects and concrete examples of a practical nature that demonstrably improve the present technical field and, as such, are not abstract, intangible or purely theoretical. Further, if any claims appended to the end of this specification contain one or more elements designated as &#x201c;means for [perform]ing [a function] . . . &#x201d; or &#x201c;step for [perform]ing [a function] . . . &#x201d;, it is intended that such elements are to be interpreted under 35 U.S.C. 112(f). However, for any claims containing elements designated in any other manner, it is intended that such elements are not to be interpreted under 35 U.S.C. 112(f).</p><p id="p-0051" num="0050">This written description uses examples to disclose the present subject matter, including the best mode, and also to enable any person skilled in the art to practice the subject matter, including making and using any devices or systems and performing any incorporated methods. The patentable scope of the subject matter is defined by the claims, and may include other examples that occur to those skilled in the art. Such other examples are intended to be within the scope of the claims if they have structural elements that do not differ from the literal language of the claims, or if they include equivalent structural elements with insubstantial differences from the literal languages of the claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer implemented method, comprising:<claim-text>establishing, via a plurality of processors, a continuous federated learning framework comprising a global model at a global site and respective local models derived from the global model at respective local sites; and</claim-text><claim-text>retraining or retuning, via the plurality of processors, the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising generating, via one or more processors at the global site, a generative model configured to generate a synthetic global dataset from the actual global dataset.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, comprising providing, via the one or more processors at the global site, the generative model and the synthetic global dataset to each of the respective local sites.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, comprising retraining or retuning, via one or more processors at each respective local site, each respective local model utilizing both the synthetic global dataset and an actual local dataset at the respective local site to locally retune weights to generate a new respective local model.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, comprising validating, via the one or more processors at each respective local site, each new respective local model utilizing an actual local test dataset at the respective local site without catastrophic forgetting.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, comprising generating, via the one or more processors at each respective local site, a respective local generative model configured to generate a respective synthetic local dataset from the actual local dataset at the respective local site.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer implemented method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, comprising providing, via the one or more processors at each respective local site, the respective local generative model, the new respective local model, and the respective synthetic local dataset to the global site.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, comprising validating, via the one or more processors at the global site, each new respective local model utilizing an actual global test dataset.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer implemented method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, comprising retraining or retuning, via one or more processors at the global site, the global model utilizing the respective synthetic local datasets from each of the respective local sites to retune global weights to generate a retrained global model.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, comprising validating, via the one or more processors at the global site, the retrained global model utilizing an actual global test dataset.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer implemented method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, comprising providing, via the one or more processors at the global site, the retrained global model to each of the respective local sites.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the actual datasets comprise medical imaging data acquired by medical imaging systems.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A deep learning-based continuous federated learning network system, comprising:<claim-text>a global site comprising a global model;</claim-text><claim-text>a plurality of local sites, wherein each respective local site of the plurality of local sites comprises a respective local model derived from the global model; and</claim-text><claim-text>a plurality of processors configured to retrain or retune the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein, at the global site, one or more processors of the plurality of processors are configured to:<claim-text>generate a generative model configured to generate a synthetic global dataset from the actual global dataset; and</claim-text><claim-text>provide the generative model and the synthetic global dataset to each of the respective local sites.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein, at each respective local site, one or more processors of the plurality of processors are configured to:<claim-text>retrain or retune the respective local model utilizing both the synthetic global dataset and an actual local dataset at the respective local site to locally retune weights to generate a new respective local model; and</claim-text><claim-text>validate the new respective local model utilizing an actual local test dataset at the respective local site.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The system of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein, at each respective local site, one or more processors of the plurality of processors are configured to:<claim-text>generate a respective local generative model configured to generate a respective synthetic local dataset from the actual local dataset at the respective local site; and</claim-text><claim-text>provide the respective local generative model, the new respective local model, and the respective synthetic local dataset to the global site.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein, at the global site, one or more processors of the plurality of processors are configured to:<claim-text>validate each new respective local model utilizing an actual global test dataset; and</claim-text><claim-text>retrain or retune the global model utilizing the respective synthetic local datasets from each of the respective local sites to retune global weights to generate a retrained global model.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein, at the global site, one or more processors of the plurality of processors are configured to:<claim-text>validate the retrained global model utilizing the actual global test dataset; and</claim-text><claim-text>provide the retrained global model to each of the respective local sites.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The system of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the actual datasets comprise medical imaging data acquired by medical imaging systems.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable medium, the computer-readable medium comprising processor-executable code that when executed by one or more processors, causes the one or more processors to:<claim-text>establish a continuous federated learning framework comprising a global model at a global site and respective local models derived from the global model at respective local sites; and</claim-text><claim-text>retrain or retune the global model and the respective local models without sharing actual datasets between the global site and the respective local sites but instead sharing synthetic datasets generated from the actual datasets.</claim-text></claim-text></claim></claims></us-patent-application>