<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004284A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004284</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17780417</doc-number><date>20201126</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04847</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>131</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>032</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>04847</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>L</subclass><main-group>67</main-group><subgroup>131</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>013</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>61</class><subclass>B</subclass><main-group>3</main-group><subgroup>032</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180501</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>13</main-group><subgroup>344</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEM AND METHOD FOR VIRTUAL REALITY BASED HUMAN BIOLOGICAL METRICS COLLECTION AND STIMULUS PRESENTATION</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>62942000</doc-number><date>20191129</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Electric Puppets Incorporated</orgname><address><city>Halifax</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Cameron</last-name><first-name>Ryan</first-name><address><city>Chester</city><country>CA</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CA2020/051620</doc-number><date>20201126</date></document-id><us-371c12-date><date>20220526</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of updating a protocol for a Virtual Reality (VR) medical test via a user device having a processor, the VR medical test being performed on a subject via a VR device worn by the subject, wherein the method is performed by the processor and the method comprises: displaying GUI elements associated with the protocol on the user device, the GUI elements having user adjustable settings for modifying a functioning of the VR medical test; receiving a selection input from the user device corresponding to a selection of the GUI elements; receiving a setting input from the user device that corresponds to the selected GUI elements; modifying the user adjustable setting for each of the selected GUI elements according to the corresponding setting input; and updating the protocol based on the user adjustable setting for each of the selected GUI elements and operations associated with the VR device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="103.04mm" wi="158.75mm" file="US20230004284A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="224.28mm" wi="158.83mm" orientation="landscape" file="US20230004284A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="189.99mm" wi="146.90mm" orientation="landscape" file="US20230004284A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="211.41mm" wi="98.30mm" file="US20230004284A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.69mm" wi="154.43mm" orientation="landscape" file="US20230004284A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="219.79mm" wi="154.43mm" orientation="landscape" file="US20230004284A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="172.72mm" wi="154.69mm" orientation="landscape" file="US20230004284A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="187.79mm" wi="157.99mm" orientation="landscape" file="US20230004284A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="127.59mm" wi="152.82mm" orientation="landscape" file="US20230004284A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="189.91mm" wi="147.74mm" orientation="landscape" file="US20230004284A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="207.26mm" wi="126.15mm" orientation="landscape" file="US20230004284A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="212.60mm" wi="154.86mm" orientation="landscape" file="US20230004284A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="197.70mm" wi="136.99mm" orientation="landscape" file="US20230004284A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="173.40mm" wi="151.05mm" orientation="landscape" file="US20230004284A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="200.66mm" wi="150.28mm" orientation="landscape" file="US20230004284A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="176.45mm" wi="147.24mm" orientation="landscape" file="US20230004284A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="176.36mm" wi="142.32mm" orientation="landscape" file="US20230004284A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="195.66mm" wi="157.06mm" orientation="landscape" file="US20230004284A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="200.66mm" wi="158.16mm" orientation="landscape" file="US20230004284A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="222.59mm" wi="141.90mm" orientation="landscape" file="US20230004284A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="135.38mm" wi="148.25mm" orientation="landscape" file="US20230004284A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="193.80mm" wi="140.04mm" orientation="landscape" file="US20230004284A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="172.30mm" wi="121.92mm" orientation="landscape" file="US20230004284A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="167.22mm" wi="122.85mm" orientation="landscape" file="US20230004284A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="215.31mm" wi="154.35mm" orientation="landscape" file="US20230004284A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="198.29mm" wi="139.11mm" orientation="landscape" file="US20230004284A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="203.28mm" wi="142.58mm" orientation="landscape" file="US20230004284A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="204.47mm" wi="158.67mm" orientation="landscape" file="US20230004284A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="205.32mm" wi="148.17mm" orientation="landscape" file="US20230004284A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="213.70mm" wi="120.65mm" orientation="landscape" file="US20230004284A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="204.98mm" wi="144.19mm" orientation="landscape" file="US20230004284A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="213.19mm" wi="130.13mm" orientation="landscape" file="US20230004284A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="224.45mm" wi="160.36mm" orientation="landscape" file="US20230004284A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="225.72mm" wi="148.67mm" file="US20230004284A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="219.79mm" wi="151.64mm" orientation="landscape" file="US20230004284A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="234.27mm" wi="148.51mm" file="US20230004284A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE</heading><p id="p-0002" num="0001">This application claims the benefit of U.S. Provisional Patent Application No. 62/942,000, filed Nov. 29, 2019, and the entire contents of U.S. Provisional Patent Application No. 62/942,000 is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">Various embodiments are described herein that relate to generating and delivering virtual reality based medical testing systems and methods.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">The following paragraphs are provided by way of background to the present disclosure. However, the following paragraphs are not an admission that anything discussed therein is prior art or part of the knowledge of persons skilled in the art.</p><p id="p-0005" num="0004">A portion of every eye exam requires subjective feedback from subjects (e.g., patients) to determine the level of functioning of the visual system. The feedback includes, for example, what letter does the subject see, or whether the subject can tell if any shapes stick out of a book. These become more challenging in a pediatric ophthalmology service or in clinics specializing in disorders of binocular vision. Current technology is antiquated, expensive, and time consuming. The market has not seen any substantial advances in several decades.</p><p id="p-0006" num="0005">Virtual reality (VR) has advanced over the years to the point that it is being used to simulate more and more real-life experiences. However, combining VR technology and clinical testing presents challenges, both for the developer (i.e., the person developing a system for use with aVR device) and for the user (i.e., the person administering the clinical test on a VR device).</p><p id="p-0007" num="0006">There is a need for a system and method for the generation and delivery of virtual reality based medical (e.g., visual) testing that addresses the challenges and/or shortcomings described above.</p><heading id="h-0004" level="1">SUMMARY OF VARIOUS EMBODIMENTS</heading><p id="p-0008" num="0007">Various embodiments of a system and method for virtual reality based human biological metrics collection and stimulus presentation, and computer products for use therewith, are provided according to the teachings herein.</p><p id="p-0009" num="0008">In one aspect, in accordance with the teachings herein, there is provided a method of updating a protocol for a Virtual Reality (VR) medical test via a user device having at least one processor, the VR medical test being performed on a subject via a VR device worn by the subject, wherein the method is performed by the at least one processor and the method comprises: displaying GUI elements associated with the protocol on the user device, the GUI elements having user adjustable settings for modifying a functioning of the VR medical test; receiving at least one selection input from the user device corresponding to a selection of at least one of the GUI elements; receiving at least one setting input from the user device that corresponds to the selection of the at least one selected GUI elements; modifying the user adjustable setting for each of the at least one selected GUI elements according to the corresponding at least one setting input; and updating the protocol based at least on the user adjustable setting for each of the at least one selected GUI elements and the plurality of operations associated with the VR device.</p><p id="p-0010" num="0009">In at least one embodiment, the method comprises determining at least one of the plurality of operations to be executed on the VR device during the VR medical test based at least in part on the at least one selection input and the at least one setting input.</p><p id="p-0011" num="0010">In at least one embodiment, the VR medical test is a VR vision test.</p><p id="p-0012" num="0011">In at least one embodiment, the GUI elements comprise at least one of one or more VR methods, one or more VR features, or one or more VR controls.</p><p id="p-0013" num="0012">In at least one embodiment, the one or more VR methods comprise at least one of: left eye tracking, right eye tracking, head rotation tracking, or laser pointer placement.</p><p id="p-0014" num="0013">In at least one embodiment, the one or more VR features comprise at least one of: a subject data area, a subject VR view, or an eye camera area.</p><p id="p-0015" num="0014">In at least one embodiment, the one or more VR controls comprise at least one of: at least one state button, at least one toggle button, at least one slider, and at least one VR controller icon.</p><p id="p-0016" num="0015">In at least one embodiment, determining at least one of the plurality of operations comprises organizing a VR visual stimulus including determining an intensity, one or more colors, and one or more shapes of at least one object in the VR visual stimulus.</p><p id="p-0017" num="0016">In at least one embodiment, determining at least one of the plurality of operations comprises filtering a visual stimulus with specific wavelengths of color at varying intensities to obtain data from the subject during VR testing relating to color sensitivity.</p><p id="p-0018" num="0017">In at least one embodiment, determining at least one of the plurality of operations comprises at least one of altering an orientation and/or position of an object in the visual stimulus, and displaying the visual stimulus to one or both eyes of the subject.</p><p id="p-0019" num="0018">In at least one embodiment, determining at least one of the plurality of operations comprises representing and recording each axis of rotation of the subject's head relative to a direction of presentation of the VR visual stimulus to the subject.</p><p id="p-0020" num="0019">In at least one embodiment, the at least one of the GUI elements comprises a user controllable setting for allowing the user to set at least one of a distance, an orientation, and an intensity of a visual VR stimulus using the user device.</p><p id="p-0021" num="0020">In at least one embodiment, the at least one of the GUI elements comprises a sensor control and the user adjustable setting therefor allows the user to select when the sensor is used to obtain measurements of the subject during testing.</p><p id="p-0022" num="0021">In at least one embodiment, the method comprises receiving user input for the sensor control for controlling at least one of an eye tracker, a camera, a force feedback sensor, an EEG sensor, and a motion tracker.</p><p id="p-0023" num="0022">In at least one embodiment, the functionality of the selection comprises at least one of displacement, rotation, or color filtering.</p><p id="p-0024" num="0023">In at least one embodiment, the method comprises: receiving at least one recording input from the user device corresponding to recording instructions associated with recording data during execution of the VR medical test on the VR device.</p><p id="p-0025" num="0024">In at least one embodiment, the recording instructions comprise at least one of tracking one or both subject eyes, recording a position of the subject's head, recording a rotation of the subject's head, and recording a selection made by the subject through a hand gesture or interaction with a VR controller.</p><p id="p-0026" num="0025">In at least one embodiment, the VR vision test is a Bagolini Striated Lens VR test and the method comprises receiving user input to adjust settings for GUI elements associated with a gradations input, a distance input, and a rotation input.</p><p id="p-0027" num="0026">In at least one embodiment, the VR vision test is a Frisby Stereopsis VR test and the method comprises receiving user input to adjust settings for GUI elements associated with a stereopsis stimulus selection input and a distance input.</p><p id="p-0028" num="0027">In at least one embodiment, the VR vision test is a Synoptophore VR test and the method further comprises receiving user input to adjust settings for GUI elements associated with a reverse eye stimulus input, a swap eye stimulus input, an actual stimulus input, and a stimulus distance input.</p><p id="p-0029" num="0028">In at least one embodiment, the VR vision test is a Lees Screen VR test and the method further comprises receiving user input to adjust settings for GUI elements associated with a show close-up stimulus input, a show distant stimulus input, and a show eye orientation input.</p><p id="p-0030" num="0029">In another aspect, in accordance with the teachings herein, there is provided a system for allowing a user to update a protocol for a Virtual Reality (VR) medical test to be performed on a subject via a VR device worn by the subject, wherein the system comprises: a display that shows a graphical user interface having GUI elements representing settings for the protocol; an input device that is adapted to receive user inputs from the user for at least one of the GUI elements; a memory unit that is adapted to store a protocol data file for the updated protocol; and a processor unit that is operatively coupled to the display, the input device and the memory unit, the processor unit having at least one processor, the memory unit comprising a non-transient computer-readable storage medium having stored thereon computer-executable instructions for execution by the processor unit to perform the method of updating a protocol for a Virtual Reality (VR) medical test in accordance with the teachings herein.</p><p id="p-0031" num="0030">In another aspect, in accordance with the teachings herein, there is provided a computer readable medium comprising a plurality of instructions that are executable on a processor of a system for adapting the system to implement a method to allow a user to update a protocol for a Virtual Reality (VR) medical test to be performed on a subject via a VR device worn by the subject, wherein the method is defined in accordance with the teachings herein</p><p id="p-0032" num="0031">In another aspect, in accordance with the teachings herein, there is provided a method for generating and displaying a graphical user interface for a Virtual Reality (VR) medical test via a user device having at least one processor, the VR medical test being performed on a subject via a VR device worn by the subject, wherein the method is performed by the at least one processor and the method comprises: displaying a subject viewing area that comprises a view of the subject during testing or during playback of a recorded test; displaying a camera settings area that comprises camera inputs for receiving user input for modifying images that are displayed in the subject viewing area; and displaying a test-specific UI area that comprises test parameters that indicate settings for controlling different aspects of VR visual stimuli that are displayed to the subject via the VR device during testing.</p><p id="p-0033" num="0032">In at least one embodiment, the method comprises showing real-time data about the subject during testing in the subject viewing area.</p><p id="p-0034" num="0033">In at least one embodiment, the subject viewing area comprises an eye camera area and the method comprises displaying a video of one or both eyes along with eye tracker data comprising pupil diameter of the subject in the eye camera area during testing or when playing back stored results of a previous test.</p><p id="p-0035" num="0034">In at least one embodiment, the method comprises displaying a subject VR view that shows what is displayed by the VR device to the subject during testing.</p><p id="p-0036" num="0035">In at least one embodiment, the method comprises: displaying main camera settings in the camera settings area; receiving user input associated with the main camera settings for a desired view; and displaying the desired view in the subject viewing area.</p><p id="p-0037" num="0036">In at least one embodiment, the method comprises displaying the camera settings area to show camera settings sliders that control at least one of eye blurring, eye rotation, color filter, or displacement during VR testing.</p><p id="p-0038" num="0037">In at least one embodiment, the method comprises changing a display of the test inputs shown in the test-specific UI area due to user input from the user device when the user selects a particular VR medical test.</p><p id="p-0039" num="0038">In at least one embodiment, the method comprises using the graphical user interface to display at least one of a Bagolini Striated Lens VR test, a Frisby Stereopsis VR test, a Synoptophore VR test, and a Lees Screen VR test.</p><p id="p-0040" num="0039">In another aspect, in accordance with the teachings herein, there is provided a system for generating and displaying a graphical user interface for a Virtual Reality (VR) medical test to allow a user to perform the VR medical test on a subject via a VR device worn by the subject, wherein the system comprises: a display for showing the graphical user interface; an input device that is adapted to receive user inputs; a memory unit that is adapted to store a protocol data file having values for settings for the VR medical test; and a processor unit that is operatively coupled to the display, the input device, and the memory unit, the processor unit having at least one processor that is configured to perform a method for generating and displaying a graphical user interface for a Virtual Reality (VR) medical test in accordance with the teachings herein.</p><p id="p-0041" num="0040">In another aspect, in accordance with the teachings herein, there is provided a computer readable medium comprising a plurality of instructions that are executable on a processor of a system for adapting the system to implement a method for generating and displaying a graphical user interface for a Virtual Reality (VR) medical that is performed on a subject via a VR device worn by the subject, wherein the method is defined in accordance with the teachings herein.</p><p id="p-0042" num="0041">Other features and advantages of the present application will become apparent from the following detailed description taken together with the accompanying drawings. It should be understood, however, that the detailed description and the specific examples, while indicating preferred embodiments of the application, are given by way of illustration only, since various changes and modifications within the spirit and scope of the application will become apparent to those skilled in the art from this detailed description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0043" num="0042">For a better understanding of the various embodiments described herein, and to show more clearly how these various embodiments may be carried into effect, reference will be made, by way of example, to the accompanying drawings which show at least one example embodiment, and which are now described. The drawings are not intended to limit the scope of the teachings described herein.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a block diagram of an example embodiment of a VR system for generating and providing VR medical tests to a subject.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows a block diagram of an example embodiment of a computing device that can be used with the VR system of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a flow chart of an example embodiment of a method of creating, updating, and conducting VR based medical tests.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a computer screen image of an example embodiment of a VR test creation interface for creating VR based medical tests.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows a letter-coded computer screen image of the VR test creation interface of <figref idref="DRAWINGS">FIG. <b>4</b></figref> for creating VR based medical tests.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> shows an example embodiment of a subject data window.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>6</b>B</figref> shows an example embodiment of a portion of a user interface for a virtual laser pointer adjustment.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example embodiment of an eye camera area.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> shows an example embodiment of a main camera settings area.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> shows a schematic drawing of an example of a virtual laser pointer simultaneously in VR view and on a computer screen, side-by-side.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> shows magnified portions of an example embodiment of a main camera settings area integrated with a camera settings sliders area with magnified portions.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example embodiment of show blur eye settings sliders.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example embodiment of show eye camera rotation settings sliders.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example embodiment of show color filter settings sliders.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows an example embodiment of show displacement settings sliders.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows a letter-coded screen image of an example embodiment of a test settings area.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>14</b>A</figref> shows an example embodiment of a test-specific UI area for a Worth 4 Dot VR test.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>14</b>B</figref> shows a schematic diagram of an example of the Worth 4 Dot VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>15</b>A</figref> shows an example embodiment of a test-specific UI area for a Bagolini Striated Lens VR test.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>15</b>B</figref> shows a schematic diagram of an example of the Bagolini Striated Lens VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>15</b>A</figref>.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> shows an example embodiment of a test-specific UI area for a Double Maddox Rod VR test.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> shows a schematic diagram of an example of the Double Maddox Rod VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>17</b>A</figref> shows an example embodiment of a test-specific UI area for a Contrast Sensitivity and Visual Acuity VR test.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>17</b>B</figref> shows a schematic diagram of an example of the Visual Acuity VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>17</b>A</figref>.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>18</b>A</figref> shows an example embodiment of a test-specific UI area for a Farnsworth D15 VR test.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>18</b>B</figref> shows a schematic diagram of an example view of the Farnsworth D15 VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>18</b>A</figref>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>19</b>A</figref> shows an example embodiment of a test-specific UI area for a Frisby Stereopsis VR test.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>19</b>B</figref> shows a schematic diagram of an example view of the Frisby Stereopsis VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>19</b>C</figref> shows an example of a close-up of a view of the Frisby Stereopsis VR test.</p><p id="p-0073" num="0072"><figref idref="DRAWINGS">FIG. <b>20</b>A</figref> shows an example embodiment of a test-specific UI area for a Synoptophore VR test.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIGS. <b>20</b>B and <b>20</b>C</figref> show schematic diagrams of first and second example representations of a Synoptophore VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIG. <b>21</b>A</figref> shows an example embodiment of a test-specific UI area for a Lees Screen VR test.</p><p id="p-0076" num="0075"><figref idref="DRAWINGS">FIGS. <b>21</b>B and <b>21</b>C</figref> show schematic diagrams of first and second example representations of the Lees Screen VR test corresponding to the UI of <figref idref="DRAWINGS">FIG. <b>21</b>A</figref>.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows a schematic diagram of an example representation of a Brock String VR test.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>23</b></figref> shows a schematic diagram of an example representation of a Visual Field VR test.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows an example embodiment of a test-specific UI area for an art program with a magnified view of a portion thereof.</p><p id="p-0080" num="0079"><figref idref="DRAWINGS">FIG. <b>25</b></figref> shows an example of a confirmation popup window in the art program of <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0081" num="0080"><figref idref="DRAWINGS">FIG. <b>26</b></figref> shows an example embodiment of a VR view of a paint mode controller with a brush menu.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>27</b></figref> shows an example of a VR view of art generated by the art program.</p><p id="p-0083" num="0082"><figref idref="DRAWINGS">FIG. <b>28</b></figref> shows an example embodiment of a test selection area.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>29</b></figref> shows an example of a large version of an icon in the test selection area of <figref idref="DRAWINGS">FIG. <b>28</b></figref>.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>30</b></figref> shows an example embodiment of a test stimulus settings window.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIGS. <b>31</b>A, <b>32</b>A, and <b>33</b>A</figref> show examples of protocol selection &#x26; authoring areas while in simple, advanced, and protocol authoring modes, respectively.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIGS. <b>31</b>B and <b>32</b>B</figref> show computer screen images of examples of an overall screen in simple and advanced protocol modes, respectively.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>33</b>B</figref> shows an example of a prompt to save current/new in protocol authoring mode.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>34</b></figref> shows an example embodiment of a recording user interface.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>35</b></figref> shows an example of a subject data window while recording.</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>36</b></figref> shows an example of a recording paused dialog box.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>37</b></figref> shows an example of a playback file open confirm dialog box.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>38</b></figref> shows a computer screen image of an example embodiment of a playback mode user interface.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIGS. <b>39</b> and <b>40</b></figref> show examples of playback mode control windows in play normally and pause modes, respectively.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>41</b></figref> shows a computer screen image of an example embodiment of a playback in VR view.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>42</b></figref> shows a flow chart of an example embodiment of a method of creating and updating a protocol.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0097" num="0096">Further aspects and features of the example embodiments described herein will appear from the following description taken together with the accompanying drawings.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0098" num="0097">Various embodiments in accordance with the teachings herein will be described below to provide an example of at least one embodiment of the claimed subject matter. No embodiment described herein limits any claimed subject matter. The claimed subject matter is not limited to devices, systems, or methods having all of the features of any one of the devices, systems, or methods described below or to features common to multiple or all of the devices, systems, or methods described herein. It is possible that there may be a device, system, or method described herein that is not an embodiment of any claimed subject matter. Any subject matter that is described herein that is not claimed in this document may be the subject matter of another protective instrument, for example, a continuing patent application, and the applicants, inventors, or owners do not intend to abandon, disclaim, or dedicate to the public any such subject matter by its disclosure in this document.</p><p id="p-0099" num="0098">It will be appreciated that for simplicity and clarity of illustration, where considered appropriate, reference numerals may be repeated among the figures to indicate corresponding or analogous elements. In addition, numerous specific details are set forth in order to provide a thorough understanding of the embodiments described herein. However, it will be understood by those of ordinary skill in the art that the embodiments described herein may be practiced without these specific details. In other instances, well-known methods, procedures, and components have not been described in detail so as not to obscure the embodiments described herein. Also, the description is not to be considered as limiting the scope of the embodiments described herein.</p><p id="p-0100" num="0099">It should also be noted that the terms &#x201c;coupled&#x201d; or &#x201c;coupling&#x201d; as used herein can have several different meanings depending in the context in which these terms are used. For example, the terms coupled or coupling can have a mechanical or electrical connotation. For example, as used herein, the terms coupled or coupling can indicate that two elements or devices can be directly connected to one another or connected to one another through one or more intermediate elements or devices via an electrical signal, radio signal, electrical connection, or a mechanical element depending on the particular context.</p><p id="p-0101" num="0100">It should also be noted that, as used herein, the wording &#x201c;and/or&#x201d; is intended to represent an inclusive-or. That is, &#x201c;X and/or Y&#x201d; is intended to mean X or Y or both, for example. As a further example, &#x201c;X, Y, and/or Z&#x201d; is intended to mean X or Y or Z or any combination thereof.</p><p id="p-0102" num="0101">It should be noted that terms of degree such as &#x201c;substantially&#x201d;, &#x201c;about&#x201d; and &#x201c;approximately&#x201d; as used herein mean a reasonable amount of deviation of the modified term such that the end result is not significantly changed. These terms of degree may also be construed as including a deviation of the modified term, such as by 1%, 2%, 5%, or 10%, for example, if this deviation does not negate the meaning of the term it modifies.</p><p id="p-0103" num="0102">Furthermore, the recitation of numerical ranges by endpoints herein includes all numbers and fractions subsumed within that range (e.g., 1 to 5 includes 1, 1.5, 2, 2.75, 3, 3.90, 4, and 5). It is also to be understood that all numbers and fractions thereof are presumed to be modified by the term &#x201c;about&#x201d; which means a variation of up to a certain amount of the number to which reference is being made if the end result is not significantly changed, such as 1%, 2%, 5%, or 10%, for example.</p><p id="p-0104" num="0103">It should also be noted that the use of the term &#x201c;window&#x201d; in conjunction with describing the operation of any system or method described herein is meant to be understood as describing a user interface for performing initialization, configuration, or other user operations.</p><p id="p-0105" num="0104">The example embodiments of the devices, systems, or methods described in accordance with the teachings herein may be implemented as a combination of hardware and software. For example, the embodiments described herein may be implemented, at least in part, by using one or more computer programs, executing on one or more programmable devices comprising at least one processing element and at least one storage element (i.e., at least one volatile memory element and at least one non-volatile memory element). The hardware may comprise input devices including at least one of a touch screen, a keyboard, a mouse, buttons, keys, sliders, and the like, as well as one or more of a display, a printer, and the like depending on the implementation of the hardware. The terms &#x201c;mouse click&#x201d; or &#x201c;mouse drag&#x201d; for example are understood to be interchangeable with the term &#x201c;touch&#x201d;, meaning tapping a touch screen or dragging a finger across a touch screen, respectively.</p><p id="p-0106" num="0105">It should also be noted that there may be some elements that are used to implement at least part of the embodiments described herein that may be implemented via software that is written in a high-level procedural language such as object-oriented programming. The program code may be written in C++, C#, JavaScript, Python, or any other suitable programming language and may comprise modules or classes, as is known to those skilled in object-oriented programming. Alternatively, or in addition thereto, some of these elements implemented via software may be written in assembly language, machine language, or firmware as needed. In either case, the language may be a compiled or interpreted language.</p><p id="p-0107" num="0106">At least some of these software programs may be stored on a computer readable medium such as, but not limited to, a ROM, a magnetic disk, an optical disc, a USB key, and the like that is readable by a device having a processor, an operating system, and the associated hardware and software that is necessary to implement the functionality of at least one of the embodiments described herein. The software program code, when read by the device, configures the device to operate in a new, specific, and predefined manner (e.g., as a specific-purpose computer) in order to perform at least one of the methods described herein.</p><p id="p-0108" num="0107">At least some of the programs associated with the devices, systems, and methods of the embodiments described herein may be capable of being distributed in a computer program product comprising a computer readable medium that bears computer usable instructions, such as program code, for one or more processing units. The medium may be provided in various forms, including non-transitory forms such as, but not limited to, one or more diskettes, compact disks, tapes, chips, solid state drives, cloud storage, and magnetic and electronic storage. In alternative embodiments, the medium may be transitory in nature such as, but not limited to, wire-line transmissions, satellite transmissions, internet transmissions (e.g., downloads), media, digital and analog signals, and the like. The computer useable instructions may also be in various formats, including compiled and non-compiled code.</p><p id="p-0109" num="0108">By way of background relating to the technical challenges presented, research into the utilization of VR in the domain of medical testing is still in its infancy. The spectrum of clinical tests developed may be utilized differently by different levels of health care professionals, i.e., by primary care professionals (screening tool), secondary eye care providers (triaging tool), and tertiary care services for diagnostic and management purposes (treatment tools). There is also a market for private practices in optometry, who are the foremost interface with the general population, which will benefit from improved medical testing while expanding their scope of practice to include some of the ophthalmic assessment capabilities offered herein.</p><p id="p-0110" num="0109">Current tests are cumbersome, time consuming, and expensive. There is a need for a system that combines multiple tests into one device that is a consumer-grade, readily available, and relatively inexpensive device. For example, current offerings for certain tests (administered using separate equipment) can cost as much as $100,000 per test or more. A system that combines multiple tests into one device not only increases efficiency, but also provides an environment that can enhance or replace multiple tests, or even create entirely new tests that are not currently developed and may not even be possible or feasible outside of the VR world.</p><p id="p-0111" num="0110">While the various teachings herein are described with respect to generating and performing VR-based medical tests including VR-based vision tests, for illustrative purposes, in alternative embodiments, the teachings herein may be used to generate and perform other types of medical tests including, but not limited to, at least one of VR-based mental, emotional, and cognitive functioning tests, for example, related to various conditions such as, but not limited to, autism, Alzheimer's, dementia, and post-traumatic stress disorder (PTSD). Generally, medical tests refer to the displaying and recording of distance, orientation, and visual/audio context data of a subject and video/audio stimulus in a medical context. As such, a medical test does not necessarily mean providing scores, diagnosis, or any kind of algorithmically derived medical assessment. The intention is to provide doctors and clinicians with appropriate expertise with more precise measurements and data so they may be better informed in making assessments, making diagnoses, and prescribing therapies.</p><p id="p-0112" num="0111">With respect to VR-based vision testing, the teachings herein may be used to generate and perform various vision tests including, but not limited to, at least one of a visual acuity test, a depth perception test such as a Brock string test, a color vision test, as well as tests for fovea functioning and binocular vision, for example.</p><p id="p-0113" num="0112">VR-based tests may assist in the measurement of eye function by presenting stimuli that approximate or improve upon the gold standard testing currently in use throughout ophthalmology and optometry. The various VR-based vision tests described herein may provide information related to, for example, at least one of visual acuity, eye function, eye and head movement, visual field, eye alignment, color detection, and pupillary response.</p><p id="p-0114" num="0113">In accordance with the teachings herein, the example embodiments that are described provide technical solutions to the challenges of implementing VR versions of physical tests that are as good as, if not better than, the Gold standard. The VR versions may, for example, improve on the Gold standard in at least one of the following areas depending on the particular test: better precision, better consistency, unlimited stimulus shape/size, stimulus being configurable to be displayed to either eye without the need for at least one of lens polarization or color filtering; and eye movement, head movement, hand movement, vocal sounds, and/or biometric data capable of being recorded with the visual stimulus context preserved in full 3D for later analysis.</p><p id="p-0115" num="0114">As it happens, visual systems function the same way in VR as they do in real life, so it is therefore possible to do many of the conventional eye tests conducted in an ophthalmological setting while benefiting from the precision and reproducibility of performing these tests in a simulated VR environment. This may greatly speed up setup and testing time, and help ensure less qualified individuals can meaningfully contribute to gathering data, helping to free up valuable time of the experts and/or the devices they use.</p><p id="p-0116" num="0115">In order to make the testing experience more precise, VR tests can use protocols, in accordance with the teachings herein, to ensure test settings are consistent from subject to subject (i.e., patients), and from appointment to appointment for the same subject. This may include setting specific values for at least one of the scale of the visual stimuli, the distance of the visual stimuli from the subject's eyes, the rotation of the visual stimulus, the lighting (e.g., light intensity) of the visual stimuli, and other context settings, for example. All of these settings can be perfectly consistent across different tests since VR is used to deliver the visual tests.</p><p id="p-0117" num="0116">In accordance with the teachings herein, VR tests may be generated and delivered using conventional VR hardware to deliver one or more VR vision tests to a subject and record the subject performance/behavior. However, in alternative embodiments, the VR vision tests may be augmented to be provided, along with ophthalmological apparatus, to a subject in order to obtain certain data about the subject's vision or data for correcting the subject's vision such as visual refractive corrections, which may be obtained when VR stimulus is combined with a phoropter equipped with, for example, deformable lenses. VR vision tests may be generated for providing Fundas images or performing visual field tests when the VR system is equipped with an appropriate ophthalmological device such as a device having a visual field greater than 110 degrees, for example. Accordingly, as hardware devices evolve, an increasing number of VR vision tests can be created using the teachings herein.</p><p id="p-0118" num="0117">In one aspect, in accordance with the teachings herein, there are provided various embodiments for a system and method for a protocol authoring tool to allow a user to create and/or modify VR based medical tests and record data from subjects who undergo the tests, as well as computer products for use therewith.</p><p id="p-0119" num="0118">In another aspect, in accordance with the teachings herein, there is at least one embodiment of a system and method for generating and administering the VR version of currently employed clinical tests on a single head-mounted VR viewing device (these tests can be referred to as VR clinical tests). The VR clinical tests that can be designed range from vision tests performed on a standard vision chart to vision tests that use eye tracking technology to detect and quantify disorders of eye movements while simultaneously confirming secondary cortical adaptations from these disturbances. In the latter case, in at least one embodiment, an Eye Tracking Sensor Device (ETS) can be embedded in a VR headset to present innovative stimuli attractive to children and monitor their responses via the ETS rather than traditional tests that include receiving verbal answers. As another example, in at least one embodiment, the VR headset may be used to perform a quasi-simultaneous assessment of the subject's two eyes by using the VR headset to present dichoptic stimuli to the subject.</p><p id="p-0120" num="0119">In another aspect, in at least one embodiment described herein, there is provided a method of recording the actual physical movement of a subject while the subject is being shown visual stimulus that is controlled by a user, presenting visual stimulus for other vision tests to the subject using pre-programmable protocols, recording data from the subject during each of these VR tests and then playing back the recorded data such that subject movement can be viewed by the user in VR on 2D monitors via an avatar representing the subject in VR. The method is implemented, for example, on one or more computers and/or electronic devices.</p><p id="p-0121" num="0120">In another aspect, in at least one embodiment described herein, there is provided a system and method which allows a user to organize visual stimuli including manipulating existing visual stimuli, which may then be presented to the subject while data from the subject is recorded and which may be played back later as described previously. In at least one embodiment, the stimulus can be automatically set up with one click. Automatic setup may be achieved with a preset, and possibly modifiable, protocol. For example, at least one embodiment of the system described herein provides means to set up and save a protocol for later access, and possibly save groups of protocols to comprise a given study. Protocols may be shared and used across systems to ensure consistent testing stimulus for every subject regardless of location.</p><p id="p-0122" num="0121">In another aspect, in at least one embodiment described herein, there is provided a system and method that enables the user to use mouse movement, or another control input, to control a stimulus that is displayed concurrently to the subject in VR. The user can then direct the subject using the control input. The subject's actions may be either self-directed or instructed by the user (e.g., clinician, ophthalmologist, or tester). For example, the subject may regard a particular provided VR visual stimulus and answer questions from the user on how it appears. The interaction between the user and subject (e.g., their questions and answers) may be recorded into (e.g., audio) data for later playback, for example, if the user has input that they have permission to do so. Alternatively, or in addition, speech recognition telemetry may be used to record a confidence score that the user responded with one of a few choices to capture relevant data (e.g., whether the subject identified the correct letters on an eye chart), which may be modified (e.g., masked, scrubbed, encrypted) to protecting identity.</p><p id="p-0123" num="0122">In another aspect, in at least one embodiment described herein, there is provided a system and method for recording the subject's movements while they are undergoing a test in VR including movement of at least one of their limbs, body, head, eyes, and pupil size. For example, the system may record biometric data by using appropriate sensor hardware and then digitally represent it in 3D virtual playback for later analysis. The recorded subject movements can then be played back to the user in the real world using a computer display, or played back to the user in VR.</p><p id="p-0124" num="0123">In another aspect, in at least one embodiment described herein, there is provided a system and method for generating and presenting visual stimuli as specific VR vision tests in which a VR vision stimulus is provided to the subject that will have the same effect on the vision of the subject as if the stimulus was provided to the subject in the real world. Data from the subject is recorded while the VR vision stimulus is being presented, and the recorded subject data may be useful in the assessment of various vision disorders depending on how the VR vision stimulus was defined.</p><p id="p-0125" num="0124">In another aspect, in at least one embodiment described herein, there is provided a system and method that allows a user to filter a VR visual stimulus so that the filtered VR visual stimulus has specific light wavelengths (i.e., specific types of color) at varying light intensities. Specific simulated light wavelengths may also be provided, as RGB displays cannot output light wavelengths; rather, they output combinations of red, green, and blue to simulate as many light wavelengths as possible. The filtered VR stimulus is presented to the subject in VR while data is recorded from the subject where the recorded subject data can be analyzed to determine the subject's color sensitivity.</p><p id="p-0126" num="0125">In another aspect, in at least one embodiment described herein, there is provided a system and method that allows a user to alter the rotation and position of a VR vision stimulus that is presented to each of the subject's individual eyes while the subject is in VR. Subject data is recorded simultaneously to the presentation of the VR stimulus. The recorded subject data can provide data relating to the subject's eye alignment and rotation.</p><p id="p-0127" num="0126">In another aspect, in at least one embodiment described herein, there is provided a system and method for representing and recording each axis of rotation of the subject's head relative to the direction of a VR visual stimulus while the subject is in VR. The various axis of rotation of the subject's head can then be played back to the user either in the real world or in VR.</p><p id="p-0128" num="0127">In another aspect, in at least one embodiment described herein, there is provided a system and method for accessing previously recorded subject data where this subject data may include data about one or more of: (a) the movement of at least one of the subject's head, hands, eyes, and fingers; and (b) the subject's interactions with VR-based controllers and test stimuli. The accessed subject data can then be displayed to another person, e.g., the user, a clinician, or a doctor, while this person is also in the same VR environment as the subject was when the subject data was recorded. For example, the system may comprise VR hardware that supports tracking several joints/body parts, or only the head, or head and hands, etc. of the subject. The software driving the hardware may be configured to communicate with whatever capabilities for tracking the hardware has and record them for later playback in VR. Support for new types of tracking and other biometric capture may be added as needed.</p><p id="p-0129" num="0128">At least some of the programs associated with the devices, systems, and methods of the embodiments described herein apply principles and/or operations relating to 3D manipulation of images. For example, the programs may incorporate six degrees of freedom (also known as &#x201c;6DOF&#x201d;) for the purposes of translation and rotation of a VR visual stimulus along or around the x-axis, y-axis, and z-axis. &#x201c;Translation&#x201d; may be used to represent movement in 3D space. Rotation may refer to pitch, yaw, and roll, for example when applied to camera rotation. Rotation may be represented by x,y,z rotation. Prismatic rotational translation effects may be measured in diopters, A, or torsional rotation, for example as used in eye movement measured in degrees. Other terms of art may also be used in the context of vision testing, film/camera work, VR, or digital 3D modelling and design, where applicable.</p><p id="p-0130" num="0129">Reference is first made to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, showing a block diagram of an example embodiment of a VR system <b>100</b> that can be used to generate VR tests and present VR stimuli to the subject for performing medical testing. The VR system <b>100</b> may also be used to record data from the subject when a VR stimulus is presented to the subject and perform an action on the subject data including at least one of storing the recorded subject data, presenting the recorded subject data to a user in VR or in the real world and sending the subject data to another device over a wired or wireless communication network. The VR medical testing (e.g., a VR visual test) may be controlled by a user (e.g., a clinician, or an operator).</p><p id="p-0131" num="0130">The VR system <b>100</b> comprises a head unit <b>112</b> that includes a left display <b>114</b> and a right display <b>116</b> that are configured to provide a left VR view and a right VR view to the subject's left eye and right eye, respectively, when the head unit <b>112</b> is worn by the subject. The left display <b>114</b> and the right display <b>116</b> display VR images that provide the subject with the impression that they are inside a realistic VR environment that visually follows the same rules of perspective and vision as in the real world. Examples of such rules include, but are not limited to, further away objects appearing smaller, closer objects occluding further objects, and perspective distorting shapes of objects depending on their relative spatial relationship. The left display <b>114</b> and the right display <b>116</b> can be virtually rotated or moved relative to each of the subject's left eye and right eye, respectively. Color and other types of image distortion filters may also be applied individually or in combination prior to displaying the visual stimuli on at least one of the left display <b>114</b> and the right display <b>116</b>.</p><p id="p-0132" num="0131">The head unit <b>112</b> may also include a left eye tracking sensor <b>124</b> disposed adjacent to the left display <b>114</b> and a right eye tracking sensor <b>126</b> disposed adjacent to the right display <b>116</b>. Each or both of the left eye tracking sensor <b>124</b> and the right eye tracking sensor <b>126</b> may be referred to as an eye tracker. The left eye tracking sensor <b>124</b> and the right eye tracking sensor <b>126</b> may generate eye tracking data of the subject's left eye and right eye, respectively. The eye tracking data can provide data specific to the subject's eyes which can be analyzed by a user device, such as a computer <b>140</b>. Examples of such eye tracking data include, but are not limited to, eye position, pupil size, or pupil shape.</p><p id="p-0133" num="0132">The computer <b>140</b> (e.g., a central computing unit) is configured to render the VR environment and send the VR data associated with the correct VR views to the left display <b>114</b> and the right display <b>116</b>. The computer <b>140</b> can also receive eye tracking data from the left eye tracking sensor <b>124</b> and right eye tracking sensor <b>126</b> to be stored by the computer <b>140</b> or sent to another user device. Additionally, the computer <b>140</b> receives position and rotation data from a head unit <b>112</b>, hand unit(s) <b>118</b>, and miscellaneous unit(s) <b>120</b> for additional processing and simulation purposes. The computer <b>140</b> utilizes an operating system and support software to provide the appropriate functions.</p><p id="p-0134" num="0133">The head unit <b>112</b> may contain sensors to collect position data and rotation data of the subject's head which can be sent to the computer <b>140</b>. The hand units <b>118</b> contain sensors to collect position and rotation data of the subject's hands or arms which can be sent to the computer <b>140</b>. Additionally, the hand units <b>118</b> (e.g., VR controllers or VR wands) can be used by the subject to provide input data that can be sent to the computer <b>140</b>, including, but not limited to, button presses, button clicks, trackpad tracking, and joystick tracking, for example, so that the user can perform certain functions in the VR environment. The miscellaneous units <b>120</b> may collect and send position and rotation data for other areas of the subject's body. Alternatively, or in addition, the miscellaneous units <b>120</b> may be implemented using sensors that can collect biometric data from the subject which include, but are not limited to, at least one of rate of breath, heart rate, galvanic skin response, electroencephalogram (EEG) data, and electrocardiogram (ECG) data. Accordingly, the miscellaneous units <b>120</b> may be one or more trackers and/or one or more physiological sensors. The miscellaneous units <b>120</b> may be controlled by a sensor control that may be accessed through software or hardware, or a combination thereof. The head unit <b>112</b>, hand units <b>118</b>, and miscellaneous units <b>120</b> may be connected to the computer <b>140</b>, for example, by physical cable, or by a wireless signals transmitted according to a wireless communication protocol, or via a wired or wireless local area network.</p><p id="p-0135" num="0134">A display <b>146</b> is connected to the computer <b>140</b> for use by the user. The user may provide inputs on various input/output (I/O) hardware <b>150</b> which may include, but are not limited to, a computer keyboard or a touchscreen display for input of commands and text entries. Other I/O hardware <b>150</b> the user may use includes a mouse for input from the user to interact with the software user interface and control certain aspects of the VR visual stimuli.</p><p id="p-0136" num="0135">In at least one embodiment, the system <b>100</b> is capable of simulating a virtual 3D world with VR images that are updated at an appropriate rate, such as at least 90 times per second, relative to the subject's head position and eyes. The computer <b>140</b> may, for example, run Windows or a similar supported operating system and be computationally powerful enough to update the volumetric scene at least <b>90</b> times per second. The left display <b>114</b> and the right display <b>116</b> may be implemented using tiny displays along with lenses that focus an image into each of the subject's eyes so as to simulate stereopsis and enable the subject to adjust interpupillary distance as required between their eyes and the displays based on the subject's particular ocular and facial physiology. In addition, the lenses may be implemented such that they are moveable so that the user can ensure as much proper visual focus as possible for the subject.</p><p id="p-0137" num="0136">In at least one embodiment, the system <b>100</b> may be compatible with one or more of OpenVR, OSVR, and OpenXR standards. The system <b>100</b> may support SteamVR. The system <b>100</b> may be run on an HTC VIVE&#x2122; or HTC VIVE&#x2122; Pro. The system <b>100</b> may use or be compatible with Oculus, Pico Neo, or Windows-based VR headsets and equipment. The system <b>100</b> may also use eye tracking software, such as Tobii VR or Pupil Labs VR eye trackers.</p><p id="p-0138" num="0137">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, shown therein is a block diagram of an example embodiment of the computer <b>140</b> that can be used with the VR system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The computer <b>140</b> may run on a single computing device (e.g., desktop, laptop, notepad), and includes a processor unit <b>144</b>, the display <b>146</b>, an interface unit <b>148</b>, the input/output (I/O) hardware <b>150</b>, a communication unit <b>152</b>, a power unit <b>154</b>, and a memory unit (also referred to as &#x201c;data store&#x201d;) <b>156</b>. In other embodiments, the computer <b>140</b> may have more or less components but generally function in a similar manner. For example, the computer <b>140</b> may be implemented using more than one computing device. The computer <b>140</b> may function as a server.</p><p id="p-0139" num="0138">The processor unit <b>144</b> may include one processor. Alternatively, there may be a plurality of processors that are used by the processor unit <b>144</b>, and these processors may function in parallel and perform certain functions. The display <b>146</b> may be, but not limited to, a computer monitor or an LCD display such as that for a tablet device or a desktop computer. The communication unit <b>152</b> includes various communication hardware for allowing the processor unit <b>144</b> to communicate with other devices. The communication hardware includes at least one of a network adapter, such as an Ethernet or 802.11x adapter, a BlueTooth radio or other short range communication device, and a wireless transceiver for wireless communication.</p><p id="p-0140" num="0139">The memory unit <b>156</b> stores program instructions for an operating system <b>158</b>, a VR test application <b>160</b>, a graphical user interface (GUI) module <b>162</b>, a VR engine <b>164</b>, an input/output (I/O) module <b>166</b>, and one or more data files <b>168</b>. The VR test application <b>160</b> comprises software instructions that, when executed, configures the processor unit <b>144</b> to operate in a particular manner to implement various functions, tools, processes, and methods for the system <b>100</b>. For example, the VR test application <b>160</b> can include programs for instructing the various hardware trackers to record and transmit movement data for at least one of the subject's body parts including one or more of their eyes. Accordingly, the functionality of the VR test application <b>160</b> may be implemented, for example, using a combination of hardware, firmware, and/or software.</p><p id="p-0141" num="0140">The graphical user interface module <b>162</b> stores, uploads, creates, generates, modifies, and/or outputs GUI elements (or GUI building blocks). The GUI elements may serve as the building blocks for a VR test creation interface. The GUI elements may be manipulated by a user to control the operation of the head unit <b>112</b> during VR medical testing. When the GUI elements are displayed, the GUI elements may be associated with a protocol on a user device. The GUI elements may have user adjustable settings for modifying the functioning of a VR medical test (e.g., a VR visual test). The GUI module <b>162</b> may, for example, operate in conjunction with the VR engine <b>164</b> to associate 2D inputs viewed on the display <b>146</b> with 3D outputs in the head unit <b>112</b>. Accordingly, the functionality of the GUI module <b>162</b> may be implemented, for example, using a combination of hardware, firmware, and/or software.</p><p id="p-0142" num="0141">The VR engine <b>164</b> generates the VR environment and outputs an image of the VR environment to the head unit <b>112</b>, such as to the left display <b>114</b> and the right display <b>116</b>. The VR engine <b>164</b> may update a virtual action and display the virtual action on the head unit <b>112</b>. For example, input from the head unit <b>112</b> or the hand unit <b>118</b> may be received by the I/O hardware <b>150</b>, processed by the I/O module <b>166</b>, and/or processed by the VR engine <b>164</b>. The VR engine <b>164</b> may then provide any necessary data (e.g., positional, graphical) to the head unit <b>112</b> to update any visual stimuli that is provided to the head unit <b>112</b>. Accordingly, the functionality of the VR engine <b>164</b> may be implemented, for example, using a combination of hardware, firmware, and/or software.</p><p id="p-0143" num="0142">The VR engine <b>164</b> may generate the images, video, and/or stimuli to be displayed in the head unit <b>112</b> using, for example, the Steam VR engine, Unity VR engine, OpenVR API, Oculus VR Plugin, PicoVR, or Unreal engine. The Unity VR engine may include a virtual reality tool kit (VRTK) plugin. The VR engine <b>164</b> may use sensor data that represent the subject's head position and orientation, for example, to update the images that are shown to the subject in the subject's field of view (FOV).</p><p id="p-0144" num="0143">The input/output module <b>166</b> receives input data that was obtained by the I/O hardware <b>150</b>, processes the input data, and/or generates output data (or signals) that are then sent to the I/O hardware <b>150</b>. The input/output module may, for example, operate in conjunction with the VR engine <b>164</b> to communicate data (or signals) between one or more of the computer <b>140</b>, the head unit <b>112</b>, the hand unit <b>118</b>, and miscellaneous units <b>120</b>. Accordingly, the functionality of the input/output module <b>166</b> may be implemented, for example, using a combination of hardware, firmware, and/or software.</p><p id="p-0145" num="0144">The data files <b>168</b> may store any temporary data (e.g., data that is not needed after a VR test is completed) or permanent data (e.g., data saved for later use), such as subject data (e.g., a subject ID), camera settings, screenshots, and recordings. The data files <b>168</b> may, for example, be generated by data processed by one or more of the VR test application <b>160</b>, the graphical user interface module <b>162</b>, the VR engine <b>164</b>, and the input/output module <b>166</b>. The data files <b>168</b> may also include various subject data for each subject that undergoes VR medical testing such as identification data, physiological data and recorded subject data during VR medical testing.</p><p id="p-0146" num="0145">In at least one embodiment, the computer <b>140</b> is compatible with, configured to operate under, or built to OpenVR standards (or functionally equivalent standards). In at least one embodiment, the computer <b>140</b> comprises components (e.g., operating system <b>158</b>, processor unit <b>144</b>, I/O hardware <b>150</b>) capable of supporting 3D authoring software and VR software to OpenVR standards.</p><p id="p-0147" num="0146">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, shown therein is a flow chart of an example embodiment of a method <b>300</b> of creating, updating, and conducting VR-based medical tests. Method <b>300</b> may be carried out, for example, by some or all of system <b>100</b>. Method <b>300</b> may be implemented, for example, on some or all of computer <b>140</b>.</p><p id="p-0148" num="0147">At <b>310</b>, the computer <b>140</b> starts the VR test application <b>160</b>.</p><p id="p-0149" num="0148">At <b>320</b>, the computer <b>140</b> initializes all required parameters for the VR test application <b>160</b> and for any other modules (e.g., GUI module <b>162</b>, VR engine <b>164</b>, input/output module <b>166</b>) as needed.</p><p id="p-0150" num="0149">At <b>330</b>, the computer <b>140</b> receives user input indicating which mode to operate under, such as authoring a protocol, conducting a VR test, or recording subject data.</p><p id="p-0151" num="0150">At <b>340</b>, the computer <b>140</b> displays a user interface according to the mode that was selected by the user. For example, if authoring a protocol was selected, the computer <b>140</b> displays a GUI for the protocol authoring mode (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>33</b>A</figref>). For example, if conducting a VR test was selected, the computer <b>140</b> displays a VR test (e.g., as shown in any one of <figref idref="DRAWINGS">FIGS. <b>14</b>A to <b>22</b></figref>). For example, if recording subject data was selected, the computer <b>140</b> displays a recording user interface (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref>).</p><p id="p-0152" num="0151">At <b>350</b>, the computer <b>140</b> receives and processes user input specific to the mode that the user selected. For example, if the &#x201c;authoring a protocol&#x201d; mode was selected by the user, the computer <b>140</b> carries out a method of creating and updating a protocol (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>42</b></figref>). As another example, if the &#x201c;conducting a VR test&#x201d; mode was selected by the user, the computer <b>140</b> allows the user to interact with the subject in VR through the head unit <b>112</b> by generating and presenting stimuli for a selected VR test (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). As another example, if the &#x201c;recording subject data&#x201d; mode was selected by the user, the computer <b>140</b> allows the user to record subject data while conducting a VR test (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>34</b></figref>).</p><p id="p-0153" num="0152">At <b>360</b>, the computer <b>140</b> stores data as required based on the mode the user selected, the data generated while in that mode, and the data (e.g., settings, recordings) that the user selected to save. Each of these modes are described in further detail below.</p><p id="p-0154" num="0153">Referring now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, shown therein is a computer screen image of an example embodiment of a VR test creation interface <b>400</b> for creating VR-based medical tests. Some or all of the VR test creation interface <b>400</b> may be implemented, for example, on some or all of computer <b>140</b>.</p><p id="p-0155" num="0154">The VR test creation interface <b>400</b> may be used, for example, to provide an integrated development environment (IDE) for a user (e.g., a clinician) or a developer (e.g., a person developing a medical test) that provides the development tools to create and/or modify a medical test, such as a VR vision test (e.g., to test binocular vision).</p><p id="p-0156" num="0155">For the purposes of describing the VR test creation interface <b>400</b>, the following terms are defined as follows:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0156">Subject: person currently viewing a medical test in VR via a VR headset; the person may be subjected to a VR based stimulus; the person may interact with VR controllers.</li>    <li id="ul0001-0002" num="0157">Subject ID: a unique way to identify a given subject, which may be configured to reveal no personally identifiable data.</li>    <li id="ul0001-0003" num="0158">User: a person (e.g., clinician) administering the test who may be trained in how to administer the VR-based medical test and interpret verbal patient responses for proper diagnosis; the user may see a version of what the subject is experiencing on a computer plus an overlaid user interface that can be accessed to alter the subject's VR experience.</li>    <li id="ul0001-0004" num="0159">Developer: a person who creates or modifies the VR-based medical test (the user and developer may be the same person).</li>    <li id="ul0001-0005" num="0160">VR space: virtual 3D space that the subject is visually immersed in through the VR technology.</li>    <li id="ul0001-0006" num="0161">Subject HUD: a heads-up display or 2D overlay that appears in front of at least one the subject's eyes but not in the VR space per se.</li>    <li id="ul0001-0007" num="0162">3D authoring a software-development environment for creating 3D or VR</li>    <li id="ul0001-0008" num="0163">software: projects (e.g., simulations, games), such as Unity 3D.</li>    <li id="ul0001-0009" num="0164">Prefab: a code module that is compatible with 3D authoring software projects and can run inside them, and interoperate with code written within guidelines specified by the 3D authoring software.</li>    <li id="ul0001-0010" num="0165">VR headset: includes at least one display inside a headset worn by the subject that simulates VR and may be used to precisely track the subject's eye movement.</li>    <li id="ul0001-0011" num="0166">Gold standard a test that has been proven over time and with multiple peer</li>    <li id="ul0001-0012" num="0167">test: reviewed studies to be effective to deliver measurements within a specified tolerance of error.</li></ul></p><p id="p-0157" num="0168">In the VR test creation interface <b>400</b>, there is a plurality of GUI elements including, but not limited to, one or more of at least one button, at least one slider, at least one icon, at least one control input, at least one indicator, at least one toggle, and at least one text box. In this example embodiment, the GUI elements may include:<ul id="ul0002" list-style="none">    <li id="ul0002-0001" num="0000">    <ul id="ul0003" list-style="none">        <li id="ul0003-0001" num="0169">A. Subject-related single state buttons <b>412</b>;</li>        <li id="ul0003-0002" num="0170">B. General buttons <b>414</b> (e.g., in grey);</li>        <li id="ul0003-0003" num="0171">C. First toggle buttons <b>416</b> with an icon in the center that indicate the ON state which may have a colored (e.g., gold) frame around them;</li>        <li id="ul0003-0004" num="0172">D. Second toggle buttons <b>418</b> without an icon that indicate the ON state with a dot in the center (e.g., colored or white depending on the function);</li>        <li id="ul0003-0005" num="0173">E. Third toggle buttons (not shown) that are not a regular button shape such as the goggle toggles in a Worth Four Dot (WFD) test, which may use a gradient glow to indicate &#x201c;on&#x201d; status;</li>        <li id="ul0003-0006" num="0174">F. Sliders <b>422</b> with detents and, where applicable, the numerical representation of their value in a user-friendly unit;</li>        <li id="ul0003-0007" num="0175">G. A first icon <b>424</b> showing the type of value a slider will affect, which may appear next to the slider, e.g. a distance slider; and</li>        <li id="ul0003-0008" num="0176">H. A VR controller icon <b>426</b>, when VR is associated with or required to provide a certain type of functionality or input by the subject during a VR vision test.</li>    </ul>    </li></ul></p><p id="p-0158" num="0177">A background menu <b>428</b> may be used to generate a background for the VR test creation interface <b>400</b>. For example, the background menu <b>428</b> may be used to select a background that may be dark grey with some texture. The overall interface may be dark and reversed text. This can help ensure it does not detract from the view in subject VR view, or add any unnecessary light to the room. For example, if a room is very bright, a white VR visual stimulus will be visible when an appropriate VR background is used. If a room is very dark, a dark VR visual stimulus will be visible when an appropriate VR background is used. Furthermore, adjusting room color based on the current VR visual stimulus can be useful to establish proper contrast.</p><p id="p-0159" num="0178">In at least one embodiment, the VR test creation interface <b>400</b> can provide a user interface that is configured to send and receive data to and from 3D authoring software, which may be system <b>100</b> or an external program that is compatible with system <b>100</b>. Alternatively, or in addition, the VR test creation interface <b>400</b> can include 3D authoring software functionality. Alternatively, or in addition, the VR test creation interface <b>400</b> can include an editor in which the functionality of the GUI elements can be programmed, such as a protocol authoring tool.</p><p id="p-0160" num="0179">In at least one embodiment, the VR test creation interface <b>400</b> can run as a standalone application. Alternatively, or in addition thereto, in at least one embodiment the VR test creation interface <b>400</b> can run on and be compatible with an available VR system. Alternatively, or in addition thereto, in at least one embodiment the VR test creation interface <b>400</b> can be compatible with eye tracking functionality of a VR system, if available and running. Alternatively, or in addition thereto, in at least one embodiment the VR test creation interface <b>400</b> can be generated without the ability to use eye tracking functionality. Alternatively, or in addition thereto, in at least one embodiment the VR test creation interface <b>400</b> can be used to enable VR data to be sent to a VR headset display in order to display VR environments reflective of the related settings determined in a main client window display such as display <b>146</b>.</p><p id="p-0161" num="0180">Referring now to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, shown therein is a letter-coded computer screen image of the VR test creation interface <b>400</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> for creating VR-based medical tests. For ease of reference, the major sections of the VR test creation interface <b>400</b> can be divided as follows:<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0000">    <ul id="ul0005" list-style="none">        <li id="ul0005-0001" num="0181">A. Subject data area <b>510</b>. This area can be used to obtain real-time data about the subject as measured by a VR headset.</li>        <li id="ul0005-0002" num="0182">B. Subject VR view <b>520</b>. This area approximately shows what the subject is seeing in VR (with limited field of view depending on the VR system and hardware display in use).</li>        <li id="ul0005-0003" num="0183">C. Eye camera area <b>530</b>. These windows display eye videos of the subject's eyes that are recorded by cameras in the head unit <b>112</b> when such cameras are available.</li>        <li id="ul0005-0004" num="0184">D. Main camera settings area <b>540</b>. These input buttons and toggles control the subject's VR view as well as which settings in the camera settings slider areas (H) are currently visible.</li>        <li id="ul0005-0005" num="0185">E. Test-specific UI area <b>550</b>. These input controls change depending on which vision test is currently selected for testing a subject or modifying a test protocol during a protocol authoring process.</li>        <li id="ul0005-0006" num="0186">F. Test selection area <b>560</b>. These input buttons allow the selection of a specific test and highlight which test is active.</li>        <li id="ul0005-0007" num="0187">G. Protocol selection &#x26; authoring area <b>570</b>. This area includes several input buttons that allow the user to create, read, update, and delete protocols.</li>        <li id="ul0005-0008" num="0188">H. Camera settings sliders area <b>580</b>. This is where, depending on which camera setting is chosen in the main camera settings area (D), a specific set of sliders (for each eye) can be accessed and altered. For example, the set of sliders may include sliders for rotating the eye cameras, sliders for displacing the eye cameras, sliders for blurring the eye cameras, sliders for applying a color filter to the cameras, sliders for turning one eye or both eye video windows on or off, sliders for turning the virtual laser pointer on or off, and/or sliders to swap eye on/off left to right and back.</li>    </ul>    </li></ul></p><p id="p-0162" num="0189">For ease of reference, sections D to H may be referred to as the bottom user interface (UI) when they are located at the bottom of the VR test creation interface <b>400</b>. Furthermore, for ease of reference, sections E and F may be referred to collectively as the test settings area and one or more of sections A to H may be referred to as the user UI.</p><p id="p-0163" num="0190">In at least one embodiment, some or all of the user UI may be optionally displayable on mobile/tablet based devices, and the subject VR experience may be displayable on mobile VR /wireless devices (e.g., when they are on the same network, which can be a Local Area Network or a long-range wireless network). Therefore, in some embodiments, a subject's VR devices (i.e., VR headset, VR controller and an input device like a keyboard or touchscreen) may be connected to a clinician's computing device across the wireless network, for example through the Internet, to allow for remote VR vision testing to be performed on the subject.</p><p id="p-0164" num="0191">In at least one embodiment, VR test creation interface <b>400</b> comprises one or more of the major sections A to H. Alternatively, or in addition, VR test creation interface <b>400</b> can be implemented such that one or more of the major sections A to H is hidden or disabled. Hiding or disabling major sections may be useful in certain contexts. For example, the user may want to prevent altering the eye position sliders by accident while running the subject through a series of protocols. Also, the user may be primarily concerned with viewing the VR environment in the GUI and want to maximize the view.</p><p id="p-0165" num="0192">In at least one embodiment, the GUI module <b>162</b> has program code for generating a subject initialize UI that is presented on the display <b>146</b> of the computer <b>140</b> to the user. On launch of the VR test application <b>160</b>, the subject initialize UI provides a message to the user to prompt the user to enter a subject ID and has a text box for receiving the subject ID. If the subject ID (e.g., a folder name in the data files <b>168</b>) exists, then camera settings, screenshots, recordings, and any other subject-specific data may be stored in the data files <b>168</b>. If the subject ID does not exist, then a new folder and default files are created and saved in the data files <b>168</b>. Input of the subject ID may be programmatically forced to conform with a subject ID naming convention (e.g., VA-001-###) as well as file folder naming standards enforced by Microsoft Windows.</p><p id="p-0166" num="0193">Referring now to <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, shown therein is an example of the subject data area <b>510</b> of the VR test creation interface <b>400</b>. In the subject data area <b>510</b> (e.g., at the top), there may be one or more of: a screenshot button <b>612</b>, a file window open button <b>614</b>, and a record scene button <b>616</b>.</p><p id="p-0167" num="0194">The screenshot button <b>612</b>, when selected by the user, may render a complete edge-to-edge image on the display <b>146</b> and immediately save the file in a .png format inside the current subject data folder, with a filename indicating the current test name (e.g., Worth 4 Dot test), date, and time. Simultaneously a file containing all UI settings when the screenshot was taken may be created as an &#x201c;.ev&#x201d; file in the same folder with the same name (except for the file extension).</p><p id="p-0168" num="0195">The file window open button <b>614</b>, when selected by the user, may launch the current subject folder in the operating system UI so the user can access files and or verify the existence of the files. There may be a preferences file for a given subject that contains at least one of the following information: displacement settings (see <figref idref="DRAWINGS">FIG. <b>12</b></figref>), color filter settings (see <figref idref="DRAWINGS">FIG. <b>11</b></figref>), blur settings (see <figref idref="DRAWINGS">FIG. <b>9</b></figref>), rotation settings (see <figref idref="DRAWINGS">FIG. <b>10</b></figref>), and subject saved InterPupillary Distance (IPD). These settings may be updated and then saved every time they are changed.</p><p id="p-0169" num="0196">The record scene button <b>616</b>, when selected, starts a recording of the subject. The recordings may contain, for example, for every frame (e.g., up to 60 or 100 per second), one or more of the following data: head/controllers position and rotation, user interface settings, eye telemetry data, and eye video. The head/controllers position and rotation may be obtained from sensors in the head unit <b>112</b> and the hand unit <b>118</b>. The user interface settings may be the settings saved by the user during operation of the VR test application <b>160</b>. The eye telemetry data and eye video may be obtained from the left eye tracking sensor <b>124</b> and the right eye tracking sensor <b>126</b>.</p><p id="p-0170" num="0197">In the subject data area <b>510</b> (e.g., in the middle), there may be an X real-time head rotation indicator <b>622</b>, a Y real-time head rotation indicator <b>624</b>, and a Z real-time head rotation indicator <b>626</b>, which indicate the position/movement of the subject's head in the X, Y and Z axes. If the user selects one of the indicators <b>622</b>, <b>624</b>, and <b>626</b> when tracking is performed on the subject, this will pause the tracking functionality (e.g., for the particular axis selected). Likewise, if tracking is paused for a particular axis, the user can select one of the indicators <b>622</b>, <b>624</b>, and <b>626</b> to continue tracking in that particular axis. Crosshairs may be shown in one of the indicators <b>622</b>, <b>624</b>, and <b>626</b> for reference (e.g., to a central position of the head). A bar, which may have a color, such as red for example, may be displayed within at least one of the indicators <b>622</b>, <b>624</b>, and <b>626</b> to show the relative position of a visual stimulus compared to the orientation of the subject's head during vision testing. In other embodiments, another object may be used instead of a bar to show the relative position of the visual stimulus.</p><p id="p-0171" num="0198">Associated with each of the real-time head rotation indicators (e.g., below them), there may be corresponding toggle indicators: an X toggle indicator <b>632</b>, a Y toggle indicator <b>634</b>, and a Z toggle indicator <b>636</b> that can be used to obtain a measurement along one of the axes during testing. For example, three &#x201c;camera heads&#x201d; icons may be displayed in the subject data area <b>510</b>, indicating that real-time rotation in the x, y, and z axis are being displayed and tracked, along with cross hairs and text displaying the degrees of rotation in the corresponding axis. The toggles <b>632</b>, <b>634</b>, and <b>636</b> indicate the axis for which each degree corresponds, and when any of the toggles <b>632</b>, <b>634</b>, and <b>636</b> are selected by the user and toggled off, the real-time update of the visual indication is stopped, which allows the user to read a prior degree setting at a certain point during a vision test. To continue the real-time update, the user can select the toggle that was toggled &#x201c;off&#x201d; so that it is now toggled &#x201c;on&#x201d;.</p><p id="p-0172" num="0199">For example, there may be three windows showing a head oriented straight on, from the profile, and from the top down. These may show the exact rotation of the head in real time. They may be paused by clicking on them, which then causes them to freeze in place at the rotation when clicked. A small red line in the Y and Z windows may show the relative position of the target test stimulus for reference.</p><p id="p-0173" num="0200">In the subject data area <b>510</b> (e.g., at the right), there may be a laser pointer adjustment button <b>638</b> (which may also be referred to as an &#x201c;edit laser toggle calibration sliders button&#x201d;). The laser pointer adjustment button <b>638</b> may, when selected by the user, reveal a laser pointer adjustment UI <b>660</b> with three sliders allowing for width, height, and IPD compensation to calibrate the screen for the laser pointer (as shown in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>). The sliders may be used to adjust the position of the laser pointer relative to the mouse. Adjustments may be necessary as the monitor resolution and resolution of the headset as well as the IPD may affect the laser pointer tracking precision. The adjustments may be made, for example, when not running the program in full screen mode, or when the headset or monitor hardware is changed and resolutions are different than before in either device.</p><p id="p-0174" num="0201">To position the VR laser pointer at the appropriate location of the displays <b>114</b> and <b>116</b> in the head unit <b>112</b>, the computer <b>140</b> may, for example, take as input values corresponding to the display part of the screen height/width in pixels for each side of the display <b>146</b>, and make a calculation that is relative to the width/height of the head unit <b>112</b> in order to properly position the laser pointer under the mouse pointer on the display <b>146</b> so that the laser pointer appears similarly in the head unit <b>112</b> on either eye. Calibrating the display <b>146</b> for the laser pointer may require the collection of the input values by the manipulation of three sliders, as the user then only has to make the visible rectangles take up as much space as possible without overlapping. The computer <b>140</b> stores the values, and as long as the software is run on the same computer <b>140</b> with the same head unit <b>112</b>, further adjustments may not be necessary.</p><p id="p-0175" num="0202">The laser pointer adjustment UI <b>660</b> may be a window having one or more of a close button <b>662</b>, a width adjust slider <b>664</b>, a height adjust slider <b>666</b>, and an IPD offset slider <b>668</b>. The width adjust slider <b>664</b> and height adjust slider <b>666</b> adjust the width and height of the boxes that appear, for example, in both the left and right sides of the monitor. One objective may be to make the boxes as wide and as tall as possible without going over a width limit or over a height limit. The IPD offset slider <b>668</b> centers the boxes based on the current IPD. There may be, for example, slight &#x201c;cross hairs&#x201d; <b>672</b> on the computer monitor, such that the centers of the large X <b>670</b> in each box line up with the center of the cross hairs on the left and right sides respectively.</p><p id="p-0176" num="0203">The laser pointer adjustment UI <b>660</b> may be used, for example, to make adjustments more than once if the IPD varies from subject to subject, while the width/height may remain the same once the hardware is set and does not change. Settings may be saved as they are changed. When complete, the close button <b>662</b> may be selected by the user to close the 3 sliders window.</p><p id="p-0177" num="0204">In a particular implementation, the current real-time hardware device IPD reported by the VR test application software <b>160</b> can be displayed below the three rotating heads in millimeters to a precision of 1/10th of a millimeter. Other degrees of precision may be used, whether greater such as 1/100th of a millimeter or lesser such as 1/2 of a millimeter.</p><p id="p-0178" num="0205">In the subject data area <b>510</b> (e.g., at the bottom), there may be one or more of: IPD real-time text <b>642</b>, a save current IPD button <b>644</b>, saved IPD text <b>646</b>, subject ID text <b>648</b>, and a change subject button <b>650</b>.</p><p id="p-0179" num="0206">The save current IPD button <b>644</b> may allow the user to store the current real-time IPD as a preference for the specific subject. As each subject may have their own IPD setting, this allows a quick reset of the IPD so the user can ensure the hardware IPD setting is consistent across visits. If the saved IPD and the current real-time IPD do not match, the current real-time IPD text, such as saved IPD text <b>646</b>, can turn red, from its original light grey color. It can return to green when the current real-time IPD is adjusted and matches the saved IPD setting, indicated, for example, directly to the right of the save current IPD button <b>644</b> in similar but green text.</p><p id="p-0180" num="0207">As an example, on HTC VIVE&#x2122;, the hardware IPD setting is controlled by a small dial on the headset itself. This may or may not be programmatically controlled. When it is not, the system <b>100</b> can read its current setting and save it in the subject preferences, for future reference. To ensure a consistent IPD setting for the subject, once the IPD is set to where the subject prefers it, the save current IPD button <b>644</b> may be pressed, and then whenever that subject is loaded into the system in the future, it can be noted whether the current IPD matches their previously chosen IPD setting. If they do not match, the IPD text turns red. When they do match, the IPD text turns back to its default white color.</p><p id="p-0181" num="0208">The change subject button <b>650</b> may be used by the user to change the subject at any time (with the exception of during recording and playback) to a different subject. Pressing the change subject button <b>650</b> may generate a prompt to go through the steps of choosing a Subject ID or creating a new one.</p><p id="p-0182" num="0209">Referring now to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, shown therein is an example embodiment of the eye camera area <b>530</b> of VR test creation interface <b>400</b>.</p><p id="p-0183" num="0210">In the eye camera area <b>530</b>, there may be one or both of a left eye video display <b>712</b> and a right eye video display <b>714</b>. In development mode or during use, for example, white areas or previous (or stock) images may represent the left and/or right eye videos. In development mode, during VR testing or during recording playback, for example, actual video of the left and/or right eyes may be displayed. The actual video may, for example, come from the left eye tracking sensor <b>124</b> and/or the right eye tracking sensor <b>126</b>, or from one or more cameras within the head unit <b>112</b>. If the corresponding eye tracking sensor allows, pupil size in mm may be displayed.</p><p id="p-0184" num="0211">The eye camera area <b>530</b> may be configured to display videos of the subject's eyes if they are available, and if not this area may just be blank. The eye camera area <b>530</b> may have, for example, 4 sizes from very small to approximately 700 px wide proportionate to contain two 4&#xd7;3 aspect ratio video windows.</p><p id="p-0185" num="0212">The eye camera area <b>530</b> may have a refresh button <b>716</b> (e.g., near the top-left corner) that may be programmed to retry the connection to the hardware video stream (e.g., from a camera) if it has become disconnected and reconnected for some reason.</p><p id="p-0186" num="0213">In the eye camera area <b>530</b>, there may be one or more toggle buttons (e.g., at the top right) that can be selected by the user to resize the eye camera window between its largest and its smallest settings. These toggle buttons may include, for example, a smallest resize button <b>722</b>, a small resize button <b>724</b>, a medium resize button <b>726</b>, and a largest resize button <b>728</b>.</p><p id="p-0187" num="0214">The eye camera area <b>530</b> may be, for example, located just above the bottom UI and almost all the way to the right, to just before the right edge of the camera settings sliders area <b>580</b>. There may be a button that the user can select to toggle between showing or hiding the camera settings UI, such as a small down arrow far right of the camera settings UI. There may also be an info button that, when selected by the user, causes a program info screen popup to be displayed (e.g., in the center of the display).</p><p id="p-0188" num="0215">Referring now to <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, shown therein is an example embodiment of the main camera settings area <b>540</b> of VR test creation interface <b>400</b>. In the main camera settings area <b>540</b>, there may be buttons and toggles that affect camera display and/or launch other settings that may be normally hidden for convenience and clarity. For example, there may be one or more of: a left eye (camera) off button <b>812</b>; a both eyes off button <b>814</b>; a right eye (camera) off button <b>816</b>; a laser pointer on in left eye toggle <b>822</b>; a swap on/off state in left/right eyes button <b>824</b>; a laser pointer on in right eye toggle <b>826</b>; a show blur eye settings button <b>832</b>; a show eye (camera) rotation settings button <b>834</b>; a show color filter settings button <b>836</b>; and a show displacement settings button <b>838</b>.</p><p id="p-0189" num="0216">The left eye off button <b>812</b> and the right eye off button <b>816</b> can operate as on/off toggles that, when selected by the user, cause the view of either the left or right eye off to be turned off, showing only a completely black screen on the monitor left/right eye area and/or in the VR headset left/right eye. The both eyes off button <b>814</b> may be selected to turn both eyes off or on at once.</p><p id="p-0190" num="0217">The swap on/off state in left/right eyes button <b>824</b> may be selected by the user to enable quick switching of which eye of the subject is provided with a VR stimulus during testing. This may enable certain kinds of testing, such as the &#x201c;swinging flashlight test&#x201d; (e.g., as long as eye video is available for the user to monitor pupillary response times and the stimulus is sufficiently bright when on).</p><p id="p-0191" num="0218">The laser pointer on in left eye toggle button <b>822</b> and the laser pointer on in right eye toggle button <b>826</b> operate as laser toggle buttons and can be selected by the user to show the laser pointer in the left eye or the right eye of the subject during VR testing. The laser pointer can track with the mouse pointer so the user and the subject in VR can see the mouse movements and relative position (of the laser pointer to the mouse). The laser pointer can appear in VR and on the computer monitor simultaneously (as shown in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref>). Given all the different possible screen sizes and headset specifications, the laser pointer may be calibrated by width/height and IPD compensation. The controls for calibration may be shown in the laser pointer section of the subject data area <b>510</b>. The user may control the laser pointer so that it is configured to appear in either the left or right eye but not both, due to stereo convergence.</p><p id="p-0192" num="0219">In at least one embodiment, <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> provides a model functioning of a virtual laser pointer, where the Physical Reality side shows an overhead view of the subject's actual head while wearing the head unit <b>112</b>. On the other side of the dotted line, the subject's head is recreated in Virtual Reality with the location of the virtual cameras determining the visual image in the videos in the hardware. The virtual cameras, like physical cameras, can be moved and/or rotated, and the VR images in the head unit <b>112</b> that are presented to each eye are updated accordingly. The virtual laser pointer appears in both the display <b>146</b> (for the user) and directly in front of one of the cameras in Virtual Reality.</p><p id="p-0193" num="0220">The laser pointer may be, for example, a small red sphere that tracks with the mouse, with both the subject and the user being able to see where it is. It may be virtually a few centimeters in front of the subject's eye in VR, such that, for example, it can only be shown to either the subject's left eye or right eye but not both or the subject will attempt to bring it into focus, which can be very uncomfortable for a stimulus that is so close to the eyes. When the user deselects both sides, the laser pointer may be turned off. Its current setting may be saved in the subject preferences whenever it changes. The laser pointer may be a handy communication feature, allowing the user to visually refer to a stimulus and help guide the subject to view certain elements/objects.</p><p id="p-0194" num="0221">Referring now to <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, shown therein is an example of the main camera settings area <b>540</b> integrated with the camera settings sliders area <b>580</b> of VR test creation interface <b>400</b> (with magnified portions for ease of reference).</p><p id="p-0195" num="0222">The left eye off button <b>812</b> and the right eye off button <b>816</b> can be toggled on and off when selected by the user. When the buttons <b>812</b> and <b>816</b> are in an off position, a cancel icon <b>813</b> can be overlaid on these eye off buttons (or a similar means may be used to indicate that they are off).</p><p id="p-0196" num="0223">There may be a collapse camera settings button <b>841</b> located, for example, at a corner of the camera settings sliders area <b>580</b>. When the collapse camera settings button <b>841</b> is selected by the user, it causes the camera settings sliders area <b>580</b> to collapse or disappear from view.</p><p id="p-0197" num="0224">When the show blur eye settings <b>832</b> is selected by the user, the camera settings sliders area <b>580</b> may display (or toggle on/off) a specific set of sliders (for each of the subject's eye) relating to eye blurring that can be accessed and altered. An example of these, the show blur eye settings sliders <b>900</b>, is shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0198" num="0225">The show blur eye settings sliders <b>900</b> (e.g., one for the left eye and one for the right eye of the subject) may be used to apply a Gaussian blur to everything in the VR Headset corresponding to the selected eye of the subject. For example, this can be implemented such that the higher the slider value, the greater the blur effect. The blur effect can be used to remove stimulus detail while maintaining general light levels for certain kinds of assessment. Blur settings can be continuously saved to the specific user's preference file in their data folder whenever they are changed.</p><p id="p-0199" num="0226">The show blur eye settings sliders <b>900</b> may be used in ophthalmologic testing, where it may be necessary to obscure detail but not light levels in one eye of the subject or the other for testing purposes. With this setting, the user can provide &#x201c;blurring&#x201d; in the VR stimulus that is presented to one or both eyes of the subject or put a portion or all of the VR stimulus out of focus to a specific intensity. This setting may be updated on change and saved in the subject preferences.</p><p id="p-0200" num="0227">For example, subject-specific preferences may be saved in a subject preferences file when the show blur eye settings sliders <b>900</b> are changed, such that when the subject returns later, the last settings used with that particular subject are loaded from the subject preferences files and set so that assessment can continue from where it left off in the last test session. The settings of the sliders, accessible from the main camera settings area <b>540</b> (as shown in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>), and toggle buttons may be saved separately from the protocols. The sliders in the test-specific settings, as well as the VR environment, the VR stimulus rotation, and which test is to be displayed, may be saved separately to a particular protocol data file.</p><p id="p-0201" num="0228">When the show eye (camera) rotation settings <b>834</b> is selected by the user, the camera settings sliders area <b>580</b> may display (or toggle on/off) a specific set of sliders (for each eye) relating to eye (camera) rotation that can be accessed and altered by the user. The show eye rotation settings <b>834</b> may be referred to as diopter settings. An example of these, the show eye camera rotation settings sliders <b>1000</b>, is shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0202" num="0229">The show eye camera rotation settings sliders <b>1000</b> may be used by the user to alter the rotation of each eye camera in the x, y, and z rotational axes. There may then be a total of 6 sliders, one for each axis on each eye. Numbers indicating the &#x201c;diopter&#x201d; converted from degrees may be indicated next to the horizontal and vertical sliders. Diopters are normally thought of as how glass prisms indicate strength, but in this case they may correspond by a fractional amount to degrees of turn in a given axial plane. The third slider, torsional rotation, is indicated in degrees and is unrelated to prisms.</p><p id="p-0203" num="0230">The show eye camera rotation settings sliders <b>1000</b> may be used by the user to adjust rotation settings (prismatic/torsional), where each eye camera in VR can be rotated around the x, y, or z axis up to + or &#x2212; some maximum number of degrees (e.g., where in the display degrees are converted to diopters). This, like holding prisms in front of an eye, can either simulate a turn or help indicate how much of a turn a patient has horizontally, vertically. Additionally, torsional rotation can be altered, which is not a prismatic effect but possible in VR as virtual eye cameras are rotatable in all three orientations. Rotation settings may be saved whenever they are changed to the subject's preference file. Sliders for each eye may represent horizontal, vertical, and torsional rotation. Next to each slider may be a &#x201c;VR control&#x201d; toggle where, if selected, the VR controller thumb pad enables the subject to move that particular slider back and forth with thumb movements.</p><p id="p-0204" num="0231">When the show color filter settings <b>836</b> is selected by the user, the camera settings sliders area <b>580</b> may display (or toggle on/off) a specific set of sliders (for each eye) relating to a color filter that can be accessed and altered. An example of show color filter settings sliders <b>1100</b> is shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0205" num="0232">The show color filter settings sliders <b>1100</b> may be adjusted by the user to set a color filter over the left and/or right eye. One slider for each eye can allow the user to select a color value in nanometers that correspond approximately to light wavelength so that the user can control which colors are filtered in a VR stimulus to each eye of the subject. It is possible that RGB color values set by the sliders may not necessarily match the color wavelength precisely as not all wavelengths of light can be accurately represented by the RGB color space available on computer hardware, but if future hardware becomes capable of displaying a wider color gamut this will not affect the function of the design. The other slider for each eye can be used to adjust the intensity or amount of filtering applied to the VR stimulus that is presented to each eye of the subject during VR testing. Lock buttons, when toggled on by the user, allow the movement of one slider on one side to move the same amount as a corresponding slider on the other side so that the VR stimuli sent to both eyes have the same filter setting. As with the other sliders, these color filter slider settings can be saved in the subject's data file whenever they are changed.</p><p id="p-0206" num="0233">The show color filter settings sliders <b>1100</b> may be used to adjust color filter settings, for example, to enable the user to set a color filter that is applied to a VR stimulus for each or both of the subject's eyes during VR testing. The top slider may be used by the user to control the color wavelength in nanometers (e.g., approximately, as not all color wavelengths can be replicated on a computer monitor) and the bottom slider intensity in percent. It has been theorized that certain color filters may assist subjects with certain forms of colorblindness, and this functionality may allow for experimentation in that regard. The settings for the show color filter settings sliders <b>1100</b> may be changed based on the subject's preferences and then saved to the subject's preference file. Pressing the lock toggle button on either side of the screen may make the sliders synch to each other so the VR stimuli shown to both eyes are equally affected simultaneously.</p><p id="p-0207" num="0234">When the show displacement settings <b>838</b> is selected by the user, the camera settings sliders area <b>580</b> may display (or toggle on/off) a specific set of sliders (for each eye) relating to displacement that can be accessed and altered. An example of these, the show displacement settings sliders <b>1200</b> is shown in <figref idref="DRAWINGS">FIG. <b>12</b></figref>.</p><p id="p-0208" num="0235">The show displacement settings sliders <b>1200</b> may work similarly to the diopter settings sliders and look nearly identical, with the main difference being that they translate the camera in distance along the x, y, or z axis. The amount of translation may be indicated in millimeters next to each slider. When moved, like all the other sliders, these slider values can be individually saved to the subject's data file for reloading when the subject returns later.</p><p id="p-0209" num="0236">The show displacement settings sliders <b>1200</b> may enable the user to adjust displacement settings, where each eye camera in VR can be translated along the x, y, or z axis up to + or &#x2212; some maximum number of millimeters. This can extend either the maximum or minimum IPD by moving the foveal center point past the maximum or minimum physical IPD setting on the hardware device, or compensate for eye deformities where they are positioned beyond normal physical locations. Displacement settings may be saved whenever they are changed to the subject's preference file. Sliders for each eye may represent horizontal, vertical, and z depth displacement. Next to each slider may be a &#x201c;VR control&#x201d; toggle where, if selected by the user, enables the subject to use a VR controller thumb pad or stick to move that particular slider back and forth with thumb movements.</p><p id="p-0210" num="0237">In at least one embodiment, next to each slider may be a toggle for VR controller control. If selected, moving the thumb on the LEFT or active VR controller (if only one is present and it is indicated as RIGHT or LEFT) can move the slider value up or down, allowing the subject to gently adjust the setting of the selected slider. A reset button can put all sliders back to zero. Whenever slider values change, their value can be saved to the subject preferences file and can be reloaded when the subject is reloaded into the system later.</p><p id="p-0211" num="0238">Referring now to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, shown therein is a letter-coded screen image of an example embodiment of the test settings area <b>1300</b> of VR test creation interface <b>400</b>.</p><p id="p-0212" num="0239">The UI for each test may be created or shown in the test-specific UI area <b>550</b>. Each VR test may consist of some kind of visual stimulus that may be based, for example, on one or more of the gold standards in ophthalmic testing. The VR stimulus may be &#x201c;attached&#x201d; to the head rotation or not attached, depending on the test requirement, and the VR position of the stimulus may always move with the position of the subject's head. If attached using the rotation lock button, the VR stimulus may always appear directly in front of the subject's face at the exact specified distance no matter where they turn their head. If not, it may still remain the specified distance but not necessarily in front of the subject's face.</p><p id="p-0213" num="0240">Each test may be represented by an icon that is a button graphic, but may also appear in the top left of the user interface, and as the current protocol if the given test is the one that is being currently displayed by the protocol.</p><p id="p-0214" num="0241">Test settings may all appear, for example, in the part of the interface that is directly above the test and protocol selection buttons, but directly below the subject visual settings controls.</p><p id="p-0215" num="0242">In general, the settings may be what are recorded by the protocols and relate to what VR stimulus needs to be presented to the subject to conduct a given VR vision test.</p><p id="p-0216" num="0243">The &#x201c;general settings&#x201d; are defined as those settings that are recorded by the protocols and relate to what VR stimulus needs to be presented to the subject to conduct a given VR vision test. In development mode, a developer may, for example, select GUI elements and associate functionality (e.g., scripts, code, triggers, events, procedures) with them to develop a VR vision test. The general settings may include one or more of:<ul id="ul0006" list-style="none">    <li id="ul0006-0001" num="0000">    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="0244">(A) Test specific settings: these settings may change depending on which test is selected.</li>        <li id="ul0007-0002" num="0245">(B) Test selection: these buttons may indicate which VR tests are available and which one is currently selected.</li>        <li id="ul0007-0003" num="0246">(C) Test stimulus settings window launch: this may be used to open a test stimulus settings window, which allows selection of the position and rotation of the test VR stimulus relative to the subject's head in any or all three axes (x,y,z rotation and position).</li>        <li id="ul0007-0004" num="0247">(D) Room colors: these may be used to define the colors of the &#x201c;room&#x201d; or virtual walls, floors, and small objects that are apparent in the room in which the subject is located in VR. If none of these are selected, there is no room displayed (i.e., no walls, floor, or objects), and only the skybox appears.</li>        <li id="ul0007-0005" num="0248">(E) Skybox colors: these may be used to define the colors of the &#x201c;skybox&#x201d; or distant sky and horizon at infinity. This may represent the inside of a giant sphere at visual infinity where the subject is effectively at its center. For example, this sphere cannot be turned off, but it can appear totally black or totally white rendering it non-apparent.</li>    </ul>    </li></ul></p><p id="p-0217" num="0249"> In at least one embodiment, the user may develop VR tests that achieve the same goals as the gold standard tests, but can be delivered more effectively. The UI for a newly developed test may be created in the test-specific UI area <b>550</b>. The user may, for example, integrate &#x201c;Art&#x201d; into the tests so researchers can more fully customize the VR environment for subjects, and the concept of integrating clinical tests into character-driven stories may be realized.</p><p id="p-0218" num="0250">Referring now to <figref idref="DRAWINGS">FIG. <b>14</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for a Worth 4 Dot test. The Worth 4 Dot VR test is a VR approximation of the real-life Worth 4 Dot test. The Worth 4 Dot (WFD) test may be used for the assessment of eye function. For example, two different VR stimuli, a larger square and a small flashlight sized one, may be shown to the subject while the subject is &#x201c;virtually&#x201d; wearing red and green filters over their eyes.</p><p id="p-0219" num="0251">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: a large WFD stimulus toggle button <b>1412</b>; a small WFD stimulus toggle button <b>1414</b>; a left green, right red camera filter toggle button <b>1416</b>; a left red, right green camera filters toggle button <b>1418</b>; a distance indicator icon <b>1420</b>; a distance slider <b>1422</b>; and a distance amount text <b>1424</b>.</p><p id="p-0220" num="0252">The Worth 4 Dot VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>14</b>B</figref>). The large WFD stimulus toggle button <b>1412</b> and small WFD stimulus toggle button <b>1414</b> can be selected by the user during VR vision testing to toggle between large stimulus and small stimulus, with sizes displayed in VR being in compliance with the size of the gold standard WFD test as approximately <b>20</b>cm squared for the large stimulus, and <b>6</b>cm squared for the small stimulus.</p><p id="p-0221" num="0253">The left green, right red camera filters toggle button <b>1416</b> and the left red, right green camera filters toggle button <b>1418</b> can be selected by the user during VR vision testing to toggle between providing green filter left/red filter right, green filter right/red filter left, or no filter to the VR stimuli that are presented to the subject.</p><p id="p-0222" num="0254">The distance slider <b>1422</b> may be set by the user to a defined stimulus distance for performing the WFD test where such distances may range from about 0.3 m to 6 m.</p><p id="p-0223" num="0255">Referring now to <figref idref="DRAWINGS">FIG. <b>15</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for a Bagolini Striated Lens VR test. The Bagolini Striated Lens VR test is a VR approximation of the real-life Bagolini Striated Lens test. The Bagolini Striated Lens test may be used to detect the presence or extent of binocular functions in the subject's eye. For example, this test may show two beams or white rods of light crossed at 45 degrees perpendicular to each other in VR to the subject. One beam may be shown in only one eye of the subject, and the other beam may be shown in only the other eye of the subject. The beams may be individually rotated torsionally.</p><p id="p-0224" num="0256">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: select gradations or deselect gradations toggle buttons <b>1512</b> (e.g., where gradient highlight indicates selection choice); a distance icon <b>1514</b>; a distance slider <b>1516</b>; a distance amount input <b>1518</b>; individual element rotation sliders <b>1520</b> (e.g., where top is left rotation, bottom is right rotation); rotation amount inputs <b>1522</b> for individual rotation sliders; and an overall rotation amount slider <b>1524</b>.</p><p id="p-0225" num="0257">The Bagolini VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>15</b>B</figref>). To set up the test, a user may use the VR test creation interface <b>400</b> to put a rotatable thin cylinder in the VR stimulus defaulting at opposite angles to make an &#x2018;x&#x2019;, with a centerpoint that appears to both of the subject's eyes.</p><p id="p-0226" num="0258">The select gradations or deselect gradations toggle buttons <b>1512</b> can be used to place colored gradations along the length of each long cylinder. The distance slider <b>1516</b> can be used to move the entire VR stimulus toward and away from the subject's eyes. The individual element rotation sliders <b>1520</b> can be used to rotate the left and right VR stimuli independently. The overall rotation amount slider <b>1524</b> can be used to rotate both stimuli simultaneously. It should be understood that whenever the terms left and right VR stimuli are used herein it is understood that they are independently presented to the left and right eyes of the subject, respectively.</p><p id="p-0227" num="0259">Referring now to <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for the Double Maddox Rod VR test. The Double Maddox Rod VR test is a VR approximation of the real-life Double Maddox Rod test. The Double Maddox Rod test may be used to determine cyclodeviations. For example, the test may show one red horizontal rod, and one white horizontal rod VR stimulus to each of the subject's eyes, respectively. The subject may be asked what is the correct rotation that should be applied to one of the rods in the VR stimulus to make it perfectly parallel to the other rod to indicate torsional rotation of their eyes.</p><p id="p-0228" num="0260">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: a VR control indicator icon <b>1612</b>; a VR controls red bar and VR controls white bar toggle button <b>1614</b>; a red bar (top) rotation slider <b>1616</b>; a white bar (bottom) rotation slider <b>1618</b>; a red bar rotation amount text indicator <b>1620</b>; a white bar rotation amount text indicator <b>1622</b>; a reset rotation button <b>1624</b>; a stimulus distance slider <b>1626</b>; a stimulus distance text indicator <b>1628</b>; and a distance icon <b>1630</b>.</p><p id="p-0229" num="0261">The Double Maddox Rod VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>16</b>B</figref>). To set up the test, the VR test creation interface <b>400</b> may begin by positioning a red rod, and a white rod extended mostly but not entirely across the visual stimulus in each eye respectively so they appear parallel to each other with red above and white below.</p><p id="p-0230" num="0262">The VR controls red bar and VR controls white bar toggle button <b>1614</b> can be used as a toggle for VR controller triggers to rotate either the red or white stimulus. The red bar (top) rotation slider <b>1616</b> and white bar (bottom) rotation slider <b>1618</b> can be used to rotate the red or white stimulus. The reset rotation button <b>1624</b> can be used to set rotation of both rods in the VR stimulus to zero. The stimulus distance slider <b>1626</b> can be used to move one or more of the VR stimuli toward or away from the subject's eyes.</p><p id="p-0231" num="0263">Referring now to <figref idref="DRAWINGS">FIG. <b>17</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for a Contrast Sensitivity and Visual Acuity VR test. The Contrast Sensitivity and Visual Acuity VR test is a VR approximation of the real-life Contrast Sensitivity and Visual Acuity test. The Contrast Sensitivity test may be used to measure the ability to distinguish between finer and finer increments of light versus dark. For example, the Contrast Sensitivity test may provide a chart to show letters that vary from high contrast to minimal contrast and the subject is asked to identify all the letters on the chart while standing in normal lighting conditions. The Visual Acuity tests may be used to measure the ability to see the details of a letter or symbol from a specific distance. For example, the Visual Acuity test may provide a typical letter-based eye chart or a symbol-based eye chart (e.g., for subjects who do not know the roman alphabet) and the subject is asked to identify all the letters or symbols on the chart.</p><p id="p-0232" num="0264">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: a scale icon <b>1712</b>; a scale amount slider <b>1714</b>; a scale amount text <b>1716</b>; a distance icon <b>1718</b>; a distance amount slider <b>1720</b>; and a distance amount text <b>1722</b>.</p><p id="p-0233" num="0265">The Contrast Sensitivity VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b>. To set up the test, the user may use the VR test creation interface <b>400</b> to begin displaying a standard letters based contrast sensitivity chart image. The scale amount slider <b>1714</b> can be adjusted by the user to resize the object's vertical and horizontal scale. The distance amount slider <b>1720</b> can also be adjusted by the user to move the VR stimulus toward or away from the subject's eyes in VR space.</p><p id="p-0234" num="0266">The Visual Acuity VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>17</b>B</figref>). To set up the test, the user may use the VR test creation interface <b>400</b> to display a standard letters based visual acuity chart image. The scale amount slider <b>1714</b> can then be adjusted by the user to resize the object's vertical and horizontal scale. The distance amount slider <b>1720</b> can also be adjusted by the user to move the VR stimulus toward or away from the subject's eyes in VR space.</p><p id="p-0235" num="0267">Referring now to <figref idref="DRAWINGS">FIG. <b>18</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for the Farnsworth D15 VR test. The Farnsworth D15 VR test is a VR approximation of the real-life Farnsworth D15 color vision test. The Farnsworth D15 test (or Farnsworth-Munsell 100 hue test) may be used to test color sensitivity. For example, the subject is presented with a VR stimulus comprising several discs at varying hues. The subject may be asked to arrange the discs in order of hue, starting with a particular one, placing discs in order of similarity to the original hue from left to right.</p><p id="p-0236" num="0268">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: a show numbered order toggle button <b>1812</b> (which overlays numbers on the arrangeable color discs in VR view displaying the correct numeric order as opposed to the order of placement by the subject); a refresh current test button <b>1814</b> (which resets the test and scrambles the arrangeable color discs); a D15 test toggle button <b>1816</b>; show/hide advanced custom test controls <b>1818</b>; a number of discs in test toggle buttons <b>1820</b>; a Color A toggle button <b>1822</b> (e.g., starting at left color); a Color B toggle button <b>1824</b> (e.g., starting at right color); and a show or hide rainbow cube in VR toggle button <b>1826</b>.</p><p id="p-0237" num="0269">The Farnsworth D15 VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref>). To set up the test, the user may use the VR test creation interface <b>400</b> to display a VR stimulus having a series of small color discs that can be dragged by the subject from a lower row to an upper row. The objective for the subject is to use the VR controller (with a small pick-up/drop object like an antenna at the end of it) trigger button to pick up and drop colored circles above the bottom row but in the proper order from left hue to right. The specific pick up and drop control may vary with the VR system hardware capabilities.</p><p id="p-0238" num="0270">The show numbers toggle <b>1812</b> can be used to display numbers indicating the proper order of hues on or off in the subject's field of view in VR. The refresh current test button <b>1814</b> can be used by the user to reset the test. The D15 test toggle <b>1816</b> can be used by the user to select and set up the standard Farnsworth D15 test. The show/hide advanced custom test controls <b>1818</b> can be used by the user to open up other options for color selection; and toggling may show/hide related buttons. The number of discs in test toggle button <b>1820</b> can be used to set the number of color hues in the test, such as 15, 11, 9, or 5. The color A toggle button <b>1822</b> and the color B toggle button <b>1824</b> can be used to set the extent of hues for the custom test and intervening colors may be calculated mathematically between those two extents. The show or hide rainbow cube in VR toggle button <b>1826</b> can be used to display a rainbow gradient cube toggle to show colors of the rainbow in the context of the VR test, which may help illustrate the effects of color filters (e.g., from subject camera controls).</p><p id="p-0239" num="0271">In VR, the subject may see the normal controllers and the color discs in a row with their order set at random. Using a VR controller, the subject may put the end of a small rod &#x201c;inside&#x201d; each disc, press the trigger to &#x201c;pick up&#x201d; the disc, and then drop it on a semi-transparent disc in the row directly above it.</p><p id="p-0240" num="0272">In VR, the subject may initially see color discs that have black rings around them and a select/drag antenna, so that the subject can then select drag targets that are transparent but easily visible, as shown in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref>. <figref idref="DRAWINGS">FIG. <b>18</b>B</figref> may also be used as a model for creating or performing a variation of the Farnsworth D15 test that generates VR visual test stimuli based on a specified range of colors.</p><p id="p-0241" num="0273">An example of what the subject sees and/or does in VR while being tested is shown in <figref idref="DRAWINGS">FIG. <b>18</b>B</figref>, which may include one or more of:<ul id="ul0008" list-style="none">    <li id="ul0008-0001" num="0000">    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="0274">(A) colored discs presented in VR for the subject to select and move by pressing a trigger button;</li>        <li id="ul0009-0002" num="0275">(B) a trigger button for selecting or moving colored discs;</li>        <li id="ul0009-0003" num="0276">(C) attaching a disc by holding the trigger button down and moving the tip of the rod at the end of the controller into a disc;</li>        <li id="ul0009-0004" num="0277">(D) matching the disc by moving the disc to a semi-transparent target in the row above, while trying to match the disc to the left as closely as possible; and</li>        <li id="ul0009-0005" num="0278">(E) placed discs, after all the discs are placed, with the actual numerical order of the discs being displayed at any time.</li>    </ul>    </li></ul></p><p id="p-0242" num="0279">Referring now to <figref idref="DRAWINGS">FIG. <b>19</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for a Stereopsis VR test. The Frisby Stereopsis VR test is a VR approximation of the real-life Frisby Stereopsis test. The Frisby Stereopsis VR test may be used to help identify the subject's depth perception ability using only the difference in location between their two eyes. For example, the test may endeavor to remove all monocular cues such as size differences, lighting cues, and visual occlusion. It may show four different patterns with one part of one of the patterns slightly closer to the subject than the rest of the patterns. The Frisby Stereopsis VR test may provide advantages over the real-world Frisby Stereopsis test by reducing or eliminating other monocular clues such as shadow/glare and/or perspective distortion.</p><p id="p-0243" num="0280">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of:<ul id="ul0010" list-style="none">    <li id="ul0010-0001" num="0000">    <ul id="ul0011" list-style="none">        <li id="ul0011-0001" num="0281">(A) Corner buttons that can be used to select which corner the stereopsis stimulus is displayed, such as a bottom-right corner button <b>1912</b>, a top-right corner button <b>1914</b>, a top-left corner button <b>1916</b>, and a bottom-left corner button <b>1918</b>. These corner buttons may be used as arrow toggles to indicate which of the four tests have the stereopsis stimulus (stimulus part that is closer to the subject).</li>        <li id="ul0011-0002" num="0282">(B) Distance toggle buttons to set the stereopsis VR stimulus at different distances from the non-stereopsis stimulus, such as a 6 mm distance toggle button <b>1920</b>, a 3 mm distance toggle button <b>1922</b>, and a 1.5 mm distance toggle button <b>1924</b>. The closer stimulus part may be compared to the rest of the test stimulus.</li>        <li id="ul0011-0003" num="0283">(C) Distance toggle buttons for overall stimulus distance from the subject, such as a 30 mm distance toggle button <b>1926</b>, a 40 mm distance toggle button <b>1928</b>, a 50 mm distance toggle <b>1930</b> button, a 60 mm distance toggle button <b>1932</b>, a 70 mm distance toggle button <b>1934</b>, and a 80 mm distance toggle button <b>1936</b>.</li>        <li id="ul0011-0004" num="0284">(D) A turn stimulus on or off toggle button <b>1938</b> (which may be unique, such as a checkmark indicating &#x201c;ON&#x201d; in an otherwise dark grey box).</li>        <li id="ul0011-0005" num="0285">(E) An icon representing the VR stimulus <b>1940</b>.</li>    </ul>    </li></ul></p><p id="p-0244" num="0286">The Stereopsis VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>19</b>B and <b>19</b>C</figref>). To set up the test, the user may use the VR test creation interface <b>400</b> to begin displaying the standard Frisby Stereopsis stimulus at the gold standard size and thickness specifications.</p><p id="p-0245" num="0287">The user can then use the corner buttons <b>1912</b>-<b>1918</b> to put a depth varied VR stimulus in any of the four corners. The depth toggle buttons can be used by the user to put the depth varied stimulus at different depths, such as 6 mm, 3 mm, and 1.5 mm. The distance toggle buttons can be used by the user to put the overall stimulus exactly at prescribed distances, such as 30, 40, 50, 60, 70, and 80 mm from the subject. The turn stimulus on or off toggle button <b>1938</b> can be set by the user so that settings can be adjusted while the VR stimulus is off, and then it can be turned on to avoid monocular or movement clues.</p><p id="p-0246" num="0288">In VR, the subject may see the Frisby Stereopsis test layout, as shown in <figref idref="DRAWINGS">FIG. <b>19</b>B</figref>. When the VR stimulus is presented so that it is viewed monocularly by the subject, it is likely very impossible for the subject to tell which part of which square is closer than the rest. When the VR stimulus is presented so that it is viewed close up by the subject, as shown in <figref idref="DRAWINGS">FIG. <b>19</b>C</figref>, monocular cues may indicate that the center of one of the squares is standing out, including color difference, occlusion, and an oblique angle for view. These are, for example, the kinds of things that the test endeavors to overcome.</p><p id="p-0247" num="0289">Referring now to <figref idref="DRAWINGS">FIG. <b>20</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for the Synoptophore VR test. The Synoptophore VR test is a VR approximation of the real-life Synoptophore test. The Synoptophore test may be used to detect abnormal retinal correspondence (ARC), or measure heterophorias and heterotropias. For example, the synoptophore may be a large, complex device that displays a slide in each eye of the subject. The slides can be translated horizontally, vertically, and depthwise, as well as rotated horizontally, vertically, or torsionally. Accordingly, the subject can move the slides until they match a certain criteria, such as a lion in the left eye appearing in a cage displayed in the right eye. Some of the slides, when shown together, may appear to show depth or stereopsis, like part of the slide image is closer to or farther away than the other. Accordingly, the slides can be moved by the subject until the subject manages to achieve stereopsis, and then the settings can determine what kind of corrective prism or surgical re-orientation of the eye the subject may require.</p><p id="p-0248" num="0290">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: a horizontally reverse left eye stimulus toggle button <b>2012</b>; a horizontally reverse right eye stimulus toggle button <b>2014</b>; a swap left and right eye stimulus toggle button <b>2016</b>; a black or white stimulus background toggle button <b>2018</b>; an actual stimulus toggle button <b>2020</b>; a stimulus distance slider <b>2022</b>; and a stimulus distance amount text indicator <b>2024</b>.</p><p id="p-0249" num="0291">The Synoptophore VR test may be set up by the user using the above elements of the test-specific UI area <b>550</b>, for example, as shown in <figref idref="DRAWINGS">FIGS. <b>20</b>B and <b>20</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>20</b>B and <b>20</b>C</figref> may also be used as models for VR representations of a Synoptophore device. To set up the test, the user may use the VR test creation interface <b>400</b> to begin displaying a standard Synoptophore VR stimulus.</p><p id="p-0250" num="0292">The horizontally reverse left eye stimulus toggle button <b>2012</b> and the horizontally reverse right eye stimulus toggle button <b>2014</b> can be used by the user to flip either slide stimulus horizontally. The swap left and right eye stimulus toggle button <b>2016</b> can be used by the user to flip in which of the subject eyes each stimulus slide is seen. The black or white stimulus background toggle button <b>2018</b> can be used to toggle between a black background and a white background. The actual stimulus toggles <b>2020</b> can be used to select or cycle through each slide art type, such as a lion, a rabbit, a swing, balls, lanterns, and a circle bisect stimulus. The stimulus distance slider <b>2022</b> can be used to move a stimulus toward or away from the subject's eyes in VR with meters displayed as in all distance sliders.</p><p id="p-0251" num="0293">In at least one embodiment, use of rotation sliders (prismatic and torsional are used along with the VR controllers so the user can select and move certain sliders as on a physical synoptophore device.</p><p id="p-0252" num="0294">Referring now to <figref idref="DRAWINGS">FIG. <b>21</b>A</figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for the Lees Screen test. The Lees Screen VR test is a VR approximation of the real-life Lees Screen test. The Lees Screen test may be used to detect incomitant strabismus, or measure incomitant ocular deviations in various positions of gaze. For example, the test may display a screen with small circular targets to one of the subject's eyes, and allow the subject to point to where the targets would be if the screen was in the other eye as well. By displaying a VR screen to only one eye, the other eye perceives that the VR screen is being presented to both eyes, so the subject can place targets to match where they think the targets are, but misaligned eyes will cause targets placed by the subject to be placed in the wrong positions. This may help determine discrepancies between the visual field, especially when the subject's eye is at its extreme extents (e.g., looking hard to the top and bottom corners). The test may affect the effectiveness for torsional comparisons between each of the subject's eyes, for example, by capturing torsional rotation as well as position of the targets. The test-specific UI area <b>550</b> may be expanded, for example, to combine features of the Lees, Harms, and Hess tests.</p><p id="p-0253" num="0295">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: an undo last marker drop button <b>2112</b>; a redo last marker undone button <b>2114</b>; a delete all markers button <b>2116</b>; a show close up stimulus toggle button <b>2118</b>; a show distant stimulus toggle button <b>2120</b>; a show/hide left eye orientation Lees Screen test button <b>2122</b>; a show/hide right eye orientation Lees Screen test button <b>2124</b>; and a black or white screen background toggle button <b>2126</b>.</p><p id="p-0254" num="0296">The Lees Screen VR test may be set up using the above elements of the test-specific UI area <b>550</b> (e.g., as shown in <figref idref="DRAWINGS">FIGS. <b>21</b>B and <b>21</b>C</figref>). To set up the test, the user may use the VR test creation interface <b>400</b> to begin displaying the Lees screen in VR in either the subject's left or right eye, where the subject's opposite eye has a VR controller indicator that allows the subject to try to line it up vertically, horizontally, and rotationally, with the grid displayed to the subject's other eye and place markers using the VR controller trigger.</p><p id="p-0255" num="0297">The undo last marker drop button <b>2112</b> and the redo last marker undone button <b>2114</b> can be used by the user to undo or redo the last marker placement to the beginning of the test. The delete all markers button <b>2116</b> can be used by the user to delete all markers. The show close up stimulus toggle button <b>2118</b> and the show distant stimulus toggle button <b>2120</b> can be used to display the VR stimulus close up with a small scale (e.g., to match the gold standard), or to display the VR stimulus as being moved far away with a large scale as an option. The show/hide left eye orientation Lees Screen test button <b>2122</b> and show/hide right eye orientation Lees Screen test button <b>2124</b> can be used to swap the display screens on which the VR stimuli are shown (i.e., the subject's left or right eye is presented with the grid). The black or white screen background toggle button <b>2126</b> can be used to choose whether the grid is black with white lines, or white with black lines.</p><p id="p-0256" num="0298">The show close up stimulus toggle button <b>2118</b> and the show distant stimulus toggle <b>2120</b> button may together be considered as a distance/scale option. The default screen stimuli may be, for example, approximately 0.75 meters away from the eye and about 1 meter square. The show distant stimulus toggle button <b>2120</b> may indicate the toggle that will place the VR object, for example, 3 meters away in VR from the subject's eyes and make the screen large enough to look the same size in the visual field as it does when it is close. An advantage of providing the Lees Screen test as described above in VR as compared to in real life is that making a huge Lees Screen type stimulus at great distance is practically prohibitive.</p><p id="p-0257" num="0299">An example of what the subject sees and/or does in VR while being tested with the Lees Screen VR test is shown in <figref idref="DRAWINGS">FIGS. <b>21</b>B and <b>21</b>C</figref>, which may include one or more of:<ul id="ul0012" list-style="none">    <li id="ul0012-0001" num="0000">    <ul id="ul0013" list-style="none">        <li id="ul0013-0001" num="0300">(A) The subject positions red targets by moving the VR controller to the parts of the grid where small circles appear to the subject's eye that is opposite to the controller. When the subject presses the VR controller trigger, this places a target at the x, y position and torsional rotation the subject desires.</li>        <li id="ul0013-0002" num="0301">(B) When a given test is finished, the left VR stimulus that was presented to the subject's left eye can be presented to the subject's right eye in the subsequent test and the right VR stimulus that was presented to the subject's right eye can be presented to the subject's left eye in the subsequent test.</li>        <li id="ul0013-0003" num="0302">(C) The VR stimulus presented to the opposite eye has a different color target but works the same way. For example, the subject may place little colored dots with cross hairs on them at positions on the Lees Screen. The subject performs the exercise for their left and right eyes where the Lees Screen is first displayed to their right eye so that the subject places targets in their left field of view so as to compare alignment of the eyes. The VR stimulus may be switched for presentation from the left eye to the right eye and vice versa, and then the targets may be a different color as they are placed at a new location by the subject. At the end of the VR visual test, both screens are shown as well as the locations of all the targets that were placed by the subject during the test so that the clinician can assess the result&#x2014;optionally creating a screen print for later analysis.</li>        <li id="ul0013-0004" num="0303">(D) When the tests on both eyes have been completed, the test results reveal where each side of the targets were placed for each eye, and the results may be analyzed.</li>        <li id="ul0013-0005" num="0304">(E) At any time during or after the tests, the colors in the VR stimulus can be swapped from black background/white grid to white background/black grid using the black or white screen background toggle button <b>2126</b>.</li>    </ul>    </li></ul></p><p id="p-0258" num="0305">The test may involve presenting a VR stimulus having a thick line stroke that is visually attached to a controller and shown to the subject's opposite eye compared to the subject's eye which is presented with a VR stimulus having an actual grid, as shown in <figref idref="DRAWINGS">FIG. <b>21</b>C</figref>. For example, as the subject is looking at the VR stimulus that is presented to the subject's left eye, the subject may press the trigger of the VR controller to leave a marker, such as a red marker, that is centered at an x, y position selected by the user and has a rotation, as shown in <figref idref="DRAWINGS">FIG. <b>21</b>C</figref>, and for the VR stimulus that is shown to the subject's right eye, a centered marker potentially having a different color but otherwise having the same x, y position and rotation may be created.</p><p id="p-0259" num="0306">For example, the subject may have a light-beam like line projecting from the VR controller that they are holding. At the end of the light beam is a small circle with cross hairs. If the subject twists the circle torsionally, and then presses the VR controller trigger, the test will show not only the location of the circle that the subject just placed, but also how the subject torsionally twisted it when the subject placed the circle at a given position. The subject's aim is to match the x, y position as well as the angle of the lines in the grid torsionally.</p><p id="p-0260" num="0307">Once both tests are complete, the user may, for example, display the position and angles of all of the targets that were placed by the subject during the test and then immediately take a screenshot so that a record of the performance for the subject is retained.</p><p id="p-0261" num="0308">In at least one embodiment, the Lees Screen VR test may include allowing the user to vary the distortion (e.g., cause less distortion) as the VR stimulus is moved farther away from the subject's eyes in VR and simultaneously becomes larger to ensure that it takes up the same VR visual field amount. This can be attributed to how in the Lees test, as compared to the Harms test, some optical distortion calculations may be used to &#x201c;pinch&#x201d; distort the VR stimulus. This VR test may be configured so that the VR stimulus is normally presented in this way so the user does not need to make any special selections. Advantageously, this aspect of the test can be done more accurately and easily in VR than in physical reality, since an object at a variable distance can easily be shown in VR and be distorted and scaled in real time as the object moves farther or nearer to the subject's eyes in VR.</p><p id="p-0262" num="0309">Referring now to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, shown therein is a schematic diagram of an example of a Brock String VR test. In at least one embodiment, the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> can be set up to perform a Brock String VR test. The Brock String VR test is a VR approximation of the real-life Brock String test. The Brock String VR test may be used to test convergence insufficiency and other anomalies of binocular vision. For example, the test may show a VR stimulus with a small gold bar that is at close proximity to the subject's eyes in VR, and a large red sphere that is several meters away from the subject's eyes in VR. If the IPD settings are correct and stereopsis is possible for the subject, then converging on the small gold bar makes it appear as if there are two red spheres. Conversely, if the subject's eyes converge on the red sphere, two gold bars may then be shown on either side of the sphere.</p><p id="p-0263" num="0310">This is analogous to what happens in the real world due to the convergence being done with the subject's eye muscles. In the real world, a subject may do this test with their thumb and another object (e.g., a light or small item) a few meters away. The subject may hold their thumb a few inches from their face and focus on it. The subject may then notice that the far away object appears as two images while their thumb is one object. If the subject does the opposite and focuses on the distant image, then two thumbs appear in their view. The Brock String VR test may have advantages over the real-life test such as more accurate and consistent results.</p><p id="p-0264" num="0311">Referring now to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, shown therein is a schematic diagram of an example of a Visual Field VR test. In at least one embodiment, the test-specific UI area <b>550</b> of the VR test creation interface <b>400</b> can be set up to allow the user to perform a Visual Field VR test on the subject. The Visual Field VR test is a VR approximation of the real-life Visual Field test. The Visual Field VR test may be used to test dysfunction in the subject's central and peripheral vision. For example, the VR stimulus for this VR test may be generated to show objects at various places within the subject's visual field while the subject keeps their gaze fixed. The objects may be visible to only one of the subject's eyes at a time. The VR stimulus may be modified such that the objects may move out of the subject's visual field and then back in. The subject may signal when they see the object come back into view in VR. Alternatively, or in addition, one of the eye tracking sensors <b>124</b> or <b>126</b> may be used to detect when the subject's eye sees the object come back into view. For example, the subject may look at the stimulus if it suddenly appears in an otherwise empty space, and the user can tell what the subject is looking at when tracking the subject's pupil.</p><p id="p-0265" num="0312">In at least one embodiment, the Visual Field VR test may involve &#x201c;virtually attaching&#x201d; a stimulus to the head of the subject in VR, so that the entire stimulus fires various flashes inside of a &#x201c;sphere shape&#x201d; in a similar fashion as the Humphrey visual field test. However, the VR stimulus is modified so that the position of the sphere shape moves with the subject's head and the subject's eye position is tracked to ensure centering relative to the head/eye gaze position of all the flashes. This ensures the widest possible visual field test as the subject can move their eyes as well as their head to ensure wider gaze assessment, even with limited VR hardware visual field extents. Accordingly, at least one of the subject's eye positions may be independently tracked by the head unit <b>112</b>, and values corresponding to the eye positions may be tracked and recorded by the computer <b>140</b>.</p><p id="p-0266" num="0313">Referring now to <figref idref="DRAWINGS">FIG. <b>24</b></figref>, shown therein is an example embodiment of the test-specific UI area <b>550</b> of VR test creation interface <b>400</b> for an art program, which may be used, for example, as a test or a practice environment.</p><p id="p-0267" num="0314">In the test-specific UI area <b>550</b>, there may be one or more (e.g., from left to right) of: a delete all particles button <b>2412</b>; a choose regular controller mode button <b>2414</b>; a choose paintbrush mode button <b>2416</b>; a delete all painted items button <b>2418</b>; a drop all painted items button <b>2420</b>; a reposition all painted items button <b>2422</b>; an allow controller to drop items toggle button <b>2424</b>; an allow painting of particles toggle button <b>2426</b>; a hide the environment toggle button <b>2428</b> (which may disable drop); and a choose color of environment toggle button <b>2430</b>. These buttons and toggles may apply in order for the left side, and in reverse for the right side.</p><p id="p-0268" num="0315">The user may set up the art program environment by using the above elements of the test-specific UI area <b>550</b>. To set up the test, the user uses the VR test creation interface <b>400</b> to begin providing an environment for a subject to &#x201c;paint&#x201d; in 3D using a VR controller trigger. The subject can toggle between &#x201c;brush&#x201d; mode and &#x201c;controller&#x201d; mode.</p><p id="p-0269" num="0316">In brush mode, the subject can create virtual objects in midair as they hold the trigger. When it is sensed that the subject applies more pressure on the trigger, the shape can be made larger in the VR environment. The shapes can be placed in the VR environment so that their position matches the rotation and position of the controller determined by the subject at the time of being instantiated. Shapes and colors can be determined based on inputs that are provided by the subject via the thumb controller when in brush mode. Additional options to allow the subject to select particle effects can be implemented in a similar manner. While in paint mode, when the subject's thumb touches the pad or stick on the VR controller, all of the paint options can be displayed in the VR environment. Movement of the subject's thumb on the thumbpad can be tracked by a tiny ball that is displayed in the VR environment. On overlap/collision between the tiny ball and any of the objects shown in the VR environment, the object can be displayed slightly larger in the VR environment which indicates selection. The object can then be displayed in normal size when the overlap/collision is no longer occurring. When the tiny ball overlaps/collides with a color sphere, the material is chosen, the material being, for example, a color, texture, image, or effect (e.g., flaming, smoking, refracting, casting shadows). At the same time, the shape selectors and particle selectors can become the same color as the selected material. When the tiny ball overlaps/collides with a shaped object in the VR environment, it becomes the selected shape. The spheres having the same color may adopt the newly selected shape too. When the tiny ball overlaps/collides with a particle selection object, it becomes &#x201c;highlighted&#x201d; with additional 3D modelling information, and the painted particles are set to that setting. When the subject presses the thumb button, all of the painted objects can be shown as being &#x201c;physical&#x201d; and they drop/bounce/collide with the ground in the VR environment. When the subject presses the thumb button again, all of these &#x201c;physical&#x201d; objects can be animated to move back to their original location. &#x201c;Groups&#x201d; of objects can be created when the subject lets go of the trigger.</p><p id="p-0270" num="0317">In controller mode, when the subject presses the trigger while touching painted &#x201c;Groups&#x201d; the groups can be selected and then the VR environment is modified to show the selected groups moving with the controller. When the subject lets go of the trigger, the group remains at its location at the time when the trigger was released. The paint brush mode can be turned on and off when the subject presses the grip buttons on the VR controller. If either VR controller is on, an icon shaped like a brush or a VR controller can appear in the UI on the display of the head unit <b>112</b> and the current mode of either brush/controller (i.e., right and/or left) can be indicated. A maximum number of groups and objects may exist. Once either the maximum number of groups or objects is reached, the earliest object or group that was created can be destroyed, and so there is never more than the maximum number of groups or objects in the VR environment.</p><p id="p-0271" num="0318">The delete all particles button <b>2412</b> can be used by the subject to delete, in the VR environment, all of the particles attached to objects and groups that were created by the corresponding left or right brush. There may be a note on confirmation with a popup that appears as shown in <figref idref="DRAWINGS">FIG. <b>25</b></figref>. The choose regular controller mode button <b>2414</b> can be selected by the subject to choose controller mode. The choose paintbrush mode button <b>2416</b> can be selected by the subject to choose paintbrush mode. The delete all painted items button <b>2418</b> can be selected by the subject to refresh and delete everything that was drawn in the VR environment by the corresponding left or right brush. The drop all painted items button <b>2420</b> can be selected by the subject to drop all the objects that were just painted to the virtual floor as they become subject to &#x201c;virtual gravity&#x201d;. The reposition all painted items button <b>2422</b> can be selected by the subject to return all of the objects that were just painted to their previous state (e.g., the previous position and rotation of these objects prior to the time when the button <b>2422</b> was selected). The allow controller to drop items toggle button <b>2424</b> can be selected by the subject to toggle between allow or do not allow the drop/return functionality on the VR controller. The allow painting of particles toggle button <b>2426</b> can be selected by the user to toggle between allow or do not allow draw particles on the VR controller. The hide the environment toggle button <b>2428</b> (which may disable drop) can be selected by the user to turn off the inner sandbox environment, so that, when off, objects cannot be &#x201c;dropped&#x201d; or they will virtually fall forever. The choose color of environment toggle button <b>2430</b> can be selected by the user to toggle the color of the inner sandbox environment.</p><p id="p-0272" num="0319">A paint mode controller shown in the VR environment may have a brush menu appear, as shown in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, when the subject's (real-life) thumb is on it (e.g., as represented in the VR image by the paintbrush with the disc and all the menu items on it). A small grey ball may be displayed and moved in the VR environment to track the position of the subject's thumb on a virtual semi-transparent disc, as shown in <figref idref="DRAWINGS">FIG. <b>26</b></figref>. Brushes may surround the outside of the disc within reach of the subject's thumb, so that the subject can choose material color, shape, and particle type, as shown in <figref idref="DRAWINGS">FIG. <b>26</b></figref>. When selected, the selection may be highlighted, as shown in <figref idref="DRAWINGS">FIG. <b>26</b></figref>.</p><p id="p-0273" num="0320">In the VR environment, the subject may see what they draw or paint in real time, in perspective, and/or in color, as shown in <figref idref="DRAWINGS">FIG. <b>27</b></figref> (without color though). This may help introduce a subject to VR. The act of creating objects in the VR environment may give the subject a sense of environmental control, which may put them at ease. This may possibly activate the subject's parasympathetic nervous system, making the subject more receptive to any tests that may follow.</p><p id="p-0274" num="0321">Referring now to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, shown therein is an example embodiment of the test selection area <b>560</b> of VR test creation interface <b>400</b>. In the test selection area <b>560</b>, there may be one or more (e.g., from left to right) of: individual test selection toggle buttons <b>2812</b> with icons; a lock stimulus tracking toggle button <b>2814</b>; a top row of toggle buttons <b>2816</b> for skybox colors; and a bottom row of toggle buttons <b>2818</b> for showing/hiding walls/floor etc. in the specified color.</p><p id="p-0275" num="0322">When an individual test selection toggle <b>2812</b> is selected by the user, a large version of its icon (e.g., as shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>) may be displayed in the top-left corner or other suitable position. The individual test selection toggle button <b>2812</b> can be used to select any of the tests with corresponding icons that, when selected, shows the given test and its current settings. Additionally, an icon may be displayed (e.g., in the top left of the UI screen) in much larger size to help indicate which test is being displayed at the time. The lock stimulus tracking toggle button <b>2814</b> can be selected by the user to lock or unlock the current main visual stimulus to the rotation of the subject's head, which may be indicated by using a small line (which might be colored) on two of the camera heads (e.g., in the top right) to show the position of the stimulus relative to the head of the subject's rotation. The top row of toggle buttons <b>2816</b> can be used to change the color of the skybox. The bottom row of toggle buttons <b>2818</b> can be used to change the color of (or hide/show) the walls and floor.</p><p id="p-0276" num="0323">Referring now to <figref idref="DRAWINGS">FIG. <b>30</b></figref>, shown therein is an example of a test stimulus settings window <b>3000</b> of VR test creation interface <b>400</b>. When opened, the test stimulus settings window <b>3000</b> may be displayed, for example, in the center of the screen or viewing area.</p><p id="p-0277" num="0324">In the test stimulus settings window <b>3000</b>, there may be one or more (e.g., from left to right) of:<ul id="ul0014" list-style="none">    <li id="ul0014-0001" num="0000">    <ul id="ul0015" list-style="none">        <li id="ul0015-0001" num="0325">(A) Lock test stimulus rotation toggle buttons in one of the three axes, which locks the stimulus rotation to the subject head rotation in the corresponding axis, such as an X-rotation lock toggle button <b>3012</b>, a Y-rotation lock toggle button <b>3014</b>, and a Z-rotation lock toggle button <b>3016</b>. For example, VR test creation interface <b>400</b> allows the user to select one or more these toggle buttons to lock or unlock rotation synchronization between the subject's head and the VR stimulus.</li>        <li id="ul0015-0002" num="0326">(B) A set the test stimulus rotation axis to the current subject head rotation toggle buttons for x, y and z axis <b>3022</b>, which is only available when the lock stimulus buttons (A) are off. For example, if rotation synchronization is off, then VR test creation interface <b>400</b> allows the user to select this button for the VR stimulus to match the rotation of the subject's head in the corresponding selected axis (x, y, or z).</li>        <li id="ul0015-0003" num="0327">(C) A reset test stimulus rotation axis to default toggle button <b>3032</b> (e.g., zero from program starting rotation). For example, if rotation synchronization is off, then VR test creation interface <b>400</b> allows the user to select this button to return the test stimulus rotation back to its default value for the selected axis x, y, or z (e.g., zero degrees).</li>        <li id="ul0015-0004" num="0328">(D) A current rotation in degrees of the test stimulus text, such as X-rotation <b>3042</b>, Y-rotation <b>3044</b>, and Z-rotation <b>3046</b>. For example, this text may indicate the current rotation of the test stimulus relative to the test subject's head, which may be updated whenever the value changes.</li>        <li id="ul0015-0005" num="0329">(E) Lock test stimulus movement toggle buttons in one of the three axes, which locks the stimulus movement to the subject head position along the corresponding axis, such as an X-movement lock toggle button <b>3052</b>, a Y-movement lock toggle button <b>3054</b>, and a Z-movement lock toggle button <b>3056</b>. For example, these displacement toggle buttons may work the same as with rotation, but with displacement (position) instead of rotation.</li>        <li id="ul0015-0006" num="0330">(F) A set the stimulus movement axis to the current subject head position buttons for x, y, and z axis <b>3062</b>, which are only available when the lock stimulus button (E) is off. For example, these buttons may be available when the displacement toggles are off.</li>        <li id="ul0015-0007" num="0331">(G) A reset stimulus movement axis to default toggle button <b>3072</b> (e.g., zero distance in VR from program starting position).</li>        <li id="ul0015-0008" num="0332">(H) Current distance in millimeters of the test stimulus text, such as X-distance text <b>3082</b>, Y-distance text <b>3084</b>, and Z-distance text <b>3086</b>. For example, the text updates may be in meters (as compared to the corresponding text items in the rotation section).</li>    </ul>    </li></ul></p><p id="p-0278" num="0333">This test stimulus settings window <b>3000</b> may have an open/hidden state that is stored in the subject preferences file, and may be reflected on opening of the program. For example, the subject preferences file may be a data file stored in the test subjects folder that is updated when corresponding subject-specific settings are changed.</p><p id="p-0279" num="0334">This test stimulus settings window <b>3000</b> may provide the user with the ability to toggle on or off continuous synchronization of the position and rotation of the test VR stimulus relative to the subject's head position in VR (which corresponds to the subject's real-life head position) in any or all three axes. These toggle states may be stored in a &#x201c;Presets&#x201d; settings file as they are test-specific.</p><p id="p-0280" num="0335">Correspondingly, buttons may be available to set the test stimulus position and rotation at the current time in any or all three axes. When these buttons are selected by the user, this number may be saved to the subject general settings file to be loaded if the toggles are set to off as directed by the currently loading preset. In addition, buttons may be available to reset the test stimulus position and rotation to zero at any or all three axes.</p><p id="p-0281" num="0336">Referring now to <figref idref="DRAWINGS">FIG. <b>31</b>A</figref>, shown therein is an example of the protocol selection &#x26; authoring area <b>570</b> of VR test creation interface <b>400</b> while in simple protocol mode. The simple protocol mode may hide one or more of the customizable settings sliders for each test (e.g., if the user is not necessarily clinical or a researcher). The overall screen in simple protocol mode may look less complex (as shown in <figref idref="DRAWINGS">FIG. <b>31</b>B</figref>). The protocol buttons may be alternatively referred to as preset buttons. For example, the simple protocol mode may be configured such that only the protocols are available and individual settings like stimulus distance, rotation, and other customizations are not available to change.</p><p id="p-0282" num="0337">In the protocol selection &#x26; authoring area <b>570</b>, there may be one or more (e.g., from left to right) of:<ul id="ul0016" list-style="none">    <li id="ul0016-0001" num="0000">    <ul id="ul0017" list-style="none">        <li id="ul0017-0001" num="0338">(A) A play next protocol button <b>3112</b>: selecting this may advance to the next protocol and move the user and subject through each of the loaded protocol settings. When the last protocol is reached, it may cycle back to the first one.</li>        <li id="ul0017-0002" num="0339">(B) Protocol selection toggle buttons <b>3114</b>: these may be highlighted as they are being displayed, and they may be clicked on for selection. For example, a gold frame may indicate the currently displayed protocol; if no gold frame exists, current settings do not match any of the protocols.</li>        <li id="ul0017-0003" num="0340">(C) A load protocol set button <b>3116</b>: this may be used to open a file browser (e.g., in Windows) and, on selection, provide a load confirm window, that prompts the user to cancel or confirm. On selection of confirm, the selected protocol set may be loaded.</li>        <li id="ul0017-0004" num="0341">(D) A switch to advanced protocol selection mode button <b>3118</b>: this may be selected to toggle between simple mode and advanced protocol selection mode.</li>    </ul>    </li></ul></p><p id="p-0283" num="0342">Referring now to <figref idref="DRAWINGS">FIG. <b>32</b>A</figref>, shown therein is an example of the protocol selection &#x26; authoring area <b>570</b> of VR test creation interface <b>400</b> while in advanced protocol selection mode. The advanced protocol selection mode may show some or all of the customizable settings sliders for each test (e.g., if the user is clinical or a researcher). The overall screen in advanced protocol selection mode may display (potentially many) more features (as shown in <figref idref="DRAWINGS">FIG. <b>32</b>B</figref>) than what is shown in the simple protocol mode.</p><p id="p-0284" num="0343">In the protocol selection &#x26; authoring area <b>570</b>, there may be one or more (e.g., from left to right) of:<ul id="ul0018" list-style="none">    <li id="ul0018-0001" num="0000">    <ul id="ul0019" list-style="none">        <li id="ul0019-0001" num="0344">(A) A load protocols from the external file button <b>3212</b>: this may be used to open a file browser (e.g., in Windows) and, on selection, provide a load confirm window, that prompts the user to cancel or confirm. On confirmation, the selected protocol set may be loaded.</li>        <li id="ul0019-0002" num="0345">(B) A switch to authoring mode button <b>3214</b>: this may be selected by the user to toggle between authoring mode and playback mode.</li>        <li id="ul0019-0003" num="0346">(C) A switch to protocol playback mode button <b>3216</b>: this may be selected by the user to toggle between playback mode and authoring mode.</li>        <li id="ul0019-0004" num="0347">(D) Protocol selection toggle buttons <b>3218</b>: these buttons may be used to select or cycle through a series of currently loaded protocols. For example, a gold frame may indicate the currently displayed protocol while if no gold frame exists, current settings do not match any of the protocols.</li>        <li id="ul0019-0005" num="0348">(E) A protocol &#x201c;next&#x201d; button <b>3220</b>: may be used to cycle through all the protocols, and when it reaches the one to the far right, it may start over at the first one.</li>    </ul>    </li></ul></p><p id="p-0285" num="0349">The VR test creation interface <b>400</b> may store all test-specific UI settings, which may be accessed either by the user directly selecting them, at which time the stored settings are applied, or by the user selecting the arrow button in the bottom right corner to cycle from one to the next and back to the start again. In this way, a series of preset tests can be quickly accessed by the user and the subject can quickly move from test to test without having to re-setup all of the settings. The presets can be stored in an external presets file.</p><p id="p-0286" num="0350">In place of, or in addition to, the load protocols from the external file button <b>3212</b>, there may be a reload protocols from the external file button, which can be selected by the user to refresh/restore the current presets from an external file (in case unsaved presets have become corrupted or edited accidentally).</p><p id="p-0287" num="0351">The protocol selection toggle buttons <b>3216</b> may be configured to capture the icon from the selected test and use it for when they are displayed. They may be highlighted when selected or when the current settings match the settings they have stored in them.</p><p id="p-0288" num="0352">For example, each protocol may have a specific icon or &#x201c;logo&#x201d;. The protocol icon matches the logo of the currently selected test during authoring mode so that it can be saved with it. If a protocol is selected and it matches one of the tests including every possible setting, that test icon to the left may be highlighted as well. If a protocol is selected and then the test settings are altered in any way, the protocol becomes unhighlighted, indicating that it no longer is matched by what is being displayed and thus all the settings precisely.</p><p id="p-0289" num="0353">The advanced protocol selection mode may operate as the simple protocol mode. For users who may want to deviate from the supplied protocols (i.e., supplied VR vision tests), then once a particular protocol is selected, individual settings for the protocol may be altered by the user, by utilizing various input GUI elements described herein to implement a desired setting. Once the settings of a given protocol do not match predefined settings for the protocol or settings from the protocol data file, the protocol icon highlighting may be disabled.</p><p id="p-0290" num="0354">The advanced protocol selection mode may be used by the user to create and/or edit protocols and protocol sets. A protocol set refers to a group of protocols. A given protocol is a collection of the current test settings for a given VR test so that the given protocol indicates the particular VR test, the positions (i.e., values) of the test sliders, the position (i.e., values) of the toggle buttons if applicable, and so on for other relevant test settings for the particular VR test, for example. Users may load a set of protocols which are for different VR tests, or the set of protocols may all be the same VR test with different settings being used for each protocol (the later allows the user to quickly perform the same VR test with different settings rather than have to manually update the settings for a particular VR test before they present it to the subject). Changes to the current test settings may be applied to a given selected protocol, and then once all of the changes to each protocol in a protocol set are acceptable, the user may save the protocol set either to overwrite the data files that correspond to the currently loaded protocol set, or to create a new custom protocol set. The user may then return to regular protocol mode, and use the file folder button to select another protocol set or reload the current one.</p><p id="p-0291" num="0355">Referring now to <figref idref="DRAWINGS">FIG. <b>33</b>A</figref>, shown therein is an example of the protocol selection &#x26; authoring area <b>570</b> of VR test creation interface <b>400</b> while in protocol authoring mode. The protocol authoring mode allows the user to edit already defined protocols.</p><p id="p-0292" num="0356">In the protocol selection &#x26; authoring area <b>570</b>, there may be one or more (e.g., from left to right) of:<ul id="ul0020" list-style="none">    <li id="ul0020-0001" num="0000">    <ul id="ul0021" list-style="none">        <li id="ul0021-0001" num="0357">(A) A load protocol set from the external file button <b>3312</b>: which may be used to open a file browser (e.g., in Windows) and, on selection, provide a load confirm window, that prompts the user to cancel or confirm. On confirmation, the selected protocol set may be loaded.</li>        <li id="ul0021-0002" num="0358">(B) A switch to protocol playback mode <b>3314</b>: may be selected to toggle between protocol authoring mode and playback mode.</li>        <li id="ul0021-0003" num="0359">(C) A protocol save button <b>3316</b>: may be selected to save current test settings to the currently selected protocol.</li>        <li id="ul0021-0004" num="0360">(D) A protocol record button <b>3320</b>: may be selected to record the current presets for a selected protocol (e.g., in internal memory), and when a preset is saved, the current test icon becomes its icon.</li>        <li id="ul0021-0005" num="0361">(E) A protocol add button <b>3322</b>: may be selected to add a default blank preset that can be overwritten with the protocol record button <b>3320</b>.</li>        <li id="ul0021-0006" num="0362">(F) A protocol selection toggle button <b>3324</b>: may be selected to cycle through the protocols in the current protocol set.</li>        <li id="ul0021-0007" num="0363">(G) A protocol delete button <b>3326</b>: may be selected to delete the currently selected protocol.</li>    </ul>    </li></ul></p><p id="p-0293" num="0364">In place of, or in addition to, the load protocols from the external file button <b>3312</b>, there may be a reload protocols from the external file button, which can be used to refresh/restore the current protocols for a given protocol from an external file (in case unsaved protocols have become corrupted or edited accidentally).</p><p id="p-0294" num="0365">The protocol save button <b>3314</b> may operate as a &#x201c;save confirm&#x201d; button, which may be used to launch a small confirm save window. The confirm save window allows the user to cancel or confirm saving all of the current protocols to the external file, overwriting what was previously there. The confirm save window may display directly over the authoring buttons, preventing them from being selected.</p><p id="p-0295" num="0366">The protocol save button <b>3314</b> may operate as a &#x201c;save current/new&#x201d; button, which may be used to launch a save window, such as prompt <b>3372</b> (as shown in <figref idref="DRAWINGS">FIG. <b>33</b>B</figref>). Prompt <b>3372</b> may display directly over one or more buttons, thus preventing them from being selected. Selecting &#x201c;Cancel&#x201d; hides the window with no change. Selecting &#x201c;Current&#x201d; overwrites the current protocol set file with the protocols as they are in the program, and hides the window, returning to previous. If &#x201c;New . . . &#x201d; is selected, this displays a prompt to enter a new protocol set filename and save it or cancel. On selecting cancel or save, the prompt/blackout disappears, returning the user to the former state. On selecting save, the new file with the indicated name is saved. The save button may be semi-transparent/disabled if the filename is invalid, and become enabled and fully opaque when the filename entered is valid.</p><p id="p-0296" num="0367">For example, there may be an external file that contains data that represents a set of protocols. While using the VR test creation interface <b>400</b>, that set may be loaded into program memory, but the file itself remains distinct. Protocols may be updated, changed, and/or edited, but they remain distinct until the protocol set is actually saved and the settings stored in temporary program memory, which is then written to the external file.</p><p id="p-0297" num="0368">In place of, or in addition to, the switch to protocol playback mode button <b>3314</b>, there may be a return to regular protocol mode button, which can be used to return when editing is completed.</p><p id="p-0298" num="0369">In place of, or in addition to, one of the above buttons, there may be a return to regular protocol mode button (from protocol authoring mode), which may be used, for example, when editing is completed.</p><p id="p-0299" num="0370">During use, the currently saved preset may be highlighted as in the standard way with toggles (e.g., to show that only one preset can be active at a time). For example, a gold frame may indicate the currently saved protocol.</p><p id="p-0300" num="0371">The protocol selection toggle buttons <b>3324</b> may, for example, indicate their order (e.g., in the bottom right of each icon) and which test they represent with their icon. A plurality of protocols may, for example, be the same test only with different test settings.</p><p id="p-0301" num="0372">Referring now to <figref idref="DRAWINGS">FIG. <b>34</b></figref>, shown therein is an example of a recording user interface <b>3400</b> of VR test creation interface <b>400</b>. The recording user interface <b>3400</b> may be configured so that one or more of all VR trackable object movement, UI settings, and objects are recordable. In addition, the recording user interface <b>3400</b> may be operated to allow a user to select one or more recorded data files for playback (i.e., display) in VR.</p><p id="p-0302" num="0373">The recording user interface <b>3400</b> may modify the subject data area <b>510</b> before, during, and after recording. For example, some buttons may be set to a disabled state while recording, as shown in <figref idref="DRAWINGS">FIG. <b>35</b></figref>.</p><p id="p-0303" num="0374">The recording user interface <b>3400</b> may provide additional status indicators, such as a recording indicator <b>3412</b> that provides a recording time stamp.</p><p id="p-0304" num="0375">The recording user interface <b>3400</b> may be configured to capture all changed data from position/rotation of headset, hand controllers, which buttons the subject has interacted with, and their data state information (e.g., trigger pressure, x-y position of thumb on thumb pad or stick).</p><p id="p-0305" num="0376">Recording may be selected by the user to capture slider position and toggle states at any given moment. The recording may be selected and configured by the user to capture the streaming video of eye cameras simultaneously. If available, recording may capture eye position/rotation and pupil size by using the eye tracking sensors <b>124</b> and <b>126</b>.</p><p id="p-0306" num="0377">A recording is started when the record scene button <b>616</b> is selected by the user. Once the record scene button <b>616</b> is selected, a confirmation window may appear to allow the user to confirm they want to commence recording. On confirmation, recording may begin, and a file may be created in the currently selected subject folder that can be accessed for later playback.</p><p id="p-0307" num="0378">During recording, a recording interface may appear and one or more elements may be recorded. For example, all UI changes, such as slider positions, toggles, and the object positions and states affected by these value changes, may be recorded. As such, during recording, a rotation of the VR user's camera (e.g., a few degrees horizontally using the slider) may be recorded and may appear when played back. The current position and rotation of the subject's head and controllers may be recorded and may appear as such on playback. At least one of eye video and eye telemetry data may also be recorded.</p><p id="p-0308" num="0379">During recording, file/open, change user, and capture screenshot functions may be disabled and the start record mode button may also be visibly disabled. These may become re-enabled when recording mode is fully exited.</p><p id="p-0309" num="0380">Once recording has been stopped, a confirmation window may appear with three choices: save the recording, resume the current recording from the current position, and cancel and discard the recording. Selecting save the recording places the current recording as a .rec file into the current subjects folder and closes the dialogue. Selecting return resumes recording and closes the dialogue box. Selecting delete deletes the recording and exits the user from recording mode, also closing the dialogue box.</p><p id="p-0310" num="0381">When the pause button is pressed, a recording paused dialogue box <b>3612</b> may appear, as shown in <figref idref="DRAWINGS">FIG. <b>36</b></figref>. After a recording has been saved, a playback file open confirm dialogue box <b>3712</b> may appear, as shown in <figref idref="DRAWINGS">FIG. <b>37</b></figref>. When a No (e.g., &#x2018;X&#x2019;) button <b>3714</b> is selected, the dialogue box <b>3712</b> closes and returns to the previous view. When a Yes (e.g., a checkmark) button <b>3716</b> is selected, the dialogue box <b>3712</b> closes and initiates a playback mode.</p><p id="p-0311" num="0382">Referring now to <figref idref="DRAWINGS">FIG. <b>38</b></figref>, shown therein is a computer screen image of an example embodiment of a playback mode user interface <b>3800</b> of VR test creation interface <b>400</b>. During playback, a playback mode control window <b>3810</b> may appear (e.g., to the right of the subject data area <b>510</b>). Playback files may be accessed by the file dialogue box provided by the operating system. If a file is a .rec file, it may prompt the open file for playback dialogue, indicating the filename and duration of the chosen file.</p><p id="p-0312" num="0383">The playback mode control window <b>3810</b> may have a variety of modes such as, but not limited to, at least one of fast reverse, play in reverse, step back continuously, step back once, step forward once, step forward continuously, play normally, and fast forward, as well as pause, stop, and seek to a specific time. <figref idref="DRAWINGS">FIG. <b>39</b></figref> shows the playback mode control window <b>3810</b> in play normally mode. <figref idref="DRAWINGS">FIG. <b>40</b></figref> shows the playback mode control window <b>3810</b> in pause mode.</p><p id="p-0313" num="0384">In the playback mode control window <b>3810</b>, there may be one or more of:<ul id="ul0022" list-style="none">    <li id="ul0022-0001" num="0000">    <ul id="ul0023" list-style="none">        <li id="ul0023-0001" num="0385">(A) A pause/play button <b>3812</b>, which toggles between pausing and playing.</li>        <li id="ul0023-0002" num="0386">(B) A stop playback and exit playback mode button <b>3814</b>.</li>        <li id="ul0023-0003" num="0387">(C) A playback head button <b>3816</b>, whose position relative to the long background rectangle it appears in front of shows the current relative time compared to the overall duration of the file. Clicking and dragging this button may affect the current playback time accordingly.</li>        <li id="ul0023-0004" num="0388">(D) A current time <b>3818</b>, displayed in minutes:seconds:frames (or other suitable time units).</li>        <li id="ul0023-0005" num="0389">(E) A seek to input text box <b>3820</b>. The user may enter minutes, seconds, and frames into these boxes and press the arrow directly to the right of them to seek to the indicated part of the .rec file.</li>        <li id="ul0023-0006" num="0390">(F) Precision jump buttons <b>3822</b>. They may function (e.g., from left to right), for example, as follows: go back 10 seconds, go back 1 second, go back 10 frames, go back 1 frame, go forward 1 frame, go forward 10 frames, go forward 1 second, go forward 10 seconds.</li>    </ul>    </li></ul></p><p id="p-0314" num="0391">In playback mode, all controls except for playback controls may be temporarily disabled during playback. Furthermore, simulated head, eyes, and hand controllers may be altered in real time reflecting the current data in the selected recording file. Non playback related controls may be disabled during playback.</p><p id="p-0315" num="0392">The user may view the recorded data in the VR environment and directly observe the simulated head, eyes, and hand controller movements as they update their position, rotation, and button states, as well as being affected by the settings in the UI.</p><p id="p-0316" num="0393">For example, the user can put the VR headset on and watch the subject's head movement, controller movement, and the VR visual stimulus objectively. The previously recorded subject may be represented by a 3D head and simulated controllers which indicate pressed buttons as well as positions based on their state at the current time index in the recorded data. In addition, color filters that were recorded as &#x201c;on&#x201d; may appear as &#x201c;sunglasses&#x201d; on the simulated head, with semi-transparent materials approximating the color filter effect (e.g., as opposed to filtering the overall cameras).</p><p id="p-0317" num="0394">In the UI, sliders may move and toggle buttons may change states reflecting current data in the selected recording file. A button in the UI may open a dialogue to choose a file for playback from the subject data folder, and on selection playback may commence. A pause button may be selected by the user to temporarily pause playback. A text box may indicate the current time index. Another text box may indicate the total duration of the current recorded data file. An input text box may enable the user to manually enter a time index to jump to. A button next to the input text box may exist which, when selected, moves the time index to the indicated value. A slider may, when dragged back and forth by the user, jump to the relative time index as a percent of the entire length of the recording. A stop button, when selected by the user, stops playback, hides playback controls, and returns the user interface to non-playback mode, at which time all UI controls will become interactable again. One or more (e.g., six) precision seek buttons may be used to seek at different rates, with the outside seek buttons moving either forward or backward 10 s, the next inner seek buttons 1 s, the next inner seek buttons 10 frames, and the innermost seek buttons 1 frame.</p><p id="p-0318" num="0395">Playback controls may also be selected through the VR controllers. For example, slider scrubbing may be accessible by the controller thumb pad or stick as it is with the UI settings sliders. The controller grip or alt buttons may be used to select play/pause playback. The trigger may be used to interact with a playback UI in VR that resembles the one in the main desktop UI. When the controller collides with a given VR UI element, the user may pull the controller trigger to activate it like a mouse click does on the desktop UI. For example, in playback mode, the user can see the original representation of the recorded subject, but with the user being a separate entity in VR, such that the user may enter the VR environment and look at the subject as the subject is being tested. The user may have various actions that they can perform in this mode, such as at least one of looking down at their controllers, seeing a UI that shows the view from the recorded subject's eye view, seeing a video of the subject's eyes, and being able to interact with a play/pause/timeline using another controller.</p><p id="p-0319" num="0396">Referring now to <figref idref="DRAWINGS">FIG. <b>41</b></figref>, shown therein is a computer screen image of an example embodiment of a playback in VR view <b>4100</b> of VR test creation interface <b>400</b>. The user may enter playback in VR view <b>4100</b>, for example, by pressing on the pause/play button <b>3812</b> while wearing a VR headset. The VR test creation interface <b>400</b> may cause the VR headset to provide, for example, a VR version of the playback mode user interface <b>3800</b>, a VR version of the playback mode user interface <b>3800</b> with some features hidden and/or additional features shown, the same view as experienced by the subject, or a combination thereof. The VR headset may thereby provide, for example, eye videos, perspectives of the subject's view, playhead pause/play status, time position, and time elapsed displayed as attached to the VR controller.</p><p id="p-0320" num="0397">In at least one embodiment, when a file is being played back, the VR test creation interface <b>400</b> shows the perspective of the subject right above the VR controller and also shows playhead position, as well as the eye videos if they are available. This may advantageously contribute to displaying test data during playback to a clinician while they themselves are in VR.</p><p id="p-0321" num="0398">Referring now to <figref idref="DRAWINGS">FIG. <b>42</b></figref>, shown therein is a flow chart of an example embodiment of a method <b>4200</b> of creating and updating a protocol. Method <b>4200</b> may be carried out, for example, by some or all of system <b>100</b>. Method <b>4200</b> may be implemented, for example, on some or all of computer <b>140</b>.</p><p id="p-0322" num="0399">Broadly speaking, some or all of method <b>4200</b> may be used to create a protocol while in protocol authoring mode (e.g., using the protocol selection &#x26; authoring area <b>570</b>). For example, to create a protocol, method <b>4200</b> includes: providing a plurality of visual objects (at <b>4232</b>); providing one or more subject environments (at <b>4232</b>); receiving one or more first inputs relating to a selection of the plurality of visual user interface objects (at <b>4234</b>, <b>4236</b>, <b>4238</b>); for each of the one or more first inputs, receiving one or more second inputs relating to a functionality of the selection of the one or more first inputs (at <b>4234</b>, <b>4236</b>, <b>4238</b>); generating measurement instructions to obtain one or more subject measurements from one or more sensors, based at least on the one or more first inputs and the one or more second inputs (at <b>4242</b>); and generating the protocol based at least on the one or more first inputs, the one or more second inputs, and the measurement instructions (at <b>4244</b>).</p><p id="p-0323" num="0400">At <b>4212</b>, the VR test application <b>160</b> is executed by the computer <b>140</b>.</p><p id="p-0324" num="0401">At <b>4214</b>, the computer <b>140</b> prompts the user for a Subject ID. The computer <b>140</b> may confirm the entry of a Subject ID or skip entry at the option of the user.</p><p id="p-0325" num="0402">At <b>4216</b>, the computer <b>140</b> receives input from the user to enter protocol authoring mode (e.g., by selecting or pressing on &#x2018;A&#x2019;).</p><p id="p-0326" num="0403">At <b>4218</b>, the computer <b>140</b> receives input from the user to read an existing protocol. The existing protocol may be selected from a series of protocols. However, the user may skip this step when creating a new protocol, for example.</p><p id="p-0327" num="0404">At <b>4220</b>, the computer <b>140</b> receives input from the user to create a new protocol or update an existing protocol.</p><p id="p-0328" num="0405">At <b>4222</b>, the computer <b>140</b> determines whether the user has decided to create a new protocol or update an existing protocol based on the user input. If the user decision is to create (C), then the method proceeds to <b>4224</b>. If the user decision is to update (U), the method proceeds to <b>4228</b>.</p><p id="p-0329" num="0406">At <b>4224</b>, the computer <b>140</b> receives input from the user to select an existing protocol. The user may do so, for example, by selecting a test icon corresponding to an existing protocol.</p><p id="p-0330" num="0407">At <b>4226</b>, the computer <b>140</b> receives input from the user to add a protocol based on an existing protocol (e.g., an existing test). The user may do so, for example, by pressing a plus (i.e., &#x2018;+&#x2019;) icon. For example, pressing plus while another protocol is selected causes a new protocol to be created that starts as a copy of the previously selected protocol, and can be edited from that point.</p><p id="p-0331" num="0408">At <b>4228</b>, the computer <b>140</b> receives input from the user to select an existing protocol for updating. The user may then update certain settings for the selected protocol. The computer <b>140</b> receives these updated settings and inputs them in a modified file for this updated protocol.</p><p id="p-0332" num="0409">At <b>4230</b>, the computer <b>140</b> receives input to confirm save. The user may do so, for example, by clicking on &#x2018;Y&#x2019;. The computer <b>140</b> then saves the modified file for the modified protocol. When the VR test application <b>160</b> is exited and returned to, the new protocol will appear.</p><p id="p-0333" num="0410">At <b>4232</b>, the computer <b>140</b> displays a new protocol matching the previously selected icon. The new protocol may be highlighted. If the new protocol is not highlighted, the user may click the new protocol to highlight it.</p><p id="p-0334" num="0411">At <b>4234</b>, the computer <b>140</b> receives input from the user to adjust the settings of the protocol using test-specific controls. For example, in a given test, distance/rotation sliders may be available, in which case the user may enter a number of degrees or meters (or other suitable unit) to specify an orientation and/or distance for a VR stimulus from the subject's eyes in VR. In another example, color filters may be available, in which case, a toggle (e.g., 0/1, on/off) may be stored to correspond with the toggle button. In another example, hiding or showing certain aspects of a stimulus may be available, in which case, toggles may be used to turn a stimulus on or off. All settings programmed as available may be recorded in a protocol in the position they are in at the time a local record button is pressed.</p><p id="p-0335" num="0412">At <b>4236</b>, the computer <b>140</b> receives input from the user to choose a background and/or floor type for the VR environment during the VR test. The default may be no background or floor, for example.</p><p id="p-0336" num="0413">At <b>4238</b>, the computer <b>140</b> receives input from the user to alter the stimulus orientation settings. The user may input the settings using, for example, the test stimulus settings window <b>3000</b>. When a given setting is locked and the local record button is pressed, the state of the lock (i.e., locked or unlocked) may be stored in the currently selected preset for saving.</p><p id="p-0337" num="0414">At <b>4240</b>, the computer <b>140</b> receives input from the user to locally record the protocol. The user may do so, for example, by selecting a button with a red circle (or the letters &#x201c;REC&#x201d;) on it.</p><p id="p-0338" num="0415">At <b>4242</b>, the computer <b>140</b> receives input from the user to select another protocol then select the new protocol to ensure the settings change to what is indicated by the protocol. For example, when the user input is to select a protocol, the computer <b>140</b> updates the currently displayed settings to match the selected protocol settings.</p><p id="p-0339" num="0416">At <b>4244</b>, the computer <b>140</b> receives input from the user to save all protocols to an external protocol file, which may be stored, for example, in the data files <b>168</b>. The computer <b>140</b> may prompt the user to confirm save.</p><p id="p-0340" num="0417">In at least one embodiment, method <b>4200</b> includes additional steps related to deleting an existing protocol. For example, the computer <b>140</b> may receive input from the user to delete a protocol, such as when the user selects a minus (&#x2018;&#x2212;&#x2019;) button on the user interface. The computer <b>140</b> may display a popup window in that case, which will allow the user to confirm deletion, for example, by selecting a yes (&#x2018;Y&#x2019;) button. The user may make the deletion permanent, for example, by carrying out act <b>4244</b>.</p><p id="p-0341" num="0418">In accordance with the teachings herein, the example embodiments that are described provide technical solutions to one or more technical problems. For example, test stimuli for vision tests often need to be precise and a consistent distance away from a subject, which may be difficult in real life due to various physical barriers and human error. However, with VR visual testing, lighting is more controllable so that it is consistent and precise from test to test, such that location, color, shadows, and glare are not issues. Furthermore, test equipment in real life can wear and get damaged, which decreases its precision. However, in a VR environment, the VR stimulus, being virtual, does not wear out or become dirty/scratched. With VR visual testing, the tests may also be fully recordable, with biometric data obtained in real time, such that it can be saved for later analysis (e.g., at up to 90 samples per second). The data may also be non-person identifiable as it can be entirely telemetry based. Users may view the data in VR so as to provide the closest thing to being with the patient when they are being analyzed while maintaining patient privacy.</p><p id="p-0342" num="0419">The example embodiments that are described may provide additional advantages over real-life visual testing. For example, prisms and color filters are cumbersome to mount or hold in front of a subject as they turn their eyes and can simply not practically be used with certain equipment like the Bagolini or Lees test. However, with VR visual testing, it is possible to apply prismatic/torsional rotation, displacement, color filter, blurring, or occlusion consistently and easily to any VR visual stimulus. In another aspect, torsional rotation of a visual stimulus in physical reality is currently impossible outside of the Synoptophore; whereas in VR, it can be applied to all vision tests.</p><p id="p-0343" num="0420">Test-specific advantages are also particularly noticeable with the Frisby Stereopsis test, which suffers in physical reality from movement of the visual stimulus (e.g., a human holding the device, where even the slightest movement can spoil the test). In a VR environment, the visual stimuli can be positioned perfectly still relative to the subject's head. The Synoptophore machine can be entirely replaced with VR software such that there are no worn out, faded slides, and in VR the visual field is much wider, which allows for the subject's eyes to be more clearly monitored and tracked during the entire test. In physical reality the stimulus used with the Synoptophore is limited while with VR visual testing, a VR laser pointer can be used by the user to point at specific parts of the VR stimulus to help direct the subject's glance. The Lees test also has many advantages in VR, particularly that the stimulus distance in VR can be varied, whereas in physical tests the VR stimulus cannot move. With VR visual testing, it is also possible to measure torsional alignment, which is currently either cumbersome or impossible in physical reality.</p><p id="p-0344" num="0421">It should be understood that the various user interfaces may have a different appearance in alternative embodiments. Furthermore, in other embodiments some of the VR vision tests described herein are not provided by the system <b>100</b> while in other embodiments more VR vision tests can be implemented in the system <b>100</b>. For example, the system <b>100</b> may be provided with other VR tests that implement other existing gold-standard tests.</p><p id="p-0345" num="0422">While the applicant's teachings described herein are in conjunction with various embodiments for illustrative purposes, it is not intended that the applicant's teachings be limited to such embodiments as the embodiments described herein are intended to be examples. On the contrary, the applicant's teachings described and illustrated herein encompass various alternatives, modifications, and equivalents, without departing from the embodiments described herein, the general scope of which is defined in the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of updating a protocol for a Virtual Reality (VR) medical test via a user device having at least one processor, the VR medical test being performed on a subject via a VR device worn by the subject, wherein the method is performed by the at least one processor and the method comprises:<claim-text>displaying GUI elements associated with the protocol on the user device, the GUI elements having user adjustable settings for modifying a functioning of the VR medical test;</claim-text><claim-text>receiving at least one selection input from the user device corresponding to a selection of at least one of the GUI elements;</claim-text><claim-text>receiving at least one setting input from the user device that corresponds to the selection of the at least one selected GUI elements;</claim-text><claim-text>modifying the user adjustable setting for each of the at least one selected GUI elements according to the corresponding at least one setting input; and</claim-text><claim-text>updating the protocol based at least on the user adjustable setting for each of the at least one selected GUI elements and a plurality of operations associated with the VR device.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises determining at least one of the plurality of operations to be executed on the VR device during the VR medical test based at least in part on the at least one selection input and the at least one setting input.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the VR medical test is a VR vision test.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the GUI elements comprise at least one of one or more VR methods, one or more VR features, or one or more VR controls.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the one or more VR methods comprise at least one of: left eye tracking, right eye tracking, head rotation tracking, or laser pointer placement.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the one or more VR features comprise at least one of: a subject data area, a subject VR view, or an eye camera area.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the one or more VR controls comprise at least one of: at least one state button, at least one toggle button, at least one slider, and at least one VR controller icon.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining at least one of the plurality of operations comprises organizing a VR visual stimulus including determining an intensity, one or more colors, and one or more shapes of at least one object in the VR visual stimulus.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining at least one of the plurality of operations comprises filtering a visual stimulus with specific wavelengths of color at varying intensities to obtain data from the subject during VR testing relating to color sensitivity.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining at least one of the plurality of operations comprises at least one of altering an orientation and/or position of an object in the visual stimulus, and displaying the visual stimulus to one or both eyes of the subject.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining at least one of the plurality of operations comprises representing and recording each axis of rotation of the subject's head relative to a direction of presentation of the VR visual stimulus to the subject.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the at least one of the GUI elements comprises a user controllable setting for allowing the user to set at least one of a distance, an orientation, and an intensity of a visual VR stimulus using the user device.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the at least one of the GUI elements comprises a sensor control and the user adjustable setting therefor allows the user to select when the sensor is used to obtain measurements of the subject during testing.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the method further comprises receiving user input for the sensor control for controlling at least one of an eye tracker, a camera, a force feedback sensor, an EEG sensor, and a motion tracker.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the functionality of the selection comprises at least one of displacement, rotation, or color filtering.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises:<claim-text>receiving at least one recording input from the user device corresponding to recording instructions associated with recording data during execution of the VR medical test on the VR device.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the recording instructions comprise at least one of tracking one or both subject eyes, recording a position of the subject's head, recording a rotation of the subject's head, and recording a selection made by the subject through a hand gesture or interaction with a VR controller.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the VR vision test is a Bagolini Striated Lens VR test and the method further comprises receiving user input to adjust settings for GUI elements associated with a gradations input, a distance input, and a rotation input.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the VR vision test is a Frisby Stereopsis VR test and the method further comprises receiving user input to adjust settings for GUI elements associated with a stereopsis stimulus selection input and a distance input.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the VR vision test is a Synoptophore VR test and the method further comprises receiving user input to adjust settings for GUI elements associated with a reverse eye stimulus input, a swap eye stimulus input, an actual stimulus input, and a stimulus distance input.</claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the VR vision test is a Lees Screen VR test and the method further comprises receiving user input to adjust settings for GUI elements associated with a show close-up stimulus input, a show distant stimulus input, and a show eye orientation input.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. A system for allowing a user to update a protocol for a Virtual Reality (VR) medical test to be performed on a subject via a VR device worn by the subject, wherein the system comprises:<claim-text>a display that shows a graphical user interface having GUI elements representing settings for the protocol;</claim-text><claim-text>an input device that is adapted to receive user inputs from the user for at least one of the GUI elements;</claim-text><claim-text>a memory unit that is adapted to store a protocol data file for the updated protocol; and</claim-text><claim-text>a processor unit that is operatively coupled to the display, the input device and the memory unit, the processor unit having at least one processor, the memory unit comprising a non-transient computer-readable storage medium having stored thereon computer-executable instructions for execution by the processor unit to perform the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. A computer readable medium comprising a plurality of instructions that are executable on a processor of a system for adapting the system to implement a method to allows a user to update a protocol for a Virtual Reality (VR) medical test to be performed on a subject via a VR device worn by the subject, wherein the method is defined according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. A method for generating and displaying a graphical user interface for a Virtual Reality (VR) medical test via a user device having at least one processor, the VR medical test being performed on a subject via a VR device worn by the subject, wherein the method is performed by the at least one processor and the method comprises:<claim-text>displaying a subject viewing area that comprises a view of the subject during testing or during playback of a recorded test;</claim-text><claim-text>displaying a camera settings area that comprises camera inputs for receiving user input for modifying images that are displayed in the subject viewing area; and</claim-text><claim-text>displaying a test-specific UI area that comprises test parameters that indicate settings for controlling different aspects of VR visual stimuli that are displayed to the subject via the VR device during testing.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises showing real-time data about the subject during testing in the subject viewing area.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the subject viewing area comprises an eye camera area and the method further comprises displaying a video of one or both eyes along with eye tracker data comprising pupil diameter of the subject in the eye camera area during testing or when playing back stored results of a previous test.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises displaying a subject VR view that shows what is displayed by the VR device to the subject during testing.</claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises:<claim-text>displaying main camera settings in the camera settings area;</claim-text><claim-text>receiving user input associated with the main camera settings for a desired view; and</claim-text><claim-text>displaying the desired view in the subject viewing area.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises displaying the camera settings area to show camera settings sliders that control at least one of eye blurring, eye rotation, color filter, or displacement during VR testing.</claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises changing a display of the test inputs shown in the test-specific UI area due to user input from the user device when the user selects a particular VR medical test.</claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the method further comprises using the graphical user interface to display at least one of a Bagolini Striated Lens VR test, a Frisby Stereopsis VR test, a Synoptophore VR test, and a Lees Screen VR test.</claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. A system for generating and displaying a graphical user interface for a Virtual Reality (VR) medical test to allow a user to perform the VR medical test on a subject via a VR device worn by the subject, wherein the system comprises:<claim-text>a display for showing the graphical user interface;</claim-text><claim-text>an input device that is adapted to receive user inputs;</claim-text><claim-text>a memory unit that is adapted to store a protocol data file having values for settings for the VR medical test; and</claim-text><claim-text>a processor unit that is operatively coupled to the display, the input device, and the memory unit, the processor unit having at least one processor that is configured to perform the method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. A computer readable medium comprising a plurality of instructions that are executable on a processor of a system for adapting the system to implement a method for generating and displaying a graphical user interface for a Virtual Reality (VR) medical that is performed on a subject via a VR device worn by the subject, wherein the method is defined according to <claim-ref idref="CLM-00024">claim 24</claim-ref>.</claim-text></claim></claims></us-patent-application>