<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003836A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003836</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17902070</doc-number><date>20220902</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>41</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>B</subclass><main-group>21</main-group><subgroup>18</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>08</class><subclass>B</subclass><main-group>21</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>88</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>415</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>21</main-group><subgroup>182</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>08</class><subclass>B</subclass><main-group>21</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>886</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>003</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">System and Method for Presence and Pulse Detection from Wireless Signals</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17006579</doc-number><date>20200828</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11448726</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17902070</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62893109</doc-number><date>20190828</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62897222</doc-number><date>20190906</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>AERIAL TECHNOLOGIES INC.</orgname><address><city>Montreal</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Martinez</last-name><first-name>Michel Allegue</first-name><address><city>Terrebonne</city><country>CA</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Ghourchian</last-name><first-name>Negar</first-name><address><city>Montreal</city><country>CA</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Grant</last-name><first-name>David</first-name><address><city>Santa Rosa Valley</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Morel</last-name><first-name>Francois</first-name><address><city>Kirkland</city><country>CA</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Tranchant</last-name><first-name>Pauline</first-name><address><city>Montreal</city><country>CA</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Jalil</last-name><first-name>Amir Minayi</first-name><address><city>Verdun</city><country>CA</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for detecting and monitoring human breathing, respiration, and heart rate using statistics about the wireless channel between two or more connected devices. A user is monitored for identifying patterns in the user's behavior that may allow the system to alert a caregiver to deviations in the user behavior that may be indicative of a potential issue, such as depression. A presence may further detected in a sensing area through the detection of spectral components in the breathing frequency range of comprises user includes transforming phase difference between spatial streams and amplitude of the samples representing frequency response of the channel for any frequency value into frequency domain to perform frequency analysis. Statistical analysis may be performed on the frequency space provided by the transformation. Micro motions may also be detected by detecting presence in a sensing area through the detection of spectral components in the micro motion frequency range.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="116.33mm" wi="128.61mm" file="US20230003836A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="132.08mm" wi="130.64mm" file="US20230003836A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="159.51mm" wi="145.88mm" file="US20230003836A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="148.51mm" wi="170.77mm" file="US20230003836A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="134.20mm" wi="148.08mm" file="US20230003836A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="170.60mm" wi="163.58mm" file="US20230003836A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="134.70mm" wi="151.21mm" file="US20230003836A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="141.14mm" wi="186.44mm" file="US20230003836A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="212.01mm" wi="54.53mm" file="US20230003836A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="237.83mm" wi="85.09mm" file="US20230003836A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="239.69mm" wi="172.64mm" file="US20230003836A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCED TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">The present patent application is a continuation and claims the priority benefit of U.S. patent application Ser. No. 17/006,579 filed Aug. 28, 2020, now U.S. Pat. No. 11,448,726, which claims the priority benefit of U.S. provisional application 62/893,109 filed Aug. 28, 2019, and U.S. provisional application No. 62/897,222 filed Sep. 6, 2019, the disclosures of which are incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0003" level="1">1. Field of the Invention</heading><p id="p-0003" num="0002">The present disclosure is generally related to motion and activity detection using Wi-Fi or a radio signal.</p><heading id="h-0004" level="1">2. Description of the Related Art</heading><p id="p-0004" num="0003">Motion detection is the process for detecting a change in the position of a user relative to its surroundings. Motion detection can be accomplished by a software-based monitoring algorithm. A motion detection system can analyze the type of motion to see if an alarm is warranted. Wi-Fi localization refers to translating observed Wi-Fi signal strengths into locations.</p><p id="p-0005" num="0004">Wi-Fi-based systems should be able to detect if a user is present in the environment even when no displacement or limb movement is produced. One solution is to use the periodic distortions caused by chest and lung breathing motion. Fine-grained channel state information (CSI) provided by pairs of Wi-Fi devices have been used to measure breathing rates, by analyzing time series in time and frequency domains often with an estimation error below one breathing cycle per minute. Detecting the effects of the respiration of a human being is desirable and can be used for presence detection and/or health monitoring. There is a need in the art for passive monitoring of human respiration from CSI.</p><heading id="h-0005" level="1">SUMMARY OF THE CLAIMED INVENTION</heading><p id="p-0006" num="0005">The present disclosure includes systems and methods that use a pair of connected devices in a sensing area to detect whether there is user presence and estimate the user's breathing rate in the frequency domain. Detection of movement or no movement occurs using passive Wi-Fi or radio signals. Biorhythm, such as heart rate and respiration, is also detected and used as a source of positioning and localization. Activity data and engagement level data associated with the user are stored for a plurality of time periods. One or more time periods are identified where the stored user activity data and the stored engagement level data are correlated. Current activity data and current engagement level data are compared to identify one or more cycles. When there is a deviation in the cycles, an alert score is incremented and once the alert score is above a threshold, an alert is sent to a device associated with the user.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTIONS OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a system able to sense objects within a sensing area via wireless signals;</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a test case scenario which can be considered a large space used to exhibit the capabilities and efficiency of the methods proposed herein;</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram of the statistical-based approach proposed herein in order to detect presence through breathing rate detection;</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of the spectrum obtained from the measured wireless signal;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an example of the ground truth data employed to validate the method proposed herein and the spectrum summarized from the measured wireless signals used for the breathing rate estimation;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of the feature space generated with the proposed method;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a Wi-Fi motion detection system;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an activity engagement module;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates a cycle module; and</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an alert module.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0017" num="0016">Systems and methods are provided for detecting and monitoring human breathing using statistics about the wireless channel between two or more connected devices, and more particularly for detecting of various levels of breathing, respiration, and heart rate for pulse detection of a user within the Wi-Fi range. Further embodiments include methods for detecting presence in a sensing area through the detection of spectral components in the breathing frequency range of beings, as well as for transforming phase difference between spatial streams and amplitude of the samples representing frequency response of the channel for any frequency value into frequency domain to perform frequency analysis; and performing statistical analysis on the frequency space provided by the transformation. Further, micro motions may be detected by detecting presence in a sensing area through the detection of spectral components in the micro motion frequency range. Some embodiments may include systems for sensing breathing of a human using a Wi-Fi motion detection.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a systems for detecting presence and/or estimate the breathing rate of a user <b>130</b> while analyzing information about the state of the wireless channel between two connected devices <b>120</b> (transceiver <b>1</b>) and <b>150</b> (transceiver <b>2</b>), independently from the particular network topology in which these devices are associated. An example can be an access point (AP) connected to a client station (STA) such as a Wi-Fi-enabled TV in IEEE 802.11n and/or IEEE 802.11 ac. The wireless link between the devices <b>120</b> and <b>150</b> is represented by <b>140</b>, where the state of the channel, frequency and phase responses, are estimated accordingly to the standard IEEE 802.11n and/or IEEE 802.11 ac. This information is reported as CSI data. The effective area where the state of the wireless channel can be estimated is 110. The area <b>100</b> is any space that belongs to any type of building where the frequency response and phase response of the channel can be estimated.</p><p id="p-0019" num="0018">A statistical approach for channel frequency response that uses distortions caused by the chest, lung, abdomen and other body parts breathing movements of a user <b>130</b> is implemented to estimate breathing rate. In order to demonstrate the effectiveness of the proposed method, an example of the measured breathing rate is described herein. This approach allows estimating the breathing of a user even though the user is not in the direct path between the two devices that are connected exchanging/estimating the frequency and phase responses of the wireless channel between the devices.</p><p id="p-0020" num="0019">As an example, two Wi-Fi-enabled devices following IEEE 802.11ac standard 220 and 260 were used as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. In the channel estimation process, one device acts as a transmitter and the other one as a receiver, in a N_tx&#xd7;N_rx multiple input multiple output (MIMO) system with N_tx transmitting antennas and N_rx receiving antennas. The Wi-Fi signal propagates between the transmitter and receiver through multiple spatial streams using Orthogonal Frequency Division Multiplexing (OFDM). In this example, used to demonstrate the approach proposed herein, CSI can be obtained at device <b>220</b> and/or <b>260</b>. A sampling rate of 33.33 Hz, which is low compared to the state-of-the-art approaches, was fixed by using an echo command in <b>220</b>, which generates traffic in the link between devices <b>220</b> and <b>260</b> making possible the estimation of the frequency response and phase response of the channel reported as CSI data. This sampling rate is used as an example and shall not limit the scope of the present approach. CSI characterizes the signal propagation between 220 and 260, and provides information on signal distortions such as attenuation, scattering, reflections, that are caused by the configuration of the environment and human motion interacting with the wireless signal used by devices <b>220</b> and <b>260</b>.</p><p id="p-0021" num="0020">Devices <b>220</b> and <b>260</b> were placed 12 meters away from each other in a large room, with tables, desks, computers and other little objects in between the devices. <figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts device placement (<b>220</b>, <b>260</b>) and floor plan of the environment setup. Several metadata files containing CSI information were captured. In the breathing condition a user was breathing in a steady and controlled fashion, matching the inhale-exhale pace to an auditory metronome. The user was staying still at several locations in the environment indicated by <b>200</b>, <b>210</b>, <b>230</b>, <b>240</b> or <b>250</b>. At each location, the user was either lying down on the ground or sitting still on a chair. The same number of captures was taken in each position, sitting or lying. Additional captures were performed when nobody was present in the environment, which generates the baseline condition in order to compare signals in the breathing condition, and signals with no one within the sensing area.</p><p id="p-0022" num="0021">The method analyzes flows of both magnitude and phase responses of the wireless channel between connected devices in multiple frequencies and spatial streams in MIMO channels. This is referred to as raw data or wireless channel measurements <b>300</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which is the input into the method. A signal pre-processing module <b>305</b> is proposed right after module <b>300</b>. Module <b>300</b> provides stable signals to the following modules. Some packages are discarded due to different reasons such as a defective estimation of the state of the frequency and/or impulse responses of the channel. The damaged packages are automatically detected and discarded. Module <b>310</b> is a combination of different spatial streams in MIMO systems. As an example, the phase difference between two different spatial streams for all the available subcarriers or frequency components is computed and transferred to module <b>315</b>. Module <b>315</b> performs a windowing strategy to analyze data either in real-time by using short window sizes, (e.g., 128 time samples (n) for all available subcarriers or frequency components), or by using large sizes of windows for analysis of the average breathing rate, (e.g., more than 4,096 samples in a sampled signals captured with sampling rate range of 10 Hz to 100 Hz). Module <b>315</b> can receive time domain series directly from the module <b>305</b>, having access to stable time series without any combination of spatial streams applied to the time series.</p><p id="p-0023" num="0022">Some examples of dimensionality of the time series, several captures were obtained containing wireless channel measurements <b>300</b> with 1,392 vectors (here named components) of CSI time series, from 58 subcarriers for each of 24 antenna phase differences while Ntx=4 and Nrx=4.</p><p id="p-0024" num="0023">One of the outputs of module <b>315</b> is used in a first feature set referred to as level 1. This level 1 will provide features directly taken from the windowing strategy implemented in module <b>315</b> and will be summarized as time domain features to the bus of features <b>345</b>, which is in charge of synchronizing all the features that were extracted at the different levels 1, 2 and 3, corresponding to the modules <b>320</b>, <b>325</b>, and <b>330</b>, respectively. The second output of module <b>315</b> is an input to the domain conversion module <b>335</b>. In module <b>335</b>, conversion from time-domain signals or time series into a different domain is performed. Some transformations performed in module <b>335</b> are, but not limited to, a discrete fourier transform (DFT), using the fast fourier transform (FFT) for example, or a discrete cosine transform (DCT), or any other transform related to frequency domain, or any wavelet transformation. In wavelet transform, the signal is decomposed in terms of some time-limited basic functions that have two degrees of freedom: scale (like frequency in sin function) which determines how compressed/stretchered the basic function is, and shift, which determines the pace where the basic function stands. Those basic functions are time-limited and there is a wide choice of basic functions for correlating with breathing measurements. The similarity between the basic function of wavelet transform and breathing signal facilitates a better extraction of breathing components from the signal.</p><p id="p-0025" num="0024">Any of the above-mentioned mathematical transforms are applied for each time series provided through module <b>315</b> or a subset of components provided by the module <b>315</b>. Only frequencies between 0.17 Hz and 0.70 Hz are considered in subsequent processing since these range covers the limits of human breathing rates. Module <b>335</b> creates inputs to the bus of features <b>345</b> and also to the module <b>340</b> that performs statistical analysis from the transformations performed in processing module <b>335</b>. The statistical processing module <b>340</b> can perform any statistical summary on the input series data, either processing independent components or summarizing those independent component statistics that are parallel data series in the domain in which the data series were yielded to processing module <b>340</b>. Some of those statistical analyses or summaries are histograms, power spectrum density (PSD) analysis, cumulative distribution function (CDF) and/or the probability density function (PDF). Processing module <b>340</b> then generates statistics that are used as statistical features of level 3 (<b>330</b>). At the decision making module <b>350</b>, the question regarding whether or not there is user presence shall be answered.</p><p id="p-0026" num="0025">All the features that were synchronized in the features bus <b>345</b> are used by decision making module <b>350</b> in decision making tasks that can use unsupervised or supervised machine learning models.</p><p id="p-0027" num="0026">The decision making module <b>350</b> aim to use features generated in features bus <b>345</b> and build a mathematical model that automatically distinguishes between the baseline condition where no user related motion or micro motion is detected and the breathing condition where at least one user is present. Two different strategies were employed to make this binary decision, which could be categorized into unsupervised and supervised models based on the validation method.</p><p id="p-0028" num="0027">In unsupervised approach only the features extracted from CSI data is used to build mathematical models. Some of those techniques include but not limited to distance-based clustering such as Gaussian mixture models (GMM), density-based clustering such as DBSCAN, centroid-based clustering such as kmeans and kmedians clustering, hierarchical clustering such as Agglomerative clustering, and Bayesian clustering algorithm such as latent dirichlet allocation (LDA) and hierarchical dirichlet processes (HDP). An example of such unsupervised methods that was applied in decision making module <b>350</b> is kmeans clustering, wherein a distance metric is defined between all the data points and then the whole observations is categorized into sub-groups based on the distance to the centroid of breathing condition and baseline condition.</p><p id="p-0029" num="0028">In supervised approach, in addition to features extracted from CSI data, for a small portion of observation data, the corresponding ground truth obtained during the validation process is also used to build the mathematical models of baseline and breathing conditions. Some of those linear and non-linear supervised techniques include but not limited to, support vector machines (SVM), decision trees, k-nearest neighbor (KNN) classification, na&#xef;ve bayes (NB), and deep neural networks (DNN). An example of such classification method that was applied in decision making module <b>350</b> is an SVM classifier with radial basis function (RBF) kernel that, based on some initial training from labelled data, builds the general model for classifying breathing and baseline conditions.</p><p id="p-0030" num="0029">As an example, a simple approach is to select the frequency component exhibiting a peak which can be considered as a bin that will potentially correspond to the user's breathing rate as illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. For each component, the frequency that yielded the maximum amplitude in the FFT output can be selected.</p><p id="p-0031" num="0030">As an example, tests were performed and the following pipeline was applied to wireless channel measurements for which the breathing rate was derived from a NEULOG respiration monitor logger sensor. The sensor is connected to a monitor belt that is attached to the user's ribcage, and records variations in air pressure from the belt at 50 samples per second. CSI and NEULOG data were collected simultaneously during more than 10 captures of one minute each, per each of the locations <b>200</b>, <b>210</b>, <b>230</b>, <b>240</b>, and <b>250</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Captures for different breathing rate, mixed breathing rate were collected and positions such as sitting and lying were used during the validation phase. Estimated rates from CSI data, using the procedure described above, were very close to rates estimated from NEULOG data, with errors below 0.012 Hz (0.72 breathing cycle per minute) and a mean error of 0.0066 Hz (0.40 breathing cycle per minute) over the measured captures, confirming the adequacy of the procedure to estimate breathing rates from CSI. Fourier transforms for NEULOG and CSI data are presented in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0032" num="0031">The procedure described above was applied to compare CSI data in the breathing and in the baseline conditions. Two features were extracted from each capture: the number of components with an amplitude peak that matched the most frequent one (over 1392 components), and the average amplitude of these peaks for the selected components only. As can be seen in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, captures for the breathing and baseline conditions can be distinguished using the method proposed herein.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates components of the system. The wireless access point <b>702</b> is a Wi-Fi access point. The Wi-Fi access point includes but not limited to an IEEE 802.11ac or 802.11n or above access points. The wireless transceiver of the wireless access point <b>702</b> is in communication with the further stationary device over a corresponding further one of the at least one radio frequency communication link. The wireless access point <b>702</b> is configured to record a further channel state information data set for the further one of the at least one radio frequency communication link at a corresponding time. In an embodiment, the determining the activity of the user in the environment includes determining the activity of the user in the environment based on a comparison of the further channel state information data set to each of the at least one channel state information profile of each of the plurality of activity profiles. In an embodiment, the activity is determined based on a sum of a similarity measurement of the channel state information data set and a similarity measurement of the further channel state information data set. A central processing unit (CPU) <b>704</b> is the electronic circuitry within a computer that carries out the instructions of a computer program by performing the basic arithmetic, logic, controlling and input/output (I/O) operations specified by the instructions. A graphics processing unit (GPU) <b>706</b> is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs <b>706</b> are used in embedded systems, mobile phones, personal computers, workstations, and game consoles. Modern GPUs manipulate computer graphics and image processing and are used for algorithms that process large blocks of data in parallel. A digital signal processor (DSP) <b>708</b> is a specialized microprocessor (or a SIP block), with its architecture optimized for the operational needs of digital signal processing. The DSP <b>708</b> measures, filters or compresses continuous real-world analog signals. An application program interface (API) <b>710</b> is a set of routines, protocols, and tools for building software applications. The API <b>710</b> specifies how software components should interact. Additionally, APIs <b>710</b> are used when programming graphical user interface (GUI) components. The API <b>710</b> provides access to the channel state data to the agent <b>714</b>. An access point compliant with either 802.11ac or 802.11n or above access point, allow a device to have multiple radio <b>712</b> antennas. Multiple radio <b>712</b> antennas enable the equipment to focus on the far end device, reducing interference in other directions, and giving a stronger useful signal. This greatly increases range and network speed without exceeding the legal power limits.</p><p id="p-0034" num="0033">An agent <b>714</b> is configured to collect data from the Wi-Fi chipset, filter the incoming data then feed and pass the data to the cloud <b>720</b> for activity identification. Depending on the configuration, the activity identification can be done on the edge, at the agent level, or in the cloud, or some combination of the two. A local profile database <b>716</b> is utilized when at least a portion of the activity identification is done on the edge. This could be a simple motion/no-motion determination profile, or a plurality of profiles for identifying activities, objects, individuals, biometrics, etc. An activity identification module <b>718</b> distinguishes between walking activities and in-place activities. In general, a walking activity causes significant pattern changes of the impulse or frequency response of a channel over time for both amplitude and phase, since walking involves significant body movements and location changes. In contrast, an in-place activity (such as watching TV on a sofa) only involves relative smaller body movements that will be captured through small distortions on magnitude and/or phase of CSI.</p><p id="p-0035" num="0034">A cloud <b>720</b> analyzes and creates profiles describing various activities. A profile database <b>722</b> is utilized when at least a portion of the activity identification is done in the cloud <b>720</b>. This could be a simple motion/no-motion determination profile, or a plurality of profiles for identifying activities, objects, individuals, biometrics, etc. A device database <b>724</b> that stores the device ID of all connected wireless access points. A profile module <b>726</b> monitors the data set resulting from continuous monitoring of a target environment, to identify multiple similar instances of an activity without a matching profile in such a data set, combine that data with user feedback to label the resulting clusters to define new profiles that are then added to the profile database <b>722</b>.</p><p id="p-0036" num="0035">An activity engagement module <b>728</b> monitors the output of the activity identification module <b>718</b> in order to populate the activity/engagement database <b>734</b> with the user activity level (such as pulse or respiration rate) and engagement level (such as the amount of time the user spends watching a TV program, reading a book, engaged in conversation, etc.) over time. A cycle module <b>730</b> performs linear operations on (convolution correlation) the data (graphs) of activity levels against the data (graphs) of engagement levels (relaxation/TV, thinking/reading, etc.) for different time periods from the data in the activity/engagement database <b>734</b>. For example, the patterns over the course of a week or every day or every Monday, in order to identify at what cycle length there are noticeable patterns for the correlation between activity level and engagement level.</p><p id="p-0037" num="0036">An alert module <b>732</b> compares the current activity and engagement levels to the cycles identified by the cycle module <b>730</b>. When the user deviates from the identified cycles, the alert score of the user may be affected. For example, a user has a pattern of being engaged with the TV every Sunday afternoon, and that engagement is correlated with increased respiration rate. The alert module <b>732</b> may see that there is an identified cycle for the current time on Sunday afternoon, determine if the user is engaged (e.g., watching TV) and if the activity level (e.g., respiration rate) is what may be expected based upon the identified cycle, and when there is a deviation add points to an alert score. This is done because a single variation from a pattern of behavior does not necessarily indicate an issue, but a series of deviations from identified patterns put together cumulatively is cause for concern when any one of the individual incidents may not prompt an alert from current monitoring systems. The activity/engagement database <b>734</b> stores the activity level of the observed user (such as respiration rate, pulse rate, etc.) along with the user level of intellectual or emotion engagement (such as watching TV, reading a book, engaged in conversation, etc.) over time. A cycle database <b>736</b> store the cycles identified by the cycle module. Cycles are time periods over which the same pattern of engagement and activity level are shown to repeat. An alert database <b>738</b> stores the current alert score of the user being observed along with the rules for who to contact in the event the observed user alert score exceeds a defined threshold(s). The alert database <b>738</b> contains the user current alert score, as calculated by the alert module <b>732</b>, and the actions that need to be taken based upon the current alert score. Table 1 (provided below) illustrates an exemplary alert database <b>738</b>.</p><p id="p-0038" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="1"><colspec colname="1" colwidth="217pt" align="center"/><thead><row><entry namest="1" nameend="1" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry namest="1" nameend="1" align="center" rowsep="1"/></row><row><entry>Current Alert Score 1.5</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="2"><colspec colname="1" colwidth="70pt" align="center"/><colspec colname="2" colwidth="147pt" align="left"/><tbody valign="top"><row><entry>Alert Score</entry><entry>Action</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row><row><entry>&#x3e;2</entry><entry>Send report to caregiver; send IOT message to</entry></row><row><entry/><entry>confirm condition of user</entry></row><row><entry>&#x3e;3</entry><entry>Send alert to caregiver</entry></row><row><entry namest="1" nameend="2" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0039" num="0037">Functioning of the activity engagement module <b>728</b> is explained with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>. The activity engagement module may constantly be monitoring the activity identification module <b>718</b> for new identified activities in step <b>800</b>. New identified activities may be received in step <b>810</b>. The current activity level (such as pulse or respiration rate) may be identified in step <b>820</b>. The current activity level may be logged for this exemplary respiration rate in the activity/engagement database <b>734</b> in step <b>830</b>. The activity defined by the activity identification module <b>718</b> may be used to determine if the user is currently engaged. For the purposes of this application, &#x201c;engaged&#x201d; may be inclusive of both emotional and intellectual engagement. Certain activities in the profile database <b>722</b> are classified as emotionally or intellectually engaging (such as watching TV, reading a book, engaging in conversation with others, etc.) in step <b>840</b>. The engagement level may be determined for the current time slice. In this example, the percentage of a given time slice (e.g., five minutes) is taken and the percentage of time the user is engaged in the activity of watching TV is used as the engagement level. For example, the user was watching TV for four out of the five minutes in a time slice. The engagement level for that time slice may be identified as 80% in step <b>850</b>. The current engagement level may be logged in the activity/engagement database <b>734</b>. The method may return to monitoring the activity identification module <b>718</b> in step <b>860</b>.</p><p id="p-0040" num="0038">Functioning of the cycle module <b>730</b> is explained with reference to <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The process begins when the system is installed, and the cycle module <b>730</b> begins querying the activity/engagement database <b>734</b> for activity data about the user, such as pulse or respiration rate, as well as emotional or intellectual engagement levels of the user, compiled over time by the activity engagement module <b>728</b> in step <b>900</b>. The minimum sample size has been met to begin to identify cycles. Depending upon the application, the minimum sample size could vary widely. For example, a more immobile user may stick to a more strict routine than an independent user.</p><p id="p-0041" num="0039">The system may need a larger sample for the more independent user who is likely to have a greater variety of activities in the routine in step <b>910</b>. The activity level over time is compared against the engagement level over time for as many different time periods are feasible across the provided sample using at least one statistical means, such as convolution, linear regression, etc. For example, if the minimum sample size was one month, each week could be looked at, or each weekday, or each Monday, or from 1:00 pm to 5:00 pm every Sunday. In this fashion, cycles in the user engagement and activity patterns can be detected and used to identify when a user is breaking routine in step <b>920</b>. Of the resulting functions from step <b>920</b>, those that are above the predetermined threshold for statistical significance may be identified (e.g., two function convoluted against each other where the resulting function is at least 80% of a step function) in step <b>930</b>. The functions identified in step <b>930</b> may be written to the cycle database <b>736</b> in step <b>940</b>. The activity/engagement database <b>734</b> may be monitored for new data, and the method may return to step <b>920</b> to identify new cycles in step <b>950</b>.</p><p id="p-0042" num="0040">Functioning of the alert module <b>732</b> is explained with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. The alert module <b>732</b> is constantly polling the cycle database <b>736</b> for newly identified cycles, specifically the times of the identified cycles in step <b>1000</b>. The alert module <b>732</b> polls the system's internal clock to determine the time. The current time may be compared to the time windows of all identified cycles in the cycle database <b>736</b>. The method may return to polling the clock if the current time does not match in step <b>1010</b>. If the current time does match an identified cycle, the user is currently engaged (e.g., watching TV) as was expected by the cycle in step <b>1020</b>. If the user is not engaged as expected, one may be added to the user alert score in the alert database <b>738</b> in step <b>1030</b>. If the user is engaged as expected, the user activity level (e.g., respiration rate) is within one standard deviation of the respiration rate expected by the cycle in step <b>1040</b>. If the user activity level is outside of the expected range, 0.5 may be added to the user alert score in the alert database <b>738</b> in step <b>1050</b>. Parties may be notified per the rules in the alert database <b>738</b> based upon the user current alert score. For example, the user may not be engaged in the first observed cycle (adding one to the user's alert score), but the user was engaged in the second cycle with a respiration rate that was lower than normally seen in the identified cycle (adding 0.5 to the alert score), while the third cycle shows the user was again not engaged (adding one to the alert score). This brings the exemplary user alert score over the first threshold of 2 in the alert database. The third time being off-cycle may trigger a report to be sent to designated recipients (e.g., caregiver(s) of the user) indicating that the user is deviating from the routines. Optionally, a message may be sent to the recipient via an IoT device in the home to check on the subject user. During the fourth cycle, the user may be engaged, but again have a respiratory rate below the expected level (adding 0.5 to the alert score) and triggering the second threshold of responses in the alert database <b>738</b>. An alert may be sent to the caregiver(s) in step <b>1060</b>. In step <b>1070</b>, the alert module <b>732</b> determines if the user alert score has reached 3. Once the alert score has reached 3 (or other chosen threshold for caregiver intervention), the alert score is reset to 0 at step <b>1080</b>.</p><p id="p-0043" num="0041">One skilled in the art may appreciate that, for the processes and methods disclosed herein, the functions performed in the processes and methods may be implemented in differing order. Furthermore, the outlined steps and operations are only provided as examples, and some of the steps and operations may be optional, combined into fewer steps and operations, or expanded into additional steps and operations without detracting from the essence of the disclosed embodiments.</p><p id="p-0044" num="0042">The foregoing detailed description of the technology has been presented for purposes of illustration and description. It is not intended to be exhaustive or to limit the technology to the precise form disclosed. Many modifications and variations are possible in light of the above teaching. The described embodiments were chosen in order to best explain the principles of the technology, its practical application, and to enable others skilled in the art to utilize the technology in various embodiments and with various modifications as are suited to the particular use contemplated. It is intended that the scope of the technology be defined by the claim.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for detecting a user, the method comprising:<claim-text>generating a baseline condition based on one or more captures by one or more connected devices in a sensing area when the user is not present in the sensing area;</claim-text><claim-text>receiving activity data from the one or more connected devices in the sensing area over a period of time based on a channel frequency response between the one or more connected devices indicating biorhythmic patterns associated with the user;</claim-text><claim-text>determining a routine of the user based on the activity data over the period of time;</claim-text><claim-text>detecting one or more deviations from the routine; and</claim-text><claim-text>sending an alert to a device associated with a designated recipient when the one or more deviations reaches a predefined threshold.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising detecting spectral components in a micro-motion frequency range of the sensing area.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising detecting a breathing rate of the user based on analyzing the biorhythmic patterns.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising determining an engagement level of the user during a subset period of time based on the activity data, wherein the engagement level is associated with intellectual or emotional engagement.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising comparing the baseline condition to the channel frequency response to identify the biorhythmic patterns that indicate that the user is present in the sensing area.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the biorhythmic patterns further indicates a pulse rate of the user.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising identifying one or more cycles corresponding to one or more time periods where the activity data repeats, wherein the routine is determined based on the cycles.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the routine is determined based on a minimum sample time period.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A system for detecting a user, the system comprising:<claim-text>one or more connected devices located in a sensing area; and</claim-text><claim-text>a cloud server that:<claim-text>generates a baseline condition based on one or more captures by the one or more connected devices in the sensing area when the user is not present in the sensing area;</claim-text><claim-text>receives activity data from the one or more connected devices in the sensing area over a period of time based on a channel frequency response between the one or more connected devices indicating biorhythmic patterns associated with the user;</claim-text><claim-text>determines a routine of the user based on the activity data over the period of time;</claim-text><claim-text>detects one or more deviations from the routine; and</claim-text><claim-text>sends an alert to a device associated with a designated recipient when the one or more deviations reaches a predefined threshold.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the cloud server further detects spectral components in a micro-motion frequency range of the sensing area.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the cloud server further detects a breathing rate of the user based on analyzing the biorhythmic patterns.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the cloud server further determines an engagement level of the user during a subset period of time based on the activity data, wherein the engagement level includes intellectual or emotional engagement.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the cloud server further compares the baseline condition to the channel frequency response to identify the biorhythmic patterns that indicate that the user is present in the sensing area.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the biorhythmic patterns further indicates a pulse rate of the user.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the cloud server further identifies one or more cycles corresponding to one or more time periods where the activity data repeats, wherein the routine is determined based on the cycles.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. A non-transitory, computer-readable storage medium having embodied thereon instructions executable by one or more processors to perform a method for detecting a user, the method comprising:<claim-text>generating a baseline condition based on one or more captures by one or more connected devices in a sensing area when the user is not present in the sensing area;</claim-text><claim-text>receiving activity data from the one or more connected devices in the sensing area over a period of time based on a channel frequency response between the one or more connected devices indicating biorhythmic patterns associated with the user;</claim-text><claim-text>determining a routine of the user based on the activity data over the period of time;</claim-text><claim-text>detecting one or more deviations from the routine; and</claim-text><claim-text>sending an alert to a device associated with a designated recipient when the one or more deviations reaches a predefined threshold.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory, computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, the method further comprising:<claim-text>detecting spectral components in a micro-motion frequency range of the sensing area.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory, computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, the method further comprising determining an engagement level of the user during a subset period of time based on the activity data, wherein the engagement level is associated with intellectual or emotional engagement.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory, computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, the method further comprising comparing the baseline condition to the channel frequency response to identify the biorhythmic patterns that indicate that the user is present in the sensing area.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory, computer-readable storage medium of <claim-ref idref="CLM-00016">claim 16</claim-ref>, the method further comprising identifying one or more cycles corresponding to one or more time periods where the activity data repeats, wherein the routine is determined based on the cycles.</claim-text></claim></claims></us-patent-application>