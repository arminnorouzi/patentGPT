<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003884A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003884</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17777337</doc-number><date>20191119</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>42</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>42</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TARGET OBJECT DETECTION APPARATUS, TARGET OBJECT DETECTION METHOD, AND NON-TRANSITORY COMPUTER-READABLE STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>NEC Corporation</orgname><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>NOMURA</last-name><first-name>Toshiyuki</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>NEC Corporation</orgname><role>03</role><address><city>Minato-ku, Tokyo</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/JP2019/045243</doc-number><date>20191119</date></document-id><us-371c12-date><date>20220517</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A target object detection apparatus (<b>20</b>) includes an image generation unit (<b>220</b>) that generates, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction; an area detection unit (<b>230</b>) that detects, from the two-dimensional image, each of at least two detection areas of detection target objects recognized by using at least two recognition means; and an identification unit (<b>240</b>) that identifies the detection target object, based on a positional relationship between the detected at least two detection areas.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="151.89mm" wi="101.52mm" file="US20230003884A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="114.47mm" wi="135.04mm" file="US20230003884A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="137.67mm" wi="128.35mm" file="US20230003884A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="113.28mm" wi="134.87mm" file="US20230003884A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="93.73mm" wi="130.30mm" file="US20230003884A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="161.04mm" wi="129.03mm" file="US20230003884A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="98.38mm" wi="203.71mm" file="US20230003884A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="187.28mm" wi="131.57mm" orientation="landscape" file="US20230003884A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="154.35mm" wi="99.23mm" file="US20230003884A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="144.27mm" wi="134.28mm" file="US20230003884A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="138.35mm" wi="101.85mm" file="US20230003884A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="135.97mm" wi="101.77mm" file="US20230003884A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="204.72mm" wi="104.31mm" orientation="landscape" file="US20230003884A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="140.12mm" wi="134.87mm" file="US20230003884A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="119.13mm" wi="99.65mm" file="US20230003884A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="202.69mm" wi="91.52mm" orientation="landscape" file="US20230003884A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="147.57mm" wi="132.50mm" file="US20230003884A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="205.82mm" wi="98.13mm" orientation="landscape" file="US20230003884A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="138.18mm" wi="102.62mm" file="US20230003884A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="138.18mm" wi="103.29mm" file="US20230003884A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="107.95mm" wi="121.84mm" file="US20230003884A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="173.48mm" wi="117.35mm" file="US20230003884A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present invention relates to a target object detection apparatus, a target object detection method, and a program.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">In a facility such as an airport, carrying-in of a specific article may be restricted. In such a facility, in many cases, person's belongings are inspected in an entrance gate to the facility, or on a passageway to the facility. As techniques relating to the inspection, there are apparatuses described in Patent Documents 1 and 2. Patent Document 1 describes that an image is generated by receiving a millimeter-wave radiated from a person. Patent Document 2 also describes that an image is generated by irradiating a microwave on a person from three directions, and analyzing a reflection wave of the microwave.</p><heading id="h-0003" level="1">RELATED DOCUMENTS</heading><heading id="h-0004" level="1">Patent Documents</heading><p id="p-0004" num="0003">[Patent Document 1] Japanese Patent Application Publication No. 2003-177175</p><p id="p-0005" num="0004">[Patent Document 2] United States Patent Application Publication No. 2016/0216371</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><heading id="h-0006" level="1">Technical Problem</heading><p id="p-0006" num="0005">When a reflection wave of an electromagnetic wave irradiated on an inspection target is analyzed, a three-dimensional shape of the inspection target, such as a person, and an accompanying object of the inspection target (for example, person's belongings) can be estimated. When three-dimensional information of the reflection wave of the electromagnetic wave is converted into a two-dimensional image in order to reduce a load of processing, there is a case where, even when a controlled article is to be detected as a detection target object by image recognition processing, the controlled article cannot be detected or is erroneously detected. Thus, there is a possibility that detection accuracy of the target object lowers.</p><p id="p-0007" num="0006">An object of the present invention is to improve detection accuracy and detection efficiency when detecting a target object by irradiating an electromagnetic wave on the target object and analyzing a reflection wave of the electromagnetic wave.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0008" num="0007">In aspects of the present invention, in order to solve the above problem, the following configurations are adopted.</p><p id="p-0009" num="0008">A first aspect relates to a target object detection apparatus.</p><p id="p-0010" num="0009">The target object detection apparatus according to the first aspect includes:</p><p id="p-0011" num="0010">an image generation unit that generates, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</p><p id="p-0012" num="0011">an area detection unit that detects, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</p><p id="p-0013" num="0012">an identification unit that identifies the detection target object, based on a positional relationship between the detected at least two detection areas.</p><p id="p-0014" num="0013">A second aspect relates to a target object detection method executed by at least one computer.</p><p id="p-0015" num="0014">The target object detection method according to the second aspect includes:</p><p id="p-0016" num="0015">by a target object detection apparatus,</p><p id="p-0017" num="0016">generating, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</p><p id="p-0018" num="0017">detecting, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</p><p id="p-0019" num="0018">identifying the detection target object, based on a positional relationship between the detected at least two detection areas.</p><p id="p-0020" num="0019">Note that, another aspect of the present invention may be a program that causes at least one computer to execute the method of the above second aspect, or may be a computer-readable storage medium that stores the program. The storage medium includes a non-transitory tangible medium.</p><p id="p-0021" num="0020">The computer program includes a computer program code that causes a computer to implement the target object detection method on the target object detection apparatus, when the computer program is executed by the computer.</p><p id="p-0022" num="0021">Note that, any combination of the above-described structural elements, and expression of the present invention, which are converted among a method, an apparatus, a system, a storage medium, a computer program and the like, are also valid as a mode of the present invention.</p><p id="p-0023" num="0022">Additionally, it is not always necessary that the various structural elements of the present invention are mutually independent entities, and a plurality of structural elements may be formed as one member, one structural element may be formed of a plurality of members, a certain structural element may be a part of another structural element, a part of a certain structural element and a part of another structural element may overlap, or the like.</p><p id="p-0024" num="0023">Additionally, in the method and the computer program of the present invention, a plurality of procedures are sequentially described, but the described order does not restrict an order of execution of procedures. Thus, when the method and the computer program of the present invention are implemented, the order of the plurality of procedures can be changed as long as no problem arises in a content of the procedures.</p><p id="p-0025" num="0024">Furthermore, the plurality of procedures of the method and the computer program of the present invention may not be restricted to execute at mutually different timings. Thus, while a certain procedure is being executed, another procedure may occur, an execution timing of a certain procedure may partly or entirely overlap an execution timing of another procedure, or the like.</p><heading id="h-0008" level="1">Advantageous Effects of Invention</heading><p id="p-0026" num="0025">According to the present invention, detection accuracy and detection efficiency is improved when detecting a target object by irradiating an electromagnetic wave on the target object and analyzing a reflection wave of the electromagnetic wave.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0009" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0027" num="0026">The above-described object, other objects, features and advantageous effects will become clearer by preferred example embodiments to be described below, and the following accompanying drawings.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram describing a usage environment of a target object detection apparatus according to an example embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating one example of a functional configuration of an irradiation apparatus.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating one example of a functional configuration of the target object detection apparatus.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating a hardware configuration of the target object detection apparatus.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating one example of an operation of the target object detection apparatus.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a functional block diagram illustrating a configuration example of the target object detection apparatus of the present example embodiment.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram for describing a difference between detection areas when a detection target object is recognized from a two-dimensional image viewed from different directions.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for describing an estimation area estimated by an estimation unit.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram for describing an identification method of an identification unit.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a flowchart illustrating procedures for identification processing of the identification unit.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a flowchart illustrating procedures for identification processing of the identification unit.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus of a modification of a second example embodiment.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a functional block diagram illustrating a configuration example of the target object detection apparatus of the present example embodiment.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart illustrating an operation example of the target object detection apparatus.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a functional block diagram illustrating a configuration example of the target object detection apparatus of the present example embodiment.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a diagram for describing an identification method of a first detection target object and a second detection target object by the target object detection apparatus.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus of a modification of a fifth example embodiment.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a flowchart illustrating procedures for identification processing of an identification unit.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flowchart illustrating procedures for identification processing of the identification unit.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram for describing an example of a computation method of distance information in an image generation unit.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram for describing an example of an estimation method of an estimation area in an estimation unit.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0010" level="1">DESCRIPTION OF EMBODIMENTS</heading><p id="p-0049" num="0048">Hereinafter, example embodiments of the present invention will be described with reference to the accompanying drawings. Note that, in all drawings, a similar structural element is denoted by a similar reference sign, and a description is omitted unless where necessary.</p><p id="p-0050" num="0049">In the example embodiments, &#x201c;acquisition&#x201d; includes at least one of acquisition (active acquisition) in which an own apparatus fetches data or information stored in another apparatus or a storage medium, and input (passive acquisition) in which an own apparatus inputs data or information being output from another apparatus. Examples of the active acquisition include issuing a request or an inquiry to another apparatus and receiving a response thereto, accessing another apparatus or a storage medium and reading, and the like. In addition, examples of the passive acquisition include reception of information being delivered (or transmitted, push-notified, or the like), and the like. Further, the &#x201c;acquisition&#x201d; may be acquisition by selection from received data or information, or may be selective reception of delivered data or information.</p><heading id="h-0011" level="1">First Example Embodiment</heading><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram describing a usage environment of a target object detection apparatus <b>20</b> according to an example embodiment. The target object detection apparatus <b>20</b> is used together with an irradiation apparatus <b>10</b> and a display apparatus <b>30</b>.</p><p id="p-0052" num="0051">The irradiation apparatus <b>10</b> irradiates an electromagnetic wave on an inspection target such as a passerby, and receives a reflection wave of the electromagnetic wave, which is reflected by the inspection target. Further, the irradiation apparatus <b>10</b> generates an intermediate-frequency signal (IF signal) by frequency-converting the received reflection wave into an intermediate-frequency band.</p><p id="p-0053" num="0052">As an electromagnetic wave irradiated by the irradiation apparatus <b>10</b>, it is preferable to use an electromagnetic wave having a wavelength being transmitted through cloth (for example, clothing) but reflected by an inspection target itself (for example, a human body) or an accompanying object of the inspection target. In one example, an electromagnetic wave is a microwave, a millimeter-wave, or a terahertz-wave, and have a wavelength of equal to or more than 30 micrometers and equal to or less than one meter. Note that, in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a horizontal direction of a plane of irradiation of an electromagnetic wave by the irradiation apparatus <b>10</b> is an x direction, a vertical direction (up-and-down direction) of the plane is a y direction, and a direction of irradiation of an electromagnetic wave is a z direction. In other words, from a viewpoint of the inspection target, a direction of movement of the inspection target is approximately the x direction, an up-and-down direction is the y direction, and a direction substantially perpendicular to the direction of movement of the inspection target is the z direction. The irradiation apparatus <b>10</b> executes, at a plurality of timings, processing of generating the above-described IF signal. The irradiation apparatus <b>10</b> executes the processing of generating the above-described IF signal, for example, ten or more times (preferably 20 or more times) per second.</p><p id="p-0054" num="0053">In an example illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the irradiation apparatus <b>10</b> is disposed in parallel (180 degrees) to a passageway <b>40</b> of an inspection target, but the irradiation apparatus <b>10</b> may be disposed to have an angle other than 180 degrees to the passageway <b>40</b>.</p><p id="p-0055" num="0054">The target object detection apparatus <b>20</b> acquires an IF signal from the irradiation apparatus <b>10</b>, processes the IF signal, and thereby generates three-dimensional position information indicating a three-dimensional shape of at least a part of an inspection target. The three-dimensional position information includes information for determining each of a distance from a part (reflection point) of an inspection target, on which an electromagnetic wave is irradiated, to the irradiation apparatus <b>10</b>, and an angle of the reflection point at a time when the irradiation apparatus <b>10</b> (for example, an antenna (not illustrated) that a reception unit <b>130</b> includes) is set as a reference. The distance determined by the three-dimensional position information may be, for example, a distance from a transmission antenna (not illustrated) that a transmission unit <b>110</b> to be described later includes, to a target part, may be a distance from a reception antenna that the reception unit <b>130</b> includes to the target tart, or may be an average value of these distances.</p><p id="p-0056" num="0055">Note that, it is preferable that the three-dimensional position information also includes information of intensity of a reflection wave at each position. When an inspection target has an accompanying object (for example, belongings), the three-dimensional position information serves also as information for determining a three-dimensional shape of at least a part of the accompanying object.</p><p id="p-0057" num="0056">The target object detection apparatus <b>20</b> determines presence/absence of an accompanying object, and causes the display apparatus <b>30</b> to display information indicating a determination result. Note that, a detection target of the target object detection apparatus <b>20</b> is not limited to the above-described accompanying object.</p><p id="p-0058" num="0057">In addition, the target object detection apparatus <b>20</b> generates, where necessary, a two-dimensional or three-dimensional image of an inspection target, and causes the display apparatus <b>30</b> to display the image. When the inspection target has an accompanying object, the image also includes the accompanying object.</p><p id="p-0059" num="0058">Note that, the irradiation apparatus <b>10</b> is also provided with a sensor (for example, a human sensor) that detects an inspection target coming into an irradiation target area, and a sensor (for example, a human sensor) that detects an inspection target going out of the irradiation target area. The irradiation apparatus <b>10</b> or the target object detection apparatus <b>20</b> can determine a plurality of IF signals associated with an identical inspection target by using detection results of these sensors. Detection of an inspection target may be performed by using a received reflection wave, instead of the sensor.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating one example of a functional configuration of the irradiation apparatus <b>10</b>. In an example illustrated in the present figure, the irradiation apparatus <b>10</b> includes the transmission unit <b>110</b>, a control unit <b>120</b>, the reception unit <b>130</b>, and a data transfer unit <b>140</b>.</p><p id="p-0061" num="0060">The transmission unit <b>110</b> irradiates an electromagnetic wave toward an area (hereinafter, referred to as &#x201c;irradiation area&#x201d;) through which an inspection target passes. The transmission unit <b>110</b> includes, for example, an omnidirectional antenna. The transmission unit <b>110</b> can change a frequency of an electromagnetic wave within a fixed range. The transmission unit <b>110</b> is controlled by the control unit <b>120</b>. The control unit <b>120</b> also controls the reception unit <b>130</b>.</p><p id="p-0062" num="0061">The reception unit <b>130</b> receives a reflection wave from an inspection target. The reception unit <b>130</b> generates an intermediate-frequency signal (IF signal) by frequency-converting a received reflection wave into an intermediate-frequency band. The control unit <b>120</b> executes control to set a proper value to the intermediate-frequency band in the reception unit <b>130</b>.</p><p id="p-0063" num="0062">The data transfer unit <b>140</b> acquires the IF signal generated in the reception unit <b>130</b>, and outputs the acquired IF signal to the target object detection apparatus <b>20</b>. It is preferable that the data transfer unit <b>140</b> also outputs a time at transmission or a time when the IF signal is generated to the target object detection apparatus <b>20</b> (hereinafter, also referred to as &#x201c;time information&#x201d;).</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating one example of a functional configuration of the target object detection apparatus <b>20</b>. The target object detection apparatus <b>20</b> includes an image generation unit <b>220</b>, an area detection unit <b>230</b>, and an identification unit <b>240</b>.</p><p id="p-0065" num="0064">The image generation unit <b>220</b> generates, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction.</p><p id="p-0066" num="0065">The area detection unit <b>230</b> detects, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means. The identification unit <b>240</b> identifies the detection target object, based on a positional relationship between the detected at least two detection areas.</p><p id="p-0067" num="0066">The inspection target is, for example, a person or baggage. The predetermined direction is a certain one direction or two directions. The detection target object includes, for example, a controlled article such as a dangerous weapon or a dangerous article, and object (for example, a body, an arm, and the like) other than the controlled article.</p><p id="p-0068" num="0067">The target object detection apparatus <b>20</b> may further include a storage apparatus <b>250</b> that stores acquired three-dimensional information, a generated two-dimensional image, an identification result, and the like. The storage apparatus <b>250</b> may be the same apparatus as the target object detection apparatus <b>20</b>, or may be a separate apparatus from the target object detection apparatus <b>20</b>. Further, the storage apparatus <b>250</b> may be a plurality of storage apparatuses.</p><p id="p-0069" num="0068">Although the at least two recognition means will be described in detail in an example embodiment to be described later, the at least two recognition means are exemplified below.</p><p id="p-0070" num="0000">(a1) The at least two recognition means are recognition means for generating two two-dimensional images viewed from different directions, and recognizing the detection target object therefrom, and two detection areas are detected from each of the two-dimensional images by the area detection unit <b>230</b>.<br/>(a2) The at least two recognition means are recognition means for recognizing two different detection target objects from one two-dimensional image viewed from a predetermined direction, and two detection areas are detected from the one two-dimensional image by the area detection unit <b>230</b>.</p><p id="p-0071" num="0069">The recognition means of the area detection unit <b>230</b> recognizes a detection target object by collating, by image recognition processing, a two-dimensional image of an object, which is registered in advance as the detection target object and viewed from a predetermined direction, and a two-dimensional image generated by the image generation unit <b>220</b>. In addition, when a method based on machine learning is used as the image recognition processing, a detection target object is recognized by using a model being learned by using, as training data, a two-dimensional image of the detection target object viewed from a predetermined direction.</p><p id="p-0072" num="0070">Although an identification method of a detection target object by the identification unit <b>240</b> will be described in an example embodiment to be described later, the identification method is exemplified below.</p><p id="p-0073" num="0000">(b1) A detection target object is recognized from each of two different two-dimensional images, and it is determined as erroneous detection when positions of objects, which are recognized as an identical detection target object, are different from each other.<br/>(b2) A detection target object is recognized from each of two different two-dimensional images, and it is determined as the detection target object when positions of objects, which are recognized as an identical detection target object, are the same.<br/>(b3) Two different detection target objects are recognized from one two-dimensional image, and the detection target object is identified based on whether a positional relationship between the detection target objects is a relationship being registered in advance. For example, in a case where one detection target object is a dangerous weapon and the other detection target object is an arm, the positional relationship between the two detection target objects is registered in advance, and each detection target object is identified when the positional relationship between the two recognized objects locates as the registered positional relationship.<br/>(b4) Two different detection target objects are recognized from one two-dimensional image, and it is identified as erroneous recognition when positions of two objects recognized as the detection target objects are the same.</p><p id="p-0074" num="0071"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating a hardware configuration of the target object detection apparatus <b>20</b>. The target object detection apparatus <b>20</b> includes a bus <b>1010</b>, a processor <b>1020</b>, a memory <b>1030</b>, a storage device <b>1040</b>, an input/output interface <b>1050</b>, and a network interface <b>1060</b>.</p><p id="p-0075" num="0072">The bus <b>1010</b> is a data transmission path for mutual data transmission/reception among the processor <b>1020</b>, the memory <b>1030</b>, the storage device <b>1040</b>, the input/output interface <b>1050</b>, and the network interface <b>1060</b>. However, a method of mutual connection among the processor <b>1020</b> and the like is not limited to bus connection.</p><p id="p-0076" num="0073">The processor <b>1020</b> is a processor being achieved by a central processing unit (CPU), a graphics processing unit (GPU), or the like.</p><p id="p-0077" num="0074">The memory <b>1030</b> is a main storage apparatus being achieved by a random access memory (RAM) or the like.</p><p id="p-0078" num="0075">The storage device <b>1040</b> is an auxiliary storage apparatus being achieved by a hard disk drive (HDD), a solid state drive (SSD), a memory card, a read only memory (ROM), or the like.</p><p id="p-0079" num="0076">The storage device <b>1040</b> stores a program module achieving each function of the target object detection apparatus <b>20</b> (for example, the acquisition unit <b>210</b>, the image generation unit <b>220</b>, the area detection unit <b>230</b>, and the identification unit <b>240</b>, and an estimation unit <b>260</b> and an output control unit <b>280</b> to be described later, and the like). The processor <b>1020</b> reads each program module in the memory <b>1030</b>, executes each program module, and thereby achieves each function relating to the program module. The storage device <b>1040</b> functions also as various kinds of storage units.</p><p id="p-0080" num="0077">The input/output interface <b>1050</b> is an interface for connecting the target object detection apparatus <b>20</b> and various kinds of input/output equipment (for example, the display apparatus <b>30</b>).</p><p id="p-0081" num="0078">The network interface <b>1060</b> is an interface for connecting the target object detection apparatus <b>20</b> to another apparatus (for example, the irradiation apparatus <b>10</b>) on a network. However, there is a case where the network interface <b>1060</b> is not used.</p><p id="p-0082" num="0079"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating one example of an operation of the target object detection apparatus <b>20</b>. First, the image generation unit <b>220</b> generates, from three-dimensional information, a two-dimensional image of an inspection target viewed from a predetermined direction (step S<b>101</b>). Then, the area detection unit <b>230</b> detects, from the two-dimensional image, a detection target object by using at least two recognition means (step S<b>103</b>). Further, the area detection unit <b>230</b> detects each of at least two detection areas of the detected detection target object (step S<b>105</b>). The identification unit <b>240</b> identifies the detection target object, based on a positional relationship between the detected at least two detection areas (step S<b>107</b>).</p><p id="p-0083" num="0080">According to the present example embodiment, the area detection unit <b>230</b> detects at least two detection areas of a detection target object recognized by using at least two recognition means, and the identification unit <b>240</b> identifies the detection target object, based on a positional relationship between the detected two detection areas. Thereby, occurrence of erroneous detection or detection failure of a detection target object by one recognition means can be suppressed, and detection accuracy of the detection target object can be improved. Moreover, since detection processing is executed by converting three-dimensional information into a two-dimensional image by the image generation unit <b>220</b>, a load of processing can be reduced, and processing efficiency can be improved.</p><heading id="h-0012" level="1">Second Example Embodiment</heading><p id="p-0084" num="0081"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus <b>20</b> of the present example embodiment. The target object detection apparatus <b>20</b> of the present example embodiment includes a configuration that executes area detection of a detection target object with use of the recognition means of the above-described (a1). Specifically, the target object detection apparatus <b>20</b> includes a configuration in which a two-dimensional image <b>222</b><i>a </i>viewed from a predetermined direction (angle A) and a two-dimensional image <b>222</b><i>b </i>viewed from a direction (angle B) being different from the angle A are generated, and a detection target object is identified by using a position (a second detection area <b>232</b><i>b </i>to be described later) of the detection target object recognized by using the two-dimensional image <b>222</b><i>b </i>of the angle B, and a position (an estimation area <b>262</b> to be described later) of the detection target object viewed from the angle B, which is estimated by using the two-dimensional image <b>222</b><i>a </i>of the angle A. When a position of the detection target object viewed from the angle B is estimated by using the two-dimensional image <b>222</b><i>a </i>of the angle A, depth information (distance information <b>224</b> to be described later) of the detection target object at the angle A is used.</p><p id="p-0085" num="0082">The target object detection apparatus <b>20</b> of the present example embodiment includes an acquisition unit <b>210</b>, an image generation unit <b>220</b><i>a</i>, an image generation unit <b>220</b><i>b</i>, an area detection unit <b>230</b><i>a</i>, an area detection unit <b>230</b><i>b</i>, an estimation unit <b>260</b>, and an identification unit <b>240</b>.</p><p id="p-0086" num="0083">The acquisition unit <b>210</b> acquires three-dimensional information <b>212</b> acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, and outputs the acquired three-dimensional information <b>212</b> to the image generation units <b>220</b><i>a </i>and <b>220</b><i>b</i>. The image generation unit <b>220</b><i>a </i>generates, from the three-dimensional information <b>212</b>, a first two-dimensional image <b>222</b><i>a </i>at a time of viewing the inspection target from a first direction (angle A). In addition, the image generation unit <b>220</b><i>a </i>generates distance information <b>224</b> to a reflection point of each of pixels constituting the first two-dimensional image <b>222</b><i>a</i>. The image generation unit <b>220</b><i>b </i>generates, from the three-dimensional information <b>212</b>, a second two-dimensional image <b>222</b><i>b </i>at a time of viewing the inspection target from a second direction (angle B) being different from the first direction (angle A).</p><p id="p-0087" num="0084">The area detection unit <b>230</b><i>a </i>detects, from the first two-dimensional image <b>222</b><i>a</i>, a first detection area <b>232</b><i>a </i>in which the detection target object is estimated to be present. The area detection unit <b>230</b><i>b </i>detects, from the second two-dimensional image <b>222</b><i>b</i>, a second detection area <b>232</b><i>b </i>in which the detection target object is estimated to be present.</p><p id="p-0088" num="0085">With respect to the detection target object viewed from the first direction (angle A), the estimation unit <b>260</b> estimates an area in which the detection target object is present when viewed from the second direction (angle B), based on a detection area detected by the area detection unit <b>230</b><i>a </i>and the distance information <b>224</b> of each position (pixel) of the detection area, and sets the estimated area as an estimation area <b>262</b>. The details of an estimation method of the estimation area <b>262</b> will be described later.</p><p id="p-0089" num="0086">The identification unit <b>240</b> identifies whether to be the detection target object, based on a positional relationship between the estimation area <b>262</b> estimated by the estimation unit <b>260</b> when viewed from either one direction (in this example, angle B) of the first direction (angle A) and the second direction (angle B), and the detection area <b>232</b><i>b </i>detected by the area detection unit <b>230</b><i>b</i>. The details of an identification method of the identification unit <b>240</b> will be described later.</p><p id="p-0090" num="0087"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram for describing a detection result when different objects are recognized from a two-dimensional image <b>222</b>. <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates an example in which a detection target object <b>62</b> accompanies a non-detection target <b>64</b>. <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates an example in which an inspection target <b>66</b> being a non-detection target further accompanies the non-detection target <b>64</b>. Each figure of <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a cross-sectional view at a height y1 of the detection target object <b>62</b>, the non-detection target <b>64</b>, and the inspection target <b>66</b>. In the present figure, a y axis is not illustrated. In addition, each area in an x-y direction and a z-y direction is not illustrated.</p><p id="p-0091" num="0088"><figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> illustrate a range of the first detection area <b>232</b><i>a </i>being detected in regard to the detection target object <b>62</b> and a range of the inspection target <b>66</b> having mutually different shapes, respectively, when the angle A (in this example, a direction parallel to the z axis) is set as a line-of-sight direction. The range of the first detection area <b>232</b><i>a</i>, which is detected by the area detection unit <b>230</b><i>a </i>when the detection target object <b>62</b> in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is viewed from the angle A, is O<sub>A1</sub>(x1, x2). The range of the first detection area <b>232</b><i>a</i>, which is detected by the area detection unit <b>230</b><i>a </i>when the inspection target <b>66</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is viewed from the angle A, is O<sub>A2</sub>(x3, x4). Herein, since the shapes of the detection target object <b>62</b> and the inspection target <b>66</b> are similar when viewed from the angle A, an area O<sub>A1</sub>(x1, x2) of the detection target object <b>62</b> in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> and an area O<sub>A2</sub>(x3, x4) of the inspection target <b>66</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> have the same range.</p><p id="p-0092" num="0089">Thus, despite the detection target object <b>62</b> and the inspection target <b>66</b> actually being objects of different shapes, when identification is executed by using the first detection area <b>232</b><i>a </i>detected by the area detection unit <b>230</b><i>a</i>, the inspection target <b>66</b> is detected as being the detection target object <b>62</b>. In this manner, when an object is to be detected based on a shape viewed from only one direction, there is a possibility of erroneous detection, and there is a possibility that detection accuracy of the detection target object <b>62</b> lowers.</p><p id="p-0093" num="0090">On the other hand, as illustrated in <figref idref="DRAWINGS">FIGS. <b>7</b>C and <b>7</b>D</figref>, the shape of the inspection target <b>66</b>, in a case where the angle B (in this example, a direction parallel to the x axis) being different from the angle A is set as the line-of-sight direction, is partly similar to the shape of the detection target object <b>62</b>. Thus, despite the detection target object <b>62</b> and the inspection target <b>66</b> actually having different shapes, an area O<sub>B2</sub>(z3, z4&#x2032;) of the inspection target <b>66</b>, which is detected by the area detection unit <b>230</b><i>b </i>when viewed from the angle B, has, in some cases, the same value as an area O<sub>B1</sub>(z1, z2) of the detection target object <b>62</b>, which is detected by the area detection unit <b>230</b><i>a</i>. In other words, the inspection target <b>66</b> is erroneously detected as having a shape different from the actual shape.</p><p id="p-0094" num="0091">As a result, in this case, too, despite the two objects, namely the detection target object <b>62</b> and inspection target <b>66</b>, actually being objects of different shapes, the inspection target <b>66</b> is detected as being the detection target object <b>62</b>. In this manner, even when an object is to be detected based on the shape viewed from two directions, there is a possibility of erroneous detection, and there remains a possibility that detection accuracy of the detection target object <b>62</b> lowers.</p><p id="p-0095" num="0092">As described above, depending on a shape of an inspection target, there is a possibility that detection accuracy lowers. Accordingly, in the present example embodiment, the detection accuracy of the detection target object <b>62</b> is improved by using various methods.</p><p id="p-0096" num="0093"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram for describing the estimation area <b>262</b> estimated by the estimation unit <b>260</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, the second detection area <b>232</b><i>b</i>, which is detected by the area detection unit <b>230</b><i>b </i>from the second two-dimensional image <b>222</b><i>b </i>of the inspection target <b>66</b> viewed from the angle B, is O<sub>B2 </sub>(z3, z4&#x2032;). This is narrower than O<sub>RB2 </sub>(z3, z4) (<figref idref="DRAWINGS">FIG. <b>8</b>A</figref>) being a range of the actual inspection target <b>66</b>.</p><p id="p-0097" num="0094">The estimation unit <b>260</b> estimates a range of O<sub>EB2</sub>(z3, z4), as the estimation area <b>262</b> of the inspection target <b>66</b>, from the first detection area <b>232</b><i>a </i>detected by the area detection unit <b>230</b><i>a</i>, and the distance information <b>224</b> (D(x)) of each of pixels (x3 to x4) of the first detection area <b>232</b><i>a</i>, which is generated by the image generation unit <b>220</b><i>a</i>. The estimation area <b>262</b> is different from the range of O<sub>B2 </sub>(z3, z4&#x2032;) of the second detection area <b>232</b><i>b </i>detected from the second two-dimensional image <b>222</b><i>b </i>with the angle B being set as the line-of-sight direction in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref>, and a value substantially equal to O<sub>RB2</sub>(z3, z4) of the range of the actual inspection target <b>66</b> is acquired. In this manner, the shape of the inspection target <b>66</b> is correctly detected by the estimation unit <b>260</b>.</p><p id="p-0098" num="0095">Next, the details of the identification method of the identification unit <b>240</b> are described. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a diagram for describing the identification method of the identification unit <b>240</b>, and illustrates a positional relationship between the detection area <b>232</b> and the estimation area <b>262</b>.</p><p id="p-0099" num="0096">The positional relationship between the detection area <b>232</b> and the estimation area <b>262</b> is indicated by a ratio R between an area AI of a region <b>270</b> in which the two areas overlap, and an area AU of a region <b>272</b> in which the two areas are combined (an equation (1)). In other words, a degree of agreement between the two areas is indicated by the ratio R.</p><p id="p-0100" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>R=AI/AU</i>&#x2003;&#x2003;Equation (1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0101" num="0097"><figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref> are flowcharts illustrating procedures for identification processing of the identification unit <b>240</b>.</p><p id="p-0102" num="0098">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the identification unit <b>240</b> computes the ratio R (step S<b>201</b>). Then, when the ratio R is more than a first threshold (for example, 0.7) (YES in step S<b>203</b>), the identification unit <b>240</b> identifies a detected object as being the detection target object <b>62</b> (step S<b>205</b>).</p><p id="p-0103" num="0099">Next, as illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the identification unit <b>240</b> computes the ratio R (step S<b>201</b>). Then, when the ratio R is less than a second threshold (for example, 0.3) (YES in step S<b>213</b>), the identification unit <b>240</b> identifies the detected object as not being the detection target object <b>62</b> (in other words, erroneous detection), and the detection target object <b>62</b> is not detected (step S<b>215</b>).</p><p id="p-0104" num="0100">The flowchart in <figref idref="DRAWINGS">FIG. <b>10</b></figref> and the flowchart in <figref idref="DRAWINGS">FIG. <b>11</b></figref> can also be combined. After computing the ratio R in step S<b>201</b>, the identification unit <b>240</b> may execute the identification processing by using the first threshold in step S<b>203</b>, and execute the identification processing by using the second threshold in step S<b>213</b> with respect to the detected object being identified as not being the detection target object <b>62</b> (NO in step S<b>203</b>). Alternatively, the identification of step S<b>213</b> may be executed prior to the identification of step S<b>203</b>.</p><p id="p-0105" num="0101">Various methods are conceivable for a computation method of the distance information <b>224</b> in the image generation unit <b>220</b> and the estimation method of the estimation area <b>262</b> by the estimation unit <b>260</b>, and the details of such methods will be described later.</p><p id="p-0106" num="0102">According to the present example embodiment, the estimation unit <b>260</b> estimates, from the first detection area <b>232</b><i>a </i>of the inspection target <b>66</b> with a predetermined direction (for example, angle A) being set as the line-of-sight direction, the estimation area <b>262</b> with another direction (for example, angle B) different from the angle A being set as the line-of-sight direction, and the identification unit <b>240</b> identifies whether the inspection target <b>66</b> is the detection target object <b>62</b>, based on the positional relationship between the estimation area <b>262</b> and the second detection area <b>232</b><i>b</i>. Besides, the positional relationship between the estimation area <b>262</b> and the second detection area <b>232</b><i>b </i>is identified by the identification unit <b>240</b>, based on the degree of agreement of the areas.</p><p id="p-0107" num="0103">In this manner, since the estimation unit <b>260</b> can estimate, based on distance information of a detection area in one line-of-sight direction, the other area, and the identification unit <b>240</b> can comprehensively make identification, based on the detected area and the estimated area, identification accuracy of the detection target object <b>62</b> can be improved even when the inspection target <b>66</b> has a shape being partly similar to a shape of the detection target object <b>62</b>. Moreover, since the positional relationship between the areas can be identified based on the degree of agreement of the areas, the detection target object <b>62</b> can efficiently be identified.</p><heading id="h-0013" level="1">Modification of the Second Example Embodiment</heading><p id="p-0108" num="0104"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus <b>20</b> of a modification of the second example embodiment. The target object detection apparatus <b>20</b> includes an estimation unit <b>260</b><i>a </i>and an estimation unit <b>260</b><i>b </i>in place of the estimation unit <b>260</b> of the target object detection apparatus <b>20</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0109" num="0105">With respect to an object recognized as a detection target object viewed from a first direction (angle A), the estimation unit <b>260</b><i>a </i>estimates, based on a detection area detected by an area detection unit <b>230</b><i>a </i>and distance information <b>224</b> of each position of the detection area, an estimation area <b>262</b><i>a </i>in which a detection target object <b>62</b> being the object recognized as the detection target object, which is viewed from a second direction (angle B), is estimated to be present.</p><p id="p-0110" num="0106">The estimation area <b>262</b><i>b </i>estimates an estimation area <b>262</b><i>b </i>in which the detection target object <b>62</b> viewed from the angle A, which is estimated from a second detection area <b>232</b><i>b </i>detected from a second two-dimensional image <b>222</b><i>b </i>viewed from a second angle (angle B), is estimated to be present.</p><p id="p-0111" num="0107">The identification unit <b>240</b> identifies whether to be the detection target object <b>62</b>, based on a positional relationship between the estimation area <b>262</b><i>a </i>estimated by the estimation unit <b>260</b><i>a </i>and the second detection area <b>232</b><i>b</i>, and a positional relationship between the estimation area <b>262</b><i>b </i>estimated by the estimation unit <b>260</b><i>b </i>and a first detection area <b>232</b><i>a. </i></p><p id="p-0112" num="0108">The similar advantageous effects as in the second example embodiment are acquired, and, moreover, detection accuracy of the detection target object <b>62</b> can be further improved since detection areas <b>232</b> are detected from two-dimensional images <b>222</b> of the detection target object <b>62</b> with the first direction and second direction being set as the line-of-sight directions, the estimation areas <b>262</b>, which are estimated from both detection areas <b>232</b> and in which the detection target object <b>62</b> with the other direction being set as the line-of-sight direction is estimated to be present, are estimated, and identification is comprehensively executed by the identification unit <b>240</b>.</p><heading id="h-0014" level="1">Third Example Embodiment</heading><p id="p-0113" num="0109">A target object detection apparatus <b>20</b> of the present example embodiment is similar to the above example embodiments, except that distance information <b>224</b> of a detection target object <b>62</b> is stored in advance, and that the distance information <b>224</b> is used for estimating an estimation area <b>262</b>. Since the target object detection apparatus <b>20</b> of the present example embodiment includes similar configuration as the target object detection apparatus <b>20</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the target object detection apparatus <b>20</b> is described with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The target object detection apparatus <b>20</b> of the present example embodiment can be combined with at least one of other configurations as long as no inconsistency occurs.</p><p id="p-0114" num="0110">The distance information <b>224</b> of the detection target object <b>62</b> is stored in advance in a storage apparatus <b>250</b>. The estimation unit <b>260</b> estimates the estimation unit <b>260</b> in which the detection target object <b>62</b> is estimated to be present when the detection target object <b>62</b> is viewed from a second direction, also for the distance information <b>224</b> of the detection target object <b>62</b>, which is stored in the storage apparatus <b>250</b>.</p><p id="p-0115" num="0111">According to the present example embodiment, since the estimation unit <b>260</b> can estimate the estimation area <b>262</b> by using the distance information <b>224</b> of the detection target object <b>62</b> being stored in advance, there is no need to use the distance information <b>224</b> generated by an image generation unit <b>220</b>, and therefore a load of computation processing of the estimation area <b>262</b> by the estimation unit <b>260</b> can be reduced, and processing efficiency can be improved. In particular, when a detection area <b>232</b> of the detection target object <b>62</b> is erroneously detected, useless processing can be omitted since there is no need to execute estimation processing of the estimation area <b>262</b> by using the erroneous detection area <b>232</b> and the erroneous distance information <b>224</b>, and detection accuracy of the detection target object <b>62</b> can be improved since the estimation area <b>262</b> can be estimated by using the correct distance information <b>224</b>.</p><heading id="h-0015" level="1">Fourth Example Embodiment</heading><p id="p-0116" num="0112"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus <b>20</b> of the present example embodiment.</p><p id="p-0117" num="0113">The target object detection apparatus <b>20</b> of the present example embodiment is similar to the target object detection apparatus <b>20</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, except that the target object detection apparatus <b>20</b> includes a configuration that controls output of an identification result. The target object detection apparatus <b>20</b> includes an output control unit <b>280</b>, in addition to the configuration of the target object detection apparatus <b>20</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The target object detection apparatus <b>20</b> of the present example embodiment can be combined with at least one of other configurations within as long as no inconsistency occurs.</p><p id="p-0118" num="0114">When the identification unit <b>240</b> makes identification as a detection target object <b>62</b>, the output control unit <b>280</b> outputs the identification result, and when the identification unit <b>240</b> makes identification as not the detection target object <b>62</b>, the output control unit <b>280</b> does not output the identification result.</p><p id="p-0119" num="0115">An output means, by which the output control unit <b>280</b> outputs the identification result, is, for example, a display apparatus <b>30</b>. Alternatively, the identification result may be stored in a storage apparatus <b>250</b>. Alternatively, voice or an alarm sound may be output from a speaker. Information being output can include at least one of information indicating detection of the detection target object <b>62</b>, information indicating the detection target object <b>62</b> being detected, and information indicating date/time of the detection and a location of the detection.</p><p id="p-0120" num="0116"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a flowchart illustrating an operation example of the target object detection apparatus <b>20</b>.</p><p id="p-0121" num="0117">A flow of the present figure is executed after step S<b>107</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In an identification result by the identification unit <b>240</b> in step S<b>107</b>, when a recognized object is identified as being the detection target object <b>62</b> (YES in step S<b>301</b>), a fact that the detected object is the detection target object <b>62</b> is output as the identification result (step S<b>303</b>). On the other hand, when the recognized object is not identified as being the detection target object <b>62</b>, or when the recognized object is identified as not being the detection target object <b>62</b> (NO in step S<b>303</b>), step S<b>303</b> is bypassed and the present processing ends. In other words, in this case, the output control unit <b>280</b> does not output the identification result.</p><p id="p-0122" num="0118">Besides, in another example, when a detection area <b>232</b> is identified as the detection target object <b>62</b> (YES in step S<b>303</b>), the output control unit <b>280</b> may display the detection area <b>232</b> in a two-dimensional image <b>222</b> with emphasis, such as by changing a color of the detection area <b>232</b>, or by displaying an image surrounding the detection area <b>232</b> in a superimposed manner.</p><p id="p-0123" num="0119">In still another example, when a recognized object is not identified as being the detection target object <b>62</b>, or when the recognized object is identified as not being the detection target object <b>62</b> (NO in step S<b>303</b>), the output control unit <b>280</b> may display the detection area <b>232</b> in the two-dimensional image <b>222</b> with emphasis, such as by changing a color of the detection area <b>232</b> to a color different from the above-described detection area <b>232</b> of the detection target object <b>62</b>, or by displaying an image surrounding the detection area <b>232</b> in a superimposed manner, the image being different from the above-described image surrounding the detection image <b>232</b> of the detection target object <b>62</b>. Thereby, a person can also confirm an object being identified as not being the detection target object <b>62</b>.</p><p id="p-0124" num="0120">According to the present example embodiment, the output control unit <b>280</b> can control whether to output an identification result of detection of the detection target object <b>62</b>. For example, the number of pieces of notification information can be reduced to a proper number, by issuing a notification when it is identified that the detection target object <b>62</b> is detected, but by not issuing a notification when it is identified as not the detection target object <b>62</b>. Thereby, a work load of a manager can be reduced. However, where necessary, even when it is identifies as not the detection target object <b>62</b>, a notification may be issued in such a way as to enable a person to make visual confirmation, or confirmation may be performed later. Thereby, detection failure of the detection target object <b>62</b> can be prevented, and erroneous detection can be reduced.</p><heading id="h-0016" level="1">Fifth Example Embodiment</heading><p id="p-0125" num="0121"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus <b>20</b> of the present example embodiment.</p><p id="p-0126" num="0122">The target object detection apparatus <b>20</b> is configured to execute area detection of a detection target object by using the recognition means of the above-described (a2). Specifically, the recognition means recognizes each of two different detection target objects from one two-dimensional image viewed from a predetermined direction.</p><p id="p-0127" num="0123">The target object detection apparatus <b>20</b> includes an acquisition unit <b>210</b>, an image generation unit <b>220</b>, an area detection unit <b>230</b><i>a</i>, an area detection unit <b>230</b><i>c</i>, and the identification unit <b>240</b>.</p><p id="p-0128" num="0124">The acquisition unit <b>210</b> is similar to that of the above example embodiments in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>12</b></figref>. The image generation unit <b>220</b> generates, from three-dimensional information <b>212</b>, a two-dimensional image <b>222</b> of an inspection target viewed from a predetermined direction (for example, angle A). The area detection unit <b>230</b><i>a </i>includes a detection target object detection unit (not illustrated) that detects a first detection target object <b>62</b><i>a</i>. The area detection unit <b>230</b><i>c </i>includes a non-detection target object detection unit (not illustrated) that detects a second detection target object <b>62</b><i>b </i>being different from the first detection target object <b>62</b><i>a. </i></p><p id="p-0129" num="0125">The first detection target object <b>62</b><i>a </i>is, for example, an object of a detection target, and an example of the first detection target object <b>62</b><i>a </i>is an article, carrying-in of which is restricted, including a controlled article such as a dangerous weapon, a dangerous article, and an illegal article. The second detection target object <b>62</b><i>b </i>is, for example, an object other than the detection target, and an example of the second detection target object <b>62</b><i>b </i>includes an article (for example, a body, or an arm) other than a controlled article.</p><p id="p-0130" num="0126">It is preferable that a combination between the first detection target object <b>62</b><i>a </i>and the second detection target object <b>62</b><i>b </i>is, for example, a combination of objects which are mutually associated in positional relationship, such as a &#x201c;gun&#x201d; and a &#x201c;person's arm&#x201d;. Alternatively, a combination between the first detection target object <b>62</b><i>a </i>and the second detection target object <b>62</b><i>b </i>may be, for example, a combination of objects having similar shapes, such as a &#x201c;gun&#x201d; and a &#x201c;person's arm&#x201d;.</p><p id="p-0131" num="0127">The area detection unit <b>230</b><i>a </i>detects a first detection area <b>232</b><i>a </i>of the first detection target object <b>62</b><i>a </i>by using the detection target object detection unit. The area detection unit <b>230</b><i>c </i>detects a third detection area <b>232</b><i>c </i>of the second detection target object <b>62</b><i>b </i>by using the non-detection target object detection unit.</p><p id="p-0132" num="0128">The identification unit <b>240</b> identifies whether to be the first detection target object <b>62</b><i>a</i>, based on a positional relationship between the first detection area <b>232</b><i>a </i>of the detected first detection target object <b>62</b><i>a </i>being detected and the third detection area <b>232</b><i>c </i>detection area of the second detection target object <b>62</b><i>b. </i></p><p id="p-0133" num="0129"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> is a diagram for describing an identification method of the first detection target object <b>62</b><i>a </i>and the second detection target object <b>62</b><i>b </i>by the target object detection apparatus <b>20</b> of the present example embodiment. Herein, the first detection target object <b>62</b><i>a </i>is a gun, and the second detection target object <b>62</b><i>b </i>is a person's arm. The first detection area <b>232</b><i>a </i>of the first detection target object <b>62</b><i>a </i>(gun) is detected by the area detection unit <b>230</b><i>a</i>. On the other hand, the third detection area <b>232</b><i>c </i>(indicated by a dot-and-dash line in the figure) of the second detection target object <b>62</b><i>b </i>(person's arm) is detected from the area detection unit <b>230</b><i>c. </i></p><p id="p-0134" num="0130">For example, since a positional relationship between the first detection target object <b>62</b><i>a </i>(gun) and the second detection target object <b>62</b><i>b </i>(person's arm) does not greatly change, the positional relationship is registered in a storage apparatus <b>250</b> in advance. Thereby, based on the relationship between the detected first detection area <b>232</b><i>a </i>and third detection area <b>232</b><i>c</i>, the identification unit <b>240</b> identifies the first detection target object <b>62</b><i>a </i>(gun) and the second detection target object <b>62</b><i>b </i>(person's arm). However, it suffices that the identification unit <b>240</b> can identify at least the first detection target object <b>62</b><i>a </i>(gun).</p><p id="p-0135" num="0131">According to the present example embodiment, an area of the first detection target object <b>62</b><i>a </i>(for example, a gun) and an area of the second detection target object <b>62</b><i>b </i>each are detected, and, based on a positional relationship between the detected areas, each of the detection target objects <b>62</b> can be identified, and thus, compared to a case of identifying only the first detection target object <b>62</b><i>a </i>(for example, a gun), there is a possibility that erroneous detection or detection failure can be reduced. Therefore, detection efficiency of the detection target object can be improved.</p><heading id="h-0017" level="1">Modification of the Fifth Example Embodiment</heading><p id="p-0136" num="0132"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a functional block diagram illustrating a configuration example of a target object detection apparatus <b>20</b> of a modification of the fifth example embodiment. In this example, the target object detection apparatus <b>20</b> includes an estimation unit <b>260</b> in addition to the configuration of the target object detection apparatus <b>20</b> in <figref idref="DRAWINGS">FIG. <b>15</b></figref>. The estimation unit <b>260</b> is similar to the estimation unit <b>260</b> and the estimation unit <b>260</b><i>a </i>of the target object detection apparatus in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>12</b></figref>.</p><p id="p-0137" num="0133">The estimation unit <b>260</b> estimates an estimation area <b>262</b> of a line-of-sight direction of angle B, in which a detection target object <b>62</b> is estimated to be present, with respect to a first detection area <b>232</b><i>a </i>detected from a first two-dimensional image <b>222</b><i>a </i>of the line-of-sight direction of angle A.</p><p id="p-0138" num="0134"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> is a diagram for describing an identification method of a first detection target object <b>62</b><i>a </i>and a second detection target object <b>62</b><i>b </i>by the target object detection apparatus <b>20</b> of the present example embodiment. Herein, the first detection target object <b>62</b><i>a </i>is a gun, and the second detection target object <b>62</b><i>b </i>is a person's arm. When the second detection target object <b>62</b><i>b </i>(person's arm) is erroneously detected as the first detection target object <b>62</b><i>a </i>(gun) by an area detection unit <b>230</b><i>c</i>, for example, two areas, i.e., an area R<b>1</b> (indicated by a broken line in the figure) and an area R<b>2</b> (indicated by a two-dot-and-dash line in the figure), are detected from an area detection unit <b>230</b><i>a </i>as the first detection areas <b>232</b><i>a</i>. On the other hand, a third detection area <b>232</b><i>c </i>(indicated by a dot-and-dash line in the figure) of the second detection target object <b>62</b><i>b </i>(person's arm) is detected from an area detection unit <b>230</b><i>b. </i></p><p id="p-0139" num="0135">Then, with respect to the area R<b>1</b> and area R<b>2</b>, the estimation unit <b>260</b> estimates each of areas <b>262</b> (area RE<b>1</b> and area RE<b>2</b>) with the line-of-sight direction of angle B. Then, based on a positional relationship between the estimated two areas, i.e., the area RE<b>1</b> and area RE<b>2</b>, and the third detection area <b>232</b><i>c</i>, the identification unit <b>240</b> identifies the first detection target object <b>62</b><i>a </i>(gun).</p><p id="p-0140" num="0136">Herein, since a position of the area RE<b>2</b> agrees with a position of the third detection area <b>232</b><i>c</i>, the area R<b>2</b> detected as the first detection target object <b>62</b><i>a </i>(gun) is identified as being the detection target object <b>62</b><i>b </i>(person's arm). On the other hand, since a position of the area RE<b>1</b> does not agree with the position of the third detection area <b>232</b><i>c</i>, the area R<b>1</b> detected as the first detection target object <b>62</b><i>a </i>(gun) is identified as not being the detection target object <b>62</b><i>b </i>(person's arm).</p><p id="p-0141" num="0137">Hereinafter, the details of the identification method based on a positional relationship between two areas in the identification unit <b>240</b> are described. The description is given with reference to flowcharts of <figref idref="DRAWINGS">FIGS. <b>18</b> and <b>19</b></figref>. In <figref idref="DRAWINGS">FIGS. <b>18</b> and <b>19</b></figref>, the same steps as in <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref> are denoted by the same reference signs, and a description thereof is omitted.</p><p id="p-0142" num="0138">First, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, when a ratio R is more than a third threshold (for example, 0.7) (YES in step S<b>403</b>), the identification unit <b>240</b> identifies a detected object as not being the detection target object <b>62</b> (in other words, erroneous detection), and the detection target object <b>62</b> is not detected (step S<b>405</b>).</p><p id="p-0143" num="0139">Next, as illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, when the ratio R is less than a fourth threshold (for example, 0.3) (YES in step S<b>413</b>), the identification unit <b>240</b> identifies the detected object as being the detection target object <b>62</b> (step S<b>415</b>).</p><p id="p-0144" num="0140">The flowchart in <figref idref="DRAWINGS">FIG. <b>18</b></figref> and the flowchart in <figref idref="DRAWINGS">FIG. <b>19</b></figref> can also be combined. After computing the ratio R in step S<b>201</b>, the identification unit <b>240</b> may execute the identification processing by using the third threshold in step S<b>403</b>, and execute the identification processing by using the fourth threshold in step S<b>413</b> with respect to the detected object being identified as not being the detection target object <b>62</b> (NO in step S<b>403</b>). Alternatively, the identification of step S<b>413</b> may be executed prior to the identification of step S<b>403</b>.</p><p id="p-0145" num="0141">According to the present example embodiment, similar advantageous effects as in the above-described fifth example embodiment can be acquired, and, moreover, since a detection target object and an object other than the detection target object are detected, the identification of the combination of the two detection manners may be executed, and therefore detection failure and erroneous detection can be reduced. Furthermore, since an estimation area can be detected by the estimation unit <b>260</b>, detection accuracy can be further improved.</p><p id="p-0146" num="0142">While the example embodiments have been described with reference to the drawings, these example embodiments are exemplification of the present invention, and various configurations other than the above can be adopted.</p><heading id="h-0018" level="2">&#x3c;Computation Method of Distance Information <b>224</b> in Image Generation Unit <b>220</b>&#x3e;</heading><p id="p-0147" num="0143"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a diagram for describing an example of a computation method of distance information <b>224</b> in an image generation unit <b>220</b>. The image generation unit <b>220</b> can generate the distance information <b>224</b> by using any one of the following computation methods (c1) to (c3), but the computation methods are not limited to these.</p><p id="p-0148" num="0144">Hereinafter, a description is given of a case where a line-of-sight direction (predetermined direction) of a two-dimensional image <b>222</b> is a z direction, that is, a direction parallel to a direction of irradiation of an electromagnetic wave.</p><p id="p-0149" num="0000">(c1) A voxel, at which amplitude of a radar image (three-dimensional information <b>212</b>) in the line-of-sight direction (z direction) becomes maximum, is selected, and an amplitude value thereof is set as a value I<sub>2D</sub>(x, y) of the two-dimensional image <b>222</b>. In addition, z coordinates of the value are set as D(x, y) of the distance information <b>224</b>. These are computed by the following equations (2) and (3).</p><p id="p-0150" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>I</mi>       <mrow>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>D</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <munder>       <mi>max</mi>       <mi>z</mi>      </munder>      <mtext>  </mtext>      <mrow>       <msub>        <mi>I</mi>        <mrow>         <mn>3</mn>         <mo>&#x2062;</mo>         <mi>D</mi>        </mrow>       </msub>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>        <mo>,</mo>        <mi>z</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.3ex" height="0.3ex">     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mn>2</mn>       <mo>)</mo>      </mrow>     </mrow>    </mpadded>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>D</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <munder>       <mrow>        <mi>arg</mi>        <mo>&#x2062;</mo>        <mi>max</mi>       </mrow>       <mi>z</mi>      </munder>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <msub>        <mi>I</mi>        <mrow>         <mn>3</mn>         <mo>&#x2062;</mo>         <mi>D</mi>        </mrow>       </msub>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>        <mo>,</mo>        <mi>z</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.3ex" height="0.3ex">     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mn>3</mn>       <mo>)</mo>      </mrow>     </mrow>    </mpadded>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0151" num="0145">(c2) A voxel, at which the amplitude of the radar image (three-dimensional information <b>212</b>) in the line-of-sight direction (z direction) first exceeds a predetermined threshold, is selected, and the amplitude value thereof is set as a value I<sub>2D</sub>(x, y) of the two-dimensional image <b>222</b>. In addition, the z coordinates of the value are set as D(x, y) of the distance information <b>224</b>.</p><p id="p-0152" num="0146">(c3) A voxel, which becomes a center-of-gravity of the amplitude of the radar image (three-dimensional information <b>212</b>) in the line-of-sight direction (z direction), is selected, and the amplitude value thereof is set as a value I<sub>2D</sub>(x, y) of the two-dimensional image <b>222</b>. In addition, the z coordinates of the value are set as D(x, y) of the distance information <b>224</b>. These are computed by the following equations (4) and (5).</p><p id="p-0153" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>2</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>D</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <msub>        <mo>&#x2211;</mo>        <mi>z</mi>       </msub>       <mrow>        <mrow>         <msub>          <mi>I</mi>          <mrow>           <mn>3</mn>           <mo>&#x2062;</mo>           <mi>D</mi>          </mrow>         </msub>         <mo>(</mo>         <mrow>          <mi>x</mi>          <mo>,</mo>          <mi>y</mi>          <mo>,</mo>          <mi>z</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2a2f;</mo>        <mi>z</mi>       </mrow>      </mrow>      <mrow>       <msub>        <mo>&#x2211;</mo>        <mi>z</mi>       </msub>       <mrow>        <msub>         <mi>I</mi>         <mrow>          <mn>3</mn>          <mo>&#x2062;</mo>          <mi>D</mi>         </mrow>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>,</mo>         <mi>y</mi>         <mo>,</mo>         <mi>z</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Equation</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>4</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-3" num="00002.3"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <msub>       <mi>I</mi>       <mrow>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>D</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <msub>       <mi>I</mi>       <mrow>        <mn>3</mn>        <mo>&#x2062;</mo>        <mi>D</mi>       </mrow>      </msub>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>       <mo>,</mo>       <mrow>        <mi>D</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>,</mo>         <mi>y</mi>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Equation</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>5</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><heading id="h-0019" level="2">&#x3c;Estimation Method of Estimation Area <b>262</b> by Estimation Unit <b>260</b>&#x3e;</heading><p id="p-0154" num="0147"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a diagram for describing an example of an estimation method of an estimation area <b>262</b> in an estimation unit <b>260</b>. The estimation unit <b>260</b> may generate distance information <b>224</b> by using any one of the following computation methods (d1) to (d4), but the computation methods are not limited to these.</p><p id="p-0155" num="0148">Hereinafter, a description is given of a case where, as illustrated in <figref idref="DRAWINGS">FIG. <b>21</b>A</figref>, a line-of-sight direction (predetermined direction) of a first two-dimensional image <b>222</b><i>a </i>is a z direction (angle A), that is, a direction parallel to a direction of irradiation of an electromagnetic wave, and the line-of-sight direction, in which a detection target object <b>62</b> is estimated to be present by the estimation unit <b>260</b>, is parallel to an x direction (angle B). In addition, <figref idref="DRAWINGS">FIG. <b>21</b>C</figref> is a cross-sectional view between heights y1 and y2, and, in the <figref idref="DRAWINGS">FIG. <b>21</b>C</figref>, an illustration relating to a y direction is omitted.</p><p id="p-0156" num="0149">(d1) An area detection unit <b>230</b><i>a </i>detects a first detection area <b>232</b><i>a </i>of the detection target object <b>62</b> being detected from a first two-dimensional image <b>222</b><i>a </i>of the line-of-sight direction angle A, and estimates, as O<sub>EB</sub>(z1, y1, z2, y2), the estimation area <b>262</b> of a target object viewed from the line-of-sight direction angle B, by using O<sub>A</sub>(x1, y1, x2, y2) in <figref idref="DRAWINGS">FIG. <b>21</b>B</figref> and D(x, y) of the distance information <b>224</b> in the first detection area <b>232</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>21</b>C</figref>.</p><p id="p-0157" num="0150">Herein, z1 is a minimum value of the distance information <b>224</b> in the detection area <b>232</b><i>a</i>, and is computed by the following equation (6).</p><p id="p-0158" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <msub>      <mi>z</mi>      <mn>1</mn>     </msub>     <mo>=</mo>     <mrow>      <munder>       <mi>min</mi>       <mrow>        <mrow>         <msub>          <mi>x</mi>          <mn>1</mn>         </msub>         <mo>&#x2264;</mo>         <mi>x</mi>         <mo>&#x2264;</mo>         <msub>          <mi>x</mi>          <mn>2</mn>         </msub>        </mrow>        <mo>,</mo>        <mrow>         <msub>          <mi>y</mi>          <mn>1</mn>         </msub>         <mo>&#x2264;</mo>         <mi>y</mi>         <mo>&#x2264;</mo>         <msub>          <mi>y</mi>          <mn>2</mn>         </msub>        </mrow>       </mrow>      </munder>      <mi>D</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.3ex" height="0.3ex">     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mn>6</mn>       <mo>)</mo>      </mrow>     </mrow>    </mpadded>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0159" num="0151">In addition, z2 is a maximum value of the distance information <b>224</b> in the first detection area <b>232</b><i>a</i>, and is computed by the following equation (7).</p><p id="p-0160" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Math</mi>      <mo>.</mo>      <mtext>   </mtext>      <mn>4</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>   <mtd>    <mi>&#xf3ba;</mi>   </mtd>  </mtr>  <mtr>   <mtd>    <mrow>     <msub>      <mi>z</mi>      <mn>2</mn>     </msub>     <mo>=</mo>     <mrow>      <munder>       <mi>max</mi>       <mrow>        <mrow>         <msub>          <mi>x</mi>          <mn>1</mn>         </msub>         <mo>&#x2264;</mo>         <mi>x</mi>         <mo>&#x2264;</mo>         <msub>          <mi>x</mi>          <mn>2</mn>         </msub>        </mrow>        <mo>,</mo>        <mrow>         <msub>          <mi>y</mi>          <mn>1</mn>         </msub>         <mo>&#x2264;</mo>         <mi>y</mi>         <mo>&#x2264;</mo>         <msub>          <mi>y</mi>          <mn>2</mn>         </msub>        </mrow>       </mrow>      </munder>      <mi>D</mi>      <mo>&#x2062;</mo>      <mrow>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.3ex" height="0.3ex">     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mn>7</mn>       <mo>)</mo>      </mrow>     </mrow>    </mpadded>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0161" num="0152">(d2) In processing of the above (d1), O<sub>A</sub>(x1, y1, x2, y2) of the detection area <b>232</b><i>a </i>is a rectangle and includes an area in which the detection target object <b>62</b> is not present, and there is a case where z1 and z2 are not correctly computed. In this case, when a value I<sub>2D</sub>(x, y) of the first two-dimensional image <b>222</b><i>a </i>is smaller than a threshold, an area of the value is regarded as a non-target object area and is excluded from a target of the computation processing of the maximum value or minimum value of the above (d1), and thereby z1 and z2 can correctly be computed. As the threshold for the above smallness, a value acquired by multiplying a maximum value of the I<sub>2D</sub>(x, y) in the detection area by n is used. 0&#x2264;n&#x2264;1, and, for example, 0.1 is used.</p><p id="p-0162" num="0153">(d3) Another method of estimating O<sub>EB</sub>(z1, y1, z2, y2) of the estimation area <b>262</b> without being affected by a non-target object area is a method of using a size of the detection target object <b>62</b> as advance knowledge. For example, computation is executed according to the following equations (8) to (10), by using a predetermined size L of the detection target object <b>62</b>, centering on D(x, y) of the distance information <b>224</b> of a position at which I<sub>2D</sub>(x, y) in the detection area <b>232</b><i>a </i>becomes maximum.</p><p id="p-0163" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mo>[</mo>  <mrow>   <mi>Math</mi>   <mo>.</mo>   <mtext>   </mtext>   <mn>5</mn>  </mrow>  <mo>]</mo> </mrow></math></maths><maths id="MATH-US-00005-2" num="00005.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>z</mi>      <mn>1</mn>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mi>D</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <msub>         <mi>x</mi>         <mi>max</mi>        </msub>        <mo>,</mo>        <msub>         <mi>y</mi>         <mi>max</mi>        </msub>       </mrow>       <mo>)</mo>      </mrow>      <mo>-</mo>      <mrow>       <mi>L</mi>       <mo>/</mo>       <mn>2</mn>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Equation</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>8</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00005-3" num="00005.3"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <msub>      <mi>z</mi>      <mn>2</mn>     </msub>     <mo>=</mo>     <mrow>      <mrow>       <mi>D</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <msub>         <mi>x</mi>         <mi>max</mi>        </msub>        <mo>,</mo>        <msub>         <mi>y</mi>         <mi>max</mi>        </msub>       </mrow>       <mo>)</mo>      </mrow>      <mo>+</mo>      <mrow>       <mi>L</mi>       <mo>/</mo>       <mn>2</mn>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mpadded width="0em" lspace="0em" depth="-0.1ex" height="0.1ex">     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mn>9</mn>       <mo>)</mo>      </mrow>     </mrow>    </mpadded>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00005-4" num="00005.4"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <msub>        <mi>x</mi>        <mi>max</mi>       </msub>       <mo>,</mo>       <msub>        <mi>y</mi>        <mi>max</mi>       </msub>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <munder>       <mrow>        <mi>arg</mi>        <mo>&#x2062;</mo>        <mi>max</mi>       </mrow>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>       </mrow>      </munder>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <msub>        <mi>I</mi>        <mrow>         <mn>2</mn>         <mo>&#x2062;</mo>         <mi>D</mi>        </mrow>       </msub>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>,</mo>        <mi>y</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>Equation</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mrow>      <mo>(</mo>      <mn>10</mn>      <mo>)</mo>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0164" num="0154">(d4) In the processing of the above (d3), the distance information <b>224</b> of a central position or a center-of-gravity position of the detection area <b>232</b><i>a</i>, instead of the position at which I<sub>2D</sub>(x, y) in the detection area <b>232</b><i>a </i>becomes maximum, may be used.</p><heading id="h-0020" level="2">&#x3c;Notification Level&#x3e;</heading><p id="p-0165" num="0155">In a combination between a first detection target object <b>62</b><i>a </i>(gun) and a second detection target object <b>62</b><i>b </i>(person's arm), according to a positional relationship between these two, the identification unit <b>240</b> may change an emergency level of notification of an identification result of the first detection target object <b>62</b><i>a. </i></p><p id="p-0166" num="0156">For example, in a case where the first detection target object <b>62</b><i>a </i>(gun) is located on an extension line of the second detection target object <b>62</b><i>b </i>(person's arm), that is, in a case of such a positional relationship that the gun is held by a hand, the identification unit <b>240</b> may determine a highest level of emergency, compared to cases of other positional relationships. For example, based on the determination result, the output control unit <b>280</b> in <figref idref="DRAWINGS">FIG. <b>16</b></figref> can change a notification level and select an output destination and/or an output content.</p><p id="p-0167" num="0157">While the present application of the invention has been particularly described with reference to example embodiments and examples, the present application of the invention is not limited to the example embodiments and examples described above. It will be understood by those of ordinary skill in the art that various changes in form and details may be made therein without departing from the spirit and scope of the present application of the invention as defined by the claims.</p><p id="p-0168" num="0158">Note that, when acquiring and using the information relating to a user in the present invention, this shall be done legally.</p><p id="p-0169" num="0159">A part or the entirety of the above-described example embodiments can be described as in the following supplementary notes, but is not limited to the following.</p><p id="p-0170" num="0000">1. A target object detection apparatus including:</p><p id="p-0171" num="0160">an image generation unit that generates, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</p><p id="p-0172" num="0161">an area detection unit that detects, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</p><p id="p-0173" num="0162">an identification unit that identifies the detection target object, based on a positional relationship between the detected at least two detection areas.</p><p id="p-0174" num="0000">2. The target object detection apparatus according to supplementary note 1, wherein</p><p id="p-0175" num="0163">the image generation unit<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0164">generates, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, and generates distance information to a reflection point of each of pixels constituting the first two-dimensional image, and</li>        <li id="ul0002-0002" num="0165">further generates, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction, and generates distance information to a reflection point of each of pixels constituting the second two-dimensional image,</li>    </ul>    </li></ul></p><p id="p-0176" num="0166">the area detection unit detects each of, from the first two-dimensional image, a first detection area in which the detection target object is estimated to be present, and, from the second two-dimensional image, a second detection area in which the detection target object is estimated to be present,</p><p id="p-0177" num="0167">the target object detection apparatus, further including</p><p id="p-0178" num="0168">an estimation unit that estimates, with respect to the detection target object viewed from the first direction, an area of the detection target object when viewed from the second direction, based on a detection area detected by the area detection unit and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area, wherein</p><p id="p-0179" num="0169">the identification unit identifies whether to be the detection target object, based on a positional relationship between the estimation area estimated by the estimation unit and the detection area detected by the area detection unit when viewed from the second direction.</p><p id="p-0180" num="0000">3. The target object detection apparatus according to supplementary note 2, wherein</p><p id="p-0181" num="0170">the estimation unit estimates, with respect to the detection target object viewed from the second direction, an area of the detection target object when viewed from the first direction, based on a detection area detected by the area detection unit and the distance information of each of positions of the detection area, and sets the estimated area as an estimation area, and</p><p id="p-0182" num="0171">the identification unit identifies whether to be the detection target object, based on a positional relationship between the estimation area and the detection area when viewed from the second direction, and a positional relationship between the estimation area estimated by the estimation unit and the detection area detected by the area detection unit when viewed from the first direction.</p><p id="p-0183" num="0000">4. The target object detection apparatus according to supplementary note 2 or 3, wherein</p><p id="p-0184" num="0172">the identification unit identifies as the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or more than a first threshold.</p><p id="p-0185" num="0000">5. The target object detection apparatus according to any one of supplementary notes 2 to 4, wherein</p><p id="p-0186" num="0173">the identification unit identifies as not the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or less than a second threshold. 6. The target object detection apparatus according to any one of supplementary notes 2 to 5, further including</p><p id="p-0187" num="0174">a storage unit that stores in advance the distance information of the detection target object, wherein</p><p id="p-0188" num="0175">the estimation unit estimates, based on the distance information of the detection target object stored in advance by the storage unit, an estimation area in which the detection target object is estimated to be present at a time when the detection target object is viewed from the second direction.</p><p id="p-0189" num="0000">7. The target object detection apparatus according to supplementary note 1, wherein</p><p id="p-0190" num="0176">the recognition means includes a first detection unit that detects a first detection target object, and a second detection unit that recognizes a second detection target object different from the first detection target object,</p><p id="p-0191" num="0177">the area detection unit detects a first detection area of the first detection target object and a second detection area of the second detection target object by using the first detection unit and the second detection unit, respectively, and</p><p id="p-0192" num="0178">the identification unit identifies whether to be the first detection target object, based on a positional relationship between the detected first detection area and the detected second detection area.</p><p id="p-0193" num="0000">8. The target object detection apparatus according to supplementary note 7, wherein</p><p id="p-0194" num="0179">the image generation unit generates, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, generates distance information to a reflection point of each of pixels constituting the first two-dimensional image, and further generates, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction,</p><p id="p-0195" num="0180">the area detection unit detects each of, from the first two-dimensional image, the first detection area of the first detection target object, and, from the second two-dimensional image, the second detection area of the second detection target object,</p><p id="p-0196" num="0181">the target object detection apparatus, further including</p><p id="p-0197" num="0182">an estimation unit that estimates an area of the first detection target object when viewed from the second direction, based on the first detection area of the first detection target object viewed from the first direction, and the distance information of each of positions of the first detection area, and setting the estimated area as an estimation area, wherein</p><p id="p-0198" num="0183">the identification unit identifies whether to be the first detection target object, based on a positional relationship between the estimation area of the first detection target object and the detection area of the second detection target object.</p><p id="p-0199" num="0000">9. The target object detection apparatus according to supplementary note 8, wherein</p><p id="p-0200" num="0184">the identification unit identifies as not the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or more than a third threshold.</p><p id="p-0201" num="0000">10. The target object detection apparatus according to supplementary note 8 or 9, wherein</p><p id="p-0202" num="0185">the identification unit identifies as the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or less than a fourth threshold.</p><p id="p-0203" num="0000">11. The target object detection apparatus according to any one of supplementary notes 1 to 10, further including</p><p id="p-0204" num="0186">an output control unit that outputs, when being identified as the detection target object by the identification unit, a result of the identification, and not outputting, when being identified as not the detection target object, a result of the identification.</p><p id="p-0205" num="0000">12. A target object detection method including:</p><p id="p-0206" num="0187">by a target object detection apparatus,</p><p id="p-0207" num="0188">generating, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</p><p id="p-0208" num="0189">detecting, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</p><p id="p-0209" num="0190">identifying the detection target object, based on a positional relationship between the detected at least two detection areas.</p><p id="p-0210" num="0000">13. The target object detection method according to supplementary note 12, further including:</p><p id="p-0211" num="0191">by the target object detection apparatus,</p><p id="p-0212" num="0192">generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, and generating distance information to a reflection point of each of pixels constituting the first two-dimensional image;</p><p id="p-0213" num="0193">further generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction, and generating distance information to a reflection point of each of pixels constituting the second two-dimensional image;</p><p id="p-0214" num="0194">detecting each of, from the first two-dimensional image, a first detection area in which the detection target object is estimated to be present, and, from the second two-dimensional image, a second detection area in which the detection target object is estimated to be present;</p><p id="p-0215" num="0195">further, estimating, with respect to the detection target object viewed from the first direction, an area of the detection target object when viewed from the second direction, based on a detected detection area and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</p><p id="p-0216" num="0196">identifying whether to be the detection target object, based on a positional relationship between the estimated estimation area and the detected detection area when viewed from the second direction. 14. The target object detection method according to supplementary note 13, further including:</p><p id="p-0217" num="0197">by the target object detection apparatus,</p><p id="p-0218" num="0198">estimating, with respect to the detection target object viewed from the second direction, an area of the detection target object when viewed from the first direction, based on a detected detection area and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</p><p id="p-0219" num="0199">identifying whether to be the detection target object, based on a positional relationship between the estimation area and the detection area when viewed from the second direction, and a positional relationship between the estimated estimation area and the detected detection area when viewed from the first direction.</p><p id="p-0220" num="0000">15. The target object detection method according to supplementary note 13 or 14, further including,</p><p id="p-0221" num="0200">by the target object detection apparatus,</p><p id="p-0222" num="0201">identifying as the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or more than a first threshold.</p><p id="p-0223" num="0000">16. The target object detection method according to any one of supplementary notes 13 to 15, further including,</p><p id="p-0224" num="0202">by the target object detection apparatus,</p><p id="p-0225" num="0203">identifying as not the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or less than a second threshold.</p><p id="p-0226" num="0000">17. The target object detection method according to any one of supplementary notes 13 to 16, further including:</p><p id="p-0227" num="0204">by the target object detection apparatus,</p><p id="p-0228" num="0205">storing in advance the distance information of the detection target object in a storage apparatus; and</p><p id="p-0229" num="0206">estimating, based on the distance information of the detection target object stored in advance in the storage apparatus, an estimation area in which the detection target object is estimated to be present at a time when the detection target object is viewed from the second direction.</p><p id="p-0230" num="0000">18. The target object detection method according to supplementary note 12, wherein</p><p id="p-0231" num="0207">the recognition means includes a first detection unit that detects a first detection target object, and a second detection unit that recognizes a second detection target object different from the first detection target object,</p><p id="p-0232" num="0208">the target object detection method further including:</p><p id="p-0233" num="0209">by the target object detection apparatus,</p><p id="p-0234" num="0210">detecting a first detection area of the first detection target object and a second detection area of the second detection target object by using the first detection unit and the second detection unit, respectively; and</p><p id="p-0235" num="0211">identifying whether to be the first detection target object, based on a positional relationship between the detected first detection area and the detected second detection area.</p><p id="p-0236" num="0000">19. The target object detection method according to supplementary note 18, further including:</p><p id="p-0237" num="0212">by the target object detection apparatus,</p><p id="p-0238" num="0213">generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, generating distance information to a reflection point of each of pixels constituting the first two-dimensional image, and further generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction;</p><p id="p-0239" num="0214">detecting each of, from the first two-dimensional image, the first detection area of the first detection target object, and, from the second two-dimensional image, the second detection area of the second detection target object;</p><p id="p-0240" num="0215">estimating an area of the first detection target object when viewed from the second direction, based on the first detection area of the first detection target object viewed from the first direction, and the distance information of each of positions of the first detection area, and setting the estimated area as an estimation area; and</p><p id="p-0241" num="0216">identifying whether to be the first detection target object, based on a positional relationship between the estimation area of the first detection target object and the detection area of the second detection target object.</p><p id="p-0242" num="0000">20. The target object detection method according to supplementary note 19, further including,</p><p id="p-0243" num="0217">by the target object detection apparatus,</p><p id="p-0244" num="0218">identifying as not the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or more than a third threshold.</p><p id="p-0245" num="0000">21. The target object detection method according to supplementary note 19 or 20, further including,</p><p id="p-0246" num="0219">by the target object detection apparatus,</p><p id="p-0247" num="0220">identifying as the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or less than a fourth threshold.</p><p id="p-0248" num="0000">22. The target object detection method according to any one of supplementary notes 12 to 21, further including,</p><p id="p-0249" num="0221">by the target object detection apparatus,</p><p id="p-0250" num="0222">outputting, when being identified as the detection target object, a result of the identification, and not outputting, when being identified as not the detection target object, a result of the identification.</p><p id="p-0251" num="0000">23. A program for causing a computer to execute:</p><p id="p-0252" num="0223">a procedure for generating, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</p><p id="p-0253" num="0224">a procedure for detecting, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</p><p id="p-0254" num="0225">a procedure for identifying the detection target object, based on a positional relationship between the detected at least two detection areas.</p><p id="p-0255" num="0000">24. The program according to supplementary note 23, for further causing a computer to execute:</p><p id="p-0256" num="0226">a procedure for generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, and generating distance information to a reflection point of each of pixels constituting the first two-dimensional image;</p><p id="p-0257" num="0227">a procedure for generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction, and generating distance information to a reflection point of each of pixels constituting the second two-dimensional image;</p><p id="p-0258" num="0228">a procedure for detecting each of, from the first two-dimensional image, a first detection area in which the detection target object is estimated to be present, and, from the second two-dimensional image, a second detection area in which the detection target object is estimated to be present;</p><p id="p-0259" num="0229">a procedure for estimating, with respect to the detection target object viewed from the first direction, an area of the detection target object when viewed from the second direction, based on a detection area detected by the procedure for detecting, and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</p><p id="p-0260" num="0230">a procedure for identifying whether to be the detection target object, based on a positional relationship between the estimated estimation area and the detected detection area when viewed from the second direction.</p><p id="p-0261" num="0000">25. The program according to supplementary note 24, for further causing a computer to execute:</p><p id="p-0262" num="0231">a procedure for estimating, with respect to the detection target object viewed from the second direction, an area of the detection target object when viewed from the first direction, based on a detection area detected by the procedure for detecting and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</p><p id="p-0263" num="0232">a procedure for identifying whether to be the detection target object, based on a positional relationship between the estimation area and the detection area when viewed from the second direction, and a positional relationship between the estimated estimation area and the detected detection area when viewed from the first direction.</p><p id="p-0264" num="0000">26. The program according to supplementary note 24 or 25, for further causing a computer to execute</p><p id="p-0265" num="0233">a procedure for identifying as the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or more than a first threshold.</p><p id="p-0266" num="0000">27. The program according to any one of supplementary notes 24 to 26, for further causing a computer to execute</p><p id="p-0267" num="0234">a procedure for identifying as not the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or less than a second threshold.</p><p id="p-0268" num="0000">28. The program according to any one of supplementary notes 24 to 27, for further causing a computer to execute:</p><p id="p-0269" num="0235">a procedure for storing in advance the distance information of the detection target object in a storage apparatus; and</p><p id="p-0270" num="0236">a procedure for estimating, based on the distance information of the detection target object stored in advance in the storage apparatus, an estimation area in which the detection target object is estimated to be present at a time when the detection target object is viewed from the second direction.</p><p id="p-0271" num="0000">29. The program according to supplementary note 23, wherein</p><p id="p-0272" num="0237">the recognition means includes a first detection unit that detects a first detection target object, and a second detection unit that recognizes a second detection target object different from the first detection target object,</p><p id="p-0273" num="0238">the program for further causing a computer to execute:</p><p id="p-0274" num="0239">a procedure for detecting a first detection area of the first detection target object and a second detection area of the second detection target object by using the first detection unit and the second detection unit, respectively; and</p><p id="p-0275" num="0240">a procedure for identifying whether to be the first detection target object, based on a positional relationship between the detected first detection area and the detected second detection area.</p><p id="p-0276" num="0000">30. The program according to supplementary note 29, for further causing a computer to execute:</p><p id="p-0277" num="0241">a procedure for generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, generating distance information to a reflection point of each of pixels constituting the first two-dimensional image, and generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction;</p><p id="p-0278" num="0242">a procedure for detecting each of, from the first two-dimensional image, the first detection area of the first detection target object, and, from the second two-dimensional image, the second detection area of the second detection target object;</p><p id="p-0279" num="0243">a procedure for estimating an area of the first detection target object when viewed from the second direction, based on the first detection area of the first detection target object viewed from the first direction, and the distance information of each of positions of the first detection area, and setting the estimated area as an estimation area; and</p><p id="p-0280" num="0244">a procedure for identifying whether to be the first detection target object, based on a positional relationship between the estimation area of the first detection target object and the detection area of the second detection target object.</p><p id="p-0281" num="0000">31. The program according to supplementary note 30, for further causing a computer to execute</p><p id="p-0282" num="0245">a procedure for identifying as not the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or more than a third threshold.</p><p id="p-0283" num="0000">32. The program according to supplementary note 30 or 31, for further causing a computer to execute</p><p id="p-0284" num="0246">a procedure for identifying as the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or less than a fourth threshold.</p><p id="p-0285" num="0000">33. The program according to any one of supplementary notes 23 to 32, for further causing the computer to execute</p><p id="p-0286" num="0247">a procedure for outputting, when being identified as the detection target object, a result of the identification, and not outputting, when being identified as not the detection target object, a result of the identification.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230003884A1-20230105-M00001.NB"><img id="EMI-M00001" he="13.38mm" wi="76.20mm" file="US20230003884A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2 MATH-US-00002-3" nb-file="US20230003884A1-20230105-M00002.NB"><img id="EMI-M00002" he="17.61mm" wi="76.20mm" file="US20230003884A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230003884A1-20230105-M00003.NB"><img id="EMI-M00003" he="7.79mm" wi="76.20mm" file="US20230003884A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230003884A1-20230105-M00004.NB"><img id="EMI-M00004" he="7.79mm" wi="76.20mm" file="US20230003884A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005 MATH-US-00005-2 MATH-US-00005-3 MATH-US-00005-4" nb-file="US20230003884A1-20230105-M00005.NB"><img id="EMI-M00005" he="18.37mm" wi="76.20mm" file="US20230003884A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A target object detection apparatus comprising:<claim-text>an image generation unit that generates, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</claim-text><claim-text>an area detection unit that detects, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</claim-text><claim-text>an identification unit that identifies the detection target object, based on a positional relationship between the detected at least two detection areas.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The target object detection apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the image generation unit<claim-text>generates, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, and generates distance information to a reflection point of each of pixels constituting the first two-dimensional image, and</claim-text><claim-text>further generates, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction, and generates distance information to a reflection point of each of pixels constituting the second two-dimensional image,</claim-text></claim-text><claim-text>the area detection unit detects each of, from the first two-dimensional image, a first detection area in which the detection target object is estimated to be present, and, from the second two-dimensional image, a second detection area in which the detection target object is estimated to be present,</claim-text><claim-text>the target object detection apparatus, further comprising</claim-text><claim-text>an estimation unit that estimates, with respect to the detection target object viewed from the first direction, an area of the detection target object when viewed from the second direction, based on a detection area detected by the area detection unit and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area, wherein</claim-text><claim-text>the identification unit identifies whether to be the detection target object, based on a positional relationship between the estimation area estimated by the estimation unit and the detection area detected by the area detection unit when viewed from the second direction.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The target object detection apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the estimation unit estimates, with respect to the detection target object viewed from the second direction, an area of the detection target object when viewed from the first direction, based on a detection area detected by the area detection unit and the distance information of each of positions of the detection area, and sets the estimated area as an estimation area, and</claim-text><claim-text>the identification unit identifies whether to be the detection target object, based on a positional relationship between the estimation area and the detection area when viewed from the second direction, and a positional relationship between the estimation area estimated by the estimation unit and the detection area detected by the area detection unit when viewed from the first direction.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The target object detection apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the identification unit identifies as the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or more than a first threshold.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The target object detection apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>the identification unit identifies as not the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or less than a second threshold.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The target object detection apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising<claim-text>a storage unit that stores in advance the distance information of the detection target object, wherein</claim-text><claim-text>the estimation unit estimates, based on the distance information of the detection target object stored in advance by the storage unit, an estimation area in which the detection target object is estimated to be present at a time when the detection target object is viewed from the second direction.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The target object detection apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>the recognition means includes a first detection unit that detects a first detection target object, and a second detection unit that recognizes a second detection target object different from the first detection target object,</claim-text><claim-text>the area detection unit detects a first detection area of the first detection target object and a second detection area of the second detection target object by using the first detection unit and the second detection unit, respectively, and</claim-text><claim-text>the identification unit identifies whether to be the first detection target object, based on a positional relationship between the detected first detection area and the detected second detection area.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The target object detection apparatus according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein<claim-text>the image generation unit generates, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, generates distance information to a reflection point of each of pixels constituting the first two-dimensional image, and further generates, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction,</claim-text><claim-text>the area detection unit detects each of, from the first two-dimensional image, the first detection area of the first detection target object, and, from the second two-dimensional image, the second detection area of the second detection target object,</claim-text><claim-text>the target object detection apparatus, further comprising</claim-text><claim-text>an estimation unit that estimates an area of the first detection target object when viewed from the second direction, based on the first detection area of the first detection target object viewed from the first direction, and the distance information of each of positions of the first detection area, and setting the estimated area as an estimation area, wherein</claim-text><claim-text>the identification unit identifies whether to be the first detection target object, based on a positional relationship between the estimation area of the first detection target object and the detection area of the second detection target object.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The target object detection apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the identification unit identifies as not the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or more than a third threshold.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The target object detection apparatus according to <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the identification unit identifies as the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or less than a fourth threshold.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The target object detection apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising<claim-text>an output control unit that outputs, when being identified as the detection target object by the identification unit, a result of the identification, and not outputting, when being identified as not the detection target object, a result of the identification.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A target object detection method comprising:<claim-text>by a target object detection apparatus,</claim-text><claim-text>generating, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</claim-text><claim-text>detecting, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</claim-text><claim-text>identifying the detection target object, based on a positional relationship between the detected at least two detection areas.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The target object detection method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>by the target object detection apparatus,</claim-text><claim-text>generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, and generating distance information to a reflection point of each of pixels constituting the first two-dimensional image;</claim-text><claim-text>further generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction, and generating distance information to a reflection point of each of pixels constituting the second two-dimensional image;</claim-text><claim-text>detecting each of, from the first two-dimensional image, a first detection area in which the detection target object is estimated to be present, and, from the second two-dimensional image, a second detection area in which the detection target object is estimated to be present;</claim-text><claim-text>further, estimating, with respect to the detection target object viewed from the first direction, an area of the detection target object when viewed from the second direction, based on a detected detection area and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</claim-text><claim-text>identifying whether to be the detection target object, based on a positional relationship between the estimated estimation area and the detected detection area when viewed from the second direction.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The target object detection method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>by the target object detection apparatus,</claim-text><claim-text>estimating, with respect to the detection target object viewed from the second direction, an area of the detection target object when viewed from the first direction, based on a detected detection area and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</claim-text><claim-text>identifying whether to be the detection target object, based on a positional relationship between the estimation area and the detection area when viewed from the second direction, and a positional relationship between the estimated estimation area and the detected detection area when viewed from the first direction.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The target object detection method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising,<claim-text>by the target object detection apparatus,</claim-text><claim-text>identifying as the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or more than a first threshold.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The target object detection method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising,<claim-text>by the target object detection apparatus,</claim-text><claim-text>identifying as not the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or less than a second threshold.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The target object detection method according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising:<claim-text>by the target object detection apparatus,</claim-text><claim-text>storing in advance the distance information of the detection target object in a storage apparatus; and</claim-text><claim-text>estimating, based on the distance information of the detection target object stored in advance in the storage apparatus, an estimation area in which the detection target object is estimated to be present at a time when the detection target object is viewed from the second direction.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The target object detection method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein<claim-text>the recognition means includes a first detection unit that detects a first detection target object, and a second detection unit that recognizes a second detection target object different from the first detection target object,</claim-text><claim-text>the target object detection method, further comprising:</claim-text><claim-text>by the target object detection apparatus,</claim-text><claim-text>detecting a first detection area of the first detection target object and a second detection area of the second detection target object by using the first detection unit and the second detection unit, respectively; and</claim-text><claim-text>identifying whether to be the first detection target object, based on a positional relationship between the detected first detection area and the detected second detection area.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The target object detection method according to <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising:<claim-text>by the target object detection apparatus,</claim-text><claim-text>generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, generating distance information to a reflection point of each of pixels constituting the first two-dimensional image, and further generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction;</claim-text><claim-text>detecting each of, from the first two-dimensional image, the first detection area of the first detection target object, and, from the second two-dimensional image, the second detection area of the second detection target object;</claim-text><claim-text>estimating an area of the first detection target object when viewed from the second direction, based on the first detection area of the first detection target object viewed from the first direction, and the distance information of each of positions of the first detection area, and setting the estimated area as an estimation area; and</claim-text><claim-text>identifying whether to be the first detection target object, based on a positional relationship between the estimation area of the first detection target object and the detection area of the second detection target object.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The target object detection method according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising,<claim-text>by the target object detection apparatus,</claim-text><claim-text>identifying as not the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or more than a third threshold.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The target object detection method according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, further comprising,<claim-text>by the target object detection apparatus,</claim-text><claim-text>identifying as the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or less than a fourth threshold.</claim-text></claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The target object detection method according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising,<claim-text>by the target object detection apparatus,</claim-text><claim-text>outputting, when being identified as the detection target object, a result of the identification, and not outputting, when being identified as not the detection target object, a result of the identification.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. A non-transitory computer-readable storage medium storing a program for causing a computer to execute:<claim-text>a procedure for generating, from three-dimensional information acquired by processing a reflection wave of an electromagnetic wave irradiated toward an inspection target, a two-dimensional image of the inspection target viewed from a predetermined direction;</claim-text><claim-text>a procedure for detecting, from the two-dimensional image, each of at least two detection areas of a detection target object recognized by using at least two recognition means; and</claim-text><claim-text>a procedure for identifying the detection target object, based on a positional relationship between the detected at least two detection areas.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, storing the program for further causing a computer to execute:<claim-text>a procedure for generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, and generating distance information to a reflection point of each of pixels constituting the first two-dimensional image;</claim-text><claim-text>a procedure for generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction, and generating distance information to a reflection point of each of pixels constituting the second two-dimensional image;</claim-text><claim-text>a procedure for detecting each of, from the first two-dimensional image, a first detection area in which the detection target object is estimated to be present, and, from the second two-dimensional image, a second detection area in which the detection target object is estimated to be present;</claim-text><claim-text>a procedure for estimating, with respect to the detection target object viewed from the first direction, an area of the detection target object when viewed from the second direction, based on a detection area detected by the procedure for detecting and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</claim-text><claim-text>a procedure for identifying whether to be the detection target object, based on a positional relationship between the estimated estimation area and the detected detection area when viewed from the second direction.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, storing the program for further causing a computer to execute:<claim-text>a procedure for estimating, with respect to the detection target object viewed from the second direction, an area of the detection target object when viewed from the first direction, based on a detection area detected by the procedure for detecting and the distance information of each of positions of the detection area, and setting the estimated area as an estimation area; and</claim-text><claim-text>a procedure for identifying whether to be the detection target object, based on a positional relationship between the estimation area and the detection area when viewed from the second direction, and a positional relationship between the estimated estimation area and the detected detection area when viewed from the first direction.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, storing the program for further causing a computer to execute<claim-text>a procedure for identifying as the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or more than a first threshold.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, storing the program for further causing a computer to execute<claim-text>a procedure for identifying as not the detection target object, when a degree of agreement between the estimation area and the detection area is equal to or less than a second threshold.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, storing the program for further causing a computer to execute:<claim-text>a procedure for storing in advance the distance information of the detection target object in a storage apparatus; and</claim-text><claim-text>a procedure for estimating, based on the distance information of the detection target object stored in advance in the storage apparatus, an estimation area in which the detection target object is estimated to be present at a time when the detection target object is viewed from the second direction.</claim-text></claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein<claim-text>the recognition means includes a first detection unit that detects a first detection target object, and a second detection unit that recognizes a second detection target object different from the first detection target object,</claim-text><claim-text>the non-transitory computer-readable storage medium storing the program for further causing a computer to execute:</claim-text><claim-text>a procedure for detecting a first detection area of the first detection target object and a second detection area of the second detection target object by using the first detection unit and the second detection unit, respectively; and</claim-text><claim-text>a procedure for identifying whether to be the first detection target object, based on a positional relationship between the detected first detection area and the detected second detection area.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00029">claim 29</claim-ref>, storing the program for further causing a computer to execute:<claim-text>a procedure for generating, from the three-dimensional information, a first two-dimensional image of the inspection target when viewed from a first direction, generating distance information to a reflection point of each of pixels constituting the first two-dimensional image, and generating, from the three-dimensional information, a second two-dimensional image of the inspection target when viewed from a second direction different from the first direction;</claim-text><claim-text>a procedure for detecting each of, from the first two-dimensional image, the first detection area of the first detection target object, and, from the second two-dimensional image, the second detection area of the second detection target object;</claim-text><claim-text>a procedure for estimating an area of the first detection target object when viewed from the second direction, based on the first detection area of the first detection target object viewed from the first direction, and the distance information of each of positions of the first detection area, and setting the estimated area as an estimation area; and</claim-text><claim-text>a procedure for identifying whether to be the first detection target object, based on a positional relationship between the estimation area of the first detection target object and the detection area of the second detection target object.</claim-text></claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00030">claim 30</claim-ref>, storing the program for further causing a computer to execute<claim-text>a procedure for identifying as not the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or more than a third threshold.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00030">claim 30</claim-ref>, storing the program for further causing a computer to execute<claim-text>a procedure for identifying as the first detection target object, when a degree of agreement between the estimation area of the first detection target object and the detection area of the second detection target object is equal to or less than a fourth threshold.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The non-transitory computer-readable storage medium according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, storing the program for further causing a computer to execute<claim-text>a procedure for outputting, when being identified as the detection target object, a result of the identification, and not outputting, when being identified as not the detection target object, a result of the identification.</claim-text></claim-text></claim></claims></us-patent-application>