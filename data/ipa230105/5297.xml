<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005298A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005298</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17931668</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20220101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>45</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>40</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>172</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>45</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>21</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DETECTING ATTEMPTS TO DEFEAT FACIAL RECOGNITION</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16516117</doc-number><date>20190718</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11450151</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17931668</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Capital One Services, LLC</orgname><address><city>McLean</city><state>VA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>MOSSOBA</last-name><first-name>Michael</first-name><address><city>Great Falls</city><state>VA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>EDWARDS</last-name><first-name>Joshua</first-name><address><city>Philadelphia</city><state>PA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BENKREIRA</last-name><first-name>Abdelkadar M'Hamed</first-name><address><city>Washington</city><state>DC</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A device may select an individual that is a candidate for authentication by facial recognition. The device may identify a facial area of the individual and an area of exposed skin of the individual. The device may obtain a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual. The device may determine, based on the first temperature and the second temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology. The device may selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="180.59mm" wi="158.75mm" file="US20230005298A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="188.55mm" wi="159.68mm" orientation="landscape" file="US20230005298A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="188.55mm" wi="159.68mm" orientation="landscape" file="US20230005298A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="223.01mm" wi="160.70mm" orientation="landscape" file="US20230005298A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="218.27mm" wi="161.21mm" orientation="landscape" file="US20230005298A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="223.60mm" wi="161.54mm" file="US20230005298A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="223.60mm" wi="161.37mm" file="US20230005298A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="223.60mm" wi="161.46mm" file="US20230005298A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 16/516,117, filed Jul. 18, 2019 (now U.S. Pat. No. 11,450,151), which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">A facial recognition technique may identify an individual that is depicted in an image or a video frame. For example, the facial recognition technique may select one or more facial features of a face depicted in the image or the video frame, and compare the selected facial features to faces in a database. The database may include a mapping of the faces to identities, thereby permitting identification of the individual that is depicted in the image or the video frame.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">According to some implementations, a method may include selecting, by a device, an individual that is a candidate for authentication by facial recognition; identifying, by the device and based on selecting the individual, a facial area of the individual and an area of exposed skin of the individual, wherein the area of exposed skin of the individual is not associated with the facial area of the individual; obtaining, by the device, a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual; determining, by the device and based on the first temperature and the second temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, wherein the first temperature corresponding to the second temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, wherein the first temperature not corresponding to the second temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology; and selectively performing, by the device, facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><p id="p-0005" num="0004">According to some implementations, a device may include one or more memories and one or more processors, communicatively coupled to the one or more memories, to select an individual that is a candidate for authentication by facial recognition; obtain a temperature associated with a facial area of the individual; determine an estimated temperature for the facial area of the individual, wherein the estimated temperature is based on conditions of an environment associated with the individual; determine, based on the temperature and the estimated temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, wherein the temperature corresponding to the estimated temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, wherein the temperature not corresponding to the estimated temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology; and selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><p id="p-0006" num="0005">According to some implementations, a non-transitory computer-readable medium may store one or more instructions that, when executed by one or more processors of, may cause the one or more processors to select an individual that is a candidate for authentication by facial recognition; identify, based on selecting the individual, a facial area of the individual and an area of exposed skin of the individual, wherein the area of exposed skin of the individual is not associated with the facial area of the individual; obtain a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual; determine an estimated temperature for the facial area of the individual, wherein the estimated temperature is based on conditions of an environment associated with the individual; determine, based on the first temperature, the second temperature, and the estimated temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, wherein the first temperature corresponding to the second temperature or the estimated temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, wherein the first temperature not corresponding to the second temperature and the estimated temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology; and selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams of one or more example implementations described herein.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an example environment in which systems and/or methods described herein may be implemented.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of example components of one or more devices of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIGS. <b>4</b>-<b>6</b></figref> are flow charts of example processes for detecting attempts to defeat facial recognition.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">The following detailed description of example implementations refers to the accompanying drawings. The same reference numbers in different drawings may identify the same or similar elements.</p><p id="p-0012" num="0011">Facial recognition may be performed to authenticate an identity of an individual, determine an identity of an individual of interest (e.g., an individual suspected of committing a crime), identify an individual of interest (e.g., identify an individual from a crowd), and/or the like. In one example, a financial institution may perform facial recognition on an individual when the individual is attempting to access an account. In particular, the financial institution may perform the facial recognition in order to authenticate an identity of the individual before permitting the individual to access the account.</p><p id="p-0013" num="0012">Sometimes, an individual may attempt to defeat facial recognition by altering an appearance of the individual's face. For example, the individual may wear a mask (e.g., a mask of another individual's face) or a facial prosthetic (e.g., a prosthetic nose) in an attempt to defeat facial recognition. Masks and facial prosthetics have reached a level of sophistication that makes detection of a mask or a facial prosthetic difficult and technically complex. As a result, a facial recognition system may misidentify, or fail to identify, the individual that is attempting to defeat facial recognition. Thus, the facial recognition system may waste resources (e.g., processor resources, memory resources, and/or the like) performing facial recognition on facial features of the individual that are falsified or intentionally obscured. In addition, misidentification of the individual may permit the individual to engage in illegal activity, such as fraudulently accessing a financial account that belongs to another individual. In such a case, a financial institution that maintains the financial account may consume resources (e.g., computing resources and/or network resources) involved in identifying, investigating, and/or correcting the illegal activity.</p><p id="p-0014" num="0013">Some implementations described herein provide a detection platform that may detect an attempt to defeat facial recognition using a face-altering technology (e.g., a mask, a facial prosthetic, and/or the like). The detection platform may identify a facial area of an individual that is a candidate for authentication by facial recognition, and may identify a characteristic of the facial area. For example, the characteristic may be a temperature of the facial area, a level of perspiration of the facial area, a depth of an eye recess of the facial area, and/or the like. Based on the characteristic, the detection platform may determine whether an appearance of the facial area is likely altered, and may selectively perform facial recognition on the individual based on whether the appearance of the facial area is likely altered. For example, the detection platform may not perform facial recognition on the individual based on determining that the appearance of the facial area is likely altered.</p><p id="p-0015" num="0014">In this way, the detection platform conserves computing resources involved in facial recognition that would otherwise be wasted if facial recognition was performed on individuals using a face-altering technology (e.g., because facial recognition is unlikely to accurately identify such individuals). Moreover, the detection platform can reduce instances of misidentification caused by face-altering technology (e.g., masks), thereby conserving computing resources involved in identifying, investigating, and/or correcting illegal activity that may otherwise be permitted by misidentification.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are diagrams of one or more example implementations <b>100</b> described herein. As shown in <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>, example implementation(s) <b>100</b> may include a detection platform. The detection platform may be associated with an entity that performs facial recognition, such as a financial institution, a government agency (e.g., a police department, a transportation security agency, and/or the like), a school, a stadium, and/or the like. In some implementations, the detection platform may be associated with a financial institution that performs facial recognition in order to authenticate an individual to access a secure area. The secure area may be an account maintained by the financial institution that may be accessed via a transaction device, such as an automated teller machine (ATM) device.</p><p id="p-0017" num="0016">The detection platform may be associated with one or more cameras and/or one or more sensors. The one or more cameras may be configured to capture an image or a video of an individual. The one or more sensors may be configured to detect and report one or more measurable parameters of the individual. For example, the one or more sensors may detect a temperature associated with the individual, a level of moisture associated with the individual, and/or the like. In some implementations, the detection platform may include the one or more cameras and/or the one or more sensors. Additionally, or alternatively, another device that is accessible to the detection platform may include the one or more cameras and/or the one or more sensors. For example, a transaction device (e.g., an ATM device) that is in communication with the detection platform may include the one or more cameras and/or the one or more sensors, and may provide data obtained by the one or more cameras and/or the one or more sensors to the detection platform.</p><p id="p-0018" num="0017">The detection platform may implement a facial recognition system. The facial recognition system may utilize a facial recognition technique to identify a face of an individual. For example, the facial recognition technique may identify facial features (e.g., eyes, nose, mouth, jaw, cheekbones, and/or the like) of the individual from an image of the individual's face, and may determine a data set that represents a relative position, size, and/or shape of the facial features. The data set may be compared to other data sets representing facial features of known individuals in order to identify the individual. The data sets representing facial features of known individuals may be stored in a data structure (e.g., a data repository, a database, a table, a list, and/or the like) of the detection platform or accessible to the detection platform.</p><p id="p-0019" num="0018">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, and by reference number <b>105</b>, the detection platform may select an individual that is a candidate for authentication by facial recognition. In some cases, the individual may be utilizing a face-altering technology, such as a mask (e.g., a latex mask), a facial prosthetic, a bandage, an eye patch, and/or the like.</p><p id="p-0020" num="0019">In some implementations, the detection platform (e.g., using a camera) may select the individual as the candidate for authentication by facial recognition if the individual enters a particular area (e.g., a bank, a stadium, an airport, and/or the like). For example, the detection platform may select the individual if the individual enters a particular area associated with a transaction device, such as a checkout area of a merchant, an ATM vestibule, and/or the like. In such a case, the particular area may include a proximity (e.g., 10 feet, 25 feet, 50 feet, and/or the like) of the transaction device. Thus, the detection platform may select the individual as the candidate for facial recognition if the individual is approaching the transaction device (e.g., enters the proximity). In some implementations, the detection platform may distinguish (e.g., utilizing a machine learning model) between individuals entering the proximity in order to use the transaction device and those merely passing through, based on a behavior of an individual, such as a direction of travel of the individual, a direction of an eye gaze of the individual, whether the individual is reaching into a bag or a pocket, and/or the like.</p><p id="p-0021" num="0020">In some implementations, the detection platform may select the individual as the candidate for authentication by facial recognition if the individual requests access to a secure area. For example, the detection platform may select the individual if the individual requests access to an account (e.g., by inserting a transaction card into a transaction device, swiping a transaction card at a transaction device, and/or the like). As another example, the detection platform may select the individual if the individual requests access to a physical area (e.g., by entering an access code or presenting a credential at an access control system of the physical area).</p><p id="p-0022" num="0021">As shown by reference number <b>110</b>, the detection platform may identify a facial area of the individual selected as the candidate for facial recognition. For example, the detection platform may capture, or obtain, an image (e.g., a photograph or a video frame of a video) of the individual in order to identify the facial area of the individual from the image. In some implementations, a camera of a transaction device (e.g., an ATM device) may capture an image of the individual and provide the image to the detection platform. Additionally, or alternatively, a camera associated with a physical area (e.g., an ATM vestibule, an airport, a stadium, and/or the like) may capture an image of the individual and provide the image to the detection platform.</p><p id="p-0023" num="0022">The detection platform may process the image of the individual to identify the facial area of the individual. For example, the detection platform may process the image using an artificial intelligence technique, such as machine learning, deep learning, and/or the like. For example, the detection platform may use a machine learning (e.g., computer vision) model to identify the facial area. For example, the detection platform may train the machine learning model based on one or more parameters associated with a facial area, such as shape, color, arrangement of features, and/or the like. The detection platform may train the machine learning model, according to the one or more parameters, using historical data associated with faces (e.g., images of faces). Using the historical data and the one or more parameters as inputs to the machine learning model, the detection platform may train the machine learning model to identify the facial area from an image of the individual. In some implementations, the detection platform may obtain and utilize a machine learning model that was trained by another device.</p><p id="p-0024" num="0023">In some implementations, the detection platform may identify one or more other bodily areas of the individual from an image of the individual, in a manner similar to that described above. For example, the detection platform may identify an area of exposed skin of the individual that is not associated with the facial area of the individual. The area of exposed skin may relate to an ear, a neck, an arm, a hand, a leg, and/or the like of the individual.</p><p id="p-0025" num="0024">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, and by reference number <b>115</b>, the detection platform may identify a characteristic of the facial area of the individual. The characteristic may be unrelated to a facial feature (e.g., unrelated to eyes, a nose, a mouth, a jaw, and/or the like) of the facial area. For example, the characteristic may relate to a temperature of the facial area or a level of moisture (e.g., perspiration) of the facial area.</p><p id="p-0026" num="0025">In some implementations, the detection platform may obtain the temperature of the facial area. In such a case, the detection platform may obtain the temperature of the facial area from a measurement of a temperature sensor. For example, the detection platform may obtain the temperature of the facial area from a measurement of an infrared (IR) sensor. In such a case, after identifying the facial area, the detection platform may cause the IR sensor to measure a level of IR light associated with the facial area. The detection platform may convert the measured level of IR light to a temperature. In some implementations, the IR sensor may be associated with an IR light source, and the IR sensor may measure a level of IR light from the IR light source that is reflected off the facial area.</p><p id="p-0027" num="0026">In some implementations, the IR sensor and/or the IR light source may be stationary, and positioned so as to be directed to the facial area of the individual (e.g., positioned so as to be directed to the facial area of the individual when the individual is using an ATM device, entering an ATM vestibule, entering an airport, and/or the like). Alternatively, the IR sensor and/or the IR light source may be configured to change position to permit the IR sensor and/or the IR light source to be directed at the facial area of the individual during a movement of the individual.</p><p id="p-0028" num="0027">In some implementations, the detection platform may include the IR sensor and/or the IR light source. Additionally, or alternatively, another device that is accessible to the detection platform may include the IR sensor and/or the IR light source. For example, a transaction device (e.g., an ATM device) that is in communication with the detection platform may include the IR sensor and/or the IR light source, and may provide IR light measurements obtained by the IR sensor to the detection platform.</p><p id="p-0029" num="0028">In some implementations, the detection platform may obtain the level of moisture (e.g., perspiration) of the facial area. In such a case, the detection platform may obtain the level of moisture using a machine learning (e.g., computer vision) model in a manner similar to that described above. For example, the detection platform may train the machine learning model to identify perspiration from an image of the facial area of the individual. In some implementations, the detection platform may obtain and utilize a machine learning model that was trained by another device. The machine learning model may determine the level of moisture as a total area of the facial area where perspiration is identified, a percentage area of the facial area where perspiration is identified, and/or the like.</p><p id="p-0030" num="0029">Additionally, or alternatively, the detection platform may obtain the level of moisture from an IR sensor, such as an IR sensor as described above. For example, the IR sensor may take IR light measurements, which may be converted to temperature measurements, from a plurality of locations on the facial area of the individual. Locations where perspiration is present may have lower temperatures than locations where perspiration is not present. Accordingly, the detection platform may identify a location as being associated with perspiration when the location has a temperature that does not satisfy a particular threshold (e.g., a threshold based on temperature measurements from the plurality of locations). In addition, the detection platform may determine the level of moisture as a total area of the facial area where perspiration is identified, a percentage area of the facial area where perspiration is identified, and/or the like.</p><p id="p-0031" num="0030">Additionally, or alternatively, the detection platform may obtain the level of moisture from a galvanometer that measures a galvanic skin response (i.e., a change in electrical resistance of skin caused by perspiration). The detection platform may determine the level of moisture by converting a level of electrical resistance measured by the galvanometer. The detection platform, or another device accessible to the detection platform (e.g., an ATM device), may include a galvanometer in order to measure a galvanic skin response of the individual. In such a case, the detection platform, or the other device, may instruct the individual to place the individual's facial area in contact with the galvanometer in order to measure a galvanic skin response of the individual's facial area.</p><p id="p-0032" num="0031">In some implementations, the detection platform also may identify a characteristic of the area of exposed skin of the individual (e.g., an area of exposed skin of an ear, a neck, a hand, and/or the like of the individual). For example, the detection platform may obtain a temperature or a level of moisture of the area of exposed skin in a manner similar to that described above.</p><p id="p-0033" num="0032">In some implementations, the identified characteristic of the facial area of the individual may be a recess associated with an eye of the individual. The detection platform may obtain a measurement of the recess using a machine learning (e.g., computer vision) model, in a manner similar to that described above. For example, the detection platform may train the machine learning model to determine a recess of an eye from an image of the facial area of the individual. In some implementations, the detection platform may obtain and utilize a machine learning model that was trained by another device.</p><p id="p-0034" num="0033">As shown by reference number <b>120</b>, the detection platform may determine whether an appearance of the facial area of the individual is likely altered by a face-altering technology (e.g., a mask, a facial prosthetic, and/or the like). For example, the detection platform may determine, based on the identified characteristic of the facial area and/or the identified characteristic of the area of exposed skin, whether the appearance of the facial area is likely altered by a face-altering technology.</p><p id="p-0035" num="0034">In some implementations, the detection platform may compare the identified characteristic of the facial area and the identified characteristic of the area of exposed skin in order to determine whether the appearance of the facial area is likely altered. For example, the detection platform may compare the temperature of the facial area and the temperature of the area of exposed skin. Since a face-altering technology may reduce heat that is emitted or reflected by the facial area relative to the area of exposed skin, the detection platform may determine that the appearance of the facial area is likely altered when the temperature of the facial area and the temperature of the area of exposed skin do not correspond (e.g., the temperature of the facial area is outside of a threshold range, such as &#xb1;1%, &#xb1;5%, and/or the like, of the temperature of the area of exposed skin). Conversely, the detection platform may determine that the appearance of the facial area is likely not altered when the temperature of the facial area and the temperature of the area of exposed skin correspond.</p><p id="p-0036" num="0035">As another example, the detection platform may compare the level of moisture (e.g., perspiration) of the facial area and the level of moisture of the area of exposed skin. Since a face-altering technology may obstruct perspiration present on the facial area, the detection platform may determine that the appearance of the facial area is likely altered when the level of moisture of the facial area and the level of moisture of the area of exposed skin do not correspond (e.g., the level of moisture of the facial area is outside of a threshold range, such as &#xb1;1%, &#xb1;5%, and/or the like, of the level of moisture of the area of exposed skin). Conversely, the detection platform may determine that the appearance of the facial area is likely not altered when the level of moisture of the facial area and the level of moisture of the area of exposed skin correspond.</p><p id="p-0037" num="0036">In some implementations, the detection platform may compare the identified characteristic of the facial area to an estimated value for the characteristic of the facial area. For example, the detection platform may compare the temperature of the facial area to an estimated temperature for the facial area. The detection platform may utilize a machine learning model to determine the estimated temperature, in a manner similar to that described above. For example, the detection platform may train the machine learning model based on one or more parameters associated with a facial temperature, such as an outdoors temperature, a location of an individual (e.g., indoors or outdoors (e.g., a transaction device, such as an ATM device, may be located indoors or outdoors)), an indoors temperature, an amount of exposed skin of an individual, and/or the like. The detection platform may train the machine learning model, according to the one or more parameters, using historical data associated with facial temperatures. Using the historical data and the one or more parameters as inputs to the machine learning model, the detection platform may train the machine learning model to determine an estimated facial temperature for the facial area of the individual. In some implementations, the detection platform may obtain and utilize a machine learning model that was trained by another device. Since a face-altering technology may reduce heat that is emitted or reflected by the facial area, the detection platform may determine that the appearance of the facial area is likely altered when the temperature of the facial area and the estimated temperature for the facial area do not correspond (e.g., the temperature of the facial area is outside of a threshold range, such as &#xb1;1%, &#xb1;5%, and/or the like, of the estimated temperature for the facial area). Conversely, the detection platform may determine that the appearance of the facial area is likely not altered when the temperature of the facial area and the estimated temperature for the facial area correspond.</p><p id="p-0038" num="0037">As another example, the detection platform may compare the level of moisture of the facial area to an estimated level of moisture for the facial area. The detection platform may utilize a machine learning model to determine the estimated moisture level, in a manner similar to that described above. Since a face-altering technology may obstruct perspiration present on the facial area, the detection platform may determine that the appearance of the facial area is likely altered when the level of moisture of the facial area and the estimated level of moisture for the facial area do not correspond (e.g., the level of moisture of the facial area is outside of a threshold range, such as &#xb1;1%, &#xb1;5%, and/or the like, of the estimated level of moisture for the facial area). Conversely, the detection platform may determine that the appearance of the facial area is likely not altered when the level of moisture of the facial area and the estimated level of moisture for the facial area correspond.</p><p id="p-0039" num="0038">In some implementations, the detection platform may determine whether the appearance of the facial area of the individual is likely altered by a face-altering technology based on the measurement of the recess of the eye of the individual. For example, the detection platform may determine whether the measurement of the recess exceeds a threshold value associated with a normal recess of an eye. The threshold value may be based on an aggregate (e.g., average) measurement determined from a plurality of individuals (e.g., a plurality of individuals having one or more facial features that are similar to those of the individual). Since a face-altering technology, such as a mask, may protrude from the individual's face, thereby causing a deeper eye recess, the detection platform may determine that the appearance of the facial area is likely altered when the measurement of the recess of the eye of the individual exceeds the threshold value. Conversely, the detection platform may determine that the appearance of the facial area is likely not altered when the measurement of the recess of the eye of the individual does not exceed the threshold value.</p><p id="p-0040" num="0039">In some implementations, the detection platform may employ any combination of the foregoing techniques for determining whether the appearance of the facial area of the individual is likely altered by a face-altering technology. For example, the detection platform may determine whether the appearance of the facial area is likely altered based on any one or more of whether a temperature of the facial area corresponds to a temperature of the area of exposed skin, whether a level of moisture of the facial area corresponds to a level of moisture of the area of exposed skin, whether a temperature of the facial area corresponds to an estimated temperature for the facial area, whether a level of moisture of the facial area corresponds to an estimated level of moisture for the facial area, or whether a recess of an eye of the individual exceeds a threshold value.</p><p id="p-0041" num="0040">In some implementations, the detection platform may determine whether the appearance of the facial area of the individual is likely altered by a face-altering technology prior to performing facial recognition on the individual. For example, the detection platform may determine whether the appearance of the facial area is likely altered before the individual requests access to a secure area, to thereby determine whether facial recognition is to be performed on the individual. For example, the detection platform may determine whether the appearance of the facial area is likely altered when the individual is approaching a transaction device (e.g., before the individual requests access to a secure area via the transaction device). In this way, the detection platform may determine a manner in which the individual is to be authenticated before the individual requests access to a secure area, thereby improving a speed and an efficiency of an authentication process.</p><p id="p-0042" num="0041">As shown by reference number <b>125</b>, the detection platform may selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area is likely altered by a face-altering technology. For example, the detection platform may determine not to perform facial recognition on the facial area based on determining that the appearance of the facial area is likely altered. In such a case, the detection platform may determine that the individual is to be authenticated according to a technique that does not include facial recognition. For example, the detection platform may prompt, or cause another device to prompt, the individual to provide a password, a personal identification number (PIN), a biometric identifier, and/or the like based on determining that the appearance of the facial area is likely altered. In this way, the detection platform conserves computing resources associated with facial recognition by determining not to perform facial recognition when facial recognition is unlikely to produce an accurate result (e.g., when the individual is utilizing a face-altering technology).</p><p id="p-0043" num="0042">In some implementations, based on determining that the appearance of the facial area is likely altered, the detection platform may administer, or cause another device to administer, a facial expression challenge to the individual. For example, the detection platform, or the other device, may request that the individual perform one or more facial expressions, such as smiling, frowning, pouting, raising one or both eyebrows, and/or the like. Continuing with the previous example, the detection platform, or the other device, may determine whether the performed facial expressions indicate that the appearance of the facial area of the individual is likely altered by a face-altering technology. This may include determining (e.g., using a machine learning model) whether a relative position, size, shape, and/or the like of the performed facial expressions correspond to reference facial expressions (e.g., facial expressions performed by one or more individuals not utilizing a face-altering technology). Based on determining that one or more of the performed facial expressions do not correspond to the reference facial expressions, the detection platform, or the other device, may determine that the individual is not to be authenticated according to facial recognition.</p><p id="p-0044" num="0043">In some implementations, the detection platform may perform one or more actions based on determining that the appearance of the facial area of the individual is likely altered by a face-altering technology. For example, if the individual is attempting to access a secure area (e.g., an account), the detection platform may lock the secure area (e.g., for a time period), require one or more additional authentication factors to access the secure area (e.g., for a time period), transmit a notification to a user device of an owner of the secure area indicating suspicious activity, transmit a notification to a user device of a law enforcement agency indicating suspicious activity, and/or the like. As another example, the detection platform may perform facial recognition on the facial area of the individual in order to determine an identity of a person being impersonated by the individual, and may lock one or more accounts associated with the person, require one or more additional authentication factors to access the one or more accounts, transmit a notification to a user device of the person indicating an impersonation attempt, transmit a notification to a user device of a law enforcement agency indicating an impersonation attempt, and/or the like.</p><p id="p-0045" num="0044">In some implementations, the detection platform may determine to authenticate the individual according to facial recognition based on determining that the appearance of the facial area of the individual is likely not altered by a face-altering technology. In such a case, the detection platform may utilize a facial recognition system, as described above, to determine an identity of the individual based on an image of the individual. In some implementations, the detection platform may determine whether the identity matches that of an owner of an account the individual was attempting to access, an owner of a credential that the individual was using, and/or the like to thereby authenticate the individual. In this way, the detection platform performs facial recognition when facial recognition is likely to produce an accurate result (e.g., when the individual is not utilizing a face-altering technology), thereby conserving computing resources and reducing instances of misidentification as well as illegal activity that may be permitted by misidentification.</p><p id="p-0046" num="0045">As indicated above, <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> are provided as one or more examples. Other examples may differ from what is described with regard to <figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref>.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an example environment <b>200</b> in which systems and/or methods described herein may be implemented. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, environment <b>200</b> may include a transaction device <b>210</b>, a camera <b>220</b>, a sensor <b>230</b>, a detection platform <b>240</b>, a computing resource <b>245</b>, a cloud computing environment <b>250</b>, and a network <b>260</b>. Devices of environment <b>200</b> may interconnect via wired connections, wireless connections, or a combination of wired and wireless connections.</p><p id="p-0048" num="0047">Transaction device <b>210</b> includes one or more devices capable of receiving, generating, storing, processing, and/or providing information relating to a transaction (e.g., a transaction relating to a cash withdrawal, a use of a transaction card, and/or the like). For example, transaction device <b>210</b> may include an ATM device, a point of sale (POS) device, a kiosk device, and/or the like. An ATM device may include an electronic telecommunications device that enables customers of financial institutions to perform financial transactions, such as cash withdrawals, deposits, transferring funds, obtaining account information, and/or the like, at any time and without direct interaction with employees of the financial institutions. A POS device may include an electronic device used to process transaction card payments at a retail location. The POS device may read information from a transaction card (e.g., a credit card, a debit card, a gift card, and/or the like), and may determine whether there are sufficient funds in an account associated with the transaction card for a transaction. The POS device may cause a transfer of funds from the account associated with the transaction card to an account of a retailer and may record the transaction. A kiosk device may include a computer terminal featuring specialized hardware and software that provides access to information and/or applications for communication, commerce, entertainment, education, and/or the like.</p><p id="p-0049" num="0048">Camera <b>220</b> includes one or more devices capable of capturing video data, an image, and/or the like. For example, camera <b>220</b> may include a video camera, a still image camera, an infrared camera, and/or the like. In some implementations, camera <b>220</b> may capture an image of an individual and may provide the image to detection platform <b>240</b>, as described elsewhere herein. In some implementations, camera <b>220</b> may process an image in a manner that is the same as or similar to that described elsewhere herein, and may provide a result of processing the image to transaction device <b>210</b> or detection platform <b>240</b> for further processing, for analysis, and/or the like, as described elsewhere herein.</p><p id="p-0050" num="0049">Sensor <b>230</b> includes one or more devices capable of detecting and reporting data relating to a measurable parameter of an individual that is a candidate for authentication by facial recognition. For example, sensor <b>230</b> may include a temperature sensor, an infrared sensor, a moisture sensor, and/or the like. In some implementations, sensor <b>230</b> may process the data in a manner that is the same as or similar to that described elsewhere herein, and may provide a result of processing the data to transaction device <b>210</b> or detection platform <b>240</b> for further processing, for analysis, and/or the like, as described elsewhere herein.</p><p id="p-0051" num="0050">Detection platform <b>240</b> includes one or more computing resources assigned to determine whether an appearance of a facial area of an individual is likely altered by a face-altering technology. For example, detection platform <b>240</b> may be a platform implemented by cloud computing environment <b>250</b> that may select an individual that is a candidate for facial recognition, identify a facial area of the individual, identify a characteristic of the facial area, determine whether an appearance of the facial area is likely altered by a face-altering technology, perform facial recognition on the facial area, and/or the like. In some implementations, detection platform <b>240</b> is implemented by computing resources <b>245</b> of cloud computing environment <b>250</b>.</p><p id="p-0052" num="0051">Detection platform <b>240</b> may include a server device or a group of server devices. In some implementations, detection platform <b>240</b> may be hosted in cloud computing environment <b>250</b>. Notably, while implementations described herein may describe detection platform <b>240</b> as being hosted in cloud computing environment <b>250</b>, in some implementations, detection platform <b>240</b> may be non-cloud-based or may be partially cloud-based. For example, in some implementations, detection platform <b>240</b> may be implemented by transaction device <b>210</b>.</p><p id="p-0053" num="0052">Cloud computing environment <b>250</b> includes an environment that delivers computing as a service, whereby shared resources, services, and/or the like may be provided to transaction device <b>210</b>, camera <b>220</b>, sensor <b>230</b>, and/or the like. Cloud computing environment <b>250</b> may provide computation, software, data access, storage, and/or other services that do not require end-user knowledge of a physical location and configuration of a system and/or a device that delivers the services. As shown, cloud computing environment <b>250</b> may include detection platform <b>240</b> and computing resource <b>245</b>.</p><p id="p-0054" num="0053">Computing resource <b>245</b> includes one or more personal computers, workstation computers, server devices, or another type of computation and/or communication device. In some implementations, computing resource <b>245</b> may host detection platform <b>240</b>. The cloud resources may include compute instances executing in computing resource <b>245</b>, storage devices provided in computing resource <b>245</b>, data transfer devices provided by computing resource <b>245</b>, and/or the like. In some implementations, computing resource <b>245</b> may communicate with other computing resources <b>245</b> via wired connections, wireless connections, or a combination of wired and wireless connections.</p><p id="p-0055" num="0054">As further shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, computing resource <b>245</b> may include a group of cloud resources, such as one or more applications (&#x201c;APPs&#x201d;) <b>245</b>-<b>1</b>, one or more virtual machines (&#x201c;VMs&#x201d;) <b>245</b>-<b>2</b>, virtualized storage (&#x201c;VSs&#x201d;) <b>245</b>-<b>3</b>, one or more hypervisors (&#x201c;HYPs&#x201d;) <b>245</b>-<b>4</b>, or the like.</p><p id="p-0056" num="0055">Application <b>245</b>-<b>1</b> includes one or more software applications that may be provided to or accessed by transaction device <b>210</b>, camera <b>220</b>, sensor <b>230</b>, and/or the like. Application <b>245</b>-<b>1</b> may eliminate a need to install and execute the software applications on transaction device <b>210</b>, camera <b>220</b>, sensor <b>230</b>, and/or the like. For example, application <b>245</b>-<b>1</b> may include software associated with detection platform <b>240</b> and/or any other software capable of being provided via cloud computing environment <b>250</b>. In some implementations, one application <b>245</b>-<b>1</b> may send/receive information to/from one or more other applications <b>245</b>-<b>1</b>, via virtual machine <b>245</b>-<b>2</b>.</p><p id="p-0057" num="0056">Virtual machine <b>245</b>-<b>2</b> includes a software implementation of a machine (e.g., a computer) that executes programs like a physical machine. Virtual machine <b>245</b>-<b>2</b> may be either a system virtual machine or a process virtual machine, depending upon use and degree of correspondence to any real machine by virtual machine <b>245</b>-<b>2</b>. A system virtual machine may provide a complete system platform that supports execution of a complete operating system (&#x201c;OS&#x201d;). A process virtual machine may execute a single program and may support a single process. In some implementations, virtual machine <b>245</b>-<b>2</b> may execute on behalf of a user, and may manage infrastructure of cloud computing environment <b>250</b>, such as data management, synchronization, or long-duration data transfers.</p><p id="p-0058" num="0057">Virtualized storage <b>245</b>-<b>3</b> includes one or more storage systems and/or one or more devices that use virtualization techniques within the storage systems or devices of computing resource <b>245</b>. In some implementations, within the context of a storage system, types of virtualizations may include block virtualization and file virtualization. Block virtualization may refer to abstraction (or separation) of logical storage from physical storage so that the storage system may be accessed without regard to physical storage or heterogeneous structure. The separation may permit administrators of the storage system flexibility in how the administrators manage storage for end users. File virtualization may eliminate dependencies between data accessed at a file level and a location where files are physically stored. This may enable optimization of storage use, server consolidation, and/or performance of non-disruptive file migrations.</p><p id="p-0059" num="0058">Hypervisor <b>245</b>-<b>4</b> provides hardware virtualization techniques that allow multiple operating systems (e.g., &#x201c;guest operating systems&#x201d;) to execute concurrently on a host computer, such as computing resource <b>245</b>. Hypervisor <b>245</b>-<b>4</b> may present a virtual operating platform to the guest operating systems and may manage the execution of the guest operating systems. Multiple instances of a variety of operating systems may share virtualized hardware resources.</p><p id="p-0060" num="0059">Network <b>260</b> includes one or more wired and/or wireless networks. For example, network <b>260</b> may include a cellular network (e.g., a long-term evolution (LTE) network, a code division multiple access (CDMA) network, a 3G network, a 4G network, a 5G network, another type of next generation network, and/or the like), a public land mobile network (PLMN), a local area network (LAN), a wide area network (WAN), a metropolitan area network (MAN), a telephone network (e.g., the Public Switched Telephone Network (PSTN)), a private network, an ad hoc network, an intranet, the Internet, a fiber optic-based network, a cloud computing network, or the like, and/or a combination of these or other types of networks.</p><p id="p-0061" num="0060">The quantity and arrangement of devices and networks shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> are provided as one or more examples. In practice, there may be additional devices and/or networks, fewer devices and/or networks, different devices and/or networks, or differently arranged devices and/or networks than those shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Furthermore, two or more devices shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented within a single device, or a single device shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be implemented as multiple, distributed devices. Additionally, or alternatively, a set of devices (e.g., one or more devices) of environment <b>200</b> may perform one or more functions described as being performed by another set of devices of environment <b>200</b>.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram of example components of a device <b>300</b>. Device <b>300</b> may correspond to transaction device <b>210</b>, camera <b>220</b>, sensor <b>230</b>, detection platform <b>240</b>, and/or computing resource <b>245</b>. In some implementations, transaction device <b>210</b>, camera <b>220</b>, sensor <b>230</b>, detection platform <b>240</b>, and/or computing resource <b>245</b> may include one or more devices <b>300</b> and/or one or more components of device <b>300</b>. As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, device <b>300</b> may include a bus <b>310</b>, a processor <b>320</b>, a memory <b>330</b>, a storage component <b>340</b>, an input component <b>350</b>, an output component <b>360</b>, and a communication interface <b>370</b>.</p><p id="p-0063" num="0062">Bus <b>310</b> includes a component that permits communication among multiple components of device <b>300</b>. Processor <b>320</b> is implemented in hardware, firmware, and/or a combination of hardware and software. Processor <b>320</b> is a central processing unit (CPU), a graphics processing unit (GPU), an accelerated processing unit (APU), a microprocessor, a microcontroller, a digital signal processor (DSP), a field-programmable gate array (FPGA), an application-specific integrated circuit (ASIC), or another type of processing component. In some implementations, processor <b>320</b> includes one or more processors capable of being programmed to perform a function. Memory <b>330</b> includes a random access memory (RANI), a read only memory (ROM), and/or another type of dynamic or static storage device (e.g., a flash memory, a magnetic memory, and/or an optical memory) that stores information and/or instructions for use by processor <b>320</b>.</p><p id="p-0064" num="0063">Storage component <b>340</b> stores information and/or software related to the operation and use of device <b>300</b>. For example, storage component <b>340</b> may include a hard disk (e.g., a magnetic disk, an optical disk, and/or a magneto-optic disk), a solid state drive (SSD), a compact disc (CD), a digital versatile disc (DVD), a floppy disk, a cartridge, a magnetic tape, and/or another type of non-transitory computer-readable medium, along with a corresponding drive.</p><p id="p-0065" num="0064">Input component <b>350</b> includes a component that permits device <b>300</b> to receive information, such as via user input (e.g., a touch screen display, a keyboard, a keypad, a mouse, a button, a switch, and/or a microphone). Additionally, or alternatively, input component <b>350</b> may include a component for determining location (e.g., a global positioning system (GPS) component) and/or a sensor (e.g., an accelerometer, a gyroscope, an actuator, another type of positional or environmental sensor, and/or the like). Output component <b>360</b> includes a component that provides output information from device <b>300</b> (via, e.g., a display, a speaker, a haptic feedback component, an audio or visual indicator, and/or the like).</p><p id="p-0066" num="0065">Communication interface <b>370</b> includes a transceiver-like component (e.g., a transceiver, a separate receiver, a separate transmitter, and/or the like) that enables device <b>300</b> to communicate with other devices, such as via a wired connection, a wireless connection, or a combination of wired and wireless connections. Communication interface <b>370</b> may permit device <b>300</b> to receive information from another device and/or provide information to another device. For example, communication interface <b>370</b> may include an Ethernet interface, an optical interface, a coaxial interface, an infrared interface, a radio frequency (RF) interface, a universal serial bus (USB) interface, a Wi-Fi interface, a cellular network interface, and/or the like.</p><p id="p-0067" num="0066">Device <b>300</b> may perform one or more processes described herein. Device <b>300</b> may perform these processes based on processor <b>320</b> executing software instructions stored by a non-transitory computer-readable medium, such as memory <b>330</b> and/or storage component <b>340</b>. As used herein, the term &#x201c;computer-readable medium&#x201d; refers to a non-transitory memory device. A memory device includes memory space within a single physical storage device or memory space spread across multiple physical storage devices.</p><p id="p-0068" num="0067">Software instructions may be read into memory <b>330</b> and/or storage component <b>340</b> from another computer-readable medium or from another device via communication interface <b>370</b>. When executed, software instructions stored in memory <b>330</b> and/or storage component <b>340</b> may cause processor <b>320</b> to perform one or more processes described herein. Additionally, or alternatively, hardware circuitry may be used in place of or in combination with software instructions to perform one or more processes described herein. Thus, implementations described herein are not limited to any specific combination of hardware circuitry and software.</p><p id="p-0069" num="0068">The quantity and arrangement of components shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are provided as an example. In practice, device <b>300</b> may include additional components, fewer components, different components, or differently arranged components than those shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Additionally, or alternatively, a set of components (e.g., one or more components) of device <b>300</b> may perform one or more functions described as being performed by another set of components of device <b>300</b>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart of an example process <b>400</b> for detecting attempts to defeat facial recognition. In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be performed by a detection platform (e.g., detection platform <b>240</b>). In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>4</b></figref> may be performed by another device or a group of devices separate from or including the detection platform, such as a transaction device (e.g., transaction device <b>210</b>), a camera (e.g., camera <b>220</b>), a sensor (e.g., sensor <b>230</b>), and/or the like.</p><p id="p-0071" num="0070">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, process <b>400</b> may include selecting an individual that is a candidate for authentication by facial recognition (block <b>410</b>). For example, the detection platform (e.g., using camera <b>220</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, input component <b>350</b>, communication interface <b>370</b>, and/or the like) may select an individual that is a candidate for authentication by facial recognition, as described above.</p><p id="p-0072" num="0071">As further shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, process <b>400</b> may include identifying, based on selecting the individual, a facial area of the individual and an area of exposed skin of the individual, wherein the area of exposed skin of the individual is not associated with the facial area of the individual (block <b>420</b>). For example, the detection platform (e.g., using camera <b>220</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may identify, based on selecting the individual, a facial area of the individual and an area of exposed skin of the individual, as described above. In some implementations, the area of exposed skin of the individual is not associated with the facial area of the individual.</p><p id="p-0073" num="0072">As further shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, process <b>400</b> may include obtaining a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual (block <b>430</b>). For example, the detection platform (e.g., using sensor <b>230</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, input component <b>350</b>, communication interface <b>370</b>, and/or the like) may obtain a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual, as described above.</p><p id="p-0074" num="0073">As further shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, process <b>400</b> may include determining, based on the first temperature and the second temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, wherein the first temperature corresponding to the second temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and wherein the first temperature not corresponding to the second temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology (block <b>440</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may determine, based on the first temperature and the second temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, as described above. In some implementations, the first temperature corresponding to the second temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology. In some implementations, the first temperature not corresponding to the second temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><p id="p-0075" num="0074">As further shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, process <b>400</b> may include selectively performing facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology (block <b>450</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology, as described above.</p><p id="p-0076" num="0075">Process <b>400</b> may include additional implementations, such as any single implementation or any combination of implementations described below and/or in connection with one or more other processes described elsewhere herein.</p><p id="p-0077" num="0076">In a first implementation, selecting the individual that is the candidate for authentication by facial recognition may include selecting the individual that is the candidate for authentication by facial recognition based on a proximity of the individual to a transaction device.</p><p id="p-0078" num="0077">In a second implementation, alone or in combination with the first implementation, process <b>400</b> may further include determining a first perspiration level on the facial area of the individual and determining a second perspiration level on the area of exposed skin of the individual, where the first temperature corresponding to the second temperature and the first perspiration level corresponding to the second perspiration level indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and where the first temperature not corresponding to the second temperature or the first perspiration level not corresponding to the second perspiration level indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology. In a third implementation, alone or in combination with one or more of the first and second implementations, the first perspiration level and the second perspiration level may be determined by one or more of computer vision, infrared moisture detection, or galvanic skin response detection.</p><p id="p-0079" num="0078">In a fourth implementation, alone or in combination with one or more of the first through third implementations, process <b>400</b> may further include determining that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and performing authentication of the individual using facial recognition, or alternatively, determining that the appearance of the facial area of the individual is likely altered by the face-altering technology, and performing authentication of the individual using a technique that does not include facial recognition.</p><p id="p-0080" num="0079">In a fifth implementation, alone or in combination with one or more of the first through fourth implementations, the face-altering technology may be at least one of a mask, a facial prosthetic, a bandage, or an eyepatch.</p><p id="p-0081" num="0080">Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows example blocks of process <b>400</b>, in some implementations, process <b>400</b> may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Additionally, or alternatively, two or more of the blocks of process <b>400</b> may be performed in parallel.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart of an example process <b>500</b> for detecting attempts to defeat facial recognition. In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by a detection platform (e.g., detection platform <b>240</b>). In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>5</b></figref> may be performed by another device or a group of devices separate from or including the detection platform, such as a transaction device (e.g., transaction device <b>210</b>), a camera (e.g., camera <b>220</b>), a sensor (e.g., sensor <b>230</b>), and/or the like.</p><p id="p-0083" num="0082">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include selecting an individual that is a candidate for authentication by facial recognition (block <b>510</b>). For example, the detection platform (e.g., using camera <b>220</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, input component <b>350</b>, communication interface <b>370</b>, and/or the like) may select an individual that is a candidate for authentication by facial recognition, as described above.</p><p id="p-0084" num="0083">As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include obtaining a temperature associated with a facial area of the individual (block <b>520</b>). For example, the detection platform (e.g., using sensor <b>230</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, input component <b>350</b>, communication interface <b>370</b>, and/or the like) may obtain a temperature associated with a facial area of the individual, as described above.</p><p id="p-0085" num="0084">As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include determining an estimated temperature for the facial area of the individual, wherein the estimated temperature is based on conditions of an environment associated with the individual (block <b>530</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may determine an estimated temperature for the facial area of the individual, as described above. In some implementations, the estimated temperature is based on conditions of an environment associated with the individual.</p><p id="p-0086" num="0085">As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include determining, based on the temperature and the estimated temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, wherein the temperature corresponding to the estimated temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and wherein the temperature not corresponding to the estimated temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology (block <b>540</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may determine, based on the temperature and the estimated temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, as described above. In some implementations, the temperature corresponding to the estimated temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology. In some implementations, the temperature not corresponding to the estimated temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><p id="p-0087" num="0086">As further shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, process <b>500</b> may include selectively performing facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology (block <b>550</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology, as described above.</p><p id="p-0088" num="0087">Process <b>500</b> may include additional implementations, such as any single implementation or any combination of implementations described below and/or in connection with one or more other processes described elsewhere herein.</p><p id="p-0089" num="0088">In a first implementation, obtaining the temperature associated with the facial area of the individual and determining whether the temperature corresponds to the estimated temperature for the facial area of the individual may include obtaining the temperature associated with the facial area of the individual and determining whether the temperature corresponds to the estimated temperature for the facial area of the individual before the individual is within a threshold distance from a transaction device.</p><p id="p-0090" num="0089">In a second implementation, alone or in combination with the first implementation, process <b>500</b> may further include determining that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and performing authentication of the individual using facial recognition, or alternatively, determining that the appearance of the facial area of the individual is likely altered by the face-altering technology, and performing authentication of the individual using a technique that does not include facial recognition.</p><p id="p-0091" num="0090">In a third implementation, alone or in combination with one or more of the first and second implementations, determining the estimated temperature for the facial area of the individual may include determining the estimated temperature for the facial area of the individual using a machine learning model.</p><p id="p-0092" num="0091">In a fourth implementation, alone or in combination with one or more of the first through third implementations, process <b>500</b> may further include determining whether a recess associated with an eye of the individual satisfies a threshold value, where the temperature corresponding to the estimated temperature and the recess associated with the eye satisfying the threshold value indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and where the temperature not corresponding to the estimated temperature or the recess associated with the eye failing to satisfy the threshold value indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><p id="p-0093" num="0092">In a fifth implementation, alone or in combination with one or more of the first through fourth implementations, process <b>500</b> may further include determining that the appearance of the facial area of the individual is likely altered by the face-altering technology, requesting that the individual perform one or more facial expressions, determining that the one or more facial expressions indicate that the appearance of the facial area of the individual is likely altered by the face-altering technology, and determining that the individual is not to be authenticated according to facial recognition.</p><p id="p-0094" num="0093">Although <figref idref="DRAWINGS">FIG. <b>5</b></figref> shows example blocks of process <b>500</b>, in some implementations, process <b>500</b> may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. Additionally, or alternatively, two or more of the blocks of process <b>500</b> may be performed in parallel.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow chart of an example process <b>600</b> for detecting attempts to defeat facial recognition. In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be performed by a detection platform (e.g., detection platform <b>240</b>). In some implementations, one or more process blocks of <figref idref="DRAWINGS">FIG. <b>6</b></figref> may be performed by another device or a group of devices separate from or including the detection platform, such as a transaction device (e.g., transaction device <b>210</b>), a camera (e.g., camera <b>220</b>), a sensor (e.g., sensor <b>230</b>), and/or the like.</p><p id="p-0096" num="0095">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, process <b>600</b> may include selecting an individual that is a candidate for authentication by facial recognition (block <b>610</b>). For example, the detection platform (e.g., using camera <b>220</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, input component <b>350</b>, communication interface <b>370</b>, and/or the like) may select an individual that is a candidate for authentication by facial recognition, as described above.</p><p id="p-0097" num="0096">As further shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, process <b>600</b> may include identifying, based on selecting the individual, a facial area of the individual and an area of exposed skin of the individual, wherein the area of exposed skin of the individual is not associated with the facial area of the individual (block <b>620</b>). For example, the detection platform (e.g., using camera <b>220</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may identify, based on selecting the individual, a facial area of the individual and an area of exposed skin of the individual, as described above. In some implementations, the area of exposed skin of the individual is not associated with the facial area of the individual.</p><p id="p-0098" num="0097">As further shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, process <b>600</b> may include obtaining a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual (block <b>630</b>). For example, the detection platform (e.g., using sensor <b>230</b>, computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, input component <b>350</b>, communication interface <b>370</b>, and/or the like) may obtain a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual, as described above.</p><p id="p-0099" num="0098">As further shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, process <b>600</b> may include determining an estimated temperature for the facial area of the individual, wherein the estimated temperature is based on conditions of an environment associated with the individual (block <b>640</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may determine an estimated temperature for the facial area of the individual, as described above. In some implementations, the estimated temperature is based on conditions of an environment associated with the individual.</p><p id="p-0100" num="0099">As further shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, process <b>600</b> may include determining, based on the first temperature, the second temperature, and the estimated temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, wherein the first temperature corresponding to the second temperature or the estimated temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and wherein the first temperature not corresponding to the second temperature and the estimated temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology (block <b>650</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may determine, based on the first temperature, the second temperature, and the estimated temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology, as described above. In some implementations, the first temperature corresponding to the second temperature or the estimated temperature indicates that the appearance of the facial area of the individual is likely not altered by the face-altering technology. In some implementations, the first temperature not corresponding to the second temperature and the estimated temperature indicates that the appearance of the facial area of the individual is likely altered by the face-altering technology.</p><p id="p-0101" num="0100">As further shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, process <b>600</b> may include selectively performing facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology (block <b>660</b>). For example, the detection platform (e.g., using computing resource <b>245</b>, processor <b>320</b>, memory <b>330</b>, storage component <b>340</b>, and/or the like) may selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology, as described above.</p><p id="p-0102" num="0101">Process <b>600</b> may include additional implementations, such as any single implementation or any combination of implementations described below and/or in connection with one or more other processes described elsewhere herein.</p><p id="p-0103" num="0102">In a first implementation, selecting the individual that is the candidate for authentication by facial recognition may include selecting the individual that is the candidate for authentication by facial recognition based on a request by the individual to access an account.</p><p id="p-0104" num="0103">In a second implementation, alone or in combination with the first implementation, process <b>600</b> may further include determining that the appearance of the facial area of the individual is likely not altered by the face-altering technology, and performing authentication of the individual using facial recognition, or alternatively, determining that the appearance of the facial area of the individual is likely altered by the face-altering technology, and performing authentication of the individual using a technique that does not include facial recognition.</p><p id="p-0105" num="0104">In a third implementation, alone or in combination with one or more of the first and second implementations, identifying the facial area of the individual and the area of exposed skin of the individual may include identifying the facial area of the individual and the area of exposed skin of the individual using computer vision.</p><p id="p-0106" num="0105">In a fourth implementation, alone or in combination with one or more of the first through third implementations, obtaining the first temperature associated with the facial area of the individual and the second temperature associated with the area of exposed skin of the individual may include obtaining the first temperature associated with the facial area of the individual and the second temperature associated with the area of exposed skin of the individual from an infrared sensor.</p><p id="p-0107" num="0106">Although <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows example blocks of process <b>600</b>, in some implementations, process <b>600</b> may include additional blocks, fewer blocks, different blocks, or differently arranged blocks than those depicted in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Additionally, or alternatively, two or more of the blocks of process <b>600</b> may be performed in parallel.</p><p id="p-0108" num="0107">The foregoing disclosure provides illustration and description, but is not intended to be exhaustive or to limit the implementations to the precise form disclosed. Modifications and variations may be made in light of the above disclosure or may be acquired from practice of the implementations.</p><p id="p-0109" num="0108">As used herein, the term &#x201c;component&#x201d; is intended to be broadly construed as hardware, firmware, or a combination of hardware and software.</p><p id="p-0110" num="0109">As used herein, satisfying a threshold may, depending on the context, refer to a value being greater than the threshold, more than the threshold, higher than the threshold, greater than or equal to the threshold, less than the threshold, fewer than the threshold, lower than the threshold, less than or equal to the threshold, equal to the threshold, or the like.</p><p id="p-0111" num="0110">It will be apparent that systems and/or methods described herein may be implemented in different forms of hardware, firmware, or a combination of hardware and software. The actual specialized control hardware or software code used to implement these systems and/or methods is not limiting of the implementations. Thus, the operation and behavior of the systems and/or methods are described herein without reference to specific software code&#x2014;it being understood that software and hardware can be designed to implement the systems and/or methods based on the description herein.</p><p id="p-0112" num="0111">Even though particular combinations of features are recited in the claims and/or disclosed in the specification, these combinations are not intended to limit the disclosure of various implementations. In fact, many of these features may be combined in ways not specifically recited in the claims and/or disclosed in the specification. Although each dependent claim listed below may directly depend on only one claim, the disclosure of various implementations includes each dependent claim in combination with every other claim in the claim set.</p><p id="p-0113" num="0112">No element, act, or instruction used herein should be construed as critical or essential unless explicitly described as such. Also, as used herein, the articles &#x201c;a&#x201d; and &#x201c;an&#x201d; are intended to include one or more items, and may be used interchangeably with &#x201c;one or more.&#x201d; Further, as used herein, the article &#x201c;the&#x201d; is intended to include one or more items referenced in connection with the article &#x201c;the&#x201d; and may be used interchangeably with &#x201c;the one or more.&#x201d; Furthermore, as used herein, the term &#x201c;set&#x201d; is intended to include one or more items (e.g., related items, unrelated items, a combination of related and unrelated items, and/or the like), and may be used interchangeably with &#x201c;one or more.&#x201d; Where only one item is intended, the phrase &#x201c;only one&#x201d; or similar language is used. Also, as used herein, the terms &#x201c;has,&#x201d; &#x201c;have,&#x201d; &#x201c;having,&#x201d; or the like are intended to be open-ended terms. Further, the phrase &#x201c;based on&#x201d; is intended to mean &#x201c;based, at least in part, on&#x201d; unless explicitly stated otherwise. Also, as used herein, the term &#x201c;or&#x201d; is intended to be inclusive when used in a series and may be used interchangeably with &#x201c;and/or,&#x201d; unless explicitly stated otherwise (e.g., if used in combination with &#x201c;either&#x201d; or &#x201c;only one of&#x201d;).</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method, comprising:<claim-text>identifying, by a device, a facial area of an individual and an area of exposed skin of the individual, wherein the area of exposed skin of the individual is not associated with the facial area of the individual;</claim-text><claim-text>obtaining, by the device, a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual;</claim-text><claim-text>determining, by the device and based on a comparison of the first temperature and the second temperature, whether the first temperature is outside of a threshold difference in temperature range with respect to the second temperature; and</claim-text><claim-text>selectively performing, by the device, facial recognition on the facial area of the individual based on determining whether the first temperature is outside of the threshold difference in temperature range with respect to the second temperature.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>identifying a characteristic of the facial area of the individual;</claim-text><claim-text>determining, based on determining whether the first temperature is outside of the threshold difference in temperature range with respect to the second temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology based on the characteristic; and</claim-text><claim-text>wherein selectively performing, by the device, facial recognition on the facial area of the individual comprises:<claim-text>selectively performing, by the device, facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>selecting the individual based on a proximity of the individual to a transaction device.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein identifying the facial area of the individual and the area of exposed skin of the individual comprises:<claim-text>identifying the facial area of the individual and the area of exposed skin of the individual using machine learning.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein obtaining the first temperature and the second temperature comprises:<claim-text>obtaining the first temperature and the second temperature from a temperature sensor.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>obtaining data identifying an actual level of moisture of the facial area; and</claim-text><claim-text>wherein selectively performing the facial recognition on the facial area of the individual comprises:<claim-text>selectively performing the facial recognition on the facial area of the individual based on a comparison of the actual level of moisture to an estimated level of moisture.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>selecting the individual based on a credential associated with the individual.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A device, comprising:<claim-text>one or more memories; and</claim-text><claim-text>one or more processors, communicatively coupled to the one or more memories, configured to:<claim-text>identify a facial area of an individual and an area of skin of the individual, wherein the area of skin of the individual is not associated with the facial area of the individual;</claim-text><claim-text>obtain a first temperature associated with the facial area of the individual and a second temperature associated with the area of skin of the individual;</claim-text><claim-text>determine whether the first temperature is outside of a threshold difference in temperature range with respect to the second temperature; and</claim-text><claim-text>selectively perform facial recognition on the facial area of the individual based on determining whether the first temperature is outside of the threshold difference in temperature range with respect to the second temperature.</claim-text></claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are further configured to:<claim-text>determine that the facial area of the individual is likely not altered by face-altering technology; and</claim-text><claim-text>wherein the one or more processors, when selectively performing facial recognition on the facial area of the individual, are configured to:<claim-text>perform authentication of the individual using facial recognition.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are further configured to:<claim-text>determine that the facial area of the individual is likely altered by face-altering technology; and</claim-text><claim-text>wherein the one or more processors, when selectively performing facial recognition on the facial area of the individual, are to:<claim-text>perform authentication of the individual using a technique that does not include facial recognition.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are further configured to:<claim-text>obtain data identifying an actual level of moisture of the facial area; and</claim-text><claim-text>wherein the one or more processors, when selectively performing facial recognition on the facial area of the individual, are to:<claim-text>selectively perform the facial recognition on the facial area of the individual based on a comparison of the actual level of moisture to an estimated level of moisture.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are further configured to:<claim-text>determine, based on determining whether the first temperature is outside of the threshold difference in temperature range with respect to the second temperature, whether an appearance of the facial area of the individual is likely altered by a face-altering technology; and</claim-text><claim-text>wherein the one or more processors, when selectively performing facial recognition on the facial area of the individual, are to:<claim-text>selectively perform facial recognition on the facial area of the individual based on whether the appearance of the facial area of the individual is likely altered by the face-altering technology.</claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors, when obtaining the first temperature and the second temperature, are configured to:<claim-text>obtain the first temperature and the second temperature from a temperature sensor.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more processors are further configured to:<claim-text>select the individual that is a candidate for authentication by facial recognition based on a proximity of the individual to the device.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable medium storing instructions, the instructions comprising:<claim-text>one or more instructions that, when executed by one or more processors, cause the one or more processors to:<claim-text>identify a facial area of an individual and an area of exposed skin of the individual, wherein the area of exposed skin of the individual is not associated with the facial area of the individual;</claim-text><claim-text>obtain a first temperature associated with the facial area of the individual and a second temperature associated with the area of exposed skin of the individual; and</claim-text><claim-text>selectively perform facial recognition on the facial area of the individual based on determining whether the first temperature is outside of a threshold difference in temperature range with respect to the second temperature.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, when executed by the one or more processors, further cause the one or more processors to:<claim-text>select the individual based on a credential associated with the individual.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, when executed by the one or more processors, further cause the one or more processors to:<claim-text>select the individual by facial recognition based on a request by the individual to access one or more of:<claim-text>an account, or a physical area.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, when executed by the one or more processors, further cause the one or more processors to:<claim-text>determine that an appearance of the facial area of the individual is likely altered by face-altering technology; and</claim-text><claim-text>perform authentication of the individual using a technique that does not include facial recognition.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, that cause the one or more processors to identify the facial area of the individual and the area of exposed skin of the individual, cause the one or more processors to:<claim-text>identify the facial area of the individual and the area of exposed skin of the individual using a camera.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more instructions, when executed by the one or more processors, further cause the one or more processors to:<claim-text>determine that the facial area of the individual is likely altered by face-altering technology; and</claim-text><claim-text>wherein the one or more instructions, that cause the one or more processors to selectively perform facial recognition on the facial area of the individual, cause the one or more processors to:<claim-text>perform authentication of the individual using a technique that does not include facial recognition.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>