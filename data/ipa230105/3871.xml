<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003872A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003872</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364603</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>72</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>723</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>13</main-group><subgroup>931</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>415</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>20</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>417</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">TRACKING OBJECTS WITH RADAR DATA</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Zoox, Inc.</orgname><address><city>Foster City</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Qian</last-name><first-name>Jifei</first-name><address><city>Campbell</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cohen</last-name><first-name>Joshua Kriser</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Wang</last-name><first-name>Chuang</first-name><address><city>Sunnyvale</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><orgname>Zoox, Inc.</orgname><role>02</role></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Sensors, including radar sensors, may be used to detect objects in an environment. In an example, a vehicle may include one or more radar sensors that sense objects around the vehicle, e.g., so the vehicle can navigate relative to the objects. A plurality of radar points from one or more radar scans are associated with a sensed object and a representation of the sensed object is determined from the plurality of radar points. The representation may be compared to track information of previously-identified, tracked objects. Based on the comparison, the sensed object may be associated with one of the tracked objects, and, alternatively, the track information may be updated based on the representation. Conversely, the comparison may indicate that the sensed object is not associated with any of the tracked objects. In this instance, the representation may be used to generate a new track, e.g., for the newly-sensed object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="110.83mm" wi="158.75mm" file="US20230003872A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="228.26mm" wi="158.07mm" orientation="landscape" file="US20230003872A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="242.15mm" wi="141.14mm" file="US20230003872A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="230.21mm" wi="158.75mm" file="US20230003872A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="228.26mm" wi="155.11mm" orientation="landscape" file="US20230003872A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="234.36mm" wi="155.79mm" file="US20230003872A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="216.49mm" wi="142.41mm" file="US20230003872A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="215.31mm" wi="136.48mm" file="US20230003872A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">Planning systems for autonomous vehicles can utilize information associated with objects in an environment to determine actions relative to those objects. For example, some existing planning systems for autonomous vehicles consider movement of objects, such as other vehicles on the road, when determining maneuvers for the autonomous vehicle to traverse through the environment. Conventional systems may rely on different types of data to determine information about the object(s). However, some conventional systems have not utilized radar data to track objects as they move relative to the autonomous vehicle, at least because the conventional systems have considered radar data on a per-point basis, which requires larger processing times and/or decreased efficiency in identifying and/or characterizing objects that may be potential obstacles to safe travel.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0002" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0003" num="0002">The detailed description is described with reference to the accompanying figures. In the figures, the left-most digit(s) of a reference number identifies the figure in which the reference number first appears. The use of the same reference numbers in different figures indicates similar or identical components or features.</p><p id="p-0004" num="0003"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic representation illustrating example systems and techniques for using radar data to track objects in an environment, as described herein.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>2</b></figref> includes textual and visual flowcharts to illustrate an example method for associating radar data with a track of a detected object, as described herein.</p><p id="p-0006" num="0005"><figref idref="DRAWINGS">FIG. <b>3</b></figref> includes textual and visual flowcharts to illustrate an example method for generating an object track based at least in part radar data generated by radar sensors on a vehicle, as described herein.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating an example computing system for using radar data to track objects, as described herein.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart illustrating an example method for tracking objects using radar data, as described herein.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating an example method of generating tracks for sensed objects using radar data, as described herein.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart illustrating an example method for controlling a vehicle, such as an autonomous vehicle, relative to a tracked object, as described herein.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0003" level="1">DETAILED DESCRIPTION</heading><p id="p-0011" num="0010">Techniques described herein are directed to characterizing movement of objects in in an environment based on radar data. For example, in implementations described herein, techniques may be used to determine radar returns that are associated with an object in an environment of the sensor, and use those returns to track the sensed object. Although many systems may benefit from the techniques described herein, an example system that implements the techniques of this disclosure may include an autonomous vehicle having multiple radar sensors (and/or sensors of other or different modalities). In one such example the autonomous vehicle can include multiple radar sensors having overlapping fields of view. A first sensor, can capture first sensor data, e.g. as a radar scan (which may include a collection of a number of measurements of radar returns as data points), of an environment and a second sensor can capture second sensor data, e.g., a second radar scan (which may include a second collection of measurements of radar returns as data points) of the environment. Each of the first scan and the second scan may return a number of points, each having associated information. Such information may include position information, e.g., a location of the point relative to the sensor, the vehicle, and/or in a coordinate system (any or all of which may be determined based on a range and/or azimuth angle of the signal), signal strength information, e.g., a radar cross-section (RCS) value, or velocity information, e.g., a velocity of the point relative to the sensor.</p><p id="p-0012" num="0011">In examples described herein, track association techniques can be used to associate returns in the radar scans to objects in the environment. For example, clustering techniques can be used to group points in the first scan with points in the second scan according to any of the information received with the returns. By way of non-limiting example, points in a similar area, e.g., having close locational proximity, may be candidates for clustering as all being related to a single object. However, in other implementations, the signal strength information, RCS, the velocity information, and/or other information determined by the one or more sensors may also or alternatively be used to cluster points according to implementations of this disclosure. For example, the signal strength may be useful to differentiate between a person standing at a street corner and a light post upon which the person is leaning. For instance, points in the first scan and in the second scan having similar characteristics, e.g., location, signal strength, velocity, may be aggregated as a point cluster, to yield a robust representation of the sensed object. In at least some instances, the multiple scans, e.g., sequential scans of the first sensor and/or the second sensor may be aggregated in a cluster associated with an object.</p><p id="p-0013" num="0012">In example implementations, a machine-trained model can aggregate radar returns (e.g., associated with the object) and generates a representation of the object based thereon. The machine-trained model may generate a two-dimensional representation of the object, e.g., as a bounding box, for a plurality of returns. The representation can include information about the object, including but not limited to a velocity of the object, a classification of the object, extents of the object, a position of the object, or the like. For example, the machine-trained model can receive the radar data (e.g., a single instance or multiple instances of the radar data) and output the representation and associated information.</p><p id="p-0014" num="0013">Techniques described herein also include comparing a representation of a sensed object generated from radar data to information about one or more previously-sensed objects. For instance, the representation generated from the sensor data may be compared to track information, e.g., an expected or predicted state of a previously-detected object. Without limitation, the track information can be based on previous sensor data, including but not limited to previous sensor data generated by the radar sensor(s) and/or by one or more additional or other sensor modalities (e.g., LiDAR sensors, image sensors, or the like).</p><p id="p-0015" num="0014">The techniques described herein can compare the representation generated from the radar sensor data to the track information using a number of techniques. In one non-limiting example, a velocity of the sensed objects, e.g., a velocity of the representation, can be compared to a velocity (an expected velocity) of the tracked object. In other examples, the position of the sensed object can be compared to a position (or expected position) of the tracked object. For instance, the positions may be positions in a two-dimensional space of a center of the representation of the sensed object and/or the tracked object. In still further examples, techniques described herein can determine an intersection over union of the representation of the sensed object from the radar data and a representation of the tracked object. Other comparisons can also be used.</p><p id="p-0016" num="0015">In examples, the techniques described herein can use the results of comparison of the representation generated from the radar sensor data to the track information to associate the sensed object with an existing track. For instance, if the comparison indicates that the sensed object likely corresponds with a track of a previously-identified object, techniques described can use the representation based on the radar data to update the track information, e.g., by predicting future behavior of the object based on updated information about the track. In examples, the sensed object may be likely to correspond to a track when the velocities, positions, and/or other attributes of the representation of the sensed object are closely related (e.g., within a threshold) of corresponding attributes of the tracked object. Without limitation, a sensed object may be associated with a tracked object if the velocity of the sensed object is within a threshold velocity of a track velocity of the tracked object and/or if a position of the sensed object is within a threshold distance of a track position of the tracked object. In still further examples, a sensed object may be associated with a tracked object if the intersection over union of the two-dimensional representation of the sensed object and a two-dimensional representation of the tracked object is equal to or above a threshold value.</p><p id="p-0017" num="0016">In instances in which a sensed object is not associated with an existing track, the radar data, e.g., the representation of the radar data, may be used to generate a new track. For instance, the representation of the radar data may be used as an estimate of the state of a new object, and subsequent radar data (including subsequent representations of sensor data) may be used to refine the track. In some instances, radar data may be used independently of other data, e.g., because radar may often be the first modality to detect an object, especially when the object is at a relatively far distance and/or when the object is occluded. In other examples, the radar data, e.g., the representation of the radar data, may be combined with other data to generate a new track (for a newly-detected object).</p><p id="p-0018" num="0017">Conventionally, object tracking may have been conducted at the exclusion of radar data. For instance, some conventional techniques may associate radar points with objects and process each of those points individually. This &#x201c;per-point&#x201d; processing is resource and time intensive, e.g., because it includes determining, for each point, whether the point is associated with an object track. In contrast, techniques described herein can generate a (single) representation of an object from a plurality of radar returns, and use only the representation to track objects or to generate new tracks for newly-sensed objects. In examples, the representation can be based on returns from a single radar scan and/or from a plurality of radar scans, e.g., from multiple radar sensors and/or from multiple scans from the same radar sensor.</p><p id="p-0019" num="0018">In some examples, after a track is updated or generated based on a representation of an object generated from the radar data, aspects of this disclosure can include determining one or more trajectories for proceeding relative to the object. In some instances, the representation information generated according to techniques described herein may be combined, or fused, with data from other sensor modalities to predict a movement of the object and/or to plan a path relative to the object.</p><p id="p-0020" num="0019">Techniques described herein are directed to leveraging sensor and perception data to enable a vehicle, such as an autonomous vehicle, to navigate through an environment while circumventing objects in the environment. Techniques described herein can utilize information sensed about the objects in the environment, e.g., by radar sensors, to more accurately determine movement associated with the object. For example, techniques described herein may be faster than conventional techniques, as they may alleviate the need for information from a plurality of different sensors. That is, techniques described herein provide a technological improvement over existing object detection, classification, prediction and/or navigation technology. In addition to improving the accuracy with which sensor data can be used to determine objects and correctly characterize motion of those objects, techniques described herein can provide a smoother ride and improve safety outcomes by, for example, more accurately providing safe passage to an intended destination.</p><p id="p-0021" num="0020">While this disclosure uses an autonomous vehicle in examples, techniques described herein are not limited application in autonomous vehicles. For example, any system that uses radar data to navigate an environment may benefit from the radar data processing techniques described. By way of non-limiting example, techniques described herein may be used on aircrafts, e.g., to identify other aircraft and/or moving objects. Moreover, non-autonomous vehicles could also benefit from techniques described herein, e.g., for collision detection and/or avoidance systems.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>1</b>-<b>6</b></figref> provide additional details associated with techniques described herein. More specifically, <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic illustration showing an example scenario <b>100</b> in which a vehicle <b>102</b> is driving on a road surface <b>104</b>. As illustrated, a second vehicle <b>106</b> is also travelling on the road surface <b>104</b>. In the example scenario <b>100</b>, the vehicle <b>102</b> is moving generally in the direction of arrow <b>107</b> and the second vehicle <b>106</b> is travelling generally in an opposite direction. For instance, the vehicle <b>102</b> and the second vehicle <b>106</b> may be driving in opposite lanes, e.g., passing each other.</p><p id="p-0023" num="0022">For illustration, the vehicle <b>102</b> can be an autonomous vehicle configured to operate according to a Level 5 classification issued by the U.S. National Highway Traffic Safety Administration, which describes a vehicle capable of performing all safety-critical functions for the entire trip, with the driver (or occupant) not being expected to control the vehicle at any time. In such an example, since the vehicle <b>102</b> can be configured to control all functions from start to stop, including all parking functions, it can be unoccupied. Additional details associated with the vehicle <b>102</b> are described below. However, the vehicle <b>102</b> is merely an example, and the systems and methods described herein can be incorporated into any ground-borne, airborne, or waterborne vehicle, including those ranging from vehicles that need to be manually controlled by a driver at all times, to those that are partially or fully autonomously controlled. In additional implementations, techniques described herein may be useful in settings other than vehicles. The techniques described in this specification may be useful in many different applications in which sensor data is used to determine information about objects in an environment.</p><p id="p-0024" num="0023">The vehicle <b>102</b> may include a plurality of sensors, including a first radar sensor <b>108</b> and a second radar sensor <b>110</b>. As illustrated, the first radar sensor <b>108</b> and the second radar sensor <b>110</b> are arranged to propagate waves generally in a direction of travel of the vehicle <b>102</b> (e.g., generally along the direction of the arrow <b>107</b>). As also illustrated, the first radar sensor <b>108</b> and the second radar sensor <b>110</b> have overlapping fields of view. Accordingly, first emitted radio waves <b>112</b>, emitted by the first radar sensor <b>108</b>, will reflect off the second vehicle <b>106</b> and return to the first radar sensor <b>108</b> where they are detected via a first radar scan. Similarly, second emitted radio waves <b>114</b>, emitted by the second radar sensor <b>110</b>, will also reflect off the second vehicle <b>106</b> and return to the second radar sensor <b>110</b> where they are detected via a second radar scan. In some examples, the first radar sensor <b>108</b> and the second radar sensor <b>110</b> may be substantially identical, except for their position on the vehicle <b>102</b>. In other examples, however, the radar sensors <b>108</b>, <b>110</b> may be differently configured. By way of non-limiting example, the radio waves <b>112</b>, <b>114</b> may be emitted at different frequencies, e.g. pulse-regulated frequencies. Also in examples, the radar sensors <b>108</b>, <b>110</b> may be configured such that scans at the sensors <b>108</b>, <b>110</b> have a different interval, e.g., a Doppler interval. In examples, features of the radar sensors <b>108</b>, <b>110</b>, including but not limited to the center frequency, the scan type, the scan pattern, frequency modulation, the pulse repetition frequency, pulse repetition interval, may be configured, e.g., to create the different Doppler intervals. Accordingly, the radar sensors <b>108</b>, <b>110</b> may both be disposed to sense objects generally in the same direction relative to the vehicle <b>102</b>, but the radar sensors <b>108</b>, <b>110</b> can be configured differently. In other examples, however, several features and functions of the radar sensors <b>108</b>, <b>110</b> may be the same or similar.</p><p id="p-0025" num="0024">The sensors <b>108</b>, <b>110</b> may receive the emitted radio waves <b>112</b>, <b>114</b> after the waves reflect off a surface in the environment, e.g., a surface of the second vehicle <b>106</b>, and the radar sensors <b>108</b>, <b>110</b> can generate radar data based on the reflection. For instance, the radar data may include diverse types of information, including but not limited to a velocity associated with each of many points representative of surfaces or objects in the environment of the sensor. By way of non-limiting example, when the sensors <b>108</b>, <b>110</b> are pulse-Doppler sensors, the sensors <b>108</b>, <b>110</b> may be able to determine a velocity of an object relative to the respective sensor.</p><p id="p-0026" num="0025">In more detail, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a plurality of first radar returns, schematically represented by points. In the illustration, first points <b>116</b>(<b>1</b>), <b>116</b>(<b>2</b>), <b>116</b>(<b>3</b>), <b>116</b>(<b>4</b>) (collectively, the first points <b>116</b>) are illustrated as circles, and represent radar returns associated with the first radar sensor <b>108</b>. That is, individual of the first points <b>116</b> are indicative of locations on the second vehicle <b>106</b> at which the first emitted waves <b>112</b> reflect. Similarly, second points <b>118</b>(<b>1</b>), <b>118</b>(<b>2</b>), <b>118</b>(<b>3</b>), <b>118</b>(<b>4</b>) (collectively the second points <b>118</b>) are illustrated as &#x201c;X&#x201d;s, and represent radar returns associated with the second radar sensor <b>110</b>. Stated differently, individual of the second points <b>118</b> are indicative of locations on the second vehicle <b>106</b> at which the second emitted waves <b>114</b> reflect.</p><p id="p-0027" num="0026">As also illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the vehicle <b>102</b> also includes a plurality of additional sensors <b>120</b>. The additional sensors <b>120</b> may be disposed to sense objects generally in the same direction as the first radar sensor <b>108</b> and/or the second radar sensor <b>110</b>, e.g., generally in the direction of the arrow <b>107</b>. Without limitation, the additional sensors <b>120</b> may be one or more of additional radar sensors, LiDAR sensors, imaging sensors (e.g., cameras), time-of-flight sensors, sonar sensors, thermal imaging devices, or any other sensor modalities. Although two instances of the additional sensors <b>120</b> are illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the vehicle <b>102</b> may include any number of additional sensors, with any number of different modalities. In examples, the first radar sensor <b>108</b>, the second radar sensor <b>110</b>, and illustrated additional sensors <b>120</b> may be disposed generally to detect objects in the direction of the arrow <b>107</b>, e.g., the second vehicle <b>106</b>, and the vehicle <b>102</b> may include a number of additional sensors disposed to detect objects at other relative positions. Without limitation, the vehicle <b>102</b> can include radar sensors and additional sensors that provide for sensing objects at 360-degrees relative to the vehicle <b>102</b>.</p><p id="p-0028" num="0027">As illustrated in the block diagram accompanying <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the first radar sensor <b>108</b>, the second radar sensor <b>110</b>, and the additional sensors <b>120</b> represent types of sensor systems <b>122</b> on the vehicle <b>102</b>. The first radar sensor <b>108</b> and the second radar sensor <b>108</b> generate radar data <b>124</b>. In examples, the radar data <b>124</b> includes position data of the respective points <b>116</b>, <b>118</b>. For example, information associated with radar returns from the points <b>116</b>, <b>118</b> may include information indicative of a location in the environment, e.g., a location of the points <b>116</b>, <b>118</b>. Moreover, when such points are associated with the second vehicle <b>106</b>, as in the illustration, a position of the second vehicle <b>106</b> can be determined. The location information may include a range and azimuth relative to the points <b>116</b>, <b>118</b> or a position in a local or global coordinate system. Also in implementations, the radar data <b>124</b> may include signal strength information. For example, the signal strength information can indicate a type of the object. More specifically, radio waves may be reflected more strongly by objects having certain shapes and/or compositions. For example, broad, flat surfaces and/or sharp edges are more highly reflective than rounded surfaces, and metal is more highly reflective than a person. In some instances, the signal strength may include a radar cross-section (RCS) measurement. As also noted above, the radar data <b>124</b> may also include velocity information. For instance, a velocity of each of the points <b>116</b>, <b>118</b> (and/or of the second vehicle <b>106</b>) may be based on a frequency of radio energy reflected from the points <b>116</b>, <b>118</b> and/or a time at which the reflected radio energy is detected.</p><p id="p-0029" num="0028">Accordingly, the radar data <b>124</b> can include a distance of respective ones of the first points <b>116</b> from the first radar sensor <b>108</b> (e.g., a range or radial distance), a velocity (e.g., a Doppler velocity) of the respective one of the first points <b>116</b> along the distance, a strength measurement (e.g., an RCS value), and/or additional information. Similarly, the radar data <b>124</b> can also include a distance of respective ones of the second points <b>118</b> from the second radar sensor <b>110</b> (e.g., a range or radial distance), a velocity (e.g., a Doppler velocity) of the respective one of the second points <b>118</b> along the associated distance, strength information, and/or additional information.</p><p id="p-0030" num="0029">In examples described in detail herein, the radar data <b>124</b> is used generally to track objects. More specifically, <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates that the vehicle <b>102</b> can include one or more vehicle computing device(s) <b>126</b> for executing functionality associated with the radar data <b>124</b>. The vehicle computing device(s) <b>126</b> include a radar processing system <b>128</b> having an associated object representation generation component <b>130</b>, as well as a track association component <b>132</b> and a track generation component <b>134</b>. These systems and components are described in turn.</p><p id="p-0031" num="0030">The radar processing system <b>128</b> generally implements functionality to receive the radar data <b>124</b> from the radar sensor(s) <b>108</b>, <b>110</b> and generate object representations <b>136</b> of objects in an environment of the vehicle <b>102</b>, such as representations of the second vehicle <b>106</b> and/or other dynamic and/or static objects in the environment. In examples, the radar processing system <b>128</b> may be a radar pipeline that processes only radar data, like the radar data <b>124</b>, e.g., at the exclusion of sensor data <b>144</b> from other sensor modalities. The radar processing system <b>128</b> may include functionality to associate returns with each other and/or with specific objects. Thus, for example, the radar processing system <b>128</b> can determine that returns associated with the first points <b>116</b> and the second points <b>118</b> are associated with each other and/or with the second vehicle <b>106</b>. The radar processing system <b>128</b> can also determine that other returns, e.g., in a same radar scan, are associated with other objects, for example, the road surface <b>104</b> proximate the second vehicle <b>106</b>, other vehicles (not shown), or the like.</p><p id="p-0032" num="0031">In some examples, the radar processing system can cluster points, e.g., the first points <b>116</b> and the second points <b>118</b>, based on information from those returns. For instance, the first points <b>116</b> and the second points <b>118</b> are closely situated, e.g., within a threshold distance, and in some instances the radar processing component can determine that those points are indicative of a single object. In examples described herein, a point cluster may include a plurality of points that have some likelihood, e.g., a level and/or degree of similarity, to identify a single object or grouping of objects that should be considered together, e.g., by a planning system of the autonomous vehicle. In aspects of this disclosure, information in addition to position information may be used to determine the point cluster. For example, while the first points <b>116</b> and the second points <b>118</b> have similar positional returns, in some implementations other points that are also close in proximity can be excluded by the radar processing system <b>128</b>, e.g., because such points may be representative of a different object. For instance, a signal strength, e.g., an RCS value, associated with one or more additional points (e.g., additional to the first points <b>116</b> and the second points <b>118</b>) may be significantly different than the signal strength associated with the first points <b>116</b> and/or the second points <b>118</b>, and thus the radar processing system <b>128</b> may exclude the additional point, even if the additional point is proximally close to the first points <b>116</b> and/or the second points <b>118</b>. In a real-world example, if a person is standing next to a fire hydrant, the returns from the fire hydrant could have a significantly stronger signal, e.g., because the fire hydrant is metal, than the returns from the person.</p><p id="p-0033" num="0032">The radar processing system <b>128</b> can use additional or different information to determine returns associated with objects. For example, when the radar sensor is a Doppler-type sensor, velocity of the objects may be used to determine the point cluster. In the illustrated example, points associated with the road surface <b>104</b> or with (not illustrated) other static objects in the environment will have significantly different velocities than the (moving) second vehicle <b>106</b>. More specifically, because the road surface <b>104</b> is stationary and the second vehicle <b>106</b> is moving, points having a similar, non-zero velocity component can be clustered as being associated the second vehicle <b>106</b>. Other information may also or alternatively be used to cluster points in accordance with the disclosure herein, and the examples provided are for illustration only. By way of non-limiting example, historical data of the object, e.g., of the second vehicle <b>106</b>, can be used to determine whether points are associated with the object. For instance, if tracking data of an object provides historical position, velocity and/or acceleration data, the track association component <b>134</b> may expect more recent, e.g., contemporaneous, returns associated with the object to have values within some range.</p><p id="p-0034" num="0033">The radar processing system <b>128</b> may be embodied as one or more data analysis structures, including one or more neural networks. Without limitation, the identification of points as being associated with the second vehicle <b>106</b> may be performed by one or more machine-learned networks. Without limitation, the radar processing system <b>128</b> may include one or more neural networks that process the radar data <b>124</b> to perform the grouping of points and association of points with objects just discussed. In at least some examples, the network can, for each return, identify an association of that point with one or more additional points, an association of that point with an object, classify the point (e.g., as being associated with a vehicle, a building, a pedestrian, or the like). In some instances, the radar processing system <b>128</b> can include some or all of the functionality described in U.S. patent application Ser. No. 16/587,605 for &#x201c;Perception System,&#x201d; filed on Sep. 30, 2019, the entirety of which is hereby incorporated by reference. Aspects of the system described in the '605 application include a top-down or two-dimensional machine learned radar perception update process in which a two-dimensional (or top-down) representation is generated from three-dimensional radar data.</p><p id="p-0035" num="0034">The radar processing system <b>128</b> may also include the object representation generation component <b>130</b> configured to determine the object representation <b>136</b>. More specifically, while the radar processing system <b>128</b> receives a plurality of radar points, e.g., from the radar sensors <b>108</b>, <b>110</b>, and/or other radar sensors, and makes some determination on a per-point basis, the object representation generation component <b>130</b> generates single representations of objects based on the per-point data. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the object representation generation component <b>130</b> generates a bounding box <b>138</b> as the object representation <b>136</b>. The bounding box <b>138</b> is a two-dimensional representation of the second vehicle <b>106</b>, generated by the object representation generation component <b>130</b> based on the first radar points <b>116</b>, the second radar points <b>118</b>, and/or other radar points, e.g., from other radar scans conducted by the first radar sensor <b>108</b>, the second radar sensor <b>110</b>, and/or other radar sensors. Although the bounding box <b>138</b> is illustrated as a two-dimensional bounding box, other instances of the object representation <b>136</b> can include other or different multi-dimensional representations, for example, a three-dimensional bounding box.</p><p id="p-0036" num="0035">The object representation <b>136</b> can also include other attributes or characteristics of objects, such as the second vehicle <b>106</b>, as determined from the radar data <b>124</b>. Without limitation, the object representation <b>136</b> can include extents of the sensed object, e.g., embodied as the length, width, area, or the like, of the bounding box <b>138</b>. The object representation can also include a position of the In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a position of the bounding box <b>138</b> may be coordinates associated with a point <b>140</b>, which may represent a center of the bounding box <b>138</b>. Although the point <b>140</b> is illustrated as being a center of the bounding box <b>138</b> the point may be other than the center. The object representation <b>136</b> can also include a velocity of the object. For instance, an arrow <b>142</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> indicates a velocity (e.g., direction and/or magnitude) associated with the bounding box <b>138</b>. The object representation <b>136</b> can also include one or more of a classification of the object (e.g., a vehicle (as in <figref idref="DRAWINGS">FIG. <b>1</b></figref>), a pedestrian, a wheeled pedestrian, a bicyclist, a construction vehicle, an articulated vehicle, a building, or the like). The object representation may also include a probability or certainty associated with the representation. For example, the probability or certainty may be a single value associated with all attributes or with individual attributes (e.g. a probability/certainty associated with the classification determination, another associated with one or more dimensions of the bounding box, or the like).</p><p id="p-0037" num="0036">The object representation <b>136</b> can be a singular representation of an object (the second vehicle <b>106</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) based on a plurality of radar points. The vehicle computing device(s) <b>126</b> can use the object representation <b>146</b> to track objects, such as the second vehicle <b>106</b>. As used herein, &#x201c;tracking an object&#x201d; generally relates to predicting movement of an object, e.g., relative to the vehicle <b>102</b>. In examples, the vehicle computing device(s) <b>126</b> may include functionality to generate and/or receive information about tracks of objects, e.g., as track information. As used herein, a track may generally describe attributes of a path of an object in the environment of the vehicle <b>102</b>. In some examples, the track may be a series of measured and/or predicted poses or states of the object, e.g., relative to the vehicle <b>102</b>. A track may include a series of multi-dimensional representations, e.g., two-dimensional bounding boxes, generated at a predetermined frequency to represent/predict movement of the object.</p><p id="p-0038" num="0037">The track association component <b>132</b> includes functionality to determine whether the object representation <b>136</b> should be associated with an existing track, e.g., of a previously-sensed object. For example, the track association component can include functionality to compare the object representation <b>136</b> to track information. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the attributes of the representation of the second vehicle <b>106</b> discussed above, e.g., attributes of the bounding box <b>138</b>, are compared to track information to determine whether the second vehicle <b>106</b> is already being tracked. As detailed further herein, particularly with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, below, the comparison of the object representation <b>136</b> to track information can include comparing a velocity of the object representation to a track velocity, a position of the object representation to a track position, or the like.</p><p id="p-0039" num="0038">The track generation component <b>134</b> can include functionality to update a previously-generated track. For example, if the track association component <b>132</b> determines that the object representation <b>136</b> is associated with a track, e.g., the object representation <b>136</b> represents an object that is already being tracked, the track generation component <b>134</b> can update the track using the object representation <b>136</b>, e.g., by predicting future movement of the second vehicle <b>106</b> using the object representation <b>136</b>. In examples, the track generation component <b>134</b> can also receive additional data, e.g., sensor data <b>144</b> from the additional sensors <b>120</b>. Without limitation, the track generation component <b>134</b> may fuse the object representation <b>136</b> with the sensor data <b>144</b> and/or other data to update the existing track of the second vehicle <b>106</b>.</p><p id="p-0040" num="0039">In other examples, the track generation component <b>134</b> can create a new track, e.g., in instances in which an object, like the second vehicle <b>106</b>, is newly detected. For example, in instances in which an object representation <b>136</b> does not match an existing track, the track generation component <b>134</b> can use the object representation <b>136</b> to generate a new track. In some instances, the track generation component <b>134</b> can receive additional data, such as the sensor data <b>144</b>, to generate a new track, e.g., using data from multiple sensor modalities. In at least some examples, the track generation component <b>134</b> can also, or alternatively, receive multiple instances of the object representation <b>136</b> to generate a new track. For example, multiple instances of the object representation <b>136</b>, e.g., based on different radar scans and/or radar scans from different times. In examples described further herein, updated and/or new track information generated by the track generation component <b>134</b> may be used to control the vehicle <b>102</b>, e.g., to navigate relative to tracked objects such as the second vehicle <b>106</b>.</p><p id="p-0041" num="0040">Techniques described herein may improve planning system accuracy and performance by determining and updating tracks of detected objects using radar data. For instance, radar sensors, like the radar sensors <b>108</b>, <b>110</b> can be among the quickest sensors on some vehicles to generate meaningful amounts of data about objects, like the second vehicle <b>106</b>. For example, the radar sensors <b>108</b>, <b>110</b> may generate data about objects that are relatively farther away than can be detected by imaging sensors, LiDAR sensors, or the like. Moreover, radar sensors may be more reliable in low-light situations, e.g., at night, and/or during certain atmospheric conditions, e.g., during rainy weather, foggy weather, snowy weather, or the like. Conventionally, however, despite these benefits of radar sensors, radar data has not been used to track objects, at least in part because conventional techniques required point-by-point consideration of radar returns. Techniques described herein, however, generate the object representation <b>136</b> from a plurality of radar points, and use the object representation <b>136</b> for tracking the sensed object. As will be appreciated, comparing a single representation to track information is much less processing- and/or time-intensive than comparing dozens, or even hundreds, of points to track information. That is, the object representation <b>136</b> is a quickly-generated, and reliable, representation of the radar data <b>124</b>. Moreover, as noted above, because radar sensors may detect objects at greater distance than other sensor modalities, the object representation <b>136</b> may promote earlier tracking of objects, thereby improving safety outcomes for the vehicle <b>102</b> as the vehicle <b>102</b> travels relative to the objects. Additional aspects of tracking objects using object representations from radar data will now be discussed with reference to <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref>.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>2</b></figref> includes textual and visual flowcharts to illustrate an example process <b>200</b> for updating track data using radar data. In examples described herein, the sensor data may be obtained by radar sensors disposed on an autonomous vehicle. In this example, the process <b>200</b> uses multiple radar sensors with overlapping fields of view to determine a representation of an object in the environment of the autonomous vehicle and then associates the object representation with track information associated with, e.g., previously generated for, the sensed object.</p><p id="p-0043" num="0042">At an operation <b>202</b>, the process <b>200</b> includes receiving a representation of an object based on radar data. An example <b>204</b> accompanying the operation <b>202</b> illustrates a vehicle <b>206</b> having a first radar sensor <b>208</b> and a second radar sensor <b>210</b> disposed on the vehicle <b>206</b>. The first radar sensor <b>208</b> and the second radar sensor <b>210</b> may correspond to the radar sensors <b>108</b>, <b>110</b>. In the illustrated example, the vehicle <b>206</b> may be traversing through the environment generally in a direction indicated by an arrow <b>212</b> (although in other implementations, the vehicle <b>206</b> may be stationary or moving in a different direction), such that the first radar sensor <b>208</b> and the second radar sensor <b>210</b> are disposed on the leading end of the vehicle <b>206</b>, e.g., to capture data about objects in front of the vehicle <b>206</b>. In the examples, an object <b>216</b>, which may be the second vehicle <b>106</b>, is disposed generally in front of the vehicle <b>206</b>. The first radar sensor <b>208</b> captures first radar data, e.g., via first radar scans, and the second radar sensor <b>210</b> captures second radar data, e.g., via second radar scans. In the illustrated embodiment, the first and second radar sensors <b>208</b>, <b>210</b> are generally configured next to each other, both facing in the direction of travel, and with significant overlap in their fields of view.</p><p id="p-0044" num="0043">In examples, the vehicle <b>206</b> may correspond to the vehicle <b>102</b>, the first radar sensor <b>208</b> may correspond to the first radar sensor <b>108</b>, and/or the second radar sensor <b>210</b> may correspond to the second radar sensor <b>110</b>, shown in, and discussed in connection with, <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The first radar sensor <b>208</b> and the second radar sensor <b>210</b> may be radar sensors that measure the range to objects and/or the velocity of objects. In some example systems, the radar sensors may be Doppler sensors, pulse-type sensors, continuous wave frequency modulation (CWFM) sensors, or the like. For example, the radar sensors <b>208</b>, <b>210</b> may emit pulses of radio energy at predetermined intervals. In some implementations, the intervals may be configurable, e.g., to promote enhanced detection of objects at relatively far distances or relatively close distances. Moreover, the first radar sensor <b>208</b> and the second radar sensor <b>210</b> may have different ambiguous ranges, e.g., to facilitate disambiguation of otherwise-ambiguous returns.</p><p id="p-0045" num="0044">The pulses of radio energy emitted from the first sensor <b>208</b> and the second sensor <b>210</b> can reflect off objects in the environment, and can be received by the radar sensors <b>208</b>, <b>210</b>, e.g., as radar data. In the example <b>204</b>, the radar sensors may generate radar returns <b>214</b> associated with an object <b>216</b>. The object <b>216</b> is embodied as another vehicle in the example <b>204</b>, although it may be any of a number of objects.</p><p id="p-0046" num="0045">The example <b>204</b> also illustrates a representation <b>218</b> of the object <b>216</b>. The representation <b>218</b> can be a two-dimensional bounding box generally representing the extents of the object <b>216</b>. As detailed herein, the representation <b>218</b> is generated from a plurality of radar returns, e.g., including the radar returns <b>214</b>, from one or more radar scans. For instance, the radar processing system <b>128</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> and detailed above, may receive sensor data generated by the radar sensors <b>208</b>, <b>210</b>, and generate the representation <b>218</b> from that data. The representation <b>218</b> has an associated position, as designated by a point <b>220</b>. The point <b>220</b> in the example <b>204</b> corresponds to the center of the bounding box that is the representation <b>218</b>, although in other examples the point <b>220</b> may be other than the center. A position of the point <b>220</b> information indicative of a location of objects in the environment, e.g., a range and azimuth relative to the vehicle <b>206</b> or a position in a local or global coordinate system. The representation <b>218</b> also includes an associated velocity (v<sub>sensed</sub>). Although not shown in the example <b>204</b>, the representation <b>218</b> can include additional or alternative attributes, including but not limited to a classification of the sensed object (e.g., vehicle), a probability or certainty associated with one or more aspects of the representation <b>218</b>, a signal strength metric, e.g. an RCS measurement, and/or other attributes. The representation <b>218</b> may be the object representation <b>136</b> in some examples.</p><p id="p-0047" num="0046">At an operation <b>222</b>, the process <b>200</b> includes receiving track information of object(s) in the environment. returns associated with an object from the radar data. An example <b>224</b> accompanying the operation <b>222</b> includes a visualization of a track <b>226</b> of an object in an environment of the vehicle <b>206</b>. In this example, the track <b>226</b> is a series of tracked object representations <b>228</b><i>a</i>, <b>228</b><i>b</i>, <b>228</b><i>c</i>, <b>228</b><i>d </i>(collectively the tracked object representations <b>228</b>). For example, the tracked object representations <b>228</b> may individually represent states of an object in the environment of the vehicle <b>206</b>. In the example, the first tracked object representation <b>228</b><i>a </i>may be first in time, and the fourth tracked object representation <b>228</b><i>d </i>may be last in time. The tracked object representations <b>228</b> include associated information about the tracked object including, but not limited to, a position of the tracked object, e.g., indicated by a point <b>230</b>, extents of the tracked object, e.g., indicated by the perimeter of the tracked object representations <b>228</b>, a velocity of the tracked object, e.g., v<sub>track</sub>, and/or other attributes. In the example <b>224</b>, the point <b>230</b> corresponds to a center of the tracked object, although in other examples the point <b>230</b> may be other than the center. In still further instances, individual of the tracked object representations <b>228</b> may generated from different algorithms, based on different sensor data, and/or be otherwise distinguished.</p><p id="p-0048" num="0047">One or more of the tracked object representations <b>228</b> can be predictions or expectations, of states of the tracked object (e.g., at some future point in time). For instance, the tracked object representations <b>228</b> may be generated based on sensor data related to the tracked object and/or one or more predictive models. The sensor data used to predict the state(s) of the tracked object can include the radar data <b>124</b> and/or the sensor data <b>144</b>. Although the track <b>226</b> is visualized as four tracked object representations <b>228</b>, the track <b>226</b> may include more or fewer observations. In at least one non-limiting example, the track <b>226</b> may include only a single representation, generally corresponding to a next-predicted state of the tracked object. Moreover, the tracked object representations <b>228</b> may have associated time information, e.g., a time at which the respective state represented by one of the tracked object representations <b>228</b> is to be achieved. The tracked object representations <b>228</b> can also include information associated with a classification of the tracked object (e.g., an automobile, a pedestrian, a bicycle, a motorcycle, or the like), probability, confidence, and/or certainty data associated with one or more aspects of the tracked object representations <b>228</b>, or the like.</p><p id="p-0049" num="0048">Although the example <b>224</b> shows only a single track <b>226</b>, in other examples the track information will include a plurality of tracks, e.g., each associated with a different tracked object. As will be appreciated, in some settings, the autonomous vehicle <b>206</b> may navigate relative to several cars, pedestrians, bicyclists, skateboarders, and/or other dynamic objects. The track information received at the operation <b>222</b> may include a track for each dynamic object.</p><p id="p-0050" num="0049">At an operation <b>232</b>, the process <b>200</b> includes associating the representation of the object to the track information. For example, the operation <b>232</b> includes comparing the representation <b>218</b> with the track <b>226</b> and/or any other tracks in the track information, to determine whether the sensed object corresponds to an already-being-tracked object. An example <b>234</b> accompanying the operation <b>232</b> illustrates techniques for implementing the operation <b>232</b>. More specifically, the example <b>234</b> includes a first comparison example <b>236</b>(<b>1</b>), a second comparison example <b>236</b>(<b>2</b>), and a third comparison example <b>236</b>(<b>3</b>) (collectively, &#x201c;the comparison examples <b>236</b>&#x201d;).</p><p id="p-0051" num="0050">The first comparison example <b>236</b>(<b>1</b>) illustrates a process for comparing velocities to determine whether a sensed object corresponds to a tracked object. More specifically, the first comparison example <b>236</b>(<b>1</b>) shows both the representation <b>218</b> of the radar data from the radar sensors <b>208</b>, <b>210</b> and the fourth tracked object representation <b>228</b><i>d</i>. In this example, the fourth tracked representation <b>228</b><i>d </i>may be the one of the tracked object representations <b>228</b> that is closest in time to a time associated with the radar data, e.g., a time at which the radar data is generated, a time at which the radar data is received at the radar sensors <b>208</b>, <b>210</b>, or the like. In other examples, the fourth tracked representation <b>218</b> may also or alternatively be selected because it is most-recently generated (or only) tracked object representation. Other criteria may also or alternatively be used to select the tracked object representation for comparison to the sensed object representation <b>218</b>.</p><p id="p-0052" num="0051">As illustrated in the first comparison example <b>236</b>(<b>1</b>), and as discussed above, the representation <b>218</b> is a singular representation of a sensed object, generated from a plurality of radar returns associated with that object. The representation <b>218</b> also has an associated velocity, v<sub>sensed</sub>, based at least in part on the velocities of the represented radar returns. Similarly, the tracked object representation <b>228</b><i>d </i>has an associated track velocity, v<sub>track</sub>. In the first comparison example <b>236</b>(<b>1</b>), the sensed velocity and the track velocity are compared, and if a difference between the sensed velocity and the track velocity is equal to or less than a threshold velocity, the representation <b>218</b> is associated with the tracked object representation <b>228</b><i>d</i>, and thus the track <b>226</b>. Stated differently, if the sensed velocity of the representation <b>218</b> is the same as, or within a threshold difference of, the (expected) velocity of an object being tracked, the sensed object can be associated with the track.</p><p id="p-0053" num="0052">The second comparison example <b>236</b>(<b>2</b>) illustrates a process for comparing positions to determine whether a sensed object corresponds to a tracked object. As with the first comparison example <b>236</b>(<b>1</b>), the second comparison example <b>236</b>(<b>2</b>) shows both the representation <b>218</b> of the radar data from the radar sensors <b>208</b>, <b>210</b> and the fourth tracked object representation <b>228</b><i>d</i>. Also, as with the previous example, the fourth tracked representation <b>228</b><i>d </i>may be selected based on any number of criteria. The representation <b>218</b> also includes the point <b>220</b>, e.g., the center of the representation <b>218</b>, and the fourth tracked object representation <b>228</b><i>d </i>includes the point <b>230</b>, e.g., the center of the fourth tracked representation <b>228</b><i>d</i>. In the second comparison example <b>236</b>(<b>2</b>), a distance, represented by a line <b>238</b>, is a distance between the point <b>220</b> and the point <b>230</b>. The operation <b>232</b> includes associating the representation <b>218</b> with the tracked object representation <b>228</b><i>d</i>, and thus with the track <b>226</b> and the object being tracked thereby, when the distance is equal to or below a threshold distance. Stated differently, if the position of the representation <b>218</b> is within a threshold distance of the (expected) position of an object being tracked, the sensed object can be associated with the track.</p><p id="p-0054" num="0053">The third comparison example <b>236</b>(<b>3</b>) illustrates a process for comparing areas to determine whether a sensed object corresponds to a tracked object. As with the first comparison example <b>236</b>(<b>1</b>) and the second comparison example <b>236</b>(<b>2</b>), the third comparison example <b>236</b>(<b>3</b>) shows both the representation <b>218</b> of the radar data from the radar sensors <b>208</b>, <b>210</b> and the fourth tracked object representation <b>228</b><i>d</i>. Also as with the previous examples, the fourth tracked representation <b>228</b><i>d </i>may be selected based on any number of criteria. The representation <b>218</b> is a two-dimensional bounding box, e.g., having a length and width, generally representing a length and width of the object <b>216</b>, and the fourth tracked object representation <b>228</b><i>d </i>is a two-dimensional representation e.g., having a length and width. In the third comparison example <b>236</b>(<b>3</b>), an area of overlap <b>240</b> is an area shared by the representation <b>218</b> and the fourth tracked object representation <b>228</b><i>d</i>. In this example, the representation <b>218</b> and the fourth tracked object representation <b>228</b><i>d </i>is equal to or greater than a threshold area. In other examples, the respective areas of the representation <b>218</b> and the fourth tracked object representation <b>228</b><i>d </i>may be used to detect an intersection over union (IOU), e.g., the ratio of the area of overlap <b>240</b> to a total area of the representations. The operation <b>232</b> can include associating the representation <b>218</b> with the tracked object representation <b>228</b><i>d</i>, and thus with the track <b>226</b> and the object being tracked thereby, when the value of the IOU is equal to or greater than a threshold value. Stated differently, if there is sufficient overlap of the representation <b>218</b> with an object being tracked, the sensed object can be associated with the track.</p><p id="p-0055" num="0054">In instances, one, two, or all three of the comparison examples <b>236</b> may be used to associate a sensed object with a tracked object. Moreover, as discussed below, other or additional example techniques may be used. However, in some instances, it may be desirable to first perform the first comparison example <b>236</b>(<b>1</b>). Specifically, radar data will provide a reliable (e.g., accurate) velocity determination, so the velocity comparison may accurately determine the association.</p><p id="p-0056" num="0055">The comparison examples <b>236</b> are for illustration only, and as will be appreciated from the following description, those having ordinary skill, with the benefit of this disclosure, may appreciate other techniques for comparing the representation <b>218</b> with the track information. Without limitation, the comparison may include comparing a classification associated with each of the representations. In still further examples, the comparison may be determined based at least in part on a confidence associated with one or both of the representation <b>218</b> and the track representations <b>228</b>. Without limitation, and using the first comparison example <b>236</b>(<b>1</b>) for illustration, the sensed velocity and/or the track velocity may have an associated uncertainty, which in some examples may be determined by a processing system or algorithm that determines those values. For example, the radar processing system <b>128</b> may include functionality to determine the sensed velocity (and/or other attributes included in the object representations <b>136</b>) from the radar data <b>124</b>, generally as discussed above, as well as functionality to determine a certainty of that velocity. In implementations, for example, the uncertainty may be used to actively adjust the velocity threshold that will result in association of the representation <b>218</b> of the radar data with the track <b>226</b> or a portion thereof.</p><p id="p-0057" num="0056">The process <b>200</b> also includes, at an operation <b>242</b>, updating the track information based on the association. For example, when, at the operation <b>232</b>, the representation <b>218</b> is determined to be associated with the track <b>226</b>, the representation <b>218</b> can be used to update the track <b>226</b>. An example <b>244</b> accompanying the operation <b>242</b> illustrates an updated track <b>226</b>&#x2032; that has been updated to include a fifth tracked object representation <b>228</b><i>e</i>. In the example <b>244</b>, the fifth tracked object representation <b>228</b><i>e </i>includes a point <b>246</b> representing a position of the representation <b>228</b><i>e </i>and a velocity, V<sub>updated</sub>. The updated track <b>226</b>&#x2032; is illustrated for example only, as more than a single tracked object representation may be generated based at least in part on the representation <b>218</b>. Without limitation, a plurality of tracked object representations may be generated based at least in part on the representation <b>218</b>, with individual of the tracked representations being associated with a different time, being based on a different prediction model, being generated from certain types of data, or the like. In at least some examples, the representation <b>218</b> may be fused with one or more additional data types, models, or the like, to generate the updated track <b>226</b>&#x2032;.</p><p id="p-0058" num="0057">From the foregoing description of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the process <b>200</b> can be used to update track information based on the representation <b>218</b>, generated from radar data. In contrast, conventional techniques required a point-by-point (or return-by-return) comparison of radar data to track information to associate those points with tracks, which is particularly time and processing intensive, often rendering tracking using radar data undesirable. The representation is a single representation of a plurality of radar points, so the process <b>200</b> requires far fewer processes to determine an association. For instance, even if more than one of the techniques described above in connection with the comparison examples <b>236</b> is used to confirm an association, the process <b>200</b> facilitates the use of radar data for tracking.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>3</b></figref> includes textual and visual flowcharts to illustrate an example process <b>300</b> for generating anew track for association with an object detected using radar data. In examples described herein, the radar data may be obtained by radar sensors disposed on an autonomous vehicle. In this example, the process <b>300</b> uses a representation of an object in the environment of the autonomous vehicle to determine that the object is not yet being tracked, and generates new track information for the object.</p><p id="p-0060" num="0059">At an operation <b>302</b>, the process <b>300</b> includes determining that an object representation from radar data does not correspond to existing track information. For example, the operation <b>302</b> can include comparing a sensed object representation with one or more tracked object representations and determined, based on the comparison, that the An example <b>304</b> accompanying the operation <b>302</b> includes a first comparison example <b>306</b>(<b>1</b>), a second comparison example <b>306</b>(<b>2</b>), and a third comparison example <b>306</b>(<b>3</b>) in which an object representation does not correspond to existing track information.</p><p id="p-0061" num="0060">The first comparison example <b>306</b>(<b>1</b>) shows a sensed object representation <b>308</b> generated from radar data. For instance, the sensed object representation may be a single representation of a plurality of radar returns, e.g., from one or more radar scans. The sensed object representation <b>308</b> may be the object representation <b>136</b> and/or the representation <b>218</b> in some examples. The object representation <b>308</b> has an associated sensed object velocity, v<sub>sensed</sub>. The first comparison example <b>306</b>(<b>1</b>) also shows a tracked object representation <b>310</b>. The tracked object representation may be associated with a track, like the track <b>226</b>, and may represent expected or predicted attributes of an object being tracked, e.g., a tracked object. In some examples, the tracked object representation <b>310</b> can correspond to one of the tracked object representations <b>228</b>. As also illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the tracked object representation <b>310</b> has an associated tracked object velocity, v<sub>track</sub>. The operation <b>302</b>, in the first comparison example <b>306</b>(<b>1</b>), includes comparing the sensed object velocity and the tracked object velocity and, based on the comparison, determining that the sensed object representation <b>308</b> does not correspond to the tracked object representation <b>310</b>. In the example, the magnitude and/or direction of the sensed object velocity and the tracked object velocity may be sufficiently different, e.g., greater than a threshold difference, that the sensed object is determined to be other than the tracked object. Accordingly, unlike in the process <b>200</b>, the sensed object represented by the sensed object representation <b>308</b> is not associated with a track of which the tracked object representation <b>310</b> is a part.</p><p id="p-0062" num="0061">The second comparison example <b>306</b>(<b>2</b>) also includes the sensed object representation <b>308</b> and the tracked object representation <b>310</b>. In this example, the sensed object representation <b>308</b> also has a position, represented by a point <b>312</b>, and the tracked object representation <b>310</b> has a position represented by a point <b>314</b>. Although the points <b>312</b>, <b>314</b> are shown as the centers of the respective representations <b>308</b>, <b>310</b>, the points may be other than centers. In this example, the points <b>312</b>, <b>314</b> are separated by a distance represented by the line <b>316</b>. In this example, the line <b>316</b> has a length greater than a threshold length. Thus, in the second comparison example <b>306</b>(<b>2</b>), the sensed object representation <b>308</b> is sufficiently far away from an expected position of a tracked object that the sensed object is determined to not correspond to the tracked object. Accordingly, unlike in the process <b>200</b>, the sensed object represented by the sensed object representation <b>308</b> is not associated with a track of which the tracked object representation <b>310</b> is a part.</p><p id="p-0063" num="0062">The third comparison example <b>306</b>(<b>2</b>) also includes the sensed object representation <b>308</b> and the tracked object representation <b>310</b>. In this example, the sensed object representation <b>308</b> is a two-dimensional representation of the sensed object, e.g., a two-dimensional bounding box having a length and a width. The tracked object representation <b>310</b> is also a two-dimensional representation, e.g., an expected or predicted bounding box of a tracked object. The example <b>304</b> also illustrates an area of overlap <b>318</b> representing a portion of the area of the sensed object representation <b>308</b> that overlaps with the tracked object representation <b>310</b>. In the third comparison example <b>306</b>(<b>3</b>), the area of overlap <b>318</b> is used to determine that the sensed object is not associated with the track. For instance, if the area of overlap <b>318</b> is below a threshold value, e.g., if the value is equal to or close to zero, the operation <b>302</b> can determine that the sensed object is not associated with the tracked object. In another example, an intersection over union (IOU) may be calculated for the sensed object representation <b>308</b> and the tracked object representation <b>310</b>, and, if the IOU is below a threshold value, the operation <b>302</b> will determine that the sensed object is not associated with the tracked object.</p><p id="p-0064" num="0063">The techniques used in the comparison examples <b>306</b> generally correspond to the techniques used in the comparison examples <b>236</b> discussed above, respectively. However, the comparison examples <b>236</b> found an association between the representations <b>218</b>, <b>228</b><i>d</i>, whereas the comparison examples <b>306</b> found no association. Other techniques, including those discussed above in connection with the comparison examples <b>236</b>, may alternatively or additionally be implemented by the operation <b>302</b>.</p><p id="p-0065" num="0064">Although the example <b>304</b> illustrates a comparison of the sensed object representation <b>308</b> to only the tracked object representation <b>310</b>, the operation <b>302</b> may include comparing the sensed object representation to any number of tracks and/or tracked object representations. In one example, if the sensed object representation <b>308</b> is associated with a vehicle, the operation <b>302</b> can include comparing the sensed object representation <b>308</b> to tracked object representations for all vehicles being tracked.</p><p id="p-0066" num="0065">At an operation <b>320</b>, the process <b>300</b> includes receiving additional data. For example, the additional data can include lidar data, image data, and/or one or more additional representations generated from radar data. An example <b>322</b> accompanying the operation <b>320</b> depicts a track generation component <b>324</b> receiving data from a radar pipeline <b>326</b>, and, optionally, from one or more of a lidar pipeline <b>328</b>, an image pipeline <b>330</b>, and/or an additional data source <b>332</b>. In this example, the radar pipeline <b>326</b> can represent a processing system that primarily processes radar data, the lidar pipeline <b>328</b> can represent a processing system the primarily processes lidar data, and/or the image pipeline <b>330</b> can represent a processing system that primarily processes image data. The additional data source <b>332</b> can represent one or more processing systems that process other types of data and/or that process multiple types of data. For example, and without limitation, the radar pipeline <b>326</b> can correspond to the radar processing system <b>128</b>. As detailed above, the radar processing system <b>128</b> receives radar data from radar sensors and generates the object representations <b>136</b> based thereon. In some examples, the track generation component <b>324</b> can identify objects from data and/or information from any of the example sources (and/or other sources). For example, when the sensed object representation <b>308</b> corresponds to a pedestrian, track generation component can identify lidar data, image data, and/or other data also associated with the pedestrian. In some instances, the track generation component <b>324</b> can correspond to the track generation component <b>134</b> discussed above.</p><p id="p-0067" num="0066">At an operation <b>334</b>, the process <b>300</b> includes generating a new track for the sensed object. For example, the track generation component <b>324</b> can generate a new track, as described herein. An example <b>336</b> accompanying the operation <b>334</b> illustrates a track <b>338</b>. More specifically, the track <b>338</b> is illustrated as a series (two in the example <b>336</b>) of representations, including the object representation <b>308</b> and a tracked object representation <b>340</b> generated by the track generation component <b>324</b> based at least in part on the object representation <b>308</b>. The tracked object representation <b>340</b> is a two-dimensional representation of the sensed object, having a position indicated by a point <b>342</b> and a velocity, v<sub>projected</sub>. As detailed herein, the tracked object representation may include additional or different data in some instances.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example system <b>400</b> for implementing the techniques described herein. In at least one example, the system <b>400</b> can include a vehicle <b>402</b>, which can be the same vehicle as the vehicle <b>102</b>, the vehicle <b>206</b>, and/or the vehicle <b>306</b> described above with reference to <figref idref="DRAWINGS">FIGS. <b>1</b>, <b>2</b>, and <b>3</b></figref>, respectively.</p><p id="p-0069" num="0068">The vehicle <b>402</b> can include one or more vehicle computing devices <b>404</b>, one or more sensor systems <b>406</b>, one or more emitters <b>408</b>, one or more communication connections <b>410</b>, at least one direct connection <b>412</b>, one or more drive modules <b>414</b>, and a user interface <b>416</b>.</p><p id="p-0070" num="0069">The vehicle computing device(s) <b>404</b> can include one or more processors <b>418</b> and memory <b>420</b> communicatively coupled with the one or more processors <b>418</b>. In the illustrated example, the vehicle <b>402</b> is an autonomous vehicle; however, the vehicle <b>402</b> could be any other type of vehicle. In the illustrated example, the memory <b>420</b> of the vehicle computing device <b>404</b> stores a localization component <b>422</b>, a perception component <b>424</b>, a planning component <b>426</b>, one or more system controllers <b>428</b>, a radar processing system <b>430</b>, a track association component <b>432</b>, and a track generation component <b>434</b>. Though depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref> as residing in the memory <b>420</b> for illustrative purposes, it is contemplated that the localization component <b>422</b>, the perception component <b>424</b>, the planning component <b>426</b>, the one or more system controllers <b>428</b>, the radar processing system <b>430</b>, the track association component <b>432</b>, and/or the track generation component <b>434</b> can additionally, or alternatively, be accessible to the vehicle <b>402</b> (e.g., stored on, or otherwise accessible by, memory remote from the vehicle <b>402</b>).</p><p id="p-0071" num="0070">In at least one example, the localization component <b>422</b> can include functionality to receive data from the sensor system(s) <b>406</b> to determine a position and/or orientation of the vehicle <b>402</b> (e.g., one or more of an x-, y-, z-position, roll, pitch, or yaw). For example, the localization component <b>422</b> can include and/or request/receive a map of an environment and can continuously determine a location and/or orientation of the autonomous vehicle within the map. In some instances, the localization component <b>422</b> can utilize SLAM (simultaneous localization and mapping), calibration, localization and mapping, simultaneously techniques, relative SLAM, bundle adjustment, non-linear least squares optimization, or the like to receive image data, LiDAR data, radar data, IMU data, GPS data, wheel encoder data, and the like to accurately determine a location of the autonomous vehicle. In some instances, the localization component <b>422</b> can provide data to various components of the vehicle <b>402</b> to determine an initial position of an autonomous vehicle for generating a candidate trajectory, as discussed herein.</p><p id="p-0072" num="0071">In some instances, the perception component <b>424</b> can include functionality to perform object detection, segmentation, and/or classification. In some examples, the perception component <b>424</b> can provide processed sensor data that indicates a presence of an entity that is proximate to the vehicle <b>402</b> and/or a classification of the entity as an entity type (e.g., car, pedestrian, cyclist, animal, building, tree, road surface, curb, sidewalk, unknown, etc.). In additional and/or alternative examples, the perception component <b>424</b> can provide processed sensor data that indicates one or more characteristics associated with a detected entity and/or the environment in which the entity is positioned. In some examples, characteristics associated with an entity can include, but are not limited to, an x-position (global position), a y-position (global position), a z-position (global position), an orientation (e.g., a roll, pitch, yaw), an entity type (e.g., a classification), a velocity of the entity, an acceleration of the entity, an extent of the entity (size), etc. Characteristics associated with the environment can include, but are not limited to, a presence of another entity in the environment, a state of another entity in the environment, a time of day, a day of a week, a season, a weather condition, an indication of darkness/light, etc. By way of non-limiting example, the perception component <b>424</b> may generate the object representations <b>136</b>, <b>218</b> from radar data, as discussed herein.</p><p id="p-0073" num="0072">The planning component <b>426</b> can determine a path for the vehicle <b>402</b> to follow to traverse through an environment. The planning component <b>426</b> can determine various routes and trajectories and various levels of detail. For example, the planning component <b>426</b> can determine a route to travel from a first location (e.g., a current location) to a second location (e.g., a target location). For the purpose of this discussion, a route can be a sequence of waypoints for travelling between two locations. As non-limiting examples, waypoints include streets, intersections, global positioning system (GPS) coordinates, etc. Further, the planning component <b>426</b> can generate an instruction for guiding the autonomous vehicle along at least a portion of the route from the first location to the second location. In at least one example, the planning component <b>426</b> can determine how to guide the autonomous vehicle from a first waypoint in the sequence of waypoints to a second waypoint in the sequence of waypoints. In some examples, the instruction can be a trajectory, or a portion of a trajectory. In some examples, multiple trajectories can be substantially simultaneously generated (e.g., within technical tolerances) in accordance with a receding horizon technique, wherein one of the multiple trajectories is selected for the vehicle <b>402</b> to navigate.</p><p id="p-0074" num="0073">In at least one example, the vehicle computing device <b>404</b> can include one or more system controllers <b>428</b>, which can be configured to control steering, propulsion, braking, safety, emitters, communication, and other systems of the vehicle <b>402</b>. These system controller(s) <b>428</b> can communicate with and/or control corresponding systems of the drive module(s) <b>414</b> and/or other components of the vehicle <b>402</b>.</p><p id="p-0075" num="0074">The radar processing system <b>430</b> can be the radar processing system <b>128</b> detailed above. Generally, the radar processing system <b>430</b> can include functionality to receive radar data and generate representations of objects from the radar data, e.g., as the object representations <b>136</b>. For example, the radar processing system <b>430</b> may receive sensor data comprising a plurality of points and information associated with the points, including position information, signal strength information, velocity information, or the like about points. The radar processing system <b>430</b> may employ one or more processing models, algorithms, or the like, to the received sensor data to determine object representations such as the object representation <b>136</b>. Each of the object representations may be a single representation generated from a plurality of radar points associated with the same, sensed object. Stated differently, the radar processing system <b>430</b> generates single representations of objects based on radar data deemed to be associated with those objects. The sensed object representations may be multi-dimensional, e.g., two- or three-dimensional bounding boxes, with associated attributes of the sensed object including but not limited to a velocity, position, classification, and/or other aspects of the sensed object's pose or state. Moreover, the radar processing system <b>430</b> can generate one or more probabilities, confidence values, and/or the like associated with the object representations <b>136</b> and/or aspects or attributes of the object representations <b>136</b>.</p><p id="p-0076" num="0075">The track association component <b>432</b> can be the track association component <b>132</b>. The track association component <b>432</b> generally includes functionality to associate sensed object representations, e.g., generated from radar data, with track information for objects already being tracked. For instance, the track association component <b>432</b> can include functionality to compare aspects of a sensed object representation, e.g., one of the object representations <b>136</b>, with tracked object representations, which may be part of a track. The example <b>234</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> demonstrates example functionality of the track association component <b>432</b>.</p><p id="p-0077" num="0076">The track generation component <b>434</b> can be the track generation component <b>134</b> and/or the track generation component <b>324</b>. The track generation component <b>324</b> generally includes functionality to receive object representations from radar data, like the object representations <b>136</b>, and update existing tracks or create new tracks based thereon. For instance, when the track association component <b>432</b> determines that a sensed object is associated with an existing track, the track generation component can generate updated track information, e.g., for appending or updating the existing track. In other examples, when an object representation associated with a sensed object does not correspond to an existing track, e.g., based on a comparison of the representation to the track information, the track generation component <b>324</b> generates a new track, e.g., like the track <b>338</b>, for association with the sensed object.</p><p id="p-0078" num="0077">Although shown separate from other components for clarity and ease of reference, functionality of the radar processing system <b>430</b>, the track association component <b>432</b>, and/or the track generation component <b>434</b> may be performed by other aspects of the vehicle <b>402</b>. Without limitation, one or more of those components may be incorporated into the perception system <b>424</b>. Aspects of this disclosure provide improved functionality resulting at least in part from use of a singular representation of a plurality of radar returns, regardless of the module, component, or system using that data according to the techniques detailed herein.</p><p id="p-0079" num="0078">In at least one example, the sensor system(s) <b>406</b> can include the radar sensors described herein. Also in examples, the sensor system(s) <b>406</b> can include LiDAR sensors, ultrasonic transducers, sonar sensors, location sensors (e.g., GPS, compass, etc.), inertial sensors (e.g., inertial measurement units (IMUs), accelerometers, magnetometers, gyroscopes, etc.), cameras (e.g., RGB, IR, intensity, depth, time of flight, etc.), microphones, wheel encoders, environment sensors (e.g., temperature sensors, humidity sensors, light sensors, pressure sensors, etc.), etc. The sensor system(s) <b>406</b> can include multiple instances of each of these or other types of sensors. For instance, and as discussed herein, implementations of this disclosure may use multiple scans from multiple sensors, e.g., multiple radar sensors, with overlapping fields of view. Thus, for example, the autonomous vehicle <b>402</b> may include a number of radar sensors. In additional examples, the LiDAR sensors can include individual LiDAR sensors located at the corners, front, back, sides, and/or top of the vehicle <b>402</b>. As another example, the camera sensors can include multiple cameras disposed at various locations about the exterior and/or interior of the vehicle <b>402</b>. The sensor system(s) <b>406</b> can provide input to the vehicle computing device <b>404</b>. Additionally, or alternatively, the sensor system(s) <b>406</b> can send sensor data, via the one or more networks <b>436</b>, to the one or more computing device(s) at a particular frequency, after a lapse of a predetermined period of time, in near real-time, etc.</p><p id="p-0080" num="0079">The emitter(s) <b>408</b> may be configured to emit light and/or sound. The emitter(s) <b>408</b> in this example include interior audio and visual emitters to communicate with passengers of the vehicle <b>402</b>. By way of example and not limitation, interior emitters can include speakers, lights, signs, display screens, touch screens, haptic emitters (e.g., vibration and/or force feedback), mechanical actuators (e.g., seatbelt tensioners, seat positioners, headrest positioners, etc.), and the like. In some examples, one or more of the interior emitters may be used to signal to the passenger that the vehicle is approaching or has arrived at an unmapped region and that continued movement in the unmapped region will require permission and/or manual control. In addition, or alternatively, the interior emitters may alert the passenger(s) that a teleoperator or other external source (e.g., a passenger-in-waiting) has taken manual control of the vehicle <b>402</b>. The emitter(s) <b>408</b> in this example can also include exterior emitters. By way of example and not limitation, the exterior emitters in this example include lights to signal a direction of travel or other indicator of vehicle action (e.g., indicator lights, signs, light arrays, etc.), and one or more audio emitters (e.g., speakers, speaker arrays, horns, etc.) to audibly communicate with pedestrians or other nearby vehicles, one or more of which comprising acoustic beam steering technology.</p><p id="p-0081" num="0080">The communication connection(s) <b>410</b> can enable communication between the vehicle <b>402</b> and one or more other local or remote computing device(s). For instance, the communication connection(s) <b>410</b> can facilitate communication with other local computing device(s) on the vehicle <b>402</b> and/or the drive module(s) <b>414</b>. Also, the communication connection(s) <b>410</b> can allow the vehicle to communicate with other nearby computing device(s) (e.g., other nearby vehicles, traffic signals, etc.). The communications connection(s) <b>410</b> also enable the vehicle <b>402</b> to communicate with a remote teleoperations computing device or other remote controllers.</p><p id="p-0082" num="0081">The communications connection(s) <b>410</b> can include physical and/or logical interfaces for connecting the vehicle computing device <b>404</b> to another computing device or a network, such as network(s) <b>436</b>. For example, the communications connection(s) <b>410</b> can enable Wi-Fi-based communication such as via frequencies defined by the IEEE 802.11 standards, short range wireless frequencies such as Bluetooth&#xae;, cellular communication (e.g., 2G, 4G, 4G, 4G LTE, 5G, etc.) or any suitable wired or wireless communications protocol that enables the respective computing device to interface with the other computing device(s).</p><p id="p-0083" num="0082">In at least one example, the vehicle <b>402</b> can include the drive module(s) <b>414</b>. In some examples, the vehicle <b>402</b> can have a single drive module <b>414</b>. In at least one example, if the vehicle <b>402</b> has multiple drive modules <b>414</b>, individual drive modules <b>414</b> can be positioned on opposite ends of the vehicle <b>402</b> (e.g., the front and the rear, etc.). In at least one example, the drive module(s) <b>414</b> can include one or more sensor systems to detect conditions of the drive module(s) <b>414</b> and/or the surroundings of the vehicle <b>402</b>. By way of example and not limitation, the sensor system(s) can include one or more wheel encoders (e.g., rotary encoders) to sense rotation of the wheels of the drive modules, inertial sensors (e.g., inertial measurement units, accelerometers, gyroscopes, magnetometers, etc.) to measure orientation and acceleration of the drive module, cameras or other image sensors, ultrasonic sensors to acoustically detect objects in the surroundings of the drive module, LiDAR sensors, radar sensors, etc. Some sensors, such as the wheel encoders can be unique to the drive module(s) <b>414</b>. In some cases, the sensor system(s) <b>406</b> on the drive module(s) <b>414</b> can overlap or supplement corresponding systems of the vehicle <b>402</b> (e.g., the sensor system(s) <b>406</b>).</p><p id="p-0084" num="0083">The drive module(s) <b>414</b> can include many vehicle systems, including a high voltage battery, a motor to propel the vehicle, an inverter to convert direct current from the battery into alternating current for use by other vehicle systems, a steering system including a steering motor and steering rack (which can be electric), a braking system including hydraulic or electric actuators, a suspension system including hydraulic and/or pneumatic components, a stability control system for distributing brake forces to mitigate loss of traction and maintain control, an HVAC system, lighting (e.g., lighting such as head/tail lights to illuminate an exterior surrounding of the vehicle), and one or more other systems (e.g., cooling system, safety systems, onboard charging system, other electrical components such as a DC/DC converter, a high voltage junction, a high voltage cable, charging system, charge port, etc.). Additionally, the drive module(s) <b>414</b> can include a drive module controller which can receive and preprocess data from the sensor system(s) <b>406</b> and to control operation of the various vehicle systems. In some examples, the drive module controller can include one or more processors and memory communicatively coupled with the one or more processors. The memory can store one or more modules to perform various functionalities of the drive module(s) <b>414</b>. Furthermore, the drive module(s) <b>414</b> also include one or more communication connection(s) that enable communication by the respective drive module with one or more other local or remote computing device(s).</p><p id="p-0085" num="0084">In at least one example, the direct connection <b>412</b> can provide a physical interface to couple the one or more drive module(s) <b>414</b> with the body of the vehicle <b>402</b>. For example, the direction connection <b>412</b> can allow the transfer of energy, fluids, air, data, etc. between the drive module(s) <b>414</b> and the vehicle. In some instances, the direct connection <b>412</b> can further releasably secure the drive module(s) <b>414</b> to the body of the vehicle <b>402</b>.</p><p id="p-0086" num="0085">The user interface <b>416</b> may include one or more devices, buttons and/or control panels via which a passenger can communicate with the vehicle <b>402</b>. In non-limiting examples, a passenger in the vehicle <b>402</b> may control functionality of the vehicle <b>402</b> via interaction(s) with the user interface <b>416</b>. In other examples, the user interface <b>416</b> may comprise a microphone configured to receive a verbal or spoken input. Generally, the user interface <b>416</b> may provide a means though which a passenger can interface with the vehicle computing device(s) <b>404</b>.</p><p id="p-0087" num="0086">In at least one example, the vehicle <b>402</b> may be in communication, via one or more network(s) <b>436</b>, with one or more computing device(s) <b>438</b>. For example, as described herein, the vehicle <b>402</b> can communicate with the one or more computing device(s) <b>438</b> via the network(s) <b>436</b>. In some examples, the vehicle <b>402</b> can receive control signals from the computing device(s) <b>438</b>. In other examples, the vehicle <b>402</b> can transmit information to the computing device(s) <b>438</b>.</p><p id="p-0088" num="0087">The computing device(s) <b>438</b> may be embodied as a fleet management system. In at least one example, the computing device(s) <b>438</b> can include processor(s) <b>440</b> and memory <b>442</b> communicatively coupled with the processor(s) <b>440</b>. In the illustrated example, the memory <b>442</b> of the computing device(s) <b>438</b> stores a track association component <b>444</b> and a track generation component <b>446</b>. In at least one example, the track association component <b>444</b> can correspond to at least a portion of the track association component <b>432</b>. Moreover, the track generation component <b>446</b> can correspond to at least a portion of the track generation component <b>434</b>.</p><p id="p-0089" num="0088">In some instances, aspects of some or all of the components discussed herein can include any models, algorithms, and/or machine learning algorithms. For example, in some instances, aspects of the components in the memory <b>420</b>, <b>442</b> can be implemented as a neural network.</p><p id="p-0090" num="0089">As described herein, an exemplary neural network is a biologically inspired algorithm which passes input data through a series of connected layers to produce an output. Each layer in a neural network can also comprise another neural network, or can comprise any number of layers (whether convolutional or not). As can be understood in the context of this disclosure, a neural network can use machine learning, which can refer to a broad class of such algorithms in which an output is generated based on learned parameters.</p><p id="p-0091" num="0090">Although discussed in the context of neural networks, any type of machine learning can be used consistent with this disclosure. For example, machine learning algorithms can include, but are not limited to, regression algorithms (e.g., ordinary least squares regression (OLSR), linear regression, logistic regression, stepwise regression, multivariate adaptive regression splines (MARS), locally estimated scatterplot smoothing (LOESS)), instance-based algorithms (e.g., ridge regression, least absolute shrinkage and selection operator (LASSO), elastic net, least-angle regression (LARS)), decisions tree algorithms (e.g., classification and regression tree (CART), iterative dichotomiser 3 (ID3), Chi-squared automatic interaction detection (CHAID), decision stump, conditional decision trees), Bayesian algorithms (e.g., na&#xef;ve Bayes, Gaussian na&#xef;ve Bayes, multinomial na&#xef;ve Bayes, average one-dependence estimators (AODE), Bayesian belief network (BNN), Bayesian networks), clustering algorithms (e.g., k-means, k-medians, expectation maximization (EM), hierarchical clustering), association rule learning algorithms (e.g., perceptron, back-propagation, hopfield network, Radial Basis Function Network (RBFN)), deep learning algorithms (e.g., Deep Boltzmann Machine (DBM), Deep Belief Networks (DBN), Convolutional Neural Network (CNN), Stacked Auto-Encoders), Dimensionality Reduction Algorithms (e.g., Principal Component Analysis (PCA), Principal Component Regression (PCR), Partial Least Squares Regression (PLSR), Sammon Mapping, Multidimensional Scaling (MDS), Projection Pursuit, Linear Discriminant Analysis (LDA), Mixture Discriminant Analysis (MDA), Quadratic Discriminant Analysis (QDA), Flexible Discriminant Analysis (FDA)), Ensemble Algorithms (e.g., Boosting, Bootstrapped Aggregation (Bagging), AdaBoost, Stacked Generalization (blending), Gradient Boosting Machines (GBM), Gradient Boosted Regression Trees (GBRT), Random Forest), SVM (support vector machine), supervised learning, unsupervised learning, semi-supervised learning, etc.</p><p id="p-0092" num="0091">Additional examples of architectures include neural networks such as ResNet50, ResNet101, VGG, DenseNet, PointNet, and the like.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIGS. <b>5</b>-<b>7</b></figref> (as well as <figref idref="DRAWINGS">FIGS. <b>2</b> and <b>3</b></figref> discussed above) illustrate example processes in accordance with embodiments of the disclosure. These processes are illustrated as logical flow graphs, each operation of which represents a sequence of operations that can be implemented in hardware, software, or a combination thereof. In the context of software, the operations represent computer-executable instructions stored on one or more computer-readable storage media that, when executed by one or more processors, perform the recited operations. Generally, computer-executable instructions include routines, programs, objects, components, data structures, and the like that perform particular functions or implement particular abstract data types. The order in which the operations are described is not intended to be construed as a limitation, and any number of the described operations can be combined in any order and/or in parallel to implement the processes.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an example process <b>500</b> for tracking objects using sensor data, such as radar data from multiple radar scans. For example, some or all of the process <b>500</b> can be performed by one or more components in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, as described herein. Without limitation, some or all of the process <b>500</b> can be performed by the radar processing system <b>420</b>, the track association component <b>432</b>, and/or the track generation component <b>434</b>.</p><p id="p-0095" num="0094">At an operation <b>502</b>, the process <b>500</b> includes receiving radar data from one or more radar sensors. As described above, the vehicle <b>102</b> includes the first and second radar sensors <b>108</b>, <b>110</b>, the vehicle <b>206</b> includes the first and second radar sensors <b>208</b>, <b>210</b>, and the vehicle <b>402</b> can include sensor system(s) <b>406</b> including a plurality of radar sensors having overlapping fields of view, e.g., to receive radar returns associated with the same objects in an environment of the vehicle <b>402</b>. The radar data can include one or more of position information of objects in an environment of the vehicle <b>402</b>, e.g., a range and direction, velocity information (e.g., a doppler velocity) for the objects, signal strength (e.g., an RCS measurement), or the like.</p><p id="p-0096" num="0095">At an operation <b>504</b>, the process <b>500</b> includes generating a representation of one or more objects based on the radar data. For example, the vehicle <b>102</b> includes the radar processing system <b>128</b> and the vehicle <b>402</b> includes the radar processing system <b>430</b>. The radar processing systems <b>128</b>, <b>430</b> include functionality to identify, from the radar data received at the operation <b>502</b>, sensed objects. In some examples, the radar processing systems <b>128</b>, <b>430</b> can include one or more trained machine learning models and/or other data processing models that receive the radar data as an input and outputs one or more representations of objects in the sensor data. Without limitation, the output may be one of the object representations <b>136</b>, the object representation <b>218</b>, and/or the sensed object representation <b>308</b>. As detailed herein, the object representation is a single representation, e.g., a bounding box of a sensed object, of a plurality of radar returns associated with that sensed object.</p><p id="p-0097" num="0096">At an operation <b>506</b>, the process <b>500</b> includes receiving track information including one or more existing tracks for one or more previously-detected objects. In implementations described herein, a vehicle is controlled to navigate relative to one or more objects in an environment. The vehicle can include functionality to determine, e.g., predict, movement of dynamic objects in the environment. Such predictions may be embodied as tracks associated with those detected objects.</p><p id="p-0098" num="0097">At an operation <b>508</b>, the process <b>500</b> includes determining whether the representation is associated with one of the existing track(s). For instance, the operation <b>508</b> can compare the sensed object representation to the track(s) and determine whether the sensed object represented by the representation generated at the operation <b>504</b> is already being tracked. The example <b>234</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and the example <b>304</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, both discussed above, illustrate example techniques for determining whether representations are associated with existing tracks. More specifically, the example <b>234</b> provides examples in which the representation is associated with a track and the example <b>304</b> provides examples in which the representation is not associated with a track. In examples, the operation <b>508</b> may be performed relative to each existing track, e.g., by comparing a representation of radar data to each existing track. In other examples, the representation may be compared only to existing tracks having some similarity, e.g., based on location, classification, or the like.</p><p id="p-0099" num="0098">If, at the operation <b>508</b>, it is determined that the representation is associated with one of the existing track(s), at an operation <b>510</b> the process <b>500</b> includes generating updated track information based on the representation. In examples described herein, once the representation based on the radar data is confirmed to be a representation of an object already being tracked, the track may be updated using the representation. As detailed herein, techniques according to this disclosure facilitate the use of radar data in tracking objects by obviating the need for per-point consideration of the returns, thereby reducing time and processing requirements that have caused conventional tracking techniques to avoid using radar data.</p><p id="p-0100" num="0099">Alternatively, if at the operation <b>508</b> it is determined that the representation is not associated with one of the existing track(s), at an operation <b>512</b> the process <b>500</b> includes generating a new track. In examples described herein, if a representation of a sensed object generated from radar data does not correspond to an existing track, the sensed object is not already being tracked. Accordingly, the operation <b>512</b>, which may be implemented by the track generation component <b>134</b>, <b>444</b> can generate a new track. In some examples, the operation <b>512</b> can correspond to the operation <b>334</b>. In examples described herein, a radar sensor may be among the first sensors on an automobile to detect objects. The techniques described herein can leverage this earlier detection by not only generating a representation of the sensed object, but also leveraging the representation to generate a new track for the sensed object.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts an example process <b>600</b> for generating a new track for a sensed object newly-detected in radar data. For example, some or all of the process <b>600</b> can be performed by one or more components in <figref idref="DRAWINGS">FIG. <b>4</b></figref> described herein. Without limitation, some or all of the process <b>600</b> can be performed by the radar processing system <b>420</b>, the track association component <b>432</b>, and/or the track generation component <b>434</b>.</p><p id="p-0102" num="0101">At an operation <b>602</b>, the process <b>600</b> includes receiving radar data from one or more radar sensors. As described above, the vehicle <b>102</b> includes the first and second radar sensors <b>108</b>, <b>110</b>, the vehicle <b>206</b> includes the first and second radar sensors <b>208</b>, <b>210</b>, and the vehicle <b>402</b> can include sensor system(s) <b>406</b> including a plurality of radar sensors having overlapping fields of view, e.g., to receive radar returns associated with the same objects in an environment of the vehicle <b>402</b>. The radar data can include one or more of position information of objects in an environment of the vehicle <b>402</b>, e.g., a range and direction, velocity information (e.g., a doppler velocity) for the objects, signal strength (e.g., an RCS measurement), or the like. In some instances, the radar data may be a point cloud, comprising a plurality of returns from a same radar scan or a plurality of radar scans. the radar returns can be represented as a point cloud and the point cloud can be passed into a machine-learned model.</p><p id="p-0103" num="0102">At an operation <b>604</b>, the process <b>600</b> includes passing the radar data to a machine learning model. For example, the radar processing system <b>128</b> can include one or more radar data processing algorithms including one or more convolutional neural networks and/or deep neural networks. In some examples, the machine learning model may be trained to output semantic information and/or state information by reviewing data logs to identify sensor data, e.g., subsets of the radar data, representing objects in an environment. Without limitation, the model may be trained on training data comprising ground truth information associated with object(s). As noted above, the radar data may be embodied as a point cloud passed to the machine learning model. In at least some examples, the point cloud can be passed to the same layer of the machine learned model, such that the machine learned model processes the plurality of radar returns, e.g., the point cloud, simultaneously.</p><p id="p-0104" num="0103">At an operation <b>606</b>, the process <b>600</b> includes receiving, from the machine learning model, a representation of a sensed object. For example, the representation may be the object representation <b>136</b> and/or may be a multi-dimensional representation, e.g., a two- or three-dimensional bounding box. The representation can include state and/or pose information about the sensed object, which can include, a classification (e.g., semantic information), a velocity, a position, an area, a volume, or the like. The representation can also include a confidence value associated with the representation and/or with one or more attributes of the representation. For example, the confidence value may be indicative of a likelihood that the representation accurately represents the sensed object. In examples, the representation is based only on the radar data, e.g., at the exclusion of other types of data. Without limitation, the operations <b>604</b>, <b>606</b> may be performed at least in part in accordance with the techniques described in U.S. patent application Ser. No. 16/587,605 for &#x201c;Perception System.&#x201d;</p><p id="p-0105" num="0104">At an operation <b>608</b>, the process <b>600</b> includes determining whether to track the sensed object. In some instances, the operation <b>608</b> can include comparing the representation of the sensed object to track information for existing tracks, e.g., as discussed above in connection with <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>3</b>, and <b>5</b></figref>. In other examples, the determination to track a sensed object may be made independently of any existing tracks. For instance, the operation <b>608</b> may include determining to generate a track based on one or more attributes of the representation. For instance, the operation <b>608</b> can include determining to generate a track in response to one or more of: a velocity of the sensed object exceeding a threshold velocity, a distance to the sensed object being equal to or less than a threshold distance, a classification of the sensed object corresponding to a predetermined classification (e.g., a vehicle), or the like. In still further examples, the operation <b>608</b> can determine to generate a track for the sensed object in response to a confidence value associated therewith being equal to or greater than a threshold confidence. In another example, the operation <b>608</b> can determine to generate a track based on a size of the sensed object. For example, tracks may be generated for relatively larger objects at a lower confidence threshold, at a closer proximity, or the like, whereas a smaller object going more slowly and/or at a greater distance may not be tracked. The operation <b>608</b> may also consider other details of the radar data, including a fidelity of the radar data, a signal to noise ratio associated therewith, or other data, to determine whether to track an object.</p><p id="p-0106" num="0105">If, at the operation <b>608</b> it is determined that the sensed object is not to be tracked, the process <b>400</b> may return to the operation <b>602</b>, e.g., to receive additional radar data from subsequent returns. Alternatively, if, at the operation <b>608</b> it is determined that the sensed object is to be tracked, at an operation <b>610</b> the process <b>600</b> includes generating a track for the sensed object. For example, the operation <b>610</b> may be implemented by the track generation component <b>434</b>. More specifically, the track generation component <b>434</b> can predict one or more future states or poses of the sensed object, e.g., at various future time increments, based on the representation. In some instances, the track may be based only on the representation, although in other instances the track may also be based on additional information. For instance, in examples described herein, the operation <b>610</b> can include generating a track from a plurality of the representations of the object, e.g., from radar scans taken at different times. Thus, the track may be based only on radar data, e.g., at the exclusion of other types of data from other sensor modalities. In still further examples, the track may be based on other types of data, from other sensor modalities. For instance, additional data from one or more of a lidar sensor, a camera, a time-of-flight sensor, or the like, may be received at the track generation component <b>434</b>. From this additional data, the track generation component can identify subsets of the data associated with the sensed object, and fuse those subsets with the representation to generate the track in some examples.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts an example process <b>700</b> for controlling an autonomous vehicle based at least in part on radar data, as discussed herein. For example, some or all of the process <b>700</b> can be performed by one or more components in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, as described herein. For example, some or all of the process <b>700</b> can be performed by the perception component <b>424</b>, the planning component <b>426</b>, the radar processing system <b>430</b> and/or the system controller(s) <b>428</b>.</p><p id="p-0108" num="0107">At an operation <b>702</b>, the process <b>700</b> can include receiving radar data associated with an object. In examples, the radar data is a representation of radar data, e.g., one of the representations <b>154</b> generated from a plurality of radar returns from one or more radar scans.</p><p id="p-0109" num="0108">At an operation <b>704</b>, the process <b>700</b> includes generating track information for the object based on radar data. As in the process <b>500</b>, the representation of the sensed object based on radar data can be used to generate an updated track for an already-tracked object and/or new tracks for newly-sensed objects.</p><p id="p-0110" num="0109">At an operation <b>706</b>, the process <b>700</b> can include generating, based on the track information and additional data, a travel path relative to the object. For example, the travel path generated at the operation <b>706</b> may be determined to travel in a manner that avoids the sensed object, follows the sensed object, passes the sensed object, or the like. In some examples, the additional data can include sensor data from one or more of the sensor system(s) <b>406</b>. Alternatively, or additionally, the additional data can include additional object representations, e.g., from previously- or subsequently-obtained radar scans.</p><p id="p-0111" num="0110">At operation <b>708</b>, the process <b>700</b> can include controlling an autonomous vehicle to follow the travel path. In some instances, the commands generated in the operation <b>708</b> can be relayed to a controller onboard an autonomous vehicle to control the autonomous vehicle to drive the travel path. Although discussed in the context of an autonomous vehicle, the process <b>700</b>, and the techniques and systems described herein, can be applied to a variety of systems utilizing sensors.</p><p id="p-0112" num="0111">The various techniques described herein can be implemented in the context of computer-executable instructions or software, such as program modules, that are stored in computer-readable storage and executed by the processor(s) of one or more computers or other devices such as those illustrated in the figures. Generally, program modules include routines, programs, objects, components, data structures, etc., and define operating logic for performing particular tasks, or implement particular abstract data types.</p><p id="p-0113" num="0112">Other architectures can be used to implement the described functionality, and are intended to be within the scope of this disclosure. Furthermore, although specific distributions of responsibilities are defined above for purposes of discussion, the various functions and responsibilities might be distributed and divided in different ways, depending on circumstances.</p><p id="p-0114" num="0113">Similarly, software can be stored and distributed in various ways and using different means, and the particular software storage and execution configurations described above can be varied in many different ways. Thus, software implementing the techniques described above can be distributed on various types of computer-readable media, not limited to the forms of memory that are specifically described.</p><heading id="h-0004" level="1">EXAMPLE CLAUSES</heading><p id="p-0115" num="0114">A: An example system includes: an autonomous vehicle; a radar sensor on the autonomous vehicle; one or more processors; and one or more non-transitory computer readable media storing instructions executable by the one or more processors, wherein the instructions, when executed, cause the system to perform operations comprising: receiving radar data captured by the radar sensor, the radar data comprising a plurality of points having individual velocities and positions; providing the radar data to a machine learned model; receiving, as an output from the machine learned model, a two-dimensional representation of a sensed object, the two-dimensional representation including a sensed velocity and a sensed position of the sensed object; determining, based at least in part on the two-dimensional representation, that the sensed object is not being tracked; generating, based on the two-dimensional representation, an estimated track of the sensed object; and controlling, based at least in part on the track, the autonomous vehicle relative to the estimated track.</p><p id="p-0116" num="0115">B: The system of example A, wherein the estimated track comprises one or more estimated future states of the sensed object.</p><p id="p-0117" num="0116">C: The system of example A or example B, wherein the object data comprises a classification of the sensed object and generating the estimated track of the sensed object comprises predicting the one or more estimated future states of the sensed object based at least in part on the two-dimensional representation and the classification.</p><p id="p-0118" num="0117">D: The system of any one of example A through example C, wherein: the object data comprises a certainty associated with the two-dimensional representation; and the generating the estimated track is based at least in part on the certainty being equal to or exceeding a threshold certainty.</p><p id="p-0119" num="0118">E: The system of any one of example A through example D, wherein the generating the estimated track is based at least in part on a size of the two-dimensional representation, a distance of the two-dimensional representation from the autonomous vehicle, or a velocity associated with the two-dimensional representation.</p><p id="p-0120" num="0119">F: The system of any one of example A through example E, wherein the determining that the sensed object is not being tracked is based at least in part on determining whether the two-dimensional representation is associated with track information associated with one or more tracked objects.</p><p id="p-0121" num="0120">G: An example method includes: receiving a plurality of radar returns associated with an environment; providing the plurality of radar returns to a machine learned model; receiving, as an output of the machine learned model, a multi-dimensional representation of the sensed object based on the plurality of radar returns; and generating, based on the multi-dimensional representation of the sensed object, an estimated track for the sensed object.</p><p id="p-0122" num="0121">H: The method of example G, wherein: the multi-dimensional representation includes a confidence associated with the multi-dimensional representation; and the generating the estimated track is based at least in part on the confidence being equal to or above a threshold confidence value.</p><p id="p-0123" num="0122">I: The method of example G or example H, wherein: the multi-dimensional representation includes at least one of a classification, a sensed velocity, or a sensed position; and the generating the estimated track is based at least in part on the classification corresponding to a predetermined object type, the sensed velocity meeting or exceeding a threshold velocity, or the sensed position being equal to or nearer than a threshold distance.</p><p id="p-0124" num="0123">J: The method of any one of example G through example I, wherein the generating the estimated track is performed in the absence of data other than radar data.</p><p id="p-0125" num="0124">K: The method of any one of example G through example J, further comprising: determining an absence of an existing track associated with the sensed object, wherein the generating the estimated track is based at least in part on the determining the absence of the existing track.</p><p id="p-0126" num="0125">L: The method of any one of example G through example K, wherein the determining the absence of the existing track is based at least in part on a comparison of the multi-dimensional representation with track information associated with the existing track, the comparison comprising at least one of a comparison of a velocity of the multi-dimensional representation with a track velocity, a comparison of a position of the multi-dimensional representation with a track position, or a comparison of an area of the multi-dimensional representation with an area of a tracked object.</p><p id="p-0127" num="0126">M: The method of any one of example G through example L, further comprising: receiving additional data from one or more additional sensors, the one or more additional sensors comprising at least one of a lidar sensor, a time-of-flight sensor, or an imaging sensor; and identifying, in the additional data, a subset of the additional data associated with the sensed object, wherein the generating the estimated track is further based at least in part on the subset of the additional data.</p><p id="p-0128" num="0127">N: The method of any one of example G through example M, wherein the estimated track comprises one or more estimated future states of the sensed object.</p><p id="p-0129" num="0128">O: The method of any one of example G through example N, wherein the object data comprises a classification of the sensed object and generating the estimated track of the sensed object comprises predicting the one or more estimated future states of the sensed object based at least in part on the multi-dimensional representation and the classification.</p><p id="p-0130" num="0129">P: The method of any one of example G through example O, wherein the plurality of radar returns associated with the sensed object are associated with a first time, the method further comprising: generating, based on additional radar returns associated with the sensed object and associated with a second time, a second multi-dimensional representation of the sensed object, wherein the generating the estimated track is based at least in part on the second multi-dimensional representation of the sensed object.</p><p id="p-0131" num="0130">Q: The method of any one of example G through example P, further comprising: generating a trajectory for controlling an autonomous vehicle relative to the new estimated track; and controlling the autonomous vehicle to travel based on the trajectory.</p><p id="p-0132" num="0131">R: The method of any one of example G through example J, wherein the plurality of radar returns are provided to a same layer of the machine learned model such that the machine learned model processes the plurality or radar returns simultaneously.</p><p id="p-0133" num="0132">S: Example non-transitory computer readable media storing instructions that, when executed by one or more processors, cause the processors to perform operations comprising: receiving a plurality of radar returns associated with an environment; providing the plurality of radar returns to a machine learned model; receiving, as an output of the machine learned model, a two-dimensional representation of the sensed object based on the plurality of radar returns and a confidence associated with the two-dimensional representation; and generating, based on the two-dimensional representation of the sensed object and on the confidence value being equal to or exceeding a threshold confidence value, an estimated track for the sensed object.</p><p id="p-0134" num="0133">T: The non-transitory computer readable media of example S, the operations further comprising: determining an absence of an existing track associated with the sensed object, wherein the generating the estimated track is based at least in part on the determining the absence of the existing track.</p><heading id="h-0005" level="1">CONCLUSION</heading><p id="p-0135" num="0134">While one or more examples of the techniques described herein have been described, various alterations, additions, permutations and equivalents thereof are included within the scope of the techniques described herein.</p><p id="p-0136" num="0135">In the description of examples, reference is made to the accompanying drawings that form a part hereof, which show by way of illustration specific examples of the claimed subject matter. It is to be understood that other examples can be used and that changes or alterations, such as structural changes, can be made. Such examples, changes or alterations are not necessarily departures from the scope with respect to the intended claimed subject matter. While the steps herein can be presented in a certain order, in some cases the ordering can be changed so that certain inputs are provided at different times or in a different order without changing the function of the systems and methods described. The disclosed procedures could also be executed in different orders. Additionally, various computations described herein need not be performed in the order disclosed, and other examples using alternative orderings of the computations could be readily implemented. In addition to being reordered, in some instances, the computations could also be decomposed into sub-computations with the same results.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A system comprising:<claim-text>an autonomous vehicle;</claim-text><claim-text>a radar sensor on the autonomous vehicle;</claim-text><claim-text>one or more processors; and</claim-text><claim-text>one or more non-transitory computer readable media storing instructions executable by the one or more processors, wherein the instructions, when executed, cause the system to perform operations comprising:<claim-text>receiving radar data captured by the radar sensor, the radar data comprising a plurality of points having individual velocities and positions;</claim-text><claim-text>providing the radar data to a machine learned model;</claim-text><claim-text>receiving, as an output from the machine learned model, a two-dimensional representation of a sensed object, the two-dimensional representation including a sensed velocity and a sensed position of the sensed object;</claim-text><claim-text>determining, based at least in part on the two-dimensional representation, that the sensed object is not being tracked;</claim-text><claim-text>generating, based on the two-dimensional representation, an estimated track of the sensed object; and</claim-text><claim-text>controlling, based at least in part on the track, the autonomous vehicle relative to the estimated track.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the estimated track comprises one or more estimated future states of the sensed object.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The system of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the object data comprises a classification of the sensed object and generating the estimated track of the sensed object comprises predicting the one or more estimated future states of the sensed object based at least in part on the two-dimensional representation and the classification.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the object data comprises a certainty associated with the two-dimensional representation; and</claim-text><claim-text>the generating the estimated track is based at least in part on the certainty being equal to or exceeding a threshold certainty.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the generating the estimated track is based at least in part on a size of the two-dimensional representation, a distance of the two-dimensional representation from the autonomous vehicle, or a velocity associated with the two-dimensional representation.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The system of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining that the sensed object is not being tracked is based at least in part on determining whether the two-dimensional representation is associated with track information associated with one or more tracked objects.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A method comprising:<claim-text>receiving a plurality of radar returns associated with an environment;</claim-text><claim-text>providing the plurality of radar returns to a machine learned model;</claim-text><claim-text>receiving, as an output of the machine learned model, a multi-dimensional representation of the sensed object based on the plurality of radar returns; and</claim-text><claim-text>generating, based on the multi-dimensional representation of the sensed object, an estimated track for the sensed object.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein:<claim-text>the multi-dimensional representation includes a confidence associated with the multi-dimensional representation; and</claim-text><claim-text>the generating the estimated track is based at least in part on the confidence being equal to or above a threshold confidence value.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein:<claim-text>the multi-dimensional representation includes at least one of a classification, a sensed velocity, or a sensed position; and</claim-text><claim-text>the generating the estimated track is based at least in part on the classification corresponding to a predetermined object type, the sensed velocity meeting or exceeding a threshold velocity, or the sensed position being equal to or nearer than a threshold distance.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the generating the estimated track is performed in the absence of data other than radar data.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>determining an absence of an existing track associated with the sensed object, wherein the generating the estimated track is based at least in part on the determining the absence of the existing track.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the determining the absence of the existing track is based at least in part on a comparison of the multi-dimensional representation with track information associated with the existing track, the comparison comprising at least one of a comparison of a velocity of the multi-dimensional representation with a track velocity, a comparison of a position of the multi-dimensional representation with a track position, or a comparison of an area of the multi-dimensional representation with an area of a tracked object.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>receiving additional data from one or more additional sensors, the one or more additional sensors comprising at least one of a lidar sensor, a time-of-flight sensor, or an imaging sensor; and</claim-text><claim-text>identifying, in the additional data, a subset of the additional data associated with the sensed object,</claim-text><claim-text>wherein the generating the estimated track is further based at least in part on the subset of the additional data.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the estimated track comprises one or more estimated future states of the sensed object.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the object data comprises a classification of the sensed object and generating the estimated track of the sensed object comprises predicting the one or more estimated future states of the sensed object based at least in part on the multi-dimensional representation and the classification.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the plurality of radar returns associated with the sensed object are associated with a first time, the method further comprising:<claim-text>generating, based on additional radar returns associated with the sensed object and associated with a second time, a second multi-dimensional representation of the sensed object,</claim-text><claim-text>wherein the generating the estimated track is based at least in part on the second multi-dimensional representation of the sensed object.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further comprising:<claim-text>generating a trajectory for controlling an autonomous vehicle relative to the new estimated track; and</claim-text><claim-text>controlling the autonomous vehicle to travel based on the trajectory.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the plurality of radar returns are provided to a same layer of the machine learned model such that the machine learned model processes the plurality or radar returns simultaneously.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. Non-transitory computer readable media storing instructions that, when executed by one or more processors, cause the processors to perform operations comprising:<claim-text>receiving a plurality of radar returns associated with an environment;</claim-text><claim-text>providing the plurality of radar returns to a machine learned model;</claim-text><claim-text>receiving, as an output of the machine learned model, a two-dimensional representation of the sensed object based on the plurality of radar returns and a confidence associated with the two-dimensional representation; and</claim-text><claim-text>generating, based on the two-dimensional representation of the sensed object and on the confidence value being equal to or exceeding a threshold confidence value, an estimated track for the sensed object.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable media of <claim-ref idref="CLM-00019">claim 19</claim-ref>, the operations further comprising:<claim-text>determining an absence of an existing track associated with the sensed object,</claim-text><claim-text>wherein the generating the estimated track is based at least in part on the determining the absence of the existing track.</claim-text></claim-text></claim></claims></us-patent-application>