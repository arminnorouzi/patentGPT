<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005469A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005469</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17852548</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>NO</country><doc-number>20210874</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>06</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>84</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0224</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>063</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>57</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>15</main-group><subgroup>02</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>25</main-group><subgroup>84</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>10</class><subclass>L</subclass><main-group>21</main-group><subgroup>0224</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND SYSTEM FOR SPEECH DETECTION AND SPEECH ENHANCEMENT</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="obligated-assignee"><addressbook><orgname>PEXIP AS</orgname><address><city>Oslo</city><country>NO</country></address></addressbook><residence><country>NO</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kim</last-name><first-name>Anna</first-name><address><city>Asker</city><country>NO</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Shaw</last-name><first-name>Eamonn</first-name><address><city>Gamle Fredrikstad</city><country>NO</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of speech detection and speech enhancement in a speech detection and speech enhancement unit of Multipoint Conferencing Node (MCN) and a method of training the same. The method comprising receiving input audio segments, and determining an acoustic environment based on input audio auxiliary information, extracting T-F-domain features from the received input audio segments, determining if each of the received input audio segments is speech by inputting the T-F domain features into a speech detection classifier trained for the determined acoustic environment, determining, when one of the received input audio segments is speech, if the received audio segment is noisy speech by inputting the T-F domain features into a noise classifier using a statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment, and applying a noise reduction mask on the received input audio segments according to the determination of the received audio segment is noisy speech</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="61.81mm" wi="140.21mm" file="US20230005469A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="198.54mm" wi="146.56mm" file="US20230005469A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="203.37mm" wi="118.70mm" file="US20230005469A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="175.18mm" wi="149.86mm" file="US20230005469A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is related to and claimed priority to Norwegian Patent Application No. 20210874, filed Jun. 30, 2021, entitled METHOD AND SYSTEM FOR SPEECH DETECTION AND SPEECH ENHANCEMENT, the entirety of which is incorporated herein by reference.</p><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present invention relates to detecting and enhancing speech in a multipoint videoconferencing session, in particular a method of speech detection and speech enhancement in a speech detection and speech enhancement unit of Multipoint Conferencing Node (MCN) and a method of training the same.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Transmission of audio and moving pictures in real-time is employed in several applications like e.g. video conferencing, team collaboration software, net meetings and video telephony. Terminals and endpoints being able to participate in a conference may be traditional stationary video conferencing endpoints, external devices, such as mobile and computer devices, smartphones, tablets, personal devices and PCs, and browser-based video conferencing terminals.</p><p id="p-0005" num="0004">Video conferencing systems allow for simultaneous exchange of audio, video and data information among multiple conferencing sites. For performing multipoint video conferencing, there usually is a Multipoint Conferencing Node (MCN) that provides switching and layout functions to allow the endpoints and terminals of multiple sites to intercommunicate in a conference. Such nodes may also be referred to as Multipoint Control Units (MCUs), Multi Control Infrastructure (MCI), Conference Nodes and Collaborations Nodes (CNs). MCU is the most common used term, and has traditionally has been associated with hardware dedicated to the purpose, however, the functions of an MCN could just as well be implemented in software installed on general purpose severs and computers, so in the following, all kinds of nodes, devices and software implementing features, services and functions providing switching and layout functions to allow the endpoints and terminals of multiple sites to intercommunicate in a conference, including (but not excluding) MCUs, MCIs and CNs are from now on referred to as MCNs.</p><p id="p-0006" num="0005">Audio quality represents a key aspect of the video conferencing experience. One major challenge is the heterogeneous and dynamic nature of the audio environment of the various conference participants. In a home office, some participants may use headsets with directional microphones that are closed to the mouth, while others may use the built-in speaker and microphone on a laptop computer. In a typical meeting room that is equipped with a video conferencing endpoint, a speaker with multiple microphones is often placed in the middle of the table, where participants are sitting at different distances to the shared audio unit. There may also be participants who are connected to the conference via a smartphone that is either handheld or using an external headset with microphone. These diverse physical setups mean that different types of noise may be picked up during the course of the conference. Relying on the user to constantly or consciously mute and unmute the microphone can be cumbersome and tiring, while disrupting the flow of the conversation.</p><p id="p-0007" num="0006">Effectively reducing the impact of noise in a video conference setting is a challenging problem. Depending on the source and the environment, the noise contributions may be stationary or non-stationary, bursty, narrow or wideband, or contain speech-like harmonics. One challenge is to reduce or eliminate noises that are disturbances in the meeting. Some noise can be minimized by audio devices on the client side which have active background noise cancellation. Non-verbal sounds produced by the speaker, such as coughing, sneezing and heavy breathing are generally undesirable but can't be removed in the same manner. Means to differentiate speech from noise, i.e. reliable speech detection, is therefore needed. In addition, speech corrupted by noise becomes less intelligible. The extent of degradation depends on the amount and the type of noise contributions. How to enhance the quality and intelligibility of speech when noise is present is the other challenge to be addressed.</p><p id="p-0008" num="0007">Known solutions a for speech detection, also known as voice activity detection (VAD), and speech enhancement are typically implemented on the client side, i.e. where speech is generated or perceived. The GSM standard features audio codecs that support VAD for better bandwidth utilization. Various enhancement algorithms implementations can be found in various high performance headsets.</p><p id="p-0009" num="0008">Recent advances in machine learning also lead to a large number of deep learning neural network (DNN) based speech processing algorithms. The Opus audio codec for example improves VAD and supports classification of speech and music using RNN (recurrent neural networks) in its Opus 1.3 release. Microsoft have also been actively pursuing real-time de-noising of speech using neural networks (venturebeat.com/2020/04/09/microsoft-teams-ai-machine-learning-real-time-noise-suppression-typing/) implemented on the client side. Virtual microphone and speaker systems, such as Nvidia RTX voice noise cancellation plugin and Krisp, can be installed on a computer and be used in conjunction with conferencing and streaming services to improve audio quality, handling both inbound (speaker) and outbound (microphone) noises.</p><p id="p-0010" num="0009">Due to the size of the resulting DNN models, which is required to cover a large set of noise types (&#x3e;100), latency is always a challenge for deployment in real-time interactive applications. The large computation resources demanded for inference means scalability is an issue in video conferences, as it cannot be simply offloaded to the cloud, which would induce added cost of communication bandwidth and delay, resulting in client side implementations as discussed above.</p><p id="p-0011" num="0010">There are, however, clear benefits for server side media handling in video conferencing. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, audio and video streams from conference participants are received and processed in parallel in the Multipoint Conferencing Node (MCN). Such an architecture may ensure end-to-end quality-of-service, balance network loads, and provide advanced processing and services. Speech detection and enhancement done in the MCN would provide the participants with better audio quality irrespective of the client-side implementation. The user is completely freed of the burden to decide if inbound or outbound noise reduction is needed.</p><p id="p-0012" num="0011">Therefore, there is a need for as system and method of a server side speech detection and enhancement method that is scalable to support a varying number of participants, meets delay requirements of interactive real-time communication, and is resource efficient, such that it can run on a general purpose CPU, and requires little overhead information to be forwarded from node to node.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0013" num="0012">In view of the above, an object of the present invention is to overcome or at least mitigate drawbacks of prior art video conferencing systems.</p><p id="p-0014" num="0013">A first aspect the invention provides a method of training a speech detection and speech enhancement unit of a Multipoint Conferencing Node (MCN), comprising:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0014">collecting, for each of a plurality of acoustic environments, a plurality of audio samples consisting of noise, clean speech and noisy speech;</li>        <li id="ul0002-0002" num="0015">labeling each of the plurality of samples with an acoustic environmental label corresponding to one of the plurality of acoustic environments;</li>        <li id="ul0002-0003" num="0016">labeling the plurality of audio samples consisting of clean speech with clean speech label;</li>        <li id="ul0002-0004" num="0017">extracting Time-Frequency (T-F) domain features from the plurality of audio samples consisting of noise, clean speech and noisy speech;</li>        <li id="ul0002-0005" num="0018">training, for each of the plurality of acoustic environments, one speech detection classifier by inputting the T-F domain features of the plurality of audio samples consisting of noise, clean speech and noisy speech, the acoustic environmental labels and the clean speech labels to a first deep neural network; and</li>        <li id="ul0002-0006" num="0019">training, for each of the plurality of environments, one statistical generative model representing the probability distributions of the T-F domain features of noisy speech, by inputting the T-F domain features of the plurality of audio samples comprising clean speech, the T-F domain features of the plurality audio samples comprising noise and the plurality of audio samples comprising noisy speech.</li>    </ul>    </li></ul></p><p id="p-0015" num="0020">Training the speech detection and speech enhancement unit separately for different of acoustic environments has several advantages. One advantage is that when the machine learning models that are optimized for the specific categories of acoustic environment, the machine learning models are smaller and require shorter time and resources to train. Another advantage is that trained machine learning model do not require large scale DNNs, hence greatly reduced computation footprint and resource utilization and makes the system possible to scale.</p><p id="p-0016" num="0021">Another advantage of training the speech detection and speech enhancement unit separately for different of acoustic environments is that it can easily be extended to accommodate different acoustic environments of interest.</p><p id="p-0017" num="0022">In one embodiment, the plurality of acoustic environments may comprise meeting room with video conferencing endpoint, home office, and public space.</p><p id="p-0018" num="0023">In one embodiment, the statistical generative model may be a Gaussian Mixture Model.</p><p id="p-0019" num="0024">A second aspect of the invention provides a method of speech detection and speech enhancement in a speech detection and speech enhancement unit of Multipoint Conferencing Node (MCN), comprising:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0025">receiving input audio segments from at least one videoconferencing participants;</li>        <li id="ul0004-0002" num="0026">determining an acoustic environment based on auxiliary information of the at least one videoconferencing participant;</li>        <li id="ul0004-0003" num="0027">extracting Time-Frequency (T-F) domain features from the received input audio segments;</li>        <li id="ul0004-0004" num="0028">determining if each of the received input audio segments is speech by inputting the T-F domain features into a speech detection classifier trained for the determined acoustic environment;</li>        <li id="ul0004-0005" num="0029">determining, when one of the received input audio segments is speech, if the received audio segment is noisy speech by inputting the T-F domain features into a noise classifier using a statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment; and</li>        <li id="ul0004-0006" num="0030">applying a noise reduction mask on the received input audio segments according to the determination of the received audio segment is noisy speech.</li>    </ul>    </li></ul></p><p id="p-0020" num="0031">In addition to the advantages mentioned above for the first aspect, and advantage of the second aspect is that noise is suppress but not eliminated, such that speech is more easily comprehensible, but the underlying acoustic background is not lost.</p><p id="p-0021" num="0032">In one embodiment, the auxiliary information of the at least one videoconferencing participant may comprise at least one of a number of participants in a video image received from the at least one videoconferencing participant, and a specification of a videoconferencing endpoint received from the at least one videoconferencing participant. In one embodiment, the acoustic environment may comprise meeting room with video conferencing endpoint, home office, and public space.</p><p id="p-0022" num="0033">In one embodiment, the speech detection and speech enhancement unit may be trained according the method of the first aspect.</p><p id="p-0023" num="0034">In one embodiment, the noise classifier may be a Bayesian classifier.</p><p id="p-0024" num="0035">In one embodiment, the noise reduction mask may be a composite noise reduction mask. In one embodiment the composite noise reduction mask may be based on an estimated binary mask (EBM) generated using the Bayesian classifier.</p><p id="p-0025" num="0036">In one embodiment, the method of the second aspect may comprise updating the statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment when the estimated probability of one the received input audio segments is noisy speech is close an estimated probability of the one received input audio segments is belonging clean speech.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0026" num="0037">A more complete understanding of the present invention, and the attendant advantages and features thereof, will be more readily understood by reference to the following detailed description when considered in conjunction with the accompanying drawings wherein:</p><p id="p-0027" num="0038"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic illustration of a multi-point videoconferencing system;</p><p id="p-0028" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating an exemplary method of providing speech detection and speech enhancement in a multi-point videoconferencing system;</p><p id="p-0029" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an exemplary illustration of Time-Frequency domain features of speech and noise; and</p><p id="p-0030" num="0041"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an exemplary flowchart illustrating an exemplary embodiment of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0031" num="0042">According to embodiments of the present invention as disclosed herein, the above-mentioned disadvantages of solutions according to prior art are eliminated or at least mitigated.</p><p id="p-0032" num="0043"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically illustrates multi-point videoconferencing system <b>100</b> with three videoconferencing endpoints <b>101</b><i>a</i>, <b>101</b><i>b</i>, <b>101</b><i>c </i>in communication with a multipoint conferencing node (MCN) <b>104</b>. In the multi-point videoconferencing system <b>100</b> all media passes through the MCN <b>104</b>. Input audio <b>102</b><i>a</i>, <b>102</b><i>b</i>, <b>102</b><i>c </i>captured at the videoconferencing endpoints <b>101</b><i>a</i>, <b>101</b><i>b</i>, <b>101</b><i>c </i>is transmitted to the MCU <b>104</b>, then the input audio <b>102</b><i>a</i>, <b>102</b><i>b</i>, <b>102</b><i>c </i>is mixed with audio from the other videoconferencing endpoints <b>101</b><i>a</i>, <b>101</b><i>b</i>, <b>101</b><i>c</i>, and output audio <b>103</b><i>a</i>, <b>103</b><i>b</i>, <b>103</b><i>c </i>is transmitted back out to the videoconferencing endpoints <b>101</b><i>a</i>, <b>101</b><i>b</i>, <b>101</b><i>c. </i></p><p id="p-0033" num="0044"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating an exemplary method of providing speech detection and speech enhancement in a multi-point videoconferencing system.</p><p id="p-0034" num="0045">The first step of a method of training a speech detection and speech enhancement unit of a Multipoint Conferencing Node (MCN) is to provide an acoustic dependent dataset <b>1</b>. In one embodiment it is collected, for each of a plurality of acoustic environments, a plurality of audio samples consisting of noise, clean speech and noisy speech. The plurality of acoustic environments may comprise three broad acoustic environment categories, such as meeting room with video conferencing endpoint, home office, and public space.</p><p id="p-0035" num="0046">Within each category, the dataset consists of noise n(k), and clean speech x(k) and noisy speech z(k) samples, where k denotes the sampling instances in time. The noisy speech may be described by z(k)=g(x(k),n(k). Depending on the specific acoustic environment class, the relationship function g(&#xb7;) may be linear (e.g. additive) or nonlinear (in the case of reverberation). It is not required that g(&#xb7;) is known. It is sufficient that each noisy speech has a known source of noise and speech it is generated from. The dataset is preferably made to capture main sources of noise and their relations to clean speech in a videoconferencing setting. The dataset should be sufficiently large to avoid overfitting of machine learning models. The dataset may preferably contain samples of different languages and utterances of people of various gender, age, and accents. The dataset may preferably contain common noise contributions from the considered acoustic environments.</p><p id="p-0036" num="0047">There are several advantages of building an acoustic environment dependent dataset such as to reduce mismatch between training and testing data, such that resulting machine learning model deliver consistent results; to reduce the complexity of data pattern expected to be captured by the machine learning model, such that the resulting compact model does not run the risk of underfitting, while avoiding overfitting which is common for large DNN, and to facilitate online updates of models.</p><p id="p-0037" num="0048">For the speech detection part of the system, each of the plurality of samples is labeled with an acoustic environmental label corresponding to one of the plurality of acoustic environments, such as meeting room with video conferencing endpoint, home office, and public space.</p><p id="p-0038" num="0049">Each of the plurality of audio samples consisting of clean speech is labeled with clean speech label. The labels may be provided in the form of timestamps indicating when clean speech starts and stops.</p><p id="p-0039" num="0050">The next step of the training method is to extract Time-Frequency (T-F) domain features <b>2</b> from the plurality of audio samples consisting of noise, clean speech and noisy speech.</p><p id="p-0040" num="0051">Extracting features from a speech signal in the T-F domain may be performed by several possible methodologies known to the skilled person. Different choices of methodology will lead to different machine learning models and variations in performance. However, there is no specific feature extraction method that is optimal in all use case scenarios. One exemplary, commonly used feature extraction methods, which generates Mel frequency coefficients is described herein with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Audio samples are packed into fixed frame sizes in milliseconds range (e.g. 10 ms). Short Time Fourier Transform is applied to the resulting audio frames in fixed batch size (e.g. 4) into the frequency domain, where there are overlapping between the frames (e.g. 50%). A windowing function (e.g. Hanning) is applied to minimize the edge effects between frames. 128 channel Mel filterbanks is applied and then Discrete Cosine Transform is applied. This results in n&#xd7;M features where n is the number of frames processed in batch and M is the number of Mel-frequency cepstrum coefficient (MFCC).</p><p id="p-0041" num="0052">The next step of the training method is training, for each of the plurality of acoustic environments, one speech detection classifier <b>3</b> by inputting the T-F domain features of the plurality of audio samples consisting of noise, clean speech and noisy speech, the acoustic environmental labels and the clean speech labels to a first deep neural network (DNN). That is, one classifier is trained per acoustic environment category, or its subcategories if needed. Weights of the trained classifier may be quantized to support for example 16 bits precision or lower. The trained DNN model typically contains an input layer, hidden layers and output layer.</p><p id="p-0042" num="0053">The next step of the training method comprises training, for each of the plurality of environments, one statistical generative model <b>5</b> representing the probability distributions of the T-F domain features of noisy speech, by inputting the T-F domain features of the plurality of audio samples comprising clean speech, the T-F domain features of the plurality audio samples comprising noise and the plurality of audio samples comprising noisy speech. The statistical generative model may in one embodiment be a Gaussian Mixture Model (GMM).</p><p id="p-0043" num="0054">The Ideal binary mask (IBM) is a known technique for noise suppression in speech. The binary mask must be estimated in practice, as the IBM is not known apriori. There have been various approaches in estimating the binary mask, both machine learning and non-machine learning based. A statistical generative model may be trained in the unsupervised manner to represent the probability distributions of the T-F features of noisy speech.</p><p id="p-0044" num="0055">For each acoustic environment category, during offline training the T-F features of the acoustic training samples are divided into two classes, Cl<sub>s</sub>, where speech is dominating, i.e. clean speech, and Cl<sub>n</sub>, where noise is dominating, i.e. noisy speech. The division is according to a priori Signal-to-Noise (SNR) ratio, which is defined as: SNR<sub>aprior</sub>=|X(&#x3c4;,k)|<sup>2</sup>/E[IN(&#x3c4;,k)|<sup>2</sup>], where X(&#x3c4;,k) is the T-F feature vectors of the clean speech signal, N(&#x3c4;,k) is that of the noisy speech signal and E[&#xb7;] is an expectations operator. For each class, a Gaussian Mixture Model (GMM) is trained to represent the distribution of the different feature vectors at each T-F instance, for the calculated SNR<sub>aprior</sub>. The GMM may be initialized using K-means clustering and optimized using standard techniques such as Expectation Maximization as known to the skilled person.</p><p id="p-0045" num="0056">Again, with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, online operation of the speech detection and speech enhancement unit of Multipoint Conferencing Node (MCN) will be described by the following method of speech detection and speech enhancement. The MCN receives input audio segments from at least one videoconferencing participant <b>101</b><i>a</i>, <b>101</b><i>b</i>, <b>101</b><i>c</i>. The MCN determines an acoustic environment <b>4</b> based on auxiliary information of the at least one videoconferencing participant. The auxiliary information of the at least one videoconferencing participant may comprise a number of participants in a video image received from the at least one videoconferencing participant, e.g. determined via face count by analyzing the video image. Home offices are typically limited to one participant, meeting rooms may have multiple faces counts, whereas public spaces can have multiple face counts but will not be identified as a meeting room. The auxiliary information of the at least one videoconferencing participant may also comprise a specification of a videoconferencing endpoint received from the at least one videoconferencing participant, such as identification of the type of videoconferencing endpoint, e.g. room system, desktop system, PC-client etc. The auxiliary information is preferably not stored, nor may it be used to identify specific participants.</p><p id="p-0046" num="0057">The next step of online operation of the speech detection and speech enhancement unit is to extract Time-Frequency (T-F) domain features <b>2</b> from the received input audio segments. This is may be done as described above with reference to the methodology of training the system. The next step is to input the extracted T-F domain features into a speech detection classifier <b>3</b> trained for the determined acoustic environment to determine if each of the received input audio segments is speech.</p><p id="p-0047" num="0058">Once determined that one of the received input audio segments is speech, it is determined if the received audio segment is noisy speech by inputting the T-F domain features into a noise classifier <b>5</b> using a statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment.</p><p id="p-0048" num="0059">Then a noise reduction mask <b>6</b> is applied on the received input audio segments according to the determination of the received audio segment is noisy speech, whereby enhanced speech is obtained in the audio segments.</p><p id="p-0049" num="0060">In one exemplary embodiment, the noise classifier is a Bayesian classifier</p><p id="p-0050" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>Cl</i><sub>s</sub><i>|Z</i>(&#x3c4;,<i>k</i>))=&#x3b3;<i>P</i>(<i>Cl</i><sub>s</sub>)<i>P</i>(<i>Z</i>(&#x3c4;,<i>k</i>)|<i>Cl</i><sub>s</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0051" num="0061">Where P(Cl<sub>s</sub>|Z(&#x3c4;,k)) is the a posteriori probability of the noisy speech feature vectors Z(&#x3c4;,k) belong to the speech dominant class Cl<sub>s</sub>, P(Z(&#x3c4;,k)|Cl<sub>s</sub>) is the corresponding a priori probability, P(Cl<sub>s</sub>) is the probability of the feature vectors of the class and &#x3b3; denotes the constant offset, i.e. probability of features Z(&#x3c4;,k). An a posteriori probability estimation is done for the noise dominant class P(Cl<sub>n</sub>|Z(&#x3c4;,k)).</p><p id="p-0052" num="0062">The noise reduction mask may in one embodiment be a composite noise reduction mask, in particular based on an estimated binary mask (EBM) generated using the Bayesian classifier.</p><p id="p-0053" num="0063">Noise reduction based on spectral gain defined in terms of SNR<sub>aprior </sub>may effectively limit the musical noise (harmonic distortions), part of the speech harmonics might also be lost in the process. Hard decision noise mask such as Ideal binary mask (IBM) suffers from this effect. The Ideal ratio mask on the other hand is a soft decision mask which is made by applying the energy ratio between the target clean speech and the received noisy speech. Since it is based on a posteriori SNR (received noisy speech), it has the drawback of inducing harmonic distortions and can result in ringing sound which is undesirable. On the other hand, less removal of noise can be used for retaining the acoustic context. Combination of the two masks may therefore be used to achieve better balance between noise reduction and speech enhancement.</p><p id="p-0054" num="0064">An estimated binary mask (EBM) is generated using the Bayesian classifier outcome as discussed above:</p><p id="p-0055" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <mi>EBM</mi>   <mo>&#x2061;</mo>   <mo>(</mo>   <mrow>    <mi>r</mi>    <mo>,</mo>    <mi>k</mi>   </mrow>   <mo>)</mo>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mn>1</mn>     </mtd>     <mtd>      <mrow>       <mrow>        <mi>if</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mrow>         <mi>P</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <msub>           <mi>Cl</mi>           <mi>x</mi>          </msub>          <mo>&#x2758;</mo>          <mrow>           <mi>Z</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mi>r</mi>            <mo>,</mo>            <mi>k</mi>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>&#x3e;</mo>       <mrow>        <mi>P</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <msub>          <mi>Cl</mi>          <mi>n</mi>         </msub>         <mo>&#x2758;</mo>         <mrow>          <mi>Z</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>r</mi>           <mo>,</mo>           <mi>k</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>)</mo>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mn>0</mn>     </mtd>     <mtd>      <mi>otherwise</mi>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0056" num="0065">The estimated ratio mask (ERM) is defined as ERM=|{circumflex over (X)}(&#x3c4;,k)|<sup>2</sup>/|Z(&#x3c4;,k)|<sup>2</sup>, and |{circumflex over (X)}(&#x3c4;,k)|<sup>2 </sup>is the estimated clean speech power. The ERM can be determined as follows:</p><p id="p-0057" num="0066">a) using a decision direct approach, which indicates that the a priori SNR of the current frame follows the closely the previous frame's a posteriori SNR, SNR<sub>aposte</sub>=|Z(&#x3c4;,k&#x2212;1)|<sup>2</sup>/E[|N(&#x3c4;,k&#x2212;1)|<sup>2</sup>]</p><p id="p-0058" num="0067">b) ERM is then the ratio of the current frame's &#x15c;NR<sub>aprio </sub>and SNR<sub>aposte </sub></p><p id="p-0059" num="0068">The composite noise reduction mask (CM) is then:</p><p id="p-0060" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>CM</i>(&#x3c4;,<i>k</i>)=&#x3b1;<i>ERM</i>(&#x3c4;,<i>k</i>)+&#x3b2;<i>EBM</i>(&#x3c4;,<i>k</i>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0061" num="0069">where the weights &#x3b1; and &#x3b2; are tuned for each of the acoustic environment class. In general, it is expected the ERM contribution to be considerably smaller than EBM, as some noise contribution is expected to retain to provide the acoustic context.</p><p id="p-0062" num="0070">The estimated CM is applied to each T-F region of the noisy speech spectrum, summing the components from the different filter bands and inverse Fourier transform is then applied to construct the resulting enhanced speech signal.</p><p id="p-0063" num="0071">In one embodiment, as illustrated with <figref idref="DRAWINGS">FIG. <b>4</b></figref>, during online operations the method may further comprise updating <b>7</b> the statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment when the estimated probability of one the received input audio segments is noisy speech is close an estimated probability of the one received input audio segments is belonging clean speech. In particular, new data can be included in the trained GMM to provide better confidence in the EBM estimation. New weights of the model can be estimated using the same EM algorithm for training the GMM offline, along with new means and variance for the mixture. Constraints may be applied to ensure convergence of the EM algorithm.</p><p id="p-0064" num="0072">In the method illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, T-F features of the current audio frame is extracted. Then a posteriori probability estimation is performed using the trained GMM. The ratio of the number of ambiguous T-F instances relative to the total number of T-F features is calculated. Then the calculated ratio is compared with a threshold. If the ratio is below the threshold, then the composite noise mask is calculated. If the calculated ratio is above the threshold, then the mean, variance and weights are recalculated, and the used to update the GMM for the next audio frame.</p><p id="p-0065" num="0073">In the preceding description, various aspects of the method and imaging processing device according to the invention have been described with reference to the illustrative embodiment. For purposes of explanation, specific numbers, systems and configurations were set forth in order to provide a thorough understanding of the system and its workings. However, this description is not intended to be construed in a limiting sense. Various modifications and variations of the illustrative embodiment, as well as other embodiments of the method and image processing device, which are apparent to persons skilled in the art to which the disclosed subject matter pertains, are deemed to lie within the scope of the present invention.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005469A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230005469A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of training a speech detection and speech enhancement unit of a Multipoint Conferencing Node (MCN), comprising:<claim-text>collecting, for each of a plurality of acoustic environments, a plurality of audio samples consisting of noise, clean speech and noisy speech;</claim-text><claim-text>labeling each of the plurality of samples with an acoustic environmental label corresponding to one of the plurality of acoustic environments;</claim-text><claim-text>labeling the plurality of audio samples consisting of clean speech with clean speech label;</claim-text><claim-text>extracting Time-Frequency (T-F) domain features from the plurality of audio samples consisting of noise, clean speech and noisy speech;</claim-text><claim-text>training, for each of the plurality of acoustic environments, one speech detection classifier by inputting the T-F domain features of the plurality of audio samples consisting of noise, clean speech and noisy speech, the acoustic environmental labels and the clean speech labels to a first deep neural network; and<claim-text>training, for each of the plurality of environments, one statistical generative model representing the probability distributions of the T-F domain features of noisy speech, by inputting the T-F domain features of the plurality of audio samples comprising clean speech, the T-F domain features of the plurality audio samples comprising noise and the plurality of audio samples comprising noisy speech.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the plurality of acoustic environments comprises meeting room with video conferencing endpoint, home office, and public space.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the statistical generative model is a Gaussian Mixture Model.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. A method of speech detection and speech enhancement in a speech detection and speech enhancement unit of Multipoint Conferencing Node (MCN), comprising:<claim-text>receiving input audio segments from at least one videoconferencing participants;</claim-text><claim-text>determining an acoustic environment based on auxiliary information of the at least one videoconferencing participant;</claim-text><claim-text>extracting Time-Frequency (T-F) domain features from the received input audio segments;</claim-text><claim-text>determining if each of the received input audio segments is speech by inputting the T-F domain features into a speech detection classifier trained for the determined acoustic environment;</claim-text><claim-text>determining, when one of the received input audio segments is speech, if the received audio segment is noisy speech by inputting the T-F domain features into a noise classifier using a statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment; and</claim-text><claim-text>applying a noise reduction mask on the received input audio segments according to the determination of the received audio segment is noisy speech.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the auxiliary information of the at least one videoconferencing participant comprises at least one of a number of participants in a video image received from the at least one videoconferencing participant, and a specification of a videoconferencing endpoint received from the at least one videoconferencing participant.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the acoustic environment comprises meeting room with video conferencing endpoint, home office, and public space.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the speech detection and speech enhancement unit is trained according the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the noise classifier is a Bayesian classifier.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the noise reduction mask is a composite noise reduction mask.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the composite noise reduction mask is based on an estimated binary mask (EBM) generated using the Bayesian classifier.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further comprising updating the statistical generative model representing the probability distributions of the T-F domain features of noisy speech trained for the determined acoustic environment when the estimated probability of one the received input audio segments is noisy speech is close an estimated probability of the one received input audio segments is belonging clean speech.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the composite noise reduction mask is based on an estimated binary mask (EBM) generated using the Bayesian classifier.</claim-text></claim></claims></us-patent-application>