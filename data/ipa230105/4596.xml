<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004597A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004597</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941971</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>58</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>9032</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>62</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>5866</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>9032</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>768</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>63</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>10</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">CONTEXTUALLY DISAMBIGUATING QUERIES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16731786</doc-number><date>20191231</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11442983</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941971</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15463018</doc-number><date>20170320</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10565256</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16731786</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GOOGLE LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Badr</last-name><first-name>Ibrahim</first-name><address><city>Zurich</city><country>CH</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Grimsmo</last-name><first-name>Nils</first-name><address><city>Adliswil</city><country>CH</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Bakir</last-name><first-name>Gokhan H.</first-name><address><city>Zurich</city><country>CH</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Anikiej</last-name><first-name>Kamil</first-name><address><city>Lachen</city><country>CH</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Kumar</last-name><first-name>Aayush</first-name><address><city>Zurich</city><country>CH</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Kuznetsov</last-name><first-name>Viacheslav</first-name><address><city>R&#xfc;schlikon</city><country>IN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for contextually disambiguating queries are disclosed. In an aspect, a method includes receiving an image being presented on a display of a computing device and a transcription of an utterance spoken by a user of the computing device, identifying a particular sub-image that is included in the image, and based on performing image recognition on the particular sub-image, determining one or more first labels that indicate a context of the particular sub-image. The method also includes, based on performing text recognition on a portion of the image other than the particular sub-image, determining one or more second labels that indicate the context of the particular sub-image, based on the transcription, the first labels, and the second labels, generating a search query, and providing, for output, the search query.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="227.50mm" wi="158.75mm" file="US20230004597A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="234.53mm" wi="164.25mm" file="US20230004597A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="207.43mm" wi="175.94mm" orientation="landscape" file="US20230004597A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="206.67mm" wi="90.68mm" file="US20230004597A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="111.68mm" wi="89.58mm" file="US20230004597A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="116.92mm" wi="89.58mm" file="US20230004597A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="206.84mm" wi="155.70mm" orientation="landscape" file="US20230004597A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">In general, a search query includes one or more terms that are submitted to a search engine upon request of the execution of a search. For example, a user may enter query terms of a search query by typing on a keyboard, or in the instance of a voice query, by speaking the query terms into a microphone of a computing device. Voice queries may be processed using speech recognition technology.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0003" num="0002">In some implementations, an image corresponding to a portion of a display of a computing device may be analyzed to aid a query processing system in answering a natural language query. For example, a user may ask a question about a photograph that the user is viewing on the computing device, such as &#x201c;What is this?&#x201d; The computing device may detect the user's utterance and capture a respective image of the computing device that the user is viewing. The computing device processes the utterance to generate a transcription of the utterance spoken by the user of the computing device. The computing device transmits the transcription and the image to a server.</p><p id="p-0004" num="0003">The server receives the transcription and the image from the computing device. The server can identify visual and textual content in the image. The server generates labels for the image that correspond to content of the image, such as locations, entities, names, types of animals, etc. The server can identify a particular sub-image in the image. The particular sub-image may be a photograph or drawing. In some aspects, the server identifies a portion of the particular sub-image that is likely of primary interest to the user, such as a historical landmark in the image. The server can perform image recognition on the particular sub-image to generate labels for the particular sub-image. The server can also generate labels for textual content in the image, such as comments that correspond to the particular sub-image, by performing text recognition on a portion of the image other than the particular sub-image. The server can generate a search query based on the received transcription and the generated labels. Further, the server may be configured to provide the search query for output to a search engine.</p><p id="p-0005" num="0004">One innovative aspect of the subject matter described in this specification is embodied in methods that include the actions of receiving an image being presented on, or corresponding to, at least a portion of a display of a computing device, and receiving a transcription of, or that corresponds to, an utterance spoken by a user of the computing device, typically at the same time as the image is being presented, identifying a particular sub-image that is included in the image, and based on performing image recognition on the particular sub-image, determining one or more first labels that indicate a context of the particular sub-image. The method also includes, based on performing text recognition on a portion of the image other than the particular sub-image, determining one or more second labels that indicate the context of the particular sub-image, based on the transcription, the first labels, and the second labels, generating a search query, and providing, for output, the search query.</p><p id="p-0006" num="0005">Such method steps, or other combinations of steps as described herein, may be carried out automatically and without further user intervention, for example in response to an automatic determination by the computing device that the method should be carried out at a particular time, or following a particular button press, spoken command or other indication from a user of the computing device that such a method is to be carried out. The methods described here may therefore provide a more efficient user interface to the user device by reducing the input required of a user to achieve desired or desirable search query generation.</p><p id="p-0007" num="0006">Other implementations of this and other aspects include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices.</p><p id="p-0008" num="0007">Implementations may each optionally include one or more of the following features. For instance, the methods can include weighting the first label differently than the second labels. The methods can also include generating the search query by substituting one or more of the first labels or the second labels for terms of the transcription. In some aspects, the methods include generating, for each of the first labels and the second labels a label confidence score that indicates a likelihood that the label corresponds to a portion of the particular sub-image that is of primary interest to the user, and selecting one or more of the first labels and second labels based on the respective label confidence scores, wherein the search query is generated based on the one or more selected first labels and second labels. Further, the methods can include accessing historical query data that includes previous search queries provided by other users, generating, based on the transcription, the first labels, and the second labels, one or more candidate search queries, comparing the historical query data to the one or more candidate search queries, and based on comparing the historical query data to the one or more candidate search queries, selecting the search query from among the one or more candidate search queries.</p><p id="p-0009" num="0008">The methods may include generating, based on the transcription, the first labels, and the second labels, one or more candidate search queries, determining, for each of the one or more candidate search queries, a query confidence score that indicates a likelihood that the candidate search query is an accurate rewrite of the transcription, and selecting, based on the query confidence scores, a particular candidate search query as the search query. Additionally, the methods can include identifying one or more images that are included in the image, generating for each of the one or more images that are included in the image, an image confidence score that indicates a likelihood that an image is an image of primary interest to the user, and based on the image confidence scores for the one or more images, selecting the particular sub-image. The methods can include receiving data indicating a selection of a control event at the computing device, wherein the control event identifies the particular sub-image. In some aspects, the computing device is configured to capture the image and capture audio data that corresponds to the utterance in response to detecting a predefined hotword.</p><p id="p-0010" num="0009">Further, the methods may include receiving an additional image of the computing device and an additional transcription of an additional utterance spoken by a user of the computing device, identifying an additional particular sub-image that is included in the additional image, based on performing image recognition on the additional particular sub-image, determining one or more additional first labels that indicate a context of the additional particular sub-image, based on performing text recognition on a portion of the additional image other than the additional particular sub-image, determining one or more additional second labels that indicate the context of the additional particular sub-image, based on the additional transcription, the additional first labels, and the additional second labels, generating a command, and performing the command. In this instance, performing the command can include performing one or more of storing the additional image in memory, storing the particular sub-image in the memory, uploading the additional image to a server, uploading the particular sub-image to the server, importing the additional image to an application of the computing device, and importing the particular sub-image to the application of the computing device. In certain aspects, the methods can include identifying metadata associated with the particular sub-image, wherein determining the one or more first labels that indicate the context of the particular sub-image is based further on the metadata associated with the particular sub-image.</p><p id="p-0011" num="0010">Advantageous implementations can include one or more of the following features. The methods can determine context of an image corresponding to a portion of a display of a computing device to aid in the processing of natural language queries. The context of the image may be determined through image and/or text recognition. Specifically, the context of the image may be used to rewrite a transcription of an utterance of a user. The methods may generate labels that refer to the context of the image, and substitute the labels for portions of the transcription. For example, a user may be viewing a photograph on a computing device and ask &#x201c;Where was this taken?&#x201d; The methods may determine that the user is referring to the photo on the screen of the computing device. The methods can extract information about the photo to determine a context of the photo, as well as a context of other portions of the image that do not include the photo. In this instance, the context information may be used to determine a location that the photo was taken. As such, the methods may use images corresponding to displays of computing devices to aid in the generation of search queries.</p><p id="p-0012" num="0011">In some aspects, the methods may identify a particular sub-image in the image that is a primary focus of the user. The methods may generate labels that correspond to the particular sub-image, and weight labels corresponding to the particular sub-image differently than other labels so that the context of the image may be more effectively determined. The methods may weight of labels based on a prominence of the particular sub-image in the image, a frequency that the particular sub-image labels appear in historical search queries, a frequency that the particular sub-image labels appear in recent search queries, etc. Therefore, the methods may identify primary points of user interest in the image to determine context of the image as a whole.</p><p id="p-0013" num="0012">The details of one or more embodiments of the invention are set forth in the accompanying drawings and the description below. Other features and advantages of the invention will become apparent from the description, the drawings, and the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of an example environment for contextually disambiguating a query.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an example system for contextually disambiguating a query.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart illustrating an example process for contextually disambiguating a query.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart illustrating an example process for selecting a particular sub-image using confidence scores.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart illustrating an example process for generating a search query using selected labels.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram of an example computing device and an example mobile computing device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0020" num="0019">Like reference numbers and designations in the various drawings indicate like elements.</p><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram of an example environment <b>100</b> for contextually disambiguating a query. The environment <b>100</b> includes a user <b>102</b> and a computing device <b>104</b>. In the environment <b>100</b>, the user <b>102</b> provides an utterance <b>103</b>, such as a query, to the computing device <b>104</b>. The user <b>102</b> may ask a question about one or more objects displayed on a graphical display of the computing device <b>104</b>. For example, the utterance <b>103</b> may include a query such as &#x201c;What is that?&#x201d; In this instance, the user <b>102</b> may be referencing objects, such as an image, text, video, or any combination thereof, that are displayed on the graphical display of the computing device <b>104</b>. The computing device <b>104</b> may include one or more computing devices such as a laptop, desktop, smartphone, tablet, or any other computing device that is known.</p><p id="p-0022" num="0021">The utterance <b>103</b> of the user <b>102</b> may be contextually ambiguous. In this instance, the utterance <b>103</b> may not directly reference the content being displayed at the computing device <b>102</b> by name. However, a context of the displayed objects may be determined and the context may be used in combination with a transcription corresponding to the utterance <b>103</b> to disambiguate the query.</p><p id="p-0023" num="0022">The computing device <b>104</b> may be configured to capture an image <b>106</b> being presented on a display of the computing device <b>104</b> when the utterance <b>103</b> of the user <b>102</b> is received. For example, the computing device <b>104</b> may capture a portion of the display that includes a photograph <b>108</b> and comments <b>116</b> that correspond to the photograph, but does not include a logo icon <b>120</b> such as an application title that the computing device <b>104</b> is running. In some examples, the image <b>106</b> corresponds to a screenshot of the computing device <b>104</b>. Alternatively, or additionally, the computing device <b>104</b> may persistently capture the displayed content and transmit particular sub-images upon detection of the utterance <b>103</b>. Further, the image <b>106</b> may be captured upon detection of a predefined hotword in the utterance <b>103</b>. The computing device <b>104</b> may transcribe the utterance <b>103</b>. In some implementations, the computing device <b>104</b> may transmit audio data corresponding to the utterance <b>103</b> to a speech recognition engine and receive a transcription of the utterance <b>103</b> from the speech recognition engine.</p><p id="p-0024" num="0023">The transcription corresponding to the utterance <b>103</b> and the image <b>106</b> may be transmitted to a server over a network for processing (e.g., disambiguation of the utterance). The server may be configured to determine a context of the image <b>106</b> by analyzing the image <b>106</b>. The server may determine the context of the image <b>106</b> by identifying and analyzing images or photographs in the image. For example, photograph <b>108</b> may be analyzed to identify that the photograph <b>108</b> includes one or more entities in the photograph <b>108</b>. Referring to the example environment <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the photograph <b>108</b> may be identified by the server and then analyzed to determine that the photograph <b>108</b> includes entities such as the Eiffel Tower <b>110</b> and a dog <b>112</b> in front of the Eiffel Tower <b>110</b>.</p><p id="p-0025" num="0024">In some examples, the server performs image recognition on the particular sub-image <b>108</b>. The image recognition is performed to determine one or more first labels that indicate a context of the particular sub-image. For example, the server may perform image recognition on the photograph <b>108</b> and determine first labels that correspond to the photograph <b>108</b>, such as Eiffel Tower, France, Paris, and dog. The image recognition can include a determination of entities in focus in the photograph <b>108</b>, entities in the foreground and background of the photograph <b>108</b>, relative sizes of entities in the photograph <b>108</b>, and the like. In some examples, the server may identify metadata associated with the particular sub-image, or the photograph <b>108</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The server can use the metadata to determine the first labels that correspond to the particular sub-image.</p><p id="p-0026" num="0025">Additionally, the server can perform text recognition on the image <b>106</b>. The server may perform text recognition on a portion of the image <b>106</b> other than the photograph <b>108</b>. The portion of the image <b>106</b> may include a title <b>114</b> of the photograph <b>108</b> and/or comments <b>116</b> that refer to the photograph <b>108</b>. For example, image <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> includes a title <b>114</b> indicating a location that the photograph <b>108</b> was taken, such as Paris, France. The image <b>106</b> also includes comments <b>116</b> that refer to the photograph <b>108</b> such as &#x201c;Dave&#x2dc;So cool, France is my favorite.&#x201d; &#x201c;Sarah&#x2dc;Didn't know you had a golden, I have one too!&#x201d; and &#x201c;Abby&#x2dc;I was just in Paris, when were you there?&#x201d;</p><p id="p-0027" num="0026">The title <b>114</b> and the comments <b>116</b> of the image <b>106</b> may be processed by the server via text recognition. By performing text recognition, the server can determine one or more second labels that further indicate the context of the particular sub-image. For example, the server may perform text recognition on the title <b>114</b> to verify that the location of the particular sub-image is Paris, France. Further, the server may perform text recognition on the comments <b>116</b> to verify that the location of the particular sub-image is Paris, France, (e.g., by performing text recognition on the phrase &#x201c;I was just in Paris.&#x201d;) Additionally, the server may perform text recognition on the comments <b>116</b> to determine that the dog <b>112</b> in the photograph <b>108</b> is a golden retriever, (e.g., by performing text recognition on the phrase &#x201c;Didn't know you had a golden . . . &#x201d;) As such, the server may generate one or more second labels such as Paris, France, and golden retriever.</p><p id="p-0028" num="0027">The server can be configured to generate a search query based on the received transcription, the first labels, and the second labels. The server may generate the search query automatically without further user intervention. For example, in response to automatically determining by the computing device <b>104</b> that the method should be carried out at a particular time, following a particular button press that precedes the utterance, following a spoken command/hotword included in the utterance, or any other indication from the user <b>102</b> of the computing device <b>104</b> that such a method is to be carried out before the transcription and the image is received by the server.</p><p id="p-0029" num="0028">The search query may be generated by rewriting the transcription. In some aspects, the transcription may be rewritten by substituting one or more of the first and/or second labels into the transcription. For example, the transcription may include &#x201c;What is that?&#x201d; In this instance, the phrase &#x201c;the Eiffel Tower&#x201d; may be substituted for the term &#x201c;that&#x201d; in the transcription. Therefore, the search query may be rewritten to include the following, &#x201c;What is the Eiffel Tower?&#x201d;</p><p id="p-0030" num="0029">In some aspects, the server is configured to generate a label confidence score for each of the first and second labels. In this instance, the label confidence scores may indicate a relative likelihood that each label corresponds to a portion of the particular sub-image that is of primary interest to the user <b>102</b>. For example, a first label may include &#x201c;Eiffel Tower&#x201d; with a confidence score of 0.8, and a second label may include &#x201c;golden retriever&#x201d; with a confidence score of 0.5. In this instance, the confidence scores may indicate that the first label corresponds to an entity that is more likely to be of primary interest to the user <b>102</b> based on the greater, respective label confidence score.</p><p id="p-0031" num="0030">Labels may be selected to generate the search query based on the confidence scores. For example, a certain number of labels with the highest confidence score may be selected to generate to search query in combination with the transcription. In another example, all labels that satisfy a particular label confidence score threshold may be used in combination with the transcription to generate the search query. In another example, the server may generate label confidence scores based on a frequency of the labels appearing in recent search queries, a frequency of the labels appearing in all historical search queries, and so on.</p><p id="p-0032" num="0031">The server can be configured to access historical search query data. The historical query data may include a number of previous search queries provided by the user <b>102</b> and/or other users. The server can generate one or more candidate search queries based on the transcription, the first labels, and the second labels, and compare the historical query data to the candidate search queries. Based on comparing the historical query data to the one or more candidate search queries, the server may select a particular candidate search query as the search query. For example, the server may select the particular candidate search query based on a comparison between a frequency of the candidate search queries appearing in recent search queries, such as queries input by the user, and/or a frequency of the candidate search queries appearing in historical search queries, such as queries entered by all users into a search engine.</p><p id="p-0033" num="0032">The server can be configured to provide the generated search query for output. For example, the server can be configured to provide the generated search query to a search engine. In another example, the server may generate the search query and transmit the search query to the computing device <b>102</b>. In this instance, the search query may be provided to the user <b>102</b> audially or visually by the computing device <b>104</b> to verify that the server has accurately rewritten the query.</p><p id="p-0034" num="0033">The server can further be configured to provide the generated search query for output and/or a search result to the computing device <b>104</b>. In this instance, the computing device <b>104</b> can be configured to receive the search query and provide a search result that corresponds to the search query for output <b>122</b>, such as &#x201c;You are looking at a photograph of the Eiffel Tower.&#x201d;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram of an example system <b>200</b> for contextually disambiguating a query. The system <b>200</b> includes the user <b>102</b>, the computing device <b>104</b>, a server <b>206</b>, an image recognition engine <b>208</b>, and a text recognition engine <b>210</b>. The computing device <b>104</b> is in communication with the server <b>206</b> over one or more networks. The computing device <b>104</b> can include a microphone or other detection mechanisms for detecting utterances of the user <b>102</b>.</p><p id="p-0036" num="0035">In an example, the user <b>102</b> may provide an utterance to the computing device <b>104</b>. The utterance may be detected and transcribed by the computing device <b>102</b>. As such, the computing device <b>104</b> may generate a transcription <b>204</b> that corresponds to the utterance of the user <b>102</b>. The computing device <b>104</b> may also be configured to capture an image <b>202</b> of a graphical display of the computing device <b>104</b>. The computing device <b>104</b> may capture the image <b>202</b> upon detecting the utterance of the user <b>102</b> or upon transcribing the utterance. Additionally, or alternatively, the computing device <b>104</b> can be configured to persistently capture the displayed contents of the computing device <b>104</b>. In this instance, a particular sub-image may be transmitted with the transcription <b>204</b> to the server <b>206</b> upon detection of the utterance.</p><p id="p-0037" num="0036">In another example, the computing device <b>104</b> can be configured to transmit the utterance of the user <b>102</b> to the server <b>206</b>. For example, the computing device <b>104</b> can be configured to detect a predefined hotword in the utterance, and upon detection of the hotword, transmit the utterance to the server <b>206</b>. In this instance, the server <b>206</b> is configured to generate a transcription corresponding to the utterance.</p><p id="p-0038" num="0037">At event (A), the server <b>206</b> receives the transcription <b>204</b> and the image <b>202</b> from the computing device <b>104</b>. The computing device <b>104</b> may transmit the transcription <b>204</b> and the image <b>202</b> automatically to the server <b>206</b>. The computing device <b>104</b> may also transmit the transcription <b>204</b> and the image <b>202</b> upon user input. For example, the user may provide the utterance as well as touch input at the graphical display of the computing device <b>104</b> indicating that the user requests a transcription corresponding to the utterance and the image to be sent to the server <b>206</b>.</p><p id="p-0039" num="0038">At event (B), the server <b>206</b> identifies a particular sub-image <b>207</b> of the image <b>202</b> and transmits the particular sub-image <b>207</b> to an image recognition engine <b>208</b>. In some aspects, the server <b>206</b> is in communication with the image recognition engine <b>208</b> over the network. In other aspects, the server <b>206</b> and the image recognition engine <b>208</b> are integrated into a single system.</p><p id="p-0040" num="0039">In some examples, the image <b>202</b> may include multiple images. The server <b>206</b> can analyze the multiple images to determine the particular sub-image <b>207</b> that is likely of interest to the user <b>102</b>. Additionally, the server <b>206</b> may receive user input indicating the particular sub-image <b>207</b> of the images in the image <b>202</b> is of primary interest to the user <b>102</b>. The server <b>206</b> may generate an image confidence score for each of the multiple images in the image <b>202</b>. The image confidence score can indicate a relative likelihood that an image is an image of primary interest to the user <b>102</b>. The server <b>206</b> may determine the particular sub-image <b>207</b>, or the image of primary interest to the user <b>102</b>, based on the generated confidence scores. For example, the server <b>206</b> may identify that the display of the computing device <b>104</b> includes a first portion and a second portion. The first portion may include a photograph and the second portion may include a logo image that corresponds to a title of the application that computing device is using. The server may be configured to generate a confidence score of 0.9 for the first portion and a confidence score of 0.3 for the second portion. In this instance, the server <b>206</b> determines that the first portion is likely to be of primary interest to the user <b>102</b> based on the generated confidence scores.</p><p id="p-0041" num="0040">The server may be configured to determine the particular sub-image <b>207</b> based on receiving data indicating a selection of a control event. The control event may correspond to the user <b>102</b> providing input at the computing device <b>104</b>. Specifically, the control event may correspond to the user <b>102</b> interacting with the display of the computing device <b>104</b>. For example, the user <b>102</b> may interact with a portion of the display that corresponds to the particular sub-image <b>207</b>. The server <b>206</b> may receive data indicating that the user <b>102</b> interacted with a portion of the display that corresponds to the particular sub-image <b>207</b>, and therefore may determine that the portion of the display corresponds to the particular sub-image <b>207</b>.</p><p id="p-0042" num="0041">At event (C), the image recognition engine <b>208</b> performs image recognition on the particular sub-image <b>207</b>. The image recognition engine <b>208</b> performs image recognition to generate labels <b>209</b> for the particular sub-image <b>207</b> that indicate a context of the particular sub-image. The labels <b>209</b> may correspond to entities in the particular sub-image <b>207</b>, such as trees or a dog. The labels <b>209</b> can also correspond to entities including specific locations or landmarks, such as the Eiffel Tower. The labels <b>209</b> may be used individually or in combination to determine a context of the particular sub-image <b>207</b>.</p><p id="p-0043" num="0042">The image recognition engine <b>208</b> may be configured to determine a portion of the particular sub-image <b>207</b> that is of primary focus of the user <b>102</b>. For example, the image recognition engine <b>208</b> can analyze the particular sub-image <b>207</b> to determine that the particular sub-image <b>207</b> includes entities such as the Eiffel Tower and a dog. The image recognition engine <b>208</b> can analyze the entities in the particular sub-image <b>207</b> and determine that the Eiffel Tower is greater in size than the dog. Based on the determination that the Eiffel Tower is proportionally greater in size to the dog, the image recognition engine <b>208</b> may determine that the Eiffel Tower <b>110</b> is likely of primary interest to the user <b>102</b>. Additionally, or alternatively, the image recognition engine <b>208</b> may be configured to analyze other aspects of the particular sub-image <b>207</b> such as foreground vs. background, entities in focus of the particular sub-image <b>207</b>, and the like. For example, the image recognition engine <b>208</b> may determine that the Eiffel Tower is in focus in the particular sub-image <b>207</b> and that the dog is out of focus. As such, the image recognition engine <b>208</b> can determine that the Eiffel Tower is likely of primary interest to the user <b>102</b>.</p><p id="p-0044" num="0043">At event (D), the server <b>206</b> identifies one or more portions <b>211</b> of the image <b>202</b> that do not include the particular sub-image. The one or more portions <b>211</b> are transmitted to the text recognition engine <b>210</b>. In some aspects, the server <b>206</b> is in communication with the text recognition engine <b>210</b> over the network. In other aspects, the server <b>206</b> and the text recognition engine <b>210</b> are integrated into a single system. Further, the server <b>206</b>, the image recognition engine <b>208</b>, and the text recognition engine <b>210</b> may be integrated into a single system. In some examples, the one or more portions <b>211</b> may include a title included in the image <b>202</b>, comments included in the image <b>202</b>, or any content in the image <b>202</b> that does not include the particular sub-image <b>207</b>.</p><p id="p-0045" num="0044">At event (E), the text recognition engine <b>210</b> performs text recognition on the one or more portions <b>211</b> of the image <b>202</b> that do not include the particular sub-image <b>207</b>. The text recognition engine <b>210</b> performs text recognition to generate labels <b>212</b> for the one or more portions <b>211</b> that indicate a context of the particular sub-image <b>207</b>. For example, the portions <b>211</b> may include comments such as &#x201c;Dave&#x2dc;So cool, France is my favorite.&#x201d; &#x201c;Sarah&#x2dc;Didn't know you had a golden, I have one too!&#x201d; and &#x201c;Abby&#x2dc;I was just in Paris, when were you there?&#x201d; The labels <b>212</b> may directly correspond to text in the one or more portions <b>211</b>. In this instance, the labels <b>212</b> may include terms such as &#x201c;France&#x201d; or &#x201c;Paris.&#x201d; The labels <b>212</b> can be inferred from the text in the one or more portions <b>211</b>. In this instance, the labels <b>212</b> may be inferred to include the phrase &#x201c;golden retriever.&#x201d; The labels <b>212</b> may be used individually or in combination to determine a context of the particular sub-image <b>207</b>.</p><p id="p-0046" num="0045">By performing text recognition, the text recognition engine <b>210</b> can determine one or more labels <b>212</b> that further indicate the context of the particular sub-image <b>207</b>. For example, the text recognition engine <b>210</b> may perform text recognition on the comments <b>116</b> to verify that the location of the particular sub-image <b>207</b> is Paris, France, (e.g., by performing text recognition on the phrase &#x201c;I was just in Paris.&#x201d;) Additionally, the text recognition engine <b>210</b> may perform text recognition on the comments to determine that the dog in the particular sub-image <b>207</b> is a golden retriever, (e.g., by performing text recognition on the phrase &#x201c;Didn't know you had a golden . . . &#x201d;) As such, the text recognition engine <b>210</b> may generate one or more labels <b>212</b> such as Paris, France, and golden retriever.</p><p id="p-0047" num="0046">At event (F), the server <b>206</b> generates a search query <b>213</b> using the transcription <b>204</b>, the labels <b>209</b> from the image recognition engine <b>208</b>, and the labels <b>212</b> from the text recognition engine <b>210</b>. The server <b>206</b> may generate the search <b>213</b> query automatically without further user intervention. For example, in response to automatically determining by the computing device <b>104</b> that the method should be carried out at a particular time, following a particular button press that precedes the utterance, following a spoken command/hotword included in the utterance, or any other indication from the user <b>102</b> of the computing device <b>104</b> that such a method is to be carried out before the transcription <b>204</b> and the image <b>202</b> is received by the server <b>206</b>.</p><p id="p-0048" num="0047">The server <b>206</b> may rewrite the transcription <b>204</b> as the search query <b>213</b>. The server <b>206</b> may substitute a subset of the labels of the image recognition engine <b>209</b> and the text recognition engine <b>212</b> into the transcription <b>204</b> to generate the search query <b>213</b>. For example, the server <b>206</b> may substitute the label of &#x201c;Eiffel Tower&#x201d; into the transcription <b>204</b> so that the generated search query <b>213</b> includes &#x201c;What is the Eiffel Tower?&#x201d;</p><p id="p-0049" num="0048">Further, at event (F) the server <b>206</b> provides the generated search query <b>213</b> for output. For example, the server <b>206</b> may provide the search query <b>213</b> to a search engine. The server <b>206</b> may receive search results from the search engine and provide the search results to the computing device <b>104</b> over the network. In some aspects, the computing device <b>104</b> may receive the search results and provide the search results as audio or visual output. For example, the server <b>206</b> may generate the search query <b>213</b> &#x201c;What is the Eiffel Tower?&#x201d; and provide the generated search query <b>213</b> to the computing device <b>104</b>. In this instance, the computing device <b>104</b> may be configured to audially output the generated search query <b>213</b> to the user <b>102</b> for verification before inputting the search query <b>213</b> to a search engine.</p><p id="p-0050" num="0049">In some examples, the server <b>206</b> generates the search query <b>213</b> according to generated weightings of the labels <b>209</b> and <b>212</b>. In this instance, the server <b>206</b> may generate a first weight for the image labels <b>209</b> that differs from a second weight for the textual labels <b>212</b>. For example, the server <b>206</b> may determine that the image labels <b>209</b> are more relevant to the transcription <b>204</b> than the textual labels <b>212</b>. As such, the server <b>206</b> may place greater emphasis on the image labels <b>209</b>, by weighting the image labels <b>209</b> more than the textual labels <b>212</b>.</p><p id="p-0051" num="0050">The server <b>206</b> may be configured to receive an additional image of the computing device <b>104</b> and an additional transcription of an additional utterance spoken by a user of the computing device <b>104</b>. The server <b>206</b> may identify an additional particular sub-image that is included in the additional image and transmit the additional particular sub-image to the image recognition engine <b>208</b> to perform image recognition on the additional particular sub-image. The image recognition engine <b>208</b> can be configured to generate one or more additional first labels for the additional particular sub-image that indicate a context of the additional particular sub-image. Likewise, the server can be configured transmit a portion of the additional image that does not include the additional particular sub-image to the text recognition engine <b>210</b> to generate one or more additional second labels based on performing text recognition on the portion of the additional image other than the additional particular sub-image.</p><p id="p-0052" num="0051">The server <b>206</b> may use the additional transcription, the additional first labels, and the additional second labels to generate a command or action. The command may be automatically performed by the server <b>206</b>, provided to the computing device <b>104</b>, and the like. In some examples, the command may include one or more actions such as storing the additional image in memory, storing the additional particular sub-image in the memory, uploading the additional image to the server <b>206</b>, uploading the additional particular sub-image to the server <b>206</b>, importing the additional image to an application of the computing device <b>104</b>, and importing the particular sub-image to the application of the computing device <b>104</b>. For example, the user <b>102</b> may be viewing visual and textual content in a notes application on the display of the computing device <b>104</b>. Using the received transcription and the generated labels, the server <b>206</b> can be configured to capture a portion of an image in the notes application and upload the portion of the image to the cloud for storage.</p><p id="p-0053" num="0052">In certain aspects, the server <b>206</b> provides the search query <b>213</b> to the computing device <b>104</b>. In this instance, the computing device <b>104</b> may provide the search query <b>213</b> for verification by the user <b>102</b>, before providing the search query <b>213</b> as input to a search engine. As such, the search query <b>213</b> may be accepted, modified, or declined by the user <b>102</b>. For example, in response to receiving the search query <b>213</b> at the computing device <b>104</b>, the user <b>102</b> may provide user input indicating that the search query <b>213</b> is to be provided to a search engine. In another example, the user <b>102</b> may provide user input indicating that the search query <b>213</b> is to be modified before being provided to the search engine. As such, the user may directly modify the search query <b>213</b>, or ask for another search query from the server <b>206</b>. In another example, the user <b>102</b> may provide user input indicating that the search query <b>213</b> is declined. As such, the user <b>102</b> may ask for another search query from the server <b>206</b>, or provide another utterance to be used in the generation of another search query.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart illustrating an example process <b>300</b> for contextually disambiguating a query. The process <b>300</b> can be performed by one or more servers or other computing devices. For example, operations of the process <b>300</b> can be performed by server <b>206</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Operations of process <b>300</b> can also be implemented as instructions stored on a non-transitory computer-readable medium, and when the instructions are executed by one or more servers (or other computing devices) the instructions cause the one or more servers to perform operations of the process <b>300</b>.</p><p id="p-0055" num="0054">At step <b>310</b>, the server receives an image and a transcription of an utterance. The image may correspond to a graphical display of a computing device in communication with the server. For example, the computing device may capture the image upon receiving the utterance. In some aspects, the image may correspond to a graphical display of the computing device when the computing device is in a camera mode. As such, the image may correspond to a photograph that the computing device captures, or is viewing by a camera in communication with the computing device. Further, the image may correspond to a video that is captured by the camera of the computing device or a video displayed at a display of the computing device. Additionally, or alternatively, the computing device may transmit background noise captured while receiving the utterance. In this instance, the server may use the background noise to generate additional labels and/or to score the generated labels.</p><p id="p-0056" num="0055">The transcription may correspond to an utterance received by the computing device. In some aspects, the transcription is generated by the computing device based on the received utterance. In other aspects, the transcription corresponds to user input received by the computing device. For example, a user may input a question via a keyboard or user interface of the computing device. The computing device may generate the transcription based on the input and provide the transcription to the server.</p><p id="p-0057" num="0056">At step <b>320</b>, the server identifies a particular sub-image included in the image. The server is configured to identify a particular sub-image out of one or more images in the image. The particular sub-image may be an image that is likely to be of primary focus or interest of a user. For example, the image may include a photograph as well as multiple other graphical icons. The server can be configured to analyze the image in order to determine that the photograph is of primary interest to the user, while the other graphical icons in the display are not of primary interest to the user.</p><p id="p-0058" num="0057">At step <b>330</b>, the server determines one or more first labels based on image recognition of the particular sub-image. The server may perform image recognition on the particular sub-image in the image to identify one or more entities in the particular sub-image, and generate respective labels for the one or more entities. Each of the one or more entities may correspond to one or more respective, first labels. The first labels may be determined in part using metadata associated with the particular sub-image. The first labels may indicate a context of the particular sub-image. In certain aspects, the server is configured to perform image recognition over the entire image. In this instance, the first labels may be generated for all entities identified by image recognition in the image.</p><p id="p-0059" num="0058">At step <b>340</b>, the server determines one or more second labels based on text recognition on a portion of the image other than the particular sub-image. The server may perform text recognition on the portion of the image other than the particular sub-image to identify textual content in the image to use in the generation of labels that indicate a context of the content. The textual content may be labeled using one or more second labels that indicate a context of the particular content.</p><p id="p-0060" num="0059">The second labels may be determined in part using metadata associated with the portion. For example, the server may be configured to access and capture code relating to displaying content on the display of the computing device. In this instance, the server may access markup code and capture the markup code to analyze for metadata that may be used in the generation of the second labels. In some aspects, the server is configured to perform text recognition over the entire image. In this instance, the second labels may be generated for all textual content identified by text recognition in the image.</p><p id="p-0061" num="0060">At step <b>350</b>, the server generates a search query based on the transcription, the first labels, and the second labels. Specifically, the server is configured to generate the search query based on the transcription, and the labels. In some examples, the server is configured to generate multiple candidate search queries based on the transcription and the labels. The candidate search queries may be ranked based on historical query data. As such, a top ranked candidate search query may be selected as the search query.</p><p id="p-0062" num="0061">At step <b>360</b>, the server provides the search query for output. The selected search query may be provided to a search engine directly. In this instance, the server may also be configured to receive one or more search results from the search engine and provide the search results for output. For example, the server may provide the search query to the search engine, select a particular search result, and provide the search result to the computing device for audial or visual output.</p><p id="p-0063" num="0062">In other aspects, the search query may be provided to the computing device. The computing device may provide the search query for audial or visual output. In this instance, the search query may be verified by a user before being provided as input to a search engine.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flow chart illustrating an example process <b>400</b> for selecting a particular sub-image using confidence scores. The process <b>400</b> can be performed by one or more servers or other computing devices. For example, operations of the process <b>400</b> can be performed by server <b>206</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Operations of process <b>400</b> can also be implemented as instructions stored on a non-transitory computer-readable medium, and when the instructions are executed by one or more servers (or other computing devices) the instructions cause the one or more servers to perform operations of the process <b>400</b>.</p><p id="p-0065" num="0064">At step <b>410</b>, the server identifies images included in an image. In certain aspects, the server receives an image from a computing device and identifies a plurality images in the image. The server may be configured to perform image recognition on the image to identify the plurality of images. The images can include photographs, icons, drawings, pictures, and the like. The images can vary in size, shape, and type. In some aspects, the images correspond to a still frame of a video. For example, the image may be of a webpage that includes multiple images and a video that is playing in the background. The image may correspond to a single, captured frame of the video playing on the web page.</p><p id="p-0066" num="0065">At step <b>420</b>, the server generates a confidence score for each of the identified images. The confidence scores may each indicate a likelihood that an image is an image of primary interest to a user viewing the image. The confidence scores can be determined based on various features of the images. For example, the server may generate greater confidence scores for large images over small images in the image. In another example, the server may generate greater confidence scores for images with a large number of identifiable entities in the image, such as landmarks, people, or animals, and vice-versa.</p><p id="p-0067" num="0066">At step <b>430</b>, the server selects a particular sub-image based on the confidence scores. The server can be configured to select the particular sub-image based on the highest confidence score. As such, the confidence scores of the images may be compared to determine which image is associated with the greatest confidence score. In some examples, the server selects multiple images. In this instance, the server can be configured to select images if each of the selected images satisfy a predetermined image confidence score threshold. This may be the case when multiple images in the image include similar entities or objects. For example, two images in an image may include the Eiffel Tower and a third image may not include the Eiffel Tower. As such, the two respective images that include the Eiffel Tower may be selected as the particular sub-images due to the similar content in each of the two images.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow chart illustrating an example process <b>500</b> for generating a search query using selected labels. The process <b>500</b> can be performed by one or more servers or other computing devices. For example, operations of the process <b>500</b> can be performed by server <b>206</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Operations of process <b>500</b> can also be implemented as instructions stored on a non-transitory computer-readable medium, and when the instructions are executed by one or more servers (or other computing devices) the instructions cause the one or more servers to perform operations of the process <b>500</b>.</p><p id="p-0069" num="0068">At step <b>510</b>, the server generates a confidence score for each of first labels and second labels. The first labels may correspond to a particular sub-image identified in an image and the second labels may correspond to a portion of the image other than the particular sub-image. For example, the particular sub-image may be a photograph of the Eiffel Tower in the image and the portion of the image other than the particular sub-image may include comments about the photograph. The confidence scores for the first and second labels each indicate a likelihood that the respective label corresponds to a portion of the particular sub-image that is of primary interest to the user.</p><p id="p-0070" num="0069">At step <b>520</b>, the server selects one or more of the first labels and the second labels based on the confidence scores. For example, the server may select a single label with the greatest confidence score. In another example, the server is configured to select labels with confidence scores that satisfy a predetermined confidence score threshold. In another example, the server is configured to select a predetermined number of labels with the greatest confidence scores.</p><p id="p-0071" num="0070">At step <b>530</b>, the server generates a search query using a received transcription, the selected first labels, and the selected second labels. The server can be configured to provide the generated search query for output. For example, the server can be configured to provide the generated search query to a search engine. In another example, the server may generate the search query and transmit the search query to a computing device. In this instance, the search query may be provided to a user audially or visually by the computing device.</p><p id="p-0072" num="0071"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a diagram of an example computing device <b>600</b> and an example mobile computing device <b>650</b>, which may be used with the techniques described herein. Computing device <b>600</b> is intended to represent various forms of digital computers, such as laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Mobile computing device <b>650</b> is intended to represent various forms of mobile devices, such as personal digital assistants, cellular telephones, smartphones, and other similar computing devices. The components shown here, their connections and relationships, and their functions, are meant to be exemplary only, and are not meant to limit implementations of the inventions described and/or claimed in this document.</p><p id="p-0073" num="0072">Computing device <b>600</b> includes a processor <b>602</b>, memory <b>604</b>, a storage device <b>606</b>, a high-speed interface <b>608</b> connecting to memory <b>604</b> and high-speed expansion ports <b>610</b>, and a low speed interface <b>612</b> connecting to low speed bus <b>614</b> and storage device <b>606</b>. Each of the components <b>602</b>, <b>604</b>, <b>606</b>, <b>608</b>, <b>610</b>, and <b>612</b>, are interconnected using various busses, and may be mounted on a common motherboard or in other manners as appropriate. The processor <b>602</b> may process instructions for execution within the computing device <b>600</b>, including instructions stored in the memory <b>604</b> or on the storage device <b>606</b> to display graphical information for a GUI on an external input/output device, such as display <b>616</b> coupled to high speed interface <b>608</b>. In other implementations, multiple processors and/or multiple buses may be used, as appropriate, along with multiple memories and types of memory. Also, multiple computing devices <b>600</b> may be connected, with each device providing portions of the necessary operations (e.g., as a server bank, a group of blade servers, or a multi-processor system).</p><p id="p-0074" num="0073">The memory <b>604</b> stores information within the computing device <b>600</b>. In one implementation, the memory <b>604</b> is a volatile memory unit or units. In another implementation, the memory <b>604</b> is a non-volatile memory unit or units. The memory <b>604</b> may also be another form of computer-readable medium, such as a magnetic or optical disk.</p><p id="p-0075" num="0074">The storage device <b>606</b> is capable of providing mass storage for the computing device <b>600</b>. In one implementation, the storage device <b>606</b> may be or contain a computer-readable medium, such as a floppy disk device, a hard disk device, an optical disk device, or a tape device, a flash memory or other similar solid state memory device, or an array of devices, including devices in a storage area network or other configurations. A computer program product may be tangibly embodied in an information carrier. The computer program product may also contain instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>604</b>, the storage device <b>606</b>, or a memory on processor <b>602</b>.</p><p id="p-0076" num="0075">The high speed controller <b>608</b> manages bandwidth-intensive operations for the computing device <b>600</b>, while the low speed controller <b>612</b> manages lower bandwidth-intensive operations. Such allocation of functions is exemplary only. In one implementation, the high-speed controller <b>608</b> is coupled to memory <b>604</b>, display <b>616</b> (e.g., through a graphics processor or accelerator), and to high-speed expansion ports <b>610</b>, which may accept various expansion cards (not shown). In the implementation, low-speed controller <b>612</b> is coupled to storage device <b>606</b> and low-speed expansion port <b>614</b>. The low-speed expansion port, which may include various communication ports (e.g., USB, Bluetooth, Ethernet, wireless Ethernet) may be coupled to one or more input/output devices, such as a keyboard, a pointing device, a scanner, or a networking device such as a switch or router, e.g., through a network adapter.</p><p id="p-0077" num="0076">The computing device <b>600</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a standard server <b>620</b>, or multiple times in a group of such servers. It may also be implemented as part of a rack server system <b>624</b>. In addition, it may be implemented in a personal computer such as a laptop computer <b>622</b>. Alternatively, components from computing device <b>600</b> may be combined with other components in a mobile device (not shown), such as mobile computing device <b>650</b>. Each of such devices may contain one or more of computing devices <b>600</b>, <b>650</b>, and an entire system may be made up of multiple computing devices <b>600</b>, <b>650</b> communicating with each other.</p><p id="p-0078" num="0077">Mobile computing device <b>650</b> includes a processor <b>652</b>, memory <b>664</b>, an input/output device such as a display <b>654</b>, a communication interface <b>666</b>, and a transceiver <b>668</b>, among other components. The mobile computing device <b>650</b> may also be provided with a storage device, such as a microdrive or other device, to provide additional storage. Each of the components <b>650</b>, <b>652</b>, <b>664</b>, <b>654</b>, <b>666</b>, and <b>668</b>, are interconnected using various buses, and several of the components may be mounted on a common motherboard or in other manners as appropriate.</p><p id="p-0079" num="0078">The processor <b>652</b> may execute instructions within the mobile computing device <b>650</b>, including instructions stored in the memory <b>664</b>. The processor may be implemented as a chipset of chips that include separate and multiple analog and digital processors. The processor may provide, for example, for coordination of the other components of the mobile computing device <b>650</b>, such as control of user interfaces, applications run by device <b>650</b>, and wireless communication by device <b>650</b>.</p><p id="p-0080" num="0079">Processor <b>652</b> may communicate with a user through control interface <b>658</b> and display interface <b>656</b> coupled to a display <b>654</b>. The display <b>654</b> may be, for example, a TFT LCD (Thin-Film-Transistor Liquid Crystal Display) or an OLED (Organic Light Emitting Diode) display, or other appropriate display technology. The display interface <b>656</b> may comprise appropriate circuitry for driving the display <b>654</b> to present graphical and other information to a user. The control interface <b>658</b> may receive commands from a user and convert them for submission to the processor <b>652</b>. In addition, an external interface <b>662</b> may be provide in communication with processor <b>652</b>, so as to enable near area communication of mobile computing device <b>650</b> with other devices. External interface <b>662</b> may provide, for example, for wired communication in some implementations, or for wireless communication in other implementations, and multiple interfaces may also be used.</p><p id="p-0081" num="0080">The memory <b>664</b> stores information within the mobile computing device <b>650</b>. The memory <b>664</b> may be implemented as one or more of a computer-readable medium or media, a volatile memory unit or units, or a non-volatile memory unit or units. Expansion memory <b>654</b> may also be provided and connected to device <b>650</b> through expansion interface <b>652</b>, which may include, for example, a SIMM (Single In Line Memory Module) card interface. Such expansion memory <b>654</b> may provide extra storage space for device <b>650</b>, or may also store applications or other information for device <b>650</b>. Specifically, expansion memory <b>654</b> may include instructions to carry out or supplement the processes described above, and may include secure information also. Thus, for example, expansion memory <b>654</b> may be provide as a security module for device <b>650</b>, and may be programmed with instructions that permit secure use of device <b>650</b>. In addition, secure applications may be provided via the SIMM cards, along with additional information, such as placing identifying information on the SIMM card in a non-hackable manner.</p><p id="p-0082" num="0081">The memory may include, for example, flash memory and/or NVRAM memory, as discussed below. In one implementation, a computer program product is tangibly embodied in an information carrier. The computer program product contains instructions that, when executed, perform one or more methods, such as those described above. The information carrier is a computer- or machine-readable medium, such as the memory <b>664</b>, expansion memory <b>654</b>, memory on processor <b>652</b>, or a propagated signal that may be received, for example, over transceiver <b>668</b> or external interface <b>662</b>.</p><p id="p-0083" num="0082">Device <b>650</b> may communicate wirelessly through communication interface <b>666</b>, which may include digital signal processing circuitry where necessary. Communication interface <b>666</b> may provide for communications under various modes or protocols, such as GSM voice calls, SMS, EMS, or MMS messaging, CDMA, TDMA, PDC, WCDMA, CDMA2000, or GPRS, among others. Such communication may occur, for example, through radio-frequency transceiver <b>668</b>. In addition, short-range communication may occur, such as using a Bluetooth, Wi-Fi, or other such transceiver (not shown). In addition, GPS (Global Positioning System) receiver module <b>650</b> may provide additional navigation- and location-related wireless data to device <b>650</b>, which may be used as appropriate by applications running on device <b>650</b>.</p><p id="p-0084" num="0083">Device <b>650</b> may also communicate audibly using audio codec <b>660</b>, which may receive spoken information from a user and convert it to usable digital information. Audio codec <b>660</b> may likewise generate audible sound for a user, such as through a speaker, e.g., in a handset of device <b>650</b>. Such sound may include sound from voice telephone calls, may include recorded sound (e.g., voice messages, music files, etc.) and may also include sound generated by applications operating on device <b>650</b>.</p><p id="p-0085" num="0084">The computing device <b>650</b> may be implemented in a number of different forms, as shown in the figure. For example, it may be implemented as a cellular telephone <b>680</b>. It may also be implemented as part of a smartphone <b>682</b>, personal digital assistant, or other similar mobile device.</p><p id="p-0086" num="0085">A number of implementations have been described. Nevertheless, it will be understood that various modifications may be made without departing from the spirit and scope of the disclosure. For example, various forms of the flows shown above may be used, with steps re-ordered, added, or removed.</p><p id="p-0087" num="0086">Embodiments of the invention and all of the functional operations described in this specification can be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the invention can be implemented as one or more computer program products, e.g., one or more modules of computer program instructions encoded on a computer readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium can be a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter effecting a machine-readable propagated signal, or a combination of one or more of them. The term &#x201c;data processing apparatus&#x201d; encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus can include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.</p><p id="p-0088" num="0087">A computer program (also known as a program, software, software application, script, or code) can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a standalone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program can be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program can be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p><p id="p-0089" num="0088">The processes and logic flows described in this specification can be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows can also be performed by, and apparatus can also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).</p><p id="p-0090" num="0089">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer can be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of nonvolatile memory, media and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory can be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0091" num="0090">To provide for interaction with a user, embodiments of the invention can be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user can provide input to the computer. Other kinds of devices can be used to provide for interaction with a user as well; for example, feedback provided to the user can be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user can be received in any form, including acoustic, speech, or tactile input.</p><p id="p-0092" num="0091">Embodiments of the invention can be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user can interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components. The components of the system can be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;) and a wide area network (&#x201c;WAN&#x201d;), e.g., the Internet.</p><p id="p-0093" num="0092">The computing system can include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p><p id="p-0094" num="0093">While this specification contains many specifics, these should not be construed as limitations on the scope of the invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of the invention. Certain features that are described in this specification in the context of separate embodiments can also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment can also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination can in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p><p id="p-0095" num="0094">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems can generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0096" num="0095">In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.</p><p id="p-0097" num="0096">Particular embodiments of the invention have been described. Other embodiments are within the scope of the following claims. For example, the steps recited in the claims can be performed in a different order and still achieve desirable results.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method implemented by one or more processors, the method comprising:<claim-text>determining, by a client device of a user, to generate a search query for the user based on an image capturing screen content displayed by the client device at a particular time and based on a voice input of the user received subsequent to the particular time;</claim-text><claim-text>processing the image of the screen content displayed by the client device at the particular time to identify a particular sub-image of a plurality of disparate sub-images included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>processing a plurality of separate portions of the particular sub-image to generate a plurality of labels that each correspond to at least one of the separate portions of the particular sub-image included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>receiving, by the client device and subsequent to the particular time, audio data including the voice input of the user;</claim-text><claim-text>selecting a particular subset of the plurality of labels based on a transcription of the voice input of the user and based on identifying a screen content type associated with a particular portion of the particular sub-image, of the plurality of separate portions of the particular sub-image, that is associated with the particular subset of the plurality of labels;</claim-text><claim-text>generating the search query for the user based on the transcription of the voice input of the user and the particular selected subset of the plurality of labels that correspond to the particular portion of the particular sub-image; and</claim-text><claim-text>providing, for display at the client device of the user, one or more search results obtained responsive to the search query that was generated for the user.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the search query for the user based on the transcription of the voice input of the user and the particular selected subset of the plurality of labels that correspond to the particular disparate portion of the particular sub-image includes generating the search query to include at least one first term included in transcription of the voice input of the user and at least one second term associated with the particular selected subset of the plurality of labels.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the image of the screen content displayed by the client device at the particular time to identify the particular sub-image of the plurality of disparate sub-images included in the image further includes processing the image of the screen content to identify an additional particular sub-image of the plurality of disparate sub-images included in the image, and further comprising:<claim-text>processing a plurality of additional separate portions of the additional particular sub-image to generate a plurality of additional labels that each correspond to at least one of the additional separate portions of the additional particular sub-image included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>selecting, for use in generating the search query for the user, at least one additional label that corresponds to at least one of the additional separate portions of the additional particular sub-image based on identifying a type of screen content respective screen content types associated with the at least one additional separate portion of the additional particular sub-image.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein generating the search query for the user using the at least one additional label that corresponds to the at least one of the additional separate portions of the additional particular sub-image includes:<claim-text>generating the transcription of the voice input of the user based on the at least one additional label; and</claim-text><claim-text>generating the search query for the user based on the transcription of the voice input.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the search query for the user includes:<claim-text>generating a plurality of candidate search queries;</claim-text><claim-text>comparing the plurality of candidate search queries to a plurality of recent search queries associated with a plurality of users; and</claim-text><claim-text>selecting a candidate search query, of the plurality of candidate search queries, to be the search query for the user based on a frequency of each of the candidate search queries of the plurality appears in the plurality of recent search queries.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the screen content displayed by the client device of the user at the particular time includes video content.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A system, comprising:<claim-text>one or more processors; and</claim-text><claim-text>memory storing instructions that, when executed by one or more of the processors, cause the one or more processors to perform operations comprising:<claim-text>determining, by a client device of a user, to generate a search query for the user based on an image capturing screen content displayed by the client device at a particular time and based on a voice input of the user received subsequent to the particular time;</claim-text><claim-text>processing the image of the screen content displayed by the client device at the particular time to identify a particular sub-image of a plurality of disparate sub-images included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>processing a plurality of separate portions of the particular sub-image to generate a plurality of labels that each correspond to at least one of the separate portions of the particular sub-image included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>receiving, by the client device and subsequent to the particular time, audio data including the voice input of the user;</claim-text><claim-text>selecting a particular subset of the plurality of labels based on a transcription of the voice input of the user and based on identifying a screen content type associated with a particular portion of the particular sub-image, of the plurality of separate portions of the particular sub-image, that is associated with the particular subset of the plurality of labels;</claim-text><claim-text>generating the search query for the user based on the transcription of the voice input of the user and the particular selected subset of the plurality of labels that correspond to the particular portion of the particular sub-image; and</claim-text><claim-text>providing, for display at the client device of the user, one or more search results obtained responsive to the search query that was generated for the user.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein generating the search query for the user based on the transcription of the voice input of the user and the particular selected subset of the plurality of labels that correspond to the particular disparate portion of the particular sub-image includes generating the search query to include at least one first term included in transcription of the voice input of the user and at least one second term associated with the particular selected subset of the plurality of labels.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein processing the image of the screen content displayed by the client device at the particular time to identify the particular sub-image of the plurality of disparate sub-images included in the image further includes processing the image of the screen content to identify an additional particular sub-image of the plurality of disparate sub-images included in the image, and the operations further comprising:<claim-text>processing a plurality of additional separate portions of the additional particular sub-image to generate a plurality of additional labels that each correspond to at least one of the additional separate portions of the additional particular sub-image included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>selecting, for use in generating the search query for the user, at least one additional label that corresponds to at least one of the additional separate portions of the additional particular sub-image based on identifying a type of screen content respective screen content types associated with the at least one additional separate portion of the additional particular sub-image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein generating the search query for the user using the at least one additional label that corresponds to the at least one of the additional separate portions of the additional particular sub-image includes:<claim-text>generating the transcription of the voice input of the user based on the at least one additional label; and</claim-text><claim-text>generating the search query for the user based on the transcription of the voice input.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein generating the search query for the user includes:<claim-text>generating a plurality of candidate search queries;</claim-text><claim-text>comparing the plurality of candidate search queries to a plurality of recent search queries associated with a plurality of users; and</claim-text><claim-text>selecting a candidate search query, of the plurality of candidate search queries, to be the search query for the user based on a frequency of each of the candidate search queries of the plurality appears in the plurality of recent search queries.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the screen content displayed by the client device of the user at the particular time includes video content.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. One or more non-transitory computer-readable storage media encoded with instructions that, when executed by one or more computers, cause the one or more computers to perform operations comprising:<claim-text>determining, by a client device of a user, to generate a search query for the user based on an image capturing screen content displayed by the client device at a particular time and based on a voice input of the user received subsequent to the particular time;</claim-text><claim-text>processing the image of the screen content displayed by the client device at the particular time to identify a particular sub-image of a plurality of disparate sub-images included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>processing a plurality of separate portions of the particular sub-image to generate a plurality of labels that each correspond to at least one of the separate portions of the particular sub-image included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>receiving, by the client device and subsequent to the particular time, audio data including the voice input of the user;</claim-text><claim-text>selecting a particular subset of the plurality of labels based on a transcription of the voice input of the user and based on identifying a screen content type associated with a particular portion of the particular sub-image, of the plurality of separate portions of the particular sub-image, that is associated with the particular subset of the plurality of labels;</claim-text><claim-text>generating the search query for the user based on the transcription of the voice input of the user and the particular selected subset of the plurality of labels that correspond to the particular portion of the particular sub-image; and</claim-text><claim-text>providing, for display at the client device of the user, one or more search results obtained responsive to the search query that was generated for the user.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The one or more non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein generating the search query for the user based on the transcription of the voice input of the user and the particular selected subset of the plurality of labels that correspond to the particular disparate portion of the particular sub-image includes generating the search query to include at least one first term included in transcription of the voice input of the user and at least one second term associated with the particular selected subset of the plurality of labels.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The one or more non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein processing the image of the screen content displayed by the client device at the particular time to identify the particular sub-image of the plurality of disparate sub-images included in the image further includes processing the image of the screen content to identify an additional particular sub-image of the plurality of disparate sub-images included in the image, and the operations further comprising:<claim-text>processing a plurality of additional separate portions of the additional particular sub-image to generate a plurality of additional labels that each correspond to at least one of the additional separate portions of the additional particular sub-image included in the image of the screen content displayed by the client device at the particular time;</claim-text><claim-text>selecting, for use in generating the search query for the user, at least one additional label that corresponds to at least one of the additional separate portions of the additional particular sub-image based on identifying a type of screen content respective screen content types associated with the at least one additional separate portion of the additional particular sub-image.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The one or more non-transitory computer-readable storage media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein generating the search query for the user using the at least one additional label that corresponds to the at least one of the additional separate portions of the additional particular sub-image includes:<claim-text>generating the transcription of the voice input of the user based on the at least one additional label; and</claim-text><claim-text>generating the search query for the user based on the transcription of the voice input.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The one or more non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein generating the search query for the user includes:<claim-text>generating a plurality of candidate search queries;</claim-text><claim-text>comparing the plurality of candidate search queries to a plurality of recent search queries associated with a plurality of users; and</claim-text><claim-text>selecting a candidate search query, of the plurality of candidate search queries, to be the search query for the user based on a frequency of each of the candidate search queries of the plurality appears in the plurality of recent search queries.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The one or more non-transitory computer-readable storage media of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the screen content displayed by the client device of the user at the particular time includes video content.</claim-text></claim></claims></us-patent-application>