<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004545A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004545</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941455</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>23</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>22</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>28</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2322</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>221</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>284</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2358</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>2365</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Including Transactional Commit Timestamps In The Primary Keys Of Relational Databases</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16978361</doc-number><date>20200904</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11474991</doc-number></document-id></parent-grant-document><parent-pct-document><document-id><country>WO</country><doc-number>PCT/US2018/022156</doc-number><date>20180313</date></document-id></parent-pct-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941455</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Google LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kanthak</last-name><first-name>Sebastian</first-name><address><city>Los Altos</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cooper</last-name><first-name>Brian Frank</first-name><address><city>Los Altos</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">In a distributed database, a transaction is to be committed at a first coordinator server and one or more participant servers <b>1210</b>. The first coordinator server is configured to receive a notification that each participant server of the transaction is prepared at a respective prepared timestamp, the respective prepared timestamp being chosen within a time range for which the respective participant server obtained at least one lock <b>1220</b>. The first coordinator server computes the commit timestamp for the transaction equal or greater than each of the prepared timestamps <b>1230</b>, and restrict the commit timestamp such that a second coordinator server sharing at least one of the participant servers for one or more other transactions at a shared shard cannot select the same commit timestamp for any of the other transactions <b>1240</b>. The transaction is committed at the commit timestamp <b>1250. </b></p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="133.27mm" wi="136.23mm" file="US20230004545A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="218.86mm" wi="142.75mm" orientation="landscape" file="US20230004545A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="124.80mm" wi="137.24mm" orientation="landscape" file="US20230004545A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="199.05mm" wi="138.09mm" orientation="landscape" file="US20230004545A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="208.79mm" wi="148.42mm" orientation="landscape" file="US20230004545A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="180.59mm" wi="143.68mm" orientation="landscape" file="US20230004545A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="213.87mm" wi="144.53mm" orientation="landscape" file="US20230004545A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="179.75mm" wi="140.04mm" orientation="landscape" file="US20230004545A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="217.17mm" wi="143.85mm" orientation="landscape" file="US20230004545A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="205.99mm" wi="145.71mm" orientation="landscape" file="US20230004545A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="207.01mm" wi="143.68mm" orientation="landscape" file="US20230004545A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="207.35mm" wi="143.17mm" orientation="landscape" file="US20230004545A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="205.32mm" wi="138.26mm" file="US20230004545A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">In database systems, concurrency control refers to a technique used to address conflicts arising as a result of allowing simultaneous access to data items in the database (&#x201c;concurrency&#x201d;). Concurrency control ensures that the database behaves consistently despite allowing simultaneous access. Multiversion concurrency-control techniques store multiple versions of a given piece of data (one per write), so as to enable greater concurrency. Systems that provide a global notion of absolute time can be integrated with multiversion concurrency control in a distributed database. The resulting distributed database is semantically equivalent to a single-machine database, in that consistent reads can be done across the entire database.</p><heading id="h-0002" level="1">BRIEF SUMMARY</heading><p id="p-0003" num="0002">The present disclosure provides for a method comprising receiving, at a first coordinator server and one or more participant servers in a distributed system, a request to commit a transaction, receiving a notification from each of the participant servers, the notification including a respective prepared timestamp, the respective prepared timestamp being chosen within a time range for which the respective participant server obtained at least one lock, computing a commit timestamp for the transaction equal or greater than each of the prepared timestamps, restricting the commit timestamp such that a second coordinator server sharing at least one of the participant servers for one or more other transactions at a shared shard cannot select the same commit timestamp for any of the other transactions, and committing, at the first coordinator server and each of the participant servers, the transaction at the commit timestamp. The at least one lock may be a writer shared lock. The request to commit a transaction may further include a mutation to update a change log recording the transaction. The commit timestamp may be included as a primary key of the change log. At least part of the change log may be stored at one of the participant servers. The request to commit a transaction may further include a mutation to update a plurality of change logs recording the transaction. The method may further comprise adding at least one column in a relation database to store the commit timestamp as a primary key in the relational database.</p><p id="p-0004" num="0003">The method may further comprise computing a hash value of a transaction ID of the transaction, and further restricting the commit timestamp for the transaction by the hash value of the transaction ID. For example, the further restricting the commit timestamp may comprise setting a predetermined number of lower bits of the commit timestamp to be equal to the hash value of the transaction ID.</p><p id="p-0005" num="0004">The method may further comprise determining that the first coordinator server received one or more requests to commit other transactions and computing a commit timestamp for each of the other transactions received at the first coordinator sever such that a total spacing between the commit timestamps is substantially minimized.</p><p id="p-0006" num="0005">The method may further comprise determining, at the shared participant server, that the hash value of the transaction ID is equal to a hash value of a transaction ID of at least one of the other transactions, and preventing, at the shared participant server, at least one of the other transactions having the same hash value from taking a lock until the transaction commits.</p><p id="p-0007" num="0006">The method may further comprise determining, at one or more of the participant servers, that a single-site transaction is to be committed at the participant server, computing a single-site commit timestamp having a predetermined pattern for the single-site transaction, the predetermined pattern being one that any multi-site transaction cannot choose as its commit timestamp, and committing the single-site transaction at the single-site commit timestamp.</p><p id="p-0008" num="0007">The method may further comprise determining, at one or more of the participant servers, that a single-site transaction is to be committed at the participant server, computing a hash value of a transaction ID of the single-site transaction, and restricting the commit timestamp for the single-site transaction by the hash value of the transaction ID of the single-site transaction. For example, restricting a commit timestamp for the single-site transaction may comprise setting a predetermined number of lower bits of the commit timestamp of the single-site transaction to be equal to the hash value of the transaction ID of the single-site transaction.</p><p id="p-0009" num="0008">The present disclosure further provides for a method comprising receiving, at a coordinator server and one or more participant servers in a distributed system, a request to commit a transaction, obtaining, by each of the participant servers, at least one exclusive lock for a time range starting at a locally chosen starting time to a predetermined upper bound, receiving, at the coordinator server, a notification that each of the participant servers is prepared at a respective locally chosen prepared timestamp within the time range, computing, at the coordinator server, a commit timestamp for the transaction equal or greater than each of the prepared timestamps, committing, at the coordinator server and each of the participant servers, the transaction at the commit timestamp, and releasing, at each of the participant servers, the at least one exclusive lock. The predetermined upper bound may be infinity. The request to commit a transaction may further include a mutation to update a change log recording the transaction. The method may further comprise adding at least one column in a relation database to store the commit timestamp as a primary key in the relational database.</p><p id="p-0010" num="0009">The present disclosure further provides for a system comprising a first coordinator server in a plurality of servers, each of the servers adapted to communicate with each other and clients in a distributed computing environment, the first coordinator server comprising one or more processors configured to receive a request to commit a transaction, receive a notification that any of the other servers functioning as participants of the transaction is prepared at a respective prepared timestamp, the respective prepared timestamp being chosen within a time range for which the respective participant server obtained at least one lock, compute a commit timestamp for the transaction equal or greater than each of the prepared timestamps such that a second coordinator server sharing at least one of the participant servers for one or more other transactions at a shared shard cannot select the same commit timestamp for any of the other transactions, and commit the transaction at the commit timestamp. The one or more processors may be further configured to compute a hash value of a transaction ID of the transaction and further restrict the commit timestamp for the transaction by the hash value of the transaction ID. The one or more processors may be further configured to determine that one or more requests to commit other transactions is received at the first coordinator server and compute a commit timestamp for each of the other transactions such that a total spacing between the commit timestamps is substantially minimized. The distributed computing environment may comprise a relational database, where the one or more processors may be further configured to add at least one column in the relation database to store the commit timestamp as a primary key in the relational database.</p><p id="p-0011" num="0010">The system may further comprise a participant server, the participant server comprising one or more processors configured to determine that a single-site transaction is to be committed at the participant saver, compute a single-site commit timestamp having a predetermined pattern for the single-site transaction, the predetermined pattern being one that any multi-site transaction cannot choose as its commit timestamp, and commit the single-site transaction at the single-site commit timestamp.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a pictorial diagram illustrating distribution of a database according to aspects of the disclosure.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram illustrating hierarchical relationships among servers in a distributed database according ding to aspects of the disclosure.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a pictorial diagram illustrating an example system according to aspects of the disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a flow chart illustrating an example method according to aspects of the disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><heading id="h-0005" level="2">Overview</heading><p id="p-0024" num="0023">The technology generally relates to methods of determining commit timestamps and providing the commit timestamps to users of a distributed database. For example, users may use commit timestamps to see snapshots of the database at various points in time, or to build a transaction log for changes made to a database. In order to provide users with meaningful commit timestamps, each transaction for the same data item must correspond to a unique commit timestamp. This way, the user may use these commit timestamps to read the different versions of the data at different specific timestamps, or to see all the changes made to the data item. Further, there are provided efficient methods of determining meaningful commit timestamps to make sure that throughput of the database is not compromised.</p><p id="p-0025" num="0024">In a distributed database, a transaction is to be committed at a first coordinator server and one or more participant servers. The first coordinator server is configured to receive a notification that each participant server of the transaction is prepared at a respective prepared timestamp, the respective prepared timestamp being chosen within a time range for which the respective participant server obtained at least one lock. The first coordinator server computes the commit timestamp for the transaction equal or greater than each of the prepared timestamps, and restrict the commit timestamp such that a second coordinator server sharing at least one of the participant servers for one or more other transactions at a shared shard cannot select the same commit timestamp for any of the other transactions.</p><p id="p-0026" num="0025">In a distributed database, data is read and/or written in multiple shards, distributed over a plurality of computing devices, such as servers, in a distributed network of datacenters. In some instances, the same shard may be replicated on multiple servers to prevent loss of data in case if one of the servers fails. Each server may store and execute actions for multiple shard and/or shard replicas. The totality of replicas of a single shard forms a group In a multi-site transaction reading and/or writing to multiple shards, one of the servers storing one of the shards (or a plurality of servers storing one of the groups) may be chosen as a &#x201c;coordinator server&#x201d; (or a &#x201c;coordinator server group&#x201d;) while all the other servers storing the other shards (or a plurality of servers storing the other groups) may be chosen as &#x201c;participant servers&#x201d; (or &#x201c;participant server groups&#x201d;). While a server (or server group) may be chosen as the coordinator server (or coordinator server group) for one or more transactions, the server (or server group) may also simultaneously be chosen as the participant server (or participant server group) for one or more other transactions. For sake of simplicity, from here on, &#x201c;shard&#x201d; is used interchangeably with &#x201c;group,&#x201d; &#x201c;coordinator server&#x201d; is used interchangeably with &#x201c;coordinator server group,&#x201d; and &#x201c;participant server&#x201d; is used interchangeably with &#x201c;participant server group.&#x201d;</p><p id="p-0027" num="0026">When a commit message for a transaction is received at the coordinator server and the participant servers for a multi-site transaction, the coordinator server and the participant servers may each acquire a lock for a range from the time the lock is acquired until an upper bound. Each of the participant servers may locally choose a prepared timestamp, write a pre areal record, and notify the coordinator server that it is prepared from the prepared timestamp and onwards. Once the coordinator server receives a prepared notification that all of the participant servers are prepared, the coordinator server may choose a commit timestamp equal or greater than any of the prepared timestamps and any commit timestamp it has previously assigned to other transactions. The coordinator server and each of the participant servers then execute the transaction at the commit timestamp. In addition to being a participant server for a multi-site transaction, a server storing a shard may also receive a request to commit a single-site transaction locally at the shard.</p><p id="p-0028" num="0027">One example method that ensures unique commit timestamps are chosen for various multi-site and/or single-site transaction at a shared shard is to only allow participant servers of a distributed transaction to take exclusive locks at the shared shard for a range of time until a commit timestamp is chosen. For example, each participant server may take an exclusive lock from a locally chosen starting time to infinity, or from a locally chosen starting time to an estimated upper bound.</p><p id="p-0029" num="0028">In another example method, the participant servers may take shared locks at the shared shard, but the commit timestamps that may be chosen in certain situations are restricted. For example, a coordinator server may restrict the commit timestamp for a multi-site transaction such that no other coordinator server sharing the same participant server at a shared shard could choose the same commit timestamp for another multi-site transaction. For instance, the coordinator server may do so by computing a hash value of a transaction ID of the transaction, and then computing the commit timestamp by finding the next available time equal or greater than any of the prepared timestamps that has its lower N-bits (e.g., lower 10 bits) equal to the hash value. In other examples, commit timestamps for single-site transactions may also be restricted. For example, single-site transactions may be assigned timestamps excluded from ones that could be chosen for multi-site transactions. In another example, a coordinator server that received requests to commit multiple transactions may choose a commit timestamp for each of the transactions such that a total spacing between all the commit timestamps is substantially minimized.</p><p id="p-0030" num="0029">To prevent two coordinator servers from choosing the same commit timestamp (a &#x201c;timestamp collision&#x201d;), for example, in the event if two transaction IDs hash to the same value (a &#x201c;hash collision&#x201d;), an exclusive lock may be taken by the participant server for one of the transactions. In another example, the participant server may be allowed to only take a writer-shared restrictive lock for the first transaction, and the second transaction is placed in a waiting queue until the participant server releases the writer-shared restrictive lock on the first transaction.</p><p id="p-0031" num="0030">In another example method, coordinator servers with shared participant servers and shared shards may communicate with each other to ensure that unique commit timestamps are chosen. In yet another example method, a global manager may choose all commit timestamps to ensure that commit timestamps are unique.</p><p id="p-0032" num="0031">The commit timestamps may be provided to users in one or more columns of a table. For example, commit timestamps may be provided in a primary key column, meaning that the commit timestamps must be unique. One particular example involves a change log that a user may maintain to keep track of all change made in a database, where the commit timestamps may be provided in one or more columns of the change log. For example, the user may request a transaction to modify some data item, and also include in that transaction request a mutation to record the transaction in the change log. For another example, the user may request a transaction that spans multiple databases, and include in that transaction request a mutation to update a separate change log for each database. For still another example, it is often desirable to query a relational database by table row creation time. For example, there are provided methods and systems with a mechanism for including table row creating timestamps in the primary keys of the relational database. The relational database can be part of a distributed database. It is advantageous to include the commit timestamps as row creating timestamps in the primary keys of the relational database since specifying primary key constraints is a very efficient way of querying relational databases. According to methods and systems described herein, there are provided schemas for commit timestamps in databases to make the commit timestamps readable in database queries and reads by allowing the schemas to add columns to the database to store the commit timestamps in the database. Using the commit timestamp as primary key in a relational distributed database has several advantages. One advantage is that the commit timestamp provides a simple guarantee. If a transaction A has a lower commit timestamp than another transaction B, then transaction A committed before B. Therefore, the user can treat the distributed database like a single-machine database and assume that all mutations that happened in transaction A were visible to transaction B. In other words, methods and systems described herein provides the appearance of executing transactions atomically in a serial order consistent with the commit timestamp and allow application developers to establish a global partial ordering to all transactions in a distributed database system. Another advantage is if the commit timestamp is based on a globally synchronized clock, which ensures that commit timestamps are accurate and consistent globally, so that the user is immune from clock-skew on different servers running shards of the database.</p><p id="p-0033" num="0032">The technology is advantageous because it provides meaningful commit timestamps to users without significantly compromising the throughput of a distributed database. Although the user's database may be highly distributed, the user may use the commit timestamps to view changes made to various data items as if the database was kept on a single machine. The technology further provides various methods to increase efficiency, for example, by avoiding exclusive locks, minimizing spacing between commit timestamps, and providing different treatments for multi-site and single-site transactions.</p><heading id="h-0006" level="2">Example Systems</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates an example system including a distributed database. A plurality of computing devices, such as servers <b>140</b>, <b>150</b>, <b>160</b>, may communicate with each other, for example, over a network <b>130</b>. The servers <b>140</b>, <b>150</b>, <b>160</b> may further communicate with a plurality of client computing devices, such as clients <b>110</b>, <b>120</b>. The servers <b>140</b>-<b>160</b> may control storage of data in one or more databases. For example, as shown each server <b>140</b>-<b>160</b> is associated with a datacenter <b>142</b>, <b>152</b>, <b>162</b>. Each datacenter <b>142</b>, <b>152</b>, <b>162</b> may include a plurality of computing devices for storing data. In the distributed database, data items of a database may be sharded onto multiple distinct shards, such as shards <b>146</b>, <b>156</b>, <b>166</b>, each shard may be replicated onto multiple computing devices, such as servers, at one datacenter, for example datacenter <b>142</b>, or across multiple datacenters, such as datacenters <b>142</b>, <b>152</b>, and <b>162</b>.</p><p id="p-0035" num="0034">Each server may store and execute actions for multiple shard and/or shard replicas. The totality of replicas of a single shard forms a group, for example, group <b>148</b> contains all replicas of one shard, group <b>158</b> contains all replicas of another shard, and group <b>168</b> contains all replicas of yet another shard. The shard replicas may be synchronized by using consensus protocols, such as a Paxos protocol. While some shards may be replicas of other shards, some shards may be causally dependent on others. For example, bits of data written in datacenter <b>142</b>, for example in shard <b>146</b> or group <b>148</b>, may affect data stored in datacenter <b>152</b>, for example in shard <b>156</b> or group <b>158</b>. The distributed database may implement a protocol, such as Paxos, to provide consensus across the system. In some current systems, consistency across the datacenters <b>142</b>, <b>152</b>, <b>162</b> is maintained by the servers <b>140</b>, <b>150</b>, <b>160</b>, which wait for a period of time (e.g., the commit wait) to pass before publishing a write transaction. In other systems, the waiting period may instead be imposed on one or more other devices, moved in time to different actions of the server, or moved to client devices seeking to read the written data.</p><p id="p-0036" num="0035">While only a few datacenters with a few servers, clients, shards, and groups are shown, any number of datacenters may be included in the distributed database, each of which may contain multiple servers (which may communicate with multiple clients), shards, and groups. Similarly, while each server <b>140</b>, <b>150</b>, <b>160</b> is shown as being associated with its own datacenter, it should be understood that in other examples the servers may be associated with one or more smaller databases. For example, one database may include multiple servers.</p><p id="p-0037" num="0036">Each of clients <b>110</b>, <b>120</b> is shown as having an application program <b>112</b>, <b>122</b> and a client library <b>114</b>, <b>124</b>, though it should be understood that additional features of client devices may also be present. Either of the clients <b>110</b>, <b>120</b> may write data to the distributed database by sending data over the network <b>130</b> to one of the servers <b>140</b>, <b>150</b>, <b>160</b>. While only a few clients are shown, it should be understood that a vast number of client devices may communicate with the distributed database over the network <b>130</b>.</p><p id="p-0038" num="0037">The datacenters <b>142</b>, <b>152</b>, <b>162</b> may be positioned a considerable distance from one another. For example, as further described in connection with <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the datacenters may be positioned in various countries around the world. Each datacenter <b>142</b>, <b>152</b>, <b>162</b> may include a number of storage devices, such as hard drives, random access memory, disks, disk arrays, tape drives, or any other types of storage devices. The datacenters <b>142</b>, <b>152</b>, <b>162</b> may implement any of a number of architectures and technologies, including, but not limited to, direct attached storage (DAS), network attached storage (NAS), storage area networks (SANs), fibre channel (FC), fibre channel over Ethernet (FCoE), mixed architecture networks, or the like. The datacenters may include a number of other devices in addition to the storage devices, such as cabling, routers, etc. Further, in some examples the datacenters <b>142</b>, <b>152</b>, <b>162</b> may be virtualized environments.</p><p id="p-0039" num="0038">Each server has a local clock <b>144</b>, <b>154</b>, <b>164</b>. Each local clock <b>144</b>, <b>154</b>, <b>164</b> may derive its time from an atomic time master <b>190</b>. Atomic time master <b>190</b> may be, for example, a reference clock in communication with one or more servers in the distributed database. As further described below in connection with <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the atomic time master <b>190</b> may derive its time from another source, such as a GPS.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a geographical illustration <b>200</b> of datacenters <b>210</b>, <b>220</b>, <b>230</b>, <b>240</b>, <b>250</b>, <b>260</b> and <b>270</b> positioned at various locations on earth. According to some examples, each datacenter may include an atomic time master. Each atomic time master may be connected to a receiver such as a GPS receiver for receiving time signals. The GPS receivers may include, for example, roof-mounted antennas <b>215</b>, <b>225</b>, <b>235</b>, <b>245</b>, <b>255</b>, <b>265</b> and <b>275</b>, which may be located on the roof above datacenters <b>210</b>, <b>220</b>, <b>230</b>, <b>240</b>, <b>250</b>, <b>260</b> and <b>270</b>. Host servers may be housed in server racks located in the datacenters <b>210</b>, <b>220</b>, <b>230</b>, <b>240</b>, <b>250</b>, <b>260</b> and <b>270</b>. As such, conduits may be installed to mute antenna cables from a host server to the roof top antennas. It may be possible to share one antenna across several receivers. This can be achieved, for example, with an antenna splitter.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example of a time platform <b>300</b> that provides a tightly synchronized global clock across datacenters. In this example, the time platform <b>300</b> is structured as a three-level hierarchy of servers, each server including its own clock, where child servers calibrate their clocks based on their parents' clocks. Application programs run on hosts <b>360</b>, <b>370</b>, <b>380</b>.</p><p id="p-0042" num="0041">Arrows point from servers that calibrate their docks to well-known servers with better clocks from which they calibrate. For example, as shown, hosts <b>360</b> calibrate their clocks based on atomic master <b>392</b>. Atomic master <b>392</b> calibrates its clock based on GPS time masters <b>302</b>, <b>304</b>. Hosts <b>370</b> calibrate their clocks based on atomic master <b>394</b> and <b>396</b>. Atomic master <b>394</b> calibrates its clock based on GPS time master <b>304</b>. Hosts <b>380</b> calibrate their clocks based on atomic master <b>396</b>, which calibrates its clock based on GPS time master <b>306</b>. In some examples, child servers may determine which parent servers to use for calibration based on, for example, geographical position, signal strength, or any other indicia. In other examples, the child/parent pairings may be predetermined. While <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows the hosts <b>360</b>, <b>370</b>, <b>380</b> calibrating to the atomic masters <b>392</b>, <b>394</b>, <b>396</b>, it should be understood that in other examples the hosts <b>360</b>, <b>370</b>, <b>380</b> may additionally or alternatively calibrate directly to the GPS time masters <b>302</b>, <b>304</b>, <b>306</b>.</p><p id="p-0043" num="0042">At each level in the hierarchy, calibration consists of polling a server's parent(s), and intersecting one or more time intervals received from the parent(s), expanded by network latency of the calibration from the hosts involved. Each server may have an associated value (&#x3b5;) representing a greatest difference in time between a time reflected on the server's local clock and times reflected by other servers' clocks in the database. Each server's value of a is derived from its parent's &#x3b5;, with adjustments to uncertainty that come from a product of oscillator frequency uncertainty and effective calibration interval, and server-to-parent network round trip time (RTT). Accordingly, in some examples, a local clock at each server may maintain a different value of &#x3b5;. In other examples, &#x3b5; may be globally consistent across devices in the system. Further, &#x3b5; may vary over time in some examples, as parameters such as the oscillator frequency uncertainty, effective calibration interval, and RTT change over time.</p><p id="p-0044" num="0043">Oscillator frequency uncertainty can be modeled as consisting of frequency instability, such as how much an oscillator drifts over short time scales, and oscillator aging, such as how much an oscillator's drift changes over long time scales. The effective calibration interval may be determined by a greater of two values: a calibration interval, such as a period of time between calibrations of the server, and how long the server may have to be disconnected from the parent.</p><p id="p-0045" num="0044">With regard to the server-to-parent network RTT, the farther away a host is from its parents, the more phase uncertainty is introduced. This uncertainty can also be modeled as two components: calibration phase uncertainty and calibration frequency uncertainty. Calibration phase uncertainty may correspond to a level of uncertainty in computing phase alignment of the oscillators. Calibration frequency uncertainty may correspond to a level of frequency uncertainty due to uncertainty in the duration of the calibration period.</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram of an example system <b>400</b> for providing commit timestamps of a distributed database to users. As shown, an example of system <b>400</b> may include a number of servers <b>410</b> and <b>470</b> coupled to a network <b>450</b>. The servers <b>410</b> and <b>470</b> may be located at different datacenters, for example, such as datacenters <b>142</b> and <b>152</b>. The system may also include a client <b>460</b> capable of communication with the servers <b>410</b> and <b>470</b> over the network <b>450</b>.</p><p id="p-0047" num="0046">The server <b>410</b> may contain a processor <b>420</b>, memory <b>430</b>, clock <b>435</b>, and other components typically present in general purpose computers. The memory <b>430</b> can store information accessible by the processor <b>420</b>, including instructions <b>432</b> that can be executed by the processor <b>420</b>. Memory can also include data <b>434</b> that can be retrieved, manipulated or stored by the processor <b>420</b>. The memory <b>430</b> may be a type of non-transitory computer readable medium capable of storing information accessible by the processor <b>420</b>, such as a hard-drive, solid state drive, tape drive, optical storage, memory card, ROM, RAM, DVD, CD-ROM, write-capable, and read-only memories. The processor <b>420</b> can be a well-known processor or other lesser-known types of processors. Alternatively, the processor <b>420</b> can be a dedicated controller such as an ASIC.</p><p id="p-0048" num="0047">The instructions <b>432</b> can be a set of instructions executed directly, such as machine code, or indirectly, such as scripts, by the processor <b>420</b>. In this regard, the terms &#x201c;instructions,&#x201d; &#x201c;steps&#x201d; and &#x201c;programs&#x201d; can be used interchangeably herein. The instructions <b>432</b> can be stored in object code format for direct processing by the processor <b>420</b>, or other types of computer language including scripts or collections of independent source code modules that are interpreted on demand or compiled in advance. Functions, methods, and routines of the instructions are explainers in more detail in the foregoing examples and the example methods below.</p><p id="p-0049" num="0048">The data <b>434</b> can be retrieved, stored or modified by the processor <b>420</b> in accordance with the instructions <b>432</b>. For instance, although the system and method is not limited by a particular data structure, the data <b>434</b> can be stored in computer registers, in a relational database as a table having a plurality of different fields and records, or XML documents. The data <b>434</b> can also be formatted in a computer-readable format such as, but not limited to, binary values, ASCII or Unicode. Moreover, the data <b>434</b> can include information sufficient to identify relevant information, such as numbers, descriptive text, proprietary codes, pointers, references to data stored in other memories, including other network locations, or information that is used by a function to calculate relevant data. For example, the data <b>434</b> can include time data that may be encoded based on the instructions <b>432</b> in a time format used to describe instants of time such as Coordinated Universal Time, Unix epoch and unambiguous International Atomic Time epoch.</p><p id="p-0050" num="0049">Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> functionally illustrates the processor <b>420</b> and memory <b>430</b> as being within the same block, the processor <b>420</b> and memory <b>430</b> may actually include multiple processors and memories that may or may not be stored within the same physical housing. For example, some of the instructions <b>432</b> and data <b>434</b> can be stored on a removable CD-ROM and others within a read-only computer chip. Some or all of the instructions and data can be stored in a location physically remote from, yet still accessible by, the processor <b>420</b>. Similarly, the processor <b>420</b> can actually include a collection of processors, which may or may not operate in parallel.</p><p id="p-0051" num="0050">Servers <b>410</b> and <b>470</b> may be at one node of network <b>450</b> and capable of directly and indirectly communicating with other nodes of the network <b>450</b>. For example, the servers <b>410</b> and <b>470</b> can include a web server that may be capable of communicating with client device <b>460</b> via network <b>450</b> such that it uses the network <b>450</b> to transmit information to a client application. Servers <b>410</b> and <b>470</b> may also include a number of computers, e.g., a load balanced server farm, that exchange information with different nodes of the network <b>450</b> for the purpose of receiving, processing and transmitting data to client devices. In this instance, the client computers will typically be at different nodes of the network <b>450</b> than the computers making up servers <b>410</b> and <b>470</b>. Although only a few servers <b>410</b>, <b>470</b> are depicted in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, it should be appreciated that a typical system can include a large number of connected servers with each being at a different node of the network <b>450</b>.</p><p id="p-0052" num="0051">Each client <b>460</b> may be configured, similarly to servers <b>410</b> and <b>470</b>, with a processor <b>462</b>, memory <b>463</b>, instructions <b>464</b>, and data <b>467</b>. Each client <b>460</b> may be a personal computer, intended for use by a person having all the internal components normally found in a personal computer such as a central processing unit (CPU), CD-ROM, hard drive, and a display device <b>465</b>, for example, a monitor having a screen, a projector, a touch-screen, a small LCD screen, a television, or another device such as an electrical device that can be operable to display information processed by the processor <b>462</b>, speakers, a modern and/or network interface device, user input component <b>466</b>, such as a mouse, keyboard, touch screen or microphone, and all of the components used for connecting these elements to one another. Moreover, computers in accordance with the systems and methods described herein may include devices capable of processing instructions and transmitting data to and from humans and other computers including general purpose computers, PDAs, tablets, mobile phones, smartwatches, network computers lacking local storage capability, set top boxes for televisions, and other networked devices.</p><p id="p-0053" num="0052">The client <b>460</b> may include an application interface module <b>468</b>. The application interface module may be used to access a service made available by a server, such as servers <b>410</b> and <b>470</b>. For example, the application interface module may include sub-routines, data structures, object classes and other type of software components used to allow servers and clients to communicate with each other. In one aspect, the application interface module <b>468</b> may be a software module operable in conjunction with several types of operating systems known in the arts. For example, the client <b>460</b> may be connected to a Structured Quay language (SQL) database server that may operate in conjunction with the application interface module <b>468</b> for saving and retrieving information data. Memory <b>463</b> coupled to a client <b>460</b> may store data <b>467</b> accessed by the application interface module <b>468</b>. The data <b>467</b> can also be stored on a removable medium such as a disk, tape, SD Card or CD-ROM, which can be connected to client <b>460</b>.</p><p id="p-0054" num="0053">Servers <b>410</b> and <b>470</b> and client <b>460</b> can be capable of direct and indirect communication such as over network <b>450</b>. For example, using an Internet socket, a client <b>460</b> can connect to a service operating on remote servers <b>410</b> and <b>470</b> through an Internet protocol suite. Servers <b>410</b> and <b>470</b> can set up listening sockets that may accept am initiating connection for sending and receiving information. The network <b>450</b>, and intervening nodes, may include various configurations and protocols including the Internet, World Wide Web, intranets, virtual private networks, wide area networks, local networks, private networks using communication protocols proprietary to one or more companies, Ethernet, WiFi (e.g., 802.81, 802.81b, g, n, or other such standards), and HTTP, and various combinations of the foregoing. Such communication may be facilitated by a device capable of transmitting data to and from other computers, such as modems (e.g., dial-up, cable or fiber optic) and wireless interfaces.</p><p id="p-0055" num="0054">Although <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows computing devices <b>410</b> and <b>460</b> as individual blocks, each of which contains its own processor and memory, the operations described herein may involve a single computing device or many computing devices. e.g., in the &#x201c;cloud.&#x201d; For example, various operations described herein as involving a single computing device (e.g., a single central processing unit (CPU) in a single server) may involve a plurality of computing devices (e.g., multiple processors in a load-balanced server farm). Similarly, memory components at different locations may store different portions of instructions <b>432</b> and collectively form a medium for storing the instructions. In some examples, device <b>460</b> may function as a thin client wherein device <b>410</b> performs all or nearly all operations that are not directly related to receiving and providing information to users via user input component <b>466</b> and display <b>465</b>. Various operations described herein as being performed by a computing device may be performed by a virtual machine. By way of example, instructions <b>432</b> may be specific to a first type of server, but the relevant operations may be performed by a second type of server running a hypervisor that emulates the first type of server. The operations may also be performed by a container, e.g., a computing environment that does not rely on an operating system tied to specific types of hardware.</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example distributed system handling a multi-site transaction. A client, for example client <b>110</b>, may send a request, for example, Request 1, to read and/or write a data item whose partitions are distributed at different datacenters, for example, at shards <b>146</b>, <b>156</b>, <b>166</b> at datacenters <b>142</b>, <b>152</b>, <b>162</b>, respectively. For clarity, shards <b>146</b>, <b>156</b>, <b>166</b> in this example are shown as located at different datacenters, hut as discussed above, various shards may also be stored in multiple servers in the same datacenter. The client <b>110</b> may choose one of servers storing one of the shards as a &#x201c;coordinator server,&#x201d; for example, the server storing shard <b>146</b> at datacenter <b>142</b>, while all the other servers storing the other shards of the transaction may be chosen as &#x201c;participant servers,&#x201d; for example, the servers storing shard <b>156</b> at datacenter <b>152</b> and shard <b>166</b> at datacenter <b>162</b>. When a commit message for the multi-site transaction is received at the coordinator server storing shard <b>146</b> and the participant servers storing shards <b>156</b> and <b>166</b>, the coordinator server storing shard <b>146</b> and participant, servers storing shards <b>156</b> and <b>166</b> may each acquire a respective lock for a time range. The time range for each of the locks may be from the time the respective locks are obtained until infinity or an upper bound. The participant servers may each locally choose a prepared timestamp (T1, T2) within the time range of the respective lock, write a prepared record, and notify the coordinator server storing shard <b>146</b>, as indicated by dotted arrows <b>510</b> and <b>520</b>, that they are prepared from their respective prepared timestamps (T1, T2) and onwards. The respective prepared timestamp may be chosen as the time at which the lock has been acquired locally, or alternatively may be chosen as the time at which information about the transaction has been recorded at the participant server. Instead of acquiring one lock, each of the servers may acquire multiple locks, for example, if the transaction requests changes to multiple cells of a shard, the server storing the shard may acquire a lock for each of the cells. Once the coordinator server storing shard <b>146</b> receives a prepared notification that all of the participant servers storing shards <b>156</b>, <b>166</b> are prepared, the coordinator server storing shard <b>146</b> may choose a commit timestamp (Tc) equal or greater than any of the prepared timestamps (T1, T2) and any commit timestamps that it has previously assigned to other transactions, and notifies each of the participant servers storing shards <b>156</b> and <b>166</b> of the commit timestamp (Tc), as indicated by dotted arrows <b>550</b> and <b>552</b>. The coordinator server storing shard <b>146</b> and each of the participant servers storing shards <b>156</b>, <b>166</b> then execute the transaction at the commit timestamp (Tc).</p><p id="p-0057" num="0056">A lock on a data item or a partition of a data item may be an exclusive kick or a shared lock. An exclusive lock does not permit any other transaction to access the same data item or partition, even if the other transactions only request a shared lick. Therefore, exclusive locks force transactions to be serialized. A shared lock on a data item or partition permits other transactions to access the same data item or partition also using shared locks. Therefore, shared locks promote parallelization of transactions, therefore increasing efficiency. A shared lock may be a reader shared lock or a writer shared lock. Writer shared locks may be used to provide efficient blind writes (e.g., writes that modify a value without reading it first), since multiple transactions may process in parallel. For example, as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the coordinator server storing shard <b>146</b> may be configured such that it cannot choose the same commit timestamp for any other transaction for which it is also acting as a coordinator, therefore, the coordinator server storing shard <b>146</b> may take a writer shared lock.</p><p id="p-0058" num="0057">Although in this example, the system's operations are shown with respect to shards <b>146</b>, <b>156</b>, and <b>166</b>, the same system's operations described above would be equally applicable with respect to shard replicas, for example, server groups storing groups <b>148</b>, <b>158</b>, and <b>168</b>, respectively, may operate the same way as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> as the servers storing shards <b>146</b>, <b>156</b>, and <b>166</b>, respectively.</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example distributed system handling two multi-site transactions where two different coordinator servers share the same participant server at a shared shard. As explained above, client <b>110</b> may send Request 1 to read and/or write a data item whose partitions are distributed at different datacenters, for example, at shards <b>146</b>, <b>156</b>, <b>166</b> at datacenters <b>142</b>, <b>152</b>, <b>162</b>, respectively. Client <b>110</b> chooses the server storing shard <b>146</b> at datacenter <b>142</b> as a first coordinator server, and the servers storing shard <b>156</b> at datacenter <b>152</b> and shard <b>166</b> at datacenter <b>162</b> as participant servers. Once the first coordinator server storing shard <b>146</b> receives a prepared notification from all of the participant servers storing shards <b>156</b>, <b>166</b> that they are prepared at their respective timestamps (T1, T2), as indicated by dotted arrows <b>610</b> and <b>620</b>, the first coordinator server storing shard <b>146</b> may choose a commit timestamp (Tc) to commit the transaction according to Request 1 and notify each of the participant servers storing shards <b>156</b> and <b>166</b> of the commit timestamp (Tc), as indicated by dotted arrows <b>650</b> and <b>652</b>. However, before the transaction according to Request 1 is committed, another client, for example client <b>120</b>, may also send a request, for example, Request 2, to read and/or write a data item whose partitions are distributed at different datacenters, for example, at shards <b>156</b>, <b>176</b>, and <b>186</b> at datacenters <b>152</b>, <b>172</b>, <b>182</b>, respectively. Client <b>120</b> may choose the server storing shard <b>176</b> at datacenter <b>172</b> as a second coordinator server, and servers storing shard <b>136</b> at datacenter <b>152</b> and shard <b>186</b> at datacenter <b>182</b> as participant servers. The second coordinator server storing shard <b>172</b> and participant servers storing shards <b>156</b> and <b>186</b> may each take a lock for a time range. The time range for each of the locks may be from the time the respective locks are obtained until infinity or an upper bound. The participant servers storing shards <b>156</b> and <b>186</b> may each locally choose a prepared timestamp (T3, T4) within the respective time range, write a prepared record, and notify the second coordinator server storing shard <b>176</b>, as indicated by dotted arrows <b>630</b> and <b>640</b>, that they are prepared from their respective prepared timestamps (T3, T4) and onwards. The respective prepared timestamp may be chosen as the time at which the lock has been acquired locally, or alternatively may be chosen as the time at which information about the transaction has been recorded at the participant server. Instead of acquiring one lock, each of the servers may acquire multiple locks, for example, if the transaction requests changes to multiple cells of a shard, the server may acquire a lock for each of the cells. Once the second coordinator server storing shard <b>176</b> receives a prepared notification that participant servers storing shards <b>156</b> and <b>186</b> are both prepared, the second coordinator server storing shard <b>176</b> may choose a commit timestamp (Tc2) equal or greater than the prepared timestamps (T3, T4) and any commit timestamps that it has previously assigned to other transactions, and notify each of the participant servers storing shards <b>156</b> and <b>186</b> of the commit timestamp (Tc2), as indicated by dotted arrows <b>660</b> and <b>662</b>. For clarity, shards <b>146</b>, <b>156</b>, <b>166</b>, <b>176</b>, and <b>186</b> in this example are shown as located at different datacenters, but as discussed above, various shards may also be stored in multiple servers in the same datacenter. Although in this example, the system's operations are shown with respect to shards <b>146</b>, <b>156</b>, <b>166</b>, <b>176</b>, and <b>186</b>, the system's operations described above would be equally applicable with respect to shard replicas (groups).</p><p id="p-0060" num="0059">However, there is a chance that the second coordinator server storing shard <b>176</b> chooses Tc2 that is the same as Tc chosen by the first coordinator server storing shard <b>146</b>, causing a &#x201c;timestamp collision&#x201d; at the participant server storing shard <b>156</b>, which is shared by both the first and the second coordinator servers for two different multi-site transactions. Such a timestamp collision at a shared shard at a shared participant server may cause problems. First, if the same commit timestamp is chosen for two transactions at the shared shard on the shared participant server, such as shard <b>156</b>, one transaction would overwrite the other such that a read at the commit timestamp Tc would only show one of the two transactions. Another consequence is that, if there is more than one shared participant servers for the two multi-site transactions, each of the shared participant servers may choose to execute the two transactions in a different order, thereby producing inconsistent results. Although it is shown that Request 1 and Request 2 come from different clients, they may also come from the same client.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example distributed system handling a multi-site transaction and a single-site transaction. As explained above, client <b>110</b> may send Request 1 to read and/or write a data item whose partitions are distributed at different datacenters, for example, at shards <b>146</b>, <b>156</b>, <b>166</b> at datacenters <b>142</b>, <b>152</b>, <b>162</b>, respectively. For clarity, shards <b>146</b>, <b>156</b>, <b>166</b> in this example are shown as located at different datacenters, but as discussed above, various shards may also be stored in multiple servers in the same datacenter. Client <b>110</b> chooses the server storing shard <b>146</b> at datacenter <b>142</b> as a coordinator server, and the servers storing shard <b>156</b> at datacenter <b>152</b> and shard <b>166</b> at datacenter <b>162</b> as participant servers. Once the coordinator server storing shard <b>146</b> receives a prepared notification that all of the participant servers storing shards <b>156</b>, <b>166</b> are prepared at their respective timestamps (T1,T2), as indicated by dotted arrows <b>710</b> and <b>720</b>, the coordinator server storing shard <b>146</b> may choose a commit timestamp (Tc) to commit the transaction according to Request 1 and notify each of the participant servers storing shards <b>156</b> and <b>166</b> of the commit timestamp (Tc), as indicated by dotted arrows <b>750</b> and <b>752</b>. However, another client, for example client <b>120</b>, may also send a request, for example Request 3, to read and/or write some data item stored only at shard <b>156</b> at datacenter <b>152</b>. A timestamp collision may also occur in this situation, if the participant server storing shard <b>156</b> chooses a commit timestamp (Tc3) for the single-site transaction according to Request 3 that happens to be the same as Tc chosen by the coordinator server storing shard <b>146</b>. As discussed above, one negative consequence of such a timestamp collision is that one transaction would overwrite the other such that a read at the commit, timestamp Tc would only show one of the two transactions. Although it is shown that Request 1 and Request 3 come from different clients, they may also come from the same client.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows an example distributed system configured to prevent a timestamp collision between two multi-site transactions at a shared participant server as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> by only allowing participant servers to take exclusive locks. For clarity, only the first coordinator server storing shard <b>146</b>, the second coordinator server storing shard <b>176</b>, and their shared participant server storing shard <b>156</b> are shown. The sequence of events at each server is depicted chronologically from top to bottom. The dotted line at the center separates events for Request 1 (left hand side) from events for Request 2 (right hand side). Further, the shaded region in the center shows events occurring at the shared participant server storing shard <b>156</b>, some of which are for Request 1 (left hand side of shaded region) and others are for Request 2 (right hand side of shaded region). The unshaded region on the left of the shaded region shows events at the first coordinator server storing shard <b>146</b> and the unshaded region on the right of the shaded region shows events at the second coordinator server storing shard <b>176</b>. Starting from the left hand side, Request 1 was received at the first coordinator server storing shard <b>146</b> at time 1234 &#x3bc;s and at shared participant server storing shard <b>156</b> at time 1235 &#x3bc;s. The first coordinator server storing shard <b>146</b> takes a lock for Request 1 with a time range from time 1235 &#x3bc;s to infinity. The shared participant server storing shard <b>156</b> then takes an exclusive lock for Request 1 with a time range from time 1236 &#x3bc;s until infinity. This is because at this point, the shared participant server storing shard <b>156</b> does not know what commit timestamp the first coordinator server storing shard <b>146</b> will ultimately choose. The shared participant server storing shard <b>156</b> sends its prepared timestamp (T1) of 1236 &#x3bc;s to the first coordinator server storing shard <b>146</b>. The first coordinator server storing shard <b>146</b> may also receive other prepared timestamps from other participant servers, such as time 1245 &#x3bc;s (T2) from participant server storing shard <b>166</b> (not shown here, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). The first coordinator sever storing shard <b>146</b> then chooses a commit timestamp (Tc), for example, 1248 &#x3bc;s, that is larger than both of the two prepared timestamps received (T1, T2) and any commit timestamps that the first coordinator server storing shard <b>146</b> has previously assigned to other transactions. The first coordinator server storing shard <b>146</b>, the shared participant server storing shard <b>156</b>, and the participant server storing shard <b>166</b> (not shown here, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>) each commits the transaction of Request 1 at commit timestamp 1248 &#x3bc;s. After that, the shared participant server storing shard <b>156</b> releases its exclusive lock fix Request 1 at time 1249 &#x3bc;s.</p><p id="p-0063" num="0062">Moving to the right hand side, although Request 2 was received at the second coordinator server storing shard <b>176</b> at time 1235 &#x3bc;s and at the shared participant server at time 1236 &#x3bc;s, and the second coordinator server storing shard <b>176</b> took a lock for Request 2 with a time range from time 1236 &#x3bc;s to infinity, because the shared participant server storing shard <b>156</b> had an exclusive lock for Request 1 and did not release it until time 1249 &#x3bc;s, nothing has happened for Request 2 at the shared participant server for shard <b>156</b> before time 1249 &#x3bc;s. The exclusive lock for Request 1 thus prevented shared participant server storing shard <b>156</b> from choosing a prepared timestamp (T3) for Request 2 that is smaller or equal to the commit timestamp (Tc) of Request 1, and since the second coordinator server storing shard <b>176</b> must choose a commit timestamp (Tc2) equal or greater than all the prepared timestamps it receives, this ensures that the second coordinator server storing shard <b>176</b> will choose a commit timestamp (Tc2) greater than the commit timestamp (Tc) for Request 1. Only when the exclusive lock was released at time 1249 &#x3bc;s, does the shared participant server storing shard <b>156</b> take an exclusive lock for Request 2 with a time range from 1250 &#x3bc;s to infinity, and sends the prepared timestamp (T3) of 1250 &#x3bc;s to the second coordinator server storing shard <b>176</b>. Once the second coordinator server storing shard <b>176</b> receives all the other prepared timestamps, for example, prepared timestamp (T4) of 1238 &#x3bc;s from the participant server storing shard <b>186</b> (not shown here, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>), it chooses commit timestamp (Tc2), for example at time 1252 &#x3bc;s, that is larger than both prepared timestamps (T3, T4) received and any commit timestamps that the second coordinator server storing shard <b>176</b> has previously assigned to other transactions. The second coordinator server storing shard <b>176</b>, the shared participant storing shard <b>156</b>, and the participant server storing shard <b>186</b> each commits the transaction of Request 2 at the commit timestamp (Tc2) at 1252 &#x3bc;s. After that, the shared participant server storing shard <b>156</b> releases its exclusive lock for Request 2 at time 1253 &#x3bc;s. As illustrated by the dotted arrow, during the exclusive lock for Request 1, parallel processing cannot be done for Request 2 at the shared participant shard <b>156</b>. Likewise, parallel processing would not have been possible at the shared participant server for shard <b>156</b> during the exclusive lock for Request 2. The locks taken by the first and second coordinator servers may be exclusive or shared.</p><p id="p-0064" num="0063">Instead of taking an exclusive lock with a range up to infinity, another example system may be configured such that the participant server takes an exclusive lock up to a predetermined upper bound. For example, the client may specify a maximum commit timestamp for the transaction. If the transaction does not commit by the maximum commit timestamp specified by the client, the transaction may be aborted. In this case, the upper bound for the time ranges mentioned earlier could be the client-specified maximum commit timestamp.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows another example distributed system configured to prevent a timestamp collision between two multi-site transactions as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> by applying restrictions to commit timestamps in such situations. For clarity, only the first coordinator server storing shard <b>146</b>, the second coordinator server storing shard <b>176</b> and their shared participant server storing shard <b>156</b> are shown. The sequence of events at each server is depicted chronologically from top to bottom. The dotted line at the center separates events for Request 1 (left hand side) from events for Request 2 (right hand side). Further, the shaded region in the center shows events occurring at the participant server storing shard <b>156</b>, some of which are for Request 1 (left hand side of shaded region) and others are for Request 2 (right hand side of shaded region). The unshaded region on the left of the shaded region shows events at the first coordinator server storing shard <b>146</b> and the unshaded region on the right of the shaded region shows events at the second coordinator server storing shard <b>176</b>. On the left hand side, Request 1 is received at the first coordinator server storing shard <b>146</b> at time 1234 &#x3bc;s and at the shared participant server storing shard <b>156</b> at time 1235 &#x3bc;s; on the right hand side, Request 2 is received at the second coordinator server storing shard <b>176</b> at 1235 &#x3bc;s and at the participant server storing shard <b>156</b> at time 1236 &#x3bc;s. On the left hand side, the first coordinator server storing shard <b>146</b> takes a lock for Request 1 with a time range from time 1235 &#x3bc;s to infinity; on the right hand side, the second coordinator server storing shard <b>176</b> takes a lock for Request 2 with a time range from time 1236 &#x3bc;s to infinity. Next, on the left hand side, the shared participant server storing shard <b>156</b> takes a shared lock for Request 1 with a time range starting at time 1236 &#x3bc;s until infinity and send its prepared timestamp (T1) of 1236 &#x3bc;s to the first coordinator server storing shard <b>146</b>; on the right hand side, the shared participant server storing shard <b>156</b> takes a shared lock for Request 2 with a time range starting at time 1237 &#x3bc;s until infinity and send its prepared timestamp (T3) of 1236 &#x3bc;s to the wooed coordinator server storing shard <b>176</b>. Each of the first and second coordinator servers may receive other prepared timestamps from other participant servers too, on the left hand side, the first coordinator server storing shard <b>146</b> also receives prepared timestamp (T2) of 1245 &#x3bc;s from the participant server storing shard <b>166</b> (not shown here, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>); on the right hand side, the second coordinator server storing shard <b>176</b> also receives prepared timestamp (T4) of 1238 &#x3bc;s from the participant server storing shard <b>186</b> (not shown here, shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>). Similarly as discussed above with respect to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, instead of taking locks up to infinity, alternatively the locks may be taken up to a predetermined upper bound. The locks taken by the first and second coordinator servers may be exclusive or shared.</p><p id="p-0066" num="0065">To prevent a timestamp collision, the first and second coordinator servers may each assume that at least one of the participant servers in their respective multi-site transactions is shared with another coordinator server, and that there may be at least one shared shard on the shared participant server, and chooses a restricted commit timestamp for their respective multi-site transactions. In the current example, participant server storing shard <b>156</b> is a shared participant, and shard <b>156</b> is the shared shard for Request 1 and Request 2. The first and second coordinator servers then each chooses a restricted commit timestamp for their respective transactions such that any other coordinator sharing the participant server storing shard <b>156</b> cannot select the same commit timestamp for another transaction, even if it were allowed to take a shared lock on behalf of that transaction. Alternatively, the participant servers in a transaction, such as participant server storing shard <b>136</b> in Request 1, may be configured to notify the coordinator server in that transaction, such as the first coordinator server storing shard <b>146</b> in Request 1, when it is being shared with another coordinator server for another transaction at a shared shard, such as the second coordinator server storing shard <b>176</b> of Request 2, and only when notified, the coordinator servers, such as the first and second coordinator servers, may choose a restricted commit timestamp for their respective transactions. By restricting the commit timestamps that coordinator servers may choose, the participant servers may take shared locks, instead of exclusive locks, which increases the efficiency of the system.</p><p id="p-0067" num="0066">In one example, the commit timestamp may be restricted by a hash value of a transaction ID, where the transaction ID is unique to the transaction. For example, the commit timestamp for a transaction may be restricted such that its lower N-bits (e.g., 3-bits, 6-bits, 10-bits, 20-bits, etc.) must equal to a hash value of the transaction ID. Referring again to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, if Request 1 has a transaction ID that hashes to 100000, and the commit timestamp (Tc) is restricted such that its lower 6-bits must equal to the hash value of the transaction ID of Request 1, then the commit timestamps (Tc) for Request 1 are restricted to 1248 &#x3bc;s (or 10011100000), 1312 &#x3bc;s (or 10100100000), 1376 &#x3bc;s (or 10101100000), etc. The first coordinator server storing shard <b>146</b> is configured to compute a maximum of all the prepared timestamps it received from the participant servers of Request 1 and any commit timestamps it has previously assigned to other transactions, and then computes the next timestamp higher than this maximum that also has its lower 6-bits equal to 100000. Similarly, if Request 2 has a transaction ID that hashes to 010111, and the commit timestamp (Tc2) is restricted such that its lower 6-bits must equal to the hash value of the transaction ID of Request 2, then the commit timestamps (Tc2) for Request 2 are restricted to 1239 &#x3bc;s (or, 10011010111), 1303 &#x3bc;s (or 10100010111), 1367 &#x3bc;s (or 10101010111), etc. The second coordinator server storing shard <b>176</b> is configured to compute a maximum of all the prepared timestamps it received from the participant servers of Request 2 and any commit timestamps it has previously assigned to other transactions, and then computes the next timestamp higher than this maximum that also has its lower 6-bits equal to 010111. Here, the first coordinator server storing shard <b>146</b> chooses commit timestamp (Tc) of 1248 &#x3bc;s and the second coordinator server storing shard <b>176</b> chooses commit timestamp (Tc2) at 1239 &#x3bc;s. Request 1 is then committed at the first coordinator server storing shard <b>146</b>, shared participant server storing shard <b>156</b>, and participant server storing shard <b>166</b> at 1248 &#x3bc;s. Request 2 is committed at the second coordinator server storing shard <b>176</b>, shared participant server storing shard <b>156</b>, and participant server storing shard <b>186</b> at 1239 &#x3bc;s. Once the transactions are committed, each of the savers then releases their respective lock. Thus, this example system is configured to ensure that the transaction according to Request 1 and the transaction according to Request 2 cannot commit at the same commit timestamp.</p><p id="p-0068" num="0067">It is still possible, however, that two unique transaction IDs may hash to the same value, causing a &#x201c;hush collision,&#x201d; which may in turn cause a timestamp collision, since the commit timestamps for the two transaction would be subject to the same restriction. The probability of collision depends on the value of N. For example, if N=0, meaning that there is no restriction, then a collision would occur at a shared participant server for two transaction if both coordinator servers choose to commit at the same time; if N=6, then a collision would occur at a shared participant server for two transactions only if the 6-bit hash values of the two transaction IDs are the same and both of the coordinator servers commit at the same time; if N=10, then a collisions would occur at a shared participant server for two transactions only if the 10-bit hash values of the two transaction IDs are the same and both of the coordinator servers commit at the same time; and so on. Therefore, by choosing a larger N, the probability of collision would be decreased. However, choosing a larger N has a trade off, as N gets larger, the spacing between available commit timestamps for the coordinator server also increase, therefore, choosing a larger N also means that the coordinator server might have to wait longer to choose a commit timestamp. For example, if N=0, the coordinator server may choose any time equal or larger than the prepared timestamps as the commit timestamp; if N=6, the coordinator server must wait to choose the next commit timestamp that has the 6-bit hash pattern, which could be up to 64 &#x3bc;s; if N=10, the coordinator server must wait to choose the next commit timestamp that has the 10-bit hash pattern, which could be up to 1.024 ins; and so on. Thus, the value of N may be optimally chosen according to specifics of the distributed system, for example, the desired latency and throughput of the system. To achieve a lower latency, N may be lowered so that the coordinator does not have to wait a long time. To achieve a higher throughput, more transactions need to be processed in parallel and allowed to share locks, therefore, N may be increased to prevent collisions between the many parallel transactions.</p><p id="p-0069" num="0068">One example system is configured to prevent a timestamp collision in such situations by using exclusive locks. For example, if transaction ID for Request 1 and transaction ID for Request 2 both hashes to the same value of 100000, an exclusive lock may be taken for Request 1 by the shared participant server at shard <b>156</b>, as depicted on the left hand side of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. However, once the exclusive lock is released at time 1249 &#x3bc;s, meaning that the transaction according to Request 1 that was causing the hash collision has been completed, the shared participant server storing shard <b>156</b> may proceed with Request 2 with a shared lock in the same way as depicted on the right hand side of <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0070" num="0069">Another example system configured to prevent a timestamp collision in a hash collision situation does so by only allowing the shared participant server to take a writer-shared restrictive lock for one transaction, and put the other transaction in a waiting queue until the participant server releases the writer-shared restrictive lock on the first transaction. For example, referring to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the shared participant server storing shard <b>156</b> may proceed with Request 1 as depicted on the left hand side of <figref idref="DRAWINGS">FIG. <b>9</b></figref> by taking a writer-shared restrictive lock for Request 1, and places Request 2 in a waiting queue until shared participant server storing shard <b>156</b> releases the writer-shared restrictive lock at time 1249 &#x3bc;s. The writer-shared restrictive lock may prevent other transactions having the same hash value (in this example, having the same restricted commit timestamp bit-pattern) from taking a lock at the shared participant server at shard <b>156</b>, but allow other transactions having different hash values (in this example, having different restricted commit timestamp bit-patterns) to take writer-shared locks at the shared participant server at shard <b>156</b>. This ensures that shared participant server storing shard <b>156</b> would choose a prepared timestamp (T3) for Request 2 that is greater than the commit timestamp (Tc) of Request 1, which ensures that the commit timestamp (Tc2) for Request 2 would be greater than the commit timestamp (Tc) of Request 1. Further, since writer-shared restrictive locks, instead of exclusive locks, are used in this example, efficiency is increased as a result of allowing parallel processing of transactions having different hash values.</p><p id="p-0071" num="0070">Another aspect of this example system restricting commit timestamps by a hash value of a transaction ID relates to efficiency. For example, if a coordinator server receives multiple requests for multi-site transactions involving a shared participant server, the coordinator server would have to choose a restricted commit timestamp for each of these multi-site transactions. As seat in the example above depicted in <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>9</b></figref>, Request 1 chose the server storing shard <b>146</b> as the coordinator server and commit timestamps (Tc) for Request 1 are restricted to 1248 &#x3bc;s (or 10011100000), 1312 &#x3bc;s (or 10100100000), 1376 &#x3bc;s (or 10101100000), etc., suppose there is also a Request 4 that also chose the server storing shard <b>146</b> as the coordinator server with commit timestamps (Tc4) restricted to 1281 &#x3bc;s (or 10100000001), 1345 its (or, 10101000001), 1473 &#x3bc;s (or 10111000001), etc., another Request 5 that also chose the server storing shard <b>146</b> as the coordinator server with commit timestamps (Tc5) restricted to 1258 &#x3bc;s (or 10011101010), 1322 &#x3bc;s (or 10100101010), 1386 &#x3bc;s (or 10101101010), etc., and yet another Request 6 that chose the server storing shard <b>146</b> as the coordinator with commit timestamps (Tc6) restricted to 1280 &#x3bc;s (or 10100000000), 1344 &#x3bc;s (or 10101000000), 1472 &#x3bc;s (or 101110000(0), etc. Thus, if the coordinator server storing shard <b>146</b> chooses to commit the transactions in the order of Request 1-Request 4-Request 5-Request 6, the commit timestamps would be 1248 &#x3bc;s, 1281 &#x3bc;s, 1322 &#x3bc;s, 1344 &#x3bc;s with a total spacing of 33 &#x3bc;s+41 &#x3bc;s+22 &#x3bc;s=964 &#x3bc;s; but if the coordinator server storing shard <b>146</b> chooses to commit the transactions in the order of Request 5-Request 4-Request 1-Request 6, the commit timestamps would be 1258 &#x3bc;s, 1281 &#x3bc;s, 1312 &#x3bc;s, 1344 &#x3bc;s, with a total spacing of 23 &#x3bc;s+31 &#x3bc;s+32 &#x3bc;s=86 &#x3bc;s, and so on. Thus, the order that the timestamps are chosen have an effect on the efficiency.</p><p id="p-0072" num="0071">In one example, a server that receives requests to commit multiple transactions with restricted commit timestamps and for which it is functioning as a coordinator may choose a commit timestamp for each of the transactions such that a total spacing between all the commit timestamps is substantially minimized. For example, continuing from the example above, the coordinator server storing shard <b>146</b> may first determine that the smallest commit timestamp that it may choose for all the transactions to be committed is 1248 &#x3bc;s for Request 1. The coordinator server storing shard <b>146</b> then arranges the lower 6-bits patterns for the other transactions, Requests 4, 5, 6, in an increasing order, which is 000000 (Request 6), 000001 (Request 4), 101010 (Request 5). The coordinator server storing shard <b>146</b> then rotates the first 6-bit pattern to the back of the ordered list until the first 6-bit pattern in the list is greater than the lower 6-bits pattern of Request 1. Thus, after 2 rotations, the list becomes 101010 (Request 5), 000000 (Request 6), 000001 (Request 4). The coordinator server storing shard <b>146</b> then chooses the next three commit timestamps in the order corresponding to this list, Request 5-Request 6-Request 4. This substantially minimizes the wait time between transactions at the same coordinator server and thus the latency with which transactions commit, therefore increasing overall efficiency. In the above example, if the coordinator server storing shard <b>146</b> chooses timestamps in the order of Request 1-Request 5-Request 6-Request 4, the commit timestamps would 1248 &#x3bc;s, 1258 &#x3bc;s, 1280 &#x3bc;s, 1281 &#x3bc;s, with a total spacing of 10 &#x3bc;s+22 &#x3bc;s+1=33 &#x3bc;s. The ordering method described above is not limited to ordering multi-site transactions, if the coordinator server also received requests to commit single-site transactions, the coordinator server may order the single-site transactions along with the multi-site transactions to substantially minimize a total spacing between all the commit timestamps, therefore further increasing overall efficiency.</p><p id="p-0073" num="0072">In other example systems, instead of commit timestamps being restricted by transaction IDs, coordinator servers that share a participant server may be restricted in other ways when selecting a commit timestamp such that any other coordinator server sharing the participant at a shared shard cannot select the same commit timestamp for another transaction, even if it were allowed to take a shared lock on behalf of that transaction. For example, the commit timestamps may be restricted by a coordinator server ID identifying the coordinator server in a similar fashion as described above with respect to transaction ID. For another example, coordinator servers with shared participant servers may be configured to communicate with each other to ensure that unique commit timestamps are chosen. For example, coordinator servers that share a participant server may send messages to each other to agree on different commit timestamps for their respective transactions, in another example system, a global manager may choose all commit timestamps to ensure that commit timestamps are unique.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example distributed system configured to prevent a timestamp collision between a multi-site transaction and a single-site transaction as shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, by applying restrictions to commit timestamps in such situations at the participant. For clarity, only one coordinator server storing shard <b>146</b> and one participant server storing shard <b>156</b> are shown. The sequence of events at each server is depicted chronologically from top to bottom. The dotted line at the center separates events for Request 1 (left hand side) from events for Request 2 (right hand side). Further, the shaded region in the center shows events occurring at the participant server storing shard <b>156</b>, some of which are for Request 1 (left hand side of shaded region) and others are for Request 2 (right hand side of shaded region). The unshaded region on the left of the shaded region shows events at the coordinator server storing shard <b>146</b>. On the left hand side, Request 1 for a multi-site transaction was received at coordinator server storing shard <b>146</b> at time 1234 &#x3bc;s and participant server storing shard <b>156</b> at time 1235 &#x3bc;s; on the right hand side, Request 3 for a single-site transaction was received at participant server storing shard <b>156</b> at time 1235. The coordinator server storing shard <b>146</b> takes a lock for Request 1 with a time range from time 1235 &#x3bc;s to infinity. Next, on the left hand side, the participant server storing shard <b>156</b> takes a shared lock for Request 1 for a time range starting at time 1236 &#x3bc;s until infinity and sends the prepared timestamp (T1) of 1236 &#x3bc;s to coordinator server storing shard <b>146</b>; on the right hand side, the participant server storing shard <b>156</b> then takes a shared lock for Request 3 for a range starting at time 1237 &#x3bc;s until infinity. The coordinator server storing shard <b>146</b> may receive prepared timestamps from all other participant servers too, such as prepared timestamp (T2) of 1245 &#x3bc;s from participant server storing shard <b>166</b> (not shown here, shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>). Similarly as discussed above with respect to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, instead of taking locks up until infinity, alternatively the locks may be taken from up to a predetermined upper bound. The lock taken by the coordinator server storing shard <b>146</b> may be exclusive or shared.</p><p id="p-0075" num="0074">The coordinator server storing shard <b>146</b> may choose a commit timestamp (Tc), for example 1248 &#x3bc;s, which is greater or equal to any of the prepared timestamps (T1, T2) it received and any commit timestamps it has previously assigned to other transactions, and has a lower 6-bit pattern of 100000. However, on the right hand side, as discussed above, if the participant server storing shard <b>156</b> freely chooses any commit timestamp (Tc3) for single-site transaction according to Request 3, there is a chance that the participant server storing shard <b>156</b> chooses the same commit timestamp (Tc) for Request 1 chosen by the coordinator server storing shard <b>146</b>. To avoid such a timestamp collision between a multi-site transaction and a single-site transaction at a participant server, the participant server may be configured to choose a restricted commit timestamp for the single-site transaction that a coordinator server would not choose for the multi-site transaction at that participant server.</p><p id="p-0076" num="0075">In one example, the commit timestamp for the single-site transaction may be restricted by a hash value of a transaction ID of the single-site transaction. For example, the participant server storing shard <b>156</b> may require that the commit timestamp (Tc3) for the single-site transaction to have its lower N-bits (e.g., 3-bits, 6-bits, 10-bits, 20-bits, etc.) equal to a hash value of a transaction ID of the single-site transaction. Referring again to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, if Request 3 has a transaction ID that hashes to 010101, and the commit timestamp is restricted such that its lower 6-bits must equal to the hash value of the transaction ID, then the commit timestamps for Request 3 are restricted to 1237 &#x3bc;s (or 10011010101), 1301 &#x3bc;s (or 10100010101), 1365 &#x3bc;s (or 10101010101), etc. Request 1 is then committed at coordinator saver storing shard <b>146</b> and participant server storing shard <b>156</b> at 1248 &#x3bc;s. Request 3 is committed at participant server storing shard <b>156</b> at 1237 &#x3bc;s. After the transactions are committed, the servers each release their respective lock. As noted above, such an example system ensures that the transaction according to Request 1 and the transaction according to Request 3 would not commit at the same commit timestamp as transaction according to Request 1.</p><p id="p-0077" num="0076">In an alternative example, the distributed system may be configured such that it treats single-site transactions differently from multi-site transactions to completely eliminate timestamp collisions between multi-site and single-site transactions. For example, instead of simply requiring that the commit timestamp for a multi-site transaction having a shared participant server to have lower N-bits equal to a hash value of the transaction ID, a coordinator server may require that commit timestamps for such multi-site transactions must have lower N bits=hash (transactionID) % 2<sup>N</sup>&#x2212;1. This way, multi-site transactions are never allowed to choose a commit timestamp with lower N-bits (e.g., 3-bits, 6-bits, 10-bits, 20-bits, etc.) equal to 2<sup>N</sup>&#x2212;1, which may be reserved exclusively for single-site transactions. Referring to <figref idref="DRAWINGS">FIG. <b>10</b></figref> again, since N=6, the single-site transaction according to Request 3 may only choose commit timestamps having lower 6 bits of 2<sup>6</sup>&#x2212;1 (or 111111), which means the commit timestamps for Request 3 are restricted to 1279 &#x3bc;s (or 10011111111), 1535 &#x3bc;s (or 10111111111), 2047 &#x3bc;s (or 111111111111), etc. In this example, since single-site transactions are required to have commit timestamps that follow an exclusive pattern different from all multi-site transactions, this may increase efficiency by eliminating the need to deal with possible timestamp collisions between single-site and multi-site transactions.</p><p id="p-0078" num="0077">The commit timestamps of a distributed database may be provided to a user in one or more columns of a table that the user may view. For example, when a user r quests a change to a data item in one or more main tables kept in the distributed database, the user may also want to record the change in another table that keeps track of all changes made in the main tables (a &#x201c;change log&#x201d;). For example, the user may request a transaction to modify some data item in the main table, and also include in that transaction a mutation to record the transaction in the change log. In such a change log, commit timestamps may be included in a primary key column, meaning that the commit timestamps must be unique, or some other column.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows an example distributed system. Here, a user of the distributed database is Big Bank, who maintains a number of tables distributed at multiple shards or groups, such as shards <b>146</b>, <b>156</b>, <b>166</b>. One table that Big Bank maintains, &#x201c;Accounts at Big Bank,&#x201d; is a summary of all accounts at Big Bank, which includes information such as Name of the account holder, the Account ID, the Account Type, and the Current Balance. Big Bank also maintains a change log for each of its accounts to keep track of all deposits and withdrawals made to the account. For example, a change log &#x201c;Adam Smith's Account History&#x201d; keeps a record of all deposits and withdrawals made to Adam Smith's account. Another table that Big Bank maintains is &#x201c;Big Bank Assets,&#x201d; which is a summary of the Bank's various assets, including Asset Type and Asset Amount.</p><p id="p-0080" num="0079">The example in <figref idref="DRAWINGS">FIG. <b>11</b></figref> further shows that Big Bank sends two requests, Request 1 and Request 2, to change certain data maintained in its database. Request 1, made on Feb. 8, 2018, at 12:29:30 PM, is a deposit to Adam Smith's account for 55,000; Request 2, made on the same day at 12:30:00 PM, is a withdrawal from Adam Smith's account for $2,000. Requests 1 and 2 both require that the Current Balance for Adam Smith in the table Accounts at Big Bank to be updated. For example, if Adam Smith's Current Balance before Request 1 was committed was $1,000, after Request 1 was completed the Current Balance would be $6,000, and after Request 2 was committed the Current Balance would be $4,000. Requests 1 and 2 also both require that the table Big Bank Asset to update the total amount of Checking Accounts at Big Bank. For example, Request 1 increases Amount for Checking Accounts by $5,000 and Request 2 reduces Amount for Checking Accounts by $2,000. Finally, Requests 1 and 2 both require that the table Adam Smith's Account History to be updated with the addition of these two transactions. For Adam Smith's Account History, Request 1 and Request 2 both also include an instruction to include commit timestamp, therefore, a column in Adam Smith's Account History would have the commit timestamp for Requests 1 and 2 inserted, which would be the time that all three tables&#x2014;Accounts at Big Bank, Big Bank Assets, and Adam Smith's Account History&#x2014;were updated according to Requests 1 and 2.</p><p id="p-0081" num="0080">As shown in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, commit timestamps of each transaction is provided in a column of Adam Smith's Account History, for example, 20180208123015123456 is the commit timestamp at which all three tables&#x2014;Accounts at Big Bank, Big Bank Assets, and Adam Smith's Account History&#x2014;were updated according to Request 1, and 20180208123017098765 is the commit timestamp at which all three tables were updated according to Request 2. Because the sequence of transactions is particularly important for bank transactions (for example, processing Request 2 before Request 1 would result in a negative balance for Adam Smith), here the commit timestamps are provided as primary key of Adam Smith's Account History. In another example, a placeholder may be placed in the commit timestamp column of Adam Smith's Account History as a request was made, and the value is updated once the commit timestamp is chosen.</p><p id="p-0082" num="0081">In a distributed system, for example as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, Accounts at Big Bank may be at shard <b>146</b>, Adam Smith's Account History may be at shard <b>156</b>, and Big Bank Assets may be at shard <b>166</b>. For Request 1, Big Bank may choose the server storing shard <b>146</b> as the first coordinator server and the servers storing shards <b>156</b> and <b>166</b> as the participant servers, while for Request 2, Big Bank may choose the server storing shard <b>166</b> as the second coordinator server and the servers storing shards <b>146</b> and <b>156</b> as the participant servers. If the first coordinator server storing shard <b>146</b> chooses a same commit timestamp for Request 1 as the second coordinator server storing shard <b>166</b> chooses for Request 2, at shared participant server storing shard <b>156</b>, the same commit timestamp would be entered for both transaction of Request 1 and transaction of Request 2. Because commit timestamp is the primary key of Adam Smith's Account History, same commit timestamp is not allowed for multiple rows, thus, one transaction would overwrite the other transaction and Adam Smith's Account History would not accurately represent the entire history of Adam Smith's account.</p><p id="p-0083" num="0082">As discussed above, unique commit timestamps may be achieved by one of the various example systems discussed above and illustrated in <figref idref="DRAWINGS">FIGS. <b>8</b>-<b>10</b></figref>. Although in the above example, the change log is at one shard, the change log may also be at multiple shards, but the example systems discussed above with respect to <figref idref="DRAWINGS">FIG. <b>8</b>-<b>10</b></figref> would be equally applicable. Likewise, in another example, the user may request a transaction that spans multiple databases, and include in that transaction request a mutation to update a separate change log for each database, and the example systems discussed above with respect to <figref idref="DRAWINGS">FIG. <b>8</b>-<b>10</b></figref> would be equally applicable. For example, continuing from the example in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, a transfer from Adam Smith's account to Bob Cat's account may require an update to the change log Adam Smith's Account History at shard <b>156</b> as well as an update to a change log Bob Cat's Account History at shard <b>176</b>.</p><heading id="h-0007" level="2">Example Methods</heading><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example method <b>1200</b> for providing unique commit timestamps to users. It should be understood that the following operations do not have to be performed in the precise order described below. Rather, various steps may be handled in a different order or simultaneously. Steps may also be added or omitted unless otherwise stated.</p><p id="p-0085" num="0084">In block <b>1210</b>, a request to commit a transaction is received at a first coordinator server and one or more participant servers in a distributed system.</p><p id="p-0086" num="0085">In block <b>1220</b>, a notification from each of the participant servers is received at the coordinator server, the notification including a respective prepared timestamp, the respective prepared timestamp being chosen within a time range during which the respective participant server obtained at least one lock.</p><p id="p-0087" num="0086">In block <b>1230</b>, a commit timestamp for the transaction equal or greater than each of the prepared timestamps is computed.</p><p id="p-0088" num="0087">In block <b>1240</b>, the commit timestamp is restricted such that a second coordinator server sharing at least one of the participant servers for one or more other transactions at a shared shard cannot select the same commit timestamp for any of the other transactions.</p><p id="p-0089" num="0088">In block <b>1250</b>, the transaction is committed at the commit timestamp at the first coordinator server and each of the participant servers.</p><p id="p-0090" num="0089">The technology is advantageous because it provides meaningful commit timestamps to users without significantly compromising the throughput of a distributed database. Although the user's database may be highly distributed, the user may use the commit timestamps to view changes made to various data items as if the database was kept on a single machine. The technology further provides various methods to increase efficiency, for example, by avoiding exclusive locks, minimizing spacing between commit timestamps, and providing different treatments for multi-site and single-site transactions.</p><p id="p-0091" num="0090">Unless otherwise stated, the foregoing alternative examples are not mutually exclusive, but may be implemented in various combinations to achieve unique advantages. As these and other variations and combinations of the features discussed above can be utilized without departing from the subject maser defined by the claims, the foregoing description of the embodiments should be taken by way of illustration rather than by way of limitation of the subject matter defined by the claims. In addition, the provision of the examples described herein, as well as clauses phrased as &#x201c;such as,&#x201d; &#x201c;including&#x201d; and the like, should not be interpreted as limiting the subject matter of the claims to the specific examples; rather, the examples are intended to illustrate only one of many possible embodiments. Further, the same reference numbers in different drawings can identify the same or similar elements.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-01-23" num="01-23"><claim-text><b>1</b>-<b>23</b>. (canceled)</claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. A computer-implemented method, comprising:<claim-text>receiving, at one or more participant servers in a distributed system at a first time, a first request to commit a first transaction;</claim-text><claim-text>providing, by the one or more participant servers, a notification including a respective prepared timestamp, the respective prepared timestamp being chosen within a time range for which the respective participant server obtained at least one first lock; and</claim-text><claim-text>committing, at each of the participant servers, the first transaction at a first commit timestamp, wherein the first commit timestamp is restricted such that the participant servers cannot use the same timestamp for one or more other transactions.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the at least one lock is a writer shared lock.</claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising releasing the at least one first lock after committing the first transaction.</claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The method according to <claim-ref idref="CLM-00026">claim 26</claim-ref>, further comprising:<claim-text>receiving, at the one or more participant servers in the distributed system at a second time, a second request to commit a second transaction; and</claim-text><claim-text>obtaining, by the one or more participant servers, a second lock for the second transaction after releasing the at least one first lock.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The method according to <claim-ref idref="CLM-00027">claim 27</claim-ref>, further comprising committing the second transaction at a second commit timestamp that is later than the first commit timestamp.</claim-text></claim><claim id="CLM-00029" num="00029"><claim-text><b>29</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising:<claim-text>receiving, at the one or more participant servers in the distributed system at a second time, a second request to commit a second transaction;</claim-text><claim-text>obtaining, by the one or more participant servers, a second lock for the second transaction; and</claim-text><claim-text>committing, by the one or more participant servers, the second transaction prior to committing the first transaction.</claim-text></claim-text></claim><claim id="CLM-00030" num="00030"><claim-text><b>30</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the transaction has a transaction ID, and further comprising:<claim-text>determining, at a shared participant server that is shared between at least two coordinator servers, that a hash value of the transaction ID is equal to a hash value for at least one of the other transactions; and</claim-text><claim-text>preventing, at the shared participant server, at least one of the other transactions having the same hash value from taking a lock until the transaction commits.</claim-text></claim-text></claim><claim id="CLM-00031" num="00031"><claim-text><b>31</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising:<claim-text>determining, at one or more of the participant servers, that a single-site transaction is to be committed at the participant server;</claim-text><claim-text>computing a single-site commit timestamp having a predetermined pattern for the single-site transaction, the predetermined pattern being one that any multi-site transaction cannot choose as its commit timestamp; and</claim-text><claim-text>committing the single-site transaction at the single-site commit timestamp.</claim-text></claim-text></claim><claim id="CLM-00032" num="00032"><claim-text><b>32</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>, further comprising:<claim-text>determining, at one or more of the participant servers, that a single-site transaction is to be committed at the participant server; and</claim-text><claim-text>computing a hash value of a transaction ID of the single-site transaction; and</claim-text><claim-text>restricting the commit timestamp for the single-site transaction by the hash value of the transaction ID of the single-site transaction.</claim-text></claim-text></claim><claim id="CLM-00033" num="00033"><claim-text><b>33</b>. The method according to <claim-ref idref="CLM-00032">claim 32</claim-ref>, further comprising:<claim-text>wherein restricting the commit timestamp for the single-site transaction comprises setting a predetermined number of lower bits of the commit timestamp of the single-site transaction to be equal to the hash value of the transaction ID of the single-site transaction.</claim-text></claim-text></claim><claim id="CLM-00034" num="00034"><claim-text><b>34</b>. The method according to <claim-ref idref="CLM-00024">claim 24</claim-ref>,<claim-text>wherein the request to commit a transaction further includes a mutation to update a change log recording the transaction.</claim-text></claim-text></claim><claim id="CLM-00035" num="00035"><claim-text><b>35</b>. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>,<claim-text>wherein the commit timestamp is included as a primary key of the change log.</claim-text></claim-text></claim><claim id="CLM-00036" num="00036"><claim-text><b>36</b>. The method of <claim-ref idref="CLM-00034">claim 34</claim-ref>,<claim-text>wherein at least part of the change log is stored at one of the participant servers.</claim-text></claim-text></claim><claim id="CLM-00037" num="00037"><claim-text><b>37</b>. The method of <claim-ref idref="CLM-00024">claim 24</claim-ref>, wherein the participant server is one of a plurality of servers in a distributed computing environment, and wherein the distributed system comprises a relational database in which the commit timestamp is stored as a primary key in the relational database.</claim-text></claim><claim id="CLM-00038" num="00038"><claim-text><b>38</b>. A computer-implemented method, comprising:<claim-text>receiving, at one or more participant servers in a distributed system, a request to commit a transaction;</claim-text><claim-text>obtaining, by each of the participant servers, at least one exclusive lock for a time range starting at a locally chosen starting time to a predetermined upper bound;</claim-text><claim-text>providing, by each of the participant servers to a coordinator server, a notification that the respective participant server is prepared at a respective prepared timestamp chosen within the respective time range;</claim-text><claim-text>committing, at each of the participant servers, the transaction at a commit timestamp that is equal or greater than each of the prepared timestamps; and</claim-text><claim-text>releasing, at each of the participant servers, the at least one exclusive lock.</claim-text></claim-text></claim><claim id="CLM-00039" num="00039"><claim-text><b>39</b>. The method according to <claim-ref idref="CLM-00038">claim 38</claim-ref>,<claim-text>wherein the predetermined upper bound is infinity.</claim-text></claim-text></claim><claim id="CLM-00040" num="00040"><claim-text><b>40</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>,<claim-text>wherein the request to commit a transaction further includes a mutation to update a change log recording the transaction.</claim-text></claim-text></claim><claim id="CLM-00041" num="00041"><claim-text><b>41</b>. The method of <claim-ref idref="CLM-00038">claim 38</claim-ref>, wherein the participant server is one of a plurality of servers in a distributed computing environment, and wherein the distributed system comprises a relational database in which the commit timestamp is stored as a primary key in the relational database.</claim-text></claim><claim id="CLM-00042" num="00042"><claim-text><b>42</b>. A system, comprising:<claim-text>a participant server, the participant server comprising one or more processors configured to:<claim-text>determine that a single-site transaction is to be committed at the participant server;</claim-text><claim-text>compute a single-site commit timestamp having a predetermined pattern for the single-site transaction, the predetermined pattern being one that any multi-site transaction cannot choose as its commit timestamp; and</claim-text><claim-text>commit the single-site transaction at the single-site commit timestamp.</claim-text></claim-text></claim-text></claim><claim id="CLM-00043" num="00043"><claim-text><b>43</b>. The system of <claim-ref idref="CLM-00042">claim 42</claim-ref>, wherein the participant server is one of a plurality of servers in a distributed computing environment, and wherein the distributed computing environment comprises a relational database in which the commit timestamp is stored as a primary key in the relational database.</claim-text></claim></claims></us-patent-application>