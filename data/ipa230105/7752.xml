<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007753A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007753</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17779593</doc-number><date>20101123</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>19212133.3</doc-number><date>20191128</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>05</class><subclass>B</subclass><main-group>47</main-group><subgroup>115</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>05</class><subclass>B</subclass><main-group>47</main-group><subgroup>155</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>H</section><class>05</class><subclass>B</subclass><main-group>47</main-group><subgroup>115</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>H</section><class>05</class><subclass>B</subclass><main-group>47</main-group><subgroup>155</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">A CONTROLLER FOR TRAINING A MACHINE FOR AUTOMATIZING LIGHTING CONTROL ACTIONS AND A METHOD THEREOF</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SIGNIFY HOLDING B.V.</orgname><address><city>EINDHOVEN</city><country>NL</country></address></addressbook><residence><country>NL</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ALIAKSEYEU</last-name><first-name>DZMITRY VIKTOROVICH</first-name><address><city>EINDHOVEN</city><country>NL</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>VAN DE SLUIS</last-name><first-name>BARTEL MARINUS</first-name><address><city>EINDHOVEN</city><country>NL</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>BORRA</last-name><first-name>TOBIAS</first-name><address><city>EINDHOVEN</city><country>NL</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/083028</doc-number><date>20101123</date></document-id><us-371c124-date><date>20220525</date></us-371c124-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for training a machine for automatizing lighting control actions, wherein the method comprises the steps of: controlling one or more lighting devices based on a first set of control parameters; controlling the one or more lighting devices based on a second set of control parameters; wherein the second set of control parameters is different from the first set of control parameters; detecting presence of a user based on a presence signal output from a presence sensing means; monitoring a response of the user related to the second set of control parameters; wherein the response is monitored during a time period; evaluating feedback of the user based on the monitored response; wherein the feedback is positive if no active response has been monitored; training the machine based on the evaluated feedback.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="182.63mm" wi="132.16mm" file="US20230007753A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="214.97mm" wi="134.20mm" file="US20230007753A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="161.37mm" wi="120.65mm" file="US20230007753A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.64mm" wi="122.17mm" file="US20230007753A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="196.77mm" wi="129.03mm" file="US20230007753A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">FIELD OF THE INVENTION</heading><p id="p-0002" num="0001">The invention relates to a method for training a machine for automatizing lighting control actions. The invention further relates to a controller and a lighting system for training a machine for automatizing lighting control actions.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Connected lighting refers to a system of one or more lighting devices which are controlled not by (or not only by) a traditional wired, electrical on-off or dimmer circuit, but rather by using a data communications protocol via a wired or more often wireless connection, e.g. a wired or a wireless network. These connected lighting networks form what is commonly known as the Internet of Things (IoT) or more specifically the Internet of Lighting (IoL). Typically, the lighting devices, or even individual lamps within a lighting device, may each be equipped with a wireless receiver or transceiver for receiving lighting control commands from a lighting control device according to a wireless networking protocol such as Zigbee, Wi-Fi or Bluetooth.</p><p id="p-0004" num="0003">Generally, these (connected) lighting systems are pre-programmed with recommended sets of lighting parameters, which are usually based on manual rules and on sensor coupling. These parameters are selected for achieving a desired light effect on an &#x2018;average&#x2019; person in an &#x2018;average&#x2019; environment. Machine learning algorithms can be used to optimize light effects for a user based on the user feedback. For finding an optimal light effect, such optimization algorithms require explicit feedback from the user for each automatic action it takes. This approach however can be tedious for the user.</p><p id="p-0005" num="0004">WO 2013/102881 discloses a method for determining user preference in light management of an area. The method comprises the sets of collecting data regarding artificial and natural light within the area, collecting data regarding weather conditions and collating the collected data with regard to temporal and geographical information associated with the space. The collected data is processed using stochastic processes to estimate the user's preferences at a specific time. Artificial lighting and natural lighting are adjusted based on the estimated user preferences. User inputs may be provided to indicate the user's satisfaction and/or dissatisfaction with the estimate. User preference setting are updated and maintained when the user is satisfied with the results.</p><heading id="h-0003" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0006" num="0005">It is an object of the present invention to provide optimal light effects for a user in an environment. It is a further object to provide an improved non-obtrusive feedback and learning mechanism based on user preference with a minimal user interaction (feedback), such that the user preference is obtained in a more natural way.</p><p id="p-0007" num="0006">According to a first aspect, the object is achieved by a method for training a machine for automatizing lighting control actions, wherein the method comprises the steps of: controlling one or more lighting devices based on a first set of control parameters; controlling the one or more lighting devices based on a second set of control parameters; wherein the second set of control parameters is different from the first set of control parameters; detecting presence of a user based on a presence signal output from a presence sensing means; monitoring a response of the user related to the second set of control parameters; wherein the response is monitored during a time period; evaluating feedback of the user based on the monitored response; wherein the feedback is positive if no active response has been monitored; training the machine based on the evaluated feedback.</p><p id="p-0008" num="0007">The method comprises controlling one or more lighting devices based on a first and a second set of control parameters, respectively. The first and the second set of control parameters are (perceivably) different. For instance, when a first and a second light effect are rendered based on the first and the second set of control parameters, respectively, the difference between the first light effect and the second light effect is perceivable by a user. For example, the second set of control parameters may be selected with a magnitude substantially different from the first set of control parameters resulting in a perceivable difference between the first and the second light effects. In another example, the duration of rendering the first and the second light effects is selected such that the difference becomes perceivable by a user. In an example, the controlling of the one or more lighting devices based on the first or the second set of control parameters may provide no light output from the one or more lighting devices, i.e. the one or more lighting devices are powered off. In this example, the perceivably difference may comprise from no light output to a light effect. The first and the second light effects may be rendered in an environment, such as an office, a factory, a house, etc.</p><p id="p-0009" num="0008">The method further comprises detecting presence of a user based on a presence signal output from a presence sensing means. The presence detection may be based on one or more of: image recognition, RF-based presence sensing, infrared detection, radar, acoustic sensing, etc. The presence detection of a user may be based on a communication device signal, such as mobile phone, computer, etc. assumed to be co-located with the user. RF-based sensing and/or image recognition may be used if the user does not carry a communication device and does not transmit or receive any signal. The presence detection may be performed in the same environment where the one or more lighting devices have been controlled based on the first and the second set of control parameters. The presence detection may be performed in the same environment where the first and the second light effects have been rendered. The method further comprises monitoring a response of the user related to the second set of control parameters. The monitoring may be performed during a time period. The monitoring may comprise observing the user reaction to the second set of control parameters. For monitoring the response, the presence of user is an important step. The time period may be predetermined or may be chosen on the fly.</p><p id="p-0010" num="0009">The method further comprises evaluating feedback of the user based on the monitored response. The feedback is considered as positive if no active response has been monitored while the user presence has been detected. In an example, if the user is pleased with the rendered light effect, (s)he will keep the rendered light effect and will not &#x2018;react&#x2019; to it, e.g. by not changing it. The &#x2018;inaction&#x2019; of the user related to the second set of control parameters represents a positive feedback. Since the machine is trained based on the evaluated feedback which comprises &#x2018;inaction&#x2019; as a positive feedback, while the user presence has been detected, a more natural and improved non-obtrusive method is provided to train a machine for optimizing light effect.</p><p id="p-0011" num="0010">In an embodiment, controlling of the one or more lighting devices based on the second set of control parameters may be triggered based on the user presence detection.</p><p id="p-0012" num="0011">In an example, when the presence of the user is detected then subsequently the one or more lighting devices are controlled based on the second set of control parameters. In this example, the user is advantageously present when the second light effect is rendered. Therefore, (s)he may differentiate between the first and the second light effects and may provide his/her feedback on the second light effect. In an example, the one or more lighting devices are not providing illumination (powered off), and then based on the user presence; the one or more lighting devices are controlled to render a second light effect.</p><p id="p-0013" num="0012">In an embodiment, the time period may start upon detecting the user presence and controlling the one or more lighting devices based on the second set of control parameters. In an embodiment the time period may be ceased when the presence is no longer detected.</p><p id="p-0014" num="0013">The time period to monitor the response of the user may be based on the user presence detection and on controlling of the one or more lighting devices based on the second set of control parameters. In an example, the time period may start when presence of the user is detected in the environment and when the second light effect is rendered in the same environment. These both conditions, i.e. the user presence and controlling of the one or more lighting devices based on the second set of control parameters should be satisfied for starting of the time period. To assign a positive feedback to user inaction, the user must be present. Furthermore, the feedback should be based on the rendered second light effect. Therefore, the time period is advantageously started upon detecting the user presence and controlling the one or more lighting devices based on the second set of control parameters. In an example, the time period may be ceased when the presence is no longer detected. For instance, when the user leaves the environment, the &#x2018;inaction&#x2019; does not anymore represent the user preference. The time period may be ceased when the controlling the one or more lighting devices to render the second light effect has been ceased.</p><p id="p-0015" num="0014">In an embodiment, the method may further comprise detecting an activity of the user; and the time period may be based on the detected activity such that the time period may be ceased when the activity is no longer detected.</p><p id="p-0016" num="0015">When the user is not active, e.g. (s)he is sleeping, said inactivity does not represent the user preference related to the rendered second light effect. Therefore, an activity of the user may be detected, and the time period may be based on the detected activity. The time period may be ceased with the activity is no longer detected. It is understood that the detected activity of the user, for instance, if (s)he is playing a video game, sleeping, etc., is different from inaction of the user related to the second set of control parameters.</p><p id="p-0017" num="0016">In an embodiment, the method may further comprise determining an identity of the user; and determining the second set of control parameters based on the determined identity.</p><p id="p-0018" num="0017">In a multiuser environment, i.e. when multiple users are present in the environment, it is important to identify the user and train the machine according to the preference of the identified user. In this example, the second set of control parameters may be based on the identified user.</p><p id="p-0019" num="0018">In an embodiment, the method may further comprise determining the second set of control parameters based on a prior evaluated feedback.</p><p id="p-0020" num="0019">The training of the machine may be an iterative process, e.g. the second set of control parameters may be based on a prior evaluated feedback. For example, if the user prefers a light effect and has indicated it via a prior evaluated feedback, the second light set of control parameters may be determined based on the prior light effect. For instance, if the user has indicated that (s)he prefers a high brightness level; the second set of control parameters may be determined such that the brightness level is in a high range. The training of the machine may iterate based on each evaluated feedback.</p><p id="p-0021" num="0020">In an embodiment, the method may further comprise: receiving a signal indicative of a field of view of the user; determining one or more lighting devices with illumination in the field of view of the user; controlling the determined one or more lighting devices based on the second set of control parameters in the field of view of the user.</p><p id="p-0022" num="0021">A field of view is an open observable area a user can see through his or her eyes or via an optical device. The one or more lighting devices may be located in the field of view of the user or at least have illumination in the field of view. Controlling of the determined one or more lighting devices based on the second set of control parameters is advantageously performed in the field of view of the user such that the user can observe the second light effect. If the rendered second light effect is not in the field of view of the user, the inaction of user may not represent the user preference related to the second set of control parameters.</p><p id="p-0023" num="0022">In an embodiment, an active response may comprise controlling the one or more lighting devices based on a third set of control parameters based on a received user input; and wherein the feedback is negative if said active response is monitored.</p><p id="p-0024" num="0023">An active response of the user related to the second set of control parameters may be to modify the second set of control parameters to produce a third set of control parameters or to select a third set of control parameters being independent of the second set of control parameters and control the one or more lighting devices thereon. The controlling of the one or more lighting devices based on a third set of control parameters may be based on a received user input, e.g. via a legacy wall switch, voice command etc. In an example, the third set of control parameters is the first set of control parameters. In alternate example, the third set of control parameters is different from the first set of control parameters. If the user does not like the second light effects, (s)he may control the one or more lighting devices to either render a different light effect or revert to the first light effect. Such monitored active response is considered as a negative feedback.</p><p id="p-0025" num="0024">In an embodiment, an active response may comprise actuating at least one actuator, by the user, related to the rendered second light effect.</p><p id="p-0026" num="0025">One of the other active responses related to the second set of control parameters may comprise actuating at least one actuator, e.g. a like or dislike button. For example, if the user actuates the like button, it is considered as a positive feedback, and if the user actuates the dislike button, it is considered as a negative feedback. The training of the machine is performed based on the evaluated feedback. The evaluation of the feedback (positive or negative) may be based on the context of the active response.</p><p id="p-0027" num="0026">In an embodiment, the one or more lighting devices may be controlled based on the second set of control parameters prior to the user presence detection.</p><p id="p-0028" num="0027">Alternative to detecting user presence and subsequently controlling the one or more lighting devices to render the second light effect, the one or more lighting devices may be controlled to render the second light effect prior to the user presence detection. To optimize the light effect, the user does not need to observe the change from the first light effect to the second light effect, only observing the second light effect and providing feedback may be sufficient for optimization.</p><p id="p-0029" num="0028">In an embodiment, the machine may be trained using reinforcement learning, and wherein the positive feedback may be a positive reward and the negative feedback may be a negative reward.</p><p id="p-0030" num="0029">Machine learning algorithms such as reinforcement learning may be used to train the machine to optimize the light effect. Reinforcement learning is a machine learning approach to optimize a policy, e.g. light control actions, by maximizing an ultimate reward through feedback. The feedback may be in the form of positive rewards and negative reward (or punishments), after a sequence of actions, e.g. rendering of the second light effect. The machine may be trained based on a positive reward if the feedback is positive and on a negative award (punishment) if the feedback is negative.</p><p id="p-0031" num="0030">According to a second aspect, the object is achieved by a controller for training a machine for automatizing lighting control actions; wherein the controller may comprise a processor arranged for executing the steps of method according to the first aspect.</p><p id="p-0032" num="0031">According to a third aspect, the object is achieved by a lighting system for training a machine for automatizing lighting control actions comprising a plurality of lighting devices arranged for illuminating an environment; a controller according to the second aspect.</p><p id="p-0033" num="0032">According to a fourth aspect, the object is achieved by a computer program product comprising instructions which, when the program is executed by a computer, cause the computer to carry out the steps of the method according to the first aspect.</p><p id="p-0034" num="0000">It should be understood that the computer program product and the system may have similar and/or identical embodiments and advantages as the above-mentioned method.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0035" num="0033">The above, as well as additional objects, features and advantages of the disclosed systems, devices and methods will be better understood through the following illustrative and non-limiting detailed description of embodiments of systems, devices and methods, with reference to the appended drawings, in which:</p><p id="p-0036" num="0034"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows schematically and exemplary an embodiment of a system for training a machine for automatizing lighting control actions;</p><p id="p-0037" num="0035"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows schematically and exemplary an embodiment of a controller for training a machine for automatizing lighting control actions;</p><p id="p-0038" num="0036"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows schematically and exemplary a flowchart illustrating an embodiment of a method for training a machine for automatizing lighting control actions;</p><p id="p-0039" num="0037"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows schematically and exemplary a machine learning approach for training a machine for automatizing lighting control actions.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0040" num="0038">All the figures are schematic, not necessarily to scale, and generally only show parts which are necessary in order to elucidate the invention, wherein other parts may be omitted or merely suggested.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0041" num="0039"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows schematically and exemplary an embodiment of a system <b>100</b> with lighting device(s) <b>110</b><i>a</i>-<i>d </i>for illuminating an environment <b>101</b>. The environment <b>101</b> may be an indoor or outdoor environment, such as office, a factory, a house, a grocery store or a hospital, a sports arena etc. The system <b>100</b> exemplary comprises four lighting devices <b>110</b><i>a</i>-<i>d</i>. The lighting devices <b>110</b><i>a</i>-<i>d </i>may be comprised in a lighting system. The lighting system may be a connected lighting system, e.g. Philips Hue, wherein the lighting devices <b>110</b><i>a</i>-<i>d </i>may be connected to an external network, e.g. Internet. A lighting device <b>110</b><i>a</i>-<i>d </i>is a device or structure arranged to emit light suitable for illuminating an environment <b>101</b>, providing or substantially contributing to the illumination on a scale adequate for that purpose. A lighting device <b>110</b><i>a</i>-<i>d </i>comprises at least one light source or lamp (not shown), such as an LED-based lamp, gas-discharge lamp or filament bulb, etc., (optionally) with an associated support, casing or other such housing. Each of the lighting devices <b>110</b><i>a</i>-<i>d </i>may take any of a variety of forms, e.g. a ceiling mounted lighting device, a wall-mounted lighting device, a wall washer, or a free-standing lighting device (and the lighting devices need not necessarily all be of the same type). In this exemplary figure, the lighting devices <b>110</b><i>a</i>-<i>c </i>are ceiling mounted and the lighting device <b>110</b><i>d </i>is a free-standing lighting device. The system <b>100</b> may contain any number/type of the lighting devices <b>110</b><i>a</i>-<i>d. </i></p><p id="p-0042" num="0040">The lighting devices <b>110</b><i>a</i>-<i>d </i>may be controlled based on a first set of control parameters. The controlling of the lighting devices <b>110</b><i>a</i>-<i>d </i>may comprise controlling one or more of: color, color temperature, intensity, beam width, beam direction, illumination intensity, other parameters of one or more of the light sources (not shown) of the lighting devices <b>110</b><i>a</i>-<i>d</i>. The lighting devices <b>110</b><i>a</i>-<i>d </i>may be controlled based on a second set of control parameters. A first and a second light effect may be rendered when the lighting devices <b>110</b><i>a</i>-<i>d </i>are controlled based on the first and the second set of control parameters respectively. The second set of control parameters may be different from the first set of control parameters such that the difference between the first light effect and the second light effect is perceivable by a user <b>120</b>. In a simple example, the light effect is a brightness level of the lighting devices <b>110</b><i>a</i>-<i>d</i>, for instance, the first light effect is a 30% brightness level, and the second light effect is a 70% brightness level. The second light effect, i.e. 70% brightness level, is determined such that the difference between the first light effect and the second light effect is perceivable by a user <b>120</b>. For example, the selection of 70% brightness level is based on an ambient light level in the environment <b>101</b> such that a difference of 50% in brightness levels is perceivable by a user <b>120</b>. In another example, the controlling of the lighting devices <b>110</b><i>a</i>-<i>d </i>based on the first set of control parameters provides no light output.</p><p id="p-0043" num="0041">In an example, the light effect comprises light scenes which can be used to enhance, e.g. entertainment experiences such as audio-visual media, set an ambience and/or a mood of a user <b>120</b>. For instance, for Philips Hue connected lighting system, the first light effect is an &#x2018;enchanted forest&#x2019; light scene and the second light effect is Go-to-sleep light scene. The first and/or the second light effect may comprise a static light scene. The first and/or the second light effect may comprise a dynamic light scene, wherein the dynamic light scene comprises light effects which change with time. For the dynamic light scene, the first and/or the second light effect may comprise a first light state and a second light state. The first light state may comprise a first (predefined) pattern and the second light state may comprise a second (predefined) pattern. The pattern may comprise a duration, level of dynamism of the light effects etc. The first and the second set light states may be related to a first and a second subset of the second set of control parameters respectively. In such an example, the training of the machine comprises automatizing the (first and/or second) subsets of the second set of control parameters.</p><p id="p-0044" num="0042">The system <b>100</b> may further comprise a presence sensing means, which is exemplary a presence sensor <b>140</b> in the figure. The system <b>100</b> may comprise any number of presence sensors. The presence sensing means may comprise a single device <b>140</b> or may comprise a presence sensing system comprising one or more devices arranged for detecting user presence. The system <b>100</b> may comprise sensors (not shown) of other modalities such as light sensor for detecting ambient light levels, a temperature sensor, a humidity sensor, a gas sensor such as a CO2 sensor, a particle measurement sensor, and/or an audio sensor. The presence sensor <b>140</b> may be arranged for sensing a signal indicative of a presence of a user <b>120</b>. The presence sensor <b>140</b> may be a passive infrared sensor, an active ultrasound sensor or an imaging sensor such as a camera. Any sensing method or presence sensor, known in the art to detect user presence, may be used for detected presence of a user <b>120</b>. The user presence may be detected based on presence signal output from a plurality of different presence sensors. The user <b>120</b> presence may be detected in the environment <b>101</b>. The presence detection may be performed continuously, periodically or at random times. In an example, the second light effect may be rendered subsequently upon detecting the presence of the user <b>120</b>. Alternatively, the second light effect may be rendered prior to the user presence detection.</p><p id="p-0045" num="0043">A signal indicative of the field of view of the user <b>120</b> may be received or the field of view may be determined based on an orientation signal output from an orientation sensor (not shown) which is able to detect the orientation of the user <b>120</b>. The field of view of the user <b>120</b> may be determined based on a user position. In an example, the user position may comprise an absolute location of the user <b>120</b> in the environment <b>101</b>. In another example, the user position may comprise a relative position of the user <b>120</b> with respect to one or more lighting devices and/or with respect to the first and the second rendered light effects. The position of the user <b>120</b> may be determined using the presence sensing means or by other means known in the art to detect position. The lighting devices <b>110</b><i>a</i>-<i>d </i>which have illumination in the field of view of the user <b>120</b> may be determined. The determined lighting devices <b>110</b><i>a</i>-<i>d </i>may be controlled to render the second light effect based on the second set of control parameters in the field of view of the user <b>120</b>. In an example, a signal indicative of a trajectory of a (moving) user <b>120</b> may be received or alternatively a trajectory of a (moving) user <b>120</b> may be determined, e.g. by using an imaging device such as a camera. The lighting devices <b>110</b><i>a</i>-<i>d </i>having illumination in the trajectory of the user <b>120</b> may be determined. The determined lighting devices <b>110</b><i>a</i>-<i>d </i>may be controlled to render the second light effect based on the second set of control parameters in the trajectory of the (moving) user <b>120</b>.</p><p id="p-0046" num="0044">A response of the user <b>120</b> may be monitored related to the second set of control parameters, wherein the response is monitored during a time period. The time period may start upon detecting the user presence and controlling the one or more lighting devices to render the second light effect. The time period may be ceased when the presence is no longer detected. The sequence of these both conditions, i.e. user presence detection and rendering of the second light effect, may be different, such that the user presence is detected first and the second light effect is rendered later or other way around. Alternatively, both conditions may occur at the same time. The time period may be predetermined and may be selected such that the time period is above a threshold value. In an example, if the time period is less than a threshold value, for instance, it starts upon detecting presence and ceased when the presence is no longer detected, but the time period is less than a threshold value, the monitored response is discarded. An example of such situation may be when the user <b>120</b> is not present in the environment <b>101</b> for a long time, e.g. (s)he has come to pick up something and has left the environment <b>101</b>. In these situations, though the lighting devices <b>110</b><i>a</i>-<i>d </i>may have been controlled to render the second light effect, but the user <b>120</b> might not have observed the second light effect. Therefore, the monitored response for such a small time period is discarded. In another example, the time period may be based on an activity of the user <b>120</b>. For example, an imaging sensor (e.g. camera) may be used to detect an activity of the user <b>120</b>. The time period is ceased when the activity is no longer detected, e.g. if the user <b>120</b> is sleeping. The time period may be based on the rendered light effect, e.g. if the one or more lighting devices <b>110</b><i>a</i>-<i>d </i>are turned off, i.e. controlled to not provide illumination to the environment <b>101</b>, the monitored response in such situations may be discarded. The monitored response, for the dynamic light scene, may be related to the first light state and/or to the second light state.</p><p id="p-0047" num="0045">A feedback of the user <b>120</b> may be evaluated based on the monitored response of the user <b>120</b>. A positive feedback may be assigned if no active response has been monitored. A positive feedback represents that the user <b>120</b>, whose presence has been detected in the environment, is pleased with the second light effect and that is why (s)he does not want to provide any active response related to the second set of control parameters. For evaluating the feedback, e.g. for evaluating &#x2018;inaction&#x2019; as positive feedback, the user presence is a key element. An active response may comprise controlling the lighting devices <b>110</b><i>a</i>-<i>d </i>to render a third light effect based on a third set of control parameters based on a received user input; and wherein the feedback may be negative if said active response is monitored. The third set of control parameters may be the first set of control parameters, which indicates that the user <b>120</b> reverts the second light effect back to the first light effect because (s)he does not like the second light effect. Therefore, a negative feedback may be assigned to such active feedback. For the dynamic light scene, the active response may comprise changing the second light state, which may indicate that the user is not pleased with the second light state. The system may further comprise a wall-switch <b>130</b> which may be arranged for controlling the lighting devices <b>110</b><i>a</i>-<i>d</i>. For example, the user <b>120</b> uses the wall-switch <b>130</b> to control the lighting devices <b>110</b><i>a</i>-<i>d</i>. Alternatively, the user <b>120</b> may use a voice command <b>133</b> or his/her mobile device <b>136</b> to control the lighting devices <b>110</b><i>a</i>-<i>d</i>. In an example, the active response may comprise actuating at least one actuator (not shown), by the user <b>120</b>, related to the rendered second light effect. The at least one actuator may be a like/dislike button or undo button, e.g. on the user's mobile device <b>136</b> to indicate his/her preference. The at least one actuator may be used to control the lighting devices <b>110</b><i>a</i>-<i>d. </i></p><p id="p-0048" num="0046">A machine may be trained based on the evaluated feedback. Machine learning algorithms, such as supervised learning, e.g. SVM, decision forest etc. may be used to train the machine. Reinforcement learning, as further discussed in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, may be used to train the machine. For reinforcement learning, the positive feedback may be a positive reward and the negative feedback may be a negative reward (or punishment). The learning algorithm may comprise iterative learning such that the determination of the second set of control parameters may be based on a prior evaluated feedback, wherein the algorithm iterative trains the machine. The training may comprise different phases, such as a feedback phase wherein in the feedback phase the feedback is evaluated based on the monitored response of the user <b>120</b>. The length of the feedback phase may comprise the time period, which is assumed to be long enough to capture sufficient information needed for training. Subsequent to the feedback phase, a training phase may be started. The training phase may be defined during which the machine is trained. For iterative learning, the feedback phase and the training phase may be iteratively used. In an example, the feedback phase may be the first week when the lighting devices <b>110</b><i>a</i>-<i>b </i>have been, e.g. initially installed, commissioned and the user <b>120</b> has started using them. The duration of the feedback phase may be defined by the user <b>120</b>. In an example, in the training phase may comprise a learning phase and a fine-tuning phase; wherein in the learning phase, the second set of control parameters may be learnt based on a user feedback. In the fine-tuning phase, the second set of control parameters may be further optimized based on further user inputs. In an example, an identity of the user <b>120</b> is determined, for instance, by the imaging sensor. The determination of the second set of control parameters may be based on the determined identity, e.g. based on the preference of the identified user.</p><p id="p-0049" num="0047"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows schematically and exemplary an embodiment of a controller <b>210</b> for training a machine for automatizing lighting control actions. The controller <b>210</b> may comprise an input unit <b>214</b> and an output unit <b>215</b>. The input <b>214</b> and the output 215 units may be comprised in a transceiver (not shown) arranged for receiving (input unit <b>214</b>) and transmitting (output unit <b>215</b>) communication signals. The communication signal may comprise control instructions to control the lighting devices <b>110</b><i>a</i>-<i>d</i>. The input unit <b>214</b> may be arranged for receiving communication signals from the switch <b>130</b> and/or from the voice command <b>133</b>. The input unit <b>214</b> may be arranged for receiving the communication signals from the user mobile device <b>136</b>. The communication signals may comprise control signals. The controller <b>210</b> may further comprise a memory <b>212</b> which may be arranged for storing communication IDs of the lighting devices <b>110</b><i>a</i>-<i>d </i>and/or the sensor <b>140</b> etc. The controller <b>210</b> may comprise a processor <b>213</b> arranged for training the machine.</p><p id="p-0050" num="0048">The controller <b>210</b> may be implemented in a unit separate from the lighting devices <b>110</b><i>a</i>-<i>d</i>/sensor <b>140</b>, such as wall panel, desktop computer terminal, or even a portable terminal such as a laptop, tablet or smartphone. Alternatively, the controller <b>210</b> may be incorporated into the same unit as the sensor <b>140</b> and/or the same unit as one of the lighting devices <b>110</b><i>a</i>-<i>d</i>. Further, the controller <b>210</b> may be implemented in the environment <b>101</b> or remote from the environment (e.g. on a server); and the controller <b>210</b> may be implemented in a single unit or in the form of distributed functionality distributed amongst multiple separate units (e.g. a distributed server comprising multiple server units at one or more geographical sites, or a distributed control function distributed amongst the lighting devices <b>110</b><i>a</i>-<i>d </i>or amongst the lighting devices <b>110</b><i>a</i>-<i>d </i>and the sensor <b>140</b>). Furthermore, the controller <b>210</b> may be implemented in the form of software stored on a memory (comprising one or more memory devices) and arranged for execution on a processor (comprising one or more processing units), or the controller <b>210</b> may be implemented in the form of dedicated hardware circuitry, or configurable or reconfigurable circuitry such as a PGA or FPGA, or any combination of these.</p><p id="p-0051" num="0049">Regarding the various communication involved in implementing the functionality discussed above, to enable the controller <b>210</b>, for example, to receive presence signal output from the presence sensor <b>140</b> and to control the light output of the lighting devices <b>110</b><i>a</i>-<i>d</i>, these may be implemented in by any suitable wired and/or wireless means, e.g. by means of a wired network such as an Ethernet network, a DMX network or the Internet; or a wireless network such as a local (short range) RF network, e.g. a Wi-Fi, ZigBee or Bluetooth network; or any combination of these and/or other means.</p><p id="p-0052" num="0050"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows schematically and exemplary a flowchart illustrating an embodiment of a method <b>300</b> for training a machine for automatizing lighting control actions. The method <b>300</b> may comprise controlling <b>310</b> one or more lighting devices <b>110</b><i>a</i>-<i>d </i>based on a first set of control parameters, e.g. to render a first light effect. The one or more lighting devices <b>110</b><i>a</i>-<i>d </i>may be further controlled <b>320</b> based on a second set of control parameters, e.g. to render a second light effect. The light effect(s) may comprise a level of luminance, chrominance, saturation, color-balance, and/or a light scene etc. The control <b>310</b>-<b>320</b> of the lighting devices <b>110</b><i>a</i>-<i>d </i>may be in a field of view of a user <b>120</b>. The control <b>310</b>-<b>320</b> of the lighting devices <b>110</b><i>a</i>-<i>d </i>may be in a trajectory of the user <b>120</b>. The control of the lighting devices <b>110</b><i>a</i>-<i>d </i>may be relative to a position of the user <b>120</b>. The position of the user <b>120</b> may be determined using the presence sensing means. The second set of control parameters may be different from the first set of control parameters, for instance, in a way that the difference between the first light effect and the second light effect is perceivable by a user <b>120</b>. A user <b>120</b> presence may be detected <b>330</b> based on a presence signal output from a presence sensing means. The presence sensing means may comprise image sensing such as a camera, RF-based presence sensing etc. The method <b>300</b> may further comprise monitoring <b>340</b> a response of the user <b>120</b> related to the second set of control parameters; wherein the response may be monitored during a time period. In an example, the time period may be based on the controlling <b>320</b> of the lighting devices <b>110</b><i>a</i>-<i>d </i>to render the second light effect and on detection <b>330</b> of user presence. In another example, an activity of the user <b>120</b> may be determined and the time period may be based on the determined activity of the user <b>120</b>.</p><p id="p-0053" num="0051">The method <b>300</b> may further comprise evaluating <b>350</b> feedback of the user <b>120</b> based on the monitored response. The feedback may be positive if no active response has been monitored. An active response may comprise controlling the one or more lighting devices <b>110</b><i>a</i>-<i>d </i>to render a third light effect based on a third set of control parameters based on a received user input; and wherein the feedback may be negative if said active response is monitored.</p><p id="p-0054" num="0052">The method may further comprise training <b>360</b> the machine based on the evaluated feedback. Machine learning algorithms may be used to train the machine. For example, supervised learning may be used. Supervised learning is the machine learning task of learning a function or model that maps an input to an output based on an input-output data pairs. It infers a function from a labeled training data set comprising of a set of training data. In supervised learning, each sample in the training data set is a pair consisting of an input (e.g. a vector) and a desired output value. For instance, the evaluated feedback is output, and the second set of control parameters is the input vector. The training data set comprises the output (feedback) and the input (the second set of control parameters). A supervised learning algorithm, such as Support vector machine (SVM), decision tree (random forest) etc., analyzes the training data set and produces an inferred function or model, which can be used for making predictions based on a new data set. In this example, a binary classifier machine may be trained, which may predict the user <b>120</b> preference for a new set of control parameters. If the model predicts a positive user preference for the new set of control parameters, the lighting devices <b>110</b><i>a</i>-<i>d </i>may be controlled to render a new light effect based on the new set of control parameters. Alternative to supervised learning, reinforcement learning may be used to train the machine as further discussed in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Other learning algorithms such as rule-based learning, probabilistic reasoning, fuzzy logic to train a machine for automatizing lighting control action known in the art may also be considered.</p><p id="p-0055" num="0053">The method <b>300</b> may be executed by computer program code of a computer program product when the computer program product is run on a processing unit of a computing device, such as the processor <b>213</b> of the controller <b>210</b>.</p><p id="p-0056" num="0054"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows schematically and exemplary a machine learning approach, i.e. Reinforcement learning, for training a machine for automatizing lighting control actions. Reinforcement learning is a machine learning approach to optimize a policy, e.g. lighting control actions, by maximizing an ultimate reward r<sub>t </sub>through feedback. The ultimate reward r<sub>t </sub>is in the form of rewards and punishments for a sequence of actions a<sub>t</sub>, e.g. rendering of a light effect. In reinforcement learning, the machine perceives its environment's state s<sub>t </sub>as a vector of features. The machine can execute actions in every state s<sub>t </sub>and based on action a<sub>t </sub>receives either a reward or a punishment r<sub>t+1 </sub>and moves to another state s<sub>t+1</sub>. The goal of the reinforcement learning is to learn a policy, that is the prescription of the optimal action at to execute in each state s<sub>t</sub>. The action is optimal if it maximizes the average reward.</p><p id="p-0057" num="0055">In an example, the agent <b>410</b> is the lighting system comprising one or more lighting devices <b>110</b><i>a</i>-<i>d</i>. The agent <b>410</b> may be arranged for taking actions a<sub>t</sub>, e.g. controlling the one or more lighting devices <b>110</b><i>a</i>-<i>d </i>to render the first and/or the second light effects. The environment <b>420</b> may be an office, a garden, a factory, a house, a grocery store or a hospital. The action a<sub>t </sub>of the agent <b>410</b> affects the environment <b>420</b>, i.e. the controlling of the one or more lighting devices <b>110</b><i>a</i>-<i>d </i>changes the illumination in the environment.</p><p id="p-0058" num="0056">A presence of a user <b>120</b> may be detected and a response of the user <b>120</b> related to the second set of control parameters may be monitored. The response may be monitored during a time period. A feedback may evaluated wherein a positive feedback may be assigned if no active response has been monitored. An active response may comprise controlling the one or more lighting devices <b>110</b><i>a</i>-<i>d </i>to render a third light effect based on a third set of control parameters based on a received user input; and wherein the feedback is negative if said active response is monitored. The positive feedback may indicate that the user is pleased with the second light effect, whereas the negative feedback may indicate that the user is not pleased and (s)he prefers to change the second light effect to a third light effect. The ultimate reward r<sub>t </sub>may comprise reward and punishment corresponding to, in this example, the positive feedback and the negative feedback respectively.</p><p id="p-0059" num="0057">The state s<sub>t </sub>is a light effect, e.g. the first light effect, the second light effect etc. With every action a<sub>t </sub>of the agent <b>410</b>, the user <b>120</b> may be monitored and the feedback is evaluated as reward and punishment. Based on the reward and punishment, reinforcement learning optimizes the light effect, i.e. action of the agent <b>410</b> for the user <b>120</b> in the environment <b>420</b>. Different reinforcement learning algorithms such as Q-Learning, State-Action-Reward-State-Action (SARSA), Deep Q Network (DQN), Deep Deterministic Policy Gradient (DDPG) etc. may be used for training the machine for automatizing lighting control actions.</p><p id="p-0060" num="0058">It should be noted that the above-mentioned embodiments illustrate rather than limit the invention, and that those skilled in the art will be able to design many alternative embodiments without departing from the scope of the appended claims.</p><p id="p-0061" num="0059">In the claims, any reference signs placed between parentheses shall not be construed as limiting the claim. Use of the verb &#x201c;comprise&#x201d; and its conjugations does not exclude the presence of elements or steps other than those stated in a claim. The article &#x201c;a&#x201d; or &#x201c;an&#x201d; preceding an element does not exclude the presence of a plurality of such elements. The invention may be implemented by means of hardware comprising several distinct elements, and by means of a suitably programmed computer or processing unit. In the device claim enumerating several means, several of these means may be embodied by one and the same item of hardware. The mere fact that certain measures are recited in mutually different dependent claims does not indicate that a combination of these measures cannot be used to advantage.</p><p id="p-0062" num="0060">Aspects of the invention may be implemented in a computer program product, which may be a collection of computer program instructions stored on a computer readable storage device which may be executed by a computer. The instructions of the present invention may be in any interpretable or executable code mechanism, including but not limited to scripts, interpretable programs, dynamic link libraries (DLLs) or Java classes. The instructions can be provided as complete executable programs, partial executable programs, as modifications to existing programs (e.g. updates) or extensions for existing programs (e.g. plugins). Moreover, parts of the processing of the present invention may be distributed over multiple computers or processors or even the &#x2018;cloud&#x2019;.</p><p id="p-0063" num="0061">Storage media suitable for storing computer program instructions include all forms of nonvolatile memory, including but not limited to EPROM, EEPROM and flash memory devices, magnetic disks such as the internal and external hard disk drives, removable disks and CD-ROM disks. The computer program product may be distributed on such a storage medium, or may be offered for download through HTTP, FTP, email or through a server connected to a network such as the Internet.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for training a machine for automatizing lighting control actions, wherein the method comprises the steps of:<claim-text>controlling one or more lighting devices based on a first set of control parameters;</claim-text><claim-text>controlling the one or more lighting devices based on a second set of control parameters; wherein the second set of control parameters is different from the first set of control parameters;</claim-text><claim-text>detecting presence of a user based on a presence signal output from a presence sensing means;</claim-text><claim-text>monitoring a response of the user related to the second set of control parameters; wherein monitoring comprises observing the user reaction to the second set of control parameters; wherein the response is monitored during a time period while the user presence has been detected; wherein the time period is ceased when the presence is no longer detected;</claim-text><claim-text>evaluating feedback of the user based on the monitored response; wherein the feedback is positive if no active response has been monitored; wherein an active response comprises controlling the one or more lighting devices based on a third set of control parameters based on a received user input; and wherein the feedback is negative if said active response is monitored; and</claim-text><claim-text>training the machine based on the evaluated feedback.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein controlling the one or more lighting devices based on the second set of control parameters is triggered based on the user presence detection.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the time period starts upon detecting the user presence and controlling the one or more lighting devices based on the second set of control parameters.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises detecting an activity of the user; and wherein the time period is based on the detected activity such that the time period is ceased when the activity is no longer detected.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises:<claim-text>determining an identity of the user; and</claim-text><claim-text>determining the second set of control parameters based on the determined identity.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises:<claim-text>determining the second set of control parameters based on a prior evaluated feedback.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the method further comprises:<claim-text>receiving a signal indicative of a field of view of the user;</claim-text><claim-text>determining one or more lighting devices with illumination in the field of view of the user;</claim-text><claim-text>controlling the determined one or more lighting devices based on the second set of control parameters in the field of view of the user.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the third set of control parameters comprises the first set of control parameters.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an active response comprises actuating at least one actuator, by the user, related to the second set of control parameters.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the one or more lighting devices are controlled based on the second set of control parameters prior to the user presence detection.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the machine is trained using reinforcement learning, and wherein the positive feedback is a positive reward and the negative feedback is a negative reward.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A controller for training a machine for automatizing lighting control actions; wherein the controller comprises a processor arranged for executing the steps of method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A lighting system for training a machine for automatizing lighting control actions comprising<claim-text>a plurality of lighting devices arranged for illuminating an environment;</claim-text><claim-text>a controller according to <claim-ref idref="CLM-00012">claim 12</claim-ref>.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. A non-transitory computer program product comprising instructions which, when the program is executed by a computer, cause the computer to carry out the steps of the method of <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>