<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005257A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005257</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17436560</doc-number><date>20201112</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202010231088.3</doc-number><date>20200327</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>10</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>77</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>80</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>94</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>176</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>751</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7715</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>806</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>95</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>753</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30184</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">ILLEGAL BUILDING IDENTIFICATION METHOD AND APPARATUS, DEVICE, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE AND TECHNOLOGY CO., LTD.</orgname><address><city>Beijing</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Guanhao</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>YE</last-name><first-name>Zhi</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE AND TECHNOLOGY CO., LTD.</orgname><role>03</role><address><city>Beijing</city><country>CN</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/128257</doc-number><date>20201112</date></document-id><us-371c12-date><date>20210903</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided are an illegal building identification method and apparatus, a device, and a storage medium, which relate to the field of cloud computing. The specific implementation scheme is: acquiring a target image and a reference image associated with the target image; extracting a target building feature of the target image and a reference building feature of the reference image, respectively; and determining, according to the target building feature and the reference building feature, an illegal building identification result of the target image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="76.96mm" wi="147.66mm" file="US20230005257A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.43mm" wi="150.20mm" file="US20230005257A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="115.74mm" wi="149.61mm" file="US20230005257A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="234.10mm" wi="161.21mm" file="US20230005257A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="180.85mm" wi="175.85mm" file="US20230005257A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="214.71mm" wi="165.35mm" file="US20230005257A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="169.84mm" wi="175.77mm" file="US20230005257A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="171.70mm" wi="125.14mm" file="US20230005257A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application claims priority to a Chinese Patent Application No. 202010231088.3 filed with the CNIPA on Mar. 27, 2020, the disclosure of which is incorporated herein by reference in its entirety.</p><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present application relates to image processing technologies, for example, the field of cloud computing, and specifically, to an illegal building identification method and apparatus, a device, and a storage medium.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In the urban construction, illegal buildings have become the focus of attention in the term of urban appearance destruction. Meanwhile, illegal buildings also pose a greater threat to the life safety of humans.</p><p id="p-0005" num="0004">At present, illegal building detection is usually carried out in the manner of options of the urban management department. However, the above-mentioned manner requires the investment of a lot of labor costs, and such a manner relying on manual inspection is inefficient, which can easily cause missing detection.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">The following is a summary of the subject matter described in detail herein. This summary is not intended to limit the scope of the claims.</p><p id="p-0007" num="0006">Embodiments of the present application provide an illegal building identification method and apparatus, a device, and a storage medium to achieve illegal building automatic identification, thereby reducing the identification cost, and improving the identification efficiency.</p><p id="p-0008" num="0007">In a first aspect, an embodiment of the present application provides an illegal building identification method. The method includes the steps described below.</p><p id="p-0009" num="0008">A target image and a reference image associated with the target image are acquired.</p><p id="p-0010" num="0009">A target building feature of the target image and a reference building feature of the reference image are extracted, respectively.</p><p id="p-0011" num="0010">An illegal building identification result of the target image is determined according to the target building feature and the reference building feature.</p><p id="p-0012" num="0011">In the present application, a target image and a reference image associated with the target image are acquired, a target building feature of the target image and a reference building feature of the reference image are extracted, respectively, and an illegal building identification result of the target image is determined according to the target building feature and the reference building feature. In the preceding technical scheme, the reference image associated with the target image is acquired, the target image and the reference image are bonded to each other, and feature extraction is performed on the bonded images, so as to perform illegal building identification on the target image based on the building feature of the reference image, thereby achieving the illegal building automatic identification and reducing the data throughput in the process of illegal building identification. Meanwhile, based on the siamese idea, building feature extraction is performed on the target image and the reference image, and then illegal building identification is performed according to the extracted building feature, thereby improving the accuracy of identification results.</p><p id="p-0013" num="0012">In an embodiment, a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold; or an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold; or a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold and an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold.</p><p id="p-0014" num="0013">In an optional implementation of the preceding application, the distance difference between the acquisition position of the target image and the acquisition position of the reference image, or the angle difference between the acquisition angle of the target image and the acquisition angle of the reference image, or the distance difference between the acquisition position of the target image and the acquisition position of the reference image and the angle difference between the acquisition angle of the target image and the acquisition angle of the reference image are limited so that the bonding of the target image and the reference image is achieved and the case in which illegal building identification is performed on the target image based on multiple reference images is avoided, thereby reducing the amount of the data operations.</p><p id="p-0015" num="0014">In an embodiment, the step in which target building feature of the target image and reference building feature of the reference image associated with the target image are extracted, respectively, includes the steps described below.</p><p id="p-0016" num="0015">A target basis feature of the target image and a reference basis feature of the reference image associated with the target image are extracted, respectively.</p><p id="p-0017" num="0016">Feature extraction is performed, at each of at least two set scales, on the target basis feature and the reference basis feature, respectively, to obtain a target building feature and a reference building feature at each of the at least two scales.</p><p id="p-0018" num="0017">In an optional implementation of the preceding application, the building feature extraction process is refined into the basis feature extraction and the feature extraction is further performed on basis features at at least two scales so that the detailed features of the images at different scales can be obtained and then the illegal building identification is performed based on the building features at at least two scales, thereby improving the accuracy of the identification results.</p><p id="p-0019" num="0018">In an embodiment, the step in which an illegal building identification result of the target image is determined according to the target building feature and the reference building feature includes the steps described below.</p><p id="p-0020" num="0019">Feature fusion is performed on the target building feature and the reference building feature at each of the at least two scales.</p><p id="p-0021" num="0020">The illegal building identification result of the target image is determined according to feature fusion results at the at least two scales.</p><p id="p-0022" num="0021">In an optional implementation of the preceding application, the illegal building identification result determination process is refined into the fusion of building features at each scale and the illegal building identification is performed according to the feature contents and results at at least two scales, thereby perfecting the illegal building identification mechanism at multiple scales.</p><p id="p-0023" num="0022">In an embodiment, the step in which feature fusion is performed on the target building feature and the reference building feature at each of the at least two scales includes the step described below.</p><p id="p-0024" num="0023">A difference between the target building feature and the reference building feature under each of the at least two scales is calculated, and the difference is taken as a feature fusion result under the each of the at least two scales.</p><p id="p-0025" num="0024">In an optional implementation of the preceding application, the feature fusion process is refined into the operation of taking the difference between the target building feature and the reference building feature at each scale as the feature fusion result, thereby perfecting the feature fusion mechanism.</p><p id="p-0026" num="0025">In an embodiment, the step in which a target basis feature of the target image and a reference basis feature of the reference image associated with the target image are extracted respectively includes the step described below.</p><p id="p-0027" num="0026">The target basis feature of the target image and the reference basis feature of the reference image associated with the target image are extracted based on a deep residual network, respectively.</p><p id="p-0028" num="0027">In an optional implementation of the preceding application, the basis feature extraction process is refined into the basis feature extraction based on the deep residual network, thereby perfecting the feature extraction manner and improving the accuracy of the feature extraction results.</p><p id="p-0029" num="0028">In an embodiment, before the reference building feature of the reference image is extracted, the method further includes the step described below.</p><p id="p-0030" num="0029">Coordinate transform is performed on the reference image according to the target image.</p><p id="p-0031" num="0030">The coordinate transform includes at least one of shrinking transform, stretching transform, rotation transform or translation transform.</p><p id="p-0032" num="0031">In an optional embodiment of the preceding application, before the feature extraction is performed on the reference image, at least one of shrinking transform, stretching transform, rotation transform or translation transform is performed on the reference image according to the target image so that the coordinates of the transformed image match with the coordinates of the target image, thereby providing a guarantee for the accuracy of the illegal building identification results.</p><p id="p-0033" num="0032">In an embodiment, the step in which coordinate transform is performed on the reference image according to the target image includes the steps described below.</p><p id="p-0034" num="0033">Target key points and target descriptors of the target image and reference key points and reference descriptors of the reference image are extracted, respectively.</p><p id="p-0035" num="0034">A matching operation is performed on the target key points and the reference key points according to the target descriptors and the reference descriptors.</p><p id="p-0036" num="0035">A transform matrix is determined according to a matching result, and the coordinate transform is performed on the reference image according to the transform matrix.</p><p id="p-0037" num="0036">In an optional implementation of the preceding application, the process of changing the coordinates of the reference image is refined into the operations of extracting key points and descriptors from the target image and the reference image, performing the key point matching according to the matching situation of the descriptors of the target image and the descriptors of the reference image, determining the transform matrix according to the matching result of key points, and then changing the coordinates of the reference image according to the determined transform matrix, thereby perfecting the processing mechanism of coordinate transform of the reference image and thus providing a guarantee for the accuracy of illegal building identification results.</p><p id="p-0038" num="0037">In an embodiment, the step in which an illegal building identification result of the target image is determined includes the steps described below.</p><p id="p-0039" num="0038">Whether the target image includes an illegal building area is determined.</p><p id="p-0040" num="0039">If the target image includes the illegal building area, position coordinates of the illegal building area are determined.</p><p id="p-0041" num="0040">In an optional implementation of the preceding application, the illegal building identification result determination process is refined into the operations of performing dichotomous classification on the building area in the target image and detecting the position coordinates of the illegal building area when the building area includes the illegal building area, thereby enriching the content of the illegal building identification result.</p><p id="p-0042" num="0041">In a second aspect, an embodiment of the present application further provides an illegal building identification apparatus. The apparatus includes an image acquisition module, a building feature extraction module, and an identification result determination module.</p><p id="p-0043" num="0042">The image acquisition module is configured to acquire a target image and a reference image associated with the target image.</p><p id="p-0044" num="0043">The building feature extraction module is configured to extract a target building feature of the target image and a reference building feature of the reference image, respectively.</p><p id="p-0045" num="0044">The identification result determination module is configured to determine, according to the target building feature and the reference building feature, an illegal building identification result of the target image.</p><p id="p-0046" num="0045">In a third aspect, an embodiment of the present application further provides an electronic device. The electronic device includes: at least one processor, and a memory communicatively connected to the at least one processor.</p><p id="p-0047" num="0046">The memory has instructions executable by the at least one processor stored thereon, where the instructions are executed by the at least one processor to cause the at least one processor to perform the illegal building identification method provided in the embodiment described in the first aspect.</p><p id="p-0048" num="0047">In a fourth aspect, an embodiment of the present application further provides a non-transitory computer-readable storage medium having computer instructions stored thereon, where the computer instructions are configured to cause a computer to perform the illegal building identification method provided in the embodiment described in the first aspect.</p><p id="p-0049" num="0048">Other effects of the preceding optional implementations are described hereinafter in connection with embodiments.</p><p id="p-0050" num="0049">After reading and understanding the drawings and detailed description, other aspects can be understood.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0051" num="0050">The drawings are intended to provide a better understanding of the scheme of the present application and not to limit the present application. In the drawings:</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of an illegal building identification method according to Embodiment one of the present application;</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of an illegal building identification method according to Embodiment two of the present application;</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an illegal building identification method according to Embodiment three of the present application;</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a flowchart of an illegal building identification method according to Embodiment four of the present application;</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>4</b>B</figref> is a structure block diagram of an image matching process according to Embodiment four of the present application;</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b>C</figref> is a schematic diagram of a reference image according to Embodiment four of the present application;</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>4</b>D</figref> is a schematic diagram of a target image according to Embodiment four of the present application;</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>4</b>E</figref> is a schematic diagram of a transformed reference image according to Embodiment four of the present application;</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>4</b>F</figref> is a structure block diagram of an image detection process according to Embodiment four of the present application;</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>4</b>G</figref> is a schematic diagram of a target image marking result according to Embodiment four of the present application;</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a structure diagram of an illegal building identification apparatus according to Embodiment five of the present application; and</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an electronic device for implementing a data access method in an embodiment of the present application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0064" num="0063">Exemplary embodiments of the present application, including details of embodiments of the present application, are described hereinafter in conjunction with the drawings to facilitate understanding. The exemplary embodiments are merely illustrative. Therefore, it will be realized by those having ordinary skill in the art that various changes and modifications may be made to the embodiments described herein without departing from the scope and spirit of the present application. Similarly, description of well-known functions and constructions is omitted hereinafter for clarity and conciseness.</p><heading id="h-0006" level="1">Embodiment One</heading><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of an illegal building identification method according to Embodiment one of the present application. The embodiment of the present application is applicable to the case of identifying illegal buildings in images. The method may be executed by an illegal building identification apparatus. The apparatus may be implemented by software, hardware or software and hardware and is specifically configured in an electronic device.</p><p id="p-0066" num="0065">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the illegal building identification method includes the steps described below.</p><p id="p-0067" num="0066">In step S<b>101</b>, a target image and a reference image associated with the target image are acquired.</p><p id="p-0068" num="0067">The target image is an image on which illegal building identification is required to be performed. The reference image is a default image without illegal buildings. The target image and the reference image may be understood as images acquired at different times for exactly or approximately the same area, where the acquisition time of the reference image is earlier than the acquisition time of the target image.</p><p id="p-0069" num="0068">It is to be understood that the reference image may be an image acquired at a set acquisition interval from the current acquisition time, and may also be an image acquired when the illegal building identification is performed for the first time. Of course, in order to avoid repeated detection, the reference image may also be replaced in real time or at regular intervals, which is not limited in the present application.</p><p id="p-0070" num="0069">In order to ensure the consistency of the target image and the reference image and further improve the accuracy of the subsequently determined illegal building identification result, in an embodiment, the distance difference between the acquisition position of the acquired target image and the acquisition position of the reference image is less than a set distance threshold, so as to ensure that the acquisition positions of the target image and the reference figure are the same or approximate. Alternatively, in an embodiment, the angle difference between the acquisition angle of the acquired target image and the acquisition angle of the reference image is less than a set angle threshold, so as to ensure that the acquisition angles of the target image and the reference image are the same or approximate. The set distance threshold and the set angle threshold may be determined by technicians according to requirements or empirical values. The acquisition angle may be an image angle or a pitch angle of an acquisition device.</p><p id="p-0071" num="0070">In order to ensure the comprehensiveness of building information contained in the image, the image is usually shot by using an unmanned aerial vehicle along a set patrol route and based on a set acquisition frequency. In order to distinguish the images of different acquisition areas, the acquired images may be sequentially numbered according to the acquisition sequence of the images. Accordingly, when the reference image is acquired, a reference image with the same image number as the target image is acquired. The acquisition frequency may be determined by technicians according to the acquisition requirements or the lens parameters of the unmanned aerial vehicle.</p><p id="p-0072" num="0071">For example, the target image and the reference image associated with the target image may be stored in advance in the electronic device locally, another storage device associated with the electronic device or the cloud. When the illegal building identification is required to be performed, the target image and the reference image are acquired from the electronic device locally, another storage device associated with the electronic device or the cloud. In order to ensure the association relationship between the target images and the reference images, the target images and the reference images may be numbered respectively according to the image acquisition positions so that the target image and the reference image at the same acquisition position have the same number. Accordingly, when the target image and the reference image are acquired, the target image and the reference image with the same number are acquired.</p><p id="p-0073" num="0072">For example, the target image may also be transmitted to the electronic device in real time when the acquisition device (such as an unmanned aerial vehicle) performs the target image acquisition. The reference image is stored in the electronic device locally, another storage device associated with the electronic device or the cloud. Accordingly, when the electronic device receives a target image acquired by the acquisition device in real time, a reference image associated with the target image is acquired from the electronic device locally, another storage device associated with the electronic device or the cloud.</p><p id="p-0074" num="0073">In step S<b>102</b>, target building features of the target image and reference building features of the reference image are extracted, respectively.</p><p id="p-0075" num="0074">Since the building group in the target image is complex and diverse, and pedestrians and vehicles on the roads have a great influence on the building area, in order to improve the accuracy of the extracted building features, in an embodiment, the building features in the target image and the reference image are extracted through deep learning so that the extracted features can reflect the semantic information in the images and the extracted features are richer and more comprehensive.</p><p id="p-0076" num="0075">It is to be understood that since the building feature extraction is performed on the entire image in the present application, the single building in the image does not need to be segmented, the feature extraction is performed based on the single building, thereby reducing the amount of computation in the feature extraction process. Meanwhile, in the case of dense distribution of buildings in the image, the single building is segmented, then the feature extraction is performed on the segmented single building, and at this point, the accuracy of the extracted features is low, which will affect the accuracy of the illegal building identification results of the final illegal building identification.</p><p id="p-0077" num="0076">It is to be noted that when the illegal building identification is performed, a to-be-identified area is usually divided according to administrative regions such as townships and towns. Therefore, the number of image samples acquired in the to-be-identified area is limited. Due to the significant difference between different images, the target image and the reference image are bonded to each other to train a feature extraction model based on the siamese idea. Accordingly, when the feature extraction is performed by using the feature extraction model, the building feature extraction is performed on the target image and the reference image associated with the target image by using the same feature extraction model and model parameters, so as to ensure the consistency of the extracted building features.</p><p id="p-0078" num="0077">In step S<b>103</b>, an illegal building identification result of the target image is determined according to the target building features and the reference building features.</p><p id="p-0079" num="0078">For example, the step in which an illegal building identification result of the target image is determined according to the target building features and the reference building features may be that: feature fusion is performed on the target building features and the reference building features and the illegal building recognition result of the target image is determined according to fused features.</p><p id="p-0080" num="0079">In an embodiment, a difference between the target building features and the reference building features may be calculated, and the resulting difference may be taken as the feature fusion result. It is to be understood that the feature fusion is performed by a manner of calculating the difference, which can highlight the fused building features corresponding to the dissimilar areas between the target image and the reference image. Accordingly, when the illegal building identification is performed by fusing building features, the accuracy of identification results can be significantly improved.</p><p id="p-0081" num="0080">In an optional implementation of the embodiment of the present application, the step in which the illegal building identification result of the target image is determined may be that: the dichotomous classification is performed on the building area in the target image: classifying the building area into the presence of the illegal building and the absence of the illegal building. Accordingly, when the illegal building identification is performed on the target image, the classification result may be obtained based on a classification model and according to the fused building features obtained after the feature fusion of the target building features and the reference building features.</p><p id="p-0082" num="0081">In another optional implementation of the embodiment of the present application, the step in which the illegal building identification result of the target image is determined may also be that: whether the target image includes an illegal building area is determined, and if the target image includes an illegal building area, position coordinates of the illegal building area are determined.</p><p id="p-0083" num="0082">For example, the illegal building area in the target image is detected based on a detection model and according to the fused building features obtained after the feature fusion of the target building features and the reference building features, and the position coordinates of the illegal building area are determined.</p><p id="p-0084" num="0083">In an embodiment, an identification loss function and a positioning loss function may be introduced in the detection model training process, and a network parameter in the detection model may be optimized and adjusted based on the identification loss function and the positioning deviation loss function. The identification loss function is set to characterize a deviation between a classification result outputted by the model and an actual classification result. The positioning loss function is set to characterize a deviation between position coordinates of the illegal building area outputted by the model and actual position coordinates of the illegal building area.</p><p id="p-0085" num="0084">The illegal building area may be represented by circular areas or rectangular areas. When the illegal building area is represented by using a circular area, the position coordinates may include a center position and a circular radius. When the illegal building area is represented by using a rectangular area, the position coordinates include coordinates of one vertex of the rectangular area, a rectangle length value, and a rectangle width value; or, the position coordinates include coordinates of at least two vertexes, such as coordinates of two vertexes on a diagonal.</p><p id="p-0086" num="0085">It is to be understood that in order to avoid the confusion of illegal building areas, a unified position coordinate determination manner is usually adopted. For example, the position coordinates include coordinates of the upper left vertex of the rectangular area, a rectangle length value, and a rectangle width value. Accordingly, with the upper left vertex as a starting point, one side of the rectangle is determined, which extends in a direction parallel to the length direction of the target image for a distance of the rectangle length value, and the other side of the rectangle is determined which extends in a direction parallel to the width direction of the target image for a distance of the rectangle width value, so as to determine the illegal building area.</p><p id="p-0087" num="0086">In order to represent the illegal building area more intuitively, the illegal building area may be marked in the target image or the reference image according to the position coordinates of the illegal building area. In order to avoid the difference between the coordinates of the illegal building area and the coordinates of the illegal building area due to the different acquisition angles, the illegal building area is usually marked in the target image.</p><p id="p-0088" num="0087">In the present application, a target image and a reference image associated with the target image are acquired, target building features of the target image and reference building features of the reference image are extracted, respectively, and an illegal building identification result of the target image is determined according to the target building features and the reference building features. In the preceding technical scheme, the reference image associated with the target image is acquired, the target image and the reference image are bonded to each other, and feature extraction is performed on the bonded images, so as to perform illegal building identification on the target image based on the building features of the reference image, thereby achieving the illegal building automatic identification and reducing the data throughput in the process of illegal building identification. Meanwhile, based on the siamese idea, building feature extraction is performed on the target image and reference image, and then illegal building identification is performed according to the extracted building features, thereby improving the accuracy of identification results.</p><heading id="h-0007" level="1">Embodiment Two</heading><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of an illegal building identification method according to Embodiment two of the present application. The embodiment of the present application is optimized on the basis of the technical schemes of the preceding embodiments.</p><p id="p-0090" num="0089">In an embodiment, the operation of &#x201c;extracting the target building features of the target image and the reference building features of the reference image associated with the target image, respectively&#x201d; is refined to &#x201c;extracting target basis features of the target image and reference basis features of the reference image associated with the target image, respectively; and performing, at each of at least two set scales, feature extraction on the target basis features and the reference basis features, respectively, to obtain the target building features and the reference building features at the at least two scales&#x201d;, so as to perfect the building feature extraction manner.</p><p id="p-0091" num="0090">As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the illegal building identification method includes the steps described below.</p><p id="p-0092" num="0091">In step S<b>101</b>, a target image and a reference image associated with the target image are acquired.</p><p id="p-0093" num="0092">In step S<b>202</b>, target basis features of the target image and reference basis features of the reference image associated with the target image are extracted, respectively.</p><p id="p-0094" num="0093">For example, the target basis features of the target image and the reference basis features of the reference image associated with the target image are extracted based on a deep residual network, respectively. The network depth of the deep residual network may be determined according to empirical values or a large number of tests. For example, the network depth may be set to 50.</p><p id="p-0095" num="0094">In step S<b>203</b>, at at least two set scales, feature extraction is performed on the target basis features and the reference basis features, respectively, to obtain the target building features and the reference building features at the at least two scales.</p><p id="p-0096" num="0095">Since the features at different scales contain different dimensions of detail information, feature mining is further performed on the extracted basis features through at least two scales, thereby obtaining a feature map with stronger characterization ability and providing a guarantee for improving the identification accuracy of illegal building identification results.</p><p id="p-0097" num="0096">The number of scales may be determined by technicians according to requirements or empirical values or determined according to a model training result during the model training process. For example, the number of scales may be set to 5.</p><p id="p-0098" num="0097">For example, the feature extraction may be performed on the target basis features and the reference basis features at at least two set scales by using a feature pyramid model, respectively, to obtain the target building features and the building features at at least two scales.</p><p id="p-0099" num="0098">In step S<b>204</b>, an illegal building identification result of the target image is determined according to the target building features and the reference building features.</p><p id="p-0100" num="0099">Since the target building features and the reference building features both contain feature maps of different scales, the target building features and the reference building features at different scales are required to be fused when the illegal building identification is performed on the target image according to the target building features and the reference building features.</p><p id="p-0101" num="0100">In an embodiment, the step in which an illegal building identification result of the target image is determined according to the target building features and the reference building features may be that: feature fusion is performed on the target building features and the reference building features at each scale and the illegal building identification result of the target image is determined according to feature fusion results at at least two scales.</p><p id="p-0102" num="0101">For example, the step in which feature fusion is performed on the target building features and the reference building features at each scale may be that: a difference between the target building features and the reference building features under each scale is calculated, and the difference is taken as a feature fusion result under the scale.</p><p id="p-0103" num="0102">It is to be noted that the difference between the target building features and the reference building features at each scale is calculated, and the difference is taken as a feature fusion result at the scale, so that the difference between the target image and the reference image at the scale can be highlighted and reference information can be richer and more comprehensive by referring to the difference between the target image and the reference image at each scale when the illegal building identification result of the target image is determined according to the feature fusion results at at least two scales, thereby improving the accuracy of the illegal building identification results.</p><p id="p-0104" num="0103">In the embodiment of the present application, the building feature extraction process of the target image and the reference image is refined into the operations of extracting the basis features of the target image and the basis features of the reference image, respectively, and performing multi-scale feature extraction on the basis features to obtain the building features, so that the illegal building identification result of the target image is determined based on the multi-scale target building features and reference building features, thereby improving the accuracy and richness of the extracted building features, improving the characterization ability of the building features, and further improving the accuracy of the illegal building identification results.</p><heading id="h-0008" level="1">Embodiment Three</heading><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flowchart of an illegal building identification method according to Embodiment three of the present application. The embodiment of the present application is optimized on the basis of the technical schemes of the preceding embodiments.</p><p id="p-0106" num="0105">In an embodiment, before the operation of &#x201c;extracting the reference building features of the reference image&#x201d;, there is an additional operation of &#x201c;performing coordinate transform on the reference image according to the target image, where the coordinate transform includes at least one of shrinking transform, stretching transform, rotation transform or translation transform&#x201d;, so that the coordinates of the target image match with the coordinates of the reference image.</p><p id="p-0107" num="0106">As shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the illegal building identification method includes the steps described below.</p><p id="p-0108" num="0107">In step S<b>301</b>, a target image and a reference image associated with the target image are acquired.</p><p id="p-0109" num="0108">In step S<b>302</b>, coordinate transform is performed on the reference image according to the target image.</p><p id="p-0110" num="0109">The coordinate transform includes at least one of shrinking transform, stretching transform, rotation transform or translation transform.</p><p id="p-0111" num="0110">In the process of acquiring the target image and the reference image, the case in which the coordinates of the target image may not match with the coordinates of the reference image due to the difference of the acquisition parameters of the acquisition devices such as the acquisition positions and the acquisition angles of the acquisition devices may occur. The mismatching between the coordinates of the target image and the coordinates of the reference image has a certain impact on the accuracy of the illegal building identification result of the target image.</p><p id="p-0112" num="0111">In order to ensure the accuracy of the illegal building identification result, before the feature extraction is performed on the reference image, the coordinate transform is required to be performed on the reference image according to the target image, so as to enable the coordinates of the reference image to match with the coordinates of the target image.</p><p id="p-0113" num="0112">In an embodiment, target orientation features of the target image and the reference orientation features of the reference image may be extracted by using a machine learning model, respectively; the deformation of the reference image relative to the target image is determined according to the matching of the target orientation features and the reference orientation features; and the reference image is adjusted according to the deformation to enable the coordinates of the adjusted reference image to match with the coordinates of the target image.</p><p id="p-0114" num="0113">For example, target key points and target descriptors of the target image and reference key points and reference descriptors of the reference image are extracted, respectively; a matching operation is performed on the target key points and the reference key points according to the target descriptors and the reference descriptors; and a transform matrix is determined according to a matching result, and the coordinate transform is performed on the reference image according to the transform matrix.</p><p id="p-0115" num="0114">For example, the key point and descriptor extraction is performed on the target image by using the scale-invariant feature transform (SIFT) algorithm to obtain the target key points and the target descriptors, and the key point and descriptor extraction is performed on the reference image by using the SIFT algorithm to obtain the reference key points and the reference descriptors. The matching operation is performed on the target key points and the reference key points by using the K-dimensional tree (KD Tree) according to the matching of the target descriptors and the reference descriptors to obtain initial matching relationships. Invalid initial matching relationships are removed by using the random sample consensus (RANSAC) algorithm to obtain a target matching relationship. A transform matrix between the reference image and the target image is determined according to the target matching relationship. The coordinate transform is performed on the reference image according to the transform matrix to enable the coordinates of the transformed reference image to match with the coordinates of the target image.</p><p id="p-0116" num="0115">In step S<b>303</b>, target building features of the target image and reference building features of the transformed reference image are extracted, respectively.</p><p id="p-0117" num="0116">It is to be noted that the operation of extracting the target building features of the target image may be performed before step S<b>302</b>, after step S<b>303</b>, or simultaneously with step S<b>302</b>, and the embodiment of the present application does not limit the execution sequence of steps S<b>302</b> and S<b>303</b>.</p><p id="p-0118" num="0117">In step S<b>304</b>, an illegal building identification result of the target image is determined according to the target building features and the reference building features.</p><p id="p-0119" num="0118">In the embodiment of the present application, the coordinate transform is performed on the reference image according to the target image before the reference building features of the reference image are extracted, to enable the coordinates of the reference image to match with the coordinates of the target image, thereby providing a guarantee for the accuracy of the illegal building identification results.</p><p id="p-0120" num="0119">On the basis of the technical scheme of each of the preceding embodiments, in order to eliminate the influence of acquisition parameters such as image size, illumination environment, and acquisition angle on the illegal building identification result, after the target image and the reference image are acquired, the target image and the reference image may be preprocessed before the target image and the reference image are processed.</p><p id="p-0121" num="0120">In an optional implementation of the embodiment of the present application, the target image, or the reference image, or the target image and the reference image are resized so that the size of the target image and the size of the reference image are consistent.</p><p id="p-0122" num="0121">In another optional implementation of the embodiment of the present application, grayscale transform (rgb2gray) is performed on the target image and the reference image to eliminate hue and saturation information of the image while retaining brightness information, thereby achieving the transformation of an RGB (red-green-blue) image or a color image into a grayscale image.</p><p id="p-0123" num="0122">In another optional implementation of the embodiment of the present application, histogram equalization is performed on the target image and the reference image to enhance the image contrast and remove the influence of factors such as illumination.</p><p id="p-0124" num="0123">It is to be noted that when the image is preprocessed by using at least two of the preceding manners, the sequence of preprocessing operations may not be limited.</p><heading id="h-0009" level="1">Embodiment Four</heading><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>4</b>A</figref> is a flowchart of an illegal building identification method according to Embodiment four of the present application. The embodiment of the present application is optimized on the basis of the technical schemes of the preceding embodiments.</p><p id="p-0126" num="0125">As shown in <figref idref="DRAWINGS">FIG. <b>4</b>A</figref>, the illegal building identification method includes steps S<b>410</b> to S<b>430</b>:</p><p id="p-0127" num="0126">S<b>410</b>, image acquisition;</p><p id="p-0128" num="0127">S<b>420</b>, image matching;</p><p id="p-0129" num="0128">S<b>430</b>, image detection.</p><p id="p-0130" num="0129">The image acquisition process includes the steps described below.</p><p id="p-0131" num="0130">In step S<b>411</b>, a target image is acquired.</p><p id="p-0132" num="0131">In step S<b>412</b>, a reference image that is at the same acquisition position as the target image is acquired.</p><p id="p-0133" num="0132">The target image is an image containing buildings acquired by controlling an unmanned aerial vehicle along a set route and at a set frequency at the current time. The reference image is an image containing buildings acquired by controlling an unmanned aerial vehicle along a set route and at a set frequency at a historical time.</p><p id="p-0134" num="0133">Every time the unmanned aerial vehicle is controlled to acquire images, the image acquisition parameters are consistent. The image acquisition parameters include an acquisition frequency and an acquisition angle. The acquisition route, acquisition frequency, and other acquisition parameters of the unmanned aerial vehicle may be determined by technicians according to requirements or empirical values.</p><p id="p-0135" num="0134">It is to be noted that even if the acquisition parameters of the unmanned aerial vehicle are set consistently in the acquisition process, the case in which the acquisition positions or acquisition angles of the images are inconsistent due to external environment or system errors may also occur. Therefore, it is only necessary to ensure that the distance difference between the acquisition position of the newly acquired target image and the acquisition position of the reference image satisfies a set distance threshold and the angle difference between the acquisition angle of the newly acquired target image and the acquisition angle of the reference image satisfies a set angle threshold. The set distance threshold and the set angle threshold may be determined by technicians according to requirements or empirical values or repeatedly determined by performing a large number of tests.</p><p id="p-0136" num="0135">For example, the resolution of images acquired by the unmanned aerial vehicle is 4000*6000, where the height is 4000 and the width is 6000.</p><p id="p-0137" num="0136">The image matching process includes the steps described below.</p><p id="p-0138" num="0137">In step S<b>421</b>, image preprocessing is performed on the reference image and the target image.</p><p id="p-0139" num="0138">With reference to <figref idref="DRAWINGS">FIG. <b>4</b>B</figref> which shows a block diagram of an image matching process, the reference image is Img1 and the target image is Img2.</p><p id="p-0140" num="0139">The image preprocessing operation includes resize transform, which is set to resize the reference image and the target image so that the size of the processed target image and the size of the processed reference image are the same. For example, the size is unified as 1000*1500.</p><p id="p-0141" num="0140">The image preprocessing operation further includes grayscale (rgb2gray) transform, which is set to transform the resized reference image and the resized target image from color images into grayscale images.</p><p id="p-0142" num="0141">The image preprocessing operation further includes histogram equalization (EqualizeHist) to eliminate the influence of different acquisition environments such as illumination on the detection result of the target image and the reference image.</p><p id="p-0143" num="0142">In step S<b>422</b>, an image matching operation is performed on the reference image and the target image to obtain a transform matrix when the reference image is transformed into the target image.</p><p id="p-0144" num="0143">The image matching operation includes key point and descriptor extraction, which is set to perform the feature extraction operation on the reference image through the SIFT algorithm to obtain reference key points and reference descriptors of the reference image, and perform the feature extraction operation on the target image through the same algorithm to obtain target key points and target descriptors of the target image.</p><p id="p-0145" num="0144">The image matching operation further includes key point matching, which is set to match the target key points and the reference key points according to the consistency of the reference descriptors and the target descriptors through the KD Tree algorithm to obtain a key point matching result.</p><p id="p-0146" num="0145">The target matching operation further includes abnormal point elimination, which is set to remove invalid matching relationships in the key point matching result through the RANSAC algorithm to obtain the final accurate matching relationships and determine the transform matrix corresponding to the accurate matching relationships.</p><p id="p-0147" num="0146">In step S<b>423</b>, coordinate transform is performed on the reference image according to the transform matrix.</p><p id="p-0148" num="0147">The coordinate transform includes at least one of shrinking transform, stretching transform, rotation transform or translation transform. The coordinates of the transformed reference image are consistent with the coordinates of the target image.</p><p id="p-0149" num="0148">For example, <figref idref="DRAWINGS">FIGS. <b>4</b>C and <b>4</b>D</figref> show reference image Img1 and target image Img2, respectively. <figref idref="DRAWINGS">FIG. <b>4</b>E</figref> shows transformed reference image Img1_trans. With the comparison of <figref idref="DRAWINGS">FIG. <b>4</b>E</figref> and <figref idref="DRAWINGS">FIG. <b>4</b>C</figref>, <figref idref="DRAWINGS">FIG. <b>4</b>E</figref> is obtained by rotating the reference image Img1 in <figref idref="DRAWINGS">FIG. <b>4</b>C</figref>. It can be seen from the comparison between <figref idref="DRAWINGS">FIG. <b>4</b>E</figref> and <figref idref="DRAWINGS">FIG. <b>4</b>D</figref> that the coordinates of the transformed reference image and the target image are consistent.</p><p id="p-0150" num="0149">The image detection process includes the steps described below.</p><p id="p-0151" num="0150">In step S<b>431</b>, basis features of the target image and basis features of the transformed reference image are extracted by using a first feature extraction model.</p><p id="p-0152" num="0151">The image detection process is illustrated with reference to the structure block diagram of the image detection process shown in <figref idref="DRAWINGS">FIG. <b>4</b>F</figref>.</p><p id="p-0153" num="0152">For example, the basis features of the target image and the basis features of the transformed reference image are extracted by using the deep residual network, respectively. The network parameters of the deep residual network used to extract the basis features of the target image are consistent with the network parameters of the deep residual network used to extract the basis features of the transformed reference image. The network depth of the deep residual network may be determined by technicians according to requirements or empirical values or repeatedly determined by performing a large number of tests. For example, the network depth may be 50.</p><p id="p-0154" num="0153">In step S<b>432</b>, building features of different scales in the basis features of the target image and building features of different scales in the basis features of the reference image are extracted by using a second feature extraction model.</p><p id="p-0155" num="0154">For example, the building features of different scales in the basis features of the target image and the building features of different scales in the basis features of the reference image are extracted, respectively, to obtain target building feature Fea1 and reference building feature Fea2.</p><p id="p-0156" num="0155">It is to be understood that since the image acquired by the unmanned aerial vehicle has high resolution and complex content, and the building area is small, has complex and diverse building groups, and is easily interfered by other factors such as pedestrians on the road, building features at different scales are required as the basis for subsequent image detection, thereby improving the accuracy of detection results.</p><p id="p-0157" num="0156">In step S<b>433</b>, a difference between the building features of the target image and the building features of the reference image at each scale is calculated to obtain a fusion feature.</p><p id="p-0158" num="0157">It is to be understood that the target building features and the reference building features at each scale are fused by using the manner of feature subtraction to obtain the fused feature Feature, which can highlight the difference between the target image and the reference image at the same scale and obtain the suspected illegal building area.</p><p id="p-0159" num="0158">It is to be noted that the sequence of the reference image and the target image is not required to be concerned when the difference operation is performed, which improves the generalization ability of the detection model.</p><p id="p-0160" num="0159">In step S<b>434</b>, whether the target image includes an illegal building area is determined based on a detection model and according to the fusion features at all scales.</p><p id="p-0161" num="0160">In step S<b>435</b>, if the target image includes the illegal building area, coordinates of the illegal building area are outputted.</p><p id="p-0162" num="0161">In step S<b>436</b>, the illegal building area is marked in the target image or the transformed reference image according to the coordinates of the illegal building area.</p><p id="p-0163" num="0162">The illegal building area includes at least one illegal building. The illegal building may be color steel plates and scaffoldings added on the basis of existing buildings as well as roof repair, and may also be houses that are built in areas where housing construction is not allowed.</p><p id="p-0164" num="0163">The detection model may be constructed based on a neural network model. In the model training stage of the detection model, an identification loss function Focal_loss and a positioning loss function SmoothL1_loss may be introduced, and a network parameter in the detection model may be optimized and adjusted based on the identification loss function and the positioning deviation loss function. The identification loss function is set to characterize a deviation between a classification result outputted by the model and an actual classification result. The positioning loss function is set to characterize a deviation between position coordinates of the illegal building area outputted by the model and actual position coordinates of the illegal building area.</p><p id="p-0165" num="0164">With reference to <figref idref="DRAWINGS">FIG. <b>4</b>G</figref> which shows a schematic diagram of a marking result, the illegal area is marked in the target image by using a rectangular box.</p><p id="p-0166" num="0165">It is to be understood that the illegal building area is marked in the target image so that the position of the illegal building area can be displayed intuitively, which provides a basis for offline law enforcement officers to perform targeted law enforcement based on the marked illegal building area, thereby reducing the workload of law enforcement officers and improving the identification efficiency of illegal building areas and the accuracy of identification results.</p><heading id="h-0010" level="1">Embodiment Five</heading><p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a structure diagram of an illegal building identification apparatus according to Embodiment five of the present application. The embodiment of the present application is applicable to the case of identifying illegal buildings in images. The apparatus is implemented by software, or hardware, or software and hardware and is specifically configured in an electronic device.</p><p id="p-0168" num="0167">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the illegal building identification apparatus <b>500</b> includes an image acquisition module <b>501</b>, a building feature extraction module <b>502</b>, and an identification result determination module <b>503</b>.</p><p id="p-0169" num="0168">The image acquisition module <b>501</b> is configured to acquire a target image and a reference image associated with the target image.</p><p id="p-0170" num="0169">The building feature extraction module <b>502</b> is configured to extract target building features of the target image and reference building features of the reference image, respectively.</p><p id="p-0171" num="0170">The identification result determination module <b>503</b> is configured to determine, according to the target building features and the reference building features, an illegal building identification result of the target image.</p><p id="p-0172" num="0171">In the present application, the image acquisition module acquires a target image and a reference image associated with the target image, the building feature extraction module extracts target building features of the target image and reference building features of the reference image, respectively, and the identification result determination module determines an illegal building identification result of the target image according to the target building features and the reference building features. In the preceding technical scheme, the reference image associated with the target image is acquired, the target image and the reference image are bonded to each other, and feature extraction is performed on the bonded images, so as to perform illegal building identification on the target image based on the building features of the reference image, thereby achieving the illegal building automatic identification and reducing the data throughput in the process of illegal building identification. Meanwhile, based on the siamese idea, building feature extraction is performed on the target image and the reference image, and then illegal building identification is performed according to the extracted building features, thereby improving the accuracy of identification results.</p><p id="p-0173" num="0172">In an embodiment, a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold, or an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold, or a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold and an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold.</p><p id="p-0174" num="0173">In an embodiment, the building feature extraction module <b>502</b> includes a basis feature extraction unit and a building feature extraction unit.</p><p id="p-0175" num="0174">The basis feature extraction unit is configured to extract target basis features of the target image and reference basis features of the reference image associated with the target image, respectively.</p><p id="p-0176" num="0175">The building feature extraction unit is configured to, perform, at at least two set scales, feature extraction on the target basis features and the reference basis features, respectively, to obtain the target building features and the reference building features at the at least two scales.</p><p id="p-0177" num="0176">In an embodiment, the identification result determination module <b>503</b> includes a feature fusion unit and an identification result determination unit.</p><p id="p-0178" num="0177">The feature fusion unit is configured to perform feature fusion on the target building features and the reference building features at each scale.</p><p id="p-0179" num="0178">The identification result determination unit is configured to determine, according to feature fusion results at at least two scales, the illegal building identification result of the target image.</p><p id="p-0180" num="0179">In an embodiment, the feature fusion unit includes a feature fusion subunit.</p><p id="p-0181" num="0180">The feature fusion subunit is configured to calculate a difference between the target building features and the reference building features at each scale, and take the difference as a feature fusion result at the scale.</p><p id="p-0182" num="0181">In an embodiment, the basis feature extraction unit includes a basis feature extraction subunit.</p><p id="p-0183" num="0182">The basis feature extraction subunit is configured to extract, based on a deep residual network, the target basis features of the target image and the reference basis features of the reference image associated with the target image, respectively.</p><p id="p-0184" num="0183">In an embodiment, the apparatus further includes a coordinate transform module, which is configured to: before the reference building features of the reference image are extracted, perform coordinate transform on the reference image according to the target image.</p><p id="p-0185" num="0184">The coordinate transform includes at least one of shrinking transform, stretching transform, rotation transform or translation transform.</p><p id="p-0186" num="0185">In an embodiment, the coordinate transform module includes a key point extraction unit, a key point matching unit, and a coordinate transform unit.</p><p id="p-0187" num="0186">The key point extraction unit, which is configured to extract target key points and target descriptors of the target image and reference key points and reference descriptors of the reference image, respectively.</p><p id="p-0188" num="0187">The key point matching unit is configured to perform a matching operation on the target key points and the reference key points according to the target descriptors and the reference descriptors.</p><p id="p-0189" num="0188">The coordinate transform unit is configured to determine, according to a matching result, a transform matrix, and perform the coordinate transform on the reference image according to the transform matrix.</p><p id="p-0190" num="0189">In an embodiment, the identification result determination module <b>503</b> includes an illegal building area determination unit and a position coordinates determination unit.</p><p id="p-0191" num="0190">The illegal building area determination unit is configured to determine, according to the target building features and the reference building features, whether the target image comprises an illegal building area.</p><p id="p-0192" num="0191">The position coordinates determination unit is configured to, if the target image comprises the illegal building area, determine position coordinates of the illegal building area.</p><p id="p-0193" num="0192">The preceding illegal building identification apparatus may execute the illegal building identification method provided by any of the embodiments of the present application and has functional modules for and beneficial effects of executing the illegal building identification method.</p><heading id="h-0011" level="1">Embodiment Six</heading><p id="p-0194" num="0193">According to the embodiment of the present application, the present application further provides an electronic device and a readable storage medium.</p><p id="p-0195" num="0194"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a block diagram of an electronic device for implementing an illegal building identification method in an embodiment of the present application. The electronic device is intended to represent each form of digital computer, for example, a laptop computer, a desktop computer, a worktable, a personal digital assistant, a server, a blade server, a mainframe computer or another applicable computer. The electronic device may also represent each form of mobile device, for example, a personal digital assistant, a cellphone, a smartphone, a wearable device or another similar computing device. Herein the shown components, the connections and relationships between these components, and the functions of these components are illustrative only and are not intended to limit the implementation of the present application as described or claimed herein.</p><p id="p-0196" num="0195">As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the electronic device includes one or more processors <b>601</b>, a memory <b>602</b>, and interfaces for connecting components, including a high-speed interface and a low-speed interface. The components are interconnected to each other by different buses and may be mounted on a common mainboard or in other manners as desired. The processor may process instructions executed in the electronic device, including instructions stored in or on the memory to make graphic information of a graphical user interface (GUI) displayed on an external input/output device (for example, a display device coupled to an interface). In other implementations, if required, multiple processors, or multiple buses, or multiple processors and multiple buses may be used with multiple memories. Similarly, multiple electronic devices may be connected, each providing some necessary operations (for example, serving as a server array, a set of blade servers or a multi-processor system). <figref idref="DRAWINGS">FIG. <b>6</b></figref> shows one processor <b>601</b> by way of example.</p><p id="p-0197" num="0196">The memory <b>602</b> is the non-transitory computer-readable storage medium provided in the present application. The memory has instructions executable by at least one processor stored thereon to cause the at least one processor to perform the illegal building identification method provided in the present application. The non-transitory computer-readable storage medium of the present application stores computer instructions for causing a computer to perform the illegal building identification method provided in the present application.</p><p id="p-0198" num="0197">The memory <b>602</b> as a non-transitory computer-readable storage medium is configured to store a non-transitory software program, a non-transitory computer-executable program and modules, for example, program instructions/modules (for example, the image acquisition module <b>501</b>, the building feature extraction module <b>502</b>, and the identification result determination module <b>503</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>) corresponding to the illegal building identification method provided in the embodiments of the present application. The processor <b>601</b> executes non-transitory software programs, instructions and modules stored in the memory <b>602</b> to execute the each function application and data processing of a server, that is, implement the illegal building identification method provided in the preceding method embodiments.</p><p id="p-0199" num="0198">The memory <b>602</b> may include a program storage region and a data storage region. The program storage region may store an operating system and an application program required by at least one function. The data storage region may store data created based on the use of the electronic device for performing the illegal building identification method. Additionally, the memory <b>602</b> may include a high-speed random-access memory and a non-transient memory, for example, at least one disk memory, a flash memory or another non-transient solid-state memory. In some embodiments, the memory <b>602</b> optionally includes memories disposed remote from the processor <b>601</b>, and these remote memories may be connected, through a network, to the electronic device for performing the illegal building identification method. Examples of the preceding network include, but are not limited to, the Internet, an intranet, a local area network, a mobile communication network and a combination thereof.</p><p id="p-0200" num="0199">The electronic device for performing the illegal building identification method may further include an input device <b>603</b> and an output device <b>604</b>. The processor <b>601</b>, the memory <b>602</b>, the input device <b>603</b>, and the output device <b>604</b> may be connected by a bus or in other manners. <figref idref="DRAWINGS">FIG. <b>6</b></figref> uses connection by a bus as an example.</p><p id="p-0201" num="0200">The input device <b>603</b> may receive input number or character information and generate key signal input related to user settings and function control of the electronic device for performing the illegal building identification method. The input device <b>603</b> may be, for example, a touchscreen, a keypad, a mouse, a trackpad, a touchpad, a pointing stick, one or more mouse buttons, a trackball or a joystick. The output device <b>604</b> may include, for example, a display device, an auxiliary lighting device (for example, a light-emitting diode (LED)) or a haptic feedback device (for example, a vibration motor). The display device may include, but is not limited to, a liquid-crystal display (LCD), an LED display, and a plasma display. In some implementations, the display device may be a touchscreen.</p><p id="p-0202" num="0201">Each implementation of the systems and techniques described herein may be implemented in digital electronic circuitry, integrated circuitry, an application-specific integrated circuit (ASIC), computer hardware, firmware, software or a combination thereof. Each implementation may include implementations in one or more computer programs. The one or more computer programs may be executable, or interpretable, or executable and interpretable on a programmable system including at least one programmable processor. The programmable processor may be a dedicated or general-purpose programmable processor for receiving data and instructions from a memory system, at least one input device and at least one output device and transmitting data and instructions to the memory system, the at least one input device and the at least one output device.</p><p id="p-0203" num="0202">These computing programs (also referred to as programs, software, software applications or codes) include machine instructions of a programmable processor. These computing programs may be implemented in a high-level procedural or object-oriented programming language or in an assembly/machine language. As used herein, the terms &#x201c;machine-readable medium&#x201d; and &#x201c;computer-readable medium&#x201d; refer to any computer program product, device or apparatus (for example, a magnetic disk, an optical disk, a memory or a programmable logic device (PLD)) for providing machine instructions and/or data for a programmable processor, including a machine-readable medium for receiving machine instructions as machine-readable signals. The term &#x201c;machine-readable signal&#x201d; refers to any signal used in providing machine instructions or data for a programmable processor.</p><p id="p-0204" num="0203">In order to provide the interaction with a user, the systems and techniques described herein may be implemented on a computer. The computer has a display device (for example, a cathode-ray tube (CRT) or an LCD monitor) for displaying information to the user and a keyboard and a pointing device (for example, a mouse or a trackball) through which the user may provide input to the computer. Other types of devices may also be configured to provide interaction with a user. For example, feedback provided for the user may be sensory feedback in any form (for example, visual feedback, auditory feedback or haptic feedback). Moreover, input from the user may be received in any form (including acoustic input, voice input or haptic input).</p><p id="p-0205" num="0204">The systems and techniques described herein may be implemented in a computing system including a back-end component (for example, serving as a data server), a computing system including a middleware component (for example, an application server), a computing system including a front-end component (for example, a user computer having a graphical user interface or a web browser through which a user may interact with implementations of the systems and techniques described herein) or a computing system including any combination of such back-end, middleware or front-end components. Components of a system may be interconnected by any form or medium of digital data communication (for example, a communication network). Examples of the communication network include a local area network</p><p id="p-0206" num="0205">(LAN), a wide area network (WAN), the Internet, and a blockchain network.</p><p id="p-0207" num="0206">The computing system may include clients and servers. A client and a server are generally remote from each other and typically interact through a communication network. The relationship between the client and the server arises by virtue of computer programs running on respective computers and having a client-server relationship to each other.</p><p id="p-0208" num="0207">In the present application, a target image and a reference image associated with the target image are acquired, target building features of the target image and reference building features of the reference image are extracted, respectively, and an illegal building identification result of the target image is determined according to the target building features and reference building features. In the preceding technical scheme, the reference image associated with the target image is acquired, the target image and the reference image are bonded to each other, and feature extraction is performed on the bonded images, so as to perform illegal building identification on the target image based on the building features of the reference image, thereby achieving the illegal building automatic identification and reducing the data throughput in the process of illegal building identification. Meanwhile, based on the siamese idea, building feature extraction is performed on the target image and the reference image, and then illegal building identification is performed according to the extracted building features, thereby improving the accuracy of identification results.</p><p id="p-0209" num="0208">It is to be understood that the preceding flow in each form may be used, with steps reordered, added or removed. For example, the steps described in the present application may be executed in parallel, in sequence or in a different order as long as the desired results of the technical schemes disclosed in the present application can be achieved. The execution sequence of these steps is not limited herein.</p><p id="p-0210" num="0209">The scope of the present application is not limited to the preceding implementations. It is to be understood by those skilled in the art that modifications, combinations, subcombinations, and substitutions may be made depending on design requirements and other factors. Any modification, equivalent substitution, improvement and the like made within the spirit and principle of the present application is within the scope of the present application.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An illegal building identification method, comprising:<claim-text>acquiring a target image and a reference image associated with the target image;</claim-text><claim-text>extracting a target building feature of the target image and a reference building feature of the reference image, respectively; and</claim-text><claim-text>determining, according to the target building feature and the reference building feature, an illegal building identification result of the target image.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold; or an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold; or a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold and an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein extracting the target building feature of the target image and the reference building feature of the reference image associated with the target image, respectively, comprises:<claim-text>extracting a target basis feature of the target image and a reference basis feature of the reference image associated with the target image, respectively; and</claim-text><claim-text>performing, at each of at least two set scales, feature extraction on the target basis feature and the reference basis feature, respectively, to obtain a target building feature and a reference building feature at each of the at least two scales.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein determining, according to the target building feature and the reference building feature, the illegal building identification result of the target image comprises:<claim-text>performing feature fusion on the target building feature and the reference building feature at each of the at least two scales; and</claim-text><claim-text>determining, according to feature fusion results at the at least two scales, the illegal building identification result of the target image.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein performing the feature fusion on the target building feature and the reference building feature at each of the at least two scales comprises:<claim-text>calculating a difference between the target building feature and the reference building feature at each of the at least two scales, and taking the difference as a feature fusion result at the each of the at least two scales.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein extracting the target basis feature of the target image and the reference basis feature of the reference image associated with the target image, respectively, comprises:<claim-text>extracting, based on a deep residual network, the target basis feature of the target image and the reference basis feature of the reference image associated with the target image, respectively.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, before extracting the reference building feature of the reference image, further comprising:<claim-text>performing coordinate transform on the reference image according to the target image;</claim-text><claim-text>wherein the coordinate transform comprises at least one of shrinking transform, stretching transform, rotation transform or translation transform.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein performing the coordinate transform on the reference image according to the target image comprises:<claim-text>extracting target key points and target descriptors of the target image, and reference key points and reference descriptors of the reference image, respectively;</claim-text><claim-text>performing a matching operation on the target key points and the reference key points according to the target descriptors and the reference descriptors; and</claim-text><claim-text>determining, according to a matching result, a transform matrix, and performing the coordinate transform on the reference image according to the transform matrix.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein determining the illegal building identification result of the target image comprises:<claim-text>determining whether the target image comprises an illegal building area; and</claim-text><claim-text>in response to determining that the target image comprises the illegal building area, determining position coordinates of the illegal building area.</claim-text></claim-text></claim><claim id="CLM-10-18" num="10-18"><claim-text><b>10</b>.-<b>18</b>. (canceled)</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An electronic device, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a memory communicatively connected to the at least one processor;</claim-text><claim-text>wherein the memory has instructions executable by the at least one processor stored thereon, wherein the instructions are executed by the at least one processor to cause the at least one processor to perform:</claim-text><claim-text>acquiring a target image and a reference image associated with the target image;</claim-text><claim-text>extracting a target building feature of the target image and reference building a feature of the reference image, respectively; and</claim-text><claim-text>determining, according to the target building feature and the reference building feature, an illegal building identification result of the target image.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable storage medium having computer instructions stored thereon, wherein the computer instructions are configured to cause a computer to perform:<claim-text>acquiring a target image and a reference image associated with the target image;</claim-text><claim-text>extracting a target building feature of the target image and a reference building feature of the reference image, respectively; and</claim-text><claim-text>determining, according to the target building feature and the reference building feature, an illegal building identification result of the target image.</claim-text></claim-text></claim><claim id="CLM-00021" num="00021"><claim-text><b>21</b>. The electronic device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold; or an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold; or a distance difference between an acquisition position of the target image and an acquisition position of the reference image is less than a set distance threshold and an angle difference between an acquisition angle of the target image and an acquisition angle of the reference image is less than a set angle threshold.</claim-text></claim><claim id="CLM-00022" num="00022"><claim-text><b>22</b>. The electronic device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein extracting the target building feature of the target image and the reference building feature of the reference image associated with the target image, respectively, comprises:<claim-text>extracting a target basis feature of the target image and a reference basis feature of the reference image associated with the target image, respectively; and</claim-text><claim-text>performing, at each of at least two set scales, feature extraction on the target basis feature and the reference basis feature, respectively, to obtain a target building feature and a reference building feature at each of the at least two scales.</claim-text></claim-text></claim><claim id="CLM-00023" num="00023"><claim-text><b>23</b>. The electronic device according to <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein determining, according to the target building feature and the reference building feature, the illegal building identification result of the target image comprises:<claim-text>performing feature fusion on the target building feature and the reference building feature at each of the at least two scales; and</claim-text><claim-text>determining, according to feature fusion results at the at least two scales, the illegal building identification result of the target image.</claim-text></claim-text></claim><claim id="CLM-00024" num="00024"><claim-text><b>24</b>. The electronic device according to <claim-ref idref="CLM-00023">claim 23</claim-ref>, wherein performing the feature fusion on the target building feature and the reference building feature at each of the at least two scales comprises:<claim-text>calculating a difference between the target building feature and the reference building feature at each of the at least two scales, and taking the difference as a feature fusion result at the each of the at least two scales.</claim-text></claim-text></claim><claim id="CLM-00025" num="00025"><claim-text><b>25</b>. The electronic device according to <claim-ref idref="CLM-00022">claim 22</claim-ref>, wherein extracting the target basis feature of the target image and the reference basis feature of the reference image associated with the target image, respectively, comprises:<claim-text>extracting, based on a deep residual network, the target basis feature of the target image and the reference basis feature of the reference image associated with the target image, respectively.</claim-text></claim-text></claim><claim id="CLM-00026" num="00026"><claim-text><b>26</b>. The electronic device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein before extracting the reference building feature of the reference image, the instructions are executed by the at least one processor to cause the at least one processor to further perform:<claim-text>performing coordinate transform on the reference image according to the target image;</claim-text><claim-text>wherein the coordinate transform comprises at least one of shrinking transform, stretching transform, rotation transform or translation transform.</claim-text></claim-text></claim><claim id="CLM-00027" num="00027"><claim-text><b>27</b>. The electronic device according to <claim-ref idref="CLM-00026">claim 26</claim-ref>, wherein performing the coordinate transform on the reference image according to the target image comprises:<claim-text>extracting target key points and target descriptors of the target image, and reference key points and reference descriptors of the reference image, respectively;</claim-text><claim-text>performing a matching operation on the target key points and the reference key points according to the target descriptors and the reference descriptors; and</claim-text><claim-text>determining, according to a matching result, a transform matrix, and performing the coordinate transform on the reference image according to the transform matrix.</claim-text></claim-text></claim><claim id="CLM-00028" num="00028"><claim-text><b>28</b>. The electronic device according to <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein determining the illegal building identification result of the target image comprises:<claim-text>determining whether the target image comprises an illegal building area; and</claim-text><claim-text>in response to determining that the target image comprises the illegal building area, determining position coordinates of the illegal building area.</claim-text></claim-text></claim></claims></us-patent-application>