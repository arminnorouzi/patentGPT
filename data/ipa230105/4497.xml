<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004498A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004498</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17940070</doc-number><date>20220908</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0862</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0875</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>38</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0811</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0815</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0862</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0875</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30047</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3844</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0811</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0815</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>6028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>681</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>1016</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2212</main-group><subgroup>452</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>12</main-group><subgroup>0888</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">ZERO LATENCY PREFETCHING IN CACHES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17151857</doc-number><date>20210119</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11474944</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17940070</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15730874</doc-number><date>20171012</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10929296</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17151857</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>TEXAS INSTRUMENTS INCORPORATED</orgname><address><city>Dallas</city><state>TX</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Olorode</last-name><first-name>Oluleye</first-name><address><city>Garland</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Venkatasubramanian</last-name><first-name>Ramakrishnan</first-name><address><city>Plano</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Ong</last-name><first-name>Hung</first-name><address><city>Plano</city><state>TX</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">This invention involves a cache system in a digital data processing apparatus including: a central processing unit core; a level one instruction cache; and a level two cache. The cache lines in the second level cache are twice the size of the cache lines in the first level instruction cache. The central processing unit core requests additional program instructions when needed via a request address. Upon a miss in the level one instruction cache that causes a hit in the upper half of a level two cache line, the level two cache supplies the upper half level cache line to the level one instruction cache. On a following level two cache memory cycle, the level two cache supplies the lower half of the cache line to the level one instruction cache. This cache technique thus prefetches the lower half level two cache line employing fewer resources than an ordinary prefetch.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="201.00mm" wi="157.99mm" file="US20230004498A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="212.85mm" wi="160.02mm" file="US20230004498A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="253.83mm" wi="167.47mm" orientation="landscape" file="US20230004498A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="247.57mm" wi="167.47mm" file="US20230004498A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="258.40mm" wi="170.10mm" orientation="landscape" file="US20230004498A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="227.75mm" wi="131.74mm" file="US20230004498A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="241.05mm" wi="165.69mm" orientation="landscape" file="US20230004498A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="199.73mm" wi="115.23mm" file="US20230004498A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="246.13mm" wi="167.47mm" orientation="landscape" file="US20230004498A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="251.88mm" wi="169.67mm" file="US20230004498A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="250.87mm" wi="164.68mm" file="US20230004498A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="189.65mm" wi="116.25mm" file="US20230004498A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/151,857, filed on Jan. 19, 2021, which is a continuation of U.S. patent application Ser. No. 15/730,874 filed on Oct. 12, 2017 (now U.S. Pat. No. 10,929,296), all of which are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD OF THE INVENTION</heading><p id="p-0003" num="0002">The technical field of this invention is digital data processing and more specifically improvements in prefetching to a cache.</p><heading id="h-0003" level="1">BACKGROUND OF THE INVENTION</heading><p id="p-0004" num="0003">Currently digital data processors operate on very fast clocks and typically execute instructions faster than they can be recalled from generic memory. A known solution to the problem of providing instructions to the digital data processors is known as instruction cache. The digital data processing system provides a small, fast memory in physical and computational proximity to the data elements that require instruction control. This small, fast memory stores a subset of the instructions required. Digital data processors often work on loops. If all or most of an instruction loop is stored in the cache, the digital data processor can be kept fed with instructions at a rate faster than recall from generic memory.</p><p id="p-0005" num="0004">As a result of these cache schemes it has become helpful to determine what instructions will be employed ahead of the actual need. Such a prefetch enables cache to store instructions for recall when needed. There are some problems with many prefetch techniques. A prefetch request consumes resources to determine if the cache already stores the data to be prefetched. Thus prefetch requests are often given a lower priority than demand requests.</p><heading id="h-0004" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0006" num="0005">This invention involves a cache system in a digital data processing apparatus. The digital data processing apparatus includes: a central processing unit core; a level one instruction cache; and a level two cache. The central processing unit core performs data processing operations in response to program instructions. The central processing unit core issues instruction requests for additional program instructions when needed via a request address. The level one instruction cache temporarily stores a subset of program instructions in level one cache lines. When the central processing unit requests an instruction at a request address, the level one instruction cache determines whether it stores the instruction at the request address. If so, the level one instruction cache supplies the requested program instructions. If not, the level one instruction cache supplies the request address to the level two cache for similar handling.</p><p id="p-0007" num="0006">The cache lines in the second level cache are twice the size of the cache lines in the first level instruction cache. Upon a miss in the level one instruction cache that causes a hit in the upper half of a level two cache line, the level two cache supplies the upper half level cache line to the level one instruction cache. On a following level two cache memory cycle, the level two cache supplies the lower half of the cache line to the level one instruction cache. The level two cache includes a register for storing this data under these circumstances.</p><p id="p-0008" num="0007">This cache technique serves as a prefetch of the lower half level two cache line under these circumstances. This prefetch employs fewer resources than an ordinary prefetch.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0009" num="0008">These and other aspects of this invention are illustrated in the drawings, in which:</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a dual scalar/vector datapath processor according to one embodiment of this invention;</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the registers and functional units in the dual scalar/vector datapath processor illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates the global scalar register file;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the local scalar register file shared by arithmetic functional units;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates the local scalar register file shared by the multiply functional units;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates local scalar register file of shared by the load/store units;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates global vector register file;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates the predicate register file;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates the local vector register file shared by arithmetic functional units;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates the local vector register file shared by the multiply and correlation functional units;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates the pipeline phases of the central processing unit according to a preferred embodiment of this invention;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates sixteen instructions of a single fetch packet;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example of the instruction coding of instructions used by this invention;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates the bit coding of a condition code extension slot <b>0</b>;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates the bit coding of a condition code extension slot <b>1</b>;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates the bit coding of a constant extension slot <b>0</b>;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a partial block diagram illustrating constant extension;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the carry control for SIMD operations according to this invention;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates one view of the combination of central processing unit core and the instruction cache and auxiliary support structures;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates a second view of the combination of central processing unit core and the instruction cache and auxiliary support structures;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates how the bits of a fetch address are parsed for handling;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>22</b></figref> illustrates the relevant details of the controller for the level two combined instructions/data cache;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flow chart illustrating operations in accordance with one aspect of this invention; and</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flow chart illustrating operations in accordance with another aspect of this invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION OF PREFERRED EMBODIMENTS</heading><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a dual scalar/vector datapath processor according to a preferred embodiment of this invention. Processor <b>100</b> includes separate level one instruction cache (L1I) <b>121</b> and level one data cache (L1D) <b>123</b>. Processor <b>100</b> includes a level two combined instruction/data cache (L2) <b>130</b> that holds both instructions and data. <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates connection between level one instruction cache <b>121</b> and level two combined instruction/data cache <b>130</b> (bus <b>142</b>). <figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates connection between level one data cache <b>123</b> and level two combined instruction/data cache <b>130</b> (bus <b>145</b>). In the preferred embodiment of processor <b>100</b> level two combined instruction/data cache <b>130</b> stores both instructions to back up level one instruction cache <b>121</b> and data to back up level one data cache <b>123</b>. In the preferred embodiment level two combined instruction/data cache <b>130</b> is further connected to higher level cache and/or main memory in a manner known in the art and not illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In the preferred embodiment central processing unit core <b>110</b>, level one instruction cache <b>121</b>, level one data cache <b>123</b> and level two combined instruction/data cache <b>130</b> are formed on a single integrated circuit. This signal integrated circuit optionally includes other circuits.</p><p id="p-0035" num="0034">Central processing unit core <b>110</b> fetches instructions from level one instruction cache <b>121</b> as controlled by instruction fetch unit <b>111</b>. Instruction fetch unit <b>111</b> determines the next instructions to be executed and recalls a fetch packet sized set of such instructions. The nature and size of fetch packets are further detailed below. As known in the art, instructions are directly fetched from level one instruction cache <b>121</b> upon a cache hit (if these instructions are stored in level one instruction cache <b>121</b>). Upon a cache miss (the specified instruction fetch packet is not stored in level one instruction cache <b>121</b>), these instructions are sought in level two combined instruction/data cache <b>130</b>. In the preferred embodiment the size of a cache line in level one instruction cache <b>121</b> equals the size of a fetch packet. The memory locations of these instructions are either a hit in level two combined instruction/data cache <b>130</b> or a miss. A hit is serviced from level two combined instruction/data cache <b>130</b>. A miss is serviced from a higher level of cache (not illustrated) or from main memory (not illustrated). As is known in the art, the requested instruction may be simultaneously supplied to both level one instruction cache <b>121</b> and central processing unit core <b>110</b> to speed use.</p><p id="p-0036" num="0035">In the preferred embodiment of this invention, central processing unit core <b>110</b> includes plural functional units to perform instruction specified data processing tasks. Instruction dispatch unit <b>112</b> determines the target functional unit of each fetched instruction. In the preferred embodiment central processing unit <b>110</b> operates as a very long instruction word (VLIW) processor capable of operating on plural instructions in corresponding functional units simultaneously. Preferably a complier organizes instructions in execute packets that are executed together. Instruction dispatch unit <b>112</b> directs each instruction to its target functional unit. The functional unit assigned to an instruction is completely specified by the instruction produced by a compiler. The hardware of central processing unit core <b>110</b> has no part in this functional unit assignment. In the preferred embodiment instruction dispatch unit <b>112</b> may operate on plural instructions in parallel. The number of such parallel instructions is set by the size of the execute packet. This will be further detailed below.</p><p id="p-0037" num="0036">One part of the dispatch task of instruction dispatch unit <b>112</b> is determining whether the instruction is to execute on a functional unit in scalar datapath side A <b>115</b> or vector datapath side B <b>116</b>. An instruction bit within each instruction called the s bit determines which datapath the instruction controls. This will be further detailed below.</p><p id="p-0038" num="0037">Instruction decode unit <b>113</b> decodes each instruction in a current execute packet. Decoding includes identification of the functional unit performing the instruction, identification of registers used to supply data for the corresponding data processing operation from among possible register files and identification of the register destination of the results of the corresponding data processing operation. As further explained below, instructions may include a constant field in place of one register number operand field. The result of this decoding is signals for control of the target functional unit to perform the data processing operation specified by the corresponding instruction on the specified data.</p><p id="p-0039" num="0038">Central processing unit core <b>110</b> includes control registers <b>114</b>. Control registers <b>114</b> store information for control of the functional units in scalar datapath side A <b>115</b> and vector datapath side B <b>116</b> in a manner not relevant to this invention. This information could be mode information or the like.</p><p id="p-0040" num="0039">The decoded instructions from instruction decode <b>113</b> and information stored in control registers <b>114</b> are supplied to scalar datapath side A <b>115</b> and vector datapath side B <b>116</b>. As a result functional units within scalar datapath side A <b>115</b> and vector datapath side B <b>116</b> perform instruction specified data processing operations upon instruction specified data and store the results in an instruction specified data register or registers. Each of scalar datapath side A <b>115</b> and vector datapath side B <b>116</b> include plural functional units that preferably operate in parallel. These will be further detailed below in conjunction with <figref idref="DRAWINGS">FIG. <b>2</b></figref>. There is a datapath <b>117</b> between scalar datapath side A <b>115</b> and vector datapath side B <b>116</b> permitting data exchange.</p><p id="p-0041" num="0040">Central processing unit core <b>110</b> includes further non-instruction based modules. Emulation unit <b>118</b> permits determination of the machine state of central processing unit core <b>110</b> in response to instructions. This capability will typically be employed for algorithmic development. Interrupts/exceptions unit <b>119</b> enable central processing unit core <b>110</b> to be responsive to external, asynchronous events (interrupts) and to respond to attempts to perform improper operations (exceptions).</p><p id="p-0042" num="0041">Central processing unit core <b>110</b> includes streaming engine <b>125</b>. Streaming engine <b>125</b> supplies two data streams from predetermined addresses typically cached in level two combined instruction/data cache <b>130</b> to register files of vector datapath side B. This provides controlled data movement from memory (as cached in level two combined instruction/data cache <b>130</b>) directly to functional unit operand inputs. This is further detailed below.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates exemplary data widths of busses between various parts. Level one instruction cache <b>121</b> supplies instructions to instruction fetch unit <b>111</b> via bus <b>141</b>. Bus <b>141</b> is preferably a 512-bit bus. Bus <b>141</b> is unidirectional from level one instruction cache <b>121</b> to central processing unit core <b>110</b>. Level two combined instruction/data cache <b>130</b> supplies instructions to level one instruction cache <b>121</b> via bus <b>142</b>. Bus <b>142</b> is preferably a 512-bit bus. Bus <b>142</b> is unidirectional from level two combined instruction/data cache <b>130</b> to level one instruction cache <b>121</b>.</p><p id="p-0044" num="0043">Level one data cache <b>123</b> exchanges data with register files in scalar datapath side A <b>115</b> via bus <b>143</b>. Bus <b>143</b> is preferably a 64-bit bus. Level one data cache <b>123</b> exchanges data with register files in vector datapath side B <b>116</b> via bus <b>144</b>. Bus <b>144</b> is preferably a 512-bit bus. Busses <b>143</b> and <b>144</b> are illustrated as bidirectional supporting both central processing unit core <b>110</b> data reads and data writes. Level one data cache <b>123</b> exchanges data with level two combined instruction/data cache <b>130</b> via bus <b>145</b>. Bus <b>145</b> is preferably a 512-bit bus. Bus <b>145</b> is illustrated as bidirectional supporting cache service for both central processing unit core <b>110</b> data reads and data writes.</p><p id="p-0045" num="0044">As known in the art, CPU data requests are directly fetched from level one data cache <b>123</b> upon a cache hit (if the requested data is stored in level one data cache <b>123</b>). Upon a cache miss (the specified data is not stored in level one data cache <b>123</b>), this data is sought in level two combined instruction/data cache <b>130</b>. The memory locations of this requested data is either a hit in level two combined instruction/data cache <b>130</b> or a miss. A hit is serviced from level two combined instruction/data cache <b>130</b>. A miss is serviced from another level of cache (not illustrated) or from main memory (not illustrated). As is known in the art, the requested instruction may be simultaneously supplied to both level one data cache <b>123</b> and central processing unit core <b>110</b> to speed use.</p><p id="p-0046" num="0045">Level two combined instruction/data cache <b>130</b> supplies data of a first data stream to streaming engine <b>125</b> via bus <b>146</b>. Bus <b>146</b> is preferably a 512-bit bus. Streaming engine <b>125</b> supplies data of this first data stream to functional units of vector datapath side B <b>116</b> via bus <b>147</b>. Bus <b>147</b> is preferably a 512-bit bus. Level two combined instruction/data cache <b>130</b> supplies data of a second data stream to streaming engine <b>125</b> via bus <b>148</b>. Bus <b>148</b> is preferably a 512-bit bus. Streaming engine <b>125</b> supplies data of this second data stream to functional units of vector datapath side B <b>116</b> via bus <b>149</b>. Bus <b>149</b> is preferably a 512-bit bus. Busses <b>146</b>, <b>147</b>, <b>148</b> and <b>149</b> are illustrated as unidirectional from level two combined instruction/data cache <b>130</b> to streaming engine <b>125</b> and to vector datapath side B <b>116</b> in accordance with the preferred embodiment of this invention.</p><p id="p-0047" num="0046">Steaming engine data requests are directly fetched from level two combined instruction/data cache <b>130</b> upon a cache hit (if the requested data is stored in level two combined instruction/data cache <b>130</b>). Upon a cache miss (the specified data is not stored in level two combined instruction/data cache <b>130</b>), this data is sought from another level of cache (not illustrated) or from main memory (not illustrated). It is technically feasible in some embodiments for level one data cache <b>123</b> to cache data not stored in level two combined instruction/data cache <b>130</b>. If such operation is supported, then upon a streaming engine data request that is a miss in level two combined instruction/data cache <b>130</b>, level two combined instruction/data cache <b>130</b> should snoop level one data cache <b>123</b> for the stream engine requested data. If level one data cache <b>123</b> stores this data its snoop response would include the data, which is then supplied to service the streaming engine request. If level one data cache <b>123</b> does not store this data its snoop response would indicate this and level two combined instruction/data cache <b>130</b> must service this streaming engine request from another level of cache (not illustrated) or from main memory (not illustrated).</p><p id="p-0048" num="0047">In the preferred embodiment of this invention, both level one data cache <b>123</b> and level two combined instruction/data cache <b>130</b> may be configured as selected amounts of cache or directly addressable memory in accordance with U.S. Pat. No. 6,606,686 entitled UNIFIED MEMORY SYSTEM ARCHITECTURE INCLUDING CACHE AND DIRECTLY ADDRESSABLE STATIC RANDOM ACCESS MEMORY.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates further details of functional units and register files within scalar datapath side A <b>115</b> and vector datapath side B <b>116</b>. Scalar datapath side A <b>115</b> includes global scalar register file <b>211</b>, L1/S1 local register file <b>212</b>, M1/N1 local register file <b>213</b> and D1/D2 local register file <b>214</b>. Scalar datapath side A <b>115</b> includes L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>. Vector datapath side B <b>116</b> includes global scalar register file <b>231</b>, L2/S2 local register file <b>232</b>, M2/N2/C local register file <b>233</b> and predicate register file <b>234</b>. Vector datapath side B <b>116</b> includes L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>. There are limitations upon which functional units may read from or write to which register files. These will be detailed below.</p><p id="p-0050" num="0049">Scalar datapath side A <b>115</b> includes L1 unit <b>221</b>. L1 unit <b>221</b> generally accepts two 64-bit operands and produces one 64-bit result. The two operands are each recalled from an instruction specified register in either global scalar register file <b>211</b> or L1/S1 local register file <b>212</b>. L1 unit <b>221</b> preferably performs the following instruction selected operations: 64-bit add/subtract operations; 32-bit min/max operations; 8-bit Single Instruction Multiple Data (SIMD) instructions such as sum of absolute value, minimum and maximum determinations; circular min/max operations; and various move operations between register files. The result may be written into an instruction specified register of global scalar register file <b>211</b>, L1/S1 local register file <b>212</b>, M1/N1 local register file <b>213</b> or D1/D2 local register file <b>214</b>.</p><p id="p-0051" num="0050">Scalar datapath side A <b>115</b> includes S1 unit <b>222</b>. S1 unit <b>222</b> generally accepts two 64-bit operands and produces one 64-bit result. The two operands are each recalled from an instruction specified register in either global scalar register file <b>211</b> or L1/S1 local register file <b>212</b>. S1 unit <b>222</b> preferably performs the same type operations as L1 unit <b>221</b>. There optionally may be slight variations between the data processing operations supported by L1 unit <b>221</b> and S1 unit <b>222</b>. The result may be written into an instruction specified register of global scalar register file <b>211</b>, L1/S1 local register file <b>212</b>, M1/N1 local register file <b>213</b> or D1/D2 local register file <b>214</b>.</p><p id="p-0052" num="0051">Scalar datapath side A <b>115</b> includes M1 unit <b>223</b>. M1 unit <b>223</b> generally accepts two 64-bit operands and produces one 64-bit result. The two operands are each recalled from an instruction specified register in either global scalar register file <b>211</b> or M1/N1 local register file <b>213</b>. M1 unit <b>223</b> preferably performs the following instruction selected operations: 8-bit multiply operations; complex dot product operations; 32-bit bit count operations; complex conjugate multiply operations; and bit-wise Logical Operations, moves, adds and subtracts. The result may be written into an instruction specified register of global scalar register file <b>211</b>, L1/S1 local register file <b>212</b>, M1/N1 local register file <b>213</b> or D1/D2 local register file <b>214</b>.</p><p id="p-0053" num="0052">Scalar datapath side A <b>115</b> includes N1 unit <b>224</b>. N1 unit <b>224</b> generally accepts two 64-bit operands and produces one 64-bit result. The two operands are each recalled from an instruction specified register in either global scalar register file <b>211</b> or M1/N1 local register file <b>213</b>. N1 unit <b>224</b> preferably performs the same type operations as M1 unit <b>223</b>. There may be certain double operations (called dual issued instructions) that employ both the M1 unit <b>223</b> and the N1 unit <b>224</b> together. The result may be written into an instruction specified register of global scalar register file <b>211</b>, L1/S1 local register file <b>212</b>, M1/N1 local register file <b>213</b> or D1/D2 local register file <b>214</b>.</p><p id="p-0054" num="0053">Scalar datapath side A <b>115</b> includes D1 unit <b>225</b> and D2 unit <b>226</b>. D1 unit <b>225</b> and D2 unit <b>226</b> generally each accept two 64-bit operands and each produce one 64-bit result. D1 unit <b>225</b> and D2 unit <b>226</b> generally perform address calculations and corresponding load and store operations. D1 unit <b>225</b> is used for scalar loads and stores of 64 bits. D2 unit <b>226</b> is used for vector loads and stores of 512 bits. D1 unit <b>225</b> and D2 unit <b>226</b> preferably also perform: swapping, pack and unpack on the load and store data; 64-bit SIMD arithmetic operations; and 64-bit bit-wise logical operations. D1/D2 local register file <b>214</b> will generally store base and offset addresses used in address calculations for the corresponding loads and stores. The two operands are each recalled from an instruction specified register in either global scalar register file <b>211</b> or D1/D2 local register file <b>214</b>. The calculated result may be written into an instruction specified register of global scalar register file <b>211</b>, L1/S1 local register file <b>212</b>, M1/N1 local register file <b>213</b> or D1/D2 local register file <b>214</b>.</p><p id="p-0055" num="0054">Vector datapath side B <b>116</b> includes L2 unit <b>241</b>. L2 unit <b>221</b> generally accepts two 512-bit operands and produces one 512-bit result. The two operands are each recalled from an instruction specified register in either global vector register file <b>231</b>, L2/S2 local register file <b>232</b> or predicate register file <b>234</b>. L2 unit <b>241</b> preferably performs instruction similar to L1 unit <b>221</b> except on wider 512-bit data. The result may be written into an instruction specified register of global vector register file <b>231</b>, L2/S2 local register file <b>222</b>, M2/N2/C local register file <b>233</b> or predicate register file <b>234</b>.</p><p id="p-0056" num="0055">Vector datapath side B <b>116</b> includes S2 unit <b>242</b>. S2 unit <b>242</b> generally accepts two 512-bit operands and produces one 512-bit result. The two operands are each recalled from an instruction specified register in either global vector register file <b>231</b>, L2/S2 local register file <b>232</b> or predicate register file <b>234</b>. S2 unit <b>242</b> preferably performs instructions similar to S1 unit <b>222</b> except on wider 512-bit data. The result may be written into an instruction specified register of global vector register file <b>231</b>, L2/S2 local register file <b>222</b>, M2/N2/C local register file <b>233</b> or predicate register file <b>234</b>.</p><p id="p-0057" num="0056">Vector datapath side B <b>116</b> includes M2 unit <b>243</b>. M2 unit <b>243</b> generally accepts two 512-bit operands and produces one 512-bit result. The two operands are each recalled from an instruction specified register in either global vector register file <b>231</b> or M2/N2/C local register file <b>233</b>. M2 unit <b>243</b> preferably performs instructions similar to M1 unit <b>222</b> except on wider 512-bit data. The result may be written into an instruction specified register of global vector register file <b>231</b>, L2/S2 local register file <b>232</b> or M2/N2/C local register file <b>233</b>.</p><p id="p-0058" num="0057">Vector datapath side B <b>116</b> includes N2 unit <b>244</b>. N2 unit <b>244</b> generally accepts two 512-bit operands and produces one 512-bit result. The two operands are each recalled from an instruction specified register in either global vector register file <b>231</b> or M2/N2/C local register file <b>233</b>. N2 unit <b>244</b> preferably performs the same type operations as M2 unit <b>243</b>. There may be certain double operations (called dual issued instructions) that employ both M2 unit <b>243</b> and the N2 unit <b>244</b> together. The result may be written into an instruction specified register of global vector register file <b>231</b>, L2/S2 local register file <b>232</b> or M2/N2/C local register file <b>233</b>.</p><p id="p-0059" num="0058">Vector datapath side B <b>116</b> includes C unit <b>245</b>. C unit <b>245</b> generally accepts two 512-bit operands and produces one 512-bit result. The two operands are each recalled from an instruction specified register in either global vector register file <b>231</b> or M2/N2/C local register file <b>233</b>. C unit <b>245</b> preferably performs: &#x201c;Rake&#x201d; and &#x201c;Search&#x201d; instructions; up to 512 2-bit PN*8-bit multiplies I/Q complex multiplies per clock cycle; 8-bit and 16-bit Sum-of-Absolute-Difference (SAD) calculations, up to 512 SADs per clock cycle; horizontal add and horizontal min/max instructions; and vector permutes instructions. C unit <b>245</b> includes also contains 4 vector control registers (CUCR<b>0</b> to CUCR<b>3</b>) used to control certain operations of C unit <b>245</b> instructions. Control registers CUCR<b>0</b> to CUCR<b>3</b> are used as operands in certain C unit <b>245</b> operations. Control registers CUCR<b>0</b> to CUCR<b>3</b> are preferably used: in control of a general permutation instruction (VPERM); and as masks for SIMD multiple DOT product operations (DOTPM) and SIMD multiple Sum-of-Absolute-Difference (SAD) operations. Control register CUCR<b>0</b> is preferably used to store the polynomials for Galios Field Multiply operations (GFMPY). Control register CUCR<b>1</b> is preferably used to store the Galois field polynomial generator function.</p><p id="p-0060" num="0059">Vector datapath side B <b>116</b> includes P unit <b>246</b>. P unit <b>246</b> performs basic logic operations on registers of local predicate register file <b>234</b>. P unit <b>246</b> has direct access to read from and write to predication register file <b>234</b>. These operations include AND, ANDN, OR, XOR, NOR, BITR, NEG, SET, BITCNT, RMBD, BIT Decimate and Expand. A commonly expected use of P unit <b>246</b> includes manipulation of the SIMD vector comparison results for use in control of a further SIMD vector operation.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates global scalar register file <b>211</b>. There are 16 independent 64-bit wide scalar registers designated A<b>0</b> to A<b>15</b>. Each register of global scalar register file <b>211</b> can be read from or written to as 64-bits of scalar data. All scalar datapath side A <b>115</b> functional units (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) can read or write to global scalar register file <b>211</b>. Global scalar register file <b>211</b> may be read as 32-bits or as 64-bits and may only be written to as 64-bits. The instruction executing determines the read data size. Vector datapath side B <b>116</b> functional units (L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>) can read from global scalar register file <b>211</b> via crosspath <b>117</b> under restrictions that will be detailed below.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates D1/D2 local register file <b>214</b>. There are 16 independent 64-bit wide scalar registers designated D<b>0</b> to D<b>16</b>. Each register of D1/D2 local register file <b>214</b> can be read from or written to as 64-bits of scalar data. All scalar datapath side A <b>115</b> functional units (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) can write to global scalar register file <b>211</b>. Only D1 unit <b>225</b> and D2 unit <b>226</b> can read from D1/D1 local scalar register file <b>214</b>. It is expected that data stored in D1/D2 local scalar register file <b>214</b> will include base addresses and offset addresses used in address calculation.</p><p id="p-0063" num="0062"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates L1/S1 local register file <b>212</b>. The embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref> has 8 independent 64-bit wide scalar registers designated AL<b>0</b> to AL<b>7</b>. The preferred instruction coding (see <figref idref="DRAWINGS">FIG. <b>13</b></figref>) permits L1/S1 local register file <b>212</b> to include up to 16 registers. The embodiment of <figref idref="DRAWINGS">FIG. <b>5</b></figref> implements only 8 registers to reduce circuit size and complexity. Each register of L1/S1 local register file <b>212</b> can be read from or written to as 64-bits of scalar data. All scalar datapath side A <b>115</b> functional units (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) can write to L1/S1 local scalar register file <b>212</b>. Only L1 unit <b>221</b> and S1 unit <b>222</b> can read from L1/S1 local scalar register file <b>212</b>.</p><p id="p-0064" num="0063"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates M1/N1 local register file <b>213</b>. The embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref> has 8 independent 64-bit wide scalar registers designated AM<b>0</b> to AM<b>7</b>. The preferred instruction coding (see <figref idref="DRAWINGS">FIG. <b>13</b></figref>) permits M1/N1 local register file <b>213</b> to include up to 16 registers. The embodiment of <figref idref="DRAWINGS">FIG. <b>6</b></figref> implements only 8 registers to reduce circuit size and complexity. Each register of M1/N1 local register file <b>213</b> can be read from or written to as 64-bits of scalar data. All scalar datapath side A <b>115</b> functional units (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) can write to M1/N1 local scalar register file <b>213</b>. Only M1 unit <b>223</b> and N1 unit <b>224</b> can read from M1/N1 local scalar register file <b>213</b>.</p><p id="p-0065" num="0064"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates global vector register file <b>231</b>. There are 16 independent 512-bit wide vector registers. Each register of global vector register file <b>231</b> can be read from or written to as 64-bits of scalar data designated B<b>0</b> to B<b>15</b>. Each register of global vector register file <b>231</b> can be read from or written to as 512-bits of vector data designated VB<b>0</b> to VB<b>15</b>. The instruction type determines the data size. All vector datapath side B <b>116</b> functional units (L2 unit <b>241</b>, S2 unit <b>242</b>, M3 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>) can read or write to global scalar register file <b>231</b>. Scalar datapath side A <b>115</b> functional units (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) can read from global vector register file <b>231</b> via crosspath <b>117</b> under restrictions that will be detailed below.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates P local register file <b>234</b>. There are 8 independent 64-bit wide registers designated P<b>0</b> to P<b>15</b>. Each register of P local register file <b>234</b> can be read from or written to as 64-bits of scalar data. Vector datapath side B <b>116</b> functional units L2 unit <b>241</b>, S2 unit <b>242</b>, C unit <b>244</b> and P unit <b>246</b> can write to P local register file <b>234</b>. Only L2 unit <b>241</b>, S2 unit <b>242</b> and P unit <b>246</b> can read from P local scalar register file <b>234</b>. A commonly expected use of P local register file <b>234</b> includes: writing one bit SIMD vector comparison results from L2 unit <b>241</b>, S2 unit <b>242</b> or C unit <b>244</b>; manipulation of the SIMD vector comparison results by P unit <b>246</b>; and use of the manipulated results in control of a further SIMD vector operation.</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates L2/S2 local register file <b>232</b>. The embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref> has 8 independent 512-bit wide vector registers. The preferred instruction coding (see <figref idref="DRAWINGS">FIG. <b>13</b></figref>) permits L2/S2 local register file <b>232</b> to include up to 16 registers. The embodiment of <figref idref="DRAWINGS">FIG. <b>9</b></figref> implements only 8 registers to reduce circuit size and complexity. Each register of L2/S2 local vector register file <b>232</b> can be read from or written to as 64-bits of scalar data designated BL<b>0</b> to BL<b>7</b>. Each register of L2/S2 local vector register file <b>232</b> can be read from or written to as 512-bits of vector data designated VBL<b>0</b> to VBL<b>7</b>. The instruction type determines the data size. All vector datapath side B <b>116</b> functional units (L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>233</b>, N2 unit <b>24</b>, C unit <b>245</b> and P unit <b>246</b>) can write to L2/S2 local vector register file <b>232</b>. Only L2 unit <b>241</b> and S2 unit <b>242</b> can read from L2/S2 local vector register file <b>232</b>.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates M2/N2/C local register file <b>233</b>. The embodiment illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> has 8 independent 512-bit wide vector registers. The preferred instruction coding (see <figref idref="DRAWINGS">FIG. <b>13</b></figref>) permits L1/S1 local register file <b>212</b> to include up to 16 registers. The embodiment of <figref idref="DRAWINGS">FIG. <b>10</b></figref> implements only 8 registers to reduce circuit size and complexity. Each register of M2/N2/C local vector register file <b>233</b> can be read from or written to as 64-bits of scalar data designated BM<b>0</b> to BM<b>7</b>. Each register of M2/N2/C local vector register file <b>233</b> can be read from or written to as 512-bits of vector data designated VBM<b>0</b> to VBM<b>7</b>. All vector datapath side B <b>116</b> functional units (L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>) can write to M2/N2/C local vector register file <b>233</b>. Only M2 unit <b>233</b>, N2 unit <b>244</b> and C unit <b>245</b> can read from M2/N2/C local vector register file <b>233</b>.</p><p id="p-0069" num="0068">The provision of global register files accessible by all functional units of a side and local register files accessible by only some of the functional units of a side is a design choice. This invention could be practiced employing only one type of register file corresponding to the disclosed global register files.</p><p id="p-0070" num="0069">Crosspath <b>117</b> permits limited exchange of data between scalar datapath side A <b>115</b> and vector datapath side B <b>116</b>. During each operational cycle one 64-bit data word can be recalled from global scalar register file A <b>211</b> for use as an operand by one or more functional units of vector datapath side B <b>116</b> and one 64-bit data word can be recalled from global vector register file <b>231</b> for use as an operand by one or more functional units of scalar datapath side A <b>115</b>. Any scalar datapath side A <b>115</b> functional unit (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) may read a 64-bit operand from global vector register file <b>231</b>. This 64-bit operand is the least significant bits of the 512-bit data in the accessed register of global vector register file <b>232</b>. Plural scalar datapath side A <b>115</b> functional units may employ the same 64-bit crosspath data as an operand during the same operational cycle. However, only one 64-bit operand is transferred from vector datapath side B <b>116</b> to scalar datapath side A <b>115</b> in any single operational cycle. Any vector datapath side B <b>116</b> functional unit (L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>) may read a 64-bit operand from global scalar register file <b>211</b>. If the corresponding instruction is a scalar instruction, the crosspath operand data is treated as any other 64-bit operand. If the corresponding instruction is a vector instruction, the upper 448 bits of the operand are zero filled. Plural vector datapath side B <b>116</b> functional units may employ the same 64-bit crosspath data as an operand during the same operational cycle. Only one 64-bit operand is transferred from scalar datapath side A <b>115</b> to vector datapath side B <b>116</b> in any single operational cycle.</p><p id="p-0071" num="0070">Streaming engine <b>125</b> transfers data in certain restricted circumstances. Streaming engine <b>125</b> controls two data streams. A stream consists of a sequence of elements of a particular type. Programs that operate on streams read the data sequentially, operating on each element in turn. Every stream has the following basic properties. The stream data have a well-defined beginning and ending in time. The stream data have fixed element size and type throughout the stream. The stream data have fixed sequence of elements. Thus programs cannot seek randomly within the stream. The stream data is read-only while active. Programs cannot write to a stream while simultaneously reading from it. Once a stream is opened streaming engine <b>125</b>: calculates the address; fetches the defined data type from level two combined instruction/data cache <b>130</b> (which may require cache service from a higher level memory); performs data type manipulation such as zero extension, sign extension, data element sorting/swapping such as matrix transposition; and delivers the data directly to the programmed data register file within central processing unit core <b>110</b>. Streaming engine <b>125</b> is thus useful for real-time digital filtering operations on well-behaved data. Streaming engine <b>125</b> frees these memory fetch tasks from the corresponding CPU enabling other processing functions.</p><p id="p-0072" num="0071">Streaming engine <b>125</b> provides the following benefits. Streaming engine <b>125</b> permits multi-dimensional memory accesses. Streaming engine <b>125</b> increases the available bandwidth to the functional units. Streaming engine <b>125</b> minimizes the number of cache miss stalls since the stream buffer bypasses level one data cache <b>123</b>. Streaming engine <b>125</b> reduces the number of scalar operations required to maintain a loop. Streaming engine <b>125</b> manages address pointers. Streaming engine <b>125</b> handles address generation automatically freeing up the address generation instruction slots and D1 unit <b>224</b> and D2 unit <b>226</b> for other computations.</p><p id="p-0073" num="0072">Central processing unit core <b>110</b> operates on an instruction pipeline. Instructions are fetched in instruction packets of fixed length further described below. All instructions require the same number of pipeline phases for fetch and decode, but require a varying number of execute phases.</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates the following pipeline phases: program fetch phase <b>1110</b>, dispatch and decode phases <b>1110</b> and execution phases <b>1130</b>. Program fetch phase <b>1110</b> includes three stages for all instructions. Dispatch and decode phases include three stages for all instructions. Execution phase <b>1130</b> includes one to four stages dependent on the instruction.</p><p id="p-0075" num="0074">Fetch phase <b>1110</b> includes program address generation stage <b>1111</b> (PG), program access stage <b>1112</b> (PA) and program receive stage <b>1113</b> (PR). During program address generation stage <b>1111</b> (PG), the program address is generated in the CPU and the read request is sent to the memory controller for the level one instruction cache <b>121</b>. During the program access stage <b>1112</b> (PA) the level one instruction cache <b>121</b> processes the request, accesses the data in its memory and sends a fetch packet to the CPU boundary. During the program receive stage <b>1113</b> (PR) the CPU registers the fetch packet.</p><p id="p-0076" num="0075">Instructions are always fetched sixteen 32-bit wide slots, constituting a fetch packet, at a time. <figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates 16 instructions <b>1201</b> to <b>1216</b> of a single fetch packet. Fetch packets are aligned on 512-bit (16-word) boundaries. The preferred embodiment employs a fixed 32-bit instruction length. Fixed length instructions are advantageous for several reasons. Fixed length instructions enable easy decoder alignment. A properly aligned instruction fetch can load plural instructions into parallel instruction decoders. Such a properly aligned instruction fetch can be achieved by predetermined instruction alignment when stored in memory (fetch packets aligned on 512-bit boundaries) coupled with a fixed instruction packet fetch. An aligned instruction fetch permits operation of parallel decoders on instruction-sized fetched bits. Variable length instructions require an initial step of locating each instruction boundary before they can be decoded. A fixed length instruction set generally permits more regular layout of instruction fields. This simplifies the construction of each decoder which is an advantage for a wide issue VLIW central processor.</p><p id="p-0077" num="0076">The execution of the individual instructions is partially controlled by a p bit in each instruction. This p bit is preferably bit <b>0</b> of the 32-bit wide slot. The p bit determines whether an instruction executes in parallel with a next instruction. Instructions are scanned from lower to higher address. If the p bit of an instruction is 1, then the next following instruction (higher memory address) is executed in parallel with (in the same cycle as) that instruction. If the p bit of an instruction is 0, then the next following instruction is executed in the cycle after the instruction.</p><p id="p-0078" num="0077">Central processing unit core <b>110</b> and level one instruction cache <b>121</b> pipelines are de-coupled from each other. Fetch packet returns from level one instruction cache <b>121</b> can take different number of clock cycles, depending on external circumstances such as whether there is a hit in level one instruction cache <b>121</b> or a hit in level two combined instruction/data cache <b>130</b>. Therefore program access stage <b>1112</b> (PA) can take several clock cycles instead of 1 clock cycle as in the other stages.</p><p id="p-0079" num="0078">The instructions executing in parallel constitute an execute packet. In the preferred embodiment an execute packet can contain up to sixteen instructions. No two instructions in an execute packet may use the same functional unit. A slot is one of five types: 1) a self-contained instruction executed on one of the functional units of central processing unit core <b>110</b> (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b>, D2 unit <b>226</b>, L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>); 2) a unitless instruction such as a NOP (no operation) instruction or multiple NOP instruction; 3) a branch instruction; 4) a constant field extension; and 5) a conditional code extension. Some of these slot types will be further explained below.</p><p id="p-0080" num="0079">Dispatch and decode phases <b>1110</b> include instruction dispatch to appropriate execution unit stage <b>1121</b> (DS), instruction pre-decode stage <b>1122</b> (D1); and instruction decode, operand reads stage <b>1222</b> (D2). During instruction dispatch to appropriate execution unit stage <b>1121</b> (DS) the fetch packets are split into execute packets and assigned to the appropriate functional units. During the instruction pre-decode stage <b>1122</b> (D1) the source registers, destination registers and associated paths are decoded for the execution of the instructions in the functional units. During the instruction decode, operand reads stage <b>1222</b> (D2) more detail unit decodes are done, as well as reading operands from the register files.</p><p id="p-0081" num="0080">Execution phases <b>1130</b> includes execution stages <b>1131</b> to <b>1135</b> (E<b>1</b> to E<b>5</b>). Different types of instructions require different numbers of these stages to complete their execution. These stages of the pipeline play an important role in understanding the device state at CPU cycle boundaries.</p><p id="p-0082" num="0081">During execute <b>1</b> stage <b>1131</b> (E<b>1</b>) the conditions for the instructions are evaluated and operands are operated on. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, execute <b>1</b> stage <b>1131</b> may receive operands from a stream buffer <b>1141</b> and one of the register files shown schematically as <b>1142</b>. For load and store instructions, address generation is performed and address modifications are written to a register file. For branch instructions, branch fetch packet in PG phase is affected. As illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, load and store instructions access memory here shown schematically as memory <b>1151</b>. For single-cycle instructions, results are written to a destination register file. This assumes that any conditions for the instructions are evaluated as true. If a condition is evaluated as false, the instruction does not write any results or have any pipeline operation after execute <b>1</b> stage <b>1131</b>.</p><p id="p-0083" num="0082">During execute <b>2</b> stage <b>1132</b> (E<b>2</b>) load instructions send the address to memory. Store instructions send the address and data to memory. Single-cycle instructions that saturate results set the SAT bit in the control status register (CSR) if saturation occurs. For 2-cycle instructions, results are written to a destination register file.</p><p id="p-0084" num="0083">During execute <b>3</b> stage <b>1133</b> (E<b>3</b>) data memory accesses are performed. Any multiply instructions that saturate results set the SAT bit in the control status register (CSR) if saturation occurs. For 3-cycle instructions, results are written to a destination register file.</p><p id="p-0085" num="0084">During execute <b>4</b> stage <b>1134</b> (E<b>4</b>) load instructions bring data to the CPU boundary. For 4-cycle instructions, results are written to a destination register file.</p><p id="p-0086" num="0085">During execute <b>5</b> stage <b>1135</b> (E<b>5</b>) load instructions write data into a register. This is illustrated schematically in <figref idref="DRAWINGS">FIG. <b>11</b></figref> with input from memory <b>1151</b> to execute <b>5</b> stage <b>1135</b>.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates an example of the instruction coding <b>1300</b> of functional unit instructions used by this invention. Those skilled in the art would realize that other instruction codings are feasible and within the scope of this invention. Each instruction consists of 32 bits and controls the operation of one of the individually controllable functional units (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b>, D2 unit <b>226</b>, L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, C unit <b>245</b> and P unit <b>246</b>). The bit fields are defined as follows.</p><p id="p-0088" num="0087">The creg field <b>1301</b> (bits <b>29</b> to <b>31</b>) and the z bit <b>1302</b> (bit <b>28</b>) are optional fields used in conditional instructions. These bits are used for conditional instructions to identify the predicate register and the condition. The z bit <b>1302</b> (bit <b>28</b>) indicates whether the predication is based upon zero or not zero in the predicate register. If z=1, the test is for equality with zero. If z=0, the test is for nonzero. The case of creg=0 and z=0 is treated as always true to allow unconditional instruction execution. The creg field <b>1301</b> and the z field <b>1302</b> are encoded in the instruction as shown in Table 1.</p><p id="p-0089" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="84pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><thead><row><entry/><entry namest="offset" nameend="3" rowsep="1">TABLE 1</entry></row></thead><tbody valign="top"><row><entry/><entry namest="offset" nameend="3" align="center" rowsep="1"/></row><row><entry/><entry>Conditional</entry><entry>creg</entry><entry>z</entry></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="6"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="70pt" align="left"/><colspec colname="2" colwidth="21pt" align="center"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="14pt" align="center"/><colspec colname="5" colwidth="49pt" align="center"/><tbody valign="top"><row><entry/><entry>Register</entry><entry>31</entry><entry>30</entry><entry>29</entry><entry>28</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row><row><entry/><entry>Unconditional</entry><entry>0</entry><entry>0</entry><entry>0</entry><entry>0</entry></row><row><entry/><entry>Reserved</entry><entry>0</entry><entry>0</entry><entry>0</entry><entry>1</entry></row><row><entry/><entry>A0</entry><entry>0</entry><entry>0</entry><entry>1</entry><entry>z</entry></row><row><entry/><entry>A1</entry><entry>0</entry><entry>1</entry><entry>0</entry><entry>z</entry></row><row><entry/><entry>A2</entry><entry>0</entry><entry>1</entry><entry>1</entry><entry>z</entry></row><row><entry/><entry>A3</entry><entry>1</entry><entry>0</entry><entry>0</entry><entry>z</entry></row><row><entry/><entry>A4</entry><entry>1</entry><entry>0</entry><entry>1</entry><entry>z</entry></row><row><entry/><entry>A5</entry><entry>1</entry><entry>1</entry><entry>0</entry><entry>z</entry></row><row><entry/><entry>Reserved</entry><entry>1</entry><entry>1</entry><entry>x</entry><entry>x</entry></row><row><entry/><entry namest="offset" nameend="5" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0090" num="0088">Execution of a conditional instruction is conditional upon the value stored in the specified data register. This data register is in the global scalar register file <b>211</b> for all functional units. Note that &#x201c;z&#x201d; in the z bit column refers to the zero/not zero comparison selection noted above and &#x201c;x&#x201d; is a don't care state. This coding can only specify a subset of the 16 global registers as predicate registers. This selection was made to preserve bits in the instruction coding. Note that unconditional instructions do not have these optional bits. For unconditional instructions these bits in fields <b>1301</b> and <b>1302</b> (<b>28</b> to <b>31</b>) are preferably used as additional opcode bits.</p><p id="p-0091" num="0089">The dst field <b>1303</b> (bits <b>23</b> to <b>27</b>) specifies a register in a corresponding register file as the destination of the instruction results.</p><p id="p-0092" num="0090">The src2/cst field <b>1304</b> (bits <b>18</b> to <b>22</b>) has several meanings depending on the instruction opcode field (bits <b>4</b> to <b>12</b> for all instructions and additionally bits <b>28</b> to <b>31</b> for unconditional instructions). The first meaning specifies a register of a corresponding register file as the second operand. The second meaning is an immediate constant. Depending on the instruction type, this is treated as an unsigned integer and zero extended to a specified data length or is treated as a signed integer and sign extended to the specified data length.</p><p id="p-0093" num="0091">The src1 field <b>1305</b> (bits <b>13</b> to <b>17</b>) specifies a register in a corresponding register file as the first source operand.</p><p id="p-0094" num="0092">The opcode field <b>1306</b> (bits <b>4</b> to <b>12</b>) for all instructions (and additionally bits <b>28</b> to <b>31</b> for unconditional instructions) specifies the type of instruction and designates appropriate instruction options. This includes unambiguous designation of the functional unit used and operation performed. A detailed explanation of the opcode is beyond the scope of this invention except for the instruction options detailed below.</p><p id="p-0095" num="0093">The e bit <b>1307</b> (bit <b>2</b>) is only used for immediate constant instructions where the constant may be extended. If e=1, then the immediate constant is extended in a manner detailed below. If e=0, then the immediate constant is not extended. In that case the immediate constant is specified by the src2/cst field <b>1304</b> (bits <b>18</b> to <b>22</b>). Note that this e bit <b>1307</b> is used for only some instructions. Accordingly, with proper coding this e bit <b>1307</b> may be omitted from instructions which do not need it and this bit used as an additional opcode bit.</p><p id="p-0096" num="0094">The s bit <b>1307</b> (bit <b>1</b>) designates scalar datapath side A <b>115</b> or vector datapath side B <b>116</b>. If s=0, then scalar datapath side A <b>115</b> is selected. This limits the functional unit to L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b> and the corresponding register files illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Similarly, s=1 selects vector datapath side B <b>116</b> limiting the functional unit to L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b>, P unit <b>246</b> and the corresponding register file illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0097" num="0095">The p bit <b>1308</b> (bit <b>0</b>) marks the execute packets. The p-bit determines whether the instruction executes in parallel with the following instruction. The p-bits are scanned from lower to higher address. If p=1 for the current instruction, then the next instruction executes in parallel with the current instruction. If p=0 for the current instruction, then the next instruction executes in the cycle after the current instruction. All instructions executing in parallel constitute an execute packet. An execute packet can contain up to twelve instructions. Each instruction in an execute packet must use a different functional unit.</p><p id="p-0098" num="0096">There are two different condition code extension slots.</p><p id="p-0099" num="0097">Each execute packet can contain one each of these unique 32-bit condition code extension slots which contains the 4-bit creg/z fields for the instructions in the same execute packet. <figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates the coding for condition code extension slot <b>0</b> and <figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates the coding for condition code extension slot <b>1</b>.</p><p id="p-0100" num="0098"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates the coding for condition code extension slot <b>0</b> having 32 bits. Field <b>1401</b> (bits <b>28</b> to <b>31</b>) specify <b>4</b> creg/z bits assigned to the L1 unit <b>221</b> instruction in the same execute packet. Field <b>1402</b> (bits <b>27</b> to <b>24</b>) specify <b>4</b> creg/z bits assigned to the L2 unit <b>241</b> instruction in the same execute packet. Field <b>1403</b> (bits <b>19</b> to <b>23</b>) specify <b>4</b> creg/z bits assigned to the S1 unit <b>222</b> instruction in the same execute packet. Field <b>1404</b> (bits <b>16</b> to <b>19</b>) specify <b>4</b> creg/z bits assigned to the S2 unit <b>242</b> instruction in the same execute packet. Field <b>1405</b> (bits <b>12</b> to <b>15</b>) specify <b>4</b> creg/z bits assigned to the D1 unit <b>225</b> instruction in the same execute packet. Field <b>1406</b> (bits <b>8</b> to <b>11</b>) specify <b>4</b> creg/z bits assigned to the D2 unit <b>245</b> instruction in the same execute packet. Field <b>1407</b> (bits <b>6</b> and <b>7</b>) is unused/reserved. Field <b>1408</b> (bits <b>0</b> to <b>5</b>) are coded a set of unique bits (CCEX<b>0</b>) to identify the condition code extension slot <b>0</b>. Once this unique ID of condition code extension slot <b>0</b> is detected, the corresponding creg/z bits are employed to control conditional execution of any L1 unit <b>221</b>, L2 unit <b>241</b>, S1 unit <b>222</b>, S2 unit <b>242</b>, D1 unit <b>224</b> and D2 unit <b>225</b> instruction in the same execution packet. These creg/z bits are interpreted as shown in Table 1. If the corresponding instruction is conditional (includes creg/z bits) the corresponding bits in the condition code extension slot <b>0</b> override the condition code bits in the instruction. Note that no execution packet can have more than one instruction directed to a particular execution unit. No execute packet of instructions can contain more than one condition code extension slot <b>0</b>. Thus the mapping of creg/z bits to functional unit instruction is unambiguous. Setting the creg/z bits equal to &#x201c;0000&#x201d; makes the instruction unconditional. Thus a properly coded condition code extension slot <b>0</b> can make some corresponding instructions conditional and some unconditional.</p><p id="p-0101" num="0099"><figref idref="DRAWINGS">FIG. <b>15</b></figref> illustrates the coding for condition code extension slot <b>1</b> having 32 bits. Field <b>1501</b> (bits <b>28</b> to <b>31</b>) specify <b>4</b> creg/z bits assigned to the M1 unit <b>223</b> instruction in the same execute packet. Field <b>1502</b> (bits <b>27</b> to <b>24</b>) specify <b>4</b> creg/z bits assigned to the M2 unit <b>243</b> instruction in the same execute packet. Field <b>1503</b> (bits <b>19</b> to <b>23</b>) specify <b>4</b> creg/z bits assigned to the C unit <b>245</b> instruction in the same execute packet. Field <b>1504</b> (bits <b>16</b> to <b>19</b>) specify <b>4</b> creg/z bits assigned to the N1 unit <b>224</b> instruction in the same execute packet. Field <b>1505</b> (bits <b>12</b> to <b>15</b>) specify <b>4</b> creg/z bits assigned to the N2 unit <b>244</b> instruction in the same execute packet. Field <b>1506</b> (bits <b>5</b> to <b>11</b>) is unused/reserved. Field <b>1507</b> (bits <b>0</b> to <b>5</b>) are coded a set of unique bits (CCEX<b>1</b>) to identify the condition code extension slot <b>1</b>. Once this unique ID of condition code extension slot <b>1</b> is detected, the corresponding creg/z bits are employed to control conditional execution of any M1 unit <b>223</b>, M2 unit <b>243</b>, C unit <b>245</b>, N1 unit <b>224</b> and N2 unit <b>244</b> instruction in the same execution packet. These creg/z bits are interpreted as shown in Table 1. If the corresponding instruction is conditional (includes creg/z bits) the corresponding bits in the condition code extension slot <b>1</b> override the condition code bits in the instruction. Note that no execution packet can have more than one instruction directed to a particular execution unit. No execute packet of instructions can contain more than one condition code extension slot <b>1</b>. Thus the mapping of creg/z bits to functional unit instruction is unambiguous. Setting the creg/z bits equal to &#x201c;0000&#x201d; makes the instruction unconditional. Thus a properly coded condition code extension slot <b>1</b> can make some instructions conditional and some unconditional.</p><p id="p-0102" num="0100">It is feasible for both condition code extension slot <b>0</b> and condition code extension slot <b>1</b> to include a p bit to define an execute packet as described above in conjunction with <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In the preferred embodiment, as illustrated in <figref idref="DRAWINGS">FIGS. <b>14</b> and <b>15</b></figref>, code extension slot <b>0</b> and condition code extension slot <b>1</b> preferably have bit <b>0</b> (p bit) always encoded as <b>1</b>. Thus neither condition code extension slot <b>0</b> nor condition code extension slot <b>1</b> can be in the last instruction slot of an execute packet.</p><p id="p-0103" num="0101">There are two different constant extension slots. Each execute packet can contain one each of these unique 32-bit constant extension slots which contains 27 bits to be concatenated as high order bits with the 5-bit constant field <b>1305</b> to form a 32-bit constant. As noted in the instruction coding description above only some instructions define the src2/cst field <b>1304</b> as a constant rather than a source register identifier. At least some of those instructions may employ a constant extension slot to extend this constant to 32 bits.</p><p id="p-0104" num="0102"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates the fields of constant extension slot <b>0</b>. Each execute packet may include one instance of constant extension slot <b>0</b> and one instance of constant extension slot <b>1</b>. <figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates that constant extension slot <b>0</b> <b>1600</b> includes two fields. Field <b>1601</b> (bits <b>5</b> to <b>31</b>) constitute the most significant 27 bits of an extended 32-bit constant including the target instruction scr2/cst field <b>1304</b> as the five least significant bits. Field <b>1602</b> (bits <b>0</b> to <b>4</b>) are coded a set of unique bits (CSTX<b>0</b>) to identify the constant extension slot <b>0</b>. In the preferred embodiment constant extension slot <b>0</b> <b>1600</b> can only be used to extend the constant of one of an L1 unit <b>221</b> instruction, data in a D1 unit <b>225</b> instruction, an S2 unit <b>242</b> instruction, an offset in a D2 unit <b>226</b> instruction, an M2 unit <b>243</b> instruction, an N2 unit <b>244</b> instruction, a branch instruction, or a C unit <b>245</b> instruction in the same execute packet. Constant extension slot <b>1</b> is similar to constant extension slot <b>0</b> except that bits <b>0</b> to <b>4</b> are coded a set of unique bits (CSTX<b>1</b>) to identify the constant extension slot <b>1</b>. In the preferred embodiment constant extension slot <b>1</b> can only be used to extend the constant of one of an L2 unit <b>241</b> instruction, data in a D2 unit <b>226</b> instruction, an S1 unit <b>222</b> instruction, an offset in a D1 unit <b>225</b> instruction, an M1 unit <b>223</b> instruction or an N1 unit <b>224</b> instruction in the same execute packet.</p><p id="p-0105" num="0103">Constant extension slot <b>0</b> and constant extension slot <b>1</b> are used as follows. The target instruction must be of the type permitting constant specification. As known in the art this is implemented by replacing one input operand register specification field with the least significant bits of the constant as described above with respect to scr2/cst field <b>1304</b>. Instruction decoder <b>113</b> determines this case, known as an immediate field, from the instruction opcode bits. The target instruction also includes one constant extension bit (e bit <b>1307</b>) dedicated to signaling whether the specified constant is not extended (preferably constant extension bit=0) or the constant is extended (preferably constant extension bit=1). If instruction decoder <b>113</b> detects a constant extension slot <b>0</b> or a constant extension slot <b>1</b>, it further checks the other instructions within that execute packet for an instruction corresponding to the detected constant extension slot. A constant extension is made only if one corresponding instruction has a constant extension bit (e bit <b>1307</b>) equal to 1.</p><p id="p-0106" num="0104"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a partial block diagram <b>1700</b> illustrating constant extension. <figref idref="DRAWINGS">FIG. <b>17</b></figref> assumes that instruction decoder <b>113</b> detects a constant extension slot and a corresponding instruction in the same execute packet. Instruction decoder <b>113</b> supplies the 27 extension bits from the constant extension slot (bit field <b>1601</b>) and the 5 constant bits (bit field <b>1305</b>) from the corresponding instruction to concatenator <b>1701</b>. Concatenator <b>1701</b> forms a single 32-bit word from these two parts. In the preferred embodiment the 27 extension bits from the constant extension slot (bit field <b>1601</b>) are the most significant bits and the 5 constant bits (bit field <b>1305</b>) are the least significant bits. This combined 32-bit word is supplied to one input of multiplexer <b>1702</b>. The 5 constant bits from the corresponding instruction field <b>1305</b> supply a second input to multiplexer <b>1702</b>. Selection of multiplexer <b>1702</b> is controlled by the status of the constant extension bit. If the constant extension bit (e bit <b>1307</b>) is 1 (extended), multiplexer <b>1702</b> selects the concatenated 32-bit input. If the constant extension bit is 0 (not extended), multiplexer <b>1702</b> selects the 5 constant bits from the corresponding instruction field <b>1305</b>. Multiplexer <b>1702</b> supplies this output to an input of sign extension unit <b>1703</b>.</p><p id="p-0107" num="0105">Sign extension unit <b>1703</b> forms the final operand value from the input from multiplexer <b>1703</b>. Sign extension unit <b>1703</b> receives control inputs Scalar/Vector and Data Size. The Scalar/Vector input indicates whether the corresponding instruction is a scalar instruction or a vector instruction. The functional units of data path side A <b>115</b> (L1 unit <b>221</b>, S1 unit <b>222</b>, M1 unit <b>223</b>, N1 unit <b>224</b>, D1 unit <b>225</b> and D2 unit <b>226</b>) can only perform scalar instructions. Any instruction directed to one of these functional units is a scalar instruction. Data path side B functional units L2 unit <b>241</b>, S2 unit <b>242</b>, M2 unit <b>243</b>, N2 unit <b>244</b> and C unit <b>245</b> may perform scalar instructions or vector instructions. Instruction decoder <b>113</b> determines whether the instruction is a scalar instruction or a vector instruction from the opcode bits. P unit <b>246</b> may only preform scalar instructions. The Data Size may be 8 bits (byte B), 16 bits (half-word H), 32 bits (word W), 64 bits (double word D), quad word (128 bit) data or half vector (256 bit) data.</p><p id="p-0108" num="0106">Table 2 lists the operation of sign extension unit <b>1703</b> for the various options.</p><p id="p-0109" num="0000"><tables id="TABLE-US-00002" num="00002"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="5"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="42pt" align="left"/><colspec colname="2" colwidth="35pt" align="left"/><colspec colname="3" colwidth="49pt" align="center"/><colspec colname="4" colwidth="77pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="4" rowsep="1">TABLE 2</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row><row><entry/><entry>Instruction</entry><entry>Operand</entry><entry>Constant</entry><entry/></row><row><entry/><entry>Type</entry><entry>Size</entry><entry>Length</entry><entry>Action</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/><entry>Scalar</entry><entry>B/H/W/D</entry><entry>&#x2002;5 bits</entry><entry>Sign extend to 64 bits</entry></row><row><entry/><entry>Scalar</entry><entry>B/H/W/D</entry><entry>32 bits</entry><entry>Sign extend to 64 bits</entry></row><row><entry/><entry>Vector</entry><entry>B/H/W/D</entry><entry>&#x2002;5 bits</entry><entry>Sign extend to operand</entry></row><row><entry/><entry/><entry/><entry/><entry>size and replicate</entry></row><row><entry/><entry/><entry/><entry/><entry>across whole vector</entry></row><row><entry/><entry>Vector</entry><entry>B/H/W</entry><entry>32 bits</entry><entry>Replicate 32-bit</entry></row><row><entry/><entry/><entry/><entry/><entry>constant across each</entry></row><row><entry/><entry/><entry/><entry/><entry>32-bit (W) lane</entry></row><row><entry/><entry>Vector</entry><entry>D</entry><entry>32 bits</entry><entry>Sign extend to 64 bits</entry></row><row><entry/><entry/><entry/><entry/><entry>and replicate across</entry></row><row><entry/><entry/><entry/><entry/><entry>each 64-bit (D) lane</entry></row><row><entry/><entry namest="offset" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0110" num="0107">It is feasible for both constant extension slot <b>0</b> and constant extension slot <b>1</b> to include a p bit to define an execute packet as described above in conjunction with <figref idref="DRAWINGS">FIG. <b>13</b></figref>. In the preferred embodiment, as in the case of the condition code extension slots, constant extension slot <b>0</b> and constant extension slot <b>1</b> preferably have bit <b>0</b> (p bit) always encoded as <b>1</b>. Thus neither constant extension slot <b>0</b> nor constant extension slot <b>1</b> can be in the last instruction slot of an execute packet.</p><p id="p-0111" num="0108">It is technically feasible for an execute packet to include a constant extension slot <b>0</b> or <b>1</b> and more than one corresponding instruction marked constant extended (e bit=1). For constant extension slot <b>0</b> this would mean more than one of an L1 unit <b>221</b> instruction, data in a D1 unit <b>225</b> instruction, an S2 unit <b>242</b> instruction, an offset in a D2 unit <b>226</b> instruction, an M2 unit <b>243</b> instruction or an N2 unit <b>244</b> instruction in an execute packet have an e bit of <b>1</b>. For constant extension slot <b>1</b> this would mean more than one of an L2 unit <b>241</b> instruction, data in a D2 unit <b>226</b> instruction, an S1 unit <b>222</b> instruction, an offset in a D1 unit <b>225</b> instruction, an M1 unit <b>223</b> instruction or an N1 unit <b>224</b> instruction in an execute packet have an e bit of <b>1</b>. Supplying the same constant extension to more than one instruction is not expected to be a useful function. Accordingly, in one embodiment instruction decoder <b>113</b> may determine this case an invalid operation and not supported. Alternately, this combination may be supported with extension bits of the constant extension slot applied to each corresponding functional unit instruction marked constant extended.</p><p id="p-0112" num="0109">Special vector predicate instructions use registers in predicate register file <b>234</b> to control vector operations. In the current embodiment all these SIMD vector predicate instructions operate on selected data sizes. The data sizes may include byte (8 bit) data, half word (16 bit) data, word (32 bit) data, double word (64 bit) data, quad word (128 bit) data and half vector (256 bit) data. Each bit of the predicate register controls whether a SIMD operation is performed upon the corresponding byte of data. The operations of P unit <b>245</b> permit a variety of compound vector SIMD operations based upon more than one vector comparison. For example a range determination can be made using two comparisons. A candidate vector is compared with a first vector reference having the minimum of the range packed within a first data register. A second comparison of the candidate vector is made with a second reference vector having the maximum of the range packed within a second data register. Logical combinations of the two resulting predicate registers would permit a vector conditional operation to determine whether each data part of the candidate vector is within range or out of range.</p><p id="p-0113" num="0110">L1 unit <b>221</b>, S1 unit <b>222</b>, L2 unit <b>241</b>, S2 unit <b>242</b> and C unit <b>245</b> often operate in a single instruction multiple data (SIMD) mode. In this SIMD mode the same instruction is applied to packed data from the two operands. Each operand holds plural data elements disposed in predetermined slots. SIMD operation is enabled by carry control at the data boundaries. Such carry control enables operations on varying data widths.</p><p id="p-0114" num="0111"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates the carry control. AND gate <b>1801</b> receives the carry output of bit N within the operand wide arithmetic logic unit (64 bits for scalar datapath side A <b>115</b> functional units and 512 bits for vector datapath side B <b>116</b> functional units). AND gate <b>1801</b> also receives a carry control signal which will be further explained below. The output of AND gate <b>1801</b> is supplied to the carry input of bit N+1 of the operand wide arithmetic logic unit. AND gates such as AND gate <b>1801</b> are disposed between every pair of bits at a possible data boundary. For example, for 8-bit data such an AND gate will be between bits <b>7</b> and <b>8</b>, bits <b>15</b> and <b>16</b>, bits <b>23</b> and <b>24</b>, etc. Each such AND gate receives a corresponding carry control signal. If the data size is of the minimum, then each carry control signal is 0, effectively blocking carry transmission between the adjacent bits. The corresponding carry control signal is 1 if the selected data size requires both arithmetic logic unit sections. Table 3 below shows example carry control signals for the case of a 512 bit wide operand such as used by vector datapath side B <b>116</b> functional units which may be divided into sections of 8 bits, 16 bits, 32 bits, 64 bits, 128 bits or 256 bits. In Table 3 the upper 32 bits control the upper bits (bits <b>128</b> to <b>511</b>) carries and the lower 32 bits control the lower bits (bits <b>0</b> to <b>127</b>) carries. No control of the carry output of the most significant bit is needed, thus only 63 carry control signals are required.</p><p id="p-0115" num="0000"><tables id="TABLE-US-00003" num="00003"><table frame="none" colsep="0" rowsep="0"><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="offset" colwidth="14pt" align="left"/><colspec colname="1" colwidth="63pt" align="center"/><colspec colname="2" colwidth="140pt" align="left"/><thead><row><entry/><entry namest="offset" nameend="2" rowsep="1">TABLE 3</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row><row><entry/><entry>Data Size</entry><entry>Carry Control Signals</entry></row><row><entry/><entry namest="offset" nameend="2" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry/></row></tbody></tgroup><tgroup align="left" colsep="0" rowsep="0" cols="3"><colspec colname="1" colwidth="35pt" align="right"/><colspec colname="2" colwidth="42pt" align="left"/><colspec colname="3" colwidth="140pt" align="left"/><tbody valign="top"><row><entry>8</entry><entry>bits (B)</entry><entry>&#x2212;000 0000 0000 0000 0000 0000 0000 0000</entry></row><row><entry/><entry/><entry>0000 0000 0000 0000 0000 0000 0000 0000</entry></row><row><entry>16</entry><entry>bits (H)</entry><entry>&#x2212;101 0101 0101 0101 0101 0101 0101 0101</entry></row><row><entry/><entry/><entry>0101 0101 0101 0101 0101 0101 0101 0101</entry></row><row><entry>32</entry><entry>bits (W)</entry><entry>&#x2212;111 0111 0111 0111 0111 0111 0111 0111</entry></row><row><entry/><entry/><entry>0111 0111 0111 0111 0111 0111 0111 0111</entry></row><row><entry>64</entry><entry>bits (D)</entry><entry>&#x2212;111 1111 0111 1111 0111 1111 0111 1111</entry></row><row><entry/><entry/><entry>0111 1111 0111 1111 0111 1111 0111 1111</entry></row><row><entry>128</entry><entry>bits</entry><entry>&#x2212;111 1111 1111 1111 0111 1111 1111 1111</entry></row><row><entry/><entry/><entry>0111 1111 1111 1111 0111 1111 1111 1111</entry></row><row><entry>256</entry><entry>bits</entry><entry>&#x2212;111 1111 1111 1111 1111 1111 1111 1111</entry></row><row><entry/><entry/><entry>0111 1111 1111 1111 1111 1111 1111 1111</entry></row><row><entry namest="1" nameend="3" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0116" num="0112">It is typical in the art to operate on data sizes that are integral powers of 2 (2<sup>N</sup>). However, this carry control technique is not limited to integral powers of <b>2</b>. One skilled in the art would understand how to apply this technique to other data sizes and other operand widths.</p><p id="p-0117" num="0113"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates one view showing the cooperation between central processing unit core <b>110</b> and a program memory controller <b>1930</b>. Central processing unit core <b>110</b> regularly generates addresses for needed instructions for its operation. In the preferred embodiment of this invention, central processing unit core <b>110</b> operates on virtual memory addresses. Also in the preferred embodiment the instructions cached in level one instruction cache <b>121</b> are accessed by these virtual addresses. As illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, this virtual address is expressed in 48 bits in the preferred embodiment. In the preferred embodiment, level two combined instruction/data cache <b>130</b> and other memories operate upon a physical address, requiring a conversion between the virtual address and the physical address for any cache misses to level one instruction cache <b>121</b> serviced by level two combined instruction/data cache <b>130</b>.</p><p id="p-0118" num="0114">Program memory controller <b>1930</b> includes a micro table look-aside buffer (&#x3bc;TLB) <b>1931</b> for address translation. If a tag comparison with TAGRAM <b>1934</b> determines the requested fetch packet is not stored in level one instruction cache <b>121</b> (miss), then this fetch packet is requested from level two combined instruction/data cache <b>130</b>. Because level one instruction cache <b>121</b> is virtually tagged and level two combined instruction/data cache <b>130</b> is physically tagged, this requires an address translation. The virtual address is supplied to micro table look-aside buffer <b>1931</b>. Address translation is typically performed using a table of most significant bits of virtual addresses and the corresponding most significant bits of physical addresses. In this example upon detecting the correct address pair, the address translation substitutes the most significant physical address bits from the table for the most significant virtual address bits of the requested address. It is typical that the least significant bits of the virtual address are the same as the least significant bits of the physical address. In this example, a complete virtual address/physical address translation table is stored in memory management unit (MMU) <b>1920</b>. In addition, level one instruction cache <b>121</b> includes micro table look-aside buffer <b>1931</b> which stores a subset of some of the address translation table entries in a cache-like fashion. When servicing an address translation, the requested virtual address is compared with address translation table entries stored in micro table look-aside buffer <b>1931</b>. If the virtual address matches a table entry in micro table look-aside buffer <b>1931</b>, the matching table entry is used for address translation. If the virtual address does not match any table entry in micro table look-aside buffer <b>1931</b>, then these address translation parameters are fetched from the memory management unit <b>1920</b>. Micro table look-aside buffer <b>1931</b> transmits a page translation entry request for the virtual address to memory management unit <b>1920</b>. Memory management unit <b>1920</b> finds the corresponding address translation entry and returns this entry to micro table look-aside buffer <b>1931</b>. Micro table look-aside buffer <b>1931</b> stores this newly fetched translation entry, typically casting out a currently stored entry to make room. Following address translation the physical address passes to level two combined instruction/data cache <b>130</b>.</p><p id="p-0119" num="0115">Branch predictor <b>1911</b> supplies the virtual fetch address to program memory controller <b>1930</b> as well as a prefetch count. Branch prediction typically stores the memory address of each conditional branch instruction encountered in the program code as it executes. This enables branch predictor <b>1911</b> to recognize a conditional branch it has encountered. Associated with the conditional instruction address is a taken/not taken branch prediction and any branching history used in dynamic branch prediction. This branch prediction information will always be limited to a fairly small section of the program code due to limits in the amount of memory and circuits which are included within branch predictor <b>1911</b>. However, based upon the current instruction memory location and the predicted path through the program code due to branch prediction, branch predictor <b>1911</b> can determine a predicted number of linearly following instruction fetch packets to be used after the current instruction fetch packet access before a branch is predicted to be taken off this linear path. This number is called the fetch packet count or the prefetch count.</p><p id="p-0120" num="0116">Central processing unit core <b>110</b> exchanges emulation information with emulation support unit <b>1932</b> which is a part of program memory controller <b>1930</b>.</p><p id="p-0121" num="0117">Central processing unit core <b>110</b> receives instructions in the form of instruction fetch packets from program memory controller <b>1930</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, these fetch packets are 512 bits (64 bytes) in the preferred embodiment. In the preferred embodiment level one instruction cache <b>121</b>, level two combined instruction/data cache <b>130</b> and any other memory store fetch packets aligned with 64 byte boundaries. Depending upon where the instructions are stored, this fetch packet may be recalled from level one instruction cache <b>121</b>, level two combined instruction/data cache <b>130</b> or other memory.</p><p id="p-0122" num="0118">Program memory controller <b>1930</b> compares a portion of the fetch address received from central processing unit core <b>110</b> with entries in TAGRAM <b>1934</b>. TAGRAM <b>1934</b> stores tag data for each cache line stored in level one instruction cache <b>121</b>. Corresponding most significant bits of the fetch address are compared with each set of tags in TAGRAM <b>1934</b>. A match between these bits of the fetch address and any tag (hit) indicates that the instructions stored at the fetch address are stored in level one instruction cache <b>121</b> at a location corresponding to the matching tag. Upon such a match, program memory controller <b>1930</b> recalls the instructions from level one instruction cache <b>121</b> for supply as a fetch packet to central processing unit core <b>110</b>.</p><p id="p-0123" num="0119">The failure of a match between these bits of the fetch address and any tag (miss) indicates that the instructions stored at the fetch address are not stored in level one instruction cache <b>121</b>. Program memory controller <b>1930</b> transmits a cache request to unified memory controller (UMC) <b>1940</b> to seek the instructions in level two combined instruction/data cache <b>130</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>). The cache request is accompanied by a physical address translated from the virtual address as discussed above. If the instructions at this address are stored in level two combined instruction/data cache <b>130</b> (hit), the request is serviced from this cache. Otherwise the request is supplied to a higher level memory (not illustrated).</p><p id="p-0124" num="0120">Program memory controller <b>1930</b> includes coherency support unit <b>1935</b>. Coherence support unit <b>1935</b> makes sure that data movements preserve the most recent instructions for supply to central processing unit core <b>110</b>.</p><p id="p-0125" num="0121"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates another view of the interface between the central processing unit core <b>110</b> and program memory controller <b>1930</b>. In the preferred embodiment, level one instruction cache <b>121</b> has a fixed cache size of 32 KB. Level one instruction cache <b>121</b> maximizes performance of the code execution and facilitates fetching instructions at a fast clock rate. Level one instruction cache <b>121</b> hides the latency associated with executing code store in a slower system memory. Each central processing unit core <b>110</b> interfaces with a separate program memory controller <b>1930</b>, which interface with the unified memory controller <b>1940</b> for level two combined instruction/data cache <b>130</b>.</p><p id="p-0126" num="0122">In the preferred embodiment level one instruction cache <b>121</b> and program memory controller <b>1930</b> include the following attributes. They comprise a 32 KB 4-way instruction cache. They are virtually indexed and virtually tagged cache with a 49-bit virtual address. They include virtualization support having an integrated micro table look-aside buffer <b>1931</b>. The cache lines have a size of 64 bytes. In the preferred embodiment this is the same size as a fetch packet. They can queue up to 8 pairs of fetch packet requests to unified memory controller <b>1940</b> to enable prefetch in a program pipeline.</p><p id="p-0127" num="0123">Even though level one instruction cache <b>121</b> line size is 64 bytes, the PMC-UMC interface is optimized so that the unified memory controller <b>1940</b> returns up to 2 dataphases (128 bytes). According to this invention more fully described below, extra returned instructions can be conditionally stored upon a service of a level one instruction cache miss.</p><p id="p-0128" num="0124">Central processing unit core <b>110</b> transmits a fetch address and a fetch packet count upon each instruction fetch request. The fetch packet count is generated by branch predictor <b>1911</b> (<figref idref="DRAWINGS">FIG. <b>19</b></figref>). The fetch packet count indicates a predicted number of sequential 64-byte cache lines to be returned to central processing unit core <b>110</b> starting from the given address. Program memory controller <b>1930</b> fetch finite state machine <b>2024</b> issues a prefetch for each of the packets and combines them into pairs in scoreboard <b>2041</b> whenever an incoming request to the scoreboard can be satisfied by the second dataphase of the previous request. A fetch packet count of 0 indicates central processing unit core <b>110</b> requests for program memory controller <b>1930</b> in an incremental mode to fetch 64-byte lines with no fetch ahead. Central processing unit core <b>110</b> must request a flush for program memory controller <b>1930</b> to exit incremental mode and resume normal operation.</p><p id="p-0129" num="0125"><figref idref="DRAWINGS">FIG. <b>21</b></figref> illustrates how a fetch address <b>2100</b> is parsed for handling by program memory controller <b>1930</b>. Fetch address <b>2100</b> is divided into: offset <b>2101</b>; set <b>2102</b>; and tag <b>2103</b>. Cache lines in level one instruction cache <b>121</b> are 64 bytes long. Assuming memory is byte addressable, then the location within a cache line of level one instruction cache <b>121</b> serves as a proxy for the six least significant bits of the address (offset <b>2101</b>). Set bits <b>2102</b> correspond directly to a physical location within level one instruction cache <b>121</b>. If level one instruction cache <b>121</b> stores an instruction, it is in a location corresponding to set bits <b>2102</b>. The tag bits <b>2103</b> are stored for comparison with the fetch address. A match (hit) indicates that the addressed instruction(s) are stored in level one instruction cache <b>121</b>. If no match is found (miss), then the instructions of the requested fetch packet must be obtained from another source than level one instruction cache <b>121</b>.</p><p id="p-0130" num="0126">Program memory controller <b>1930</b> operates in plural instruction phases. <figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates phases: <b>2010</b>; <b>2020</b>; <b>2030</b>; <b>2040</b>; and <b>2050</b>. Operations take place simultaneously during phase <b>2010</b>, <b>2020</b>, <b>2030</b>, <b>2040</b> and <b>2050</b> on differing fetch requests.</p><p id="p-0131" num="0127">Instruction fetch unit <b>111</b> (part of central processing unit core <b>110</b>, see <figref idref="DRAWINGS">FIG. <b>1</b></figref>) determines the memory address of the next instruction fetch packet. This fetch address is supplied to one input of multiplexer <b>2011</b> active in phase <b>2010</b>. This fetch address is also supplied to fetch address register <b>2022</b> active in phase <b>2020</b>. As part of branch prediction, instruction fetch unit <b>111</b> also supplies a fetch packet count register <b>2023</b> active in phase <b>2020</b>.</p><p id="p-0132" num="0128">The combination of multiplexers <b>2011</b> and <b>2012</b> supply one of three addresses to TAGRAM <b>1934</b> for tag comparison. Multiplexer <b>2011</b> selects between the fetch address received from central processing unit core <b>110</b> and a prefetch address from prefetch finite state machine <b>2024</b>. Formation of this prefetch address is described above. Multiplexer <b>2012</b> selects between the output of multiplexer <b>2011</b> and the virtual address in program memory controller scoreboard <b>2041</b> corresponding to a return from unified memory controller <b>1940</b>. An access from program memory controller scoreboard <b>2041</b> has greatest priority. An access from central processor unit core <b>110</b> has the next highest priority. An access from prefetch finite state machine <b>2024</b> has the lowest priority.</p><p id="p-0133" num="0129">During phase <b>2020</b> prefetch finite state machine (FSM) <b>2024</b> optionally generates a prefetch request. The prefetch request includes an address calculated from the central processing unit core <b>110</b> request address and the fetch packet count as described above. Prefetch finite state machine <b>2024</b> supplies the next prefetch address to multiplexer <b>2011</b>. Prefetch finite state machine <b>2024</b> supplies a micro table look-aside buffer request to micro table look-aside buffer <b>2035</b> for page translation data for the prefetch address if it is a different page than the initial request from central processing unit core <b>110</b>.</p><p id="p-0134" num="0130">Also during phase <b>2020</b> the address selected by the multiplexers <b>2011</b> and <b>2012</b> in the prior phase <b>2010</b> are supplied to TAGRAM <b>1934</b> to begin tag comparison.</p><p id="p-0135" num="0131">In phase <b>2030</b> the tag comparison completes. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref>, tag compare <b>2031</b> separately compares the tag portion <b>2103</b> of the presented address with data stored in the four banks of TAGRAM <b>1934</b>. The comparison generates either a hit or a miss. A hit indicates that instructions at the requested address are stored in memory <b>121</b>. In this case multiplexer <b>2036</b> supplies these instructions from memory <b>121</b> to central processing unit core <b>110</b>.</p><p id="p-0136" num="0132">The tag compare of program memory controller <b>1930</b> obtains way information in parallel with information on the requested line. For cache hits the way information is needed to locate the requested fetch packet. For cache misses the way information determines the cache line evicted (written-over) by data returned from a higher level memory. On a level one instruction cache miss, program memory controller <b>1930</b> stores this way information in scoreboard <b>2041</b> with other data on the requested line. Once the data returns from level two combined instruction/data cache <b>130</b>, program memory controller <b>1930</b> consults scoreboard <b>2041</b> to determine which way to store. A line to be allocated (whether demand or prefetch) is invalidated once the request is generated to avoid false hits by newer accesses while return data of the requested line is pending.</p><p id="p-0137" num="0133">Upon a miss, program memory controller <b>1930</b> operating in phase <b>2040</b> seeks the instructions stored at that address from level two combined instruction/data cache <b>130</b> via unified memory controller <b>1940</b>. This includes: establishing an entry in program memory controller scoreboard <b>2041</b>; receiving way information from FIFO replacement unit <b>2033</b> selected by multiplexer <b>2034</b>; and receiving the translated physical address from micro table look-aside buffer <b>1931</b>. Program memory controller scoreboard <b>2041</b> generates a request signal to unified memory controller <b>1940</b> for the instructions stored at this translated physical address.</p><p id="p-0138" num="0134">Program memory controller <b>1930</b> does not search in-flight requests stored in scoreboard <b>2041</b> for possible match between prior requests. Thus it is possible that two or more requests for the same cache line to be allocated to different ways of the same set. This could cause two or more matches upon tag compare if the same set is requested in the future. Whenever this occurs, program memory controller <b>1930</b> invalidates one of the duplicated tags and the corresponding cache way to free up the way for a new entry. This invalidation only occurs when a set with duplicate tags is accessed for a hit/miss decision on another request. In the preferred embodiment program memory controller <b>1930</b> keeps the most significant valid way (i.e. the way denoted by the MSB of the set's valid bits) while invalidating other ways. For example, if way <b>0</b> and way <b>2</b> have identical tags and are valid, then way <b>2</b> is kept and way <b>0</b> is invalidated. L1P does not invalidate duplicate tags on emulation accesses.</p><p id="p-0139" num="0135">In phase <b>2050</b> (which may include more than one phase depending upon the location of the instructions sought) unified memory controller <b>1940</b> services the instruction request. This process includes determining whether the requested instructions are stored in level two combined instruction/data cache <b>130</b>. On a cache hit to level two combined instruction/data cache <b>130</b>, unified memory controller <b>1940</b> supplies the instructions from level two combined instruction/data cache <b>130</b>. On a cache miss to level two combined instruction/data cache <b>130</b>, unified memory controller <b>1940</b> seeks these instructions from another memory. This other memory could be an external third level cache or and an external main memory. The number of phases required to return the requested instructions depend upon whether they are cached in level two combined instruction/data cache <b>130</b>, they are cached in an external level three cache or they are stored in external main memory.</p><p id="p-0140" num="0136">All instructions returned from unified memory controller <b>1940</b> are stored in memory <b>121</b>. Thus these instructions are available for later use by central processing unit core <b>110</b>. If the instruction request triggering the request to unified memory controller <b>1940</b> was directly from central processing unit core <b>110</b> (demand fetch), multiplexer <b>2036</b> contemporaneously supplies the returned instructions directly to central processing unit core <b>110</b>. If the request triggering the request to unified memory controller <b>1940</b> was a prefetch request, then multiplexer <b>2036</b> blocks supply of these instructions to central processing unit core <b>110</b>. These instructions are merely stored in memory <b>121</b> based upon an expectation of future need by central processing unit core <b>110</b>.</p><p id="p-0141" num="0137"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is a partial schematic diagram illustrating relevant parts of unified memory controller <b>1940</b>. Program memory controller <b>1930</b> supplies a requested address to unified memory controller <b>1940</b> upon a level one cache miss.</p><p id="p-0142" num="0138">Unified memory controller <b>1940</b> receives requests from program memory controller <b>1930</b> in the form of requested addresses. Program memory controller <b>1930</b> makes these requests upon a cache miss into level one instruction cache <b>121</b>. The instructions stored at the requested address are not stored in level one instruction cache <b>121</b> and are sought for level two unified instruction/data cache <b>130</b>. Thus program memory controller <b>1930</b> sends requested address to unified memory controller <b>1940</b>.</p><p id="p-0143" num="0139">The requested address is transmitted to tags <b>2201</b>. In a manner known in the art, the requested address is compared with partial addresses store in tags <b>2201</b> to determine whether level two combined instruction/data cache <b>130</b> stores the instructions at the requested address. Upon detecting no match (miss), unified memory controller <b>1940</b> transmits a service request to a next level memory. This next level memory could be an external level three cache or an external main memory. This next level memory will ultimately return the data or instructions at the requested address. This return data or instructions are stored in level two combined instruction/data cache <b>130</b>. This storage typically involves casting out and replacing another entry in level two combined instruction/data cache <b>130</b>. The original request is then serviced from level two combined instruction/data cache <b>130</b>.</p><p id="p-0144" num="0140">Upon detecting a match (hit), tags <b>2201</b> transmits an indication of the address to level two combined instruction/data cache <b>130</b>. This indication enables level two combined instruction/data cache to locate and recall a cache line corresponding to the requested address. This recalled cache line is stored in register <b>2201</b>.</p><p id="p-0145" num="0141">Register <b>2202</b> is illustrated as having an upper half and a lower half. The cache line size in level two combined instruction/data cache <b>130</b> is twice the cache line size in level one instruction cache <b>121</b>. Thus, recall of one cache line from level two combined instruction/data cache <b>130</b> can supply two cache lines for level one instruction cache <b>121</b>. Multiplexer <b>2203</b> and multiplexer controller <b>2204</b> select either the upper half or the lower half of the level two combined instruction/data cache line for supply to program memory controller <b>1930</b>.</p><p id="p-0146" num="0142">Multiplexer controller <b>2204</b> receives the requested address from program memory controller <b>1930</b>. In most circumstances one bit of this address controls the selection of multiplexer <b>2203</b>. If this address bit is 0, then the requested address is in the lower half of the level two combined instruction/data cache line stored in register <b>2202</b>. Multiplexer controller <b>2204</b> causes multiplexer <b>2202</b> to select the lower half of register <b>2203</b> for supply to program memory controller <b>1930</b>. If this address bit is 1, then the requested address is in the upper half of the level two combined instruction/data cache line stored in register <b>2202</b>, and multiplexer controller <b>2204</b> causes multiplexer <b>2202</b> to select this upper half. In the preferred embodiment cache lines in level one instruction cache <b>121</b> are 64 bytes and cache lines in level two combined instruction/data cache <b>130</b> are 128 bytes. For this cache line size selection, the controlling address bit is bit <b>7</b>, because 2<sup>7 </sup>equals 128.</p><p id="p-0147" num="0143">In the prior art the unselected half of the level two combined instruction/data cache line stored in register <b>2202</b> would not be used. In the prior art it is discarded by being written over upon the next recall of a cache line from level two combined instruction/data cache.</p><p id="p-0148" num="0144">This invention is an exception to this prior art process. This invention is applicable to a demand fetch. A demand fetch is a requested set of instruction directly from central processing unit core <b>110</b> and not a prefetch from prefetch finite state machine <b>2024</b>. This invention is applicable to such a demand fetch that is a miss to level one instruction cache <b>121</b> and a hit to the upper half of the level two combined instruction/data cache line <b>130</b>. Under these circumstances (demand fetch which is level one cache miss and a hit to the upper half of level two combined instruction/data cache line), initially the upper half stored in register <b>2202</b> is selected by multiplexer <b>2203</b> for supply to program memory controller <b>1930</b>. Upon the next memory cycle multiplexer controller <b>2204</b> controls multiplexer <b>2203</b> to supply the lower half of register <b>2202</b>. Upon the return to program memory controller <b>1930</b> this is treated as a prefetch return. The difference in treatment of demand fetch returns and prefetch returns is described below.</p><p id="p-0149" num="0145"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a flow chart of operation <b>2300</b> according to a prefetch technique of this invention. Operation <b>2300</b> illustrates only the part of the operation of program memory controller <b>1930</b> and unified memory controller <b>1940</b> relevant to this invention. Operations relevant to this aspect of the invention begin at start block <b>2301</b> upon an instruction fetch.</p><p id="p-0150" num="0146">Test block <b>2302</b> determines if the fetch address of an instruction fetch just submitted for tag match results in a miss within program memory controller <b>190</b>. If the fetch address was not a miss (No at test block <b>2302</b>), then this invention is not applicable. Flow proceeds to continue block <b>2303</b> to other aspects of the fetch process not relevant to this invention. In this case, if a fetch address is not a miss, then it is a hit. The instructions sought are stored in level one instruction cache <b>121</b>. This fetch is thus serviced from level one instruction cache <b>121</b>. These processes are not relevant to this invention.</p><p id="p-0151" num="0147">If the fetch was a miss (Yes at test block <b>2302</b>), then the fetch is submitted to unified memory controller <b>1940</b> for service. As a part of this process, test block <b>2304</b> determines if this is a hit to tags <b>2201</b>. If this is a hit (yes at test block <b>2304</b>), then flow proceeds to continue block <b>2305</b> to other aspects of the fetch process not relevant to this invention. In this case if it is not a hit (No at test block <b>2404</b>), then it is a miss. The instruction fetch is serviced by higher level memory, which would typically be an external level three cache or an external main memory.</p><p id="p-0152" num="0148">If test block <b>2304</b> determines the requested address is a hit into unified memory controller (Yes at test block <b>2304</b>), then block <b>2306</b> fetches and buffers the instructions at the requested address. As illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>, the tag matching the requested address permits identification of the storage location within level two combined instruction/data cache <b>130</b>. The instructions stored at the corresponding line within level two combined instructions/data cache <b>130</b> are recalled and stored in register <b>2202</b> in upper and lower halves as illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>.</p><p id="p-0153" num="0149">Next test block <b>2307</b> determines if the fetch address was a demand fetch to the upper half of a cache line in level two combined instruction/data cache <b>130</b>. A fetch request is a demand fetch if the request was issued directly from central processing unit core <b>110</b>. A fetch request issued by prefetch finite state machine <b>2074</b> is a prefetch request and not a demand fetch. As described above the size of cache lines in level one instruction cache <b>121</b> is half the same as the size of cache lines in level two combined instruction/data cache <b>130</b>. The addresses of these cache lines are aligned so that each cache line of level one instruction cache <b>121</b> corresponds to one of the lower half or the upper half of a cache line in level two combined instruction/data cache <b>130</b>. If the instruction fetch was not both a demand fetch and to the upper half of a cache line in level two combined instruction/data cache <b>130</b> (No at test block <b>2307</b>), then process <b>2300</b> proceeds to continue block <b>2308</b> to other aspects of the fetch process not relevant to this invention. This invention is not applicable to a demand fetch to a lower half a cache line in level two combined instruction/data cache <b>130</b> or to a prefetch request. Process <b>2300</b> proceeds according to this invention only if the instruction fetch was a demand fetch to the upper half of a cache line in level two combined instruction/data cache <b>130</b> (yes at test block <b>2307</b>).</p><p id="p-0154" num="0150">If the fetch address was a demand fetch to the upper half of a cache line in level two combined instruction/data cache <b>130</b> (yes at test block <b>2307</b>), then block <b>2309</b> supplies the upper half level two combined instruction/data cache line to program memory controller <b>1930</b> as a demand fetch. There is preferably a side channel to the instruction return to program memory controller <b>1930</b> to indicate if the returned instructions were in response to a demand fetch or in response to a prefetch. As noted below in <figref idref="DRAWINGS">FIG. <b>24</b></figref>, program memory controller <b>1930</b> handles these two types of fetches differently.</p><p id="p-0155" num="0151">On the next memory cycle, block <b>2310</b> supplies the lower half level two combined instruction/data cache line to program memory controller <b>1930</b> as a prefetch. Signaling program memory controller <b>1930</b> this return is a prefetch changes how it is handled as noted in <figref idref="DRAWINGS">FIG. <b>24</b></figref>. This provides a virtual prefetch of the lower half level two combined instruction/data cache line that is handled similarly to a prefetch request issued by prefetch finite state machine <b>2074</b>.</p><p id="p-0156" num="0152">Handling the prefetch of this invention differs slightly from handling other prefetches.</p><p id="p-0157" num="0153"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a flow chart <b>2400</b> illustrating the response of program memory controller <b>1930</b> to a return from unified memory controller <b>1940</b>. Operation <b>2400</b> illustrates only the part of the operation of program memory controller <b>1930</b> relevant to this invention. Operations relevant to this aspect of the invention begin at start block <b>2401</b>.</p><p id="p-0158" num="0154">Test block <b>2402</b> determines whether a cache service is received from unified memory controller <b>1940</b>. If there is no cache service return (no at test block <b>2402</b>), then this invention is not applicable. Process <b>2400</b> continues with continue block <b>2403</b>.</p><p id="p-0159" num="0155">Upon receipt of a cache service return from unified memory controller <b>1940</b> (yes at test block <b>2402</b>), test block <b>2404</b> determines whether the cache service return is to a demand request. As noted above, a demand request is issued directly from central processing unit core <b>110</b>. If this is a demand request return (yes at test block <b>2404</b>), then block <b>2405</b> forwards the returned instructions to central processing unit core <b>110</b>. Because central processing unit core <b>110</b> has directly requested these instructions (demand fetch), central processing unit core <b>110</b> is waiting for the instructions. Central processing unit core <b>110</b> may even be stalled and not producing results. Thus the cache service return is forwarded directly to central processing unit core <b>110</b> with the goal to reduce any stall time. Process <b>2400</b> then advances to block <b>2406</b>.</p><p id="p-0160" num="0156">If this is not a demand request return (no at test block <b>2404</b>) or if this was a demand request return (yes at test block <b>2404</b>) following block <b>2406</b> suppling demand request returned instructions to central processing unit core <b>110</b>, then block <b>2406</b> stores the returned instructions in level one instruction cache <b>121</b>. The existence a cache service return from unified memory controller <b>1940</b> (test block <b>2402</b>) implies a cache miss in program memory controller <b>1930</b>. Thus the returned instruction should be stored in level one instruction <b>121</b> whether the triggering event was a demand request or a prefetch request. In this regard the supply of the lower half level two cache line following an upper half L2 hit is treated as a prefetch request.</p><p id="p-0161" num="0157">The prefetch of this invention differs from a prefetch issued by prefetch finite state machine <b>2074</b> in several regards. Firstly, the prefetch of this invention is issued under different circumstances that a prefetch issued by prefetch finite state machine <b>2074</b>. Prefetch finite state machine <b>2074</b> issues a prefetch requests following a demand request if the prefetch count is nonzero. Unified memory controller <b>1940</b> issues the prefetch of this invention upon a level one instruction cache miss and a level two combined instruction/data cache hit to an upper half of a level two cache line.</p><p id="p-0162" num="0158">Secondly, the processes in response differ. A prefetch request issued by prefetch finite state machine <b>2074</b> submits a request address to TAGRAM <b>1934</b> to determine whether the fetch packet is stored in level one instruction cache <b>121</b>. The prefetch address is based upon the address of the demand fetch and the prefetch count. Upon a cache hit, program memory controller <b>1930</b> takes no further action. Upon a cache miss, program memory controller <b>1930</b> submits the prefetch address to unified memory controller <b>1940</b> for cache service. In contrast the present invention starts with a level one instruction cache <b>121</b> demand fetch miss. The same service request to unified memory controller <b>1940</b> to service the initial demand fetch miss triggers a prefetch of the other half of the corresponding level two combined instruction/data cache <b>130</b>.</p><p id="p-0163" num="0159">The prefetch of this invention uses a minimum of resources of the digital data processing system. This prefetch is begun by a level one instruction cache miss. The hardware for making such a level one instruction cache hit/miss determination is needed for normal operation. This invention does not require additional hardware or processes of the program memory controller. This invention does no level one instruction cache tag compare on the prefetch address (lower half of the level two unified cache line), assuming this is a miss. This prefetch requires a level two cache hit. The hardware for making such a level two cache hit/miss determination is needed for normal operation. This invention does not require additional hardware or processes of the unified memory controller. Because level two cache lines are twice the size of level one instruction cache lines, the unified memory controller must include some technique to select the upper half line or the lower half line of the level two cache for supply to the level one instruction cache. This invention uses this technique to support prefetch. Rather than being discarded, the lower half level two cache line is supplied to level one instruction cache <b>121</b> the next cycle. Thus this invention performs a prefetch which requires minimal additional resources.</p><p id="p-0164" num="0160">Other prefetch techniques query the tag RAMS, compare tags for hit/miss determination and advance independently through the pipeline for each prefetch request. This incurs additional latency penalties and power consumption. This invention ties the prefetch to the corresponding demand access and neither reads tag RAMS, nor does a tag compare. In this invention the prefetch request inherits the miss determination of the corresponding demand access and uses its individual way information from discrete registers.</p><p id="p-0165" num="0161">This invention has been described in conjunction with a very long instruction word (VLIW) central processing unit core. Those skilled in the art would realize the teachings of this application are equally applicable to a central processing unit core fetching individual instructions that are serviced by a level one instruction cache having a cache line size equal to the length of plural instructions.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An apparatus comprising:<claim-text>a processor configured to issue a demand fetch request for a first program instruction;</claim-text><claim-text>a first cache coupled to the processor; and</claim-text><claim-text>a memory controller configured to:<claim-text>receive the demand fetch;</claim-text><claim-text>in response to the first program instruction not stored in the first cache, determine a hit for the first program instruction in a second cache;</claim-text><claim-text>on a first clock cycle, transmit, the first program instruction, wherein the first program instruction is associated with a first portion of a cache line of the second cache; and</claim-text><claim-text>on a second clock cycle, transmit, a second program instruction, wherein the second program instruction is associated with a second portion of the cache line.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the memory controller is a first memory controller;</claim-text><claim-text>the first memory controller is coupled to a second memory controller;</claim-text><claim-text>the second memory controller is coupled to the processor and the first cache; and</claim-text><claim-text>the second memory controller is configured to receive the first program instruction from the first memory controller and transmit the first program instruction to the processor.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first portion of the cache line is an upper half of the cache line; and</claim-text><claim-text>the second portion of the cache line is a lower half of the cache line.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the second portion of the cache line is a prefetch response.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the first cache is configured to store the first program instruction from the second cache.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the demand fetch is a set of instructions directly from the processor.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the memory controller includes a multiplexer coupled to a multiplexer controller.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein:<claim-text>the memory controller is a first memory controller;</claim-text><claim-text>the first memory controller is coupled to a second memory controller; and</claim-text><claim-text>the multiplexer controller is coupled to the second memory controller.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>the multiplexer is coupled a register storing the cache line.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The apparatus of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein:<claim-text>the multiplexer is configured to transmit, based on a multiplexer controller selection, the first program instruction or the second program instruction to the second memory controller.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A method comprising:<claim-text>issuing, by a processor, a demand fetch request for a first program instruction;</claim-text><claim-text>receiving, by a memory controller, the demand fetch;</claim-text><claim-text>determining, by the memory controller, a hit for the first program instruction in a second cache in response to the first program instruction not stored in the first cache;</claim-text><claim-text>transmitting on a first clock cycle, by the memory controller, the first program instruction, wherein the first program instruction is associated with a first portion of a cache line of the second cache; and</claim-text><claim-text>transmitting on a second clock cycle, by the memory controller, a second program instruction, wherein the second program instruction is associated with a second portion of the cache line.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the memory controller is a first memory controller;</claim-text><claim-text>the first memory controller is coupled to a second memory controller;</claim-text><claim-text>the second memory controller is coupled to the processor and the first cache; and</claim-text><claim-text>the second memory controller is configured to receive the first program instruction from the first memory controller and transmit the first program instruction to the processor.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the first portion of the cache line is an upper half of the cache line; and</claim-text><claim-text>the second portion of the cache line is a lower half of the cache line.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the second portion of the cache line is a prefetch response.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the first cache is configured to store the first program instruction from the second cache.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the demand fetch is a set of instructions directly from the processor.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein:<claim-text>the memory controller includes a multiplexer coupled to a multiplexer controller.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein:<claim-text>the memory controller is a first memory controller;</claim-text><claim-text>the first memory controller is coupled to a second memory controller; and</claim-text><claim-text>the multiplexer controller is coupled to the second memory controller.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein:<claim-text>the multiplexer is coupled a register storing the cache line.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein:<claim-text>the multiplexer is configured to transmit, based on a multiplexer controller selection, the first program instruction or the second program instruction to the second memory controller.</claim-text></claim-text></claim></claims></us-patent-application>