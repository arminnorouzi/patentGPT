<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004714A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004714</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17851096</doc-number><date>20220628</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202110747494.X</doc-number><date>20210701</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>279</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>279</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND DEVICE FOR PRESENTING PROMPT INFORMATION, AND SOTRAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>FUJITSU LIMITED</orgname><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>CAO</last-name><first-name>Yiling</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ZHENG</last-name><first-name>Zhongguang</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>FANG</last-name><first-name>Lu</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>SUN</last-name><first-name>Jun</first-name><address><city>Beijing</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>FUJITSU LIMITED</orgname><role>03</role><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method of presenting prompt information includes: generating a mask vector for an entity, which is used to identify a position of the entity in a statement composed of the entity and context; generating a first vector and a second vector based on the entity and the context; generating a third vector based on the mask vector and the second vector; concatenating the first vector and the third vector to generate a fourth vector; predicting which concept of multiple predefined concepts the entity corresponds to, based on the fourth vector by a first classifier; predicting which type of multiple predefined types the entity corresponds to, based on the fourth vector by a second classifier; jointly training the first the second classifiers; determining a concept to which the entity corresponds based on prediction result of the trained first classifier; and generating the prompt information based on the determined concept.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="97.11mm" wi="98.13mm" file="US20230004714A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="191.77mm" wi="157.56mm" file="US20230004714A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="110.15mm" wi="126.83mm" file="US20230004714A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="210.48mm" wi="133.27mm" file="US20230004714A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="116.76mm" wi="170.52mm" file="US20230004714A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application claims priority from Chinese Patent Application No. 202110747494.X, filed on Jul. 1, 2021, the contents of which are incorporated by reference herein in its entirety.</p><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure generally relates to entity disambiguation technology, and more particularly, to a method and device for presenting prompt information regarding an entity in text to a user by utilizing the entity disambiguation technology, and a storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">In a practical language environment, there is often a situation where an entity name corresponds to multiple concepts. For example, an entity name &#x201c;apple&#x201d; in text may refer to a kind of fruit or the Apple Inc. In order to solve such problem of ambiguity caused by the same entity name, entity disambiguation technology has been proposed and developed.</p><p id="p-0005" num="0004">The entity disambiguation technology can link an entity mention (entity) in text to an appropriate concept in a knowledge graph, which plays a fundamental role in many domains such as question answering, semantic search, and information extraction. A concept indicates a matter or thing that is distinguishable and exists independently. The knowledge graph contains a large number of concepts, and different concepts may be related to one another. For example, in reality, there are two people with similar names, i.e., Michael Jeffrey Jordan who is a basketball star, and Michael Owen Jordan who is a professional in machine learning field. Accordingly, the knowledge graph may contain two concepts: &#x201c;a basketball star Michael Jeffrey Jordan&#x201d; and &#x201c;a machine-learning expert Michael Owen Jordan&#x201d;. In addition, there may be a number of sport concepts associated with &#x201c;a basketball star Michael Jeffrey Jordan&#x201d; and a number of computer science concepts associated with &#x201c;a machine-learning expert Michael Owen Jordan&#x201d; in the knowledge graph. When an entity &#x201c;Michael Jordan&#x201d; appears in text, it is required to determine, based on a context, whether the entity refers to &#x201c;a basketball star Michael Jeffrey Jordan&#x201d; or &#x201c;a machine-learning expert Michael Owen Jordan&#x201d; in the knowledge graph.</p><p id="p-0006" num="0005">When it is determined, with the entity disambiguation technology, that the entity appearing in the text corresponds to a specific concept, prompt information based on the determined concept may be presented to a user who is reading the text, so that the user can immediately understand the entity correctly. For example, with regard to an entity &#x201c;Apple&#x201d; appearing in the text &#x201c;Apple released a new mobile phone today . . . &#x201d; a prompt &#x201c;Apple Inc.: an American high-tech company whose products include iPhones . . . &#x201d; may be provided to the user.</p><p id="p-0007" num="0006">However, due to high complexity of natural language, the entity disambiguation technology faces a challenge of how to identify the correct meaning of an entity in a context and associate the entity with the correct concept in the knowledge graph.</p><p id="p-0008" num="0007">At present, there are mainly two entity disambiguation methods. One is to model an entity disambiguation problem using a ranking model, and the other is to model an entity disambiguation problem using a classification model.</p><p id="p-0009" num="0008">The method using the ranking model includes a step of generating candidate concepts and a step of sorting the candidate concepts. Simple rules are generally used in the step of generating candidate concepts. However, this often leads to failure in determining proper candidate concepts, and in turn generates cascading errors in the step of sorting.</p><p id="p-0010" num="0009">The method using the classification model is to model the entity disambiguation problem as a single-label text classification task. For example, a model is described in &#x201c;Medical concept normalization in social media posts with recurrent neural networks&#x201d;, Elena Tutubalina et al., Journal of Biomedical informatics, June 2018, which model consists of a neural network part and an auxiliary feature part. In the neural network part, a Gated Recurrent Unit (GRU) network and an attention mechanism network are used to encode entities. In the auxiliary feature part, the model is enhanced with TF-IDF similarity and Word2Vec similarity.</p><p id="p-0011" num="0010">However, the conventional entity disambiguation methods have the following problems:</p><p id="p-0012" num="0011">the Word2Vec similarity features do not have sufficient semantic information, and therefore it is difficult to correctly determine a semantic-level similarity between an entity and a concept;</p><p id="p-0013" num="0012">information of context of the entity is not used effectively;</p><p id="p-0014" num="0013">an entity usually belongs to a category (such as disease, crop. etc.), the conventional methods however do not utilize category information of the entity.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0015" num="0014">In order to solve one or more problems in the conventional methods, a new method for entity disambiguation using a classification model is provided in the present disclosure, which adopts an advanced BERT model and utilizes context information of an entity, and additionally utilizes information about a category to which the entity belongs through multi-task learning. Therefore, meaning of the entity can be identified more accurately.</p><p id="p-0016" num="0015">According to an aspect of the present disclosure, a computer-implemented method of presenting prompt information to a user who is viewing an electronic text by utilizing a neural network is provided. The electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters. The method includes: generating a mask vector for the entity, wherein the mask vector is used to identify a position of the entity in a statement composed of the entity and the context; generating a first vector and a second vector based on the entity and the context by a BERT layer in the neural network; generating a third vector based on the mask vector and the second vector by an entity average layer in the neural network; concatenating the first vector and the third vector by a concatenation layer in the neural network to generate a fourth vector; predicting which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector by a first classifier in the neural network; predicting which type of a plurality of predefined types the entity corresponds to, based on the fourth vector by a second classifier in the neural network; jointly training the first classifier and the second classifier; and determining a concept to which the entity corresponds based on a prediction result of the trained first classifier, and generating the prompt information for presentation to the user based on the determined concept.</p><p id="p-0017" num="0016">According to another aspect of the present disclosure, a device for presenting prompt information to a user who is viewing an electronic text by utilizing a neural network is provided. The electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters. The device includes a memory one or more processors. The memory stores a computer program. The processors are configured to execute the computer program to perform operations of: generating a mask vector for the entity, wherein the mask vector is used to identify a position of the entity in a statement composed of the entity and the context; generating a first vector and a second vector based on the entity and the context by a BERT layer in the neural network; generating a third vector based on the mask vector and the second vector by an entity average layer in the neural network; concatenating the first vector and the third vector by a concatenation layer in the neural network to generate a fourth vector; predicting which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector by a first classifier in the neural network; predicting which type of a plurality of predefined types the entity corresponds to, based on the fourth vector by a second classifier in the neural network; jointly training the first classifier and the second classifier; and determining a concept to which the entity corresponds based on a prediction result of the trained first classifier, and generating the prompt information for presentation to the user based on the determined concept.</p><p id="p-0018" num="0017">According to yet another aspect of the present disclosure, a device for presenting prompt information to a user who is viewing an electronic text by utilizing a neural network is provided. The electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters. The device includes a mask vector generation module, a BERT module, an entity average module, a concatenation module, a concept classification module, a type classification module, and a presentation module. The mask vector generation module is configured to generate a mask vector for the entity, wherein the mask vector is used to identify a position of the entity in a statement composed of the entity and the context. The BERT module is configured to generate a first vector and a second vector based on the entity and the context. The entity average module is configured to generate a third vector based on the mask vector and the second vector. The concatenation module is configured to concatenate the first vector and the third vector to generate a fourth vector. The concept classification module is configured to predict which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector. The type classification module is configured to predict which type of a plurality of predefined types the entity corresponds to, based on the fourth vector. The concept classification module and the type classification module are jointly trained. The presentation module is configured to determine a concept to which the entity corresponds based on a prediction result of the trained concept classification module, and generate the prompt information for presentation to the user based on the determined concept.</p><p id="p-0019" num="0018">According to yet another aspect of the present disclosure, a storage medium storing a computer program is provided. The computer program, when executed by a computer, causes the computer to perform the method as described above.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically shows architecture of a neural network according to the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>2</b></figref> schematically shows an entity, a context, and a mask vector.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a module diagram of a device for presenting prompt information about an entity to a user according to the present disclosure.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flowchart of a method for presenting prompt information about an entity to a user according to the present disclosure.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram showing an exemplary configuration of computer hardware for implementing the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b></figref> schematically shows architecture of a neural network according to the present disclosure. The neural network includes an embedding layer <b>110</b>, a BERT layer <b>120</b>, an entity average layer <b>130</b>, a concatenation layer <b>140</b>, a main classifier <b>150</b>, and an auxiliary classifier <b>160</b>. An entity, context of the entity, and a mask vector for the entity are inputted to the neural network. These layers will be described in detail below.</p><p id="p-0026" num="0025">The entity and the context of the entity are inputted to the embedding layer <b>110</b>. <figref idref="DRAWINGS">FIG. <b>2</b></figref> shows formats of the entity and the context. As shown in the first row in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, &#x201c;M<b>1</b> M<b>2</b> . . . &#x201d; represents a character string of an entity, &#x201c;L<b>1</b> L<b>2</b> . . . &#x201d; represents a character string before the entity and &#x201c;R<b>1</b> R<b>2</b> . . . &#x201d; represents a character string following the entity. A marker character &#x201c;$&#x201d; is added between the first character of the entity and the preceding text, and a marker character &#x201c;$&#x201d; is added between the last character of the entity and the following text. Another character other than &#x201c;$&#x201d; may be used as the marker character, which is not limited in the present disclosure.</p><p id="p-0027" num="0026">The One-Hot encoding of the entity and the context shown in the first row in <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be represented as a two-dimensional vector [batch_size, sequence_length], where batch_size represents the number of input documents, and sequence_length represents a length (number of characters) of a statement composed of the entity and the context. The two-dimensional vector of the entity and the context is inputted to the embedding layer <b>110</b> to be converted into a non-sparse vector V<sub>0 </sub>[batch_size, sequence_length, embedding_dim], where embedding_dim represents a dimension of embedding. The vector V<sub>0 </sub>is inputted to the BERT layer <b>120</b>.</p><p id="p-0028" num="0027">BERT is an acronym for &#x201c;Bidirectional Encoder Representation from Transformers&#x201d;. The BERT layer can generate a semantic representation which contains rich semantic information for the text. In particular, the BERT layer can generate, for each word in the text, a vector representation which corresponds to the word and incorporates semantic information of the full text. For a specific task of natural language processing, the semantic representation produced by a pre-trained BERT layer may be fine-tuned to adapt to the specific task.</p><p id="p-0029" num="0028">In the present disclosure, the BERT layer <b>120</b> generates a first vector V<sub>1 </sub>and a second vector V<sub>2 </sub>based on the input vector V<sub>0</sub>. The first vector V<sub>1 </sub>represents overall semantic information of the statement composed of the entity and the context, with a dimension of [batch_size, bert_dim]. The second vector V<sub>2 </sub>includes hidden vectors for respective words (the entity is one of the words) in the statement, with a dimension of [batch_size, sequence_length, bert_dim], where bert_dim represents a hidden layer dimension (i.e., output dimension) of the BERT layer <b>120</b>.</p><p id="p-0030" num="0029">The second vector V<b>2</b> generated by the BERT layer <b>120</b> is inputted to the entity averaging layer <b>130</b>. In addition, a mask vector V<sub>m </sub>for the entity is inputted to the entity average layer <b>130</b>. Generation of the mask vector V<sub>m </sub>will be described below with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0031" num="0030">As shown in the first row in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the entity &#x201c;M<b>1</b> M<b>2</b> . . . &#x201d;, the marker character &#x201c;$&#x201d; and the context &#x201c;L<b>1</b> L<b>2</b> . . . &#x201d; and &#x201c;R<b>1</b> R<b>2</b> . . . &#x201d; are mapped to a mask vector V<sub>m </sub>having a fixed length on a character basis, such that in the mask vector V<sub>m</sub>, elements at positions corresponding to the characters of the entity and the mark character &#x201c;$&#x201d; are set to 1, and elements at positions corresponding to the characters of the context are set to 0, as shown in the second row. Therefore, position of the entity in the statement composed of the entity and the context may be identified based on the mask vector V<sub>m</sub>.</p><p id="p-0032" num="0031">In particular, during the mapping process, when a predetermined length of the mask vector V<sub>m</sub>, is greater than a total length of characters of the entity, the marker characters and the context, that is, when there is a position in the mask vector V<sub>m </sub>which does not correspond to any one of the entity, the marker characters and the context, an element at such position is set to 0. On the other hand, when the predetermined length of the mask vector V<sub>m </sub>is less than the total length of characters of the entity, the marker characters and the context, the context may be truncated such that the total length is equal to the length of the mask vector V<sub>m</sub>, so as to enable the mapping process.</p><p id="p-0033" num="0032">Referring to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the entity average layer <b>130</b> multiplies the second vector V<sub>2 </sub>and the mask vector V<sub>m </sub>to obtain a hidden vector for the entity. As described above, the second vector V<sub>2 </sub>includes hidden vectors for respective words (the entity is one of the words) in the statement composed of the entity and the context, and the mask vector V<sub>m </sub>may be used to identify the position of the entity in the statement. Therefore, the hidden vector corresponding to the entity may be extracted by multiplying the second vector V<sub>2 </sub>and the mask vector V<sub>m</sub>.</p><p id="p-0034" num="0033">The hidden vector for the entity includes hidden vectors for respective characters in the entity. The entity average layer <b>130</b> further calculates an average vector of the hidden vectors for respective characters on a character dimension, to obtain a third vector V<sub>3</sub>. The third vector V<sub>3 </sub>is inputted to the concatenation layer <b>140</b>. A dimension of vector inputted to the entity average layer <b>130</b> is [batch_size, sequence_length, bert_dim], and a dimension of the output vector V<sub>3 </sub>is [batch_size, bert_dim].</p><p id="p-0035" num="0034">The concatenation layer <b>140</b> receives the first vector V<sub>1 </sub>from the BERT layer <b>120</b> and the third vector V<sub>3 </sub>from the entity average layer <b>130</b>. Since the first vector V<sub>1 </sub>and the third vector V<sub>3 </sub>each have the dimension of [batch_size, bert_dim], the concatenation layer <b>140</b> may concatenate the first vector V<sub>1 </sub>and the third vector V<sub>3 </sub>to generate a fourth vector V<sub>4</sub>. The fourth vector V<sub>4 </sub>has a dimension of [batch_size, bert_dim*2], and is inputted to the main classifier <b>150</b> and the auxiliary classifier <b>160</b>.</p><p id="p-0036" num="0035">The main classifier <b>150</b> predicts which concept in the knowledge graph the entity corresponds to, based on the fourth vector V<sub>4</sub>, and then outputs an identifier (ID) of the predicted concept. A dimension of the output concept ID is [batch_size, class_dim], where class_dim represents the number of concept IDs.</p><p id="p-0037" num="0036">The auxiliary classifier <b>160</b> predicts which type of a plurality of predetermined types the entity belongs to, based on the fourth vector V<b>4</b>, and then outputs an ID of the predicted type. A dimension of the output type ID is [batch_size, type_dim], where type_dim represents the number of type IDs.</p><p id="p-0038" num="0037">Similar to pre-establishing a knowledge graph containing a large number of concepts, a number of types may be defined in advance, such as &#x201c;drug side effects&#x201d;, &#x201c;chemical raw materials&#x201d;, and the like. A type may cover a wider range than a concept. For example, an entity &#x201c;Yao Ming&#x201d; may correspond to a concept of &#x201c;a basketball player Yao Ming&#x201d; and a type of &#x201c;athlete&#x201d;.</p><p id="p-0039" num="0038">In the present disclosure, the main classifier <b>150</b> and the auxiliary classifier <b>160</b> may be jointly trained. A training process will be described in detail below.</p><p id="p-0040" num="0039">In the training process, a training dataset is inputted to the neural network as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The training dataset includes text (including entity and context of the entity), a mask vector corresponding to the text, a plurality of predefined concepts, and a plurality of predefined types. The neural network is trained based on ground truth and prediction results of the main classifier <b>150</b> and the auxiliary classifier <b>160</b> by using a loss function, to obtain optimal parameters of the neural network. During the training process, initial parameters of the BERT layer <b>120</b> may be set as parameters of the BERT pre-trained model.</p><p id="p-0041" num="0040">In addition, a first loss function F<b>1</b> is established for the main classifier <b>150</b>, and a second loss function F<b>2</b> is established for the auxiliary classifier <b>160</b>. A weighted sum of the first loss function and the second loss function is calculated to obtain a final loss function F. The main classifier <b>150</b> and the auxiliary classifier <b>160</b> are jointly trained based on the loss function F. In a preferred embodiment, both the first loss function F<b>1</b> and the second loss function F<b>2</b> are cross-entropy loss functions. Furthermore, since the classification task of predicting concepts is the main task in the present disclosure, it is preferable to apply a greater weight to the first loss function F<b>1</b> than the second loss function F<b>2</b> in calculating the weighted sum.</p><p id="p-0042" num="0041">During the training process, the task of the auxiliary classifier <b>160</b> is associated to the task of the main classifier <b>150</b>. The prediction result of the auxiliary classifier <b>160</b> is a certain type which has a coarser granularity, while the prediction result of the main classifier <b>150</b> is a certain concept which has a finer granularity. There is a hierarchical relationship between the prediction results of the two classifiers. For example, for the entity &#x201c;Yao Ming&#x201d;, the auxiliary classifier <b>160</b> may output the type of &#x201c;athlete&#x201d;, and the main classifier <b>150</b> may output the concept of &#x201c;a basketball player Yao Ming&#x201d;. Since jointly learning on multiple tasks may achieve better effects than learning on a single task, provision of the auxiliary classifier <b>160</b> may help the main classifier <b>150</b> to improve the learning effect.</p><p id="p-0043" num="0042">When the training is completed, it is possible to, in practical applications, use the main classifier <b>150</b> to predict a concept to which an entity in the text corresponds, and then generate prompt information for a user based on the concept, and thereby help the user who is reading the text to understand the entity correctly. The prompt information may be provided in various manners (for example, visually or audibly). For example, when the user is viewing a document, the meaning of the entity may be presented to the user by a hyperlink or a pop-up window, or in an audible way.</p><p id="p-0044" num="0043">In particular, there may be a situation in the practical applications where an entity does not correspond to any one of concepts in the knowledge graph, that is, none of the concepts is suitable to explain the entity. In this case, a threshold may be set. For each of the concepts in the knowledge graph, the trained main classifier <b>150</b> may output a probability that the entity corresponds to the concept. When the largest probability among predicted probabilities is greater than the threshold, it is determined that the concept associated with the largest probability corresponds to the entity, and then the prompt information for the user may be generated based on the determined concept. On the other hand, if all the predicted probabilities are less than the threshold, it means that the entity is not suitable to be classified into any of the concepts. In this case, no prompt information is to be generated for the entity.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>3</b></figref> schematically shows a module diagram of a device for presenting prompt information to a user according to the present disclosure, and <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows a flowchart of a method for presenting prompt information to a user according to the present disclosure.</p><p id="p-0046" num="0045">Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the device according to the present disclosure includes a mask vector generation module <b>310</b>, an embedding module <b>320</b>, a BERT module <b>330</b>, an entity average module <b>340</b>, a concatenation module <b>350</b>, a concept classification module <b>360</b>, a type classification module <b>370</b>, and a presentation module <b>380</b>.</p><p id="p-0047" num="0046">In step S<b>410</b>, a mask vector V<sub>m </sub>(as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) for identifying a position of an entity is generated by the mask vector generation module <b>310</b> based on the entity and context of the entity. Then, the mask vector V<sub>m </sub>is inputted to the entity average module <b>340</b>. Furthermore, the entity and the context are inputted to the embedding module <b>320</b> to be converted into a non-sparse vector V<sub>0</sub>.</p><p id="p-0048" num="0047">In step S<b>420</b>, a first vector V<sub>1 </sub>and a second vector V<sub>2 </sub>are generated by the BERT module <b>330</b> based on the vector V<sub>0 </sub>received from the embedding module <b>320</b>. The first vector V<sub>1 </sub>represents overall semantic information of a statement composed of the entity and the context, and the second vector V<sub>2 </sub>includes hidden vectors for respective words in the statement.</p><p id="p-0049" num="0048">In step S<b>430</b>, a third vector V<sub>3 </sub>is generated by the entity average module <b>340</b> based on the mask vector V<sub>m </sub>generated by the mask vector generation module <b>310</b> and the second vector V<sub>2 </sub>generated by the BERT module <b>330</b>. Specifically, the entity average module <b>340</b> multiplies the second vector V<sub>2 </sub>and the mask vector V<sub>m </sub>to obtain a hidden vector for the entity, and the hidden vector for the entity includes hidden vectors for respective characters in the entity. Then, the entity average module <b>340</b> calculates an average vector of the hidden vectors for respective characters, as the third vector V<sub>3</sub>.</p><p id="p-0050" num="0049">In step S<b>440</b>, the first vector V<sub>1 </sub>generated by the BERT module <b>330</b> and the third vector V<sub>3 </sub>generated by the entity average module <b>340</b> are concatenated by the concatenation module <b>350</b> to generate a fourth vector V<sub>4</sub>.</p><p id="p-0051" num="0050">In step S<b>450</b>, the concept classification module <b>360</b> performs the concept classification task based on the fourth vector V<sub>4 </sub>generated by the concatenation module <b>350</b>. For each of a number of predefined concepts, the concept classification module <b>360</b> outputs a probability that the entity corresponds to the concept. It is possible to predict which concept the entity corresponds to, based on the output probabilities.</p><p id="p-0052" num="0051">In step S<b>460</b>, the type classification module <b>370</b> performs the type classification task based on the fourth vector V<sub>4 </sub>generated by the concatenation module <b>350</b>. For each of a number of predefined types, the type classification module <b>370</b> outputs a probability that the entity corresponds to the type. It is possible to predict which type the entity corresponds to, based on the output probabilities.</p><p id="p-0053" num="0052">In step S<b>470</b>, the concept classification module <b>360</b> and the type classification module <b>370</b> are jointly trained based on the loss function F. Specifically, the loss function F is a weighted sum of a first loss function F<sub>1 </sub>for the concept classification module <b>360</b> and a second loss function F<sub>2 </sub>for the type classification module <b>370</b>. In a preferred example, a weight applied to the first loss function F<sub>1 </sub>may be greater than a weight applied to the second loss function F<sub>2</sub>.</p><p id="p-0054" num="0053">In step S<b>480</b>, a concept to which the entity corresponds is predicted by using the trained concept classification module <b>360</b>. Prompt information for presentation to a user is generated by the presentation module <b>380</b> based on the predicted concept, to help the user understand the entity correctly. The presentation module <b>380</b> may present the prompt information in the visual and/or audible manner.</p><p id="p-0055" num="0054">It should be noted that the method according to the present disclosure does not have to be performed in the order shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, but may be performed in a different order as long as it is technically possible.</p><p id="p-0056" num="0055">The neural network model and the method of the present disclosure have been described in detail with reference to the embodiments. The present disclosure provides a new entity disambiguation scheme, in which the advanced BERT model is adopted, context information of an entity is utilized, and information on a type to which the entity belongs is effectively utilized through multi-task learning. As such, performance of the neural network model may be improved, a possibility of correctly identifying the meaning of the entity is increased, and more accurate prompt information may be provided to the user.</p><p id="p-0057" num="0056">The method described in the embodiments may be implemented by software, hardware, or a combination of software and hardware. Programs included in the software may be stored in advance in a storage medium provided inside or outside the device. As an example, during execution, these programs are written to a random access memory (RAM) and executed by a processor (such as a CPU), implementing the methods and processes described herein.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a block diagram showing an exemplary configuration of computer hardware for implementing the method of the present disclosure based on a program. The computer hardware is an example of the device for presenting prompt information according to the present disclosure.</p><p id="p-0059" num="0058">As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in a computer <b>500</b>, a central processing unit (CPU) <b>501</b>, a read only memory (ROM) <b>502</b>, and a random access memory (RAM) <b>503</b> are connected to each other via a bus <b>504</b>.</p><p id="p-0060" num="0059">An input/output interface <b>505</b> is further connected to the bus <b>504</b>. The input/output interface <b>505</b> is connected with the following components: an input unit <b>506</b> configured with a keyboard, a mouse, a microphone, and the like; an output unit <b>507</b> configured with a display, a speaker, and the like; a. storage unit <b>508</b> configured with a hard disk, a non-volatile memory, and the like; a communication unit <b>509</b> formed by a network interface card, such as a local area network (LAN) card, a modem, and the like; and a drive <b>510</b> that drives a removable medium <b>511</b>, where the removable medium <b>511</b> is a magnetic disk, an optical disk, a magneto-optical disk, or a semiconductor memory, for example.</p><p id="p-0061" num="0060">In the computer having such structure, the CPU <b>501</b> loads a program stored in the storage unit <b>508</b> into the RAM <b>503</b> via the input/output interface <b>505</b> and the bus <b>504</b>, and executes the program so as to execute the above-described method.</p><p id="p-0062" num="0061">The program to be executed by the computer (CPU <b>501</b>) may be recorded on a removable medium <b>511</b>. The removable medium <b>511</b> serves as a package medium, which is formed by, for example, a magnetic disk (including a floppy disk), an optical disk (including a compact disk-read only memory (CD)-ROM)), a digital versatile disc (DVD), a magneto-optical disc, or a semiconductor memory. Furthermore, the program to be executed by the computer (CPU <b>501</b>) can be provided via a wired or wireless transmission medium such as a local area network, the Internet, or digital satellite broadcasting.</p><p id="p-0063" num="0062">When the removable medium <b>511</b> is installed in the drive <b>510</b>, the program may be installed in the storage unit <b>508</b> via the input/output interface <b>505</b>. In addition, the program may be received by the communication unit <b>509</b> via a wired or wireless transmission medium and may be installed in the storage unit <b>508</b>. Alternatively, the program may be pre-installed in the ROM <b>502</b> or the storage unit <b>508</b>.</p><p id="p-0064" num="0063">The program executed by the computer may be a program that performs processes according to the order described in this specification, or may be a program that performs processes in parallel or when necessary (for example, when invoked).</p><p id="p-0065" num="0064">The units or means described herein are only in a logical sense and do not strictly correspond to physical devices or physical entities. For example, functions of a unit described herein may be implemented by multiple physical entities, or functions of multiple units described herein may he implemented by a single physical entity. Furthermore, features, components, elements, steps, and the like, described in an embodiment are not limited to that embodiment, but may also be applied to other embodiments, for example, to replace certain features, components, elements, steps, and the like, in the other embodiments, or to combine with them.</p><p id="p-0066" num="0065">The scope of the present disclosure is not limited to the specific embodiments described herein. It should be understood by those of ordinary skill in the art that, depending upon design requirements and other factors, various modifications or variations of the embodiments herein may be made without departing from the principles and spirit of the disclosure. The scope of the present disclosure is defined by the appended claims and their equivalents.</p><heading id="h-0007" level="1">APPENDIX</heading><p id="p-0067" num="0066">(1) A computer-implemented method of presenting prompt information to a user who is viewing an electronic text by utilizing a neural network, wherein the electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters,</p><p id="p-0068" num="0067">the method comprising:</p><p id="p-0069" num="0068">generating a mask vector (V<sub>m</sub>) for the entity, the mask vector being used to identify a position of the entity in a statement composed of the entity and the context;</p><p id="p-0070" num="0069">generating a first vector (V<sub>1</sub>) and a second vector (V<sub>2</sub>) based on the entity and the context, by a BERT layer in the neural network;</p><p id="p-0071" num="0070">generating a third vector (V<sub>3</sub>) based on the mask vector (V<sub>m</sub>) and the second vector (V<sub>2</sub>), by an entity average layer in the neural network;</p><p id="p-0072" num="0071">concatenating the first vector (V<sub>1</sub>) and the third vector (V<sub>3</sub>) by a concatenation layer in the neural network to generate a fourth vector (V<sub>4</sub>);</p><p id="p-0073" num="0072">predicting which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a first classifier in the neural network;</p><p id="p-0074" num="0073">predicting which type of a plurality of predefined types the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a second classifier in the neural network;</p><p id="p-0075" num="0074">jointly training the first classifier and the second classifier; and</p><p id="p-0076" num="0075">determining a concept to which the entity corresponds based on a prediction result of the trained first classifier, and generating the prompt information for presentation to the user based on the determined concept.</p><p id="p-0077" num="0076">(2) The method according to (1), wherein generating the mask vector (V<sub>m</sub>) for the entity further comprises:</p><p id="p-0078" num="0077">adding a marker character between a first character of the entity and the preceding text, and adding the marker character between a last character of the entity and the following text;</p><p id="p-0079" num="0078">mapping the entity, the marker characters and the context to a vector with a fixed length on a character basis, to generate the mask vector (V<sub>m</sub>),</p><p id="p-0080" num="0079">wherein in the mask vector (V<sub>m</sub>), elements at positions corresponding to the characters of the entity and the marker characters are set to 1, and elements at positions corresponding to the characters of the context are set to 0.</p><p id="p-0081" num="0080">(3) The method according to (2), wherein when the length of the vector is greater than a total length of characters of the entity, the marker characters and the context, elements at positions not corresponding to the entity, the marker characters and the context are set to 0, and</p><p id="p-0082" num="0081">wherein when the length of the vector is less than the total length of characters of the entity, the marker characters and the context, the context is truncated such that the total length is equal to the length of the vector.</p><p id="p-0083" num="0082">(4) The method according to (1). wherein the first vector (V<sub>1</sub>) represents overall semantic information of the statement composed of the entity and the context, and the second vector (V<sub>2</sub>) includes hidden vectors for respective words in the statement.</p><p id="p-0084" num="0083">(5) The method according to (4), wherein generating the third vector (V<sub>3</sub>) further comprises:</p><p id="p-0085" num="0084">multiplying the second vector (V<sub>2</sub>) and the mask vector (V<sub>m</sub>) to obtain a hidden vector for the entity, wherein the hidden vector for the entity includes hidden vectors for respective characters in the entity;</p><p id="p-0086" num="0085">calculating, on the character dimension, an average vector of the hidden vectors for the respective characters in the entity, as the third vector (V<sub>3</sub>).</p><p id="p-0087" num="0086">(6) The method according to (1), wherein jointly training the first classifier and the second classifier further comprises:</p><p id="p-0088" num="0087">establishing a first loss function for the first classifier and a second loss function for the second classifier;</p><p id="p-0089" num="0088">performing a weighted addition to the first loss function and the second loss function, wherein a weight for the first loss function is greater than a weight for the second loss function; and</p><p id="p-0090" num="0089">training the first classifier and the second classifier in such a manner that the resulted loss function after the weighted addition is minimized.</p><p id="p-0091" num="0090">(7) The method according to (6), wherein each of the first loss function and the second loss function is a cross-entropy loss function.</p><p id="p-0092" num="0091">(8) The method according to (1), further comprising:</p><p id="p-0093" num="0092">for each of the plurality of concepts, predicting a probability that the entity corresponds to the concept by the trained first classifier;</p><p id="p-0094" num="0093">if a maximum probability among the predicted probabilities is greater than a predetermined threshold, determining that a concept associated with the maximum probability corresponds to the entity; and</p><p id="p-0095" num="0094">if all the predicted probabilities are less than the predetermined threshold, generating no prompt information for the entity.</p><p id="p-0096" num="0095">(9) The method according to (1), wherein the prompt information is presented to the user in at least one of a visual manner and an auditory manner.</p><p id="p-0097" num="0096">(10) A device for presenting prompt information to a user who is viewing an electronic text by utilizing a neural network, wherein the electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters,</p><p id="p-0098" num="0097">the device comprising:</p><p id="p-0099" num="0098">a memory storing a computer program; and</p><p id="p-0100" num="0099">one or more processors executing the computer program to perform operations of:</p><p id="p-0101" num="0100">generating a mask vector (V<sub>m</sub>) for the entity, the mask vector being used to identify a position of the entity in a statement composed of the entity and the context;</p><p id="p-0102" num="0101">generating a first vector (V<sub>1</sub>) and a second vector (V<sub>2</sub>) based on the entity and the context, by a BERT layer in the neural network;</p><p id="p-0103" num="0102">generating a third vector (V<sub>3</sub>) based on the mask vector (V<sub>m</sub>) and the second vector (V<sub>2</sub>), by an entity average layer in the neural network;</p><p id="p-0104" num="0103">concatenating the first vector (V<sub>1</sub>) and the third vector (V<sub>3</sub>) by a concatenation layer in the neural network to generate a fourth vector (V<sub>4</sub>);</p><p id="p-0105" num="0104">predicting which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a first classifier in the neural network;</p><p id="p-0106" num="0105">predicting which type of a plurality of predefined types the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a second classifier in the neural network;</p><p id="p-0107" num="0106">jointly training the first classifier and the second classifier; and</p><p id="p-0108" num="0107">determining a concept to which the entity corresponds based on a prediction result of the trained first classifier, and generating the prompt information for presentation to the user based on the determined concept.</p><p id="p-0109" num="0108">(11) A storage medium storing a computer program that, when executed by a computer, causes the computer to perform the method of presenting prompt information to a user according to any one of (1) to (9).</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method of presenting prompt information to a user who is viewing an electronic text by utilizing a neural network, wherein the electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters,<claim-text>the method comprising:</claim-text><claim-text>generating a mask vector (V<sub>m</sub>) for the entity, the mask vector being used to identify a position of the entity in a statement composed of the entity and the context;</claim-text><claim-text>generating a first vector (V<sub>1</sub>) and a second vector (V<sub>2</sub>) based on the entity and the context, by a BERT layer in the neural network;</claim-text><claim-text>generating a third vector (V<sub>3</sub>) based on the mask vector (V<sub>m</sub>) and the second vector (V<sub>2</sub>), by an entity average layer in the neural network;</claim-text><claim-text>concatenating the first vector (V<sub>1</sub>) and the third vector (V<sub>3</sub>) by a concatenation layer in the neural network to generate a fourth vector (V<sub>4</sub>);</claim-text><claim-text>predicting which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a first classifier in the neural network;</claim-text><claim-text>predicting which type of a plurality of predefined types the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a second classifier in the neural network;</claim-text><claim-text>jointly training the first classifier and the second classifier; and</claim-text><claim-text>determining a concept to which the entity corresponds based on a prediction result of the trained first classifier, and generating the prompt information for presentation to the user based on the determined concept.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein generating the mask vector (V<sub>m</sub>) for the entity further comprises:<claim-text>adding a marker character between a first character of the entity and the preceding text, and adding the marker character between a last character of the entity and the following text;</claim-text><claim-text>mapping the entity, the marker characters and the context to a vector with a fixed length on a character basis, to generate the mask vector (V<sub>m</sub>),</claim-text><claim-text>wherein in the mask vector (V<sub>m</sub>), elements at positions corresponding to the characters of the entity and the marker characters are set to 1, and elements at positions corresponding to the characters of the context are set to 0.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein when the length of the vector is greater than a total length of characters of the entity, the marker characters and the context, elements at positions not corresponding to the entity, the marker characters and the context are set to 0, and<claim-text>wherein when the length of the vector is less than the total length of characters of the entity, the marker characters and the context, the context is truncated such that the total length is equal to the length of the vector.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the first vector (V<sub>1</sub>) represents overall semantic information of the statement composed of the entity and the context, and the second vector (V<sub>2</sub>) includes hidden vectors for respective words in the statement.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein generating the third vector (V<sub>3</sub>) further comprises:<claim-text>multiplying the second vector (V<sub>2</sub>) and the mask vector (V<sub>m</sub>) to obtain a hidden vector for the entity, wherein the hidden vector for the entity includes hidden vectors for respective characters in the entity;</claim-text><claim-text>calculating, on the character dimension, an average vector of the hidden vectors for the respective characters in the entity, as the third vector (V<sub>3</sub>).</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein jointly training the first classifier and the second classifier further comprises:<claim-text>establishing a first loss function for the first classifier and a second loss function for the second classifier;</claim-text><claim-text>performing a weighted addition to the first loss function and the second loss function, wherein a weight fix the first loss function is greater than a weight for the second loss function; and</claim-text><claim-text>training the first classifier and the second classifier in such a manner that the resulted loss function after the weighted addition is minimized.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein each of the first loss function and the second loss function is a cross-entropy loss function.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>for each of the plurality of concepts, predicting a probability that the entity corresponds to the concept by the trained first classifier;</claim-text><claim-text>if a maximum probability among the predicted probabilities is greater than a predetermined threshold, determining that a concept associated with the maximum probability corresponds to the entity; and</claim-text><claim-text>if all the predicted probabilities are less than the predetermined threshold, generating no prompt information for the entity.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A device for presenting prompt information to a user who is viewing an electronic text by utilizing a neural network, wherein the electronic text includes an entity and context of the entity, and the entity and the context are each composed of one or more characters,<claim-text>the device comprising:</claim-text><claim-text>a memory storing a computer program; and</claim-text><claim-text>one or more processors executing the computer program to perform operations of:</claim-text><claim-text>generating a mask vector (V<sub>m</sub>) for the entity, the mask vector being used to identify a position of the entity in a statement composed of the entity and the context;</claim-text><claim-text>generating a first vector (V<sub>1</sub>) and a second vector (V<sub>2</sub>) based on the entity and the context, by a BERT layer in the neural network;</claim-text><claim-text>generating a third vector (V<sub>3</sub>) based on the mask vector (V<sub>m</sub>) and the second vector (V<sub>2</sub>), by an entity average layer in the neural network;</claim-text><claim-text>concatenating the first vector (V<sub>1</sub>) and the third vector (V<sub>3</sub>) by a concatenation layer in the neural network to generate a fourth vector (V<sub>4</sub>);</claim-text><claim-text>predicting which concept of a plurality of predefined concepts the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a first classifier in the neural network;</claim-text><claim-text>predicting which type of a plurality of predefined types the entity corresponds to, based on the fourth vector (V<sub>4</sub>), by a second classifier in the neural network;</claim-text><claim-text>jointly training the first classifier and the second classifier; and</claim-text><claim-text>determining a concept to which the entity corresponds based on a prediction result of the trained first classifier, and generating the prompt information for presentation to the user based on the determined concept.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A storage medium storing a computer program that, when executed by a computer, causes the computer to perform the method of presenting prompt information to a user according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim></claims></us-patent-application>