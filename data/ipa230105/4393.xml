<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004394A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004394</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17366244</doc-number><date>20210702</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>38</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3804</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3842</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">THREAD PRIORITIES USING MISPREDICTION RATE AND SPECULATIVE DEPTH</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>International Business Machines Corporation</orgname><address><city>Armonk</city><state>NY</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Eickemeyer</last-name><first-name>Richard J.</first-name><address><city>Rochester</city><state>MN</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Fatehi</last-name><first-name>Ehsan</first-name><address><city>Edinburg</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Griswell, JR.</last-name><first-name>John B.</first-name><address><city>Austin</city><state>TX</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Gorti</last-name><first-name>Naga P.</first-name><address><city>Austin</city><state>TX</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods and systems for determining a priority of a threads is described. A processor can execute branch instructions of the thread. The processor can predict branch instruction outcomes of the branch instructions of the thread. The processor can increment a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread. The processor can determine the priority of the thread based on the misprediction count of the thread.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="100.41mm" wi="139.78mm" file="US20230004394A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="191.77mm" wi="141.82mm" file="US20230004394A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="209.47mm" wi="135.30mm" file="US20230004394A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="193.97mm" wi="114.98mm" file="US20230004394A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="203.79mm" wi="139.45mm" file="US20230004394A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="210.65mm" wi="147.74mm" file="US20230004394A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="199.73mm" wi="147.74mm" file="US20230004394A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="185.42mm" wi="123.61mm" file="US20230004394A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">The present invention relates to executions in a processor and more specifically to determining thread priorities using misprediction rate and speculative depth.</p><p id="p-0003" num="0002">Modern computer systems typically contain several integrated circuits (ICs), including a processor which may be used to process information in the computer system. The data processed by a processor may include computer instructions which are executed by the processor as well as data which is manipulated by the processor using the computer instructions. The computer instructions and data are typically stored in a main memory in the computer system.</p><p id="p-0004" num="0003">Processors typically process instructions by executing the instruction in a series of small steps. In some cases, to increase the number of instructions being processed by the processor (and therefore increase the speed of the processor), the processor may be pipelined. Pipelining refers to providing separate stages in a processor where each stage performs one or more of the small steps necessary to execute an instruction, i.e., several instructions are overlapped in execution. In some cases, the pipeline (in addition to other circuitry) may be placed in a portion of the processor referred to as the processor core. Some processors may have multiple processor cores, and in some cases, each processor core may have multiple pipelines. Where a processor core has multiple pipelines, groups of instructions (referred to as issue groups) may be issued to the multiple pipelines in parallel and executed by each of the pipelines in parallel.</p><p id="p-0005" num="0004">A sequence of instructions can sometimes be referred to as a thread. A processor core, or multiple processor cores, can perform multithreading&#x2014;such as executing multiple threads or instructions streams concurrently. In multithreading, the threads may be independent from one another, and may share various resources of the processor core or the multiple processor cores. Multithreading may be used in conjunction with pipelining to increase processing speed. Multithreading can allow instructions from one thread to be processed through a pipeline in response to another thread not being able to be processed for various reasons (e.g., a cache miss resulting in a required data for executing a particular instruction is not immediately available). Thus, the situation where all instructions are held up in response to a particular instruction not being able to be executed can be avoided by executing multithreading. In an example, processors configured to perform multithreading can be referred to as simultaneous multithreading (SMT) processors.</p><heading id="h-0002" level="1">SUMMARY</heading><p id="p-0006" num="0005">The summary of the disclosure is given to aid understanding of the computer processing systems and methods of classifying branch instructions, and not with an intent to limit the disclosure or the invention. The present disclosure is directed to a person of ordinary skill in the art. It should be understood that various aspects and features of the disclosure may advantageously be used separately in some instances, or in combination with other aspects and features of the disclosure in other instances. Accordingly, variations and modifications may be made to the memory systems, architectural structure and method of operation to achieve different effects.</p><p id="p-0007" num="0006">In an example, a processor configured to determine a priority of a thread is generally described. The processor can include a processor pipeline including one or more execution units configured to execute branch instructions of a thread. The processor can further include a branch predictor associated with the processor pipeline and configured to predict branch instruction outcomes of the branch instructions of the thread. The processor can further include a misprediction counter configured to increment a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread. The processor can further include an instruction fetch unit configured to determine a priority of the thread based on the misprediction count of the thread.</p><p id="p-0008" num="0007">In an example, a computer system configured to determine a priority of a thread is generally described. The computing system can include a memory and a processor. The processor can include a processor pipeline including one or more execution units configured to execute branch instructions of a thread. The processor can further include a branch predictor associated with the processor pipeline and configured to predict branch instruction outcomes of the branch instructions of the thread. The processor can further include a misprediction counter configured to increment a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread. The processor can further include an instruction fetch unit configured to determine a priority of the thread based on the misprediction count of the thread.</p><p id="p-0009" num="0008">In an example, a method for determining a priority of a thread is generally described. The method can include executing, by a processor, branch instructions of the thread. The method can further include predicting, by the processor, branch instruction outcomes of the branch instructions of the thread. The method can further include incrementing, by the processor, a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread. The method can further include determining, by the processor, the priority of the thread based on the misprediction count of the thread.</p><p id="p-0010" num="0009">Further features as well as the structure and operation of various embodiments are described in detail below with reference to the accompanying drawings. In the drawings, like reference numbers indicate identical or functionally similar elements.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a general computing or data processing system in accordance with an embodiment.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> a block diagram of a processor in accordance with an embodiment</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example implementation of a misprediction counter in accordance with an embodiment.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example implementation of a depth counter in accordance with an embodiment.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example flowchart example of determining thread priorities using misprediction rate and speculative depth in an embodiment.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example flowchart example of determining thread priorities using misprediction rate and misprediction resolve time in an embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example flowchart describing a method for thread priorities using misprediction rate and speculative depth in an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">The following description is made for illustrating the general principles of the invention and is not meant to limit the inventive concepts claimed herein. In the following detailed description, numerous details are set forth in order to provide an understanding of a processor, its architectural structure, and its method of operation, however, it will be understood by those skilled in the art that different and numerous embodiments of the processor, architectural structure, and method of operation may be practiced without those specific details, and the claims and invention should not be limited to the embodiments, subassemblies, features, processes, methods, aspects, features or details specifically described and shown herein. Further, particular features described herein can be used in combination with other described features in each of the various possible combinations and permutations.</p><p id="p-0019" num="0018">Unless otherwise specifically defined herein, all terms are to be given their broadest possible interpretation including meanings implied from the specification as well as meanings understood by those skilled in the art and/or as defined in dictionaries, treatises, etc.</p><p id="p-0020" num="0019">The term &#x201c;workload&#x201d; of a processor refers to the number of instructions being executed by the processor during a given period or at a particular instant of time.</p><p id="p-0021" num="0020">A computing or data processing system <b>100</b> suitable for storing and/or executing program code may take many forms and in one embodiment may include at least one processor <b>102</b>, which may be or be part of a controller, coupled directly or indirectly to memory devices or elements through a system bus, as shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Computing system <b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is shown with a processor <b>102</b>, Random Access Memory (RAM) <b>103</b>, nonvolatile memory <b>104</b>, device specific circuits <b>101</b>, and I/O interface <b>105</b>. Alternatively, the RAM <b>103</b> and/or nonvolatile memory <b>104</b> may be contained in the processor <b>102</b> as could the device specific circuits <b>101</b> and I/O interface <b>105</b>. The processor <b>102</b> may comprise, for example, an off-the-shelf microprocessor, custom processor, Field Programmable Gate Array (FPGA), Application Specific Integrated Circuit (ASIC), discrete logic, etc., or generally any device for executing instructions. The RAM <b>103</b> is typically used to hold variable data, stack data, executable instructions, etc., and may include Dynamic Random Access Memory or DRAM.</p><p id="p-0022" num="0021">According to various approaches, the nonvolatile memory <b>104</b> may comprise any type of nonvolatile memory such as, but not limited to, Electrically Erasable Programmable Read Only Memory (EEPROM), flash Programmable Read Only Memory (PROM), battery backup RAM, hard disk drives, etc. The nonvolatile memory <b>104</b> is typically used to hold the executable firmware and any nonvolatile data containing programming instructions that can be executed to cause the processor <b>102</b> to perform certain functions.</p><p id="p-0023" num="0022">In some embodiments, the I/O interface <b>105</b> may include a communication interface that allows the processor <b>102</b> to communicate with devices external to the controller. Examples of the communication interface may comprise, but are not limited to, serial interfaces such as RS-232, USB (Universal Serial Bus), Small Computer Systems Interface (SCSI), RS-422 or a wireless communication interface such as Wi-Fi, Bluetooth, near-field communication (NFC) or other wireless interfaces. The computing system <b>100</b> may communicate with an external device via the communication interface <b>105</b> in any communication protocol such as Automation/Drive Interface (ADI).</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a block diagram of a processor <b>102</b> according to an embodiment. The processor <b>102</b> may include at least a memory <b>202</b>, an instruction cache <b>204</b>, an instruction fetch unit <b>206</b>, a branch predictor <b>208</b>, and a processor pipeline or a processing pipeline <b>210</b>. The processor <b>102</b> may be included within a computer processor or otherwise distributed within a computer system. Instructions and data can be stored in memory <b>202</b>, and the instruction cache <b>204</b> may access instructions in memory <b>202</b> and store the instructions to be fetched. The memory <b>202</b> may include any type of volatile or nonvolatile memory, such as cache memory. The memory <b>202</b> and instruction cache <b>204</b> can include multiple cache levels. A data cache (not depicted) may also be included in the processor <b>102</b>. In one embodiment, instruction cache <b>204</b> may be configured to provide instructions in an 8-way set associative structure. Alternatively, any other desired configuration and size may be employed. For example, instruction cache <b>204</b> may be implemented as a fully associative, set associative, or direct mapped configuration.</p><p id="p-0025" num="0024">In <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a simplified example of the instruction fetch unit <b>206</b> and the processing pipeline <b>210</b> are depicted. In various embodiments, the processor <b>102</b> may include multiple processing pipelines <b>210</b> and instruction fetch units <b>206</b>. In an embodiment, the processing pipeline <b>210</b> includes a decode unit <b>20</b>, an issue unit <b>22</b>, an execution unit <b>24</b>, and write-back logic <b>26</b>. In an example, the instruction fetch unit <b>206</b> and/or the branch predictor <b>208</b> may also be part of the processing pipeline <b>210</b>. The processing pipeline <b>210</b> may also include other features, such as error checking and handling logic, reorder buffer, one or more parallel paths through the processing pipeline <b>210</b>, and other features now or hereafter known in the art. While a forward path through the processor <b>102</b> is depicted in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, other feedback and signaling paths may be included between elements of the processor <b>102</b>.</p><p id="p-0026" num="0025">Branch instructions (or &#x201c;branch&#x201d;) can be either unconditional, meaning that the branch is taken every time that the instruction is encountered in the program, or conditional, meaning that the branch is either taken or not taken, depending upon a condition. The processor <b>102</b> can provide conditional branch instructions which allow a computer program to branch from one instruction to a target instruction (thereby skipping intermediate instructions, if any) if a condition is satisfied. If the condition is not satisfied, the next instruction after the branch instruction may be executed without branching to the target instruction. Most often, the instructions to be executed following a conditional branch are not known with certainty until the condition upon which the branch depends has been resolved. The branch predictor <b>208</b> can attempt to predict the outcome of conditional branch instructions in a program before the branch instruction is executed. If a branch is mispredicted, all of the speculative work, beyond the point in the program where the branch is encountered, must be discarded. For example, when a conditional branch instruction is encountered, the processor <b>102</b> may predict which instruction will be executed after the outcome of the branch condition is known. Then, instead of stalling the processing pipeline <b>210</b> when the conditional branch instruction is issued, the processor may continue issuing instructions beginning with the predicted next instruction.</p><p id="p-0027" num="0026">In a conditional branch, control can be transferred to the target address depending upon the results of a previous instruction. Conditional branches may be either resolved or unresolved branches depending on whether the result of the previous instruction is known at the time of the execution of the branch. If the branch is resolved, then it is known whether the branch is to be executed. If the conditional branch is not executed, the next sequential instruction stream immediately following the branch instruction is executed. If the conditional branch is executed, then the instruction stream starting at the target address is executed.</p><p id="p-0028" num="0027">The instruction fetch unit <b>206</b> fetches instructions from the instruction cache <b>204</b> according to an instruction address, for further processing by the decode unit <b>20</b>. The decode unit <b>20</b> decodes instructions and passes the decoded instructions, portions of instructions, or other decoded data to the issue unit <b>22</b>. The decode unit <b>20</b> may also detect branch instructions which were not predicted by branch predictor <b>208</b>. The issue unit <b>22</b> analyzes the instructions or other data and transmits the decoded instructions, portions of instructions, or other data to one or more execution units in the execution unit <b>24</b> based on the analysis. The execution unit <b>24</b> executes the instructions and determines if the predicted branch direction is incorrect. The branch direction may be &#x201c;taken&#x201d;, in which subsequent instructions are fetched from the target address of the branch instruction. Conversely, the branch direction may be &#x201c;not taken&#x201d;, in which subsequent instructions are fetched from memory locations consecutive to the branch instruction. When a mispredicted branch instruction is detected, instructions subsequent to the mispredicted branch can be discarded from the various units of processor <b>102</b>. The execution unit <b>24</b> may include a plurality of execution units, such as fixed-point execution units, floating-point execution units, load/store execution units, and vector multimedia execution units. The execution unit <b>24</b> may also include specialized branch predictors to predict the target of a multi-target branch. The write-back logic <b>26</b> writes results of instruction execution back to a destination resource <b>220</b>. The destination resource <b>220</b> may be any type of resource, including registers, cache memory, other memory, I/O circuitry to communicate with other devices, other processing circuits, or any other type of destination for executed instructions or data. One or more of the processor pipeline units may also provide information regarding the execution of conditional branch instructions to the branch predictor <b>208</b>.</p><p id="p-0029" num="0028">In an embodiment, processor <b>102</b> may perform branch prediction in order to speculatively fetch instructions subsequent to conditional branch instructions. Branch predictor <b>208</b> is included to perform such branch prediction operations. In an embodiment, instruction cache <b>204</b> may provide to the branch predictor <b>208</b> an indication of the instruction address being fetched, so that branch predictor <b>208</b> may determine which branch target addresses to select for forming a branch prediction. The branch predictor <b>208</b> may be coupled to various parts of the processing pipeline <b>210</b>, such as, for example, execution unit <b>24</b>, decode unit <b>20</b>, reorder buffer, etc. to determine if the predicted branch direction is correct or incorrect.</p><p id="p-0030" num="0029">To facilitate multithreading, instructions from different threads can be interleaved in some fashion at some point in the overall processor pipeline. An example technique to interleave instructions from different threads involves interleaving instructions on a cycle-by-cycle basis based on interleaving rules. For example, instructions from the different threads can be interleaved such that a processor can perform an instruction from a first thread in a first clock cycle, and then an instruction from a second thread in a second clock cycle, and subsequently another instruction from the first thread in a third clock cycle and so forth. Some interleaving techniques may involve assigning a priority to each thread and then interleaving instructions from the different threads based on the assigned priorities. For example, if a first thread is assigned to a higher priority than a second thread, an interleaving rule may require that twice as many instructions from the first thread assigned with the higher priority be included in the interleaved stream as compared to instructions from the second thread assigned with the lower priority. Various different interleaving rules can be set, such as rules designed for resolving threads with the same priority, or rules that interleave instructions from relatively less important threads periodically (e.g., performing instruction from a lower priority thread every X cycles).</p><p id="p-0031" num="0030">Thread interleaving based on priorities can allow processor resources to be allotted based on the assigned priorities. However, thread priorities sometimes do not take into account processor events, such as branch mispredictions, that may affect the ability of threads to advance through a processor pipeline. These event can sometimes impact the efficiency of processor resources allotted between different instruction threads in a multi-thread processor. For example, priority based techniques that give higher priority to threads with fewer instructions in the decode, rename, and instruction queue stages of the pipeline sometimes can be inefficient at reducing the number of wrong-path instructions caused by branch mispredictions (e.g., incorrectly speculated instructions) in the pipeline. These wrong-path instructions can tie up the fetch bandwidth and other valuable resources of the processor, such as instruction queues and other functional units.</p><p id="p-0032" num="0031">Efficiency and/or performance of the processor <b>102</b> can be improved by reducing the number of wrong-path instructions in the processing pipeline <b>210</b>. For example, threads with higher rate of mispredictions can be delayed (e.g., fetched slower by the instruction fetch unit) in the processing pipeline <b>210</b>, causing a reduction in the number of wrong-path instructions in the processing pipeline <b>210</b>. Further, a number of instructions following a first unfinished or unresolved branch instruction processing pipeline <b>210</b> can be tracked to prevent an excessive number of potentially wrong-path instructions being performed.</p><p id="p-0033" num="0032">In an embodiment, the processor <b>102</b> can be a SMT processor configured to perform multithreading. The processor <b>102</b> can use one or more instruction queues <b>212</b> to collect instructions from the one or more different threads. The instruction fetch unit <b>206</b> can fetch instructions stored in the instruction cache <b>204</b> and fill the instruction queues <b>212</b> with the fetched instructions. Performance of the processor <b>102</b> can depend on how the instruction fetch unit <b>206</b> fill these instruction queues <b>212</b>. The instruction fetch unit <b>206</b> can be configured to assign and manage priorities of the different threads, and based on these priorities, decide which instructions and/or which threads to fetch and send these fetched instructions to the instruction queues <b>212</b>. The processor <b>102</b> can further include a thread scheduler <b>214</b> configured to schedule and distribute the instructions in the instruction queues <b>212</b> to the processing pipeline <b>210</b>.</p><p id="p-0034" num="0033">In an embodiment, the processor <b>102</b> may include a misprediction counter <b>230</b> and a depth counter <b>240</b>. The misprediction counter <b>230</b> can be configured to maintain or record a misprediction count that represents a number of branch mispredictions of one or more threads. The depth counter <b>240</b> can be configured to maintain a speculative depth count that represents a speculative depth of one or more threads. The speculative depth can be a parameter representing a number of instructions that have not been executed to completion (e.g., that are predicted to be taken or not taken via the branch predictor <b>208</b>, and the real branch path is unknown at the time) following a first unfinished branch in the processing pipeline <b>210</b>. The processor <b>102</b>, or the instruction fetch unit <b>206</b>, can use the misprediction count being outputted from the misprediction counter <b>230</b>, and the speculative depth count being outputted from the depth counter <b>240</b>, to assign and/or modify priorities assigned to one or more threads. In an example, the misprediction counter <b>230</b> and the depth counter <b>240</b> can be implemented as individual modules in the processing pipeline <b>210</b>, or can be implemented by, for example, in one of the stages implemented by the instruction fetch unit <b>206</b>, the branch prediction unit <b>208</b>, the decode stage, or other stages in the processing pipeline <b>210</b>.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example implementation of a misprediction counter in accordance with an embodiment. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, instructions from a plurality of threads, such as threads <b>301</b>, <b>302</b>, and <b>303</b>, can be fetched by the instruction fetch unit <b>206</b> for processing in the processing pipeline <b>210</b>. The instruction fetch unit <b>206</b> can be configured to maintain and manage a priority parameter (&#x201c;priority&#x201d;) of the threads <b>301</b>, <b>302</b>, <b>303</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the priorities of the threads <b>301</b>, <b>302</b>, <b>303</b> are denote as P<b>1</b>, P<b>2</b>, P<b>3</b>, respectively. The instruction fetch unit <b>206</b> can fetch instructions from the threads <b>301</b>, <b>302</b>, <b>303</b> based on the priorities P<b>1</b>, P<b>2</b>, P<b>3</b>, and the fetched instructions can be interleaved into the processing pipeline <b>210</b> (e.g., by the thread scheduler <b>214</b> shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>). In an example, the instruction fetch unit <b>206</b> can prioritize fetching instructions from threads that have higher priority. Although the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref> includes three threads, it will be apparent to a person or ordinary skill in the art that the methods and systems described herein can be applied to any number of threads being execute by a processor.</p><p id="p-0036" num="0035">The branch predictor <b>208</b> can predict branch instruction outcomes of branch instructions among the threads <b>301</b>, <b>302</b>, <b>303</b>. For branch instructions that are fetched for processing in the processing pipeline <b>210</b>, actual execution of these fetched branch instructions can be the same or different from their corresponding branch instruction prediction outcome predicted by the branch predictor <b>208</b>. A correct prediction of a branch instruction by the branch predictor <b>208</b> can result in an actual execution of the branch instruction being the same as a prediction by the branch predictor <b>208</b>. An incorrect prediction, or a misprediction, of a branch instruction by the branch predictor <b>208</b> can result in an actual execution of the branch instruction being different from the prediction by the branch predictor <b>208</b>.</p><p id="p-0037" num="0036">In an embodiment, a variety of suitable branch prediction algorithms may be employed by the branch predictor <b>208</b>. The branch predictor <b>208</b> may include any combination of primary branch prediction structures or circuits, such as a branch target buffer (BTB) or a branch target address cache (BTAC), a branch history table (BHT), one or more pattern history tables (PHT), or the like. The BTB may be set associative and include multiple sets (columns) of BTB entries and each BTB entry may include, for example, a branch address tag and a predicted target address. The BTB may also be direct mapped or fully associative. The BTB may be a global buffer that records the outcome of every branch that executes, or it may be a per-branch buffer that records only the past history of the same branch. The BHT may hold branch history information that indicates whether a branch is predicted as taken or not taken. The BHT can also include prediction strength indicators for BHT direction predictions (e.g., strong not taken, weak not taken, weak taken, and strong taken). The BHT may be indexed based on an instruction address. A PHT may hold prediction strength indicators for direction predictions and tags associated with branch prediction patterns. To predict a branch in existing branch prediction hardware, the branch instruction's address is combined with the current value of the branch history. This can be a global branch history of the last k branch outcomes (such as a PHT) or a table that has a per-branch history, i.e. the last k outcomes of the same branch (such as a BHT). The resulting value is used to index into a predictor table in order to read off the prediction. After the branch actually executes, the outcome of the branch is shifted into the BTB.</p><p id="p-0038" num="0037">The branch predictor <b>208</b> can be configured to detect occurrences of correct and incorrect branch predictions of one or more different threads. In an example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, upon an actual execution of a branch instruction BR of the thread <b>302</b> in the processing pipeline <b>210</b>, a result <b>330</b> of the execution of BR can be provided to the branch predictor <b>208</b>. The branch predictor <b>208</b> can use the result <b>330</b> to determine whether there is an occurrence of a correct or an incorrect branch prediction of BR. In response to the branch predictor <b>208</b> determining an occurrence of an incorrect branch prediction of BR, the branch predictor <b>208</b> may send misprediction data <b>332</b> to the misprediction counter <b>230</b>. The misprediction data <b>332</b> can include, for example, an identifier of the thread <b>302</b> and an indicator that indicates the occurrence of the misprediction or incorrect prediction of BR. In response to the branch predictor <b>208</b> determining an occurrence of a correct branch prediction of BR, the branch predictor <b>208</b> may not need to send the misprediction data <b>332</b> to the misprediction counter <b>230</b>.</p><p id="p-0039" num="0038">The misprediction counter <b>230</b> can maintain a misprediction count for each one of the threads <b>301</b>, <b>302</b>, <b>303</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in response to receiving the misprediction data <b>332</b> indicating an occurrence of misprediction of BR, the misprediction counter <b>230</b> can increment a misprediction count of the thread <b>302</b>. In an embodiment, the misprediction counter <b>230</b> can be further configured to determine a rate of misprediction of the threads <b>301</b>, <b>302</b>, and <b>303</b>. For example, the misprediction counter <b>230</b> can determine a number of misprediction counts per one thousand instructions. The misprediction counter <b>230</b> can compare the rate of misprediction of the threads <b>301</b>, <b>302</b>, and <b>303</b> with a misprediction threshold. In an example, a misprediction threshold can be set to, for example, five misprediction counts per one thousand instructions.</p><p id="p-0040" num="0039">Using the thread <b>302</b> as an example, for every one thousand instructions of the thread <b>302</b> being performed in the processing pipeline <b>210</b>, the misprediction counter <b>230</b> can compare the misprediction count of the thread <b>302</b> with the misprediction threshold. If the misprediction count of the thread <b>302</b> is less than or equal to the misprediction threshold (e.g., less than or equal to five misprediction counts out of the one thousand instructions), the misprediction counter <b>230</b> can notify the instruction fetch unit <b>206</b> to add an amount of priority credit (denoted as C) to the priority P<b>2</b> of the thread <b>302</b>. The addition of C to P<b>2</b> can increase the priority P<b>2</b> of the thread <b>302</b>. Note that the misprediction threshold and the value of C can be arbitrary and can be programmable dependent on a desired implementation of the system <b>100</b>. For example, to reduce a frequency of adjusting the priorities of threads using the misprediction counts, the misprediction threshold can be set to a higher value. By allowing the instruction fetch unit <b>206</b> to adjust priorities of threads using misprediction counts and misprediction rates determined by the misprediction counter <b>230</b>, threads that may have mispredictions less frequently can be prioritized over threads that may have mispredictions more frequently.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example implementation of a depth counter in accordance with an embodiment. In an embodiment, in addition to using the misprediction count for adjusting priorities of threads, the instruction fetch unit <b>206</b> can further use outputs from the depth counter <b>240</b> to adjust priorities of threads. The depth counter <b>240</b> can maintain a speculative depth count for one or more different threads, such as the threads <b>301</b>, <b>302</b>, <b>303</b> shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The speculative depth count can indicate a speculative depth of a thread. By way of example, speculative depth can be defined as a number of instructions that have not been executed to completion (e.g., that are predicted to be taken or not taken via the branch predictor <b>208</b>, and the real branch path is unknown at the time) following a first unfinished branch in the processing pipeline <b>210</b>.</p><p id="p-0042" num="0041">In an example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the depth counter <b>240</b> can receive speculative data <b>440</b> from the processing pipeline <b>210</b>, where the speculative data <b>440</b> can indicate a detection of a first unfinished or unresolved branch instruction of a thread in the processing pipeline <b>210</b>. The speculative depth data <b>440</b> can include, for example, an identifier of the thread having the first unfinished branch instruction and an indication of the cycle in which the detection of the first unfinished branch instruction occurred. For example, in the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the speculative data <b>440</b> can indicate that the branch instruction BR of the thread <b>302</b> occurred at a cycle <b>402</b>. The depth counter <b>240</b> can receive the speculative data <b>440</b> and can use the speculative data <b>440</b> to determine a speculative depth of the thread <b>302</b>. For example, the depth counter <b>240</b> can record and count a number of instructions of the thread <b>302</b> that have not been executed to completion following the cycle <b>402</b> (e.g., cycle where the unfinished branch instruction BR of the thread <b>302</b> occurred) in the processing pipeline <b>210</b>. In the example shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the speculative depth count can be up to a value of &#x201c;3&#x201d; at a cycle <b>404</b>.</p><p id="p-0043" num="0042">In an embodiment, for every cycle subsequent to the cycle <b>402</b> where the first unfinished branch BR occurred, the depth counter <b>240</b> can notify the instruction fetch unit <b>206</b> to add the priority credit C to the priority P<b>2</b> of the thread <b>302</b> (shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). In another embodiment, the depth counter <b>240</b> can notify the instruction fetch unit <b>206</b> to add the priority credit C to the priority P<b>2</b> of the thread <b>302</b>, for every cycle subsequent to the cycle <b>402</b>, in response to the misprediction rate of the thread <b>302</b> being less than the misprediction threshold. For example, at the cycle <b>404</b>, if the misprediction rate of the thread <b>302</b> is less than the misprediction threshold, the priority of the thread <b>302</b> can be P<b>2</b>+3C at the cycle <b>404</b> since the priority credit C was added to P<b>2</b> three times. Further, at each cycle, the depth counter <b>240</b> can compare the speculative depth count of the thread <b>302</b> with a speculative depth threshold. If the speculative depth count of the thread <b>302</b> is less than or equal to the speculative depth threshold, the depth counter <b>240</b> can continue to notify the instruction fetch unit <b>206</b> to add the priority credit C at each cycle. However, if the speculative depth count of the thread <b>302</b> is greater than the speculative depth threshold, the depth counter <b>240</b> can notify the instruction fetch unit <b>206</b> to stop adding the priority credit C. Note that the speculative depth threshold can be arbitrary and can be programmable dependent on a desired implementation of the system <b>100</b>.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates an example flowchart example of determining thread priorities using misprediction rate and/or speculative depth in an embodiment. The process <b>500</b> can include one or more operations, actions, or functions as illustrated by one or more of blocks <b>502</b>, <b>504</b>, <b>506</b>, <b>508</b>, <b>510</b>, <b>512</b>, and/or <b>514</b>. Although illustrated as discrete blocks, various blocks can be divided into additional blocks, combined into fewer blocks, eliminated, or performed in parallel, depending on the desired implementation.</p><p id="p-0045" num="0044">An example process <b>500</b> shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref> can be executed by the processor <b>102</b> to use at least one or both of the outputs from the misprediction counter <b>230</b> and the depth counter <b>240</b> to adjust priorities of threads. The process <b>500</b> can begin at block <b>502</b>, where the processor <b>102</b> can be programmed to either use a priority credit feature or not. The priority credit feature can be a feature for the instruction fetch unit <b>206</b> to adjust priorities of threads using the misprediction count, denoted as M in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The misprediction count M can be a count of mispredictions per a certain amount (e.g., one thousand, or other arbitrary number) of instructions. If the processor <b>102</b> is not programmed to use the priority credit feature, the process <b>500</b> can proceed to block <b>504</b>. At block <b>504</b>, the processor <b>102</b> can operate under its usual operation mode without the priority credit feature.</p><p id="p-0046" num="0045">If the processor <b>102</b> is programmed to use the priority credit feature, the process <b>500</b> can proceed to block <b>506</b>. At block <b>506</b>, the processor <b>102</b> can activate the misprediction counter <b>230</b>. The processor <b>102</b> can be further programmed to either use the speculative depth, denoted as S in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in addition to using M, or not to use S in addition to M (e.g., use M only). If the processor <b>102</b> is not programmed to use S in addition to M, the process <b>500</b> can proceed to block <b>508</b>.</p><p id="p-0047" num="0046">At block <b>508</b>, the processor <b>102</b> can determine a value of M for a thread, and can compare M to a misprediction threshold denoted as M&#x2032; in <figref idref="DRAWINGS">FIG. <b>5</b></figref>. In response to M not being less than or equal to M&#x2032; (or being greater than M&#x2032;), the process <b>500</b> can proceed to block <b>512</b>. At block <b>512</b>, the priority, denoted as P, of the thread can be maintained. In response to M being less than or equal to M&#x2032;, the process <b>500</b> can proceed to block <b>514</b>. At block <b>514</b>, the processor <b>102</b> or the instruction fetch unit <b>206</b> can add a priority credit C to the priority P to increase a priority of the thread.</p><p id="p-0048" num="0047">If the processor <b>102</b> is programmed to use S in addition to M, the process <b>500</b> can proceed to block <b>510</b>. At block <b>510</b>, the processor <b>102</b> can determine M for the thread, and also S of the thread. The processor <b>102</b> can compare M to M&#x2032;, and can compare S to a speculative depth threshold S&#x2032;. In response to M not being less than or equal to M&#x2032; (or being greater than M&#x2032;) and S not being less than or equal to S&#x2032;, the process <b>500</b> can proceed to block <b>512</b>. In response to M being less than or equal to M&#x2032; and S being less than or equal to S&#x2032;, the process <b>500</b> can proceed to block <b>514</b>. The block <b>510</b> can be performed for every cycle subsequent to a cycle where a first unfinished branch instruction occurred, until S becomes greater than S&#x2032;.</p><p id="p-0049" num="0048">In an embodiment, at block <b>510</b>, if M&#x2032;=5 and S&#x2032;=30, then the system <b>100</b> may allow a thread with a relatively low rate of misprediction (e.g., less than five mispredictions per one thousand instructions) to have relatively deeper speculations (e.g., up to thirty instruction speculations subsequent to a first unfinished branch instruction allowed) in the processing pipeline <b>210</b>. In another embodiment, at block <b>510</b>, if M&#x2032;=15 and S&#x2032;=10, then the system <b>100</b> may allow a thread to have a relatively high rate of misprediction (e.g., up to ten mispredictions per one thousand instructions) but may not allow the thread to have too many instruction speculations (e.g., up to ten instruction speculations subsequent to a first unfinished branch instruction allowed) in the processing pipeline <b>210</b>. In another example, in response to an increase of M&#x2032;, S&#x2032; can be decreased to prevent a thread from having excessive mispredictions and large speculative depth at the same time. Further, in an example, more than one set of thresholds M&#x2032; and S&#x2032; can be used for determining whether to increment P or not. For example, a first set of thresholds can be M&#x2032;=5 and S&#x2032;=40, and a second set of thresholds can be M&#x2032;=10 and S&#x2032;=30. The instruction fetch unit can increment the priority P by the credit C in response to the values of M and S being less than one of the sets of thresholds. For example, P can be incremented by C in response to either 1) M&#x3c;5 and S&#x3c;40, or 2) M&#x3c;10 and S&#x3c;30. Therefore, the values for M&#x2032; and S&#x2032; can be programmed such that threads with lower branch mispredictions, deeper speculation can be allowed, but threads with higher branch mispredictions may have to stop speculations earlier. By using the misprediction rates of threads to determine their priority, the number of wrong-path instructions in the processing pipeline <b>210</b> can be reduced. By using both the misprediction rates and speculative depth counts, the amount of wrongly speculated instructions resulting from wrongly speculated paths can be controlled and used for adjusting thread priorities to reduce the possibility of having wrongly speculated instructions occupying processor resources.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example flowchart example of determining thread priorities using misprediction rate and/or misprediction resolve time in an embodiment. The process <b>600</b> can include one or more operations, actions, or functions as illustrated by one or more of blocks <b>602</b>, <b>604</b>, <b>606</b>, <b>608</b>, <b>610</b>, <b>612</b>, and/or <b>614</b>. Although illustrated as discrete blocks, various blocks can be divided into additional blocks, combined into fewer blocks, eliminated, or performed in parallel, depending on the desired implementation.</p><p id="p-0051" num="0050">An example process <b>600</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref> can be executed by the processor <b>102</b> to use the outputs from the misprediction counter <b>230</b>, while monitoring a resolve time of a most recent misprediction of the processing pipeline <b>210</b>, to adjust priorities of threads. The process <b>600</b> can begin at block <b>602</b>, where the processor <b>102</b> can be programmed to either use a priority credit feature or not. The priority credit feature can be a feature for the instruction fetch unit <b>206</b> to adjust priorities of threads using the misprediction count, denoted as M in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The misprediction count M can be a count of mispredictions per a certain amount (e.g., one thousand, or other desired or determined number) of instructions. If the processor <b>102</b> is not programmed to use the priority credit feature, the process <b>600</b> can proceed to block <b>604</b>. At block <b>604</b>, the processor <b>102</b> can operate under its usual operation mode without the priority credit feature.</p><p id="p-0052" num="0051">If the processor <b>102</b> is programmed to use the priority credit feature, the process <b>600</b> can proceed to block <b>606</b>. At block <b>606</b>, the processor <b>102</b> can activate the misprediction counter <b>230</b>. The processor <b>102</b> can be further programmed to either use a resolve time of a most recent branch misprediction, denoted as F, in the processing pipeline <b>210</b>, in addition to using M, or not to use F in addition to M (e.g., use M only). In an example, F can denote a resolve time that maybe required to resolve the most recent branch misprediction in the processing pipeline <b>210</b>. If the processor <b>102</b> is not programmed to use F in addition to M, the process <b>600</b> can proceed to block <b>608</b>.</p><p id="p-0053" num="0052">At block <b>608</b>, the processor <b>102</b> can determine a value of M for a thread, and can compare M to a misprediction threshold denoted as M&#x2032; in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In response to M not being less than or equal to M&#x2032; (or being greater than M&#x2032;), the process <b>600</b> can proceed to block <b>612</b>. At block <b>612</b>, the priority, denoted as P, of the thread can be maintained. In response to M being less than or equal to M&#x2032;, the process <b>600</b> can proceed to block <b>614</b>. At block <b>614</b>, the processor <b>102</b> or the instruction fetch unit <b>206</b> can add a priority credit C to the priority P to increase a priority of the thread.</p><p id="p-0054" num="0053">If the processor <b>102</b> is programmed to use F in addition to M, the process <b>600</b> can proceed to block <b>610</b>. At block <b>610</b>, the processor <b>102</b> can determine M for the thread, and also F of the thread. The processor <b>102</b> can compare M to M&#x2032;, and can compare F to a flush time threshold F&#x2032;. The flush time threshold F&#x2032; can be a flush time limit for the processing pipeline to flush instructions. In response to M not being less than or equal to M&#x2032; (or being greater than M&#x2032;) and F not being less than or equal to F&#x2032;, the process <b>600</b> can proceed to block <b>612</b>. In response to M being less than or equal to M&#x2032; and F being less than or equal to F&#x2032;, the process <b>600</b> can proceed to block <b>614</b>. By using both the misprediction rates and the resolve time to resolve the most recent branch misprediction, priority credits can be added to a priority of a thread that may have mispredicted branches resolved relatively quickly (e.g., before the processing pipeline flushes).</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example flowchart describing a method for thread priorities using misprediction rate and speculative depth in an embodiment. The process <b>700</b> can include one or more operations, actions, or functions as illustrated by one or more of blocks <b>702</b>, <b>704</b>, <b>706</b>, and/or <b>708</b>. Although illustrated as discrete blocks, various blocks can be divided into additional blocks, combined into fewer blocks, eliminated, or performed in parallel, depending on the desired implementation.</p><p id="p-0056" num="0055">The process <b>700</b> can begin at block <b>702</b>. At block <b>702</b>, a processor can execute branch instructions of a thread. The process <b>700</b> can proceed from block <b>702</b> to block <b>704</b>. At block <b>704</b>, the processor can predict branch instruction outcomes of the branch instructions of the thread. The process <b>700</b> can proceed from block <b>704</b> to block <b>706</b>. At block <b>706</b>, the processor can increment a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread. The process <b>700</b> can proceed from block <b>706</b> to block <b>708</b>. At block <b>708</b>, the processor can determine a priority of the thread based on the misprediction count of the thread.</p><p id="p-0057" num="0056">In an example, the processor can determine a misprediction rate of the thread using the misprediction count, and determine the priority of the thread based on the misprediction rate. The processor can compare the misprediction rate with a misprediction rate threshold. In response to the misprediction rate being greater than a misprediction rate threshold, the processor can maintain the priority of the thread. In response to the misprediction rate being less than the misprediction rate threshold, the processor can increment the priority of the thread by a predefined credit.</p><p id="p-0058" num="0057">In an example, the processor can increment a speculative depth count of the thread in response to an occurrence of an unfinished branch instruction in the execution of the branch instructions of the thread. The processor can determine the priority of the thread using the speculative depth count. In another example, for each cycle subsequent to the unfinished branch instruction, the processor can compare the speculative depth count with a speculative depth threshold. In response to the misprediction rate being less than a misprediction rate threshold, and in response to the speculative depth count being less than the speculative depth threshold, the processor can increment the priority of the thread by a predefined credit until the speculative depth count is greater than the speculative depth threshold.</p><p id="p-0059" num="0058">In an example, the processor can compare a resolve time of a most recent misprediction in the processor pipeline with a flush time threshold. The processor can, in response to the resolve time being less than the flush time threshold, increment the priority of the thread by a predefined credit.</p><p id="p-0060" num="0059">The present invention may be a system, a method, and/or a computer program product at any possible technical detail level of integration. The computer program product may include a computer readable storage medium (or media) having computer readable program instructions thereon for causing a processor to carry out aspects of the present invention.</p><p id="p-0061" num="0060">The computer readable storage medium can be a tangible device that can retain and store instructions for use by an instruction execution device. The computer readable storage medium may be, for example, but is not limited to, an electronic storage device, a magnetic storage device, an optical storage device, an electromagnetic storage device, a semiconductor storage device, or any suitable combination of the foregoing. A non-exhaustive list of more specific examples of the computer readable storage medium includes the following: a portable computer diskette, a hard disk, a random access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM or Flash memory), a static random access memory (SRAM), a portable compact disc read-only memory (CD-ROM), a digital versatile disk (DVD), a memory stick, a floppy disk, a mechanically encoded device such as punch-cards or raised structures in a groove having instructions recorded thereon, and any suitable combination of the foregoing. A computer readable storage medium, as used herein, is not to be construed as being transitory signals per se, such as radio waves or other freely propagating electromagnetic waves, electromagnetic waves propagating through a waveguide or other transmission media (e.g., light pulses passing through a fiber-optic cable), or electrical signals transmitted through a wire.</p><p id="p-0062" num="0061">Computer readable program instructions described herein can be downloaded to respective computing/processing devices from a computer readable storage medium or to an external computer or external storage device via a network, for example, the Internet, a local area network, a wide area network and/or a wireless network. The network may comprise copper transmission cables, optical transmission fibers, wireless transmission, routers, firewalls, switches, gateway computers and/or edge servers. A network adapter card or network interface in each computing/processing device receives computer readable program instructions from the network and forwards the computer readable program instructions for storage in a computer readable storage medium within the respective computing/processing device.</p><p id="p-0063" num="0062">Computer readable program instructions for carrying out operations of the present invention may be assembler instructions, instruction-set-architecture (ISA) instructions, machine instructions, machine dependent instructions, microcode, firmware instructions, state-setting data, configuration data for integrated circuitry, or either source code or object code written in any combination of one or more programming languages, including an object oriented programming language such as Smalltalk, C++, or the like, and procedural programming languages, such as the &#x201c;C&#x201d; programming language or similar programming languages. The computer readable program instructions may execute entirely on the user's computer, partly on the user's computer, as a stand-alone software package, partly on the user's computer and partly on a remote computer or entirely on the remote computer or server. In the latter scenario, the remote computer may be connected to the user's computer through any type of network, including a local area network (LAN) or a wide area network (WAN), or the connection may be made to an external computer (for example, through the Internet using an Internet Service Provider). In some embodiments, electronic circuitry including, for example, programmable logic circuitry, field-programmable gate arrays (FPGA), or programmable logic arrays (PLA) may execute the computer readable program instructions by utilizing state information of the computer readable program instructions to personalize the electronic circuitry, in order to perform aspects of the present invention.</p><p id="p-0064" num="0063">Aspects of the present invention are described herein with reference to flowchart illustrations and/or block diagrams of methods, apparatus (systems), and computer program products according to embodiments of the invention. It will be understood that each block of the flowchart illustrations and/or block diagrams, and combinations of blocks in the flowchart illustrations and/or block diagrams, can be implemented by computer readable program instructions.</p><p id="p-0065" num="0064">These computer readable program instructions may be provided to a processor of a general purpose computer, special purpose computer, or other programmable data processing apparatus to produce a machine, such that the instructions, which execute via the processor of the computer or other programmable data processing apparatus, create means for implementing the functions/acts specified in the flowchart and/or block diagram block or blocks. These computer readable program instructions may also be stored in a computer readable storage medium that can direct a computer, a programmable data processing apparatus, and/or other devices to function in a particular manner, such that the computer readable storage medium having instructions stored therein comprises an article of manufacture including instructions which implement aspects of the function/act specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0066" num="0065">The computer readable program instructions may also be loaded onto a computer, other programmable data processing apparatus, or other device to cause a series of operational steps to be performed on the computer, other programmable apparatus or other device to produce a computer implemented process, such that the instructions which execute on the computer, other programmable apparatus, or other device implement the functions/acts specified in the flowchart and/or block diagram block or blocks.</p><p id="p-0067" num="0066">The flowchart and block diagrams in the Figures illustrate the architecture, functionality, and operation of possible implementations of systems, methods, and computer program products according to various embodiments of the present invention. In this regard, each block in the flowchart or block diagrams may represent a module, segment, or portion of instructions, which comprises one or more executable instructions for implementing the specified logical function(s). In some alternative implementations, the functions noted in the blocks may occur out of the order noted in the Figures. For example, two blocks shown in succession may, in fact, be executed substantially concurrently, or the blocks may sometimes be executed in the reverse order, depending upon the functionality involved. It will also be noted that each block of the block diagrams and/or flowchart illustration, and combinations of blocks in the block diagrams and/or flowchart illustration, can be implemented by special purpose hardware-based systems that perform the specified functions or acts or carry out combinations of special purpose hardware and computer instructions.</p><p id="p-0068" num="0067">The terminology used herein is for the purpose of describing particular embodiments only and is not intended to be limiting of the invention. It must also be noted that, as used in the specification and the appended claims, the singular forms &#x201c;a&#x201d;, &#x201c;an&#x201d; and &#x201c;the&#x201d; include plural referents unless otherwise specified. It will be further understood that the terms &#x201c;comprises&#x201d; and/or &#x201c;comprising,&#x201d; when used in this specification, specify the presence of stated features, integers, steps, operations, elements, and/or components, but do not preclude the presence or addition of one or more other features, integers, steps, operations, elements, components, and/or groups thereof.</p><p id="p-0069" num="0068">The corresponding structures, materials, acts, and equivalents of all means or step plus function elements, if any, in the claims below are intended to include any structure, material, or act for performing the function in combination with other claimed elements as specifically claimed. The description of the present invention has been presented for purposes of illustration and description, but is not intended to be exhaustive or limited to the invention in the form disclosed. Many modifications and variations will be apparent to those of ordinary skill in the art without departing from the scope and spirit of the invention. The embodiment was chosen and described in order to best explain the principles of the invention and the practical application, and to enable others of ordinary skill in the art to understand the invention for various embodiments with various modifications as are suited to the particular use contemplated.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A processor comprising:<claim-text>a processor pipeline comprising one or more execution units configured to execute instructions of a thread;</claim-text><claim-text>a branch predictor associated with the processor pipeline and configured to predict branch instruction outcomes of branch instructions among the instructions of the thread;</claim-text><claim-text>a misprediction counter configured to:<claim-text>increment a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread; and</claim-text><claim-text>determine a misprediction rate of the thread using the misprediction count, wherein the misprediction rate indicates a number of misprediction counts per a specific number of instructions among the instructions of the thread being executed by the processor pipeline; and</claim-text></claim-text><claim-text>an instruction fetch unit configured to determine a priority of the thread based on the misprediction rate of the thread.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. (canceled)</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The processor of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein:<claim-text>the misprediction counter is configured to compare the misprediction rate with a misprediction rate threshold;</claim-text><claim-text>the instruction fetch unit is configured to:<claim-text>in response to the misprediction rate being greater than a misprediction rate threshold, maintain the priority of the thread; and</claim-text><claim-text>in response to the misprediction count being less than the misprediction threshold, increment the priority of the thread by a predefined credit.</claim-text></claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The processor of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising a depth counter configured to increment a speculative depth count of the thread in response to an occurrence of an unfinished branch instruction in the execution of the branch instructions of the thread, wherein the instruction fetch unit is further configured to use the speculative depth count to determine the priority of the thread.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The processor of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein for each cycle subsequent to the unfinished branch instruction:<claim-text>the depth counter is configured to compare the speculative depth count with a speculative depth threshold; and</claim-text><claim-text>the instruction fetch unit is configured to, in response to the speculative depth count being less than the speculative depth threshold, increment the priority of the thread by a predefined credit.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The processor of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the instruction fetch unit is configured to increment the priority of the thread by the predefined credit at each cycle subsequent to the unfinished branch instruction until the speculative depth count is greater than the speculative depth threshold.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The processor of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the instruction fetch unit is configured to:<claim-text>compare a resolve time of a most recent misprediction in the processor pipeline with a flush time threshold; and</claim-text><claim-text>in response to the resolve time being less than the flush time threshold, increment the priority of the thread by a predefined credit.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A computing system comprising:<claim-text>a memory; and</claim-text><claim-text>a processor, wherein the processor comprises:</claim-text><claim-text>a processor pipeline comprising one or more execution units configured to execute instructions of a thread;</claim-text><claim-text>a branch predictor associated with the processor pipeline and configured to predict branch instruction outcomes of branch instructions among the instructions of the thread;</claim-text><claim-text>a misprediction counter configured to:<claim-text>increment a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread; and</claim-text><claim-text>determine a misprediction rate of the thread using the misprediction count, wherein the misprediction rate indicates a number of misprediction counts per a specific number of instructions among the instructions of the thread being executed by the processor pipeline; and</claim-text></claim-text><claim-text>an instruction fetch unit configured to determine a priority of the thread based on the misprediction rate of the thread.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. (canceled)</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein:<claim-text>the misprediction counter is configured to compare the misprediction rate with a misprediction rate threshold;</claim-text><claim-text>the instruction fetch unit is configured to:<claim-text>in response to the misprediction rate being greater than a misprediction rate threshold, maintain the priority of the thread; and</claim-text><claim-text>in response to the misprediction count being less than the misprediction threshold, increment the priority of the thread by a predefined credit.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising a depth counter configured to increment a speculative depth count of the thread in response to an occurrence of an unfinished branch instruction in the execution of the branch instructions of the thread, wherein the instruction fetch unit is further configured to use the speculative depth count to determine the priority of the thread.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computing system of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein for each cycle subsequent to the unfinished branch instruction:<claim-text>the depth counter is configured to compare the speculative depth count with a speculative depth threshold; and</claim-text><claim-text>the instruction fetch unit is configured to, in response to the speculative depth count being less than the speculative depth threshold, increment the priority of the thread by a predefined credit.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computing system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the instruction fetch unit is configured to increment the priority of the thread by the predefined credit at each cycle subsequent to the unfinished branch instruction until the speculative depth count is greater than the speculative depth threshold.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computing system of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the instruction fetch unit is configured to:<claim-text>compare a resolve time of a most recent misprediction in the processor pipeline with a flush time threshold; and</claim-text><claim-text>in response to the resolve time being less than the flush time threshold, increment the priority of the thread by a predefined credit.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method comprising:<claim-text>executing, by a processor, instructions of a thread;</claim-text><claim-text>predicting, by the processor, branch instruction outcomes of branch instructions among the instructions of the thread;</claim-text><claim-text>incrementing, by the processor, a misprediction count of the thread in response to an actual execution of a branch instruction of the thread being different from a corresponding branch instruction prediction outcome of the thread;</claim-text><claim-text>determining, by the processor, a misprediction rate of the thread using the misprediction count, wherein the misprediction rate indicates a number of misprediction counts per a specific number of instructions among the instructions of the thread being executed by the processor pipeline; and</claim-text><claim-text>determining, by the processor, a priority of the thread based on the misprediction rate of the thread.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. (canceled)</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>comparing, by the processor, the misprediction rate with a misprediction rate threshold;</claim-text><claim-text>in response to the misprediction rate being greater than a misprediction rate threshold, maintaining, by the processor the priority of the thread; and</claim-text><claim-text>in response to the misprediction rate being less than the misprediction rate threshold, incrementing, by the processor, the priority of the thread by a predefined credit.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>incrementing, by the processor, a speculative depth count of the thread in response to an occurrence of an unfinished branch instruction in the execution of the branch instructions of the thread; and</claim-text><claim-text>determining, by the processor, the priority of the thread using the speculative depth count.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, further comprising, for each cycle subsequent to the unfinished branch instruction:<claim-text>comparing, by the processor, the speculative depth count with a speculative depth threshold; and</claim-text><claim-text>in response to the misprediction rate being less than a misprediction rate threshold, and in response to the speculative depth count being less than the speculative depth threshold, incrementing, by the processor, the priority of the thread by a predefined credit until the speculative depth count is greater than the speculative depth threshold.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>comparing, by the processor, a resolve time of a most recent misprediction in the processor pipeline with a flush time threshold; and</claim-text><claim-text>in response to the resolve time being less than the flush time threshold, incrementing, by the processor, the priority of the thread by a predefined credit.</claim-text></claim-text></claim></claims></us-patent-application>