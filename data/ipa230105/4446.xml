<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004447A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004447</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17857107</doc-number><date>20220704</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>5077</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>505</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>5088</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>5072</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2209</main-group><subgroup>505</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>2209</main-group><subgroup>503</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HARVESTING AND USING EXCESS CAPACITY ON LEGACY WORKLOAD MACHINES</invention-title><us-related-documents><us-provisional-application><document-id><country>US</country><doc-number>63218384</doc-number><date>20210704</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>CloudNatix, Inc.</orgname><address><city>Saratoga</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Seth</last-name><first-name>Rohit</first-name><address><city>Saratoga</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kaneda</last-name><first-name>Kenji</first-name><address><city>Cupertino</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Behera</last-name><first-name>Somik</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Fu</last-name><first-name>Guangrui</first-name><address><city>Palo Alto</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Lin</last-name><first-name>Ruyang</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Some embodiments provide a novel method for deploying containerized applications. The method of some embodiments deploys a data collecting agent on a machine that operates on a host computer and executes a set of one or more workload applications. From this agent, the method receives data regarding consumption of a set of resources allocated to the machine by the set of workload applications. The method assesses excess capacity of the set of resources for use to execute a set of one or more containers, and then deploys the set of one or more containers on the machine to execute one or more containerized applications. In some embodiments, the set of workload applications are legacy workloads deployed on the machine before the installation of the data collecting agent. By deploying one or more containers on the machine, the method of some embodiments maximizes the usages of the machine, which was previously deployed to execute legacy non-containerized workloads.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.02mm" wi="112.18mm" file="US20230004447A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="126.24mm" wi="114.22mm" file="US20230004447A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="189.65mm" wi="118.79mm" file="US20230004447A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="225.21mm" wi="173.14mm" orientation="landscape" file="US20230004447A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="243.16mm" wi="170.86mm" orientation="landscape" file="US20230004447A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="167.47mm" wi="116.59mm" file="US20230004447A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="232.41mm" wi="174.75mm" orientation="landscape" file="US20230004447A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="204.05mm" wi="99.82mm" file="US20230004447A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="243.16mm" wi="166.54mm" orientation="landscape" file="US20230004447A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="207.18mm" wi="168.06mm" orientation="landscape" file="US20230004447A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="195.50mm" wi="124.63mm" file="US20230004447A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="221.74mm" wi="161.37mm" orientation="landscape" file="US20230004447A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="234.19mm" wi="167.13mm" orientation="landscape" file="US20230004447A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="193.55mm" wi="146.73mm" orientation="landscape" file="US20230004447A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><p id="p-0002" num="0001">In recent years, there has been a surge of migrating workloads from private datacenters to public clouds. Accompanying this surge has been an ever increasing number of players providing public clouds for general purpose compute infrastructure as well as specialty services. Accordingly, more than ever, there is a need to efficiently manage workloads across different public clouds of different public cloud providers.</p><heading id="h-0002" level="1">BRIEF SUMMARY</heading><p id="p-0003" num="0002">Some embodiments provide a novel method for harvesting excess compute capacity in a set of one or more datacenters, and using the harvested excess capacity to deploy containerized applications. The method of some embodiments deploys data collecting agents on several machines (e.g., virtual machines, VMs, or Pods) operating on one or more host computers in a datacenter and executing a set of one or more workload applications. In other embodiments, the data collecting agents are deployed on hypervisors executing on host computers. In some embodiments, these workload applications are legacy non-containerized workloads that were deployed on the machines before the installation of the data collecting agents.</p><p id="p-0004" num="0003">From each agent deployed on a machine, the method iteratively (e.g., periodically) receives consumption data that specifies how much of a set of resources that is allocated to the machine is used by the set of workload applications. For each machine, the method iteratively (e.g., periodically) computes excess capacity of the set of resources allocated to the machine. The method uses the computed excess capacities to deploy on at least one machine a set of one or more containers to execute one or more containerized applications. By deploying one or more containers on one or more machines with excess capacity, the method of some embodiments maximizes the usages of the machine(s). The method of some embodiments is implemented by a set of one or more controllers, e.g., a controller cluster for a virtual private cloud (VPC) with which the machine is associated.</p><p id="p-0005" num="0004">In some embodiments, the method stores the received, collected data in a time series database, and assesses the excess capacity by analyzing the data stored in this database to compute a set of excess capacity values for the set of resources (e.g., one excess capacity value for the entire set, or one excess capacity value for each resource in the set). The set of resources in some embodiments include at least one of a processor, a memory, and a disk storage of the host computer on which the set of workload applications execute.</p><p id="p-0006" num="0005">In some embodiments, the received data includes data samples regarding amounts of resources consumed at several instances in time. Some embodiments store raw, received data samples in the time series database, while other embodiments process the raw data samples to derive other data that is then stored in the time series database. The method of some embodiments analyzes the raw data samples, or derived data, stored in the time series database, in order to compute the excess capacity of the set of resources. In some embodiments, the set of resources includes different portions of different resources in a group of resources of the host computer that are allocated to the machine (e.g., portions of a processor core, a memory, and/or a disk of a host computer that are allocated to a VM on which the legacy workloads execute).</p><p id="p-0007" num="0006">To deploy the set of containers, the method of some embodiments deploys a workload first Pod, configures the set of containers to operate within the workload first Pod, and installs one or more applications to operate within each configured container. In some embodiments, the method also defines an occupancy, second Pod on the machine, and associates with this Pod a set of one or more resource consumption data values collected regarding consumption of the set of resources by the set of workload applications, or derived from this collected data. Some embodiments deploy an occupancy, second Pod on the machine, while other embodiments simply define one such Pod in a data store in order to emulate the set of workload applications. Irrespective of how the second Pod is defined or deployed, the method of some embodiments provides data regarding the set of resource consumption values associated with the occupancy, second Pod to a container manager for the container manager to use to manage the deployed set of containers on the machine. These embodiment use the occupancy Pod because the container manager does not manage nor has insight into the management of the set of workload applications.</p><p id="p-0008" num="0007">The method of some embodiments iteratively collects data regarding consumption of the set of resources by the set of containers deployed on the workload first Pod. The container manager iteratively analyzes this data along with consumption data associated with the occupancy, second Pod (i.e., with data regarding the use of the set of resources by the set of workload applications). In each analysis, the container manager determines whether the host computer has sufficient resources for the deployed set of containers. When it determines that the host computer does not have sufficient resources, the container manager designates one or more containers in the set of containers for migration from the host computer. Based on this designation, the containers are then migrated to one or more other host computers.</p><p id="p-0009" num="0008">The method of some embodiments uses priority designations (e.g., designates the occupancy, second Pod as a lower priority Pod than the workload first Pod) to ensure that when the set of resources are constrained on the host computer, the containerized workload Pod will be designated for migration from the host computer, or designated for a reduction of their resource allocations. This migration or reduction of resources, in turn, ensures that the computer resources have sufficient capacity for the set of workload application. In some embodiments, one or more containers in the set of containers can be migrated from the resource constrained machine, or have their allocation of the resources reduced.</p><p id="p-0010" num="0009">After deploying the set of containers, the method of some embodiments provides configuration data to a set of load balancers that configure these load balancers to distribute API calls to one or more containers in the set of containers as well as to other containers executing on the same host computer or on different host computers. When a subset of containers in the deployed set of containers is moved to another computer or machine, the method of some embodiments then provides updated configuration data to the set of load balancers to account for the migration of the subset of containers.</p><p id="p-0011" num="0010">Some embodiments provide a method for optimizing deployment of containerized applications across a set of one or more VPCs. The method is performed by a set of one or more global controllers in some embodiments. The method collects operational data from each cluster controller of a VPC that is responsible for deploying containerized applications in its VPC. The method analyzes the operational data to identify modifications to the deployment of one or more containerized applications in the set of VPCs. The method produces a recommendation report for displaying on a display screen, in order to present the identified modifications as recommendations to an administrator of the set of VPCs</p><p id="p-0012" num="0011">When the containerized applications execute on machines operating on host computers in one or more datacenters, the identified modifications can include moving a group of one or more containerized applications in a first VPC from a larger, first set of machines to a smaller, second set of machines. The second set of machines can be a smaller subset of the first set of machines, or can include at least one other machine not in the first set of machines. In some embodiments, moving the containerized applications to the smaller, second set of machines reduces the cost for deployment of the containerized applications by using less deployed machines to execute the containerized applications.</p><p id="p-0013" num="0012">The optimization method of some embodiments analyzes operational data by (1) identifying possible migrations of each of a group of containerized applications to new candidate machines for executing containerized application, (2) for each possible migration, using a costing engine to compute a cost associated with the migration, (3) using the computed costs to identify the possible migrations that should be recommended, and (4) including in the recommendation report each possible migration that is identified as a migration that should be recommended. In response to user input accepting a recommended migration of a first containerized application from a first machine to a second machine, the method directs a first cluster controller set of the first VPC to direct the migration of the first containerized application.</p><p id="p-0014" num="0013">In some embodiments, the computed costs are used to calculate different output values of a cost function, with each output value associated with a different deployment of the group of containerized applications. Some of these embodiments use the calculated output values of the cost function to identify the possible migrations that should be recommended. The computed costs include financial costs for deploying a set of containerized applications in at least two different public clouds (e.g., two different public clouds operated by two different public cloud providers).</p><p id="p-0015" num="0014">The optimization method of some embodiments also analyzes operational data by identifying possible adjustments to resources allocated to each of a group of containerized applications, and produces a recommendation report by generating a recommended adjustment to at least a first allocation of a first resource to at least a first container/Pod on which a first container application executes.</p><p id="p-0016" num="0015">The preceding Summary is intended to serve as a brief introduction to some embodiments of the invention. It is not meant to be an introduction or overview of all inventive subject matter disclosed in this document. The Detailed Description that follows and the Drawings that are referred to in the Detailed Description will further describe the embodiments described in the Summary as well as other embodiments. Accordingly, to understand all the embodiments described by this document, a full review of the Summary, Detailed Description, the Drawings, and the Claims is needed. Moreover, the claimed subject matters are not to be limited by the illustrative details in the Summary, Detailed Description, and the Drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0003" level="1">BRIEF DESCRIPTION OF FIGURES</heading><p id="p-0017" num="0016">The novel features of the invention are set forth in the appended claims. However, for purposes of explanation, several embodiments of the invention are set forth in the following figures.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> conceptually illustrate two processes that implement the method of some embodiments of the invention.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a VPC controller cluster of some embodiments.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates examples of occupancy Pods that are defined on machines with legacy workloads.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a process that is performed in some embodiments to continuously monitor consumption of resources on machines with containerized workloads, and to migrate, or to adjust resource allocations, to the containerized workloads when the process detects a lack of resources for the legacy workloads on these machines.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of migrating containerized application(s) to free up additional resources for the legacy workload application(s) on the same machine.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of reducing the allocation of resources to containerized application(s) to free up additional resources for the legacy workload application(s) on the same machine.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a process that some embodiments use to pack containerized and legacy workloads on fewer machines in order to reduce expenses associated with the deployment of the machines in one or more public or private cloud.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of one packing solution performed by the process of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example of a global controller with a recommendation engine that generates cost simulation results and optimization plans.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a process that a recommendation engine of a global controller performs in some embodiments to provide recommendations regarding optimized deployments of workloads and to implement a recommendation that is selected by an administrator.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of re-deployment of workloads pursuant to a recommendation generated by the recommendation engine.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates a user interface through which a global controller provides the right-sizing recommendation in some embodiments.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>14</b></figref> conceptually illustrates an electronic system with which some embodiments of the invention are implemented.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0004" level="1">DETAILED DESCRIPTION</heading><p id="p-0031" num="0030">In the following detailed description of the invention, numerous details, examples, and embodiments of the invention are set forth and described. However, it will be clear and apparent to one skilled in the art that the invention is not limited to the embodiments set forth and that the invention may be practiced without some of the specific details and examples discussed.</p><p id="p-0032" num="0031">Some embodiments provide a novel method for deploying containerized applications. The method of some embodiments deploys a data collecting agent on a machine that operates on a host computer and executes a set of one or more workload applications. From this agent, the method receives data regarding consumption of a set of resources allocated to the machine by the set of workload applications. The method assesses excess capacity of the set of resources that is available for use to execute a set of one or more containers, and then deploys the set of one or more containers on the machine to execute one or more containerized applications. In some embodiments, the set of workload applications are legacy workloads deployed on the machine before the installation of the data collecting agent. By deploying one or more containers on the machine, the method of some embodiments maximizes the usages of the machine, which was previously deployed to execute legacy non-containerized workloads.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIGS. <b>1</b> and <b>2</b></figref> conceptually illustrate two processes <b>100</b> and <b>200</b> that implement the method of some embodiments of the invention. These processes will be explained by <figref idref="DRAWINGS">FIG. <b>3</b></figref>, which illustrates a VPC controller cluster <b>300</b> of some embodiments. This controller cluster executes the process <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to harvest excess compute capacity on machines deployed in the VPC <b>305</b>, and executes the process <b>200</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> to use the harvested excess capacity to deploy containerized applications on these machines. The illustrations of the processes <b>100</b> and <b>200</b> is conceptual for some embodiments, as in these embodiments, the operations of these processes are performed by multiple sub-processes.</p><p id="p-0034" num="0033">Multiple VPCs <b>305</b> are illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Each of these VPCs is deployed in a public or private cloud in some embodiments. Each cloud includes one or more datacenters, with the public clouds having datacenters that are used by multiple tenants and the private clouds having datacenters that are used by one entity. As shown, each VPC has its own VPC controller cluster <b>300</b> (implemented by one or more controller servers) that communicates with a cluster of global controllers <b>310</b>.</p><p id="p-0035" num="0034">In some embodiments, a network administrator computer <b>315</b> interacts through a network <b>320</b> (e.g., a local area network, a wide area network, and/or the internet) with the global controller clusters <b>310</b> to specify workloads, policies for managing the workloads, and the VPC(s) managed by the administrator. The global controller cluster <b>310</b> then directs through the network <b>320</b> the VPC controller cluster <b>300</b> to deploy these workloads and effectuate the specified policies.</p><p id="p-0036" num="0035">Each VPC includes several host computers <b>325</b>, each of which executes one or more machines <b>330</b> (e.g., virtual machines, VMs, or Pods). Some or all of these machines <b>330</b> execute legacy workloads <b>335</b> (e.g., legacy applications), and are managed by legacy compute managers (not shown). The VPC controller cluster <b>300</b> communicates with the host computers <b>325</b> and their machines <b>330</b> through a network <b>340</b> (e.g., through the LAN of the datacenter(s) in which the VPC is defined).</p><p id="p-0037" num="0036">Each VPC controller cluster <b>300</b> performs the process <b>100</b> to harvest excess capacity of machines <b>330</b> in its VPC <b>305</b>. In some embodiments, the process <b>100</b> initially deploys (at <b>105</b>) a data collecting agent <b>345</b> on each of several machines <b>330</b> in the VPC <b>305</b>. In some embodiments, the VPC controller cluster <b>300</b> has a cluster agent <b>355</b> that directs the deployment of the data collecting agents <b>345</b> on the machines <b>330</b>. Some or all of these machines <b>330</b> execute legacy workloads <b>335</b> (e.g., legacy applications, such as webserver, application servers, database servers). These machines are referred to below as legacy workload machines.</p><p id="p-0038" num="0037">From each deployed agent <b>345</b>, the process <b>100</b> receives (at <b>110</b>) consumption data (e.g., operational metric data) that can be used to identify the portion of a set of the host-computer resources that is consumed by the set of legacy workload applications that execute on the agent's machine. In some embodiments, the set of host-computer resources is the set of resources of the host computer <b>325</b> that has been allocated to the machine <b>330</b>. When multiple machines <b>330</b> execute on a host computer <b>325</b>, the host computer's resources are partitioned into multiple resources sets with each resource set being allocated to a different machine. Examples of such resources include processor resources (e.g., processor cores or portions of processor cores), memory resources (e.g., portion of the host computer RAM), disk resources (e.g., portion of non-volatile semiconductor or hard disk storage), etc.</p><p id="p-0039" num="0038">Each deployed agent <b>345</b> in some embodiments collects operational metrics from an operating system of the agent's machine <b>330</b>. For instance, in some embodiments, the operating system of each machine has a set of APIs that the deployed agent <b>345</b> on that machine <b>330</b> can use to collect the desired operational metrics, e.g., the amount of CPU cycles consumed by the workload applications executing on the machine, the amount of memory and/or disk used by the workload applications, etc. In some embodiments, each deployed agent <b>345</b> iteratively pushes (e.g., periodically sends) its collected operational metric data since its previous push operation, while in other embodiments the VPC controller cluster <b>300</b> iteratively pulls (e.g., periodically retrieves) the operational metrics collected by each deployment agent since its previous pull operation.</p><p id="p-0040" num="0039">In some embodiments, the cluster agent <b>355</b> of the VPC controller cluster <b>300</b> receives the collected operational metrics (through a push or pull model) from the agents <b>345</b> on the machines <b>330</b> and stores these metrics in a set of one or more data stores <b>360</b>. The set of data stores includes a time series data store (e.g., such as Prometheus database) in some embodiments. The cluster agent <b>355</b> stores the received data in the time series data store as raw data samples regarding different amounts of resources (e.g., different amounts of processor resource, memory resource, and/or disk resource that are allocated to each machine) consumed at different instances in time by the workload applications executing on the machine.</p><p id="p-0041" num="0040">Conjunctively, or alternatively, a data analyzer <b>365</b> of the VPC controller cluster <b>300</b> in some embodiments analyzes (at <b>115</b>) the collected data to derive other data that is stored in the time series database. In some embodiments, the processed data expresses computed excess capacity on each machine <b>330</b>, while in other embodiments, the processed data is used to compute this excess capacity. The excess capacity computation of some embodiments uses machine learning models that extrapolate future predicted capacity values by analyzing a series of actual capacity values collected from the machines.</p><p id="p-0042" num="0041">The excess capacity of each machine in some embodiments is expressed as a set of one or more capacity values that express an overall excess capacity of the machine <b>330</b> for the set of resources allocated to the machine, or an excess capacity per each of several resources allocated to the machine (e.g., one excess capacity value for each resource in the set resources allocated to the machine). Some embodiments store the excess capacity values computed at <b>115</b> in the time series data store <b>360</b> as additional data samples to analyze.</p><p id="p-0043" num="0042">In some embodiments, the excess capacity computation (at <b>115</b>) is performed by the Kubernetes (K8) master <b>370</b> of the VPC controller cluster <b>300</b>. In other embodiments, the K8 master <b>370</b> just uses the computed excess capacities to migrate containerized workloads deployed by the process <b>200</b> or to reduce the amount of resources allocated to the containerized workloads. In these embodiments, the K8 master <b>370</b> directs the migration of the containerized workloads, or the reduction of resource to these workloads, after it retrieves the computed excess capacities and detects that one or more machines no longer have sufficient capacity for both the legacy workloads and containerized workloads deployed on the machine(s). The migration containerized workloads and the reduction of resource to these workloads will be further described below by reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0044" num="0043">At <b>120</b>, the process <b>100</b> (e.g., the cluster agent <b>355</b>) defines an occupancy Pod on each machine executing legacy workload (e.g., executing legacy workload applications), and associates with this occupancy Pod the set of one or more resource consumption values (i.e., the metrics received at <b>110</b>, or values derived from these metrics) regarding consumption of the set of resources by the set of workload applications.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates examples of occupancy Pods <b>405</b> that are defined on machines <b>330</b><i>a</i>-<i>d </i>with legacy workloads <b>335</b>. This figure illustrates two deployment stages <b>402</b> and <b>404</b> of four machines <b>330</b><i>a</i>-<i>d</i>. Three of these machines <b>330</b><i>a</i>, <b>330</b><i>c</i>, and <b>330</b><i>d </i>have occupancy Pods <b>405</b>. Dashed lines are used to draw the occupancy Pods in this figure in order to illustrate that while these Pods are actually deployed on each machine <b>330</b> in some embodiments, in other embodiments they are just Pods that are defined in the data store set <b>360</b> to emulate the legacy workloads for the K8 master <b>370</b>, or for a kubelet <b>385</b> that is configured on each agent <b>345</b> to operate with the K8 master <b>370</b>. As described below, the kubelet enforces QoS in some embodiments by reducing allocation of resources or removing lower priority Pods when there is a resource contention (e.g., between legacy workloads and containerized workloads).</p><p id="p-0046" num="0045">Specifically, in some embodiments, the VPC controller cluster <b>300</b> deploys the occupancy Pod because neither the K8 manager <b>370</b> nor the kubelets <b>385</b> manage or have insight into the management of the set of legacy workload applications <b>335</b>. Hence, the VPC controller cluster <b>300</b> uses the occupancy Pod <b>405</b> as a mechanism to relay information to the K8 manager <b>370</b> and the kubelets <b>385</b> regarding the usages of resources by the legacy workload applications <b>335</b> on each machine <b>330</b>. As mentioned above, these resource consumption values are stored in the data store(s) <b>360</b> in some embodiments, and are accessible to the K8 master <b>370</b>. The K8 master <b>370</b> uses this data to manage the deployed set of containers as mentioned above and further described below.</p><p id="p-0047" num="0046">In some embodiments, the VPC controller cluster <b>300</b> uses priority designations (e.g., designates an occupancy Pod <b>405</b> on a machine <b>330</b> as having a higher priority than containerized workload Pods) to ensure that when the set of resources are constrained on the host computer, the containerized workload Pod will be designated for migration from the host computer, or designated for a reduction of their resource allocations. This migration or reduction of resources, in turn, ensures that the computer resources have sufficient capacity for the set of workload application. In some embodiments, one or more containers in the set of containers can be migrated from the resource constrained machine, or have their allocation of the resources reduced.</p><p id="p-0048" num="0047">To compute the excess capacity, the cluster agent <b>355</b> of the VPC controller cluster <b>300</b> in some embodiments estimates the peak CPU/memory usage of legacy workloads <b>335</b> by analyzing the data sample records stored in the time series database <b>360</b>, and sets the request of the occupancy Pod <b>405</b> to the peak usage of legacy workloads <b>335</b>. The occupancy Pod <b>405</b> prevents containerized workloads from being scheduled on machines that do not have sufficient resources due to legacy workloads <b>335</b>. In some embodiments, the peak usage of legacy workloads <b>335</b> is calculated by subtracting the Pod total usage from the machine total usage.</p><p id="p-0049" num="0048">The cluster agent <b>355</b> sets the QoS class of occupancy Pods <b>405</b> to guaranteed by setting the resource limits, and by setting the priority of occupancy Pods <b>405</b> to a value higher than the default priority. Based on these two settings, bias the eviction process of the kubelet <b>385</b> operating within each host agent <b>345</b> to prefer evicting containerized workloads over occupancy Pods. Since both occupancy Pods and containerized workloads are in the guaranteed QoS class, the kubelet <b>385</b> evicts containerized workloads, which have lower priority than occupancy Pods. The priority of the occupancy Pods is also needed to allow occupancy Pods to preempt containerized workloads that are already running on a machine. Once occupancy Pods become guaranteed, the OOM (ONAP (Open Network Automation Platform) Operation Manager) operating on the machine <b>330</b> will prefer evicting containerized workloads over evicting occupancy Pods since the usage of occupancy Pods in some embodiments is close to 0 (just &#x201c;sleep&#x201d; process).</p><p id="p-0050" num="0049">As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the process <b>100</b> of some embodiments loops through <b>110</b>-<b>120</b> (1) to iteratively collect consumption data regarding the amount of the set of resources consumed on each machine by the legacy workloads <b>335</b> and by any containerized applications that are newly deployed by the process <b>200</b>, and (2) to analyze the collected data to maintain up to date excess capacity data and to ensure that any deployed containerized application does not impair the performance of any legacy workloads <b>335</b> deployed on the machines <b>330</b>. In each iteration, the process <b>100</b> identifies any newly deployed legacy workloads <b>335</b>, for which it then defines an occupancy Pod as described above.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates the process <b>200</b> that uses the computed excess capacities of the legacy workload machines in order to select one or more of these machines and to deploy one or more sets of containers on these machines to execute containerized applications. As mentioned above, the process <b>200</b> is executed by the VPC controller cluster <b>300</b> in some embodiments. In other embodiments, this process is performed by the global controller cluster <b>310</b>.</p><p id="p-0052" num="0051">The process <b>200</b> starts each time that one or more sets of containerized applications have to be deployed in a VPC <b>305</b>. The process <b>200</b> initially selects (at <b>205</b>) a machine in the VPC with excess capacity. This machines can be a legacy workload machine with excess capacity, or a machine that executes no legacy workloads. In some embodiments, the process <b>200</b> selects legacy workload machines so long as such machines are available with a minimum excess capacity of X % (e.g., 30%). When there are multiple such machines, the process <b>200</b> selects the legacy workload machine in the VPC with the highest excess capacity in some embodiments.</p><p id="p-0053" num="0052">When the VPC <b>305</b> does not have legacy workload machines with the minimum excess capacity, the process <b>200</b> selects (at <b>205</b>) a machine that does not execute any legacy workloads. In some embodiments, the machines that are considered (at <b>205</b>) by the process <b>200</b> for the new deployment are virtual machines executing on host computers. However, in other embodiments, these machines can include BareMetal host computers and/or Pods. At <b>210</b>, the process <b>200</b> selects a set of one or more containers that need to be deployed in the VPC. Next, at <b>215</b>, the process <b>200</b> deploys a workload Pod on the machine selected at <b>205</b>, deploys the container set selected at <b>210</b> onto this workload Pod, and installs and configures one or more applications to run on each container in the container set deployed at <b>215</b>.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates the deployment of such workload Pods and containerized applications on these Pods. As mentioned above, the first stage <b>402</b> illustrates four machines <b>330</b><i>a</i>-<i>d</i>, three of which <b>330</b><i>a</i>, <b>330</b><i>c</i>, and <b>330</b><i>d </i>execute legacy workloads <b>335</b>, and have an associated occupancy Pod <b>405</b>, which as mentioned above models the resource consumption of the legacy workloads for the K8 manager <b>370</b> and/or its associated kubelets <b>385</b>. The second stage <b>404</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> shows two workload Pods <b>420</b> deployed on two machines <b>330</b><i>a </i>and <b>330</b><i>c</i>. On each workload Pod, a container <b>430</b> executes, and an application <b>440</b> executes on each container.</p><p id="p-0055" num="0054">At <b>220</b>, the process <b>200</b> adjusts the excess capacity of the selected machine to account for the new workload Pod <b>420</b> that was deployed on it at <b>215</b>. In some embodiments, this adjustment is just a static adjustment of the machine's capacity (as stored on the VPC controller cluster data store <b>360</b>) for a first time period, until data samples are collected by the agent <b>345</b> (executing on the selected machine <b>330</b>) a transient amount of time after the workload Pod starts to operate on the selected machine. In other embodiments, the process <b>200</b> does not adjust the excess capacity value of the selected machine <b>330</b>, but rather allows for this value to be adjusted by the VPC controller cluster processes after the consumption data values are received from the agent <b>345</b> deployed on the machine.</p><p id="p-0056" num="0055">After <b>220</b>, the process <b>200</b> determines (at <b>225</b>) whether it has deployed all the containers that need to be deployed. If so, it ends. Otherwise, it returns to <b>205</b> to select a machine for the next container set that needs to be deployed, and then repeats its operations <b>210</b>-<b>225</b> for the next container set. By deploying one or more containers on legacy workload machines, the process <b>200</b> of some embodiments maximizes the usages of these machines, which were previously deployed to execute legacy non-containerized workloads.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a process <b>500</b> that is performed in some embodiments to continuously monitor consumption of resources on machines with containerized workloads, and to migrate, or to adjust resource allocations, to the containerized workloads when the process detects a lack of resources for the legacy workloads on these machines. The process <b>500</b> is performed iteratively in some embodiments by the K8 master <b>370</b> and/or the kubelet <b>385</b> of the machine.</p><p id="p-0058" num="0057">As shown, the process <b>500</b> collects (at <b>505</b>) data regarding consumption of resources by legacy and containerized workloads executing on machines in the VPC. At <b>510</b>, the process analyzes the collected data to determine whether it has identified a lack of sufficient resources (e.g., memory, CPU, disk, etc.) for any of the legacy workloads. If not, the process returns to <b>505</b> to collect additional data regarding resource consumption.</p><p id="p-0059" num="0058">Otherwise, when the process identifies (at <b>510</b>) that the set of resources allocated to a machine are not sufficient for a legacy workload application executing on the machine, the process modifies (at <b>515</b>) the deployment of the containerized application(s) on the machine to make additional resources available to the legacy workload application. Examples of such a modification include (1) migrating one or more containerized workloads that are deployed on the machine to another machine in order to free up additional resources for the legacy workload application(s) on the machine, or (2) reducing the allocation of resources to one or more containerized workloads on the machine to free up more of the resources for the legacy workload application(s) on the machine.</p><p id="p-0060" num="0059"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates an example of migrating containerized application(s) to free up additional resources for the legacy workload application(s) on the same machine. In this example, the legacy workload <b>335</b> on machine <b>330</b><i>a </i>is consuming more resources (e.g., more CPU, memory and/or disk resources) and this additional consumption does not leave sufficient amount of resources available on the machine <b>330</b><i>a </i>for the containerized workload Pod <b>420</b>. This additional resource consumption is depicted by the larger size of the legacy workloads <b>335</b> and its associated occupancy Pod <b>405</b> as compared to the representations of these two items in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Because of this additional consumption, the workload Pod <b>420</b> has migrated from the machine <b>330</b><i>a </i>to the machine <b>330</b><i>d</i>, so that the legacy workload <b>335</b> can consume additional resources on the machine <b>330</b><i>a. </i></p><p id="p-0061" num="0060">When migrating a containerized application to a new machine, the process <b>500</b> moves the containerized application to a machine (with or without legacy workloads) that has sufficient resource capacity for the migrating containerized application. To identify such machines, the process <b>500</b> uses the excess capacity computation of the process <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in some embodiments.</p><p id="p-0062" num="0061"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates an example of reducing the allocation of resources to containerized application(s) to free up additional resources for the legacy workload application(s) on the same machine. This figure shows two operational stages <b>702</b> and <b>704</b> of the machine <b>330</b><i>a</i>. The first operational stage <b>702</b> shows that as in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the legacy workload <b>335</b> on machine <b>330</b><i>a </i>in <figref idref="DRAWINGS">FIG. <b>7</b></figref> is consuming more resources (e.g., more CPU, memory and/or disk resources) in the set of resources allocated to the machine <b>330</b><i>a</i>, and this additional resource consumption is depicted by the larger size of the legacy workloads <b>335</b> and its associated occupancy Pod <b>405</b>. The second operational stage <b>704</b> then shows the workload Pod <b>420</b> remaining on the machine <b>330</b><i>a </i>but having less resources allocated to it. This reduced allocation level is as depicted by the smaller size of the workload Pod <b>420</b> in the second stage <b>704</b>.</p><p id="p-0063" num="0062">When the process <b>500</b> moves the containerized workload to another machine, the process <b>500</b> configures (at <b>520</b>) forwarding elements and/or load balancers in the VPC to forward API (application programming interface) requests that are sent to the containerized application to the new machine that now executes the containerized application. In some embodiments, the migrated containerized application is part of a set of two or more containerized applications that perform the same service. In some such embodiments, load balancers (e.g., L7 load balancers) distribute the API requests that are made for the service among the containerized applications. After deploying the set of containers, some embodiments provide configuration data to configure a set of load balancers to distribute API calls among the containerized applications that perform the service. When a container is migrated to another computer or machine to free up resources for legacy workloads, the process <b>500</b> in some embodiments provides updated configuration data to the set of load balancers to account for the migration of the container. After <b>520</b>, the process <b>500</b> returns to <b>505</b> to continue its monitoring of the resource consumption of the legacy and containerized workloads.</p><p id="p-0064" num="0063">Some embodiments use the excess capacity computations in other ways. <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates a process <b>800</b> that some embodiments use to pack containerized and legacy workloads on fewer machines in order to reduce expenses associated with the deployment of the machines in one or more public or private cloud. The process <b>800</b> is performed by the global controller cluster <b>310</b> and the VPC controller cluster(s) <b>300</b> of one or more VPCs <b>305</b>.</p><p id="p-0065" num="0064">As shown, the process <b>800</b> starts (at <b>805</b>) when an administrator directs the global controller cluster <b>310</b> through its user interface (e.g., its web interface or APIs) to reduce the number of machines on which the legacy and containerized workloads managed by the administrator are deployed. In some embodiments, these machines can operate in one or more VPCs defined in one or more public or private clouds. When the machines operate in more than one VPC, the administrator's request to reduce the number of machines uses can identify the VPC(s) in which the machines should be examined for the workload migration and/or packing. Alternatively, the administrator's request does not identify any specific VPC to explore in some embodiments.</p><p id="p-0066" num="0065">Next, at <b>810</b>, the process <b>800</b> identifies a set of machines to examine, and for each machine in the set, identifies excess capacity of the set of resources allocated to the machine. The set of machines includes the machines currently deployed in each of the explored VPC (i.e., in each VPC that has a machine that should be examined for workload migration and/or packing). In some embodiments, a capacity-harvesting agent <b>345</b> executes on each examined machine and iteratively collects resource consumption data, as described above. In these embodiments, the process <b>800</b> uses the collected resource consumption data (e.g., the data stored in a time series data store <b>360</b>) to compute available excess capacity of each examined machine.</p><p id="p-0067" num="0066">At <b>815</b>, the process <b>800</b> explores different solutions for packing different combinations of legacy and containerized workloads onto a smaller set of machines than the set of machines identified at <b>810</b>. The process <b>800</b> then selects (at <b>820</b>) one of the explored solutions. In some embodiments, the process <b>800</b> uses a constrained optimization search process to explore the different packing solutions and to select an optimal solution from the explored solutions.</p><p id="p-0068" num="0067">The constrained optimization search process of some embodiments uses a cost function that accounts for one or more types of costs. Examples of such costs in some embodiments include resource consumption efficiency cost (meant to reduce the wasting of excess capacity), financial cost (accounting for cost of deploying machines in public clouds), affinity cost (meant to bias towards closer placement of applications that communicate with each other), etc. In other embodiments, the process <b>800</b> does not use constrained optimization search processes, but rather uses simpler processes (e.g., greedy processes) to select a packing solution for packing the legacy and containerized workloads onto a smaller set of machines.</p><p id="p-0069" num="0068">After selecting a packing solution, the process <b>800</b> migrates (at <b>825</b>) one or more legacy workloads and/or containerized workloads in order to implement the selected packing solution. The process <b>800</b> configures (at <b>830</b>) forwarding elements and/or load balancers in one or more affected VPCs to forward API (application programming interface) requests that are sent to the migrated workload applications to the new machine on which the workload applications now execute.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>9</b></figref> illustrates an example of one packing solution performed by the process <b>800</b>. This solution is presented in two operational stages <b>902</b> and <b>904</b> of four machines <b>930</b><i>a</i>-<i>d</i>. Each of these machines executes one or more workloads and a capacity harvesting agent <b>345</b>. The first stage <b>902</b> shows the first machine <b>930</b><i>a </i>executing legacy and containerized workloads LWL<b>1</b> and CWL<b>1</b>, the second machine <b>930</b><i>b </i>executing a legacy workload LWL<b>2</b>, the third machine <b>930</b><i>c </i>executing containerized workload CWL<b>2</b>, and the fourth machine <b>930</b><i>d </i>executing a legacy workload LWL<b>3</b>. The second stage <b>904</b> shows that all the legacy and containerized workloads have been packed onto the first and second machines <b>930</b><i>a </i>and <b>930</b><i>b</i>. This stage depicts the third and fourth machines <b>930</b><i>c</i>-<i>d </i>in dashed lines to indicate that these machines have been taken offline as they are no longer used for deployment of any legacy or containerized workload applications.</p><p id="p-0071" num="0070">The packing solution depicted in stage <b>904</b> required the migration of the containerized workload CWL<b>2</b> and the legacy workload LWL<b>3</b> to the second machine <b>930</b><i>b </i>respectively from the third and fourth machines <b>930</b><i>c </i>and <b>930</b><i>d</i>. Before selecting this packing solution, the process <b>800</b> in some embodiments would explore other packing solutions, such as moving the containerized workload CWL<b>2</b> to the first machine <b>930</b><i>a</i>, moving the legacy workload LWL<b>3</b> to the first machine <b>930</b><i>a</i>, moving the containerized workload CWL<b>2</b> to the fourth machine <b>930</b><i>d</i>, moving the legacy workload LWL<b>3</b> to the third machine <b>930</b><i>c</i>, moving the first legacy workload LWL<b>1</b> and containerized workload CLW<b>1</b> to one or more other machines, etc. In the end, the process <b>800</b> in these embodiments selects the packing solution shown in stage <b>904</b> because this solution resulted in an optimal solution with a best computed cost (as computed by the cost function used by the constrained optimization search process).</p><p id="p-0072" num="0071">Instead of having a user request the efficient packing of workloads onto fewer machines, or in conjunction with this feature, some embodiments use automated processes to provide recommendations for the dynamic optimization of deployments in order to efficiently pack and/or migrate workloads, and thereby reducing the cost of deployments. <figref idref="DRAWINGS">FIGS. <b>10</b>-<b>13</b></figref> illustrate examples of the dynamic optimization approach of some embodiments.</p><p id="p-0073" num="0072">In these embodiments, the global controller <b>310</b> has a recommendation engine that performs the cost optimization. It retrieves historical data from time series database, and generates cost simulation results as well as optimization plans. The recommendation engine generates a report that includes these plans and results. The administrator reviews this report and decides whether to apply one or more of the presented plans. When the administrator decides to apply the plan for one or more of the VPCs, the global controller sends a command to the cluster agent of each affected VPC. Each cluster agent that receives a command then makes the API calls to cloud infrastructure managers (e.g., the AWS managers) to execute the plan (e.g., resize instance types).</p><p id="p-0074" num="0073"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates an example of a global controller <b>310</b> with a recommendation engine <b>1020</b> that generates cost simulation results and optimization plans. In addition to the recommendation engine <b>1020</b>, the global controller <b>310</b> includes an API gateway <b>1005</b>, a workload manager <b>1010</b>, a secure VPC interface <b>1015</b>, a cluster monitor <b>1040</b> and a cluster metric data store <b>1035</b>. The recommendation engine <b>1020</b> includes an optimization search engine <b>1025</b> and a costing engine <b>1030</b>.</p><p id="p-0075" num="0074">The API gateway <b>1005</b> enables secure communication between the global controller <b>310</b> and the network administrator computer <b>315</b> through the intervening network <b>320</b>. Similarly, the secure VPC interface <b>1015</b> allows the global controller <b>310</b> to have secure (e.g., VPN protected) communication with one or more VPC controller cluster(s) of one or more VPCs. The workload manager <b>1010</b> of the global controller <b>310</b> uses the API gateway <b>1005</b> and the secure VPC interface <b>1015</b> to have secure communications with the network administrators and VPC clusters. Through the gateway <b>1005</b>, the workload manager can receive instructions from the network administrators, which it can then relay to the VPC controller clusters through the VPC interface <b>1015</b>.</p><p id="p-0076" num="0075">The cluster monitor <b>1040</b> receives operational metrics from each VPC controller cluster through the VPC interface <b>1015</b>. These operational metrics are metrics collected by the capacity harvesting agents <b>345</b> deployed on the machines in each VPC. The cluster monitor <b>1040</b> stores the received operational metrics in the cluster metrics data store <b>1035</b>. This data store is a time series database in some embodiments. In some embodiments, the received metrics are stored as raw data samples collected at different instances in time, while in other embodiments they are processed and stored as processed data samples for different instances in time.</p><p id="p-0077" num="0076">The recommendation engine <b>1020</b> retrieves data samples from the time series database, and generates cost simulation results as well as optimization plans. The recommendation engine uses its optimization search engine <b>1025</b> to identify different optimization solutions, and uses its costing engine <b>1030</b> to compute a cost for each identified solution. For instance, as described above for <figref idref="DRAWINGS">FIGS. <b>8</b> and <b>9</b></figref>, the constrained optimization search in some embodiments explores different packing solutions and identifies one or more optimal solutions from the explored solutions. Moreover, the costing engine <b>1030</b> uses in some embodiments uses a cost function that accounts for one or more types of costs. Examples of such costs in some embodiments include resource consumption efficiency cost (meant to reduce the wasting of excess capacity), financial cost (accounting for cost of deploying machines in public clouds), affinity cost (meant to bias towards closer placement of applications that communicate with each other), etc.</p><p id="p-0078" num="0077">The recommendation engine <b>1020</b> generates a report that identifies the usage results that it has identified, as well as the cost simulation and optimization plan that engine has generated. The recommendation engine <b>1020</b> then provides this report to the network administrator through one or more electronic mechanisms, such as email, web interface, API, etc. The administrator reviews this report and decides whether to apply one or more of the presented plans. When the administrator decides to apply the plan for one or more of the VPCs, the workload manager <b>1010</b> of the global controller <b>310</b> sends a command to the cluster agent of the controller cluster of each affected VPC. Each cluster agent that receives a command then makes the API calls to cloud infrastructure managers (e.g., the AWS managers) to execute the plan (e.g., resize instance types).</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>11</b></figref> illustrates a process <b>1100</b> that the recommendation engine <b>1020</b> of the global controller <b>310</b> performs in some embodiments to provide recommendations regarding optimized deployments of workloads and to implement a recommendation that is selected by an administrator. As shown, the process <b>1100</b> initially collects (at <b>1105</b>) placement information regarding current deployment of legacy and containerized workloads. In some embodiments, these machines can operate in one or more VPCs defined in one or more public or private clouds. To perform the operation at <b>1105</b>, the process <b>1100</b> retrieves this data from a data store of the global controller.</p><p id="p-0080" num="0079">Next, at <b>1110</b>, the process <b>1100</b> computes excess capacity of the machines identified at <b>1105</b>. The process <b>1100</b> performs this computation by retrieving and analyzing the data samples stored in the time series database <b>1035</b>, as described above. For each identified machine in the set, the process <b>1100</b> identifies excess capacity of the set of resources allocated to the machine. In some embodiments, a capacity-harvesting agent <b>345</b> executes on each examined machine and iteratively collects resource consumption data, as described above. In these embodiments, the process <b>1100</b> uses the collected resource consumption data (e.g., the data stored in a time series data store <b>360</b>) to compute available excess capacity of each examined machine.</p><p id="p-0081" num="0080">At <b>1115</b>, the process <b>1100</b> explores different solutions for packing different combinations of legacy and containerized workloads onto existing and new machines in one or more VPCs. In some embodiments, the search engine <b>1025</b> uses a constrained optimization search process to explore the different packing solutions and to select an optimal solution from the explored solutions. The constrained optimization search process of some embodiments uses the costing engine <b>1030</b> to compute a cost function that accounts for one or more types of costs. Examples of such costs in some embodiments include resource consumption efficiency cost (meant to reduce the wasting of excess capacity), financial cost (accounting for cost of deploying machines in public clouds), affinity cost (meant to bias towards closer placement of applications that communicate with each other), etc.</p><p id="p-0082" num="0081">The process <b>1100</b> then generates (at <b>1120</b>) a report that includes one or more recommendations for one or more possible optimizations to the current deployment of the legacy and containerized workloads. It then provides (at <b>1120</b>) this report to the network administrator through one or more mechanisms, such as (1) an email to the administrator, (2) a browser interface through which the network administrator can query the global controller's webservers to retrieve the report, (3) an API call to a monitoring program used by the network administrator, etc.</p><p id="p-0083" num="0082">The administrator reviews this report and accept (at <b>1125</b>) one or more of the presented recommendations. The recommendation engine <b>1020</b> then directs the workload manager <b>1010</b> to instruct (at <b>1130</b>) the VPC controller cluster(s) to migrate one or more legacy workloads and/or containerized workloads in order to implement the selected recommendation. For this migration, the VPC controllers also configures (at <b>1135</b>) forwarding elements and/or load balancers in one or more affected VPCs to forward API (application programming interface) requests that are sent to the migrated workload applications to the new machine on which the workloads now execute. The process <b>1100</b> then ends.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates an example of re-deployment of workloads pursuant to a recommendation generated by the recommendation engine <b>1020</b>. This example presents two stages <b>1202</b> and <b>1204</b> of workload deployments for an entity (e.g., a corporation). Both stages show the workloads deployed on public cloud machines, which in turn execute on host computers (not shown). The workloads include legacy workloads (LWLs) and containerized workloads (CWLs). Each machine is also shown to execute a capacity harvesting agent A.</p><p id="p-0085" num="0084">The first stage <b>1202</b> shows that initially a number of workloads for one entity are deployed in three different VPCs that are defined in the public clouds of two different public cloud providers, with a first VPC <b>1205</b> being deployed in a first availability zone <b>1206</b> of a first public cloud provider, a second VPC <b>1208</b> being deployed in a second availability zone <b>1210</b> of the first public cloud provider, and a third VPC <b>1215</b> being deployed in a datacenter of a second public cloud provider.</p><p id="p-0086" num="0085">The second stage <b>1204</b> shows the deployment of the workloads after an administrator accepts a recommendation to move all the workloads to the public cloud of the first public cloud provider. As shown, all the workloads in the third VPC <b>1215</b> have migrated to the two availability zones <b>1206</b> and <b>1210</b> of the first public cloud provider. The third VPC appears with dashed lines to indicate that it has be terminated. In some embodiments, the migration of the workloads from the third VPC reduces the deployment cost of the entity as it packs more workloads on the fewer number of public cloud machines, and consumes less external network bandwidth as it would eliminate bandwidth that is consumed by communication between machines in different public clouds of different public cloud providers.</p><p id="p-0087" num="0086">In some embodiments, the global controller provides the right-sizing recommendation via a user interface (UI) <b>1300</b> illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>. This UI shows the cost associated with the resizing of one workload (e.g., containerized workload) so that a network administrator can assess the impact of optimization. Specifically, the UI provides controls to see the cost and risk impact of the right-sizing a workload as well as allow the administrator to customize the recommendation before applying. The administrator can then select (e.g., click a button) to apply the recommendation.</p><p id="p-0088" num="0087">In some embodiments, the recommendation engine in the VPC cluster controller communicates with the global controller to apply the recommendations automatically by performing the set of steps a human operator would take in resizing a VM, a Pod, or a container. These steps include non-disruptively adjusting the CPU capacity, memory capacity, disk capacity, GPU capacity available to a container or Pod without requiring a restart.</p><p id="p-0089" num="0088">These steps in some embodiments also include non-disruptively adjusting the CPU capacity, memory capacity, disk capacity, GPU capacity available to a VM with hot resize when supported by underlying Virtualization platforms. In platforms that do not support hot resize, some embodiments ensure the VM's identity and state remain unchanged, by ensuring the VM's OS and data volumes are snapshotted and re-attached to the resized VMs. Some embodiments also persist the VM's externally facing IP or in case of VM Pool, maintain a consistent load balanced IP post resize. In this manner, some embodiments in a closed loop fashion performed all necessary steps to resize VM similar to how a human operator would resize it even when the underlying virtualization platforms do not support hot resize.</p><p id="p-0090" num="0089">In the UI <b>1300</b>, the administrator can view recommendations versus usage metrics for every several different types of resource consumed by the workload (e.g., the container being monitored). In this example, a window <b>1301</b> displays a vCPU resource <b>1305</b>, and a memory resource <b>1310</b>, along with a savings option <b>1315</b>. For the selected vCPU resource <b>1305</b>, the window <b>1310</b> illustrates (1) an average vCPU usage <b>1302</b> corresponding to an average observed (actual) usage of the vCPU by the monitored workload, (2) a max vCPU usage <b>1306</b> corresponding to a maximum observed usage of the vCPU by the monitored workload, (3) a limit usage <b>1304</b> corresponding to a configured maximum vCPU usage for the monitored workload, and (4) a request usage <b>1308</b> corresponding to a configured minimum vCPU usage for the monitored workload.</p><p id="p-0091" num="0090">In some embodiments, the UI <b>1300</b> also provides visualization of other vCPU usages, such as P99% vCPU usage and P95% vCPU usage, as well as recommended min and maximum vCPU usages. In sum, there are at least three types of usage parameters that the UI <b>1300</b> can display in some embodiments. These are configured max and min usage parameters, observed max and min usage parameters and recommended max and min usage parameters. In some of these embodiments, the configured and recommended parameters are shown as straight or curved line graphs, while the observed parameters are shown as waves with solid interiors.</p><p id="p-0092" num="0091">In the example of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the wave <b>1322</b> is max observed usage (P100), the wave <b>1324</b> is P99 usage (usage that is observed for the 99 percentile), and the wave <b>1326</b> is the average usage (also called the P50 usage). An X percentile usage means that X % of the usage samples should be below this given usage number, and only 100-X % of the usage sample are allowed to be higher than PX usage. <figref idref="DRAWINGS">FIG. <b>13</b></figref> also illustrates a configured max usage (limit) <b>1332</b>, a recommended max usage (limit) <b>1334</b>, and a recommended max vCPU (limit) <b>1336</b> for autopilot mode, which will be described below.</p><p id="p-0093" num="0092">The UI <b>1300</b> allows an administrator to adjust the recommended vCPU max and min usages through the slider controls. In this example, the network administrator can adjust the recommended max CPU through the slider <b>1340</b>, and adjust the recommended min CPU usage through the slider <b>1342</b>, before accepting/applying the recommendation. As shown, the UI includes sliders for memory max and min usages, as well as cost and saving sliders, which will be described further below.</p><p id="p-0094" num="0093">The UI <b>1300</b> allows an administrator to visualize and adjust memory metrics memory option <b>1310</b> in the window <b>1301</b>. Selection of this option enables Memory Resource Metric Visualization, which allows the administrator to visualize recommendations and adjust these recommendation in much the same way as the CPU recommendations can be visualized and adjusted.</p><p id="p-0095" num="0094">The third option <b>1315</b> in the window <b>1301</b> is the &#x201c;Savings&#x201d; option. Enabling this radio button lets the user visualize (1) cost (e.g., money spent) for the configured max CPU or memory resource, (2) cost used (e.g., money spent) for used CPU or Memory resource, and (3) cost recommended (e.g., the recommended amount of money that should be spent) for the recommended amount of resources to consume. The delta between the recommended cost recommended and spent cost is &#x201c;Savings&#x201d;. The Cost UI control lets the administrator adjust its target cost and see the controls for CPU/Mem on the left hand side dynamically move to account for the administrator's desire for a target cost.</p><p id="p-0096" num="0095">When the administrator is satisfied with a recommendations and any adjustment made to the recommendation by the user, the administrator can direct the global controller to apply the recommendation through the Apply control <b>1350</b>. Selection of this control presents the apply now control <b>1352</b>, the re-deploy control <b>1354</b>, and the auto-pilot control <b>1356</b>. The selection of the apply now control <b>1352</b> updates the resource configuration of the machine (e.g., Pod or VM at issue) just-in-time.</p><p id="p-0097" num="0096">When the &#x201c;apply now&#x201d; option is selected for a Pod, some embodiments leverage the capacity harvesting agent to reconfigure the Pod's CPU/memory settings. For VMs, some embodiments use another set of techniques to adjust the size just-in-time. For instance, some embodiments take a snapshot of VM's disk, then create a new VM with new CPU/memory settings, attach the disk snapshot and point old VM's public facing IP to the new VM. Some embodiments also allow for scheduled &#x201c;re-size&#x201d; of the VM so that the VM can be re-sized during maintenance window of the VM.</p><p id="p-0098" num="0097">The selection of the apply via re-deploy control <b>1354</b> re-deploys the machine with new resource configuration. The selection of the auto-pilot control <b>1356</b> causes the presentation of the window <b>1358</b>, which directs the administrator to specify a policy around how many times the machine can be restarted in order to &#x201c;continuously&#x201d; apply right-sizing rules. The apply controls <b>1350</b> in other embodiments include additional controls such as a dismiss control to show prior dismissed recommendations.</p><p id="p-0099" num="0098">In some embodiments, the recommendations are applicable for a workload, which is the aggregate of the Pods in a set of one or more Pods. The sizes of the Pods in the set of Pods are adjusted using techniques available in K8s and OSS. Some of these techniques are described in https://github.com/kubernetes/enhancements/issues/1287. Some embodiments also adjust the Pod size via a re-deploy option <b>1354</b>, or an auto-pilot with max Pod restart options <b>1356</b> and <b>1358</b> that iteratively re-deploys until the desired metrics are achieved.</p><p id="p-0100" num="0099">Also, in some embodiments, the right-sizing recommendations computes the CPU/memory savings and modeled cost in order to allow the administrator to assess the financial impact of the right-sizing. Some embodiments: (1) compute the [Cost per VM/2]/[# of CPU MilliCores] and model the cost per MilliCore consumed by a container running on a VM, and (2) take [Cost per VM/2]/[# of Mem. MiB] and model the cost per MiB consumed by a container running on a VM.</p><p id="p-0101" num="0100">Many of the above-described features and applications are implemented as software processes that are specified as a set of instructions recorded on a computer readable storage medium (also referred to as computer readable medium). When these instructions are executed by one or more processing unit(s) (e.g., one or more processors, cores of processors, or other processing units), they cause the processing unit(s) to perform the actions indicated in the instructions. Examples of computer readable media include, but are not limited to, CD-ROMs, flash drives, RAM chips, hard drives, EPROMs, etc. The computer readable media does not include carrier waves and electronic signals passing wirelessly or over wired connections.</p><p id="p-0102" num="0101">In this specification, the term &#x201c;software&#x201d; is meant to include firmware residing in read-only memory or applications stored in magnetic storage, which can be read into memory for processing by a processor. Also, in some embodiments, multiple software inventions can be implemented as sub-parts of a larger program while remaining distinct software inventions. In some embodiments, multiple software inventions can also be implemented as separate programs. Finally, any combination of separate programs that together implement a software invention described here is within the scope of the invention. In some embodiments, the software programs, when installed to operate on one or more electronic systems, define one or more specific machine implementations that execute and perform the operations of the software programs.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>14</b></figref> conceptually illustrates an electronic system <b>1400</b> with which some embodiments of the invention are implemented. The electronic system <b>1400</b> may be a computer (e.g., a desktop computer, personal computer, tablet computer, server computer, mainframe, a blade computer etc.), or any other sort of electronic device. As shown, the electronic system includes various types of computer readable media and interfaces for various other types of computer readable media. Specifically, the electronic system <b>1400</b> includes a bus <b>1405</b>, processing unit(s) <b>1410</b>, a system memory <b>1425</b>, a read-only memory <b>1430</b>, a permanent storage device <b>1435</b>, input devices <b>1440</b>, and output devices <b>1445</b>.</p><p id="p-0104" num="0103">The bus <b>1405</b> collectively represents all system, peripheral, and chipset buses that communicatively connect the numerous internal devices of the electronic system <b>1400</b>. For instance, the bus <b>1405</b> communicatively connects the processing unit(s) <b>1410</b> with the read-only memory (ROM) <b>1430</b>, the system memory <b>1425</b>, and the permanent storage device <b>1435</b>. From these various memory units, the processing unit(s) <b>1410</b> retrieve instructions to execute and data to process in order to execute the processes of the invention. The processing unit(s) may be a single processor or a multi-core processor in different embodiments.</p><p id="p-0105" num="0104">The ROM <b>1430</b> stores static data and instructions that are needed by the processing unit(s) <b>1410</b> and other modules of the electronic system. The permanent storage device <b>1435</b>, on the other hand, is a read-and-write memory device. This device is a non-volatile memory unit that stores instructions and data even when the electronic system <b>1400</b> is off. Some embodiments of the invention use a mass-storage device (such as a magnetic or optical disk and its corresponding disk drive) as the permanent storage device <b>1435</b>.</p><p id="p-0106" num="0105">Other embodiments use a removable storage device (such as a floppy disk, flash drive, etc.) as the permanent storage device. Like the permanent storage device <b>1435</b>, the system memory <b>1425</b> is a read-and-write memory device. However, unlike storage device <b>1435</b>, the system memory is a volatile read-and-write memory, such a random access memory. The system memory stores some of the instructions and data that the processor needs at runtime. In some embodiments, the invention's processes are stored in the system memory <b>1425</b>, the permanent storage device <b>1435</b>, and/or the read-only memory <b>1430</b>. From these various memory units, the processing unit(s) <b>1410</b> retrieve instructions to execute and data to process in order to execute the processes of some embodiments.</p><p id="p-0107" num="0106">The bus <b>1405</b> also connects to the input and output devices <b>1440</b> and <b>1445</b>. The input devices enable the user to communicate information and select commands to the electronic system. The input devices <b>1440</b> include alphanumeric keyboards and pointing devices (also called &#x201c;cursor control devices&#x201d;). The output devices <b>1445</b> display images generated by the electronic system. The output devices include printers and display devices, such as cathode ray tubes (CRT) or liquid crystal displays (LCD). Some embodiments include devices such as a touchscreen that function as both input and output devices.</p><p id="p-0108" num="0107">Finally, as shown in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, bus <b>1405</b> also couples electronic system <b>1400</b> to a network <b>1465</b> through a network adapter (not shown). In this manner, the computer can be a part of a network of computers (such as a local area network (&#x201c;LAN&#x201d;), a wide area network (&#x201c;WAN&#x201d;), or an Intranet, or a network of networks, such as the Internet. Any or all components of electronic system <b>1400</b> may be used in conjunction with the invention.</p><p id="p-0109" num="0108">Some embodiments include electronic components, such as microprocessors, storage and memory that store computer program instructions in a machine-readable or computer-readable medium (alternatively referred to as computer-readable storage media, machine-readable media, or machine-readable storage media). Some examples of such computer-readable media include RAM, ROM, read-only compact discs (CD-ROM), recordable compact discs (CD-R), rewritable compact discs (CD-RW), read-only digital versatile discs (e.g., DVD-ROM, dual-layer DVD-ROM), a variety of recordable/rewritable DVDs (e.g., DVD-RAM, DVD-RW, DVD+RW, etc.), flash memory (e.g., SD cards, mini-SD cards, micro-SD cards, etc.), magnetic and/or solid state hard drives, read-only and recordable Blu-Ray&#xae; discs, ultra density optical discs, any other optical or magnetic media, and floppy disks. The computer-readable media may store a computer program that is executable by at least one processing unit and includes sets of instructions for performing various operations. Examples of computer programs or computer code include machine code, such as is produced by a compiler, and files including higher-level code that are executed by a computer, an electronic component, or a microprocessor using an interpreter.</p><p id="p-0110" num="0109">While the above discussion primarily refers to microprocessor or multi-core processors that execute software, some embodiments are performed by one or more integrated circuits, such as application specific integrated circuits (ASICs) or field programmable gate arrays (FPGAs). In some embodiments, such integrated circuits execute instructions that are stored on the circuit itself.</p><p id="p-0111" num="0110">As used in this specification, the terms &#x201c;computer&#x201d;, &#x201c;server&#x201d;, &#x201c;processor&#x201d;, and &#x201c;memory&#x201d; all refer to electronic or other technological devices. These terms exclude people or groups of people. For the purposes of the specification, the terms display or displaying means displaying on an electronic device. As used in this specification, the terms &#x201c;computer readable medium,&#x201d; &#x201c;computer readable media,&#x201d; and &#x201c;machine readable medium&#x201d; are entirely restricted to tangible, physical objects that store information in a form that is readable by a computer. These terms exclude any wireless signals, wired download signals, and any other ephemeral or transitory signals.</p><p id="p-0112" num="0111">While the invention has been described with reference to numerous specific details, one of ordinary skill in the art will recognize that the invention can be embodied in other specific forms without departing from the spirit of the invention. For instance, a number of the figures conceptually illustrate processes. The specific operations of these processes may not be performed in the exact order shown and described. The specific operations may not be performed in one continuous series of operations, and different specific operations may be performed in different embodiments. Furthermore, the process could be implemented using several sub-processes, or as part of a larger macro process.</p><p id="p-0113" num="0112">Also, while the excess capacity harvesting agents are deployed on machines executing on host computers in several of the above-described embodiments, these agents in other embodiments are deployed outside of these machines on the host computers (e.g., on hypervisors executing on the host computers) on which these machines operate. Therefore, one of ordinary skill in the art would understand that the invention is not to be limited by the foregoing illustrative details, but rather is to be defined by the appended claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of optimizing deployment of containerized applications across a plurality of virtual private clouds (VPCs) defined in a set of one or more datacenters, the method comprising:<claim-text>at a set of one or more global controllers:<claim-text>collecting operational data from each of a plurality of cluster controllers each of which is associated with one VPC and is responsible for deploying containerized applications in its associated VPC;</claim-text><claim-text>analyzing the operational data to identify modifications to the deployment of one or more containerized applications in one or more VPCs;</claim-text><claim-text>producing a recommendation report for display to present the identified modifications as recommendations to an administrator of the plurality of VPCs.</claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the containerized applications execute on machines operating on host computers in the set of datacenters.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the identified modifications comprise moving a group of one or more containerized applications in a first VPC from a larger, first set of machines to a smaller, second set of machines.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the first and second sets of machines include machines that are in both sets of machines but the second set of machines has fewer machines.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the second set of machines is a smaller subset of the first set of machines.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the second set of machines is not just a subset of the first set of machines as the second set of machines includes at least one machine not in the first set of machines.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein moving to the smaller, second set of machines reduces a cost of the deployment of the containerized applications by using less deployed machines to execute the containerized applications.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing operational data comprises:<claim-text>identifying possible migrations of each of a group of containerized applications to new candidate machines for executing containerized application;</claim-text><claim-text>for each possible migration, using a costing engine to compute a cost associated with the migration;</claim-text><claim-text>using the computed costs to identify the possible migrations that should be recommended;</claim-text><claim-text>including in the recommendation report each possible migration that is identified as a migration that should be recommended.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the computed costs are used to calculate different output values of a cost function, with each output value associated with a different deployment of the group of containerized applications, and</claim-text><claim-text>using the computed costs comprises using the calculated output values of the cost function to identify the possible migrations that should be recommended.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the computed costs comprise financial costs for deploying a set of containerized applications in at least two different public clouds.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the two different public clouds are operated by two different public cloud providers.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein<claim-text>analyzing operational data comprises identifying possible adjustments to resources allocated to each of a group of containerized applications;</claim-text><claim-text>producing a recommendation report comprises generating a recommended adjustment to at least a first allocation of a first resource to at least a first container/Pod on which a first container application executes.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein collecting operational data comprises collecting time-series operational data stored in databases used by the cluster controllers to store different sets of operational data collected at different times from host computers on which the containerized applications operate in each VPC.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref> further comprising:<claim-text>receiving user input accepting a recommended migration of a first containerized application from a first machine in a first VPC to a second machine in the first VPC;</claim-text><claim-text>directing a first cluster controller set of the first VPC to direct he migration of the first containerized application.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the first and second machines execute on first and second host computers respectively.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first and second machines execute on a same host computer.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing operational data comprises:<claim-text>identifying possible migrations of each of a group of containerized applications to new candidate machines for executing containerized application;</claim-text><claim-text>including in the recommendation report one or more of the possible migrations along with a financial cost saving associated with each possible migration.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein analyzing operational data comprises:<claim-text>identifying adjustment to allocations of resources of a set of host computers to each of a group of containerized applications executing on the set of host computers;</claim-text><claim-text>including in the recommendation report one or more of the identified resource allocation adjustments along with a cost associated with each identified resource allocation adjustment.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory machine readable medium storing a program that when executed by at least one processing unit of a computer optimizes deployment of containerized applications across a plurality of virtual private clouds (VPCs) defined in a set of one or more datacenters, the program comprising sets of instructions for:<claim-text>collecting operational data from each of a plurality of cluster controllers each of which is associated with one VPC and is responsible for deploying containerized applications in its associated VPC;</claim-text><claim-text>analyzing the operational data to identify modifications to the deployment of one or more containerized applications in one or more VPCs;</claim-text><claim-text>producing a recommendation report for display to present the identified modifications as recommendations to an administrator of the plurality of VPCs.</claim-text></claim-text></claim></claims></us-patent-application>