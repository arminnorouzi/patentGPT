<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005228A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005228</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17944796</doc-number><date>20220914</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20200101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>169</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20130101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20180101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>451</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>006</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>40</main-group><subgroup>169</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>0482</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>13</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>11</main-group><subgroup>60</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>17</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180201</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>453</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUGMENTED REALITY SYSTEM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17146589</doc-number><date>20210112</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11461981</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17944796</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16534756</doc-number><date>20190807</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10922893</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17146589</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15146398</doc-number><date>20160504</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10431005</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16534756</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62157234</doc-number><date>20150505</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>PTC, Inc.</orgname><address><city>Boston</city><state>MA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Parfenov</last-name><first-name>Vladimir</first-name><address><city>Hopkinton</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Jordan</last-name><first-name>Kevin Elliott</first-name><address><city>Cape Elizabeth</city><state>ME</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Dertien</last-name><first-name>Steven Thomas</first-name><address><city>Holliston</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Baum</last-name><first-name>Moshe Jacob</first-name><address><city>Carlisle</city><state>MA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Gosselin</last-name><first-name>Andre</first-name><address><city>Montreal</city><country>CA</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>Prideaux-Ghee</last-name><first-name>Stephen</first-name><address><city>Bristol</city><country>GB</country></address></addressbook></inventor><inventor sequence="06" designation="us-only"><addressbook><last-name>Heppelman</last-name><first-name>James E.</first-name><address><city>Needham</city><state>MA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An example augmented reality system includes: obtaining information about an instance of a device; recognizing the instance of the device based on the information; selecting a digital twin for the instance of the device, with the digital twin being unique to the instance of the device; and generating augmented reality content based on the digital twin and an actual graphic of the instance of the device.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="72.22mm" wi="158.75mm" file="US20230005228A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="169.59mm" wi="140.55mm" orientation="landscape" file="US20230005228A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="232.16mm" wi="123.95mm" orientation="landscape" file="US20230005228A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="221.83mm" wi="140.46mm" orientation="landscape" file="US20230005228A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="197.61mm" wi="165.95mm" orientation="landscape" file="US20230005228A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="198.29mm" wi="100.50mm" file="US20230005228A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="118.53mm" wi="134.54mm" file="US20230005228A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="222.59mm" wi="146.47mm" file="US20230005228A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="203.03mm" wi="147.57mm" file="US20230005228A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="213.70mm" wi="153.33mm" file="US20230005228A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="128.95mm" wi="162.48mm" file="US20230005228A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="215.05mm" wi="160.36mm" file="US20230005228A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="211.67mm" wi="161.12mm" file="US20230005228A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="217.34mm" wi="161.63mm" file="US20230005228A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="233.51mm" wi="156.97mm" file="US20230005228A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="222.00mm" wi="160.78mm" file="US20230005228A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="218.19mm" wi="164.17mm" file="US20230005228A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="209.38mm" wi="138.43mm" orientation="landscape" file="US20230005228A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="166.12mm" wi="135.89mm" orientation="landscape" file="US20230005228A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="211.33mm" wi="143.00mm" orientation="landscape" file="US20230005228A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">The benefit of priority is hereby claimed to U.S. Provisional Application No. 62/157,234, entitled &#x201c;Augmented Reality System&#x201d;, which was filed on May 5, 2015. U.S. Provisional Application No. 62/157,234 is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">This specification relates generally to an augmented reality system.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Augmented reality (AR) is produced by superimposing computer-generated graphics onto real-world images or video.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0005" num="0004">This specification describes examples of systems, methods, and apparatus, including computer program products, for generating AR content, and for using that AR content to improve the human-machine interaction.</p><p id="p-0006" num="0005">An example process includes: generating a declarative model of a device, where the declarative model includes annotations defining attributes of the device; compiling the declarative model to produce a compiled model, where the compiled model includes the annotations; identifying a feature of the device based on an actual graphic of the device; linking information from the compiled model to the feature in the actual graphic of the device based on the annotations; generating augmented reality content based on the linked information from the compiled model and the feature in the actual graphic; and displaying the augmented reality content on a screen of a computing device. The linking operation may include: determining that the annotations describe the feature; and associating the information to the feature based on the comparison.</p><p id="p-0007" num="0006">An example process includes: receiving an actual graphic of a device; and combining the actual graphic with computer graphics to generate augmented reality content for the device, where the computer graphics illustrate, through computer-generated animation, an action to be performed on the device. The example process may include running the computer-generated animation. The combining operation may include: locating, based on a digital twin for the device, a feature of the device on which the action is to be performed; locating the feature on the actual graphic; and superimposing the computer graphics over the actual graphic so that the feature on the computer graphics aligns to the feature on the actual graphic and remains aligned when the computer-generated animation is run. The digital twin may include data about the device in three dimensions. When run, the computer-generated animation may focus-in on the feature and presents the feature from different perspectives based on the data from the digital twin.</p><p id="p-0008" num="0007">An example process includes: identifying a component of a device; generating, on-demand, a digital twin of part of the device proximate to the component, where the digital twin is generated based, at least in part, on sensor readings from the component and information about the device available from one or more sources, and where the digital twin includes information that models a structure and function of the part of the device and the component; generating augmented reality content for the part and the component based on at least one of the information in the digital twin or an actual graphic of the part and the component; and displaying the augmented reality content on a display device. The part of the device may be less than an entirety of the device, and the part of the device may be within a threshold distance of the component. All of the process may be performed on a mobile device. At least part of the process may be performed on a mobile device and at least part of the method may be performed on a computing device that is remote from the mobile device. Displaying may be performed on a mobile device. Operations for identifying the component, generating the digital twin, and generating the augmented reality content may be performed on a computing device that is remote from the mobile device.</p><p id="p-0009" num="0008">An example process includes: obtaining information about an instance of a device; recognizing the instance of the device based on the information; selecting a digital twin for the instance of the device, with the digital twin being unique to the instance of the device; and generating augmented reality content based on the digital twin and an actual graphic of the instance of the device. The information may include one or more of: a Bluetooth address, a media access control (MAC) address, an Internet Protocol (IP) address, a serial number, a quick response (QR) code, a bar code, a radio frequency identifier (RFID), a subscriber identification module (SIM), a subnet address, or global positioning system (GPS) coordinates. Obtaining the information may include querying the device, querying one or more data sources containing the information, or querying both the device and the one or more data sources. Operations for obtaining the information may include recognizing distinctive attributes of the device based on an image of the device and based on stored information about the device.</p><p id="p-0010" num="0009">An example process includes: recognizing an instance of a device; determining a position of a user relative to the instance of the device; selecting a digital twin for the instance of the device, where the digital twin is unique to the instance of the device; and generating augmented reality content based on the digital twin and an actual graphic of the instance of the device, where the augmented reality content is generated based also on the position of the user relative to the instance of the device. Generating the augmented reality content may include: determining the position of the user by determining a distance that the user is from the instance of the device; and including, in the augmented reality content, an amount of the device that is appropriate for the distance. In an example, the farther the user is away from the instance of the device, the more of the device is included in the augmented reality content. In an example, the amount of the device that is included in the augmented reality content changes as the distance changes. In an example, the amount varies between a maximum amount that comprises an entire view of the device and lesser amounts that comprise close-up views of one or more parts of the device.</p><p id="p-0011" num="0010">An example process includes: recognizing an instance of a first device; recognizing an instance of a second device; determining that the first device and the second device are within a threshold proximity of each other; identifying, in response to the determining, a first digital twin for the first device and a second digital twin for the second device; following interaction of the first device and the second device, updating the first digital twin of the first device with information from the second digital twin of the second device, or updating the second digital twin of the second device with information from the first digital twin of the first device; generating augmented reality content based on at least one of the first digital twin or the second digital twin; and displaying the augmented reality content on a display device. At least one of the first device or the second device may be an intelligent device, and operations for at least one of updating the first digital twin or the second digital twin may be performed through direct device communication.</p><p id="p-0012" num="0011">An example process includes: obtaining data representing an entity that is configurable; incorporating content into a graphical representation of the entity; manipulating the content within the graphical representation to produce a graphical representation of a configured entity; generating data representing the configured entity; incorporating the data into a digital twin for the configured entity; and enabling performance a simulation on the configured entity based on the digital twin. Information from the digital twin may be exported to a computer-based simulation system where the simulation is performed, results of the simulation may be received from the computer-based simulation system, and the results may be incorporated into the digital twin.</p><p id="p-0013" num="0012">An example process includes: obtaining data from digital twins of multiple instances of a same type of device, where each digital twin is based, at least in part, on sensor readings from a corresponding instance of the device and information about the device available from one or more sources, and where each digital twin includes information that models a structure and function of each corresponding instance of the device; performing a simulation of a version of the device using the data, where the simulation provides an expected operation of the version of the device, and where the expected operation is predicted based, at least in part, on the data; and outputting results of the simulation for display on a computing device.</p><p id="p-0014" num="0013">An example process includes: obtaining information about an instance of a device; recognizing the instance of the device based on the information; selecting a digital twin for the instance of the device, where the digital twin is unique to the instance of the device; determining, based at least in part on the digital twin, that there has been a change in at least one component of the instance of the device; updating a bill of materials for the device automatically using information from the digital twin to produce an updated bill of materials; and storing the updated bill of materials in computer memory. The example process may also include: identifying an order for the at least one component; determining that a technician bound for the instance of the device is in possession of the at least component; receiving confirmation that the at least one component has been changed in the instance of the device; and updating the digital twin to reflect a change in the at least one component.</p><p id="p-0015" num="0014">An example process includes: receiving sensor readings from an instance of a device; generating a digital twin for the instance of the device based on the sensor readings and based on information about the structure and function of the device obtained from one or more sources; generating augmented reality content based on the digital twin and an actual graphic of the device; presenting the augmented reality content on a display device; updating the digital twin in real-time based at least in part on changes in the sensor readings to produce an updated digital twin; updating the augmented reality content in real-time based on the updated digital twin to produce updated augmented reality content; and presenting the updated augmented reality content in real-time on the display device. Generating the augmented reality content may include: identifying content of the actual graphic based at least in part on annotations relating to the actual graphic; querying the digital twin for the content; and superimposing computer graphics generated at least in part based on the digital twin over content in the actual graphic, where the computer graphics are superimposed based on locations of the content in the actual graphic.</p><p id="p-0016" num="0015">An example process includes: obtaining information about an instance of a device; recognizing the instance of the device based on the information; selecting a digital twin for the instance of the device, where the digital twin is unique to the instance of the device; generating augmented reality content based on the digital twin and an actual graphic of the instance of the device, where the augmented reality content includes computer graphics representing one or more virtual controls for the device; associating the one or more virtual controls with one or more corresponding actual controls on the device that are remotely accessible; receiving data indicating that one or more of the virtual controls have been selected on a screen of a computing device; and in response to receipt of the data, remotely activating one or more of the actual controls that corresponds to one of more of the virtual controls that have been selected. The one or more virtual controls may be part of a control panel that is represented by the computer graphics. The control panel may represent a physical control panel on the device. The control panel may include a computer-generated construct representing actual controls on the device that differs from a physical organization of the controls on the device.</p><p id="p-0017" num="0016">An example process includes: obtaining sensor readings from an instance of a device over a period of time; generating a digital twin for the instance of the device based on the sensor readings and based on information about the device obtained from one or more sources, where the digital twin maintains a record of the sensor readings over the period of time and corresponding operation of the device over the period of time; and diagnosing a problem associated with the device based on data from the digital twin. The sensor readings may include environmental readings. Diagnosing the problem may include: generating augmented reality content for the device based on the digital twin and an actual graphic of the device, where the augmented reality content includes video showing performance of the device over the period of time, and where the video includes computer graphics showing the environmental readings and showing a component of the device having the problem; and analyzing the augmented reality content to diagnose the problem.</p><p id="p-0018" num="0017">An example process includes: generating data for a graphical user interface (UI), where the GUI is configured to display augmented reality content, and where the GUI includes tools for interacting with displayed augmented reality content; outputting the data for display on a screen of a computing device; receiving input from one or more of the tools of the GUI, where the input is for affecting the augmented reality content; updating a digital twin of a device that is a subject of the displayed augmented reality content based on the input, where the digital twin includes information that models a structure and function of the device; and updating displayed augmented reality content based on one or more updates to the digital twin. The input may include mark-ups or text added to the displayed augmented reality content. The input may include instructions to generate, and to display, one or more widgets with the augmented reality content, where the one or more widgets are interactive to obtain information from sensors associated with the device. The example process may include publishing updated augmented reality content to social media.</p><p id="p-0019" num="0018">An example process includes: receiving sensor readings from one or more physical sensors on an instance of a device; generating one or more virtual sensor readings based on the sensor readings from the one or more physical sensors; generating a digital twin for the instance of the device based on the sensor readings and the virtual sensor readings, where the digital twin also is based on information about the structure and function of the device obtained from one or more sources; generating augmented reality content based on the digital twin and an actual graphic of the device, where the augmented reality content includes readings from the virtual sensors or graphical depictions of the virtual sensors; and outputting data to produce a display on a screen of a computing device, with the data representing the augmented reality content. Operations for generating the virtual sensor readings may include determining a value for a physical quantity that is different from physical quantities sensed by the one or more physical sensors.</p><p id="p-0020" num="0019">An example method performed by one or more processing devices includes: generating, by the one or more processing devices, a declarative model of a device, with the declarative model comprising annotations defining attributes of the device; compiling, by the one or more processing devices, the declarative model to produce a compiled model, with the compiled model comprising the annotations; identifying, by the one or more processing devices, a feature of the device based on an actual graphic of the device; linking, by the one or more processing devices, information from the compiled model to the feature in the actual graphic of the device based on the annotations; generating, by the one or more processing devices, augmented reality content based on the linked information from the compiled model and the feature in the actual graphic; and generating and outputting data to display the augmented reality content on a screen of a computing device. The example method may include one or more of the following features, either alone or in combination.</p><p id="p-0021" num="0020">Linking may include determining that the annotations describe the feature; and associating the information to the feature may be based on the determining. The augmented reality content may include computer graphics, with the computer graphics illustrating, through computer-generated animation, an action to be performed on the device. The computer graphics may be superimposed over the actual graphic so that a feature of the computer graphics aligns to the feature on the actual graphic and remains aligned when the computer-generated animation is run. When run, the computer-generated animation focuses-in on the feature and presents the feature from different perspectives based on the linked information from the compiled model. Displaying may be performed on a mobile device, and generating the declarative model, compiling, identifying, linking, and generating the augmented reality content may be performed on a computing device that is remote from the mobile device.</p><p id="p-0022" num="0021">The device may be an instance of a general device and the compiled model may define a digital twin of the instance, with the instance being a unique example of the general device that is differentiated from other instances of the general device. Generating the augmented reality content may include: obtaining information about the instance; recognizing the instance based on the information about the instance; and selecting the digital twin of the instance for generation of the augmented reality content. The information about the instance may include one or more of: a Bluetooth address, a media access control (MAC) address, an Internet Protocol (IP) address, a serial number, a quick response (QR) code, a bar code, a radio frequency identifier (RFID), a subscriber identification module (SIM), a subnet address, or global positioning system (GPS) coordinates. Obtaining the information may include: querying the device, querying one or more data sources containing the information about the instance, or querying both the device and the one or more data sources; or recognizing distinctive attributes of the device based on an image of the device and based on stored information about the device.</p><p id="p-0023" num="0022">The device may be an instance of a general device and the compiled model may define a digital twin of the instance. The example method may include determining, based at least in part on the digital twin, that there has been a change in at least one component of the instance; updating a bill of materials for the device automatically using information from the digital twin to produce an updated bill of materials; and storing the updated bill of materials in computer memory. The device may be an instance of a general device and the compiled model may define a digital twin of the instance. The digital twin may be based on sensor readings obtained from sensors on the device and may be based on information about the structure and function of the device obtained from one or more sources. The example method may include: updating the digital twin in real-time based at least in part on changes in the sensor readings to produce an updated digital twin; updating the augmented reality content in real-time based on the updated digital twin to produce updated augmented reality content; and presenting the updated augmented reality content in real-time on the display device.</p><p id="p-0024" num="0023">An example method performed by one or more processing devices may include recognizing, by the one or more processing devices, an instance of a device; determining, by the one or more processing devices, a position of a user relative to the instance of the device; selecting, by the one or more processing devices, a digital twin for the instance of the device, with the digital twin being unique to the instance of the device; and generating, by the one or more processing devices, augmented reality content based on the digital twin and an actual graphic of the instance of the device, with the augmented reality content being generated based also on the position of the user relative to the instance of the device. The example method may include one or more of the following features, either alone or in combination.</p><p id="p-0025" num="0024">Generating the augmented reality content may include determining the position of the user by determining a distance that the user is from the instance of the device; and including, in the augmented reality content, an amount of the device that is appropriate for the distance. The farther the user is away from the instance of the device, the more of the device may be included in the augmented reality content. The amount of the device that is included in the augmented reality content may change as the distance changes. The amount may vary between a maximum amount that comprises an entire view of the device and lesser amounts that comprise close-up views of one or more parts of the device.</p><p id="p-0026" num="0025">An example method performed by one or more processing devices may include receiving, at the one or more processing devices, sensor readings from one or more physical sensors on an instance of a device; generating, by the one or more processing devices, one or more virtual sensor readings based on the sensor readings from the one or more physical sensors; generating, by the one or more processing devices, a digital twin for the instance of the device based on the sensor readings and the virtual sensor readings, with the digital twin also being based on information about the structure and function of the device obtained from one or more sources; generating, by the one or more processing devices, augmented reality content based on the digital twin and an actual graphic of the device, with the augmented reality content comprising readings from the virtual sensors or graphical depictions of the virtual sensors; and generating data for display on a screen of a computing device, with the data representing the augmented reality content. The example method may include one or more of the following features, either alone or in combination.</p><p id="p-0027" num="0026">Generating the virtual sensor readings may include determining a value for a physical quantity that is different from physical quantities sensed by the one or more physical sensors. The digital twin may maintain a record of the sensor readings over the period of time and corresponding operation of the device over the period of time. The example method may include diagnosing a problem associated with the device based on data from the digital twin. The augmented reality content may include one or more virtual controls with one or more corresponding actual controls on the device that are remotely accessible. The example method may include receiving data indicating that one or more of the virtual controls have been selected on a screen of a computing device; and in response to receipt of the data, remotely activating one or more of the actual controls that corresponds to one of more of the virtual controls that have been selected.</p><p id="p-0028" num="0027">Any two or more of the features described in this specification, including in this summary section, may be combined to form embodiments not specifically described in this specification.</p><p id="p-0029" num="0028">All or part of the methods, systems, and techniques described herein may be implemented as a computer program product that includes instructions that are stored on one or more non-transitory machine-readable storage media, and that are executable on one or more processing devices. Examples of non-transitory machine-readable storage media include, e.g., read-only memory, an optical disk drive, memory disk drive, random access memory, and the like. All or part of the methods, systems, and techniques described herein may be implemented as an apparatus, method, or system that includes one or more processing devices and memory storing instructions that are executable by the one or more processing devices to perform the stated operations.</p><p id="p-0030" num="0029">The details of one or more implementations are set forth in the accompanying drawings and the description below. Other features and advantages will be apparent from the description and drawings, and from the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">DESCRIPTION OF THE DRAWINGS</heading><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an image of a display screen showing AR content.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an image of part of an automobile and a computer graphic generated based on a digital twin of the automobile.</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an instance of an excavator and a computer graphic generated based on a digital twin of the excavator.</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example architecture on which the AR system described herein may be implemented.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flowchart showing an example process for generating AR content.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an image of an instance of a bicycle and its sensors.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIGS. <b>7</b> to <b>9</b></figref> show AR content created for a generator, and displayed on a tablet computing device pointed at the generator.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. <b>10</b> to <b>14</b></figref> show AR content generated for a video of a bicycle.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIGS. <b>15</b> to <b>18</b></figref> show AR content generated for a video of a different bicycle.</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows an image of a loader.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>20</b></figref> shows AR content generated for the loader.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIGS. <b>21</b> to <b>29</b></figref> show AR content comprising frames of a computer-generated animation sequence illustrating how to perform an action on the loader.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>30</b></figref> shows AR content generated at a first distance from an airplane.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>31</b></figref> shows AR content generated at a second distance from the airplane, where the second distance is less than the first distance.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a perspective view of digital AR glasses and the AR content displayed thereon and viewable by a user.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0046" num="0045">Like reference numerals in different figures indicate like elements.</p><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0047" num="0046">Described herein are example implementations of an augmented reality (AR) system. In some examples, AR includes superimposing computer graphics onto real-world image or video (referred to herein as &#x201c;actual graphics&#x201d;). AR content thus includes actual graphics of a device, system, or other entity augmented with computer graphics, such as imagery, animation (video), text, or the like.</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a display of a tablet computing device, which includes AR content <b>100</b>. In this example, AR content <b>100</b> includes an actual graphic of a motorcycle <b>101</b> and computer graphics <b>102</b> that are rendered at appropriate locations over the actual graphic. The actual graphic is typically captured via a camera or other appropriate imaging device. The computer graphics are generated by a computing device, such as a server or tablet computing device, based on information about the device displayed, in this case, the motorcycle. The computer graphics may relate to the device in some way, and may include images, video, text, or any other appropriate information. In the example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the computer graphics includes images showing the inner architecture of motorcycle <b>101</b>, and textual instructions about actions that should be taken with respect to a particular part highlighted relative to the rest of the architecture. In image <b>104</b>, the part is a shock absorber, and the action <b>105</b> is to &#x201c;Check for leaks&#x201d;. In image <b>106</b>, the part is a front wheel brake, and the action <b>107</b> is &#x201c;Calibration needed&#x201d;. The actions to be taken may be based on readings from sensors placed on the actual device and/or based on service history data feed alerts from a server for identified objects.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>32</b></figref> shows an example of digital glasses <b>115</b> that may be used to display AR content <b>117</b>. In this example AR content <b>117</b> is the same as AR content <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. AR content <b>117</b> includes content that a viewer wearing digital glasses <b>115</b> sees through the lenses of the digital glasses when wearing the digital glasses. The AR content may be generated by a computing device in communication with the digital glasses or by one or more processing devices on-board the digital glasses. As explained herein, the AR content may be generated based on information about the device displayed, in this case, the motorcycle. In the example of digital glasses, the real-world content (actual graphic) is transmitted through the lenses of the digital glasses and directly to the user's eye, as is the case with any glasses. The AR content, in this example, includes the computer-generated graphics, which are computer-generated and displayed on the inside of the lens to augment what the user is already viewing, thereby resulting in AR content <b>117</b>. The actual graphic, however, may be captured by the digital glasses, or a camera thereof, and processed in real-time as described herein to position the computer graphics at appropriate locations on real-world scenes that the user is viewing.</p><p id="p-0050" num="0049">Although a tablet computing device and digital glasses have been shown thus far, AR content of the type described herein may be generated on, and displayed on, any appropriate computing device and/or display screen.</p><p id="p-0051" num="0050">Each instance of a device, such as motorcycle <b>101</b>, may also have a digital twin (DT). A DT includes a computer-generated representation of information and parameters (e.g., characteristics) representing the physical device (the Physical Twin or PT) or portions thereof. The DT can be generated by a computing device by binding information and parameters about physical devices to digital information describing those same devices. The information and parameters may originate from, and/or be managed by, systems such as, but not limited to, PLM (product lifecycle management), CAD (computer-aided design), SLM (service level management), ALM (application lifecycle management), CPM (connected product management), ERP (enterprise resource planning), CRM (customer relationship management), and/or EAM (enterprise asset management). The information and parameters can cover a range of characteristics stored, e.g., in a bill of material (BOM) associated with the device (e.g., EBOM&#x2014;engineering BOM, MBOM&#x2014;manufacturing BOM, or SBOM&#x2014;service BOM), the device's service data and manuals, the device's behavior under various conditions, the device's relationship to other device(s) and artifacts connected to the device, and software that manages, monitors, and/or calculates the device's conditions and operations in different operating environments. A DT, like its corresponding PT, may have a lifecycle that can be managed as the DT progresses through various stages. A DT can be generated, modified, instantiated, published, connected, and analyzed; can become intelligent; and can mature, retire, and cease to exist.</p><p id="p-0052" num="0051">In some implementations, a DT can be generated at the time that the PT is manufactured, but before the PT becomes has a network connection (for PTs that are capable of network connection). For example, the AR system may obtain information indicating that a washing machine having a given configuration and a given serial number has been manufactured. The AR system may create, or tag, a DT for the washing machine based on information such as that described herein. Operational information about the instance washing machine, in this example may not be available prior its use; however, that information can be incorporated into the DT as the information is obtained. The DT may be linked to a device, such as the washing machine, after the device is identified or connected to a network.</p><p id="p-0053" num="0052">An instance of a device (or &#x201c;device instance&#x201d;) includes a unique example of a general device that is differentiated from other devices, including those of like type. For example, a motorcycle may have a vehicle identification (ID) number that distinguishes it from all other motorcycles, including those that are the same make and model. As described herein, different types of information may be used to identify the instance of a device. A DT is specific to a device instance. For example, there may be a single DT for each corresponding device instance. In this context, a device is not limited to an individual article, but rather includes any appropriate apparatus, system, software, structure or other entity that can be modeled.</p><p id="p-0054" num="0053">A device's DT is generated based on design data, manufacturing data, and/or any other appropriate information (e.g., product specifications) about the device, which may be generic to all such devices. The DT is also generated based on sensor data that is obtained for the particular instance of the device. In the motorcycle example of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the DT may be generated using data that describes the structure and function of the motorcycle. This data may be obtained from any appropriate public or private database, such as product specifications. The structure and function are common to all motorcycles having the same make, model, etc. as motorcycle <b>101</b>. The DT, however, is also generated based on sensor data, which may be obtained from readings taken from sensors placed on, or near, the instance of the device (e.g., motorcycle <b>101</b>). Since that sensor data is unique to motorcycle <b>101</b>, the DT for motorcycle <b>101</b> will be unique relative to DTs for other motorcycles, including those that are identical in structure and function.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example of an actual graphic <b>201</b> of part of a car and a three-dimensional (3D) graphical model <b>202</b> generated from the car's DT. The model may be generated using computer software, such as a CAD tool, and may be rendered on the display of any appropriate computing device. <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows a model <b>300</b> based on a DT of an excavator <b>301</b> rendered on a display <b>302</b> of a desktop computer. As noted, the DT is created based, in part, on sensor data. Sensors may be placed at different locations interior and/or exterior to a device instance (e.g., motorcycle <b>101</b>, car <b>201</b>, excavator <b>301</b>). Sensor readings are obtained from those sensors, and are processed by a computing device to identify information about the device, such as its operational characteristics, environment, and the like. This information is incorporated into the DT for that device, resulting in a DT representing an operation and status that reflects that operation and status of the actual device instance. For example, in the case of excavator <b>301</b>, model <b>300</b> may simulate, on-screen, operation and status of the excavator <b>301</b> and may be used to generate additional information using virtual sensing (described herein), such as calculated force reactions on mechanism joints. The operation may be represented, on-screen, by computer animation depicting the motion of the excavator. The status may include, for example, the state of various parts on the excavator, such as the amount of wear of the tread on tracks <b>304</b>. Generally, any information that can be obtained from, or derived from, sensor readings on an actual device instance can be incorporated into the DT for that device instance.</p><p id="p-0056" num="0055">The DT for a device instance may have numerous uses including, but not limited to, generating AR content for the device. For example, the example AR system described herein may superimpose computer graphics that are based on, or represents, the DT or portions thereof onto an actual graphic of a device instance. Example processes performed by the example AR system identify an instance of a device, generate AR content for the device using the DT for that device, and use that AR content in various ways to facilitate human-machine interaction with the device, including real-life, real-time interaction between the user and the device. Additional processes performed by the example AR system are also disclosed.</p><p id="p-0057" num="0056"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example architecture <b>400</b> on which the AR system, and its processes described herein, may be implemented. The AR system, however, is not limited to use with this architecture, and may be implemented on any appropriate architecture.</p><p id="p-0058" num="0057">In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, AR system <b>400</b> includes a front end <b>401</b> and a back end <b>402</b>. Front end <b>401</b> includes one or more mobile computing devices (or simply, mobile devices). A mobile device may include any appropriate device capable of displaying digital imagery including, but not limited to, digital (AR) glasses, a smartphone, a digital camera, a tablet computing device, and so forth. A mobile device <b>404</b> may include one or more processing devices <b>405</b> (e.g., microprocessors) and memory <b>406</b> storing instructions <b>407</b> that are executable by the one or more processing devices. The instructions are part of one or more computer programs that are used to implement at least part of the AR system. For example, the instructions may be part of an application (or &#x201c;app&#x201d;) that performs operations including, for example, displaying AR content to a user. Mobile device <b>404</b> also includes one or more sensing mechanisms, such as a camera for capturing actual graphics, such as real-world images and video. Mobile device <b>404</b> may also be connected to, and accessible over, a wireless network, such as a long term evolution (LTE) network or a Wi-Fi network.</p><p id="p-0059" num="0058">The subject <b>410</b> of AR content may be any appropriate device, examples of which are described herein. Such devices may be intelligent or not. Intelligent devices typically include one or more embedded processing devices (e.g., microprocessors, embedded controllers, or the like) and memory storing instructions that are executable by the one or more processing devices. The instructions may be part of one or more computer programs that are used to implement device-specific control and communication functions. Intelligent devices may also be connected to, and accessible over, a wireless network.</p><p id="p-0060" num="0059">Back end <b>402</b> includes one or more computing devices <b>412</b>, examples of which include servers, desktop computers, and mobile devices. A back end computing device includes one or more processing devices <b>415</b> (e.g., microprocessors) and memory <b>416</b> storing instructions <b>417</b> that are executable by the one or more processing devices. The instructions are part of one or more computer programs that are used to implement at least part of the AR system. For example, the instructions may be part of a computer program to generate DTs, to analyze DT content, to communicate with other systems <b>420</b> and databases <b>421</b> containing device information, and so forth. A back end computing device may also be connected to, and accessible over, a wired or wireless network. In some implementations, the AR system may not include the back end <b>402</b>, but rather may be implemented solely on the front end.</p><p id="p-0061" num="0060">Front end <b>401</b> and back end <b>402</b> may communicate with each other, and with other systems, such as those described herein, over one or more computer networks, which may include wireless and/or wired networks.</p><p id="p-0062" num="0061">Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in an example process <b>500</b> to produce AR content, a declarative model is generated (<b>501</b>) for a device. The declarative model is generated in computer code, and is compiled (<b>502</b>) to produce a compiled model comprised of intermediate object code that can be read by an AR player. The declarative model, and thus the compiled model, defines the DT, or at least a part of the DT, for the device. In this example, the AR player is executed on the front-end of the AR system, and comprises computer code that is executable to generate AR content based on the compiled model and on an actual graphic of the device. Any appropriate declarative computing language may be used to generate the declarative model based on information available about the device, such as product specifications or other information.</p><p id="p-0063" num="0062">Annotations may be incorporated into the computer code to describe features of the device in the declarative model. In this regard, an annotation may include semantic data that is stored in association with actual design data. In some examples, a graphical model of a device may be annotated to identify, among other things, features and attributes of the device. The model may be annotated to identify locations and functions of features of the device. The annotations may include attributes of those features, such as size, shape, color, etc. Any appropriate techniques may be used to annotate the model. For example, metadata may be associated with specific features in the model. In some implementations, a look-up table (LUT) or other appropriate construct may be used to associate coordinates of the model with corresponding annotations.</p><p id="p-0064" num="0063">In some implementations, the annotations may be computer generated. For example, a computer program may associate annotations with the model based on information about the model obtained from any appropriate source(s). For example, the computer program may store the annotations along with objects in the model or otherwise associate the two using look-up table(s) (LUTs) or other constructs. In some implementations, the annotations may be incorporated manually. For example, users may enter the annotations into a graphic user interface (GUI), and a computer program may associate the annotations with objects in the model. In some implementations, the annotations may be generated using a combination of manual entry and computer-automated techniques. For example, a user may be prompted to manually confirm the result of annotations assigned by the computer to elements of a model.</p><p id="p-0065" num="0064">Take bicycle <b>600</b> of <figref idref="DRAWINGS">FIG. <b>6</b></figref> as an example. In this case, the declarative model includes computer code and annotations that identifies parts of the bicycle, such as its wheels, frame, handlebars, and seat. The declarative model may also include computer code an annotations describing attributes of each of these parts, where attributes includes any appropriate descriptive information. The computer code and annotations may also define attributes of the bicycle as a whole, such as its height, weight, wheel base, color, etc. The declarative model may define the interconnection and function of each of the parts of a device. In the bicycle example, the declarative model may specify that both the back wheel and pedals are connected to a sprocket, and that movement of the pedals causes movement of the sprocket, resulting in movement of the back wheel. The declarative model also includes the locations, identities, and functions of sensors <b>601</b> on the device.</p><p id="p-0066" num="0065">In this regard, sensors are deployed at appropriate locations on an actual device instance (e.g., a PT), such as bicycle <b>600</b> in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The types of sensors deployed may vary depending upon the physical quantity to measure. The sensors sense physical quantities and provide information (e.g., data) about those physical quantities to a computing device, in some cases, in real-time. For example, pressure sensors may be used on the bicycle axles to identify the amount of force applied thereto during use. The computing device may be any appropriate device, such as a smartphone, table, server, and the like. The information may be provided wirelessly directly to the computing device or over any appropriate computer network.</p><p id="p-0067" num="0066">Each sensor <b>601</b> may be tagged and uniquely identifiable, e.g., based on a unique identifier associated therewith that is transmitted along with sensor readings. Information from the sensors is received by the computing device, and is incorporated into the declarative model defining the architecture of the device. For example, the declarative model may include that front and back sensors are positioned at the front and back wheels, respectively, and that those sensors each records a number of rotations-per-minute (RPM) and/or a force applied to each wheel in appropriate units. For example, the declarative model may also indicate the distance between sensors or between features of the device and various sensors. Sensors may be deployed at any appropriate locations interior to, or exterior to, a device, and may be used to sense any appropriate physical quantity. For example, in complex machinery, such as an automobile, sensors may be place on the wheel exteriors, on the seats, on various parts of the automobile engine, on the axle, and so forth. Generally, any appropriate information obtained through, or derived from, sensor readings may be incorporated into the declarative model.</p><p id="p-0068" num="0067">In general, the declarative model may define distances between any appropriate features or physical properties of a device, or even other relationships such as a process that would exist between any two given sensors if that process is applied at the declarative model level.</p><p id="p-0069" num="0068">As indicated above, the declarative model is compiled (<b>502</b>) to produce a compiled model that is readable by, and understandable by, the AR player. The AR player may generate AR content based on the compiled model and based on annotations in the compiled model representing the actual device.</p><p id="p-0070" num="0069">To generate AR content for a device, the AR system links (<b>504</b>) information from the compiled model to corresponding information in the actual graphic of the device, and generates (<b>505</b>) AR content based on the linked information. The AR system outputs (<b>506</b>) data representing the AR content for display on a display screen of a computing device, such as a tablet computing device. By way of example, the AR player may identify objects and their attributes in the actual graphic using object recognition techniques or other appropriate processes. For example, in the bicycle case, the AR player may detect a round object having a particular radius and a black color. The compiled model may be read to locate a feature of the device that has the same attributes as the identified feature. The attributes may be specified in annotations in the model. After such a feature is identified in the compiled model, the AR system determines that two are the same feature. Any appropriate number of attributes may be used to correlate features from the actual graphic to features in the compiled model. In the example provided above, the AR system may determine that the round object having a particular radius and a black color is a front wheel of the bicycle. When that determination is made, the AR system links the information from the compiled model to the feature shown in the actual graphic. For example, the compiled model may contain information describing the make, model, tread, and so forth of the tire. The compiled model may also contain sensor readings, or other information. That information is then linked to the tire in the actual graphic, and used to generate AR content.</p><p id="p-0071" num="0070">The AR player may generate AR content by rendering computer graphics generated from data in the DT over appropriate locations of the actual graphic. For example, the AR player may identify an element of a graphic in the manner described above, obtain information about that graphic from the annotations and/or other information available in the compiled model, and generate the graphic based on information from the compiled model and/or sensor readings.</p><p id="p-0072" num="0071">In some implementations, the computer graphics that form part of the AR content may overlay the same element shown in an actual graphic to enhance or explain an aspect of the element. In some implementations, the computer graphics do not overlay the element, but rather are adjacent to, or reference, the element. For example, text (part of a digital image) may not necessarily overlay parts of an image, but rather may be adjacent to parts relating to the text. In an example, <figref idref="DRAWINGS">FIGS. <b>7</b> to <b>9</b></figref> show AR content <b>701</b> for a generator <b>700</b>. The AR content, in the example of <figref idref="DRAWINGS">FIG. <b>9</b></figref> includes an actual graphic <b>703</b> of the generator and computer-generated graphics <b>704</b> superimposed over parts of the actual graphic.</p><p id="p-0073" num="0072">As noted, the AR content may be generated for an image or video. Thus, the AR content may be static (unchanging) or dynamic (changeable over time). In the case of video, features in frames of video may be identified using appropriate object identification and object tracking technique. The computer graphics portion of the AR content may track movement frame-by-frame of the actual device during playback of the video. The video may be real-time video, although that is not a requirement. In the case of real-time video, the DT may be generated or updated in real-time, and the resulting computer graphics superimposed on frames in real-time. Updating the DT may include changing the declarative model and/or the compiled model, and/or other data used to define the DT.</p><p id="p-0074" num="0073">In some implementations, real-time may not mean simultaneous, but rather may include action that occurs on a continuous basis taking into account delays associated with processing, data transmission, hardware, and the like. For example, in the case of a bicycle, while that bicycle is being ridden, real-time video is captured. Sensor readings from sensors on the bicycle may be received by a computing device and incorporated, in real-time, into the DT for the bicycle, as described herein. The AR player renders appropriate computer graphics in the video that tracks movement of the device (e.g., the bicycle) in the video. An example of AR content generated during use of a bicycle is shown in <figref idref="DRAWINGS">FIGS. <b>10</b> to <b>14</b></figref>.</p><p id="p-0075" num="0074"><figref idref="DRAWINGS">FIGS. <b>10</b> to <b>14</b></figref> show an example of a bicycle <b>1000</b> having sensors deployed thereon. As shown in <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref>, the instance of bicycle <b>1000</b> may be recognized by scanning the bicycle's serial number from QR code <b>1001</b>. As described below, other techniques may be used in lieu of, or in addition to, the QR code scanning depicted in the figures to identify the instance of bicycle <b>1000</b>. In this regard, <figref idref="DRAWINGS">FIGS. <b>10</b> and <b>11</b></figref> show AR content <b>1002</b> (&#x201c;Scan Serial Number&#x201d;) providing instructions on how to recognize bicycle <b>1000</b>. Following recognition, information from the sensors is read by a computing devices, and is incorporated into a DT that defines the instance of the bicycle. Computer graphics that are based on information from the DT are generated and displayed on a display screen. As shown in <figref idref="DRAWINGS">FIGS. <b>12</b> to <b>14</b></figref>, the resulting AR content includes sensor readings <b>1004</b>, <b>1005</b>, and so forth that are superimposed on a video showing use of the bicycle <b>1000</b>. As shown, sensor readings <b>1004</b> and <b>1005</b> change in real-time based on readings obtained for the sensors. The AR content may also depict the sensors at their appropriate locations, as shown.</p><p id="p-0076" num="0075">If the foregoing AR content is generated on digital (AR) glasses, the computer graphics are not superimposed on video, but rather computer graphics are generated and displayed on the interior of the glasses' lenses to augment what the viewer is already seeing through the lenses.</p><p id="p-0077" num="0076">The AR system may use information from the DT to determine the locations at which the sensor readings are to be displayed. In the case of a bicycle, for example, the AR system may query the DT for the bicycle to determine that the tires are 26 inches in diameter. The AR system may display a sensor reading for each tire at an appropriate location on an actual graphic (e.g., image or video) of the bicycle. For example, a reading may be positioned 14 inches from the axle of each tire, so that the reading is near to the tire, but does not overlap the tire. Similarly, if the tire is 15 inches in diameter, the AR system may query the DT for the bicycle so that the AR system knows to position the sensor reading at an 8 inch location from the axle rather than the 14 inch location of the larger tire.</p><p id="p-0078" num="0077">In some implementations, the sensors may not all be actual sensors, but rather may include virtual sensors. A virtual sensor is not a physical sensor, but rather a sensor reading that is derived from readings of physical sensors or a computed value that is generated based on information other than readings of physical sensors. In the bicycle example, although there is no actual sensor for sensing the bicycle's center of gravity, that information can be derived from sensor readings taken from actual sensors on the bicycle. Accordingly, in the AR system, the bicycle has a virtual sensor for center of gravity. AR content generated for the bicycle may include computer graphics representing the virtual sensor and readings therefrom. The virtual sensor may be represented in the same manner as the physical sensors in the AR content, or it may include indicia (such as a marking) to distinguish it from the physical sensors.</p><p id="p-0079" num="0078">Derived sensor readings can be defined as attributes of a model of a device, e.g. by annotating a function call. Like physical sensors, virtual sensors may have both inputs and outputs. A virtual sensor input, when rendered in the AR space, may be presented as a rotary dial or radio controls or perhaps some less skeuomorphic representation. On selecting a virtual sensor control in the AR space, a software function is called with an appropriate parameter value. The software function then returns a virtual sensor value, which may be displayed in the AR content. The look of the control may be defined in an AR editor suite, and the connection to the software may be handled in the AR player at runtime.</p><p id="p-0080" num="0079">An aspect of the example AR system described herein involves use of a mobile device (e.g., smartphone, tablet, digital glasses, camera) to display AR content in real-time. For example, in an implementation, a user of a mobile device may capture a video of a device using the device's camera. An application (or &#x201c;app&#x201d;) for the AR player on the mobile device may be executed to implement features of the AR system on the mobile device. In operation, the AR system may recognize an instance of a device that is the subject of an actual graphic, such as video, associate that instance with a corresponding DT, and generate AR content for display on the mobile device in real-time. During real-time operation, the DT of a device instance may be updated continuously based on changing sensor readings, and that updated DT may be used in generating the updated AR content. Example processes for updating the DT in real-time (e.g., continuously, as readings are received) are described herein.</p><p id="p-0081" num="0080">Techniques may be implemented for using sensing mechanisms, such as a digital camera or GPS tracking, to recognize a specific instance of a device, and to automatically select a DT (e.g., a PLM configuration and associated CAD model(s)) associated with the device. For example, a device instance may be an intelligent device that includes a processor and some type of transmitter/receiver. In this case, the AR system may query the device for its identifying information. For example, if the AR system is implemented, in part, on an app on a mobile device, the app may query the device instance directly. If the device instance is on a network, and the AR system is implemented using a remote computing device, the remote computing device may query the device instance over the network. For devices that do not have sufficient intelligence to receive and respond to a query, information contained on the device, or the device itself, may be used to identify the device.</p><p id="p-0082" num="0081">Any appropriate identifying information may be used that uniquely identifies the device. For example, the identifying information may be, or include, any combination of unique or semi-unique identifiers, such as a Bluetooth address, a media access control (MAC) address, an Internet Protocol (IP) address, a serial number, a quick response (QR) code or other type of bar code, a subnet address, a subscriber identification module (SIM), or the like. Tags, such as RFIDs, may be used for identification. For devices that do not move, the identifying information may be, or include, global positioning system (GPS) or other coordinates that defines the location of the device. For devices that do not include intelligence or other specific identifiers (like bar codes or readable serial numbers), unique features of the device may be used to identify the device instance. For example, a database may store information identifying markings, wear, damage, or other distinctive features of a device, together with a unique identifier for the device. A sensing mechanism (e.g., a camera) may capture an image of the device. The AR system may compare that image a stored image. Comparison may be performed on the mobile device or on a remote computer. The result of the comparison may identify the device.</p><p id="p-0083" num="0082">After the device is identified, the DT for that device may be selected (e.g., from memory, a location on a network, or elsewhere). The DT may be used in implementing the AR techniques described herein. In the bicycle example of <figref idref="DRAWINGS">FIGS. <b>15</b> to <b>18</b></figref>, a user may point their mobile device <b>1010</b> at the bicycle and an AR app on the mobile device may identify the instance of the bicycle as described herein. The app may then display video <b>1011</b> of the device using information from the DT for that instance of the device. For example, as shown in <figref idref="DRAWINGS">FIGS. <b>17</b> and <b>18</b></figref>, the AR system incorporates, into video displayed on the mobile device, sensor readings <b>1009</b> using computer graphics. As the video plays (e.g., as the bicycle is ridden), the system intermittently updates those sensor readings. The intermittent updates may be periodic (e.g., at set intervals), continuous, or at random times. Accordingly, by virtue of the example AR system described herein, Information, such as sensor readings, can be shown in the context of the systems, functions and components of a device within a graphical display of the device. In some implementations, the computer graphics may include a dashboard showing multiple readings relating to the device and/or controls for the device or sensors on the device.</p><p id="p-0084" num="0083">AR may also be used to highlight specific components on the device, such as parts in need of replacement, or to display a virtual control panel or dashboard specific to the device to control, or otherwise affect, operation of the device. In some implementations, computer graphics may be incorporated into (e.g., superimposed onto) a graphic of a device in order to illustrate, graphically, textually, and/or through animation, how to perform maintenance, or otherwise interact with, the device. To this end, the AR system identifies an area of interest on the device. The area of interest may be based on a state of the device or component thereof and/or it may be based on a status of the user. Regarding the state of the device, the area of interest may include a part of the device that has malfunctioned. Regarding the status of the user, the area of interest may be an area to which the user has some interest or relationship. For example, if the user logs into the AR system as a technician, the system may know to identify parts of the device that require repair. That is, the technician may log into the system and, based on the technician's identity, the system may determine that the technician has a particular specialty. In that instance, the AR system may generate AR content showing, or other otherwise prominently featuring, only those components that relate to the technician's specialty. For example, if the technician's specialty is braking systems, the AR system may generate AR content showing, or other otherwise prominently featuring, only those components of a device instance that relate to its braking system.</p><p id="p-0085" num="0084">Other information may also be used to by the AR system to identify an area of interest on the device. For example, a task to perform may be used to identify the area of interest. In the technician example above, if the technician's specialty is braking, but the technician is on step #4 of a procedure (and the AR system knows this), the AR system may highlight, or otherwise display in computer graphics, only the specific information or elements of a device that are relevant to step #4.</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows an example of a loader <b>1012</b> to illustrate use of AR content to show how to perform maintenance on a device. The AR system identifies the instance of the loader using any appropriate technique, such as those described herein. In the example of <figref idref="DRAWINGS">FIG. <b>20</b></figref>, the AR system obtains the DT for the loader and uses the DT for the loader to generate computer graphic imagery <b>1014</b> to superimpose onto an image of the loader. In this example, the image of the loader is a &#x201c;live&#x201d; image of the loader viewed by a camera, glasses, or the like.</p><p id="p-0087" num="0086">In the example of <figref idref="DRAWINGS">FIGS. <b>21</b> to <b>29</b></figref>, the AR system obtains the DT for the loader and uses the DT for the loader to generate computer graphic animation to superimpose onto an image of the loader. Any appropriate animation may be generated. In this example, the animation shows a guided, step-by-step process for removing conduits within the loader. Referring to <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the animation begins at an image of the entire loader, and shows system-level components of the loader. The animation then focuses-in on the area of interest (<figref idref="DRAWINGS">FIG. <b>23</b></figref>). The animation includes step-by-step operations (of which <figref idref="DRAWINGS">FIGS. <b>25</b> to <b>28</b></figref> are examples) showing how to remove conduits <b>1015</b> and <b>1016</b>. As shown, at some point, the computer animation is displayed, but not the actual image. Since the DT for the loader includes information that models the loader in three dimensions (3D), the animation may enable viewing of the loader, and components thereof, from different angles, as shown in <figref idref="DRAWINGS">FIG. <b>29</b></figref>. Accordingly, different perspectives may be generated depending upon which is most appropriate for the task at hand. Although not shown in <figref idref="DRAWINGS">FIGS. <b>21</b></figref> to <b>29</b>, the animation may include textual instructions or computer-generated or recorded audio to enhance the guided service or repair experience (in this case, conduit removal). In the example of <figref idref="DRAWINGS">FIGS. <b>21</b> to <b>29</b></figref>, the AR content is generated on a mobile device; however, the AR content may be generated using any appropriate local and/or remote computing device(s).</p><p id="p-0088" num="0087">The AR system generates the AR content in the loader example by linking information from the DT for the loader to features an actual graphic for the loader (which may be a live view of the loader). For example, the AR player may recognize features of the loader using the techniques described herein, interrogate the DT to identify information about those features and to identify locations of those same features on the actual graphic. In this example, the locations at which the animation is to begin, and the locations through which the animation is to proceed, are identified based on information in the DT. In some implementations, the actual graphic of the loader is tagged with semantic data that identifies visual elements that are used to identify and to lock onto physical elements in an AR scene. Accordingly, scene or frame annotations thus may be used identify where part-relative placement computer graphics is to occur.</p><p id="p-0089" num="0088">As shown in the loader example, in some cases, only a specific area of a device may be of concern. Accordingly, in some implementations, the AR system may focus only on a specific area of a device, and generate AR content for that area only. In this regard, a device may contain numerous components, data sources, and potential scenarios in which some insights may be needed more than others. However, it is not always practical to create and to maintain, or possible to predict, a DT for all possible components, data sources, scenarios, or combinations thereof. Accordingly, in some implementations, the AR system or any other appropriate system, user, or process is configured to generate DTs on-demand and in real-time. Those DTs are generated only for subset(s) or systems of a device that are required to represent and to investigate a given scenario in a given context. So, for example, with a large system like an airplane, a DT can be generated for only a component of interest in the airplane, such as its left wing, and that DT may be used to generate AR content that relates only that component. In some cases, real-time, on-demand DTs are generated for an entire device as well.</p><p id="p-0090" num="0089">In an example implementation, the AR system identifies a relevant component of the device that is needed to analyze a given scenario. This information can be obtained using any appropriate methods. For example, the information can be obtained directly from the device (e.g., by querying the device), where appropriate. This information can also be obtained from a remote management system, such as a CPM system, which may identify the relevant components in response to a query, appropriate analytical techniques, or combinations thereof. Even without a supporting system such as CPM, in some cases a front-end client, such as a tablet computer, digital (AR) glasses, or other connected mobile device, can provide the relevant data directly, including, e.g., identification information, CAD data, device sensor output data, and so forth. The AR system generates, in real-time, an ad-hoc DT that contains data for only aspects of the device that are relevant to the analysis at hand. For example, the DT may represent a part of the device proximate to the component and the component itself. Example processes for generating the DT are described herein. The AR system generates AR content for the aspects of the device that are relevant. The generated AR content may be part of a real-time (e.g., live) or recorded sequence, as described herein.</p><p id="p-0091" num="0090">In different implementations, a local computing device (e.g., a front end device) is used to render AR content and a remote computing device (e.g., a back end device), if any included in the AR system, may each have different capabilities. The capabilities of these different devices may dictate where a DT, and thus AR content, is generated. For example, the DT and AR content may be generated locally; the DT and AR content may be generated remotely and only displayed locally; or the DT and AR content may be generated using a combination of local and remote processing resources. In some implementations, the local computing device may have no onboard sensing capability and be capable only of external monitoring; in some implementations, the local computing device may include basic onboard sensing and no processing capability; in some implementations, the local computing device may include onboard sensing and basic processing capability; and in some implementations, the local computing device may include onboard sensing and processing capability equivalent at least to that of a desktop computer. In some implementations, there may be no remote computing device, but rather only mobile-to-mobile device connection; in some implementations, the remote computing device may be capable of only signal exchange, but not processing; in some implementations, the remote computing device may be capable of device and data management, basic processing, and routing to integrated peripheral systems; and in some implementations, the remote computing device may be capable of advanced servicing and data processing.</p><p id="p-0092" num="0091">Depending on the combined processing power of the local and remote computing devices in a given scenario, DT content can be populated on-demand without requiring significant amounts of provided/identified data. For example, a direct connection may exist between intelligent devices that include their own on-board processing resources and dataset to generate a digital representation. Using direct communication, such devices can create a DT locally, which can be used to analyze, on-demand, an interaction between devices. For example, an intelligent air pump may interact with an intelligent vehicle that has knowledge of its payload distribution and expected terrain conditions. The vehicle may communicate to the air pump an amount of air pressure to apply to each wheel to compensate for the payload and terrain. The DTs for both the air pump and the vehicle may be updated in real-time based on the interaction, e.g., to update the air pressure in the wheels of the vehicle DT and to update the use history of the air pump in the air pump's DT. DT updates may occur, e.g., via communications between processing devices on the intelligent devices and/or the AR system. In another example, a smart torque wrench may be in proximity to an airplane. A DT may be generated on-the-fly for a relevant portion of the airplane and for the smart torque wrench. Following use of the smart torque wrench on a bolt of the airplane, the DTs for both the airplane and torque wrench may be updated to reflect the use. For the airplane, its DT may specify the amount of torque of the bolt; and, for the torque wrench, its DT may specify that the wrench was used on the airplane and that it is now partially stripped, for example. In another example, a user holding a smart device can provide two relatively unintelligent devices the required on-demand computing resources to create a temporary DT and to provide insight in addition to what is read from the sensors. In still another example, a remote computing device, such as a server, may include most of the intelligence and processing resources in the AR system, enabling creation of an on-demand DT for identified device(s). For example, the remote computing device may track excavator arm linkages and thereby create an ad-hoc DT representing a subset assembly of the operating arm by processing equivalent CAD models of the identified items. The remote computing device may analyze the motion of that subset assembly and present expected operation results as part of AR content. In this example, excessive accelerations indicate weak damping, implying that damper oil may be leaking from a pump. Typically, AR content may be displayed on the local device or output to other AR clients.</p><p id="p-0093" num="0092">In some implementations, the AR system can determine the relative position or proximity of a user to a device instance (a PT), and the AR system can adjust the level of detail of information presented on AR content based on the relative position or proximity. In some implementations, the relative position or proximity may be determined or inferred, from an image that is captured or it may be determined directly based on GPS or other signals that are indicative of the distance between the user and the device. In the airplane example above, if the user's mobile device captures a graphic (e.g., image or video) of a majority of the airplane, the AR system may infer that the user is far from the airplane and, therefore, the AR content generated by the AR system on the mobile device includes high-level information for the entire airplane. <figref idref="DRAWINGS">FIG. <b>30</b></figref> shows an example of AR content <b>1020</b> that may be generated in this circumstance. The AR content in this example includes an actual image of an airplane <b>1021</b>, a computer graphic <b>1022</b> showing internal components of the airplane at their appropriate locations, and textual instructions <b>1023</b> defining an action to be taken on the airplane. If user's mobile device captures or zooms-in on a part of the airplane, the AR system infers that the user is closer to the airplane and, in particular, close to that part. Therefore, the AR content that the system generates includes lower-level information for that part only, or at least for the part that is displayed. That lower-level information may be more detailed than the higher-level information, and may be specific to the part. For example, in <figref idref="DRAWINGS">FIG. <b>31</b></figref>, the user is closer to the airplane than in <figref idref="DRAWINGS">FIG. <b>30</b></figref>, and has captured an image close to the airplane's cargo hold. Accordingly, the AR content <b>1030</b> generated relates specifically to the cargo hold, in this example. The AR content in this example includes an actual image of an airplane <b>1031</b>, a computer graphic <b>1032</b> showing internal components of the airplane at their appropriate locations, and textual instructions <b>1033</b> defining an action to be taken on the airplane. The AR content may be generated using any of the example processes described herein.</p><p id="p-0094" num="0093">In some implementations, display of the AR content is dynamic. For example, as the distance between the user and the device changes, the AR content may change. In an example, if a user is far away from an airplane, and the user's mobile device is able to capture an image of the entire airplane, the AR content may relate to the entire airplane. As the user moves closer to the airplane, and the image on the user's mobile device focuses-in on a particular area or part of the airplane, the AR content will change in real-time and also focus-in on that particular area or part. The converse occurs where the user starts close and moves further away.</p><p id="p-0095" num="0094">In some implementations, proximity between two device instances can promote interaction between their DTs in the AR system. In the airplane example above, a torque wrench may be within a threshold proximity of a bolt on an airplane. That the two are in proximity may be determined based on a graphic captured, e.g., by a mobile device. For example, the graphic may show the torque wrench and airplane, and the instances thereof may be identified according to the example processes described herein. After data indicating the proximity of the two is incorporated in the DTs for the airplane and the torque wrench, through the AR system, the DT for the airplane can be used to specify, for the DT of the torque wrench, the amount by which to tighten the bolt. That is, the DT for the airplane includes information indicating how tightly the bolt is fixed to the airplane, and the manufacturer's specified tightness. By querying the DT for the airplane, the AR system can determine the difference between actual and specified tightness. The AR system then can incorporate, into the DT of the torque wrench, the amount by which the bolt should be tightened. This information may be displayed in AR content showing the torque wrench and the part of the airplane containing the bolt. If the torque wrench is used to tighten the bolt, the amount by which the bolt was tightened may be incorporated into the DT of the airplane by the AR system. Likewise, the DT for the torque wrench can be updated by the AR system to include this use in its use history, along with other appropriate information. In some implementations, communication between two intelligent devices can cause the DTs for those devices to be updated. In the example above, the torque wrench may communicate, to the airplane, that the torque wrench was used to tighten the bolt to a specified torque. This information may be received by the airplane and, from there, communicated to the AR system to update the airplane's DT. Accordingly, communications between intelligent devices may aid in keeping their DTs up-to-date.</p><p id="p-0096" num="0095">Taking again the above torque wrench example, in some implementations, information about the torque and the bolt may originate from some other system, not technically the DT, but one that provides the same types of information for AR purposes. In an example implementation of this type, there may be three-way communication between, for example, a manufacturing system that specifies torqueing bolts to a specific value, element(s) of the AR system understanding that the wrench and the bolt should be torqued to the specific value, and the DT (e.g., for the wrench and/or device containing the bolt) actually recording what the torque was when it was completed. In some implementations, this could be done using the DT as a proxy for the manufacturing system. The AR system may know if the DT is providing the value or if some other system provides the value. In some implementations, the DT may be programmed with code so that the DT knows its identity and/or can discover from upstream systems what that identity is.</p><p id="p-0097" num="0096">AR content may be used to invoke operations or commands within the AR system for a device instance. That is, AR content may be used to control an actual device, thereby replacing the traditional machine-human interface with an AR interface. Referring to the generator example of <figref idref="DRAWINGS">FIGS. <b>7</b> to <b>9</b></figref>, computer graphics may be incorporated into an actual graphic (an image) of the generator to produce AR content for the generator. The AR content may be displayed on a mobile device in response to capturing an image of the generator.</p><p id="p-0098" num="0097">The computer graphics may represent controls, such as switches, knobs, keyboards, or the like, located on the interior or exterior of the generator. For example, the computer graphics may include a control panel that represents a physical control panel on the generator or in the interior of the generator. The computer graphics may include a virtual control panel comprising a computer-generated construct representing actual controls on or in the device, but that differs from a physical organization of the controls on the device. In this example, the virtual control panel may organize the controls in any appropriate manner, and its organization may be customized by a user of a device.</p><p id="p-0099" num="0098">The controls may be used to enter commands, such as start, stop, reset, test, update, etc., to control operation of the generator. The AR content shows those controls, and where those controls are located on the generator. Functions associated with the controls are associated with corresponding elements of the computer graphics. For example, the computer graphics may include a start button. The AR system may link operation of the generator to the start button in the AR content such that touching the start button the AR content causes the generator to start. For example, the generator may be on a computer network. Control functions of the generator may be accessible over that network. The AR system may connect to the generator, and its control functions, over the computer network. The AR system may receive a receive a selection of an element on the AR content, match that selection to a corresponding control function on the generator based on a location of the selection on-screen, and instruct the generator, over the computer network, to implement that function. Accordingly, control over a physical device is implemented using AR content representing that physical device.</p><p id="p-0100" num="0099">To summarize the above example, a user's mobile device may capture an image of the generator; AR content thereof may be generated and displayed on the mobile device; the mobile device may receive a selection of a control element depicted in the AR content; the selection may be transmitted to the AR system; and the AR system may control operation of the generator in accordance with the selection. As described herein, information about the generator may be used to update its DT, and thereby update information on the AR content, e.g., in real-time. Any appropriate controls or control functions may implemented using AR, and the techniques described herein are not limited to use with a generator, to controlling a static (e.g., non-movable) device, or to the particular control functions listed.</p><p id="p-0101" num="0100">Data gathered via the AR system may be used for various purposes. For example, the data may be used to produce an &#x201c;as-maintained&#x201d; BOM. Generally, an as-maintained BOM reflects parts included in a most recent configuration of a device. In this regard, devices may be serviced from time-to-time, resulting in replacement of parts on those devices or inclusion of additional parts on the devices. Heretofore, it was required that technicians manually update a BOM for a device in order to keep the BOM up-to-date. Using the AR system described herein, maintenance of the BOM for a device instance can be automated, at least partially.</p><p id="p-0102" num="0101">To this end, the AR system may recognize a device instance using any appropriate technique. The BOM of the device instance may be updated using information from the DT for the device instance. For example, the DT may include information about replacement parts for the device instance, such as their serial numbers, QR codes, and so forth. This information may be obtained using any appropriate techniques, including those described herein. Changes to parts may be detected by comparing differences in identifying information for original and replaced parts. For example, if the AR system detects that the serial number of a particular part has changed, the AR system will instruct that the BOM for the device be updated to reflect that one part has been replaced with another part.</p><p id="p-0103" num="0102">Information about replaced parts may also be used to update the BOM. This information may include identifying information, such as that described herein (e.g., serial number), along with any other relevant information that has changed (e.g., differences in performance information, composition, and so forth). Such information may be obtained from any appropriate source based on the identifying information for the replaced parts. For example, the AR system may identify the serial number of a replaced tire on a bicycle. The AR system may obtain information about that tire, such as its size, tread, composition, or the like, from product specifications or other appropriate sources. That information may be incorporated into the DT for the bicycle automatically, along with identifying information for the replaced part.</p><p id="p-0104" num="0103">In some implementations, the AR system may determine, from one or more other systems such as those described above, that a replacement part has been ordered for a device instance. The AR system may also detect that a technician is in proximity to the device instance based, e.g., on location monitoring software on the technician's mobile device, and that the technician has the replacement part. This information may trigger the AR system to update the device's DT and its BOM to reflect that the replacement part has been included in the device. In some implementations, the AR system may request confirmation from the technician or, if the device is intelligent, from the device itself, if such information is available.</p><p id="p-0105" num="0104">In some implementations, the AR system may be used to generate 3D graphic designs, and corresponding DTs for those designs. For example, the AR system may connect to a PLM system or other appropriate system to obtain graphics. The graphics may be computer-generated 3D graphics, computer-generated two-dimensional (2D) graphics, actual graphics such as images or video, or any other appropriate type of graphic. The graphics may represent a space or device that can be reconfigured digitally. For example, the graphic may include a prototype stage of a vehicle cabin. Through interaction with the AR system, a user may add components to the vehicle cabin, adjust their positions, change their kinematic set-up, change their geometry, and so forth. For example, the AR system may display the graphic on a mobile device, along with a menu that enables the user to select components to incorporate into the display. The user may drag and drop, or otherwise position, the components on the display, and manipulate the components using any appropriate techniques.</p><p id="p-0106" num="0105">At any appropriate point in the design process, using the AR system, the user can initiate a simulation of the reconfigured graphic for a given scenario. For example, the AR system may receive instructions, initiated by the user, to conduct a simulation of a current design under specified conditions. In the case of a vehicle cabin, the simulation may be over recorded loads from previous experiments using that cabin. In response, the AR system generates a DT for the design by incorporating information from DTs for the original graphic and for the components incorporated into the graphic by the AR system through user interaction. The resulting DT models the design, including its architectural and operational aspects. The AR system, or any other appropriate computer-based system, may use information from the DT to simulate the response of the design to specified stimuli, such as the recorded loads noted above. The response to these stimuli may be recorded in the DT, and reported to the user in real-time on their mobile device or other appropriate computing device. In some implementations, digital construct(s) other than a DT may be used to perform the simulation for the design.</p><p id="p-0107" num="0106">When a design provides an appropriate response in the simulation, the AR system stores the manipulations performed in the AR context to produce the design. The manipulations may include, e.g., locations of components, identities of components, ordering of components, changes to component size or structure, and so forth. The AR system reports those manipulations to a CAD system and update/revise associative CAD models of the manipulated objects. The CAD system may use information about the manipulations and components in generating blueprints or other models for a buildable version of the design.</p><p id="p-0108" num="0107">In some implementations, the AR system may be used to replay, through simulation, a historical account of a device based upon input from a user or device interaction. The replay can be streamed into the AR player or other appropriate audio/visual application. In some implementations, the AR system enables additional operations vis-&#xe0;-vis the device, such as status checks, causality analyses, and diagnostic analysis. In an example implementation, one or more sensors on a device records telemetry information from the device, and records ambient and environmental conditions for the device. This information, along with other appropriate information, such as that described herein, is incorporated into the DT for the device. Changes to the DT over time, particularly with respect to telemetry and conditions, are retained, resulting in a historical record of the operation of the device. This information is stored, in association, with time and date information. Accordingly, when a malfunction of the device occurs, information about the conditions at the time of the malfunction can be obtained by querying the DT.</p><p id="p-0109" num="0108">The system (e.g., via the AR player) enables selection of an identified time window of interest, and selection of specific data points of interest, or the selection of a previously recorded indicator of an event such as a malfunction. For example, the system may include one or more options that can be manipulated by the user to select a time of interest, a device of interest, and one or more components of the device. For example, the time of interest may include a time window during which a malfunction of the device occurred or may have occurred. The device of interest may be selected, and components in that device, particularly those relating to the malfunction, may be selected. The AR system generates, and presents, AR content for the device over the time of interest. For example, the AR system may generate an AR video showing operation of the device during the time of the malfunction, operation of the component(s) of interest during that malfunction, and the operating conditions (e.g., temperature, humidity levels, and the like) during that malfunction. For example, the temperature may be displayed in a computer graphic, and the component operation may be displayed using computer animation. The device itself, or portion(s) thereof, may be displayed as a real-world image or video.</p><p id="p-0110" num="0109">By providing AR content of the type described above, the AR system may enable performance of diagnostics or the like to determine the cause of the malfunction. In some implementations, the information from the DT may be used to recreate the malfunction digitally, using computer graphics, thereby providing a user with additional insight into what actually occurred.</p><p id="p-0111" num="0110">In some implementations, information from instances of devices having at least some features in common (e.g., type such as &#x201c;bicycle&#x201d;, model, make, or the like) may be aggregated by the AR system. Information aggregated over time may be used to inform subsequent design decisions. For example, the performance of numerous instances of the same make and model of a device may be used to determine what effect design changes will have over various operating conditions. In the bicycle example used herein, information from numerous instances of a bicycle model may be aggregated over a period of time by the AR system. Because numerous instances of the same bicycle are used, information about a range of operating conditions is available. That information may be incorporated into, and obtained from, DTs for the various bicycle instances. The AR system may query those DTs for that information, and provide that information to a CAD (or other appropriate) computer-based design system. The CAD system may use the information from the DTs of various bicycle instances to inform a simulation of a new bicycle design, and to determine or to predict performance of that new design under the range of conditions for which information is available.</p><p id="p-0112" num="0111">As explained, an app may be used to implement the AR player and/or other functionality in the AR system. The app is thus part of the AR system. In some implementations, the app is executable on a mobile device and enables different stake holders (including end consumers) to build customized AR experiences. The customized AR experiences may be built by executing operations including, but not limited to, navigating device DT information, capturing relevant information as AR widgets, and sharing custom AR views with other users.</p><p id="p-0113" num="0112">The app may include tools or other controls to implement, on a mobile device, any appropriate functionality of the AR system described herein. For example, the app may include tools to recognize and track devices using available techniques, including, e.g., contour recognition based on device CAD data. The app may include tools to access the DT of a device through the AR system. The app may include tools to enable users to select parts from a BOM of a device (e.g., from the DT) directly in an AR view and to navigate to different types of available information about those parts using known web browsing techniques. In some implementations, operations to access information about parts in the BOM can also include navigating through different DT repositories, since individual parts can be managed by separate systems. Device CAD data may enable mapping between a selection of a part in AR view and location of information about that part online.</p><p id="p-0114" num="0113">The app may include tools to enable selection and marking of available information to be incorporated into custom widgets, and to enable positioning of custom widgets in 3D space around the device parts. The widgets may be instructional and/or interactive. For example, widgets may be used to provide repair or operational instructions. Widgets may be selectable to obtain current sensor readings. Widgets may be added or deleted to either add or delete information relating to one or sensors to the augmented reality content. Generally, widgets may be added to provide any appropriate information and functionality. The app may include tools to receive, through a user interface, user-provided comments and mark-ups to AR content, to incorporate those comments and mark-ups into the AR content, and to allow the user to publish the result to social media or elsewhere online (e.g., on the Internet or an intranet).</p><p id="p-0115" num="0114">Computing devices that may be used to implement all or part of the front end and/or back end of the AR system include various forms of digital computers, examples of which include, but are not limited to, laptops, desktops, workstations, personal digital assistants, servers, blade servers, mainframes, and other appropriate computers. Mobile devices that may be used to implement all or part of the front end and/or back end of the AR system include, but are not limited to, personal digital assistants, cellular telephones, smartphones, cameras and other similar computing devices. The computing devices described herein, their connections and relationships, and their functions, are meant to be examples only, and are not meant to limit implementations of the technology described and/or claimed.</p><p id="p-0116" num="0115">All or part of the processes described herein and their various modifications (hereinafter referred to as &#x201c;the processes&#x201d;) can be implemented, at least in part, via a computer program product, e.g., a computer program tangibly embodied in one or more information carriers, e.g., in one or more tangible machine-readable storage media, for execution by, or to control the operation of, data processing apparatus, e.g., a programmable processor, a computer, or multiple computers</p><p id="p-0117" num="0116">A computer program can be written in any form of programming language, including compiled or interpreted languages, and it can be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program can be deployed to be executed on one computer or on multiple computers at one site or distributed across multiple sites and interconnected by a network.</p><p id="p-0118" num="0117">Actions associated with implementing the processes can be performed by one or more programmable processors executing one or more computer programs to perform the functions of the calibration process. All or part of the processes can be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) and/or an ASIC (application-specific integrated circuit).</p><p id="p-0119" num="0118">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read-only storage area or a random access storage area or both. Elements of a computer (including a server) include one or more processors for executing instructions and one or more storage area devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from, or transfer data to, or both, one or more machine-readable storage media, such as mass storage devices for storing data, e.g., magnetic, magneto-optical disks, or optical disks. Machine-readable storage media suitable for embodying computer program instructions and data include all forms of non-volatile storage area, including by way of example, semiconductor storage area devices, e.g., EPROM, EEPROM, and flash storage area devices; magnetic disks, e.g., internal hard disks or removable disks; magneto-optical disks; and CD-ROM and DVD-ROM disks.</p><p id="p-0120" num="0119">Each computing device, such as a tablet computer, may include a hard drive for storing data and computer programs, and a processing device (e.g., a microprocessor) and memory (e.g., RAM) for executing computer programs. Each computing device may include an image capture device, such as a still camera or video camera. The image capture device may be built-in or simply accessible to the computing device.</p><p id="p-0121" num="0120">Each computing device may include a graphics system, including a display screen. A display screen, such as an LCD or a CRT (Cathode Ray Tube) displays, to a user, images that are generated by the graphics system of the computing device. As is well known, display on a computer display (e.g., a monitor) physically transforms the computer display. For example, if the computer display is LCD-based, the orientation of liquid crystals can be changed by the application of biasing voltages in a physical transformation that is visually apparent to the user. As another example, if the computer display is a CRT, the state of a fluorescent screen can be changed by the impact of electrons in a physical transformation that is also visually apparent. Each display screen may be touch-sensitive, allowing a user to enter information onto the display screen via a virtual keyboard. On some computing devices, such as a desktop or smartphone, a physical QWERTY keyboard and scroll wheel may be provided for entering information onto the display screen. Each computing device, and computer programs executed thereon, may also be configured to accept voice commands, and to perform functions in response to such commands. For example, the process described herein may be initiated at a client, to the extent possible, via voice commands.</p><p id="p-0122" num="0121">Elements of different embodiments described herein may be combined to form other embodiments not specifically set forth above. Elements may be left out of the processes, computer programs, user interfaces, etc. described herein without adversely affecting their operation or the operation of the system in general. Furthermore, various separate elements may be combined into one or more individual elements to perform the functions described herein.</p><p id="p-0123" num="0122">Other implementations not specifically described herein are also within the scope of the following claims.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method performed by one or more processing devices, comprising:<claim-text>generating, by the one or more processing devices, a declarative model of a device, the declarative model comprising annotations defining attributes of the device;</claim-text><claim-text>compiling, by the one or more processing devices, the declarative model to produce a compiled model, the compiled model comprising the annotations;</claim-text><claim-text>identifying, by the one or more processing devices, a feature of the device based on an actual graphic of the device;</claim-text><claim-text>linking, by the one or more processing devices, information from the compiled model to the feature in the actual graphic of the device based on the annotations;</claim-text><claim-text>generating, by the one or more processing devices, augmented reality content based on the linked information from the compiled model and the feature in the actual graphic; and</claim-text><claim-text>outputting data to display the augmented reality content on a screen of a computing device.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein linking comprises:<claim-text>determining that the annotations describe the feature; and</claim-text><claim-text>associating the information to the feature based on the determining.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the augmented reality content comprises computer graphics, the computer graphics illustrating, through computer-generated animation, an action to be performed on the device;<claim-text>wherein the computer graphics are superimposed over the actual graphic so that a feature of the computer graphics aligns to the feature on the actual graphic and remains aligned when the computer-generated animation is run; and</claim-text><claim-text>wherein, when run, the computer-generated animation focuses-in on the feature and presents the feature from different perspectives based on the linked information from the compiled model.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein displaying is performed on a mobile device, and generating the declarative model, compiling, identifying, linking, and generating the augmented reality content are performed on a computing device that is remote from the mobile device.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is an instance of a general device and the compiled model defines a digital twin of the instance, the instance being a unique example of the general device that is differentiated from other instances of the general device; and<claim-text>wherein generating the augmented reality content comprises:<claim-text>obtaining information about the instance;</claim-text><claim-text>recognizing the instance based on the information about the instance; and</claim-text><claim-text>selecting the digital twin of the instance for generation of the augmented reality content.</claim-text></claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the information about the instance comprises one or more of: a Bluetooth address, a media access control (MAC) address, an Internet Protocol (IP) address, a serial number, a quick response (QR) code, a bar code, a radio frequency identifier (RFID), a subscriber identification module (SIM), a subnet address, or global positioning system (GPS) coordinates.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the obtaining the information comprises:<claim-text>querying the device, querying one or more data sources containing the information about the instance, or querying both the device and the one or more data sources; or</claim-text><claim-text>recognizing distinctive attributes of the device based on an image of the device and based on stored information about the device.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is an instance of a general device and the compiled model defines a digital twin of the instance, and wherein the method further comprises:<claim-text>determining, based at least in part on the digital twin, that there has been a change in at least one component of the instance;</claim-text><claim-text>updating a bill of materials for the device automatically using information from the digital twin to produce an updated bill of materials; and</claim-text><claim-text>storing the updated bill of materials in computer memory.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the device is an instance of a general device and the compiled model defines a digital twin of the instance, the digital twin being based on sensor readings obtained from sensors on the device and being based on information about the structure and function of the device obtained from one or more sources; and<claim-text>wherein the method further comprises:<claim-text>updating the digital twin in real-time based at least in part on changes in the sensor readings to produce an updated digital twin;</claim-text><claim-text>updating the augmented reality content in real-time based on the updated digital twin to produce updated augmented reality content; and</claim-text><claim-text>presenting the updated augmented reality content in real-time on the display device.</claim-text></claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A method performed by one or more processing devices, comprising:<claim-text>recognizing, by the one or more processing devices, an instance of a device;</claim-text><claim-text>determining, by the one or more processing devices, a position of a user relative to the instance of the device;</claim-text><claim-text>selecting, by the one or more processing devices, a digital twin for the instance of the device, the digital twin being unique to the instance of the device; and</claim-text><claim-text>generating, by the one or more processing devices, augmented reality content based on the digital twin and an actual graphic of the instance of the device, the augmented reality content being generated based also on the position of the user relative to the instance of the device.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein determining a position of a user comprises determining the position of the user by determining a distance that the user is from the instance of the device; and<claim-text>wherein generating the augmented reality content comprises including, in the augmented reality content, an amount of the device that is appropriate for the distance.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein, the farther the user is away from the instance of the device, the more of the device is included in the augmented reality content.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the amount of the device that is included in the augmented reality content changes as the distance changes.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the amount varies between a maximum amount that comprises an entire view of the device and lesser amounts that comprise close-up views of one or more parts of the device.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A method performed by one or more processing devices, comprising:<claim-text>receiving, at the one or more processing devices, sensor readings from one or more physical sensors on an instance of a device;</claim-text><claim-text>generating, by the one or more processing devices, one or more virtual sensor readings based on the sensor readings from the one or more physical sensors;</claim-text><claim-text>generating, by the one or more processing devices, a digital twin for the instance of the device based on the sensor readings and the virtual sensor readings, the digital twin also being based on information about the structure and function of the device obtained from one or more sources;</claim-text><claim-text>generating, by the one or more processing devices, augmented reality content based on the digital twin and an actual graphic of the device, the augmented reality content comprising readings from the virtual sensors or graphical depictions of the virtual sensors; and</claim-text><claim-text>generating data for display on a screen of a computing device, the data representing the augmented reality content.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein generating the virtual sensor readings comprises determining a value for a physical quantity that is different from physical quantities sensed by the one or more physical sensors.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the digital twin maintains a record of the sensor readings over the period of time and corresponding operation of the device over the period of time; and<claim-text>wherein the method comprises diagnosing a problem associated with the device based on data from the digital twin.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the augmented reality content comprises one or more virtual controls with one or more corresponding actual controls on the device that are remotely accessible; and<claim-text>wherein the method further comprises:<claim-text>receiving data indicating that one or more of the virtual controls have been selected on a screen of a computing device; and</claim-text><claim-text>in response to receipt of the data, remotely activating one or more of the actual controls that corresponds to one of more of the virtual controls that have been selected.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>