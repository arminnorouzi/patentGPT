<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005212A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005212</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943775</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2020-110737</doc-number><date>20200626</date></priority-claim><priority-claim sequence="02" kind="national"><country>JP</country><doc-number>2020-110753</doc-number><date>20200626</date></priority-claim><priority-claim sequence="03" kind="national"><country>JP</country><doc-number>2020-110767</doc-number><date>20200626</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20110101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20140101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>525</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>15</main-group><subgroup>20</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>525</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>2300</main-group><subgroup>6661</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e98">INFORMATION PROCESSING DEVICE, INFORMATION PROCESSING METHOD, AND INFORMATION PROCESSING PROGRAM</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17356836</doc-number><date>20210624</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11481962</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17943775</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>GREE, Inc.</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>NISHIDA</last-name><first-name>Ryosuke</first-name><address><city>Tokyo</city><country>JP</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>GREE, Inc.</orgname><role>03</role><address><city>Tokyo</city><country>JP</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">An information processing device for drawing an object arranged in a three-dimensional virtual space, in an expression viewed from a virtual camera, the object including a field object associated with a two-dimensional plane defined by a first axis and a second axis, and a specific object arranged on the field object, the device including: a change processing unit changing a region of the field object falling within a viewing angle of the virtual camera; and a deformation processing unit deforming the field object, in which in a case where the region is changed by the change processing unit, the deformation processing unit makes a deformation mode of the field object different when it is determined that the specific object is positioned in the region after being changed and when it is determined that the specific object is not positioned in the region after being changed.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="61.55mm" wi="112.44mm" file="US20230005212A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="183.13mm" wi="149.78mm" orientation="landscape" file="US20230005212A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="230.55mm" wi="120.23mm" file="US20230005212A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="155.53mm" wi="102.70mm" file="US20230005212A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="219.71mm" wi="144.36mm" file="US20230005212A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="230.12mm" wi="139.70mm" file="US20230005212A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="222.59mm" wi="146.73mm" file="US20230005212A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="232.49mm" wi="145.46mm" file="US20230005212A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="209.63mm" wi="125.31mm" file="US20230005212A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="202.44mm" wi="123.95mm" orientation="landscape" file="US20230005212A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="233.85mm" wi="111.00mm" file="US20230005212A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="237.24mm" wi="115.06mm" file="US20230005212A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="198.88mm" wi="152.65mm" orientation="landscape" file="US20230005212A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="151.89mm" wi="154.43mm" file="US20230005212A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="229.70mm" wi="144.27mm" file="US20230005212A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="237.24mm" wi="116.16mm" file="US20230005212A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="212.85mm" wi="152.74mm" orientation="landscape" file="US20230005212A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="118.96mm" wi="148.76mm" file="US20230005212A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="210.23mm" wi="151.81mm" orientation="landscape" file="US20230005212A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="207.01mm" wi="147.83mm" orientation="landscape" file="US20230005212A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="225.89mm" wi="72.14mm" file="US20230005212A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="136.99mm" wi="147.07mm" file="US20230005212A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="187.62mm" wi="145.46mm" file="US20230005212A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="182.80mm" wi="154.43mm" file="US20230005212A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="230.38mm" wi="151.64mm" orientation="landscape" file="US20230005212A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="194.56mm" wi="153.75mm" file="US20230005212A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="143.93mm" wi="142.16mm" file="US20230005212A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="181.78mm" wi="152.32mm" file="US20230005212A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="236.30mm" wi="145.80mm" file="US20230005212A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="143.09mm" wi="129.71mm" file="US20230005212A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="231.14mm" wi="127.76mm" file="US20230005212A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="139.02mm" wi="132.00mm" file="US20230005212A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="194.06mm" wi="152.99mm" orientation="landscape" file="US20230005212A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="217.93mm" wi="151.05mm" file="US20230005212A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority from U.S. patent application Ser. No. 17/356,836, filed on Jun. 24, 2021, entitled &#x201c;INFORMATION PROCESSING DEVICE, INFORMATION PROCESSING METHOD, AND INFORMATION PROCESSING PROGRAM,&#x201d; which in turn claims priority from Japanese Patent Application No. JP-2020-110737, Japanese Patent Application No. JP-2020-110753, and Japanese Patent Application No. JP-2020-110767, all filed on Jun. 6, 2020, the entire contents of which are hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to an information processing device, an information processing method, and an information processing program.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">An information processing device for drawing an object arranged in a three-dimensional virtual space, in an expression viewed from a virtual camera arranged in the virtual space, has been known.</p><heading id="h-0004" level="1">PRIOR ART DOCUMENT</heading><heading id="h-0005" level="1">Patent Literature</heading><p id="p-0005" num="0004">[Patent Literature 1] JP-A-2013-208269</p><heading id="h-0006" level="1">SUMMARY</heading><p id="p-0006" num="0005">In the related art as described above, it is difficult to generate various expressions according to a relationship between the virtual camera and a specific object (for example, a position relationship).</p><p id="p-0007" num="0006">Therefore, in one aspect, an object of the invention is to generate various expressions according to a relationship between a virtual camera and a specific object.</p><heading id="h-0007" level="1">Solution to Problem</heading><p id="p-0008" num="0007">In one aspect, an information processing device for drawing an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, in an expression viewed from a virtual camera arranged in the virtual space,</p><p id="p-0009" num="0008">the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis, and a specific object arranged on the field object, the device including:</p><p id="p-0010" num="0009">a change processing unit changing a region of the field object falling within a viewing angle of the virtual camera; and</p><p id="p-0011" num="0010">a deformation processing unit deforming the field object,</p><p id="p-0012" num="0011">in which in a case where the region is changed by the change processing unit, the deformation processing unit determines whether or not the specific object is positioned in the region after being changed, and makes a deformation mode of the field object different when it is determined that the specific object is positioned in the region after being changed and when it is determined that the specific object is not positioned in the region after being changed, is provided.</p><p id="p-0013" num="0012">In one aspect, according to the invention, various expressions according to a relationship between a virtual camera and a specific object can be generated.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0008" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of a game system according to this embodiment;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a field image;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a plan view illustrating the entire field surface forming a field object and the entire background surface forming a background object;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a perspective view illustrating a part of the field surface and the background surface;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory diagram illustrating various position relationships;</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic view illustrating an example of the field image obtained by being drawn in an expression viewed from a virtual camera;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an explanatory diagram of an example of a deformation parameter for attaining bending deformation of the field surface;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is an explanatory diagram of the bending deformation of the field surface based on a function;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is an explanatory diagram of an applied scene of the bending deformation of the field surface;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is an (first) explanatory diagram illustrating a relationship between the virtual camera and the bending deformation of the field surface;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is an (second) explanatory diagram illustrating the relationship between the virtual camera and the bending deformation of the field surface;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is an (third) explanatory diagram illustrating the relationship between the virtual camera and the bending deformation of the field surface;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an explanatory diagram of a freedom degree of a change in a position of the virtual camera;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an explanatory diagram of a rotation of a visual line direction of the virtual camera;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an explanatory diagram of a camera parameter;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>11</b>A</figref> is an explanatory diagram of a specific object region or the like;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>11</b>B</figref> is an explanatory diagram of various object regions;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example of a functional block diagram relevant to a drawing function of a server device;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an explanatory diagram of deformation parameter data;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an explanatory diagram of distance parameter data;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is an explanatory diagram of direction parameter data;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a schematic flowchart illustrating a processing flow that is attained by a server control unit;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic flowchart illustrating an example of first distance parameter calculation processing (step S<b>1608</b>);</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is an explanatory diagram of an interpolation processing range;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a schematic flowchart illustrating an example of direction parameter calculation processing (step S<b>1610</b>);</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a schematic flowchart illustrating an example attack angle parameter calculation processing (step S<b>1612</b>);</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a schematic flowchart illustrating an example of deformation processing associated with a movement of a predetermined object (step S<b>1615</b>);</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is an explanatory diagram of bending deformation processing;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a schematic flowchart illustrating first deformation parameter calculation processing (step S<b>2102</b>) of a first deformation parameter calculation unit;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a schematic flowchart illustrating second deformation parameter calculation processing (step S<b>2104</b>) of a second deformation parameter calculation unit;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a schematic flowchart illustrating deformation parameter adjustment processing (step S<b>2106</b>) of a deformation parameter adjustment unit;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a schematic flowchart illustrating an example of origin setting processing (step S<b>2108</b>) of an origin setting processing unit;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>26</b>A</figref> is an explanatory diagram of an internally dividing point Pi;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a schematic flowchart illustrating an example of second distance parameter calculation processing (step S<b>1623</b>);</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is a plan view of the field object;</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>29</b>A</figref> is a diagram illustrating an example of a field image according to a position E<b>1</b>;</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>29</b>B</figref> is a diagram illustrating an example of a field image according to a position E<b>2</b>;</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>29</b>C</figref> is a diagram illustrating an example of a field image according to a position E<b>3</b>;</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>29</b>D</figref> is a diagram illustrating an example of the field image according to the position E<b>3</b> according to another operation example;</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>29</b>E</figref> is a diagram illustrating an example of the field image according to the position E<b>3</b> according to still another operation example;</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is an example of a functional block diagram relevant to a drawing function of a server device according to a modification example;</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>31</b></figref> is an (first) explanatory diagram of a setting method of the interpolation processing range; and</p><p id="p-0056" num="0055"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is an (second) explanatory diagram of the setting method of the interpolation processing range.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0009" level="1">DETAILED DESCRIPTION</heading><p id="p-0057" num="0056">Hereinafter, each embodiment will be described in detail, with reference to the attached drawings.</p><p id="p-0058" num="0057">(Outline of Game System)</p><p id="p-0059" num="0058">The outline of a game system <b>1</b> according to one embodiment of the invention will be described with reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>. <figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of the game system <b>1</b> according to this embodiment. <figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram illustrating an example of a field image. The game system <b>1</b> includes a server device <b>10</b> and one or more terminal devices <b>20</b>. In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, for convenience, three terminal devices <b>20</b> are illustrated, and the number of terminal devices <b>20</b> may be two or more.</p><p id="p-0060" num="0059">The server device <b>10</b>, for example, is an information processing device that is managed by a game operator, such as a server. The terminal device <b>20</b>, for example, is an information processing device that is used by a user, such as a mobile phone, a smart phone, a tablet terminal, a personal computer (PC), or a game device. The terminal device <b>20</b> is capable of executing an application of a game according to this embodiment. The application of the game may be received in the terminal device <b>20</b> from the server device <b>10</b> or a predetermined application distribution server through a network <b>30</b>, or may be stored in advance in a storage device provided in the terminal device <b>20</b> or a storage medium that can be read by the terminal device <b>20</b>, such as a memory card. The server device <b>10</b> and the terminal device <b>20</b> are connected through the network <b>30</b> such that communication can be performed. For example, various processings relevant to the game are executed by cooperation between the server device <b>10</b> and the terminal device <b>20</b>.</p><p id="p-0061" num="0060">Note that, the network <b>30</b> may be a wireless communication network, the internet, a virtual private network (VPN), a wide area network (WAN), a wired network, an arbitrary combination thereof, or the like.</p><p id="p-0062" num="0061">Here, the outline of the game according to this embodiment will be described. The game according to this embodiment, for example, is a role playing game, a simulation game, or the like, and a game content is used, in association with the execution of the game. For example, the game according to this embodiment is a game for moving a game content on a field within a three-dimensional virtual space.</p><p id="p-0063" num="0062">The game content is electronic data that is used in the game, and for example, includes an arbitrary medium such as a card, an item, a point, a currency in a service (or a currency in a game), a ticket, a character, an avatar, and a parameter. In addition, the game content may be game-related information such as level information, status information, game parameter information (a physical power value, an offensive capability, and the like) or capability information (a skill, an ability, a spell, a job, and the like). In addition, the game content is electronic data that can be retrieved, possessed, used, managed, exchanged, synthesized, enhanced, sold, disposed, or bestowed by the user in the game, but a use mode of the game content is not limited to that clearly specified herein.</p><p id="p-0064" num="0063">Hereinafter, unless otherwise stated, the &#x201c;game content possessed by the user&#x201d; indicates a game content associated with a user ID of the user. In addition, &#x201c;applying the game content to the user&#x201d; indicates associating the game content with the user ID. In addition, &#x201c;wasting the game content possessed by the user&#x201d; indicates dissolving the association between the user ID and the game content. In addition, &#x201c;consuming the game content possessed by the user&#x201d; indicates enabling some effects or influences to be generated in the game, in accordance with the dissolution of the association between the user ID and the game content. In addition, &#x201c;selling the game content possessed by the user&#x201d; indicates dissolving the association between the user ID and the game content to associate another game content (for example, a virtual currency, an item, or the like) with the user ID. In addition, &#x201c;assigning a game content possessed by a certain user to another user&#x201d; indicates dissolving the association between a user ID of the certain user and the game content to associate the game content with a user ID of another user.</p><p id="p-0065" num="0064">The game according to this embodiment schematically includes a first game part, a second game part, and a third game part.</p><p id="p-0066" num="0065">In the first game part, the user conducts the game while exploring the field within the virtual space by manipulating a user character. Specifically, the user character is moved on the field, in accordance with the user manipulation. In the field, for example, various areas such as a town and a dungeon are provided, and for example, various events according to the area, such as a conversation with a resident character of the town, and a matchup with an enemy character encountered in the dungeon, occur. The event is executed, and thus, a main story of the game progresses. In addition, in the first game part, for example, in the case of winning the matchup with the enemy character, for example, the game content such as an item, a virtual currency, or a character can be applied to the user. The applied game content, for example, can be used in the third game part described below.</p><p id="p-0067" num="0066">In the second game part, the user changes a possession situation of the game content. The user, for example, collects various game media such as an item, a virtual currency, and a character. Specifically, in a case where the user character is moved in a specific area provided on the field, such as a mine and a fishing hole, or the game content such as a specific character is selected (for example, a touch manipulation with respect to a screen), a sub-event that can be retrieved by the game content occurs. The sub-event, for example, includes the progress of a sub-story, the execution of a mini game, and the like, but the contents of the sub-event are not limited thereto. Various game media can be applied to the user, in accordance with an execution result of the sub-event. The applied game content, for example, can be used in the third game part described below.</p><p id="p-0068" num="0067">In the third game part, the user changes a parameter relevant to the game content. The user, for example, enhances the user character. Specifically, the game content that is applied to the user in the first game part and the second game part, as described above, is consumed, and thus, various game parameters of the user character are changed. The game parameter, for example, includes a level, an HP, an offensive capability, a defensive capability, an attribution, a skill, and the like of the user character, but is not limited thereto. The user character is enhanced in accordance with a change in the game parameter of the user character. Probability that the user character wins the matchup with the enemy character in the first game part increases due to the enhancement of the user character.</p><p id="p-0069" num="0068">As described above, in the game according to this embodiment, the user repeatedly performs the first game part, the second game part, and the third game part.</p><p id="p-0070" num="0069">(Configuration of Server Device)</p><p id="p-0071" num="0070">The configuration of the server device <b>10</b> will be described in detail. The server device <b>10</b> includes a server computer. The server device <b>10</b> may be attained in cooperation with a plurality of server computers.</p><p id="p-0072" num="0071">The server device <b>10</b> includes a server communication unit <b>11</b>, a server storage unit <b>12</b>, and a server control unit <b>13</b>.</p><p id="p-0073" num="0072">The server communication unit <b>11</b> includes an interface that transmits and receives information by performing communication with respect to the external device in a wireless or wired manner. The server communication unit <b>11</b>, for example, may include a wireless local area network (LAN) communication module, a wired LAN communication module, or the like. The server communication unit <b>11</b> is capable of transmitting and receiving information with respect to the terminal device <b>20</b> through the network <b>30</b>.</p><p id="p-0074" num="0073">The server storage unit <b>12</b>, for example, is a storage device, and stores various information items and programs that are necessary for the processing of the game. For example, the server storage unit <b>12</b> stores the application of the game.</p><p id="p-0075" num="0074">In addition, the server storage unit <b>12</b> stores various images (texture images) for projection (texture mapping) with respect to various objects arranged within the three-dimensional virtual space.</p><p id="p-0076" num="0075">For example, the server storage unit <b>12</b> stores an image of the user character. Hereinafter, the user character will be referred to as a first game content, and an object that is drawn (arranged) on a field object (described below), on the basis of an image of the first game content, may be referred to as a first object. Note that, in this embodiment, only one first object is arranged in the virtual space, but two or more first objects may be arranged. Note that, the first object may be a group of a plurality of first game media. In addition, the first game content (and the first object based thereon) that is used in the virtual space may be capable of being suitably exchanged by the user.</p><p id="p-0077" num="0076">In addition, the server storage unit <b>12</b>, for example, stores an image according to the game content, such as a building, a wall, a tree, or a non player character (NPC). Hereinafter, a game content that is an arbitrary game content different from the first game content (for example, a building, a wall, a tree, an NPC, or the like) and can be arranged on the field object described below will be referred to as a second game content, and an object onto which the second game content is projected may be referred to as a second object. Note that, in this embodiment, the second object may include an object that is fixed with respect to the field object described below, an object that is movable with respect to the field object described below, or the like. In addition, the second object may include an object that is perennially arranged on the field object described below, an object that is arranged only in a case where a predetermined condition is satisfied, or the like.</p><p id="p-0078" num="0077">In addition, the server storage unit <b>12</b>, for example, stores an image of the background such as the sky or the distant landscape (a background image). Hereinafter, an object onto which the background image is projected will also be referred to as a background object. Note that, a plurality of types of background images may be prepared, and may be used differently.</p><p id="p-0079" num="0078">In addition, the server storage unit <b>12</b> stores an image of the field (for example, a ground surface) (the field image). The field image is projected onto a field surface described below. Hereinafter, an object in which the field image is projected onto the field surface may be referred to as the field object. Note that, the field object is used as a virtual field (the ground surface) within the virtual space.</p><p id="p-0080" num="0079">Here, as illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, in the field image, for example, a texture coordinate system including a u axis and a v axis that are orthogonal to each other is set. In this embodiment, in the field image, a horizontal passage <b>14</b>, a vertical passage <b>15</b>, and a curved road <b>17</b> are set. The horizontal passage <b>14</b>, the vertical passage <b>15</b>, and the curved road <b>17</b> form a passage in which the first object or the like in the field object can be moved. Note that, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, a specific passage configuration is illustrated, but the passage configuration is arbitrary. In addition, in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the field image is in the shape of a rectangle, and may be in other forms. In addition, a plurality of types of field images may be prepared, and may be used differently.</p><p id="p-0081" num="0080">In addition, the server storage unit <b>12</b> stores correspondence information in which the second object and texture coordinates of the field image are associated. The correspondence information is used by the server control unit <b>13</b> that executes processing of arranging the second object on the field object.</p><p id="p-0082" num="0081">The server control unit <b>13</b> is a dedicated microprocessor or a CPU that attains a specific function by reading a specific program. For example, the server control unit <b>13</b> executes the application of the game, in accordance with the user manipulation with respect to the display unit <b>23</b>. In addition, the server control unit <b>13</b> executes various processings relevant to the game.</p><p id="p-0083" num="0082">For example, the server control unit <b>13</b> displays the field image on which the field object, the first object, and the like are displayed, on the display unit <b>23</b>. In addition, the server control unit <b>13</b> relatively moves the first object on the field object with respect to the field object, within the virtual space, in accordance with a predetermined user manipulation. The details of specific processing of the server control unit <b>13</b> will be described below.</p><p id="p-0084" num="0083">(Configuration of Terminal Device)</p><p id="p-0085" num="0084">The configuration of the terminal device <b>20</b> will be described in detail. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the terminal device <b>20</b> includes a terminal communication unit <b>21</b>, a terminal storage unit <b>22</b>, a display unit <b>23</b>, an input unit <b>24</b>, and a terminal control unit <b>25</b>.</p><p id="p-0086" num="0085">The terminal communication unit <b>21</b> includes an interface that transmits and receives information by performing communication with respect to the external device in a wireless or wired manner. The terminal communication unit <b>21</b>, for example, may include a wireless communication module corresponding to a mobile communication standard such as long term evolution (LTE) (Registered Trademark), a wireless LAN communication module, a wired LAN communication module, or the like. The terminal communication unit <b>21</b> is capable of transmitting and receiving information with respect to the server device <b>10</b> through the network <b>30</b>.</p><p id="p-0087" num="0086">The terminal storage unit <b>22</b>, for example, includes a primary storage device and a secondary storage device. For example, the terminal storage unit <b>22</b> may include a semiconductor memory, a magnetic memory, an optical memory, or the like. The terminal storage unit <b>22</b> stores various information items and programs that are received from the server device <b>10</b> and are used in the processing of the game. The information and the program used in the processing of the game may be retrieved from the external device through the terminal communication unit <b>21</b>. For example, an application program of the game may be retrieved from a predetermined application distribution server. Hereinafter, the application program may be simply referred to as an application. In addition, for example, a part or all of the information relevant to the user described above, information relevant to a game content that is an opponent, and the like may be retrieved from the server device <b>10</b>.</p><p id="p-0088" num="0087">The display unit <b>23</b>, for example, includes a display device such as a liquid crystal display or an organic electro-luminescence (EL) display. The display unit <b>23</b> is capable of displaying various images. The display unit <b>23</b>, for example, includes a touch panel, and functions as an interface that detects various user manipulations.</p><p id="p-0089" num="0088">The input unit <b>24</b>, for example, includes an input interface including a touch panel that is provided integrally with the display unit <b>23</b>. The input unit <b>24</b> is capable of receiving user input with respect to the terminal device <b>20</b>. In addition, the input unit <b>24</b> may include a physical key, or may further include an arbitrary input interface including a pointing device such as a mouse.</p><p id="p-0090" num="0089">The terminal control unit <b>25</b> includes one or more processors. The terminal control unit <b>25</b> controls the operation of the entire terminal device <b>20</b>.</p><p id="p-0091" num="0090">The terminal control unit <b>25</b> transmits and receives information through the terminal communication unit <b>21</b>. For example, the terminal control unit <b>25</b> receives various information items and programs used in the processing of the game from at least one of the server device <b>10</b> and another external server. The terminal control unit <b>25</b> stores the received information and program in the terminal storage unit <b>22</b>.</p><p id="p-0092" num="0091">The terminal control unit <b>25</b> activates the application of the game, in accordance with the manipulation of the user. The terminal control unit <b>25</b> executes the game, in cooperation with the server device <b>10</b>. For example, the terminal control unit <b>25</b> displays various images used in the game (for example, various field images described below) on the display unit <b>23</b>. For example, a graphic user interface (GUI) detecting the user manipulation may be displayed on the screen. The terminal control unit <b>25</b> is capable of detecting the user manipulation with respect to the screen through the input unit <b>24</b>. For example, the terminal control unit <b>25</b> is capable of detecting a tap manipulation, a long tap manipulation, a click manipulation, a swipe manipulation, and the like of the user. The tap manipulation is a manipulation in which the user touches the display unit <b>23</b> with a finger, and then, releases the finger. The terminal control unit <b>25</b> transmits manipulation information to the server device <b>10</b>.</p><p id="p-0093" num="0092">(Drawing Function in Game)</p><p id="p-0094" num="0093">The server control unit <b>13</b> displays the field image on the display unit <b>23</b>, in cooperation with terminal device <b>20</b>, and updates the field image, in accordance with the progress of the game. In this embodiment, the server control unit <b>13</b> draws the object arranged in the three-dimensional virtual space, in an expression viewed from the virtual camera arranged in the virtual space, in cooperation with terminal device <b>20</b>.</p><p id="p-0095" num="0094">Note that, drawing processing described below is attained by the server control unit <b>13</b>, but in another embodiment, a part or all of the drawing processing described below may be attained by the server control unit <b>13</b>. For example, in the following description, at least a part of the field image displayed on the terminal device <b>20</b> may be set to web display in which the field image is displayed on the terminal device <b>20</b>, on the basis of data generated by the server device <b>10</b>, and at least a part of the screen may be set to native display in which the screen is displayed by a native application installed in the terminal device <b>20</b>.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>3</b></figref> and <figref idref="DRAWINGS">FIG. <b>4</b></figref> are explanatory diagrams of an example of the field object and the background object. <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a plan view illustrating the entire field surface <b>70</b> forming the field object and the entire background surface <b>72</b> forming the background object, and <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a perspective view illustrating a part of the field surface <b>70</b> and the background surface <b>72</b> when viewed in an oblique direction including a direction component of an arrow R<b>0</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a virtual camera <b>60</b> is also schematically illustrated. In addition, in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the background surface <b>72</b> is illustrated in the form of the background object onto which the background image including a picture of the cloud or the sun is projected.</p><p id="p-0097" num="0096">In the following description, the movement of various objects indicates a movement within the virtual space. In addition, a visible range of the various objects indicates a range that is visible when viewed from the virtual camera <b>60</b> (that is, a range within a viewing angle <b>62</b> of the virtual camera <b>60</b>).</p><p id="p-0098" num="0097">In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a coordinate system of x, y, and z as a space coordinate system of the virtual space (hereinafter, also referred to as a &#x201c;global coordinate system&#x201d;) is illustrated. Note that, the origin of the global coordinate system may be fixed to an arbitrary position. Hereinafter, a positive side of a z direction is set to the upper side of the virtual space, and a negative side is set to the lower side of the virtual space. Note that, in this embodiment, an x axis is an example of a first axis, a y axis is an example of a second axis, and a z axis is an example of a third axis. Hereinafter, the terms of an x direction, a y direction, and a z direction indicate a direction parallel to the x axis, a direction parallel to the y axis, and a direction parallel to the z axis, respectively. For example, the z direction indicates a direction parallel to the z axis passing through an arbitrary point within an xy plane, unless otherwise stated.</p><p id="p-0099" num="0098">The field surface <b>70</b> is arranged by being associated with the xy plane of the virtual space. In this embodiment, as an example, the field surface <b>70</b> is arranged by being associated with the xy plane such that the u axis, the v axis, and the origin of the texture coordinate system of the projected field image are coincident with the x axis, the y axis, and the origin of the global coordinate system. Note that, in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, as a state before being associated, the u axis, the v axis, and the origin of the texture coordinate system are illustrated by being separated from the x axis, the y axis, and the origin of the global coordinate system. The field surface <b>70</b> is not capable of being subjected to a translational movement (a linear movement), in each direction of the x direction, the y direction, and the z direction. Here, in another embodiment, the field surface <b>70</b> may be capable of being subjected to the translational movement in the global coordinate system.</p><p id="p-0100" num="0099">When a plane parallel to the xy plane is set to a regular state, the field surface <b>70</b> can be deformed from the regular state. As described above, in this embodiment, the field object is shaped on the basis of the deformable field surface <b>70</b>. That is, the field object is shaped on the basis of the field surface <b>70</b> deformed from the regular state, and thus, is deformed with respect to the plane parallel to the xy plane. Hereinafter, the deformation relevant to the field surface <b>70</b> and the field object indicates deformation in a case where the plane parallel to the xy plane is set to a regular shape (state), unless otherwise stated. Note that, the field object in a deformed state may be attained by projecting the field image onto the field surface <b>70</b> in the deformed state, or may be attained by deforming the field surface <b>70</b> after projecting the field image onto the field surface <b>70</b> in the regular state.</p><p id="p-0101" num="0100">Note that, in a case where the field image is projected, in the regular state, the field surface <b>70</b> is capable of inheriting the texture coordinates of the projected field image. That is, each position on the field surface <b>70</b> onto which the field image is projected, substantially, can be specified by the texture coordinate system of the field image (refer to <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Hereinafter, a coordinate system for specifying each of the positions on the field surface <b>70</b> is coincident with the texture coordinate system of the field image projected onto the field surface <b>70</b>, and may be referred to a &#x201c;field coordinate system&#x201d;.</p><p id="p-0102" num="0101">The background surface <b>72</b> extends in the z direction of the background object. Here, in another embodiment, the background surface <b>72</b> may be arranged obliquity with respect to the z direction. In <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the background surface <b>72</b> is arranged to surround the field surface <b>70</b>. In this case, the background surface <b>72</b> may be fixed with respect to the global coordinate system, or may be movable only in the z direction, as described below. Here, in another embodiment, the background surface <b>72</b> may be arranged to surround only a part of the field surface <b>70</b>. In this case, the background surface <b>72</b> may be subjected to a rotation movement, in accordance with the rotation of the virtual camera <b>60</b> described below. In addition, in still another embodiment, as with the field surface <b>70</b>, the background surface <b>72</b> may be deformable.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is an explanatory diagram illustrating various position relationships when a plane including a visual line direction V of the virtual camera <b>60</b> illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref> and the z direction (hereinafter, may be referred to as a &#x201c;Vz plane&#x201d;) is viewed perpendicularly. In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, a first object <b>3</b> positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b> is schematically illustrated. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic view illustrating an example of the field image obtained by being drawn in an expression viewed from the virtual camera <b>60</b>.</p><p id="p-0104" num="0103">In <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the viewing angle <b>62</b> of the virtual camera <b>60</b> (the viewing angle when viewed in a direction perpendicular to the z direction) is schematically illustrated between boundary lines <b>6211</b> and <b>6212</b>. Note that, in this embodiment, the viewing angle of the virtual camera <b>60</b> is constant, and in another embodiment, the viewing angle of the virtual camera <b>60</b> may be variable.</p><p id="p-0105" num="0104">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in the viewing angle <b>62</b>, the boundary line <b>6211</b> on the upper side intersects with the background surface <b>72</b> (refer to a point P<b>2</b>), and the boundary line <b>6212</b> on the lower side intersects with the field surface <b>70</b> (refer to a point P<b>1</b>). In this case, as illustrated in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, a field image G<b>60</b> includes the background surface <b>72</b> (and the background object associated therewith) and the field surface <b>70</b> (and the field object associated therewith). Note that, in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the first object <b>3</b> is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, and thus, the field image G<b>60</b> includes the expression of the first object <b>3</b>.</p><p id="p-0106" num="0105">Here, in this embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the field surface <b>70</b> is subjected to bending deformation such that a virtual horizon line HL (<figref idref="DRAWINGS">FIG. <b>6</b></figref>) is expressed by the field surface <b>70</b> (and the field object associated therewith). Specifically, the field surface <b>70</b> is deformed in a direction toward the lower side as being far away in the visual line direction V (that is, as being directed toward the background surface <b>72</b>). Note that, such deformation may be attained only within the range of the viewing angle of the virtual camera <b>60</b>, or may be attained in the entire field surface <b>70</b>.</p><p id="p-0107" num="0106">In a case where the entire field surface <b>70</b> is deformed, the shape of the field surface <b>70</b> at the time of being cut in the Vz plane (a shape expressed by the line as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>) may be approximately the same in any sectional position (that is, may be an approximately equal sectional surface). Note that, being approximately the same is the concept of allowing an error within 10%. Note that, as described above, the field object is shaped on the basis of the field surface <b>70</b>, and thus, the shape of the field object at the time of being cut in the Vz plane is identical to the shape of the field surface <b>70</b> at the time of being cut in the Vz plane, but the field object may have a shape slightly different from the shape of the field surface <b>70</b> (for example, fine unevenness or the like).</p><p id="p-0108" num="0107">As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the horizon line HL in such expression is formed by an intersection point P<b>3</b> of a tangent line <b>6213</b> with respect to the field surface <b>70</b> from the virtual camera <b>60</b> (a tangent line within the viewing angle <b>62</b>). Note that, in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the first object <b>3</b> is positioned in front of the horizon line HL, and thus, a part of the expression of the horizon line HL is hidden by the first object <b>3</b>. On the contrary, in a case where the first object <b>3</b> is positioned on the deeper side from the horizon line HL (a side close to the background surface <b>72</b>), a part or all of the first object <b>3</b> is hidden by the field object.</p><p id="p-0109" num="0108">Here, a height H<b>1</b> of the horizon line HL (refer to <figref idref="DRAWINGS">FIG. <b>6</b></figref>) depends on an angle &#x3b1; of the tangent line <b>6213</b> with respect to the boundary line <b>6212</b>. The angle &#x3b1; can be changed in accordance with a bending mode of the field surface <b>70</b>. For example, in the case of a field surface <b>70</b>&#x2032; illustrated by a dash-dot-dash line in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, an angle &#x3b1;&#x2032; of a tangent line <b>6213</b>&#x2032; with respect to the boundary line <b>6212</b> is less than the angle &#x3b1;, and thus, the height H<b>1</b> of the horizon line HL decreases (not illustrated). As described above, it is found that the height of the horizon line HL can be changed by changing the bending mode of the field surface <b>70</b>. Note that, in a case where the height of the horizon line HL is changed, the range of the background surface <b>72</b> falling within the viewing angle of the virtual camera <b>60</b> is also changed in accordance with the change.</p><p id="p-0110" num="0109">As described above, in this embodiment, the field surface <b>70</b> is subjected to the bending deformation in a direction toward the lower side as being close to the background surface <b>72</b> when viewed in the visual line direction V, and thus, the horizon line HL can be suitably expressed. In addition, the height H<b>1</b> of the horizon line HL (and a visible range of the field object, the background object, or the like associated therewith) can be freely changed by changing a deformation mode (a deformation degree or the like) of the bending deformation. Hereinafter, the bending deformation in a mode where the field surface <b>70</b> is deformed in the direction toward the lower side as being close to the background surface <b>72</b> when viewed in the visual line direction V may be simply referred to as &#x201c;bending deformation of the field surface <b>70</b>&#x201d;.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is an explanatory diagram of an example of a deformation parameter for attaining the bending deformation of the field surface <b>70</b>.</p><p id="p-0112" num="0111">In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a two-dimensional coordinate system of Xc and Yc within the Vz plane (hereinafter, may be referred to as a &#x201c;local coordinate system&#x201d;) is defined. An XcYc plane is a plane parallel to the Vz plane, a Yc axis is an axis parallel to the z axis, and a positive side of the Yc axis corresponds to the upper side of the virtual space. In <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a function F<b>1</b> for setting the deformation mode of the field surface <b>70</b> is illustrated.</p><p id="p-0113" num="0112">The function F<b>1</b> is a function in which the value of a Yc coordinate monotonically decreases non-linearly as an absolute value of the value of an Xc coordinate increases. In addition, the function F<b>1</b> is symmetrical about the Yc axis. Here, in another embodiment, in the function F<b>1</b>, the value of the Yc coordinate may monotonically decrease linearly as the absolute value of the value of the Xc coordinate increases, and/or the function F<b>1</b> may be asymmetrical about the Yc axis. In this embodiment, as an example, the function F<b>1</b> is a quadratic function, and is represented as described below.</p><p id="p-0114" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>yc=&#x2212;A</i>1&#xd7;(<i>xc</i>)<sup>2 </sup><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0115" num="0113">Here, xc is the value of the Xc coordinate, yc is the value of the Yc coordinate, and A<b>1</b> is a coefficient for setting the deformation degree (hereinafter, will be referred to as a &#x201c;deformation parameter A<b>1</b>&#x201d;).</p><p id="p-0116" num="0114">Note that, the function F<b>1</b> described above is merely an example, and for example, different functions as described below may be used.</p><p id="p-0117" num="0115">When xc&#x3e;a, yc=&#x2212;A<b>1</b>&#xd7;(xc&#x2212;a)<sup>2 </sup></p><p id="p-0118" num="0116">When xc&#x2264;&#x2212;a, yc=&#x2212;A<b>1</b>&#xd7;(xc+a)<sup>2 </sup></p><p id="p-0119" num="0117">When &#x2212;a&#x2264;xc&#x2264;a, yc=0</p><p id="p-0120" num="0118">In this case, a is a positive integer, and a flat plane is attained in a range of &#x2212;a&#x2264;xc&#x2264;a.</p><p id="p-0121" num="0119">Alternatively, when xc&#x3e;&#x2212;a<b>1</b>, yc=&#x2212;A<b>1</b>&#xd7;(xc)<sup>2 </sup></p><p id="p-0122" num="0120">When xc&#x2264;&#x2212;a<b>1</b>, yc=0</p><p id="p-0123" num="0121">In this case, al may be a positive fixed value, and the position of xc=&#x2212;a<b>1</b> may be set to be coincident with an intersection point between the boundary line <b>6212</b> on the lower side of the viewing angle <b>62</b> and the field surface <b>70</b> (the point P<b>1</b> in <figref idref="DRAWINGS">FIG. <b>5</b></figref>).</p><p id="p-0124" num="0122">In addition, in another embodiment, a plurality of types of functions may be prepared, and different functions may be selected in accordance with various conditions.</p><p id="p-0125" num="0123"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is an explanatory diagram of the bending deformation of the field surface <b>70</b> based on the function F<b>1</b>.</p><p id="p-0126" num="0124">The field surface <b>70</b> is deformed in accordance with the function F<b>1</b>. Accordingly, the deformation degree of the field surface <b>70</b> increases as the value of the deformation parameter A<b>1</b> increases. In <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, as described above, the field surface <b>70</b> is subjected to the bending deformation with the approximately equal sectional surface.</p><p id="p-0127" num="0125">Here, an example of an applied scene of the bending deformation of the field surface <b>70</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref> to <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>.</p><p id="p-0128" num="0126"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a plan view of a field object <b>77</b>, <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is an explanatory diagram of a bending deformation state of the field surface according to a position M<b>1</b>, <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is an explanatory diagram of a bending deformation state of the field surface according to a position M<b>2</b>, and <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is an explanatory diagram of a bending deformation state of the field surface according to a position M<b>3</b>. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the field object <b>77</b> is illustrated in an expression in which the field image is projected onto the field surface <b>70</b> in the regular state. In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the position M<b>1</b> to the position M<b>3</b> are exemplified, and the position M<b>2</b> and the position M<b>3</b> are in the vicinity of each of a start position and an end position of the curved road <b>17</b>. Note that, in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> to <figref idref="DRAWINGS">FIG. <b>8</b>C</figref>, for the convenience of drawing, the curved road <b>17</b> and the like include a portion separated from the field surface <b>70</b> (a portion outside the viewing angle <b>62</b> of the virtual camera <b>60</b>), but the entire curved road <b>17</b> and the like may be projected onto the field surface <b>70</b>.</p><p id="p-0129" num="0127">Here, each of the position M<b>1</b> and the position M<b>3</b> corresponds to an intersection point position between the visual line direction V of the virtual camera <b>60</b> and the xy plane (or the field surface <b>70</b> before being deformed). In <figref idref="DRAWINGS">FIG. <b>8</b></figref>, a projection vector V&#x2032; of the virtual camera <b>60</b> (a projection vector V&#x2032; of the visual line direction V on the xy plane) at the time of the position M<b>2</b> is illustrated. In addition, the position M<b>1</b> and the position M<b>3</b> correspond to the position of the first object <b>3</b>. That is, the bending deformation of the field surface <b>70</b> associated with the movement of the virtual camera <b>60</b> when the first object <b>3</b> is moved to the position M<b>3</b> from the position M<b>1</b> along the curved road <b>17</b> will be described.</p><p id="p-0130" num="0128">When the intersection point position between the visual line direction V of the virtual camera <b>60</b> and the field surface <b>70</b> is positioned in the position Ml, the bending deformation of the field surface <b>70</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is attained. When the intersection point position between the visual line direction V of the virtual camera <b>60</b> and the field surface <b>70</b> is positioned in the position M<b>2</b>, the bending deformation of the field surface <b>70</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> is attained. When the intersection point position between the visual line direction V of the virtual camera <b>60</b> and the field surface <b>70</b> is positioned in the position M<b>3</b>, the bending deformation of the field surface <b>70</b> as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b>C</figref> is attained.</p><p id="p-0131" num="0129">As described above, in a case where the visual line direction V of the virtual camera <b>60</b> is changed in association with the movement of the first object <b>3</b> to the position M<b>3</b> from the position M<b>1</b>, the bending deformation of the field object <b>77</b> according to the changed visual line direction V (bending deformation in the same deformation mode when viewed in the visual line direction V of the virtual camera <b>60</b>) is attained.</p><p id="p-0132" num="0130">However, in a case where the position of the virtual camera <b>60</b> in the virtual space (the position with respect to the field object) is changed, a region within the viewing angle of the virtual camera <b>60</b> in the virtual space (for example, the region of the field object) is changed, and thus, the field image can be diversified. Here, even in such a case, when the state of the region within the viewing angle of the virtual camera <b>60</b> is monotonous, the field image is not capable of being diversified even in a case where the position of the virtual camera <b>60</b> (the position with respect to the field object) is changed. Accordingly, the position of the virtual camera <b>60</b> in the virtual space (the position with respect to the field object) can be changed, and the field image can be effectively diversified by increasing the number of second objects arranged on the field object, the type of second object, and the like. Hereinafter, the position of the virtual camera <b>60</b> indicates the position with respect to the field object (a relative position), unless otherwise stated.</p><p id="p-0133" num="0131">However, in this embodiment, as described above, the field surface <b>70</b> (and the field object associated therewith) is subject to the bending deformation, but in a case where the deformation degree at the time of the bending deformation is perennially constant, the field image obtained by being drawn in an expression viewed from the virtual camera <b>60</b> is likely to be monotonous.</p><p id="p-0134" num="0132">Note that, in a case where the position of the virtual camera <b>60</b> can be changed, the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed in association with a change in the position of the virtual camera <b>60</b>, and thus, in a case where various second objects are arranged on the field object in various modes, the appearance of the first object that is a manipulation target of the user or an overlap condition of a plurality of second objects is changed, and therefore, the field image obtained by being drawn in an expression viewed from the virtual camera <b>60</b> can be diversified.</p><p id="p-0135" num="0133">Here, as a merit for diversifying a game field, a part of the game field can be emphasized, the visibility of the user with respect to the object is improved, and the object that is a manipulation target or a manipulation part is clarified, and thus, manipulation properties are improved. In addition, within a limited screen, even in a case where there are a plurality of objects, a situation within the virtual space can be expressed without impairing the visibility (without limiting an information amount). Such an effect is particularly remarkable on a narrow screen such as a smart phone.</p><p id="p-0136" num="0134">In addition, the expression of the horizon line, or the like can be attained by simple processing such as the bending deformation of the field object, and thus, a processing load can also be suppressed. In addition, an (hidden) object out of the viewing angle of the virtual camera <b>60</b> in the field object may be not drawn, and thus, the processing load can be suppressed.</p><p id="p-0137" num="0135">At this time, in this embodiment, not only can the position of the virtual camera <b>60</b> be changed, but also the deformation degree according to the bending deformation of the field surface <b>70</b> (and the field object associated therewith) can be changed. In a case where the deformation degree according to the bending deformation of the field surface <b>70</b> (and the field object associated therewith) is changed, the appearance when viewed from the virtual camera <b>60</b> is different even in the same region of the field object, and thus, the field image obtained by being drawn in an expression viewed from the virtual camera <b>60</b> can be further diversified.</p><p id="p-0138" num="0136">In addition, in this embodiment, the deformation degree according to the bending deformation of the field surface <b>70</b> is changed by using one material for the field object (the field image and the field surface <b>70</b>), and thus, field objects in various forms are attained. Accordingly, the efficiency of a storage region for the field object can be attained, compared to a case where various field objects (a field object in a fixed form that is not deformable) are prepared in advance. That is, the field objects in various forms can be attained by efficiently using the storage region.</p><p id="p-0139" num="0137">Note that, as described above, in a case where the deformation degree of the field surface <b>70</b> is changed, the user may feel a sense of discomfort due to a change in the height H<b>1</b> of the horizon line HL. For example, in a case where the deformation degree of the field surface <b>70</b> is changed in a state where the position of the virtual camera <b>60</b> is fixed, the user easily feels a sense of discomfort.</p><p id="p-0140" num="0138">Therefore, in this embodiment, in a case where the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed, in accordance with the change, the deformation degree of the field surface <b>70</b> is changed. For example, in a case where the position of the virtual camera <b>60</b> is changed, in accordance with the change, the deformation degree of the field surface <b>70</b> is changed. Accordingly, a disadvantage that may occur due to a change in the deformation degree of the field surface <b>70</b> (that is, a sense of discomfort that can be felt by the user) can be reduced.</p><p id="p-0141" num="0139">For example, in a case where the position of the virtual camera <b>60</b> is within one or more specific positions or specific ranges, the deformation degree of the field surface <b>70</b> may be greater than that in a case where the position of the virtual camera <b>60</b> is not within the specific position or the specific range. Accordingly, the field image when the position of the virtual camera <b>60</b> is in the specific position or the specific range (that is, the expression of the various objects within the viewing angle of the virtual camera <b>60</b>) can be expressed in a mode different from that of the field image at the time of being in another position. For example, an effect of emphasizing the field image according to the specific position, compared to the field image according to another position or of applying specific meaning to the field image according to the specific position can be attained. Note that, the specific position or the specific range, for example, may be set corresponding to a position or the like in which the first object is bent while being moved (for example, a position in which the horizontal passage <b>14</b> intersects with the vertical passage <b>15</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>), or may be set corresponding to a position in which an object to be emphasized (for example, an object according to a game content having a low occurrence probability) is arranged. In this case, for example, the specific position may be a position having a predetermined relationship with respect to the position of the object. In addition, the specific position may be dynamically changed. For example, only in a case where the object according to the game content having a low occurrence probability is arranged, one or more specific positions may include a specific position that is set corresponding to the position in which the object is arranged.</p><p id="p-0142" num="0140">Here, in a case where the position of the virtual camera <b>60</b> can be changed, the region of the field object within the viewing angle of the virtual camera <b>60</b> is changed, in association with a change in the position of the virtual camera <b>60</b>. In this case, when various second objects are arranged on the field object in various modes, the appearance of the first object that is a manipulation target of the user or an overlapping condition of a plurality of second objects is changed, and thus, the field image obtained by being drawn in an expression viewed from the virtual camera <b>60</b> can be diversified. In addition, even in a case where an overlap (overlapping) occurs between the second object and/or the first object, as described above, the overlapping objects are arranged in positions shifted from each other up and down by adjusting the deformation degree of the field surface <b>70</b>. Therefore, in the overlapping objects, the visibility of individual objects or one or more focused objects can be increased.</p><p id="p-0143" num="0141">In addition, the field image obtained by being drawn in an expression viewed from the virtual camera <b>60</b> is diversified, and thus, a part of such a field image can be emphasized, and the visibility of the user with respect to the object is improved. Further, the object that is a manipulation target or a manipulation part is clarified, and thus, manipulation properties are improved. In addition, within a limited screen, even in a case where there are a plurality of objects, a situation within the virtual space can be expressed without impairing the visibility (without limiting an information amount). Such an effect is particularly remarkable on a narrow screen such as a smart phone.</p><p id="p-0144" num="0142">In addition, the expression of the horizon line, or the like can be attained by simple processing such as the bending deformation of the field object, and thus, the processing load can also be suppressed. In addition, the (hidden) object out of the viewing angle of the virtual camera <b>60</b> in the field object may not be drawn, and thus, the processing load can be suppressed.</p><p id="p-0145" num="0143"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an explanatory diagram of a freedom degree of a change in the position of the virtual camera <b>60</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the change in the position of the virtual camera <b>60</b> includes a change V<b>1</b> along the visual line direction V, changes V<b>2</b> and V<b>3</b> in a direction intersecting with the visual line direction V, and a combination thereof. The change V<b>2</b> is a change within the Vz plane, V<b>3</b> is a change in a direction perpendicular to the Vz plane. Note that, the change in the position of the virtual camera <b>60</b> may be attained by the displacement (the movement) of the virtual camera <b>60</b> in the global coordinate system, may be attained by the displacement (the movement) of the field object in the global coordinate system, or may be attained by a combination thereof.</p><p id="p-0146" num="0144">As described above, in this embodiment, a mode in which the position (the relative position) of the virtual camera <b>60</b> is changed includes a mode in which the position with respect to the field object (the relative position) is changed in the direction (V<b>2</b> and V<b>3</b>) intersecting with the visual line direction V (hereinafter, will be referred to as a &#x201c;first change mode&#x201d;) and a mode in which the position with respect to the field object (the relative position) is changed in the direction (V<b>1</b>) along the visual line direction V (hereinafter, will be referred to as a &#x201c;second change mode&#x201d;).</p><p id="p-0147" num="0145">At this time, a change in the deformation degree of the field surface <b>70</b> may be attained in association with any one of the first change mode and the second change mode, or may be attained in association with both of the first change mode and the second change mode. For example, the position of the virtual camera <b>60</b> is changed in the direction intersecting with the visual line direction V and is changed along the visual line direction V.</p><p id="p-0148" num="0146">Note that, a change attained simultaneously when the deformation degree of the field surface <b>70</b> is changed (that is, a change in the region of the field object within the viewing angle of the virtual camera <b>60</b>) is not limited to the change in the position with respect to the field object (the relative position), and may be attained by the rotation of the visual line direction V of the virtual camera <b>60</b> (refer to <figref idref="DRAWINGS">FIG. <b>10</b></figref>). In addition, an optical parameter of the virtual camera <b>60</b> is set to a variable value, and the value of the optical parameter of the virtual camera <b>60</b> is changed, and thus, the region of the field object falling within the viewing angle of the virtual camera <b>60</b> can be changed. Such an optical parameter, for example, may be an optical parameter relevant to a zoom amount of the virtual camera <b>60</b>, such as a focal distance or a viewing angle.</p><p id="p-0149" num="0147">In this embodiment, as an example, not only can the position of the virtual camera <b>60</b> with respect to the field object (the relative position) be changed, but also the visual line direction V of the virtual camera <b>60</b> can be changed. Specifically, the visual line direction V of the virtual camera <b>60</b> can be rotated around an axis parallel to the z direction (an example of the third axis) (hereinafter, will be referred to as a &#x201c;revolution axis Pc&#x201d;). Note that, the visual line direction V of the virtual camera <b>60</b> may be set such that only a rotation rotated around the revolution axis Pc can be performed, or a rotation around another axis (a planetary rotation described below) can also be performed. For example, the visual line direction V of the virtual camera <b>60</b> may be set such that a rotation around an axis perpendicular to the Vz plane can be performed. That is, in a mode where an attack angle of the virtual camera <b>60</b> (an attack angle parameter &#x3c8; described below) is changed, the visual line direction V of the virtual camera <b>60</b> may be rotated. In this case, the rotation center may be a position having a predetermined relationship with respect to the position of the virtual camera <b>60</b>, and the predetermined relationship may be fixed, or may be changed.</p><p id="p-0150" num="0148">Hereinafter, the rotation of the virtual camera <b>60</b> indicates a rotation in a mode where the visual line direction V is rotated around the revolution axis Pc, unless otherwise stated. The direction of the virtual camera <b>60</b> indicates the direction of the visual line direction V of the virtual camera <b>60</b>.</p><p id="p-0151" num="0149"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an explanatory diagram of the rotation (the change) of the visual line direction V of the virtual camera <b>60</b>, and is a diagram schematically illustrating the virtual camera <b>60</b> and the viewing angle <b>62</b> viewed in the z direction. In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the virtual camera <b>60</b> is schematically illustrated in two positions during the rotation.</p><p id="p-0152" num="0150">In <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the revolution axis Pc according to the visual line direction V of the virtual camera <b>60</b> passes through the visual line direction V of the virtual camera <b>60</b> and is offset to the backward side of the visual line direction V from the virtual camera <b>60</b> when viewed in the z direction. In this case, when the virtual camera <b>60</b> is rotated by 360 degrees, the virtual camera <b>60</b> draws a circular locus C<b>70</b> around the revolution axis Pc when viewed in the z direction. Here, the position of the revolution axis Pc is arbitrary, and may be a position having a predetermined relationship with respect to the virtual camera <b>60</b>, and the predetermined relationship may be fixed, or may be changed. Herein, the revolution axis Pc is distinguished from a planetary rotation axis in the case of passing through the virtual camera <b>60</b>, and thus, is set to be different from the planetary rotation axis.</p><p id="p-0153" num="0151">Note that, in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the visual line direction V is a direction that passes through the revolution axis Pc and is separated from the revolution axis Pc perennially during the rotation of the virtual camera <b>60</b> when viewed in the z direction, but is not limited thereto. For example, the visual line direction V may be a direction that passes through the revolution axis Pc and is directed toward the revolution axis Pc perennially during the rotation of the virtual camera <b>60</b> when viewed in the z direction. That is, the revolution axis Pc may pass through the visual line direction V of the virtual camera <b>60</b> and may be offset to the forward side (the farther side) of the visual line direction V from the virtual camera <b>60</b> when viewed in the z direction. In this case, the revolution axis Pc, for example, may be set to pass through a predetermined object that the user wants to see (for example, a predetermined object described below) from all directions. In addition, in another embodiment, the visual line direction V may be subjected to the planetary rotation during the rotation (the revolution) of the virtual camera <b>60</b> when viewed in the z direction. That is, the virtual camera <b>60</b> may be set such that a rotation around a planetary rotation axis <b>61</b> (the axis parallel the z axis (another example of the third axis)) (the planetary rotation) can be performed. Alternatively, the visual line direction V may be set such that the planetary rotation can be performed independently from the revolution.</p><p id="p-0154" num="0152">However, as described above, the horizon line HL is formed by the bending deformation of the field surface <b>70</b>. Accordingly, in a case where the visual line direction V is changed in association with the rotation of the virtual camera <b>60</b>, in association with the change, the deformation mode of the bending deformation of the field surface <b>70</b> is changed. That is, in a case where the visual line direction V is changed in association with the rotation of the virtual camera <b>60</b>, in association with the change, the deformation mode of the bending deformation of the field surface <b>70</b> is changed such that an Xc axis of the local coordinate system is positioned within the Vz plane based on the visual line direction V after being changed. Accordingly, even in a case where the visual line direction V is changed in association with the rotation of the virtual camera <b>60</b>, the horizon line HL within the virtual space can be attained in a mode where there is no sense of discomfort.</p><p id="p-0155" num="0153">Next, the further details of a drawing function of the server device <b>10</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref> and the subsequence.</p><p id="p-0156" num="0154">Here, first, a camera parameter used in the description of <figref idref="DRAWINGS">FIG. <b>12</b></figref> and the subsequence will be described, and then, the details of the server device <b>10</b> will be described, with reference to <figref idref="DRAWINGS">FIG. <b>11</b></figref>.</p><p id="p-0157" num="0155"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an explanatory diagram of the camera parameter. In <figref idref="DRAWINGS">FIG. <b>11</b></figref>, the field surface <b>70</b> (the regular state) positioned in the global coordinate system is illustrated. Note that, as described above, the field surface <b>70</b> can be subjected to the bending deformation, but the translational movement or the rotation of the entire field surface <b>70</b> in the global coordinate system is not capable of being performed. Accordingly, the coordinates of each position of the field surface <b>70</b> in the field coordinate system can be converted into the coordinates of the global coordinate system in a predetermined conversion method, and the reverse conversion can also be performed. Note that, hereinafter, for description, the origin of the field coordinate system is identical to the origin of the global coordinate system, the u axis of the field coordinate system (=the u axis of the texture coordinate system) is coincident with the x axis of the global coordinate system, and the v axis of the field coordinate system (=the v axis of the texture coordinate system) is coincident with the y axis of the global coordinate system. Note that, hereinafter, the field surface <b>70</b> indicates the field surface <b>70</b> in a state where the field image is projected (the field surface <b>70</b> of the field object), unless otherwise stated.</p><p id="p-0158" num="0156">In this embodiment, the camera parameter includes two position parameters (X, Y), a distance parameter A<b>2</b>, a direction parameter &#x3b8;, and an attack angle parameter &#x3c8;. In a case where the values of all of the parameters are set, the virtual camera <b>60</b> can be uniquely positioned with respect to the global coordinate system.</p><p id="p-0159" num="0157">The position parameter X is an x coordinate of an intersection point on the xy plane of the visual line direction V, a position parameter Y is a y coordinate of the intersection point on the xy plane of the visual line direction V, and the distance parameter A<b>2</b> is a distance to the virtual camera <b>60</b> from the intersection point on the xy plane of the visual line direction V (a distance along the visual line direction V). The direction parameter &#x3b8; is an angle between a projection vector V&#x2032; of the visual line direction V on the xy plane and the x axis. The attack angle parameter &#x3c8; is an angle between the visual line direction V and the xy plane. Note that, in this embodiment, the attack angle parameter &#x3c8; is used, but the attack angle parameter &#x3c8; may be omitted. That is, the attack angle parameter &#x3c8; may have a constant value (a fixed value).</p><p id="p-0160" num="0158">Note that, such camera parameters are used in the following description, and in the actual processing, different parameters may be equivalently used.</p><p id="p-0161" num="0159"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example of a functional block diagram relevant to the drawing function of the server device <b>10</b>. <figref idref="DRAWINGS">FIG. <b>13</b></figref> is an explanatory diagram of deformation parameter data. Note that, in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, &#x201c;-&#x201d; indicates that it is arbitrary, and &#x201c; . . . &#x201d; indicates the repetition of the same. <figref idref="DRAWINGS">FIG. <b>14</b></figref> is an explanatory diagram of distance parameter data. Similarly, in <figref idref="DRAWINGS">FIG. <b>14</b></figref> (the same applies to <figref idref="DRAWINGS">FIG. <b>15</b></figref>), &#x201c; . . . &#x201d; indicates the repetition of the same. <figref idref="DRAWINGS">FIG. <b>15</b></figref> is an explanatory diagram of direction parameter data.</p><p id="p-0162" num="0160">The server device <b>10</b> includes a drawing information storage unit <b>130</b>, a manipulation information retrieval unit <b>132</b>, a drawing data transmission unit <b>134</b>, and a drawing processing unit <b>140</b>. Note that, the drawing information storage unit <b>130</b> can be attained by the server storage unit <b>12</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the manipulation information retrieval unit <b>132</b> and the drawing data transmission unit <b>134</b> can be attained by the server communication unit <b>11</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the drawing processing unit <b>140</b> can be attained by the server control unit <b>13</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Hereinafter, in the processing of each unit, &#x201c;calculation&#x201d; is the concept including processing of only reading out a calculation value, a setting value, or the like stored as data.</p><p id="p-0163" num="0161">In the drawing information storage unit <b>130</b>, various information items and data items used by the drawing processing unit <b>140</b> are stored.</p><p id="p-0164" num="0162">The data stored in the drawing information storage unit <b>130</b> includes deformation parameter data <b>13</b>A according to the specific position described above. Hereinafter, the specific position is a position in which a predetermined object described below can be positioned. That is, the specific position is a position that can be coincident with the position of the predetermined object described below.</p><p id="p-0165" num="0163">In the deformation parameter data <b>13</b>A, the value of the deformation parameter A<b>1</b> that is different from a normal value &#x3b2;<sub>0 </sub>is associated with each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> according to the specific position described above. Note that, the normal value &#x3b2;<sub>0 </sub>of the deformation parameter A<b>1</b> is a constant value, but in a case where a plurality of types of field images G<b>60</b> are prepared, the normal value &#x3b2;<sub>0 </sub>may be a variable value that can be different for each field image. In addition, the value of the direction parameter &#x3b8; may be further associated with the position parameters (X, Y) of the virtual camera <b>60</b> according to the specific position. For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, the value of the deformation parameter A<b>1</b> according to the value of the direction parameter &#x3b8; is associated with each of the values (X, Y) (X<sub>A</sub>, Y<sub>A</sub>), (X<sub>B</sub>, Y<sub>B</sub>), (X<sub>C</sub>, Y<sub>C</sub>), and the like of the position parameters according to the specific position, for each specific position. For example, in the deformation parameter data <b>13</b>A of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in a specific position A=(X<sub>A</sub>, Y<sub>A</sub>), even in a case where the direction of the virtual camera <b>60</b> is any direction, a value &#x3b2;<sub>1 </sub>of the deformation parameter A<b>1</b> is associated. On the other hand, in a specific position C=(X<sub>C</sub>, Y<sub>C</sub>), when the direction of the virtual camera <b>60</b> is a direction parameter &#x3b8;=&#x3b8;<sub>C1</sub>, a value &#x3b2;<sub>3 </sub>of the deformation parameter A<b>1</b> is associated, and when the direction of the virtual camera <b>60</b> is a direction parameter &#x3b8;=&#x3b8;<sub>C2</sub>, a value &#x3b2;<sub>4 </sub>of the deformation parameter A<b>1</b> is associated. Hereinafter, as with the specific position C, a specific position in which the value of the deformation parameter A<b>1</b> is changed in accordance with the direction of the virtual camera <b>60</b> may be referred to as a &#x201c;specific position in which the deformation degree is changed during the revolution of the virtual camera <b>60</b>&#x201d;, and on the deformation parameter data <b>13</b>A, a direction in which a value different from the normal value &#x3b2;<sub>0</sub>, such as the value &#x3b2;<sub>3 </sub>or the value &#x3b2;<sub>4</sub>, is associated (a direction in which the direction parameter &#x3b8;=&#x3b8;<sub>C1 </sub>or &#x3b8;<sub>C2</sub>) will be referred to as a &#x201c;specific direction&#x201d;. The specific direction is fixed, but may be changed. Note that, in another embodiment, the specific position in which the deformation degree is changed during the revolution of the virtual camera <b>60</b> may not be set. In addition, in the specific position C, two specific directions are set, but only one specific direction may be set, or three or more specific directions may be set.</p><p id="p-0166" num="0164">In addition, the data stored in the drawing information storage unit <b>130</b> includes deformation parameter data <b>13</b>B according to the specific object. In the deformation parameter data <b>13</b>B, the value of the deformation parameter A<b>1</b> that is different from the normal value &#x3b2;<sub>0 </sub>is associated with the specific object. The specific object is an arbitrary object different from the predetermined object described below. For example, it is preferable that the specific object is an object that the user is allowed to watch, in the second objects. For example, the specific object may be an object according to a moving object (for example, a character), or may be an object according to a fixed object. In the deformation parameter data <b>13</b>B of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in a specific object G<b>1</b>, a value &#x3b2;<sub>G1 </sub>of the deformation parameter A<b>1</b> is associated, and in a specific object G<b>2</b>, a value &#x3b2;<sub>G2 </sub>of the deformation parameter A<b>1</b> is associated. On the other hand, in a specific object G<b>3</b>, when the direction of the virtual camera <b>60</b> is a direction parameter &#x3b8;=&#x3b8;<sub>C3</sub>, a value &#x3b2;<sub>G3 </sub>of the deformation parameter A<b>1</b> is associated, and when the direction of the virtual camera <b>60</b> is a direction parameter &#x3b8;=&#x3b8;<sub>C4</sub>, a value &#x3b2;<sub>G4 </sub>of the deformation parameter A<b>1</b> is associated. As with the case of the deformation parameter data <b>13</b>A, hereinafter, as with the specific object G<b>3</b>, a specific object in which the value of the deformation parameter A<b>1</b> is changed in accordance with the direction of the virtual camera <b>60</b> may be referred to as a &#x201c;specific object in which the deformation degree is changed during the revolution of the virtual camera <b>60</b>&#x201d;, and on the deformation parameter data <b>13</b>B, a direction in which a value different from the normal value &#x3b2;<sub>0</sub>, such as the value &#x3b2;<sub>G3 </sub>or the value &#x3b2;<sub>G4</sub>, is associated (a direction in which the direction parameter &#x3b8;=&#x3b8;<sub>C3 </sub>or &#x3b8;<sub>C4</sub>) will be referred to as a &#x201c;specific direction&#x201d;. The specific direction is fixed, but may be changed. For example, in a case where the specific object G<b>3</b> has a front direction, the specific direction may be changed in accordance with a change in the front direction of the specific object G<b>3</b>.</p><p id="p-0167" num="0165">In addition, the data stored in the drawing information storage unit <b>130</b> includes distance parameter data <b>14</b>A according to the specific position described above. In the distance parameter data <b>14</b>A, the value of the distance parameter A<b>2</b> that is different from a normal value &#x3b3;<sub>0 </sub>is associated with each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> according to the specific position. Note that, the normal value &#x3b3;<sub>0 </sub>of the distance parameter A<b>2</b> is a constant value, but in a case where there are a plurality of types of field images G<b>60</b> are prepared, the normal value &#x3b3;<sub>0 </sub>may be a variable value that can be different for each field image. The value of the distance parameter A<b>2</b> according to the specific position of one virtual camera <b>60</b> may be set such that the corresponding region (and an object positioned within the region) is captured by the virtual camera <b>60</b> positioned in the specific position with a desired distance sense. For example, in the case of allowing the user to see a specific second object at a close distance, the value of the distance parameter A<b>2</b> in which the specific second object is captured by the virtual camera <b>60</b> at the close distance may be associated with each of the values of the position parameters (X, Y) according to the specific position. For example, in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, in a specific position A=(X<sub>A</sub>, Y<sub>A</sub>), a value &#x3b3;<sub>1 </sub>of the distance parameter A<b>2</b> is associated, and in a specific position B=(X<sub>B</sub>, Y<sub>B</sub>), a value &#x3b3;<sub>2 </sub>of the distance parameter A<b>2</b> is associated, and the same applies to the followings. Note that, in this embodiment, as an example, the value of the distance parameter A<b>2</b> that is defined by the distance parameter data <b>14</b>A, such as the value &#x3b3;<sub>1 </sub>or the value &#x3b3;<sub>2</sub>, is significantly less than the normal value &#x3b3;<sub>0</sub>. Note that, a distance between the virtual camera <b>60</b> and the field object decreases as the value of the distance parameter A<b>2</b> decreases.</p><p id="p-0168" num="0166">In addition, the data stored in the drawing information storage unit <b>130</b> includes distance parameter data <b>14</b>B according to the specific object. In the distance parameter data <b>14</b>B, the value of the distance parameter A<b>2</b> that is less than the normal value &#x3b3;<sub>0 </sub>is associated with each specific object. In the distance parameter data <b>14</b>B of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, in the specific object G<b>1</b>, a value &#x3b3;<sub>G1 </sub>of the distance parameter A<b>2</b> is associated, and in the specific object G<b>2</b>, a value &#x3b3;<sub>G2 </sub>of the distance parameter A<b>2</b> is associated.</p><p id="p-0169" num="0167">Note that, in this embodiment, in the distance parameter data <b>14</b>A according to the specific position, a specific position in which the value of the distance parameter A<b>2</b> is changed in accordance with the value of the direction parameter &#x3b8; is not defined, but as with the deformation parameter data <b>13</b>A described above, a specific position in which the value of the distance parameter A<b>2</b> is changed in accordance with the value of the direction parameter &#x3b8; may be defined. That is, a specific position in which the value of the distance parameter A<b>2</b> is changed during the revolution of the virtual camera <b>60</b> may be defined. In this case, preferably, in the specific position in which the deformation degree is changed during the revolution of the virtual camera <b>60</b>, the value of the distance parameter A<b>2</b> is associated with the specific direction according to the specific position. For example, in the specific position C=(X<sub>C</sub>, Y<sub>C</sub>), when the direction of the virtual camera <b>60</b> is the direction parameter &#x3b8;=&#x3b8;<sub>C1</sub>, a value &#x3b3;<sub>31 </sub>of the distance parameter A<b>2</b> may be associated, and when the direction of the virtual camera <b>60</b> is the direction parameter &#x3b8;=&#x3b8;<sub>C2</sub>, a value &#x3b3;<sub>32 </sub>of the distance parameter A<b>2</b> may be associated. Accordingly, when a bending deformation degree is changed during the revolution of the virtual camera <b>60</b>, the value of the distance parameter A<b>2</b> can be changed.</p><p id="p-0170" num="0168">In addition, similarly, in this embodiment, in the distance parameter data <b>14</b>B according to the specific object, the specific object in which the value of the distance parameter A<b>2</b> is changed in accordance with the value of the direction parameter &#x3b8; is not defined, but as with the deformation parameter data <b>13</b>B described above, the specific object in which the value of the distance parameter A<b>2</b> is changed in accordance with the value of the direction parameter &#x3b8; may be defined. That is, the specific object in which the value of the distance parameter A<b>2</b> is changed during the revolution of the virtual camera <b>60</b> may be defined. Even in this case, preferably, in the specific object which the deformation degree is changed during the revolution of the virtual camera <b>60</b>, the value of the distance parameter A<b>2</b> is associated with the specific direction according to the specific object. For example, in the specific object G<b>3</b>, when the direction of the virtual camera <b>60</b> is the direction parameter &#x3b8;=&#x3b8;<sub>C3</sub>, a value &#x3b3;<sub>G31 </sub>of the distance parameter A<b>2</b> may be associated, when the direction of the virtual camera <b>60</b> is the direction parameter &#x3b8;=&#x3b8;<sub>C4</sub>, a value &#x3b3;<sub>G32 </sub>of the distance parameter A<b>2</b> may be associated. Accordingly, when the bending deformation degree is changed during the revolution of the virtual camera <b>60</b>, the value of the distance parameter A<b>2</b> can be changed.</p><p id="p-0171" num="0169">In addition, the data stored in the drawing information storage unit <b>130</b> includes the direction parameter data. In the direction parameter data, the value of the direction parameter &#x3b8; is associated with each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> according to a direction change position. The value of the direction parameter &#x3b8; according to the position of one virtual camera <b>60</b> may be set such that a desired region falls within the viewing angle of the virtual camera <b>60</b> of the position. For example, in the case of allowing the user to see a specific second object, the value of the direction parameter &#x3b8; may be associated with each of the values of the position parameters (X, Y) according to the position such that the specific second object is positioned in the region falling within the viewing angle. The direction change position, for example, may be set corresponding to a position and the like in which the direction of the first object is changed while the first object is moved (for example, a start position and an end position of the curved road <b>17</b> illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, an intersecting position between the horizontal passage <b>14</b> and the vertical passage <b>15</b>, and the like). For example, in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, in a direction change position T<b>1</b> (X<sub>P1</sub>, Y<sub>P1</sub>), a value &#x3b8;<sub>1 </sub>of the direction parameter &#x3b8; is associated, in a direction change position T<b>2</b> (X<sub>P2</sub>, Y<sub>P2</sub>), a value &#x3b8;<sub>2 </sub>of the direction parameter &#x3b8; is associated, and the same applies to the followings. As with the specific positions A and B, the direction change positions T<b>1</b> and T<b>2</b> may be a position in which the value of the deformation parameter A<b>1</b> that is different from the normal value &#x3b2;<sub>0 </sub>is associated, and/or may be a position in which the value of the distance parameter A<b>2</b> that is different from the normal value &#x3b3;<sub>0 </sub>is associated.</p><p id="p-0172" num="0170">In addition, the data stored in the drawing information storage unit <b>130</b> includes attack angle parameter data. In the attack angle parameter data, even though it is not illustrated, as with the direction parameter data, the value of the attack angle parameter &#x3c8; may be associated with each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> according to an attack angle change position. The value of the attack angle parameter &#x3c8; according to the position of one virtual camera <b>60</b> may be set such that a desired region falls within the viewing angle of the virtual camera <b>60</b> of the position. For example, in the case of allowing the user to see a specific second object, the value of the attack angle parameter &#x3c8; may be associated with each of the values of the position parameters (X, Y) according to the position such that the specific second object is positioned in the region falling within the viewing angle.</p><p id="p-0173" num="0171">Note that, it is not necessary that the data stored in the drawing information storage unit <b>130</b> is managed in segmentations as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, <figref idref="DRAWINGS">FIG. <b>14</b></figref>, and <figref idref="DRAWINGS">FIG. <b>15</b></figref>, and the data may be suitably integrally managed.</p><p id="p-0174" num="0172">The manipulation information retrieval unit <b>132</b> retrieves the manipulation information of the user. Note that, the manipulation information of the user is generated in accordance with various manipulations of the user in the terminal device <b>20</b>. Note that, the manipulation information may be generated by a gesture, voice input, or the like. In this embodiment, the manipulation information includes a movement instruction of a predetermined object, and a rotation instruction of the virtual camera <b>60</b>. The movement instruction of the predetermined object is an instruction for changing the position of the predetermined object with respect to the field object (hereinafter, may be simply referred to as the &#x201c;position of the predetermined object&#x201d;), and may include an instruction such as a movement direction or a movement amount. The rotation instruction of the virtual camera <b>60</b> is an instruction for attaining the rotation of the virtual camera <b>60</b> described above, and may include an instruction such as a rotation type (the rotation around the revolution axis Pc, that is, the revolution, the rotation around the planetary rotation axis <b>61</b>, that is, the planetary rotation, or the rotation in which the value of the attack angle parameter &#x3c8; is changed), or a rotation direction. The predetermined object is arbitrary, and in this embodiment, preferably, is the first object. In addition, the manipulation information may include a movement instruction of the virtual camera <b>60</b>, and the like.</p><p id="p-0175" num="0173">The drawing data transmission unit <b>134</b> transmits field image drawing data that is generated by the drawing processing unit <b>140</b> to the terminal device <b>20</b>. Note that, as described above, in another embodiment, a part or all of the drawing processing of the drawing processing unit <b>140</b> may be attained on the terminal device <b>20</b> side. For example, in a case where the drawing processing unit <b>140</b> is attained by the terminal device <b>20</b>, the drawing data transmission unit <b>134</b> may be omitted.</p><p id="p-0176" num="0174">The drawing processing unit <b>140</b> generates the field image drawing data, on the basis of various data items in the drawing information storage unit <b>130</b>, the manipulation information from the terminal device <b>20</b>, or the like.</p><p id="p-0177" num="0175">The drawing processing unit <b>140</b> includes a change processing unit <b>142</b>, a second movement processing unit <b>144</b>, a deformation processing unit <b>145</b>, a projection processing unit <b>146</b>, a background processing unit <b>147</b>, and a drawing data generation unit <b>148</b>.</p><p id="p-0178" num="0176">The change processing unit <b>142</b> changes the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, in accordance with the manipulation information and the like. For example, in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is changed, and each of the values of the position parameters (X, Y) is changed, the change processing unit <b>142</b> executes various processings according to the change.</p><p id="p-0179" num="0177">In this embodiment, a specific object is arranged on a field object <b>77</b>, and thus, in a case where the range of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed, the specific object may be positioned within the region. In other words, the region of the field object falling within the viewing angle <b>62</b> of the virtual camera <b>60</b> substantially has all possible combinations of the values of the camera parameters, and among them, in a case where there is a region in which the specific object is positioned (hereinafter, may be referred to as a &#x201c;specific object region&#x201d;), there are a region around the specific object region (an example of a predetermined object region) and the like. In addition, in this embodiment, the value of the camera parameter can be changed by comparatively small resolution, and thus, the specific object region includes a first object region in which the specific object is positioned in the center portion, and a second object region in which the specific object is positioned in the end portion.</p><p id="p-0180" num="0178">Here, the specific object and the like will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>. In <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, regions R<b>1</b>, R<b>2</b>, and R<b>3</b> of the field object <b>77</b> falling within the viewing angle <b>62</b> of the virtual camera <b>60</b>, respectively, at the time of three types of camera parameters different from each other (here, camera parameters <b>1</b>, <b>2</b>, and <b>3</b>) are illustrated. In the camera parameters <b>1</b>, <b>2</b>, and <b>3</b>, at least one value of each element (X, Y, A<b>2</b>, &#x3b8;, and &#x3c8;) of the camera parameter described above is different. Accordingly, three regions illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref> are regions different from each other. Then, in three regions R<b>1</b>, R<b>2</b>, and R<b>3</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, in the regions R<b>1</b> and R<b>3</b>, specific objects G<b>5</b> and G<b>7</b> are arranged. Accordingly, in this case, the regions R<b>1</b> and R<b>3</b> are an example of the specific object region. On the other hand, in the region R<b>2</b>, the specific object is not arranged. Accordingly, in this case, the region R<b>2</b> is an example of the predetermined object region in which the specific object is not positioned. Note that, in a case where the specific object is a moving object, a region that was the specific object region at one time point may not be the specific object region at another time point. For example, in the case of the region R<b>2</b>, when a specific object G<b>6</b> that was positioned out of the viewing angle <b>62</b> at the time point is moved and is in a state of being positioned in the region R<b>2</b>, the region R<b>2</b> is the specific object region.</p><p id="p-0181" num="0179">The change processing unit <b>142</b> includes a first movement processing unit <b>1420</b>, a distance change unit <b>1421</b>, a direction change unit <b>1422</b>, an attack angle change unit <b>1423</b>, an update reflection unit <b>1424</b>, and a rotation processing unit <b>1425</b>.</p><p id="p-0182" num="0180">In a case where a predetermined first movement condition is established, the first movement processing unit <b>1420</b> updates each of the values of the position parameters (X, Y) of the virtual camera <b>60</b>. The predetermined first movement condition is arbitrary, and for example, may be satisfied by the movement of the predetermined object based on the movement instruction of the predetermined object in the manipulation information, or may be satisfied on the basis of a progress situation of the game or other factors.</p><p id="p-0183" num="0181">The distance change unit <b>1421</b> associates the value of the distance parameter A<b>2</b> with each of the values of the position parameters (X, Y) after being updated. In this embodiment, the distance change unit <b>1421</b> calculates the value of the distance parameter A<b>2</b> associated with each of the values of the position parameters (X, Y) after being updated, with reference to the distance parameter data in the drawing information storage unit <b>130</b>. At this time, on the distance parameter data <b>14</b>A, in a case where the value of the distance parameter A<b>2</b> is not associated with each of the values of the position parameters (X, Y) after being updated, an interpolation value may be associated with each of the values of the position parameters (X, Y) after being updated. An example of a calculation method of the interpolation value will be described below. Note that, in another embodiment, on the distance parameter data, in a case where the value of the distance parameter A<b>2</b> is not associated with each of the values of the position parameters (X, Y) after being updated, the distance change unit <b>1421</b> may associate the normal value &#x3b3;<sub>0 </sub>as it is, instead of the interpolation value.</p><p id="p-0184" num="0182">The distance change unit <b>1421</b> includes a first distance change unit <b>14211</b> and a second distance change unit <b>14212</b>. Note that, unlike the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, in another embodiment in which the specific position is not set, the first distance change unit <b>14211</b> may be omitted.</p><p id="p-0185" num="0183">The first distance change unit <b>14211</b> calculates the value of the distance parameter A<b>2</b>, on the basis of a relationship between position of the virtual camera <b>60</b> and the specific position (first distance parameter calculation processing). In this embodiment, the first distance change unit <b>14211</b> calculates the value of the distance parameter A<b>2</b>, on the basis of the position of the virtual camera <b>60</b>, with reference to distance parameter data <b>14</b>A according to the specific position as described above with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>. An example of the first distance parameter calculation processing of the first distance change unit <b>14211</b> will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0186" num="0184">The second distance change unit <b>14212</b> associates the value of the distance parameter A<b>2</b> associated with the specific object with each of the values of the position parameters (X, Y) after being updated (second distance parameter calculation processing). In this embodiment, in a case where the specific object is positioned in the region of the field object falling within the viewing angle <b>62</b> of the virtual camera <b>60</b>, the second distance change unit <b>14212</b> calculates the value of the distance parameter A<b>2</b>, on the basis of the specific object positioned in the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, with reference to the distance parameter data <b>14</b>B according to the specific position as described above with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>.</p><p id="p-0187" num="0185">In addition, in a case where the specific object is positioned in the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, the second distance change unit <b>14212</b> may calculate the value of the distance parameter A<b>2</b>, in accordance with the position of the specific object in the region. In this case, in the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, the second distance change unit <b>14212</b> may change the value of the distance parameter A<b>2</b> in a case where the specific object is positioned in the center portion of the region and in a case where the specific object is positioned in the end portions on both sides of the region. Specifically, in the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, the second distance change unit <b>14212</b> may decrease the value of the distance parameter A<b>2</b> in a case where the specific object is positioned in the center portion of the region and in a case where the specific object is positioned in the end portions on both sides of the region. Accordingly, when the specific object is positioned in the vicinity of the center within the viewing angle of the virtual camera <b>60</b>, the specific object can be effectively emphasized. Note that, in the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, the center portion, for example, may be a portion of a range <b>771</b> within a distance L<b>0</b> having a center line CT as the center, in the specific object region on the upper side of <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>. In this case, the distance L<b>0</b> is arbitrary insofar as the distance L<b>0</b> is significantly less than a distance L<b>1</b> of the entire specific object region, and for example, may be approximately L<b>1</b>/2. In this case, portions according to ranges <b>772</b> and <b>773</b> on both sides of the range <b>771</b> correspond to the end portions of the center portion of the region. An example of the second distance parameter calculation processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>27</b></figref>.</p><p id="p-0188" num="0186">The direction change unit <b>1422</b> associates the value of the direction parameter &#x3b8; with each of the values of the position parameters (X, Y) after being updated. In this embodiment, the direction change unit <b>1422</b> calculates the value of the direction parameter &#x3b8; according to each of the values of the position parameters (X, Y), with reference to the direction parameter data in the drawing information storage unit <b>130</b>. At this time, on the direction parameter data, in a case where the value of the direction parameter &#x3b8; is not associated with each of the values of the position parameters (X, Y) after being updated, the direction change unit <b>1422</b> may calculate an interpolation value. An example of a calculation method of the interpolation value will be described below. Then, the direction change unit <b>1422</b> associates the calculated value of the direction parameter &#x3b8; with each of the values of the position parameters (X, Y) after being updated. Note that, in another embodiment, on the direction parameter data, in a case where the value of the direction parameter &#x3b8; is not associated with each of the values of the position parameters (X, Y) after being updated, the direction change unit <b>1422</b> associate the normal value &#x3b8;<sub>0 </sub>as it is. The normal value &#x3b8;<sub>0 </sub>may set such that the visual line direction V is at a right angle with respect to the movement direction of the predetermined object when viewed in the z direction.</p><p id="p-0189" num="0187">The attack angle change unit <b>1423</b> associates the value of the attack angle parameter &#x3c8; with each of the values of the position parameters (X, Y) after being updated. In this embodiment, the attack angle change unit <b>1423</b> calculates the value of the attack angle parameter &#x3c8; according to each of the values of the position parameters (X, Y), with reference to the attack angle parameter data in the drawing information storage unit <b>130</b>. At this time, on the attack angle parameter data, in a case where the value of the attack angle parameter &#x3c8; is not associated with each of the values of the position parameters (X, Y) after being updated, the attack angle change unit <b>1423</b> may calculate an interpolation value. An example of a calculation method of the interpolation value will be described below. Then, attack angle change unit <b>1423</b> associates the calculated value of the attack angle parameter &#x3c8; with each of the values of the position parameters (X, Y) after being changed. Note that, in another embodiment, on the attack angle parameter data, in a case where the value of the attack angle parameter &#x3c8; is not associated with each of the values of the position parameters (X, Y) after being updated, the attack angle change unit <b>1423</b> may associate the normal value &#x3c8;<sub>0 </sub>as it is.</p><p id="p-0190" num="0188">The update reflection unit <b>1424</b> positions the virtual camera <b>60</b> with respect to the global coordinate system, on the basis of each of the values of the position parameters (X, Y) after being updated, and various parameters associated with each of the values of the position parameters (X, Y) after being updated (the distance parameter A<b>2</b>, the direction parameter &#x3b8;, and the attack angle parameter &#x3c8;). Accordingly, the virtual camera <b>60</b> is positioned with respect to the field surface <b>70</b> (and the field object associated therewith).</p><p id="p-0191" num="0189">In a case where a predetermined rotation condition is established, the rotation processing unit <b>1425</b> executes rotation processing of the virtual camera <b>60</b>. The predetermined rotation condition, for example, may be determined on the basis of the manipulation information (the rotation instruction of the virtual camera <b>60</b>), or may be satisfied on the basis of the progress situation of the game or other factors.</p><p id="p-0192" num="0190">The rotation processing unit <b>1425</b> may include a revolution processing unit <b>14251</b>, a planetary rotation processing unit <b>14252</b>, and an attack angle processing unit <b>14253</b>. Note that, in another embodiment, a part or all of the revolution processing unit <b>14251</b>, the planetary rotation processing unit <b>14252</b>, and the attack angle processing unit <b>14253</b> may be omitted.</p><p id="p-0193" num="0191">The revolution processing unit <b>14251</b> attains the rotation of the visual line direction V around the revolution axis Pc separated from the virtual camera <b>60</b> (refer to <figref idref="DRAWINGS">FIG. <b>10</b></figref>). Note that, the revolution processing unit <b>14251</b> may suitably set the position of the revolution axis Pc, in accordance with the position of the virtual camera <b>60</b>, the position of the predetermined object, or the like.</p><p id="p-0194" num="0192">The planetary rotation processing unit <b>14252</b> attains the rotation of the visual line direction V around the planetary rotation axis <b>61</b> that is the axis parallel to the z direction passing through the virtual camera <b>60</b> (refer to <figref idref="DRAWINGS">FIG. <b>10</b></figref>).</p><p id="p-0195" num="0193">The attack angle processing unit <b>14253</b> attains a change in the attack angle parameter &#x3c8; (refer to <figref idref="DRAWINGS">FIG. <b>5</b></figref>) that is an angle between the visual line direction V of the virtual camera <b>60</b> and the xy plane (that is, a rotation around an axis that is perpendicular to the Vz plane passing through the virtual camera <b>60</b>).</p><p id="p-0196" num="0194">Note that, in one processing cycle, two or more processing units of the revolution processing unit <b>14251</b>, the planetary rotation processing unit <b>14252</b>, and the attack angle processing unit <b>14253</b> may simultaneously attain the processing.</p><p id="p-0197" num="0195">In a case where a predetermined second movement condition is established, the second movement processing unit <b>144</b> updates the position of the predetermined object with respect to the field object. Note that, the predetermined object is arbitrary insofar as the predetermined object is an object of which the position with respect to the field object can be changed, and preferably, as described above, is the first object. The predetermined second movement condition is arbitrary, and for example, may be satisfied by the manipulation information (the movement instruction of the predetermined object), or may be satisfied on the basis of the progress situation of the game and other factors. Note that, the position of the predetermined object, for example, may be defined by the texture coordinate system of the field image.</p><p id="p-0198" num="0196">The deformation processing unit <b>145</b> executes bending deformation processing in which the field surface <b>70</b> (and the field object associated therewith) is subjected to the bending deformation, on the basis of the position and the direction of the virtual camera <b>60</b>. The bending deformation of the field surface <b>70</b> is as described above. In the deformation parameter data <b>13</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, for example, in a case where each of the values of the position parameters (X, Y) corresponds to the specific position A (X<sub>A</sub>, Y<sub>A</sub>), the deformation processing unit <b>145</b> performs the bending deformation with respect to the field surface <b>70</b> (and the field object associated therewith), on the basis of the value &#x3b2;<sub>1 </sub>of the deformation parameter A<b>1</b>, even in a case where the direction of the virtual camera <b>60</b> is any direction. In addition, in a case where each of the values of the position parameters (X, Y) corresponds to the specific position C (X<sub>C</sub>, Y<sub>C</sub>), the deformation processing unit <b>145</b> performs the bending deformation with respect to the field surface <b>70</b> (and the field object associated therewith), on the basis of the value <b>133</b> of the deformation parameter A<b>1</b>, when the direction of the virtual camera <b>60</b> is &#x201c;&#x3b8;<sub>C1</sub>&#x201d;, and performs the bending deformation with respect to the field surface <b>70</b> (and the field object associated therewith), on the basis of the value &#x3b2;<sub>4 </sub>of the deformation parameter A<b>1</b>, when the direction of the virtual camera <b>60</b> is &#x201c;&#x3b8;<sub>C2</sub>&#x201d;.</p><p id="p-0199" num="0197">In this embodiment, the deformation processing unit <b>145</b> includes a first deformation parameter calculation unit <b>1451</b>, a second deformation parameter calculation unit <b>1452</b>, a deformation parameter adjustment unit <b>1453</b>, an origin setting processing unit <b>1454</b>, and a deformation function application unit <b>1455</b>. Note that, unlike the example illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in another embodiment in which the specific position is not set, the first deformation parameter calculation unit <b>1451</b> and the deformation parameter adjustment unit <b>1453</b> may be omitted.</p><p id="p-0200" num="0198">The first deformation parameter calculation unit <b>1451</b> calculates the value of the deformation parameter A<b>1</b>, on the basis of the position and the direction of the virtual camera <b>60</b>, with reference to the deformation parameter data <b>13</b>A according to the specific position as described above with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref> (first deformation parameter calculation processing). An example of the first deformation parameter calculation processing of the first deformation parameter calculation unit <b>1451</b> will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>23</b></figref>.</p><p id="p-0201" num="0199">The second deformation parameter calculation unit <b>1452</b> calculates the value of the deformation parameter A<b>1</b>, on the basis of the position and the direction of the virtual camera <b>60</b>, with reference to the deformation parameter data <b>13</b>B according to the specific object as described above with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref> (second deformation parameter calculation processing). An example of the second deformation parameter calculation processing of the second deformation parameter calculation unit <b>1452</b> will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0202" num="0200">In a case where the value of the deformation parameter A<b>1</b> that is calculated by the first deformation parameter calculation unit <b>1451</b> is not coincident with the value of the deformation parameter A<b>1</b> that is calculated by the second deformation parameter calculation unit <b>1452</b>, the deformation parameter adjustment unit <b>1453</b> calculates an interpolation value by interpolation processing. An example of the interpolation processing according to the deformation parameter will be described below. Note that, in another embodiment, in a case where the value of the deformation parameter A<b>1</b> that is calculated by the first deformation parameter calculation unit <b>1451</b> is not coincident with the value of the deformation parameter A<b>1</b> that is calculated by the second deformation parameter calculation unit <b>1452</b>, the deformation parameter adjustment unit <b>1453</b> may preferentially use one (for example, the value of the deformation parameter A<b>1</b> that is calculated by the second deformation parameter calculation unit <b>1452</b>).</p><p id="p-0203" num="0201">The origin setting processing unit <b>1454</b> sets the position in which the origin O of the local coordinate system in the field object is associated (hereinafter, may be referred to as an &#x201c;origin position&#x201d;), on the basis of the position and the direction of the virtual camera <b>60</b>. The origin setting processing unit <b>1454</b> sets the origin position, in two or more types of setting modes. In this embodiment, as an example, the origin setting processing unit <b>1454</b> sets the origin position, on the basis of the position of the predetermined object, as the first type of setting mode, and sets the origin position, on the basis of the position of the specific object different from the predetermined object, as the second type of setting mode. For example, the origin setting processing unit <b>1454</b> determines whether or not the specific object is positioned in the region falling within the viewing angle of the virtual camera <b>60</b>, and in a case where it is determined that the specific object is positioned in the region falling within the viewing angle of the virtual camera <b>60</b>, the origin setting processing unit <b>1454</b> sets the origin position, on the basis of the position of the specific object. On the other hand, in a case where it is determined that the specific object is not positioned in the region falling within the viewing angle of the virtual camera <b>60</b>, the origin setting processing unit <b>1454</b> sets the origin position, on the basis of the position of the predetermined object. Note that, in another embodiment, the origin setting processing unit <b>1454</b> may set the origin position, on the basis of the position of the predetermined object, and the like, regardless of whether or not the specific object is positioned in the region falling within the viewing angle of the virtual camera <b>60</b>, or may set the origin position, on the basis of other factors.</p><p id="p-0204" num="0202">The deformation function application unit <b>1455</b> performs the bending deformation with respect to the field surface <b>70</b>, on the basis of the origin position that is set by the origin setting processing unit <b>1454</b>, the value of the deformation parameter A<b>1</b> that is calculated (set) by the deformation parameter adjustment unit <b>1453</b>, and the direction of the virtual camera <b>60</b>, and on the basis of the function F<b>1</b> described above with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0205" num="0203">The projection processing unit <b>146</b> arranges various objects other than the background object (the second object and the like) with respect to the field surface <b>70</b> that is subjected to the bending deformation by the deformation processing unit <b>145</b>. The arrangement of various objects can be attained on the basis of the correspondence information described above. At this time, for the predetermined object, the projection processing unit <b>146</b> arranges the predetermined object in the position after being moved that is calculated by the second movement processing unit <b>144</b>. Note that, as described above, the field image may be projected after the field surface <b>70</b> is subjected to the bending deformation.</p><p id="p-0206" num="0204">The background processing unit <b>147</b> arranges the background object with respect to the field surface <b>70</b> that is subjected to the bending deformation by the deformation processing unit <b>145</b>. The background processing unit <b>147</b> sets the position of the background object in the z direction, on the basis of the bending deformation degree of the field surface <b>70</b>. Specifically, the background processing unit <b>147</b> sets the position of the background object with respect to the field object along the z direction, on the basis of the height H<b>1</b> of the virtual horizon line HL that is expressed by the field object (refer to <figref idref="DRAWINGS">FIG. <b>6</b></figref>). For example, in a case where the height H<b>1</b> of the horizon line HL decreases due to a change in the bending deformation degree of the field surface <b>70</b>, the background processing unit <b>147</b> moves the position of the background object in the z direction to the lower side. For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, in a case where the angle &#x3b1; is changed to the angle &#x3b1;&#x2032;, the background processing unit <b>147</b> may move the position of the background object in the z direction to the lower side by a distance &#x394;<b>1</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the distance &#x394;<b>1</b> is a distance between an intersection point P<b>4</b> of the tangent line <b>6213</b> with respect to the field surface <b>70</b> from the virtual camera <b>60</b> (the tangent line within the viewing angle <b>62</b>), with respect to the background surface <b>72</b>, and an intersection point P<b>5</b> of the tangent line <b>6213</b>&#x2032; with respect to the background surface <b>72</b>. In this case, the background object can be arranged in a mode where a sense of discomfort due to the change does not occur, even in a case where the height H<b>1</b> of the horizon line HL is changed due to the change in the bending deformation degree of the bending deformation.</p><p id="p-0207" num="0205">Note that, in a predetermined case, the background processing unit <b>147</b> may not change the position of the background object with respect to the field object along the z direction. For example, in a case where a change amount in the bending deformation degree of the bending deformation by the deformation processing unit <b>145</b> is comparatively small, the position of the background object with respect to the field object along the z direction may not be changed.</p><p id="p-0208" num="0206">The drawing data generation unit <b>148</b> generates the field image (the drawing data) including the expression of the various objects viewed from the virtual camera <b>60</b>.</p><p id="p-0209" num="0207">Next, the operation of the server control unit <b>13</b> according to the drawing function will be further described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> and the subsequence. In the following processing flow diagram (a flowchart), a processing order of each step may be changed, unless a relationship between input and output of each of the steps is impaired.</p><p id="p-0210" num="0208"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a schematic flowchart illustrating a processing flow that is attained by the server control unit <b>13</b>.</p><p id="p-0211" num="0209">The processing illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref> may be executed for each predetermined processing cycle. The predetermined processing cycle may be identical to a frame cycle (an update cycle) of the field image. Note that, hereinafter, a value &#x201c;before being updated&#x201d; corresponds to the previous value based on a certain processing cycle (k+1) (a value derived from the previous processing cycle (k)), and a value &#x201c;after being updated&#x201d; corresponds to the current value derived from the processing cycle (k+1). Note that, here, as an example, in the first processing cycle, the position (u(<b>0</b>), v(<b>0</b>)) of the predetermined object after being updated is set in a predetermined initial position, each value (X(<b>0</b>), Y(<b>0</b>)) of the position parameters (X, Y) of the virtual camera <b>60</b> after being updated is set to be identical to the position (u(<b>0</b>), v(<b>0</b>)) of the predetermined object, and the values (X(<b>0</b>), Y(<b>0</b>), &#x3b3;(<b>0</b>), &#x3b8;(<b>0</b>), &#x3c8;(<b>0</b>)) of the camera parameters are set to the normal values thereof, respectively.</p><p id="p-0212" num="0210">In step S<b>1600</b>, the manipulation information retrieval unit <b>132</b> acquires the manipulation information. Note that, the manipulation information may be received from the terminal device <b>20</b>, in interruption processing, and may be stored in a predetermined storage unit of the server storage unit <b>12</b>. In this case, the manipulation information retrieval unit <b>132</b> sequentially reads out the manipulation information from the predetermined storage unit.</p><p id="p-0213" num="0211">In step S<b>1602</b>, the second movement processing unit <b>144</b> determines whether or not the movement instruction of the predetermined object is included in the manipulation information obtained in step S<b>1600</b>. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>1604</b>, and in the other case, the process proceeds to step S<b>1616</b>.</p><p id="p-0214" num="0212">In step S<b>1604</b>, the second movement processing unit <b>144</b> calculates the position of the predetermined object after being moved, on the basis of the movement instruction of the predetermined object in the manipulation information obtained in step S<b>1600</b>. Here, the position of the predetermined object after being moved is set to (u(k+1), v(k+1)) in the field coordinate. Note that, the position of the predetermined object before being moved is set to (u(k), v(k)) in the field coordinate. In this case, a movement vector in the field coordinate system is (u(k+1)&#x2212;u(k), v(k+1)&#x2212;v(k)). Note that, the movement instruction of the predetermined object may be an instruction indicating such a movement vector (the movement direction).</p><p id="p-0215" num="0213">In step S<b>1606</b>, the first movement processing unit <b>1420</b> calculates each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) of the virtual camera <b>60</b> after being updated, on the basis of the position (u(k+1), v(k+1)) of the predetermined object after being moved that is obtained in step S<b>1604</b>. Note that, each of the values of the position parameters (X, Y) before being updated is set to (X(k), Y(k)). In this case, a change vector of each of the values of the position parameters (X, Y) is (X(k+1)&#x2212;X(k), Y(k+1)&#x2212;Y(k)). In this case, (X(k+1), Y(k+1)) may be calculated to be (X(k+1)&#x2212;X(k), Y(k+1)&#x2212;Y(k))=(u(k+1)&#x2212;u(k), v(k+1)&#x2212;v(k)).</p><p id="p-0216" num="0214">In step S<b>1608</b>, the first distance change unit <b>14211</b> of the distance change unit <b>1421</b> calculates a value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated that is associated with (X(k+1), Y(k+1)) described above, on the basis of each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> (first distance parameter calculation processing). A specific example of the first distance parameter calculation processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>17</b></figref> and <figref idref="DRAWINGS">FIG. <b>18</b></figref>.</p><p id="p-0217" num="0215">In step S<b>1610</b>, the direction change unit <b>1422</b> calculates a value &#x3b8;(k+1) of the direction parameter &#x3b8; that is associated with (X(k+1), Y(k+1)) described above, on the basis of each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> (direction parameter calculation processing). A specific example of the direction parameter calculation processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>19</b></figref>.</p><p id="p-0218" num="0216">In step S<b>1612</b>, the attack angle change unit <b>1423</b> calculates a value &#x3c8;(k+1) of the attack angle parameter &#x3c8; that is associated with (X(k+1), Y(k+1)) described above, on the basis of each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> (attack angle parameter calculation processing). A specific example of the attack angle parameter calculation processing will be outlined below, with reference to <figref idref="DRAWINGS">FIG. <b>20</b></figref>.</p><p id="p-0219" num="0217">In step S<b>1615</b>, the deformation processing unit <b>145</b> executes deformation processing associated with the movement of the predetermined object. A specific example of the deformation processing associated with the movement of the predetermined object will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref>.</p><p id="p-0220" num="0218">In step S<b>1616</b>, the second movement processing unit <b>144</b> sets the position (u(k+1), v(k+1)) of the predetermined object after being updated to the position (u(k), v(k)) of the predetermined object before being updated. That is, the current value is identical to the previous value.</p><p id="p-0221" num="0219">In step S<b>1617</b>, the rotation processing unit <b>1425</b> determines whether or not the rotation instruction of the virtual camera <b>60</b> in the manipulation information obtained in step S<b>1600</b>. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>1618</b>, and in the other case, the current processing cycle ends as it is.</p><p id="p-0222" num="0220">In step S<b>1618</b>, the rotation processing unit <b>1425</b> executes the rotation processing of the virtual camera <b>60</b>, on the basis of the manipulation information obtained in step S<b>1600</b>. The rotation processing is as described above.</p><p id="p-0223" num="0221">In step S<b>1619</b>, the deformation processing unit <b>145</b> executes the bending deformation of the field surface <b>70</b> (and the field object associated therewith) described above with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, and the like, on the basis of the visual line direction V of the virtual camera <b>60</b> after the rotation processing in step S<b>1618</b>.</p><p id="p-0224" num="0222">In step S<b>1620</b>, the background processing unit <b>147</b> determines whether or not the current value &#x3b2;(k+1) of the deformation parameter A<b>1</b> that is used in the current processing cycle is changed with respect to the previous value &#x3b2;(k). In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>1622</b>, and in the other case, the process proceeds to step S<b>1623</b>. Note that, in a modification example, in step S<b>1620</b>, the background processing unit <b>147</b> determines whether or not a change amount of the current value &#x3b2;(k+1) of the deformation parameter A<b>1</b> that is used in the current processing cycle with respect to the previous value &#x3b2;(k) is a predetermined amount or more. Accordingly, in a case where the change amount is the predetermined amount or less, step S<b>1622</b> can be skipped, and thus, a processing load for calculating the position of the background object in the z direction can be reduced. In addition, in another modification example, in step S<b>1620</b>, the current value &#x3b2;(k+1) of the deformation parameter A<b>1</b> may not be directly compared with the previous value &#x3b2;(k) of the deformation parameter A<b>1</b>. For example, unlike this embodiment, in another embodiment where a specific position in which the deformation degree is changed during the revolution of the virtual camera <b>60</b>, such as the specific position C described above, is not set, in a case where processing contents to step S<b>1619</b> in the current processing cycle are the revolution, a determination result in this step S<b>1620</b> may be set to &#x201c;negative determination (NO)&#x201d;, without directly comparing the current value &#x3b2;(k+1) of the deformation parameter A<b>1</b> with the previous value &#x3b2;(k) of the deformation parameter A<b>1</b>.</p><p id="p-0225" num="0223">In step S<b>1623</b>, the second distance change unit <b>14212</b> of the distance change unit <b>1421</b> executes the second distance parameter calculation processing. A specific example of the second distance parameter calculation processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>27</b></figref>.</p><p id="p-0226" num="0224">In step S<b>1624</b>, the update reflection unit <b>1424</b> positions the virtual camera <b>60</b> in the global coordinate system, on the basis of each of the values (X(k+1), Y(k+1), &#x3b3;(k+1), &#x3b8;(k+1), &#x3c8;(k+1)) of the various parameters after being updated that are obtained in step S<b>1606</b> to step S<b>1612</b>, or each of the values (X(k+1), Y(k+1), &#x3b3;(k+1), &#x3b8;(k+1), &#x3c8;(k+1)) of various parameters after being updated that are obtained in step S<b>1606</b>, step S<b>1610</b> to step S<b>1612</b>, and step S<b>1623</b>.</p><p id="p-0227" num="0225">In step S<b>1625</b>, the drawing data generation unit <b>148</b> generates various drawing data items after being updated (the drawing data of the field image) in the current processing cycle.</p><p id="p-0228" num="0226">In step S<b>1626</b>, the drawing data transmission unit <b>134</b> transmits the drawing data generated in step S<b>1625</b> to the terminal communication unit <b>21</b>. Note that, in a case where the drawing data is received, the terminal communication unit <b>21</b> updates the display of the field image on the display unit <b>23</b>, on the basis of the drawing data.</p><p id="p-0229" num="0227">As described above, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, the drawing data reflecting the manipulation information can be generated on the basis of the manipulation information received from the terminal device <b>20</b>, and the generated drawing data can be transmitted to the terminal device <b>20</b>. Accordingly, the update of the field image associated with the progress of the game can be attained in real time.</p><p id="p-0230" num="0228"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a schematic flowchart illustrating an example of the first distance parameter calculation processing (step S<b>1608</b>). <figref idref="DRAWINGS">FIG. <b>18</b></figref> is an explanatory diagram of an interpolation processing range, and is a perspective view illustrating the field surface <b>70</b>.</p><p id="p-0231" num="0229">In step S<b>1700</b>, the first distance change unit <b>14211</b> determines whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> corresponds to an arbitrary specific position set in the distance parameter data <b>14</b>A (refer to <figref idref="DRAWINGS">FIG. <b>14</b></figref>). In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>1702</b>, and in the other case, the process proceeds to step S<b>1704</b>.</p><p id="p-0232" num="0230">In step S<b>1702</b>, the first distance change unit <b>14211</b> associates the value of the distance parameter A<b>2</b> that is associated with a specific position corresponding to each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated, with the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated. For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref>, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated corresponds to the specific position A (X<sub>A</sub>, Y<sub>A</sub>), the value &#x3b3;(k+1) of the distance parameter A<b>2</b> is set to</p><p id="p-0233" num="0231">In step S<b>1704</b>, the first distance change unit <b>14211</b> determines whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> is within an interpolation processing range associated with the arbitrary specific position set in the distance parameter data <b>14</b>A (refer to <figref idref="DRAWINGS">FIG. <b>14</b></figref>). The interpolation processing range may be set in association with the texture coordinate system (=the field coordinate system), for each of the specific positions. In this embodiment, as illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the interpolation processing range is simply set to be within a circular region having a radius r, with the specific position as the center. In this case, it may be determined whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated is within the circular region having the radius r, with the specific position as the center. Here, in another embodiment, the interpolation processing range may be defined by a region in another form. For example, in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is positioned within the interpolation processing range, and the values of the distance parameter A<b>2</b> and the attack angle parameter are normal values &#x3b3;<sub>0 </sub>and &#x3c8;<sub>0</sub>, respectively, the interpolation processing range may be set such that the specific position according to the interpolation processing range is positioned in the region falling within the viewing angle of the virtual camera <b>60</b>, even at the time of the value of the arbitrary direction parameter &#x3b8;. The same applies to other interpolation processing ranges described below. In <figref idref="DRAWINGS">FIG. <b>18</b></figref>, on the field surface <b>70</b>, three specific positions Ps(<b>1</b>) to Ps(<b>3</b>) are schematically illustrated, and interpolation processing ranges Rs(<b>1</b>) to Rs(<b>3</b>) associated with the specific positions, respectively, are schematically illustrated. Note that, in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, the interpolation processing range Rs(<b>2</b>) and the interpolation processing range Rs(<b>3</b>) overlap with each other, and an overlap region Rs&#x2032; is illustrated by a hatching region. Note that, each of the specific positions Ps(<b>1</b>), Ps(<b>2</b>), and Ps(<b>3</b>) is set to the distance parameter data <b>14</b>A (refer to <figref idref="DRAWINGS">FIG. <b>14</b></figref>), as with the specific positions A and B, and the like. Note that, the interpolation processing range (the same applies to interpolation processing ranges according to other parameters described below) may be defined in advance by the distance parameter data <b>14</b>A and the like.</p><p id="p-0234" num="0232">In step S<b>1706</b>, the first distance change unit <b>14211</b> calculates a distance between each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated and the specific position according to the interpolation processing range Rs to which each of the values belongs (an interpolation distance). For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated is positioned within the interpolation processing range Rs(<b>1</b>), a distance d(<b>1</b>) between (X(k+1), Y(k+1)) and the specific position Ps(<b>1</b>) according to the interpolation processing range Rs(<b>1</b>) is calculated. On the other hand, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref>, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated is positioned within the overlap region Rs&#x2032;, a distance d(<b>2</b>) between (X(k+1), Y(k+1)) and the specific position Ps(<b>2</b>) according to the interpolation processing range Rs(<b>2</b>) and a distance d(<b>3</b>) between (X(k+1), Y(k+1)) and the specific position Ps(<b>3</b>) according to the interpolation processing range Rs(<b>3</b>) are calculated.</p><p id="p-0235" num="0233">In step S<b>1708</b>, the first distance change unit <b>14211</b> calculates an interpolation value of the distance parameter A<b>2</b>, on the basis of the distance obtained in step S<b>1706</b>. For example, an interpolation value &#x3b3;(<b>1</b>) of the distance parameter A<b>2</b> according to the distance d(<b>1</b>) described above may be calculated by the following formula.</p><p id="p-0236" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;(1)=(&#x3b3;<sub>1</sub>&#x2212;&#x3b3;<sub>0</sub>)/<i>r</i>&#xd7;(<i>r&#x2212;d</i>(1))+&#x3b3;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0237" num="0234">The value &#x3b3;<sub>1 </sub>is a value associated with the specific position Ps(<b>1</b>), and is less than the normal value &#x3b3;<sub>0</sub>, as described above.</p><p id="p-0238" num="0235">On the other hand, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated is positioned within the overlap region Rs&#x2032;, an interpolation value &#x3b3;(Rs&#x2032;) may be calculated by the following formula, on the basis of an interpolation value &#x3b3;(<b>2</b>) of the distance parameter A<b>2</b> according to the distance d(<b>2</b>) described above and an interpolation value &#x3b3;(<b>3</b>) of the distance parameter A<b>2</b> according to the distance d(<b>3</b>) described above.</p><p id="p-0239" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;(<i>Rs</i>&#x2032;)=<i>B</i>0&#x3b3;(2)+(1&#x2212;<i>B</i>0)&#xd7;&#x3b3;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0240" num="0236">Here, &#x3b3;(<b>2</b>) and &#x3b3;(<b>3</b>) are as described below.</p><p id="p-0241" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;(2)=(&#x3b3;<sub>2</sub>&#x2212;&#x3b3;<sub>0</sub>)/<i>r</i>&#xd7;(<i>r&#x2212;d</i>(2))+&#x3b3;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0242" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;(3)=(&#x3b3;<sub>3</sub>&#x2212;&#x3b3;<sub>0</sub>)/<i>r</i>&#xd7;(<i>r&#x2212;d</i>(3))+&#x3b3;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0243" num="0237">The values &#x3b3;<sub>2 </sub>and &#x3b3;<sub>3 </sub>are values associated with the specific positions Ps(<b>2</b>) and Ps(<b>3</b>), respectively, and are less than the normal value &#x3b3;<sub>0</sub>, as described above. B<b>0</b> is a coefficient that is changed within a range of 0 to 1, is close to 1 as each of the values of the position parameters (X, Y) after being updated is close to the specific position Ps(<b>2</b>), and is 1 at a boundary position on the specific position Ps(<b>2</b>) side in the overlap region Rs&#x2032;. In addition, the coefficient B<b>0</b> is close to 0 as each of the values of the position parameters (X, Y) after being updated is close to the specific position Ps(<b>3</b>), and is 0 at a boundary position on the specific position Ps(<b>3</b>) side in the overlap region Rs&#x2032;. For example, B<b>0</b> may be as described below.</p><p id="p-0244" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i>0=(<i>r&#x2212;d</i>(2))/{(<i>r&#x2212;d</i>(2))+(<i>r&#x2212;d</i>(3))}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0245" num="0238">In step S<b>1710</b>, the first distance change unit <b>14211</b> sets the interpolation value calculated in step S<b>1708</b> to the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated.</p><p id="p-0246" num="0239">In step S<b>1712</b>, the first distance change unit <b>14211</b> sets the normal value &#x3b3;<sub>0 </sub>to the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated.</p><p id="p-0247" num="0240">As described above, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, the value of the distance parameter A<b>2</b> can be gradually changed in tandem with a change in each of the values of the position parameters (X, Y) for each predetermined processing cycle, and thus, for example, a gentle change in the distance can be attained, compared to a case where the normal value &#x3b3;<sub>0 </sub>is rapidly changed to the value &#x3b3;<sub>1 </sub>or the like in a processing cycle reaching the specific position. As a result thereof, the value of the distance parameter A<b>2</b> can be changed while reducing a sense of discomfort that can be felt by the user.</p><p id="p-0248" num="0241"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a schematic flowchart illustrating an example of the direction parameter calculation processing (step S<b>1610</b>).</p><p id="p-0249" num="0242">In step S<b>1900</b>, the direction change unit <b>1422</b> determines whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> corresponds to an arbitrary direction change position set in the direction parameter data (refer to <figref idref="DRAWINGS">FIG. <b>15</b></figref>). In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>1902</b>, and in the other case, the process proceeds to step S<b>1904</b>.</p><p id="p-0250" num="0243">In step S<b>1902</b>, the direction change unit <b>1422</b> sets the value of the direction parameter &#x3b8; associated with the direction change position corresponding to each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated to the value &#x3b8;(k+1) of the direction parameter &#x3b8;. For example, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated corresponds to the direction change position T<b>1</b> (X<sub>P1</sub>, Y<sub>P1</sub>), the value &#x3b8;(k+1) of the direction parameter &#x3b8; is set to &#x3b8;<sub>1</sub>.</p><p id="p-0251" num="0244">In step S<b>1904</b>, the direction change unit <b>1422</b> determines whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> is within an interpolation processing range associated with the arbitrary direction change position set in the direction parameter data (refer to <figref idref="DRAWINGS">FIG. <b>15</b></figref>). The interpolation processing range may be set for each of the direction change positions. In this embodiment, as with the interpolation processing range according to the distance parameter A<b>2</b>, the interpolation processing range is simply set to be within the circular region having the radius r, with the direction change position as the center (refer to <figref idref="DRAWINGS">FIG. <b>18</b></figref>). Here, in another embodiment, the interpolation processing range may be defined by a region in another form. In addition, in another embodiment, the interpolation processing range may be set with respect to a part of all of the direction change position. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>1906</b>, and in the other case, the process proceeds to step S<b>1912</b>.</p><p id="p-0252" num="0245">In step S<b>1906</b>, the direction change unit <b>1422</b> calculates a distance between each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated and the specific position according to the interpolation processing range Rs to which each of the values belongs. A calculation method of the distance may be the same as that in step S<b>1706</b> described above.</p><p id="p-0253" num="0246">In step S<b>1908</b>, the direction change unit <b>1422</b> calculates the interpolation value of the direction parameter &#x3b8;, on the basis of the distance obtained in step S<b>1906</b>. A calculation method of the interpolation value may be the same as that in step S<b>1708</b> described above.</p><p id="p-0254" num="0247">In step S<b>1910</b>, the direction change unit <b>1422</b> sets the interpolation value calculated in step S<b>1908</b> to the value &#x3b8;(k+1) of the direction parameter &#x3b8;.</p><p id="p-0255" num="0248">In step S<b>1912</b>, the direction change unit <b>1422</b> sets the normal value &#x3b8;<sub>0 </sub>to the value &#x3b8;(k+1) of the direction parameter &#x3b8;. The normal value &#x3b8;<sub>0 </sub>may be set such that the projection vector V&#x2032; is perpendicular to the movement vector (u(k+1)&#x2212;u(k), v(k+1)&#x2212;v(k)) of the predetermined object.</p><p id="p-0256" num="0249">As described above, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the value of the direction parameter &#x3b8; can be gradually changed in tandem with a change in each of the values of the position parameters (X, Y) for each predetermined processing cycle, and thus, for example, a gentle change in the direction, in which a sense of discomfort that can be felt by the user can be reduced, can be attained, compared to a case where the normal value &#x3b8;<sub>0 </sub>is rapidly changed to the value &#x3b8;<sub>1 </sub>or the like in a processing cycle reaching the direction change position.</p><p id="p-0257" num="0250"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a schematic flowchart illustrating an example of the attack angle parameter calculation processing (step S<b>1612</b>). The processing illustrated in <figref idref="DRAWINGS">FIG. <b>20</b></figref> is substantially identical to the direction parameter calculation processing illustrated in <figref idref="DRAWINGS">FIG. <b>19</b></figref> described above, except that the parameters are different, and thus, the description thereof will be omitted.</p><p id="p-0258" num="0251"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a schematic flowchart illustrating an example of the deformation processing associated with the movement of the predetermined object (step S<b>1615</b>). <figref idref="DRAWINGS">FIG. <b>22</b></figref> is an explanatory diagram of the bending deformation processing, and is a perspective view in which the local coordinate system is associated on the field surface <b>70</b> onto which the field image illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> is projected.</p><p id="p-0259" num="0252">In step S<b>2102</b>, the first deformation parameter calculation unit <b>1451</b> calculates the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated, with reference to the deformation parameter data <b>13</b>A according to the specific position (<figref idref="DRAWINGS">FIG. <b>13</b></figref>) (the first deformation parameter calculation processing). Hereinafter, the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated that is obtained in the first deformation parameter calculation processing will be referred to as a first value &#x3b2;&#x2032;(k+1) of the deformation parameter A<b>1</b> after being updated, for distinguishment. A specific example of the first deformation parameter calculation processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>23</b></figref>.</p><p id="p-0260" num="0253">In step S<b>2104</b>, the second deformation parameter calculation unit <b>1452</b> calculates the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated, with reference to the deformation parameter data <b>13</b>B according to the specific object (<figref idref="DRAWINGS">FIG. <b>13</b></figref>) (the second deformation parameter calculation processing). Hereinafter, the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated that is obtained in the second deformation parameter calculation processing to a second value &#x3b2;&#x2033;(k+1) of the deformation parameter A<b>1</b> after being updated, for distinguishment. A specific example of the second deformation parameter calculation processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>24</b></figref>.</p><p id="p-0261" num="0254">In step S<b>2106</b>, the deformation parameter adjustment unit <b>1453</b> sets the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being finally updated, on the basis of the first value &#x3b2;&#x2032;(k+1) of the deformation parameter A<b>1</b> after being updated that is obtained in step S<b>2102</b> and the second value &#x3b2;&#x2033;(k+1) of the deformation parameter A<b>1</b> after being updated that is obtained in step S<b>2104</b> (deformation parameter adjustment processing). A specific example of the deformation parameter adjustment processing will be described below, with reference to <figref idref="DRAWINGS">FIG. <b>25</b></figref>.</p><p id="p-0262" num="0255">In step S<b>2108</b>, the origin setting processing unit <b>1454</b> executes origin setting processing for setting the origin position (the position in which the origin O of the local coordinate system in the field object is associated) (an example of the predetermined position). A specific example of the origin setting processing will be outlined below, with reference to <figref idref="DRAWINGS">FIG. <b>26</b></figref>.</p><p id="p-0263" num="0256">In step S<b>2110</b>, the deformation function application unit <b>1455</b> associates the field coordinate system of the field surface <b>70</b> (the texture coordinate system of the field image) with the local coordinate system (refer to <figref idref="DRAWINGS">FIG. <b>7</b></figref> and <figref idref="DRAWINGS">FIG. <b>22</b></figref>), on the basis of the origin O associated with the origin position set in step S<b>2108</b>, and the value &#x3b8;(k+1) of the direction parameter &#x3b8;. Specifically, an axis that passes through the origin set in step S<b>2108</b> and has the value &#x3b8;(k+1) of the direction parameter &#x3b8; with respect to the x direction is set to the Xc axis. Note that, in the example illustrated in <figref idref="DRAWINGS">FIG. <b>22</b></figref>, a state is illustrated in which the local coordinate system is associated, with (u(k+1), v(k+1)) on the field surface <b>70</b> as the origin position.</p><p id="p-0264" num="0257">In step S<b>2112</b>, the deformation function application unit <b>1455</b> performs the bending deformation with respect to the field surface <b>70</b>, on the basis of the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated that is set in step S<b>2106</b> and the local coordinate system associated with the field coordinate system of the field surface <b>70</b> in step S<b>2110</b>. In this case, for example, the deformation function application unit <b>1455</b> is capable of performing the bending deformation with respect to the field surface <b>70</b>, on the basis of the function F<b>1</b> described above with reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0265" num="0258"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is a schematic flowchart illustrating the first deformation parameter calculation processing of the first deformation parameter calculation unit <b>1451</b> (step S<b>2102</b>).</p><p id="p-0266" num="0259">In step S<b>2300</b>, the first deformation parameter calculation unit <b>1451</b> determines whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> corresponds to the arbitrary specific position set in the deformation parameter data <b>13</b>A (refer to <figref idref="DRAWINGS">FIG. <b>13</b></figref>). In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2302</b>, and in the other case, the process proceeds to step S<b>2304</b>.</p><p id="p-0267" num="0260">In step S<b>2302</b>, the first deformation parameter calculation unit <b>1451</b> sets the value of the deformation parameter A<b>1</b> associated with the specific position corresponding to each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated to the first value &#x3b2;&#x2032;(k+1) of the deformation parameter A<b>1</b> after being updated. For example, in the deformation parameter data <b>13</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated corresponds to the specific position A (X<sub>A</sub>, Y<sub>A</sub>), the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated is set to &#x3b2;<sub>1</sub>.</p><p id="p-0268" num="0261">In step S<b>2304</b>, the first deformation parameter calculation unit <b>1451</b> determines whether or not each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated that are obtained in step S<b>1606</b> is within an interpolation processing range associated with the arbitrary specific position set in the deformation parameter data (refer to <figref idref="DRAWINGS">FIG. <b>13</b></figref>). The interpolation processing range may be set for each of the specific positions. In this embodiment, as with the interpolation processing range associated with the specific position according to the distance parameter A<b>2</b>, the interpolation processing range is simply set to be within the circular region having the radius r, with the specific position as the center (refer to <figref idref="DRAWINGS">FIG. <b>18</b></figref>). Here, in another embodiment, the interpolation processing range may be defined by a region in another form, as described above.</p><p id="p-0269" num="0262">In step S<b>2306</b>, the first deformation parameter calculation unit <b>1451</b> calculates a distance d(k+1) between each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated and the specific position according to the interpolation processing range to which each of the values belongs. A calculation method of the distance may be the same as that in step S<b>1706</b> described above.</p><p id="p-0270" num="0263">In step S<b>2308</b>, the first deformation parameter calculation unit <b>1451</b> calculates an interpolation value of the deformation parameter A<b>1</b>, on the basis of the distance d(k+1) obtained in step S<b>2306</b>. A calculation method of the interpolation value may be the same as that in step S<b>1708</b> described above.</p><p id="p-0271" num="0264">Here, in the case of a specific position in which the deformation degree is changed during the revolution of the virtual camera <b>60</b>, such as the specific position C of the deformation parameter data <b>13</b>A in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, calculation may be performed as described below, on the basis of the value &#x3b8;(k+1) of the direction parameter &#x3b8;. Here, the specific position C will be described. First, a value &#x3b2;(C<sub>0</sub>) of the deformation parameter A<b>1</b> associated with the specific position C may be calculated by Formula (1) described below when &#x3b8;<sub>C1</sub>&#x2212;&#x394;&#x3b8;<b>1</b>&#x2264;&#x3b8;(k+1)&#x2264;&#x3b8;<sub>C1</sub>+&#x394;&#x3b8;<b>1</b>, and may be calculated by Formula (2) described below when &#x3b8;<sub>C2</sub>&#x2212;&#x394;&#x3b8;<b>1</b>&#x2264;&#x3b8;(k+1)&#x2264;&#x3b8;<sub>C2</sub>+&#x394;&#x3b8;<b>1</b>, and in the other range, the value &#x3b2;(C<sub>0</sub>) may be set to the normal value &#x3b2;<sub>0</sub>.</p><p id="p-0272" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>C</i><sub>0</sub>)=&#x2212;(&#x3b2;<sub>3</sub>&#x2212;&#x3b2;<sub>0</sub>)/&#x394;&#x3b8;1&#xd7;|(&#x3b8;(<i>k+</i>1)&#x2212;&#x3b8;<sub>C1</sub>)|+&#x3b2;<sub>3 </sub>&#x2003;&#x2003;Formula (1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0273" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>C</i><sub>0</sub>)=&#x2212;(&#x3b2;<sub>4</sub>&#x2212;&#x3b2;<sub>0</sub>)/&#x394;&#x3b8;1&#xd7;|(&#x3b8;(<i>k+</i>1)&#x2212;&#x3b8;<sub>C2</sub>)|+&#x3b2;<sub>4 </sub>&#x2003;&#x2003;Formula (2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0274" num="0265">For example, in Formula (1), a value obtained by multiplying an absolute value of (&#x3b8;(k+1)&#x2212;&#x3b8;<sub>C1</sub>) by (&#x3b2;<sub>3</sub>&#x2212;&#x3b2;<sub>0</sub>)/&#x394;&#x3b8;<b>1</b> is subtracted from &#x3b2;<sub>3</sub>, and is set to &#x3b2;(C<sub>0</sub>). Here, &#x394;&#x3b8;<b>1</b> is a value for setting the interpolation angle range, and &#x3b2;<sub>3 </sub>and &#x3b2;<sub>4 </sub>are the value of the deformation parameter A<b>1</b> associated with the specific direction of the specific position C, and are greater than the normal value &#x3b2;<sub>0</sub>, as described above. Note that, here, in Formula (1) and Formula (2), the same &#x394;&#x3b8;<b>1</b> is used, but different &#x394;&#x3b8;<b>1</b> may be used.</p><p id="p-0275" num="0266">Then, in a case where a distance between the specific position C and (X(k+1), Y(k+1)) is set to a distance d(d<sub>C</sub>), an interpolation value &#x3b2;(d<sub>C</sub>) of the deformation parameter A<b>1</b> according to the distance d(d<sub>C</sub>) may be calculated by the following formula, by using &#x3b2;(C<sub>0</sub>) described above.</p><p id="p-0276" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>d</i><sub>C</sub>)=(&#x3b2;(<i>C</i><sub>0</sub>)&#x2212;&#x3b2;<sub>0</sub>)/<i>r</i>&#xd7;(<i>r&#x2212;d</i>(<i>d</i><sub>C</sub>))+&#x3b2;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0277" num="0267">In addition, for example, a case where the interpolation processing ranges respectively according to the specific position A and the specific position B include the overlap region Rs&#x2032; is the same as the case of the distance parameter A<b>2</b>. Specifically, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated is positioned within the overlap region Rs&#x2032;, when a distance between the specific position A and (X(k+1), Y(k+1)) is set to a distance d(d<sub>A</sub>), and a distance between the specific position B and (X(k+1), Y(k+1)) is set to a distance d(d<sub>B</sub>), an interpolation value &#x3b2;(Rs&#x2032;) may be calculated by the following formula, on the basis of an interpolation value &#x3b2;(d<sub>A</sub>) of the deformation parameter A<b>1</b> according to the distance d(d<sub>A</sub>) and an interpolation value &#x3b2;(d<sub>B</sub>) of the deformation parameter A<b>1</b> according to the distance d(d<sub>B</sub>).</p><p id="p-0278" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>Rs</i>&#x2032;)=<i>B</i>1&#x3b2;(<i>d</i><sub>A</sub>)+(1&#x2212;<i>B</i>1)&#xd7;&#x3b2;(<i>d</i><sub>B</sub>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0279" num="0268">Here, &#x3b2;(d<sub>A</sub>) and &#x3b2;(d<sub>B</sub>) are as described below.</p><p id="p-0280" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>d</i><sub>A</sub>)=(&#x3b2;<sub>1</sub>&#x2212;&#x3b2;<sub>0</sub>)/<i>r</i>&#xd7;(<i>r&#x2212;d</i>(<i>d</i><sub>A</sub>))+&#x3b2;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0281" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>d</i><sub>B</sub>)=(&#x3b2;<sub>2</sub>&#x2212;&#x3b2;<sub>0</sub>)/<i>r</i>&#xd7;(<i>r&#x2212;d</i>(<i>d</i><sub>B</sub>))+&#x3b2;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0282" num="0269">B<b>1</b> is a coefficient that is changed within a range of 0 to 1, is close to 1 as each of the values of the position parameters (X, Y) after being updated is close to the specific position A, and is 1 at a boundary position on the specific position A side in the overlap region Rs&#x2032;. In addition, the coefficient B<b>1</b> is close to 0 as each of the values of the position parameters (X, Y) after being updated is close to the specific position B, and is 0 at a boundary position on the specific position B side in the overlap region Rs&#x2032;. For example, B<b>1</b> may be as described below.</p><p id="p-0283" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>B</i>1=(<i>r&#x2212;d</i>(<i>d</i><sub>A</sub>))/{(<i>r&#x2212;d</i>(<i>d</i><sub>A</sub>))+(<i>r&#x2212;d</i>(<i>d</i><sub>B</sub>))}<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0284" num="0270">In step S<b>2310</b>, the first deformation parameter calculation unit <b>1451</b> sets the interpolation value calculated in step S<b>2308</b> to the first value &#x3b2;&#x2032;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0285" num="0271">In step S<b>2312</b>, the first deformation parameter calculation unit <b>1451</b> sets the normal value &#x3b2;<sub>0 </sub>to the first value &#x3b2;&#x2032;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0286" num="0272"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a schematic flowchart illustrating the second deformation parameter calculation processing of the second deformation parameter calculation unit <b>1452</b> (step S<b>2104</b>).</p><p id="p-0287" num="0273">In step S<b>2400</b>, the second deformation parameter calculation unit <b>1452</b> determines whether or not the specific object is positioned in the region of the field object that is positioned within the viewing angle of the virtual camera <b>60</b> positioned in an absolute coordinate system on the basis of each of the values (X(k+1), Y(k+1), &#x3b3;(k+1), &#x3b8;(k+1), &#x3c8;(k+1)) of the camera parameters after being updated that are obtained in step S<b>1608</b> to step S<b>1612</b> (hereinafter, may be simply referred to as a &#x201c;region of the field object within the viewing angle of the virtual camera <b>60</b>&#x201d; or may be simply referred to as a &#x201c;region within the viewing angle&#x201d;). Note that, the region of the field object within the viewing angle of the virtual camera <b>60</b> is unambiguously set on the basis of each of the values of the camera parameters.</p><p id="p-0288" num="0274">Note that, the distance of the region of the field object in the regular state within the viewing angle of the virtual camera <b>60</b> in a horizontal direction (for example, a distance in the field coordinate system) is set in accordance with the value of the distance parameter A<b>2</b>. Accordingly, for example, in a case where the distance of the field object within the viewing angle of the virtual camera <b>60</b> in the horizontal direction is set to L<b>1</b>, it may be determined whether or not L<b>1</b>/2&#x2264;d<sub>L</sub>(k+1), on the basis of a distance d<sub>L</sub>(k+1) between the projection vector V&#x2032; of the virtual camera <b>60</b> and the specific object in a direction perpendicular to the projection vector V&#x2032; (hereinafter, may be simply referred to as the &#x201c;horizontal direction&#x201d;). In this case, when L<b>1</b>/2&#x2264;d<sub>L</sub>(k+1), it may be determined that the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>.</p><p id="p-0289" num="0275">In this embodiment, in a case where each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated is positioned in the interpolation processing range associated with the specific object, it is determined that the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>. In this case, as described above, an interpolation processing range associated with one specific object may be set such that the one specific object is positioned in the region falling within the viewing angle of the virtual camera <b>60</b> in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is positioned within the interpolation processing range.</p><p id="p-0290" num="0276">In this step S<b>2400</b>, in a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2402</b>, and in the other case, the process proceeds to step S<b>2410</b>.</p><p id="p-0291" num="0277">In step S<b>2402</b>, the second deformation parameter calculation unit <b>1452</b> determines whether or not the specific object is positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b>. For example, in a case where the distance d<sub>L</sub>(k+1) between the projection vector V&#x2032; of the virtual camera <b>60</b> and the specific object in the horizontal direction is 0, the second deformation parameter calculation unit <b>1452</b> may determine that the specific object is positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b>. Alternatively, in a case where the distance d<sub>L</sub>(k+1) in the horizontal direction is a predetermined distance (a distance significantly less than L<b>1</b>/2, for example, L<b>1</b>/4) or less, the second deformation parameter calculation unit <b>1452</b> may determine that the specific object is positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b>. Note that, the projection vector V&#x2032; of the virtual camera <b>60</b> can be derived on the basis of each of the values (X(k+1), Y(k+1), &#x3b8;(k+1)) of the position parameters and the direction parameters after being updated that are obtained in step S<b>1608</b> and step S<b>1610</b>. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2404</b>, and in the other case, the process proceeds to step S<b>2406</b>.</p><p id="p-0292" num="0278">In step S<b>2404</b>, the second deformation parameter calculation unit <b>1452</b> sets the value of the deformation parameter A<b>1</b> associated with the specific object positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b> to the second value &#x3b2;&#x2033;(k+1) of the deformation parameter A<b>1</b> after being updated. For example, in the case of the deformation parameter data <b>13</b>B illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref>, when the specific object G<b>1</b> is positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b>, the value &#x3b2;<sub>G1 </sub>of the deformation parameter A<b>1</b> associated with the specific object G<b>1</b> is associated with the second value &#x3b2;&#x2033;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0293" num="0279">In step S<b>2406</b>, the second deformation parameter calculation unit <b>1452</b> calculates the interpolation value of the deformation parameter A<b>1</b>, on the basis of the distance d<sub>L</sub>(k+1) between the projection vector V&#x2032; of the virtual camera <b>60</b> and the specific object in the horizontal direction. For example, an interpolation value &#x3b2;(d<sub>L</sub>) may be calculated by the following formula, by using the distance d<sub>L</sub>(k+1) in the horizontal direction described above.</p><p id="p-0294" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;(<i>d</i><sub>L</sub>)=(&#x3b2;<sub>G</sub>*&#x2212;&#x3b2;<sub>0</sub>)/<i>L</i>1/2&#xd7;(<i>L</i>1/2&#x2212;<i>d</i>(<i>d</i><sub>L</sub>))+&#x3b2;<sub>0 </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0295" num="0280">A value &#x3b2;<sub>G</sub>* is the value of the deformation parameter A<b>1</b> associated with the specific object positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, and is the value &#x3b2;<sub>G1 </sub>in the case of the specific object G<b>1</b>. L<b>1</b>/2 is half of the distance L<b>1</b> of the region of the field object within the viewing angle of the virtual camera <b>60</b> in the horizontal direction.</p><p id="p-0296" num="0281">In step S<b>2408</b>, the second deformation parameter calculation unit <b>1452</b> sets the interpolation value of the deformation parameter A<b>1</b> that is obtained in step S<b>2406</b> to the second value &#x3b2;&#x2033;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0297" num="0282">In step S<b>2410</b>, the second deformation parameter calculation unit <b>1452</b> sets the normal value &#x3b2;<sub>0 </sub>to the second value &#x3b2;&#x2033;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0298" num="0283"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is a schematic flowchart illustrating the deformation parameter adjustment processing of the deformation parameter adjustment unit <b>1453</b> (step S<b>2106</b>).</p><p id="p-0299" num="0284">In step S<b>2500</b>, the deformation parameter adjustment unit <b>1453</b> determines whether or not both of the first value &#x3b2;&#x2032;(k+1) obtained in step S<b>2102</b> and the second value &#x3b2;&#x2033;(k+1) obtained in step S<b>2104</b> are the normal value &#x3b2;<sub>0</sub>. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2502</b>, and in the other case, the process proceeds to step S<b>2506</b>.</p><p id="p-0300" num="0285">In step S<b>2502</b>, the deformation parameter adjustment unit <b>1453</b> sets a parameter state to a &#x201c;normal state&#x201d;. The normal state corresponds to a state in which the value of the deformation parameter A<b>1</b> is the normal value &#x3b2;<sub>0</sub>.</p><p id="p-0301" num="0286">In step S<b>2504</b>, the deformation parameter adjustment unit <b>1453</b> associates the normal value &#x3b2;<sub>0 </sub>with the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated. In this case, the adjustment of the deformation parameter adjustment unit <b>1453</b> is not attained.</p><p id="p-0302" num="0287">In step S<b>2506</b>, the deformation parameter adjustment unit <b>1453</b> determines whether or not the second value &#x3b2;&#x2033;(k+1) obtained in step S<b>2104</b> is the normal value &#x3b2;<sub>0</sub>. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2508</b>, and in the other case, the process proceeds to step S<b>2512</b>.</p><p id="p-0303" num="0288">In step S<b>2508</b>, the deformation parameter adjustment unit <b>1453</b> sets the parameter state to a &#x201c;first state&#x201d;. The first state corresponds to a state in which the value of the deformation parameter A<b>1</b> is the first value &#x3b2;&#x2032;(k+1).</p><p id="p-0304" num="0289">In step S<b>2510</b>, the deformation parameter adjustment unit <b>1453</b> associates the first value &#x3b2;&#x2032;(k+1) obtained in step S<b>2102</b> with the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0305" num="0290">In step S<b>2512</b>, the deformation parameter adjustment unit <b>1453</b> determines whether or not the first value &#x3b2;&#x2032;(k+1) obtained in step S<b>2102</b> is the normal value &#x3b2;<sub>0</sub>. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2514</b>, and in the other case (that is, in a case where both of the values are not the normal value), the process proceeds to step S<b>2518</b>.</p><p id="p-0306" num="0291">In step S<b>2514</b>, the deformation parameter adjustment unit <b>1453</b> sets the parameter state to a &#x201c;second state&#x201d;. The second state corresponds to a state in which the value of the deformation parameter A<b>1</b> is the second value &#x3b2;&#x2033;(k+1). Hereinafter, in the second state, a specific object according to the second value &#x3b2;&#x2033;(k+1) may be referred to as a &#x201c;specific object of an attentive target&#x201d;.</p><p id="p-0307" num="0292">In step S<b>2516</b>, the deformation parameter adjustment unit <b>1453</b> associates the second value &#x3b2;&#x2033;(k+1) obtained in step S<b>2104</b> with the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated.</p><p id="p-0308" num="0293">In step S<b>2518</b>, the deformation parameter adjustment unit <b>1453</b> determines whether or not d<sub>L</sub>(k+1)&#x3e;d(k+1), on the basis of the distance d<sub>L</sub>(k+1) between the projection vector V&#x2032; of the virtual camera <b>60</b> and the specific object in the horizontal direction, and the interpolation distance d(k+1) obtained in step S<b>2306</b>. Note that, in the case of the value of the deformation parameter A<b>1</b> in which the first value &#x3b2;&#x2032;(k+1) obtained in step S<b>2102</b> is associated with the specific position (refer to step S<b>2302</b>), the interpolation distance d(k+1) is 0. Similarly, in the case of the value of the deformation parameter A<b>1</b> in which the second value &#x3b2;&#x2033;(k+1) obtained in step S<b>2104</b> is associated with the specific object (refer to step S<b>2404</b>), the distance d<sub>L</sub>(k+1) in the horizontal direction is 0. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2508</b>, and in the other case, the process proceeds to step S<b>2514</b>.</p><p id="p-0309" num="0294">As described above, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>25</b></figref>, in a case where both of the first value &#x3b2;&#x2032;(k+1) obtained in step S<b>2102</b> and the second value &#x3b2;&#x2033;(k+1) obtained in step S<b>2104</b> are not the normal value &#x3b2;<sub>0 </sub>(for example, in a case where the specific position and the position of the specific object simultaneously belong to the region within the viewing angle), one of the first value &#x3b2;&#x2032;(k+1) and the second value &#x3b2;&#x2033;(k+1) is selected in a mode where the smaller of the distance d(k+1) and the distance d<sub>L</sub>(k+1) in the horizontal direction, described above, takes a priority. Accordingly, when the specific object is positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b>, the value of the deformation parameter A<b>1</b> associated with the specific object is preferentially used.</p><p id="p-0310" num="0295">Here, in a modification example, in a case where both of the first value &#x3b2;&#x2032;(k+1) obtained in step S<b>2102</b> and the second value &#x3b2;&#x2033;(k+1) obtained in step S<b>2104</b> are not the normal value &#x3b2;<sub>0 </sub>(for example, in a case where the specific position and the position of the specific object simultaneously belong to the region within the viewing angle), an average value or a weighted synthesis value may be used. In the case of weighting, for example, a synthesis value &#x3b2;<sub>com</sub>(k+1) after being updated may be calculated as described below, by using weighted coefficients w<b>1</b> and w<b>2</b>.</p><p id="p-0311" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;<sub>com</sub>(<i>k+</i>1)={<i>w</i>1&#xd7;&#x3b2;&#x2032;(<i>k+</i>1)+<i>w</i>2&#xd7;&#x3b2;&#x2033;(<i>k+</i>1)}/(<i>w</i>1+<i>w</i>2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0312" num="0296">In this case, for example, the weighted coefficients w<b>1</b> and w<b>2</b> may be as described below.</p><p id="p-0313" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>1=1/<i>d</i>(<i>k+</i>1), here, when <i>d</i>(<i>k+</i>1)=0, &#x3b2;<sub>com</sub>(<i>k+</i>1)=&#x3b2;&#x2032;(<i>k+</i>1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0314" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>2=1/<i>d</i><sub>L</sub>(<i>k+</i>1), here, when <i>d</i><sub>L</sub>(<i>k+</i>1)=0, &#x3b2;<sub>com</sub>(<i>k+</i>1)=&#x3b2;&#x2033;(<i>k+</i>1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0315" num="0297"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a schematic flowchart illustrating an example of the origin setting processing of the origin setting processing unit <b>1454</b> (step S<b>2108</b>). <figref idref="DRAWINGS">FIG. <b>26</b>A</figref> is an explanatory diagram of an internally dividing point Pi.</p><p id="p-0316" num="0298">In step S<b>2600</b>, the origin setting processing unit <b>1454</b> determines whether or not the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>. Note that, a determination method may be the same as that in step S<b>2400</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref> described above. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2602</b>, and in the other case, the process proceeds to step S<b>2610</b>. Note that, hereinafter, the specific object indicates a specific object that is determined to be positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b> in step S<b>2600</b>.</p><p id="p-0317" num="0299">In step S<b>2602</b>, the origin setting processing unit <b>1454</b> determines whether or not the specific object is positioned in the center portion of the region of the field object within the viewing angle of the virtual camera <b>60</b>. Note that, a determination method may be the same as that in step S<b>2402</b> of <figref idref="DRAWINGS">FIG. <b>24</b></figref> described above. In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2604</b>, and in the other case, the process proceeds to step S<b>2606</b>.</p><p id="p-0318" num="0300">In step S<b>2604</b>, the origin setting processing unit <b>1454</b> sets the position of the specific object to the origin position. That is, in the field surface <b>70</b> (the field surface <b>70</b> onto which the field image is projected), the origin setting processing unit <b>1454</b> associates the origin O of the local coordinate system with the position of the specific object. Accordingly, in this case, the origin O of the function F<b>1</b> that is used in the bending deformation is associated with the position of the specific object. Here, it is not necessary that the origin O of the function F<b>1</b> that is used in the bending deformation is exactly coincident with the position of the specific object, and the origin O may be in the vicinity of the position.</p><p id="p-0319" num="0301">In step S<b>2606</b>, the origin setting processing unit <b>1454</b> calculates each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated, and a position (X<sub>Pi</sub>(k+1), Y<sub>Pi</sub>(k+1)) of the internally dividing point Pi with respect to the position of the specific object, on the basis of the distance d<sub>L</sub>(k+1) between the projection vector V&#x2032; of the virtual camera <b>60</b> and the specific object in the horizontal direction. As illustrated in <figref idref="DRAWINGS">FIG. <b>26</b>A</figref>, the internally dividing point Pi is a position at the time of internally dividing a region between each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated and the position of the specific object by m:(1&#x2212;m). At this time, m is close to 0 as the distance d<sub>L</sub>(k+1) in the horizontal direction increases, and when the distance d<sub>L</sub>(k+1) in the horizontal direction is L<b>1</b>/2, m=0. Note that, when m=0, the internally dividing point Pi is coincident with each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated. L<b>1</b> is as described above, and is the maximum value of the distance d<sub>L</sub>(k+1) in the horizontal direction that can be taken when the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>. In addition, m is close to 1 as the distance d<sub>L</sub>(k+1) in the horizontal direction decreases, and when the distance d<sub>L</sub>(k+1) in the horizontal direction is 0, m=1. Note that, when m=1, the internally dividing point Pi is coincident with the position of the specific object.</p><p id="p-0320" num="0302">In step S<b>2608</b>, the origin setting processing unit <b>1454</b> sets the position of the internally dividing point Pi that is obtained in step S<b>2606</b> to the origin position. That is, in the field surface <b>70</b> (the field surface <b>70</b> onto which the field image is projected), the origin setting processing unit <b>1454</b> associates the origin O of the local coordinate system with the position (X<sub>Pi</sub>(k+1), Y<sub>Pi</sub>(k+1)) of the internally dividing point Pi that is obtained in step S<b>2606</b>. Accordingly, in this case, the origin O of the function F<b>1</b> that is used in the bending deformation is associated with the position (X<sub>Pi</sub>(k+1), Y<sub>Pi</sub>(k+1)) of the internally dividing point Pi. Here, it is not necessary that the origin O of the function F<b>1</b> that is used in the bending deformation is exactly coincident with the position (X<sub>Pi</sub>(k+1), Y<sub>Pi</sub>(k+1)) of the internally dividing point Pi, and the origin O may be in the vicinity of the position.</p><p id="p-0321" num="0303">In step S<b>2610</b>, the origin setting processing unit <b>1454</b> sets the position (u(k+1), v(k+1)) of the predetermined object after being moved to the origin position. That is, in the field surface <b>70</b> (the field surface <b>70</b> onto which the field image is projected), the origin setting processing unit <b>1454</b> associates the origin O of the local coordinate system with the position (u(k+1), v(k+1)) of the predetermined object after being moved. Accordingly, in this case, the origin O of the function F<b>1</b> that is used in the bending deformation is associated with the position (u(k+1), v(k+1)) of the predetermined object after being moved. Here, it is not necessary that the origin O of the function F<b>1</b> that is used in the bending deformation is exactly coincident with the position (u(k+1), v(k+1)) of the predetermined object after being moved, and the origin O may be in the vicinity of the position.</p><p id="p-0322" num="0304">As described above, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, in a case where it is determined that the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, the origin position is set on the basis of the position of the specific object. Accordingly, the entire specific object is likely to be visible, and the specific object can be effectively emphasized. For this reason, it is preferable that the specific object is an object that the user is allowed to watch.</p><p id="p-0323" num="0305">In addition, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, in a case where it is determined that the specific object is not positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, the origin position is set on the basis of the position (u(k+1), v(k+1)) of the predetermined object. Accordingly, the entire predetermined object is likely to be visible, and the predetermined object can be effectively emphasized. In addition, in a case where a state in which it is determined that the specific object is not positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b> is transitioned to a state in which it is determined that the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, the origin position is changed toward the position of the specific object from the position of the predetermined object, and thus, a change (an effect) in the appearance of the field image due to the change described above may occur. Accordingly, the specific object can be effectively emphasized.</p><p id="p-0324" num="0306">In addition, according to the processing illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref>, in a processing cycle where the state in which it is determined that the specific object is not positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b> is transitioned to the state in which it is determined that the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, the origin position is not rapidly changed to the position of the specific object from the position of the predetermined object. That is, in a case where it is determined that the specific object is positioned in the region of the field object within the viewing angle of the virtual camera <b>60</b>, after that, the origin position is gradually changed to the position of the specific object from the position of the predetermined object as the interpolation distance d(k+1) decreases (refer to step S<b>2608</b>). Accordingly, a sense of discomfort that can be felt by the user due to the change can be reduced, compared to a case where the change in the origin position as described above is attained by a comparatively small number of processing cycles (for example, one processing cycle).</p><p id="p-0325" num="0307"><figref idref="DRAWINGS">FIG. <b>27</b></figref> is a schematic flowchart illustrating an example of the second distance parameter calculation processing (step S<b>1623</b>).</p><p id="p-0326" num="0308">In step S<b>2700</b>, the second distance change unit <b>14212</b> determines whether or not the parameter state is the second state. As described above, the second state corresponds to a state in which the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated is the second value &#x3b2;&#x2033;(k+1). In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2702</b>, and in the other case, the process ends as it is. Note that, in a case where the process ends as it is, the value of the distance parameter A<b>2</b> is set by the value calculated by the first distance change unit <b>14211</b> described above (step S<b>1608</b>).</p><p id="p-0327" num="0309">In step S<b>2702</b>, the second distance change unit <b>14212</b> determines whether or not the second value &#x3b2;&#x2033;(k+1) is the interpolation value calculated by using the value of the deformation parameter A<b>1</b> associated with the specific object of the attentive target (the interpolation value associated in step S<b>2408</b>). In a case where a determination result is &#x201c;YES&#x201d;, the process proceeds to step S<b>2704</b>, and in the other case (that is, in a case where the second value &#x3b2;&#x2033;(k+1) is the value of the deformation parameter A<b>1</b> associated with the specific object of the attentive target), the process proceeds to step S<b>2703</b>.</p><p id="p-0328" num="0310">In step S<b>2703</b>, the second distance change unit <b>14212</b> associates a value (a regular value) &#x3b3;<sub>G</sub>* of the distance parameter A<b>2</b> associated with the specific object of the attentive target with the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated. In this case, the value (the regular value) &#x3b3;<sub>G</sub>* of the distance parameter A<b>2</b> associated with the specific object of the attentive target is associated with the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated, instead of the value calculated by the first distance change unit <b>14211</b> described above (step S<b>1608</b>). In this case, the update reflection unit <b>1424</b> positions the virtual camera <b>60</b> in the global coordinate system, on the basis of each of the values (X(k+1), Y(k+1), &#x3b3;(k+1), &#x3b8;(k+1), &#x3c8;(k+1)) of various parameters after being updated that are obtained in step S<b>1606</b>, step S<b>1610</b> to step S<b>1612</b>, and step S<b>1623</b> (refer to step S<b>1624</b>).</p><p id="p-0329" num="0311">In step S<b>2704</b>, the second distance change unit <b>14212</b> calculates a ratio between the second value &#x3b2;&#x2033;(k+1) and the value of the deformation parameter A<b>1</b> associated with the specific object of the attentive target. That is, in a case where the value of the deformation parameter A<b>1</b> associated with the specific object of the attentive target is set to &#x3b2;<sub>G</sub>*, the ratio=&#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>* is calculated. Note that, in the deformation parameter data <b>13</b>B of <figref idref="DRAWINGS">FIG. <b>13</b></figref>, in a case where the specific object of the attentive target is the specific object G<b>1</b>, the ratio=&#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G1 </sub>is calculated.</p><p id="p-0330" num="0312">Here, the ratio=&#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>* represents the closeness of the position of the specific object of the attentive target with respect to the center portion of the region within the viewing angle. When &#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>*=1, the specific object of the attentive target is close to the end portion of the region within the viewing angle (the end portion in the horizontal direction) as &#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>* decreases corresponding to a state in which the specific object of the attentive target is positioned in the center portion of the region within the viewing angle.</p><p id="p-0331" num="0313">In step S<b>2706</b>, the second distance change unit <b>14212</b> calculates an interpolation value &#x3b3;&#x2032;<sub>G</sub>*, on the basis of the ratio=&#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>* obtained in step S<b>2704</b>. Specifically, in a case where the value (the regular value) of the distance parameter A<b>2</b> associated with the specific object of the attentive target is set to &#x3b3;<sub>G</sub>*, the interpolation value &#x3b3;&#x2032;<sub>G</sub>* may be as described below.</p><p id="p-0332" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;&#x2032;<sub>G</sub>*=&#x3b2;&#x2033;(<i>k+</i>1)/&#x3b2;<sub>G</sub>*&#xd7;&#x3b3;<sub>G</sub>*<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0333" num="0314">In this case, the interpolation value &#x3b3;&#x2032;<sub>G</sub>* is close to the value &#x3b3;<sub>G</sub>* associated with the specific object of the attentive target as &#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>* is close to 1 (that is, as the specific object of the attentive target is close to the center portion of the region within the viewing angle). Note that, in the distance parameter data <b>14</b>B of <figref idref="DRAWINGS">FIG. <b>14</b></figref>, in a case where the specific object of the attentive target is the specific object G<b>1</b>, the value &#x3b3;<sub>G</sub>*=&#x3b3;<sub>G1</sub>.</p><p id="p-0334" num="0315">In step S<b>2708</b>, the second distance change unit <b>14212</b> associates the interpolation value &#x3b3;&#x2032;<sub>G</sub>* obtained in step S<b>2706</b> with the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated. In this case, the value calculated by the second distance change unit <b>14212</b> is associated with the value of the distance parameter A<b>2</b>, instead of the value calculated by the first distance change unit <b>14211</b> described above (step S<b>1608</b>). In this case, the update reflection unit <b>1424</b> positions the virtual camera <b>60</b> in the global coordinate system, on the basis of each of the values (X(k+1), Y(k+1), &#x3b3;(k+1), &#x3b8;(k+1), &#x3c8;(k+1)) of various parameters after being updated that are obtained in step S<b>1606</b>, step S<b>1610</b> to step S<b>1612</b>, and step S<b>1623</b> (refer to step S<b>1624</b>).</p><p id="p-0335" num="0316">Note that, in the processing illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>, the regular value &#x3b3;<sub>Gr </sub>to the interpolation value &#x3b3;&#x2032;<sub>G</sub>* are calculated in accordance with the ratio=&#x3b2;&#x2033;(k+1)/&#x3b2;<sub>G</sub>*, but the invention is not limited thereto. For example, the regular value &#x3b3;<sub>G</sub>* to the interpolation value &#x3b3;&#x2032;<sub>G</sub>* may be calculated in accordance with a ratio between the distance d<sub>L</sub>(k+1) and L<b>1</b>/2 in the horizontal direction described above. The details are as described below.</p><p id="p-0336" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;&#x2032;<sub>G</sub><i>*=d</i><sub>L</sub>(<i>k+</i>1)/<i>L</i>1/2&#xd7;&#x3b3;<sub>G</sub>*<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0337" num="0317">In addition, in the processing illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>, in a case where it is determined that the parameter state is the second state in step S<b>2700</b>, the process proceeds to step S<b>2702</b>, but the invention is not limited thereto. For example, in step S<b>2700</b>, it is determined whether or not the specific object of the attentive target is positioned in the region within the viewing angle, and in a case where it is determined that the specific object of the attentive target is positioned in the region within the viewing angle, the process may proceed to step S<b>2702</b>.</p><p id="p-0338" num="0318">In addition, in the processing illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>, for example, in a case where the specific position and the position of the specific object simultaneously belong to the region within the viewing angle, the value of the distance parameter A<b>2</b> can be comparatively greatly changed when the parameter state is transitioned to the second state from the first state. Such a comparatively great change may be suitably corrected (may be filtered such that the change decreases). Accordingly, a sense of discomfort that can be felt by the user due to such a comparatively great change (and a steep change in the field image associated therewith), and the like can be reduced. For example, in a case where the parameter state is transitioned to the second state from the first state, the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated may be further corrected in a direction close to the value &#x3b3;(k) of the distance parameter A<b>2</b> before being updated when a difference between the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated and the value &#x3b3;(k) of the distance parameter A<b>2</b> before being updated is greater than a predetermined threshold value. In addition, in a case where the value &#x3b3;(k) of the distance parameter A<b>2</b> before being updated is greater than the value &#x3b3;(k) of the distance parameter A<b>2</b> before being updated, the value &#x3b3;(k) of the distance parameter A<b>2</b> before being updated may be maintained such that the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated is not greater than the value &#x3b3;(k) of the distance parameter A<b>2</b> before being updated. In this case, a sense of discomfort that can be felt by the user due to an unstable mode in which the value of the distance parameter decreases, increases, and decreases again (and an unstable change in the field image associated therewith), and the like can be reduced.</p><p id="p-0339" num="0319">Next, an example of an applied scene of the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>27</b></figref> will be described with reference to <figref idref="DRAWINGS">FIG. <b>28</b></figref> to <figref idref="DRAWINGS">FIG. <b>29</b>C</figref>.</p><p id="p-0340" num="0320"><figref idref="DRAWINGS">FIG. <b>28</b></figref> to <figref idref="DRAWINGS">FIG. <b>29</b>C</figref> are explanatory diagrams of the applied scene of the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, <figref idref="DRAWINGS">FIG. <b>28</b></figref> is a plan view of the field object <b>77</b>, <figref idref="DRAWINGS">FIG. <b>29</b>A</figref> illustrates an example of a field image G<b>24</b>A according to a position E<b>1</b>, and <figref idref="DRAWINGS">FIG. <b>29</b>B</figref> illustrates an example of a field image G<b>24</b>B according to a position E<b>2</b>. <figref idref="DRAWINGS">FIG. <b>29</b>C</figref> illustrates an example of a field image G<b>24</b>C according to a position E<b>3</b>. In <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the field object <b>77</b> is illustrated in an expression in which the field image is projected onto the field surface <b>70</b> in the regular state. In <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the position E<b>1</b> to the position E<b>3</b> are exemplified, the position E<b>3</b> corresponds to a position in which the horizontal passage <b>14</b> intersects with the vertical passage <b>15</b>, and a plurality of street plant objects <b>16</b> that are the second object are arranged on both sides of the vertical passage <b>15</b>. The plurality of street plant objects <b>16</b> are erected on the field object <b>77</b>, and extend in the z direction. In addition, in the vertical passage <b>15</b>, a floating object <b>19</b> such as a balloon is arranged. Here, the floating object <b>19</b> is set to the specific object G<b>2</b>. In <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the floating object <b>19</b> is arranged on a side farther away from the position E<b>3</b> than the street plant object <b>16</b> in the v direction. The position E<b>2</b> corresponds to a position in front of the position E<b>3</b> from the position E<b>1</b>, and is a position in which the distance &#x26; between the projection vector V&#x2032; of the virtual camera <b>60</b> and the specific object in the horizontal direction is L<b>1</b>/2 or less, as described above, but is significantly greater than 0. Note that, as described above, L<b>1</b> is the distance of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> in the horizontal direction. Note that, in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, as an example, the plurality of street plant objects <b>16</b> are linearly arranged along the v direction, but may be arranged in zigzags in a mode of being slightly offset in a u direction, may be arranged only one side of the vertical passage <b>15</b>, or may be arranged in two or more rows.</p><p id="p-0341" num="0321">Here, a drawing function when the first object <b>3</b> is moved to the position E<b>3</b> from the position E<b>1</b> on the field object <b>77</b> the will be described. Note that, such a movement may be attained by the manipulation of the user, or may be attained as the output of a demonstration image. Note that, the positions E<b>1</b> and E<b>2</b> are not the specific position, there is no specific position between the position E<b>1</b> and the position E<b>3</b>, and the positions E<b>1</b> and E<b>2</b> are not positioned within an interpolation processing range according to another specific position or another specific object. In addition, as illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> and <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the value &#x3b2;<sub>G2 </sub>of the deformation parameter A<b>1</b> and the value &#x3b3;<sub>G2 </sub>of the distance parameter A<b>2</b> are respectively associated with the floating object <b>19</b> (the specific object G<b>2</b>).</p><p id="p-0342" num="0322">Each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> in a state where the first object <b>3</b> is positioned in the position E<b>1</b> corresponds to the position of the first object <b>3</b>, the value of the direction parameter &#x3b8; of the virtual camera <b>60</b> is set such that the normal value &#x3b8;<sub>0</sub>, that is, the projection vector V&#x2032; (refer to <figref idref="DRAWINGS">FIG. <b>11</b></figref>) is perpendicular to the movement direction of the first object <b>3</b> (in this case, the u direction). At this time, the first object <b>3</b> may be positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, and the field image G<b>24</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>A</figref> may be drawn. Note that, in this case, the horizon line HL has a height according to the normal value &#x3b2;<sub>0 </sub>of the deformation parameter A<b>1</b>, and the first object <b>3</b> and the like have a display size according to the normal value &#x3b2;<sub>0 </sub>of the distance parameter A<b>2</b>.</p><p id="p-0343" num="0323">In a case where the first object <b>3</b> is moved toward the position E<b>3</b> from the position E<b>1</b> along the horizontal passage <b>14</b> (the u direction) by a movement amount Au, for each processing cycle, each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is moved along the u direction by the movement amount &#x394;u, for each of the processing cycles. Note that, during this time, the value of the direction parameter &#x3b8; of the virtual camera <b>60</b> is fixed.</p><p id="p-0344" num="0324">Each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> in a state where the first object <b>3</b> reaches the position E<b>2</b> corresponds to a position of the first object <b>3</b> that reaches the position E<b>2</b>, and the value of the direction parameter &#x3b8; is the same as that in the state where the first object <b>3</b> is positioned in the position E<b>1</b>. At this time, the floating object <b>19</b> may be positioned in the end portion (the end portion on the right side) of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, and the field image G<b>24</b>B illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>B</figref> may be drawn. Note that, in this case, the horizon line HL has a height according to the second value &#x3b2;&#x2033; of the deformation parameter A<b>1</b> (the value &#x3b2; of the deformation parameter A<b>1</b> after being updated that is obtained in the second deformation parameter calculation processing (step S<b>2104</b>)), and the first object <b>3</b> and the like have a display size according to the value &#x3b3;&#x2032;<sub>G</sub>* (the interpolation value &#x3b3;&#x2032;<sub>G</sub>*) of the distance parameter A<b>2</b> that is calculated by the second distance change unit <b>14212</b>. Note that, the interpolation value &#x3b3;&#x2032;<sub>G</sub>* is less than the normal value &#x3b3;<sub>0</sub>, and thus, as illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>B</figref>, the display size of the first object <b>3</b> is greater than that of the field image G<b>24</b>A illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>A</figref>.</p><p id="p-0345" num="0325">Each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> in a state where the first object <b>3</b> reaches the position E<b>3</b> corresponds to the position of the first object <b>3</b> that reaches the position E<b>3</b>, and the value of the direction parameter &#x3b8; is the same as that in the state where the first object <b>3</b> is positioned in the position E<b>1</b>. At this time, the floating object <b>19</b> may be positioned in the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, and the field image G<b>24</b>C illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>C</figref> may be drawn. Note that, in this case, the horizon line HL has a height according to the value &#x3b2;<sub>G2 </sub>of the deformation parameter A<b>1</b> (the value of the deformation parameter A<b>1</b> associated with the floating object <b>19</b>), and has a display size according to the value &#x3b3;<sub>G2 </sub>of the distance parameter A<b>2</b> (the value of the distance parameter A<b>2</b> associated with the floating object <b>19</b>).</p><p id="p-0346" num="0326">Here, as described above, the value &#x3b2;<sub>G2 </sub>of the deformation parameter A<b>1</b> is greater than the normal value &#x3b2;<sub>0</sub>. Accordingly, in the field image G<b>24</b>C, the deformation degree of the bending deformation of the field object <b>77</b> is greater than that in the field image G<b>24</b>A. For this reason, as schematically illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>29</b>C</figref>, the position of the horizon line HL in the image is significantly changed. In addition, the value &#x3b3;<sub>G2 </sub>of the distance parameter A<b>2</b> is greater than the normal value &#x3b3;<sub>0</sub>. Accordingly, in the field image G<b>24</b>C, the display size of the floating object <b>19</b> increases.</p><p id="p-0347" num="0327">Here, <figref idref="DRAWINGS">FIG. <b>29</b>D</figref> illustrates an example of a field image G<b>24</b>D according to the position E<b>3</b> according to another operation example. In another operation example illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>D</figref>, unlike the operation example described above, the value of the distance parameter A<b>2</b> associated with the position E<b>3</b> is the same normal value &#x3b3;<sub>0 </sub>as the value of the distance parameter A<b>2</b> associated with the position E<b>1</b>. In this case, the height H<b>1</b> of the horizon line HL is changed between the field image G<b>24</b>A and the field image G<b>24</b>D, but the display size of the first object <b>3</b> and the like is the same. Accordingly, the display size of the floating object <b>19</b> positioned on the deeper side from the street plant object <b>16</b> is comparatively small, and the floating object <b>19</b> is less likely to be emphasized.</p><p id="p-0348" num="0328">In contrast, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, as described above, in the field image G<b>24</b>C, the display size of the floating object <b>19</b> comparatively increases, and thus, the floating object <b>19</b> can be effectively emphasized. Accordingly, the user can be attracted by the floating object <b>19</b>.</p><p id="p-0349" num="0329">In addition, in another operation example of drawing the field image G<b>24</b>D as illustrated in <figref idref="DRAWINGS">FIG. <b>29</b>D</figref>, various expressions within the virtual space viewed from the virtual camera <b>60</b> can be attained, but in a case where a change amount of the deformation degree (a change amount of the height H<b>1</b> of the horizon line HL) comparatively increases, the user may feel a sense of discomfort, compared to an operation example of a comparative example (not illustrated) in which the value of the deformation parameter A<b>1</b> is set to be perennially constant.</p><p id="p-0350" num="0330">In contrast, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the display size of the first object <b>3</b> and the like is also changed while the height H<b>1</b> of the horizon line HL is changed between the field image G<b>24</b>A and the field image G<b>24</b>B. Accordingly, a sense of discomfort that can be felt by the user due to a change in the deformation degree (a change in the height H<b>1</b> of the horizon line HL) can be reduced. That is, a change in the display size of the first object <b>3</b> and the like is likely to be noticeable for the user, and is impressive, and thus, a sense of discomfort that can be caused by the change in the height H<b>1</b> of the horizon line HL disappears. As described above, a sense of discomfort that can be caused by the change in the deformation degree (the change in the height H<b>1</b> of the horizon line HL) can be reduced while attaining various expressions within the virtual space viewed from the virtual camera <b>60</b>, by making the associated value of the deformation parameter A<b>1</b> different, and by making the associated value of the distance parameter A<b>2</b> different, between the position E<b>1</b> and the position E<b>3</b>. Regarding such an effect, a sense of discomfort can be further reduced by having a predetermined relationship between a change in the value of the distance parameter A<b>2</b> and a change in the value of the deformation parameter A<b>1</b>. For example, in a case where the change in the value of the distance parameter A<b>2</b> is steep, a condition is obtained in which a sense of discomfort can be reduced even in a case where the change in the value of the deformation parameter A<b>1</b> is steep.</p><p id="p-0351" num="0331">In addition, a transition mode of the field image can be novel, and the attractiveness of the game can also be increased, by making the associated value of the deformation parameter A<b>1</b> different, and by making the associated value of the distance parameter A<b>2</b> different, between the position E<b>1</b> and the position E<b>3</b>. In addition, the presence of the vertical passage <b>15</b> can be emphasized, and the vertical passage <b>15</b> or the second object (for example, the street plant object <b>16</b>) that is arranged around the vertical passage <b>15</b> can be emphasized, along with the floating object <b>19</b> that is the specific object. Such an effect is particularly remarkable in a case where the field image is output to a comparatively small screen, such as the screen of a smart phone that is the terminal device <b>20</b>. In addition, a restriction such as not arranging another second object in order for the specific object to be emphasized is relaxed, and thus, a freedom degree of arranging the second object on the field object increases. In addition, from the same reason, a freedom degree of a region on the field object in which the first object is movable can also be increased.</p><p id="p-0352" num="0332">In addition, for example, as illustrated in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, in a case where the second objects such as the street plant objects <b>16</b> overlapping with each other along the visual line direction V are arranged, a sense of depth can be increased, and an impressive expression can be attained, by increasing the bending degree of the bending deformation of the field object <b>77</b>. Accordingly, the user, for example, can also be motivated to move the first object <b>3</b> along the vertical passage <b>15</b>.</p><p id="p-0353" num="0333">Here, <figref idref="DRAWINGS">FIG. <b>29</b>E</figref> illustrates an example of a field image G<b>24</b>E according to the position E<b>3</b> according to still another operation example. In still another operation example, unlike the operation example described above (the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>), the origin position (the position of the field object <b>77</b> in which the origin O of the local coordinate system is associated) is set to the position of the first object <b>3</b>. In this case, the first object <b>3</b> is positioned in the highest position of the field object <b>77</b>, and thus, the entire first object <b>3</b> is visible, but a part (the lower portion) of the floating object <b>19</b> is hidden by the deeper side from the horizon line HL. That is, the entire floating object <b>19</b> is not visible, and is comparatively less likely to be emphasized.</p><p id="p-0354" num="0334">In contrast, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the origin position is set to the position of the floating object <b>19</b> that is the specific object, instead of the position of the first object <b>3</b>. In this case, the first object <b>3</b> is positioned in the highest position of the field object <b>77</b>, and thus, the entire first object <b>3</b> is visible. Accordingly, the floating object <b>19</b> that is the specific object can be effectively emphasized, and the entire floating object <b>19</b> (for example, in a floating state) can be visible. In addition, in a case where the range of the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> is comparatively widened, a state in which the origin position is set to the position of the floating object <b>19</b> is likely to be continued for a comparatively long period of time, and thus, the floating object <b>19</b> to be notable can be effectively emphasized.</p><p id="p-0355" num="0335">In addition, for example, even in a case where an overlap (overlapping) occurs between the specific object and the first object and/or the second object in the visual line direction V of the virtual camera <b>60</b> when the specific object is positioned in the center portion of the region within the viewing angle, as described above, the origin position is set to the position of the floating object <b>19</b>, and thus, objects that are capable of overlapping with each other are easily separated from each other up and down. That is, the overlapping first object and/or second object relatively slide in a lower direction within the viewing angle with respect to the specific object, and thus, the number of objects overlapping with the specific object can be reduced. Accordingly, the floating object <b>19</b> to be notable can be effectively emphasized.</p><p id="p-0356" num="0336">Then, in a case where the specific object is an object relevant to the manipulation of the user (for example, a movement target), the input with respect to the specific object is easily performed by emphasizing the specific object, and thus, manipulation properties are improved. In addition, the influence of the presence of the second object in front and back of the specific object can be reduced to some extent, and thus, a freedom degree of the arrangement of the second object can be increased, and various field images can be attained.</p><p id="p-0357" num="0337">As described above, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, a deformation mode of the field object <b>77</b> can be different by changing the value of the deformation parameter A<b>1</b> or the setting mode of the origin position in a case where it is determined that the specific object is not positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> and in a case where it is determined that the specific object is positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>. Accordingly, an expression can be attained in which the specific object can be effectively emphasized, such as emphasizing the specific object while diversifying the field image.</p><p id="p-0358" num="0338">In addition, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the value of the deformation parameter A<b>1</b> is changed in a case where the floating object <b>19</b> is positioned in the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of a second range) and in a case where the floating object <b>19</b> is positioned in the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of a first range). Accordingly, the deformation degree of the field object <b>77</b> is changed even while the floating object <b>19</b> is positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, and thus, a display effect such as changing the appearance of the floating object <b>19</b> can be obtained.</p><p id="p-0359" num="0339">In addition, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, in a case where the floating object <b>19</b> is positioned in the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of the first range) (refer to the range <b>771</b> in <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, for the center portion), the value of the deformation parameter A<b>1</b> increases, compared to a case where the floating object <b>19</b> is positioned in the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of the second range) (refer to ranges <b>772</b> and <b>773</b> in FIG. <b>11</b>A, for the end portion). Accordingly, when the floating object <b>19</b> reaches a position on the field image that is comparatively easily viewed (the center portion), the floating object <b>19</b> can be more effectively emphasized by increasing a bending deformation degree of the field object <b>77</b>.</p><p id="p-0360" num="0340">In addition, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, as the position of the floating object <b>19</b> is changed to the center portion from the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, the value of the deformation parameter A<b>1</b> is changed (increases) to the value &#x3b2;<sub>G2 </sub>(an example of a first setting value) from the interpolation value (an example of a second setting value) that is greater than the normal value &#x3b2;<sub>0 </sub>(an example of a predetermined setting value), through one or more interpolation values (an example of the second setting value). Accordingly, in a processing cycle in which the floating object <b>19</b> reaches the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, a gentle change is attained, compared to a case where the normal value &#x3b2;<sub>0 </sub>of the deformation parameter A<b>1</b> is changed to the value &#x3b2;<sub>G2</sub>. Accordingly, a sense of discomfort that can be felt by the user due to such a change in the deformation degree can be effectively reduced. The same applies to a change to the value &#x3b3;<sub>G2 </sub>of the distance parameter A<b>2</b> from the normal value &#x3b3;<sub>0 </sub>of the distance parameter A<b>2</b>, or a change in the origin position. In addition, in the case of not using the interpolation described above, a restriction such as not allowing the specific positions or the specific objects to be close to each other easily occurs in order to avoid a steep change. At this time, a freedom degree of the arrangement of the specific position or the specific object can be increased by using the interpolation described above. As a result thereof, the field image can be further diversified.</p><p id="p-0361" num="0341">In addition, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, for example, the value of the deformation parameter A<b>1</b> that is more suitable for the interpolation can be associated with each position between the position E<b>2</b> and the position E<b>3</b>, and thus, storage capacity for deformation parameter data can be efficiently used, compared to a case where each value of the deformation parameter A<b>1</b> is associated with each position on the deformation parameter data. The same applies to other parameters such as the distance parameter.</p><p id="p-0362" num="0342">Note that, in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the movement of the first object <b>3</b> to the position E<b>3</b> from the position E<b>1</b> has been described, but the reverse may be attained in the case of the movement of the first object <b>3</b> to the position E<b>1</b> from the position E<b>3</b>. In addition, in <figref idref="DRAWINGS">FIG. <b>28</b></figref>, the movement of the first object <b>3</b> to the position E<b>3</b> from the position E<b>1</b> has been described, but the same may be attained in the case of the movement of the first object <b>3</b> to the position E<b>3</b> from the specific position.</p><p id="p-0363" num="0343">Note that, in a case where the specific position and the position E<b>3</b> (the position of the virtual camera <b>60</b> when the floating object <b>19</b> is positioned in the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>) are comparatively close to each other, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, as described above, the interpolation value of the deformation parameter A<b>1</b> or the distance parameter A<b>2</b> that is calculated on the basis of the value of the deformation parameter A<b>1</b> or the value of the distance parameter A<b>2</b> associated with the specific position, and the interpolation value of the deformation parameter A<b>1</b> or the distance parameter A<b>2</b> that is calculated on the basis of the value of the deformation parameter A<b>1</b> or the value of the distance parameter A<b>2</b> associated with the specific object according to the position E<b>3</b> are adjusted in accordance with whether or not d<sub>L</sub>(k+1)&#x3e;d(k+1) (refer to step S<b>2518</b>).</p><p id="p-0364" num="0344">In addition, in this embodiment, even in a case where two or more specific objects are positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, processing may be performed in the same concept as that in a case where the specific position and the specific object are comparatively close to each other. For example, here, a case is assumed in which the specific object includes a first specific object and a second specific object, and the specific object region described above includes an object region in which both of the first specific object and the second specific object are positioned. At this time, the object region includes a first object region in which the first specific object is positioned in the center portion, a second object region in which both of the first specific object and the second specific object are not positioned in the center portion, and a third object region in which the second specific object is positioned in the center portion.</p><p id="p-0365" num="0345">Here, the first specific object region to the third specific object region will be described with reference to <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>. In <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>, regions R<b>4</b>, R<b>5</b>, and R<b>6</b> of the field object <b>77</b> falling within the viewing angle <b>62</b> of the virtual camera <b>60</b>, respectively, at the time of three types of camera parameters different from each other (here, camera parameters <b>4</b>, <b>5</b>, and <b>6</b>) are illustrated. In the camera parameters <b>4</b>, <b>5</b>, and <b>6</b>, at least one value of each element (X, Y, A<b>2</b>, &#x3b8;, and &#x3c8;) of the camera parameter described above is different. Accordingly, three regions illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>B</figref> are regions different from each other. Then, in three regions R<b>4</b>, R<b>5</b>, and R<b>6</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>, in the region R<b>4</b>, a first specific object G<b>11</b> is arranged in the center portion (refer to the range <b>771</b>). Accordingly, in this case, the region R<b>4</b> is an example of the first object region. On the other hand, in the region R<b>5</b>, both of the first specific object and the second object are not arranged in the center portion (refer to range <b>771</b>). That is, the first specific object and the second object are arranged in the end portions on both sides of the center portion in the region R<b>5</b> (refer to the ranges <b>772</b> and <b>773</b>), respectively. Accordingly, in this case, the region R<b>5</b> is an example of the second object region. In the region R<b>6</b>, a second specific object G<b>12</b> is arranged in the center portion (refer to the range <b>771</b>). Accordingly, in this case, the region R<b>6</b> is an example of the third object region.</p><p id="p-0366" num="0346">In this case, the interpolation value of the deformation parameter A<b>1</b> or the distance parameter A<b>2</b> (an example of the first setting value) that is calculated on the basis of the value of the deformation parameter A<b>1</b> associated with the first specific object (and the first object region associated therewith) or the value of the distance parameter A<b>2</b>, and the interpolation value of the deformation parameter A<b>1</b> or the distance parameter A<b>2</b> (an example of the first setting value) that is calculated on the basis of the value of the deformation parameter A<b>1</b> associated with the second specific object (and the third object region associated therewith) of the value of the distance parameter A<b>2</b> are adjusted in accordance with whether or not d<sub>L1</sub>(k+1)&#x3e;d<sub>L2</sub>(k+1). In this case, d<sub>L1</sub>(k+1) is the distance of the first specific object in the horizontal direction, and d<sub>L2</sub>(k+1) is the distance of the second specific object in the horizontal direction. Specifically, in a case where d<sub>L1</sub>(k+1)&#x3e;d<sub>L2</sub>(k+1), the interpolation value of the deformation parameter A<b>1</b> or the distance parameter A<b>2</b> (an example of the second setting value) that is calculated on the basis of the value of the deformation parameter A<b>1</b> associated with the second specific object or the value of the distance parameter A<b>2</b> may be adopted, and in a case where d<sub>L1</sub>(k+1)&#x2264;d<sub>L2</sub>(k+1), the interpolation value of the deformation parameter A<b>1</b> or the distance parameter A<b>2</b> (an example of the second setting value) that is calculated on the basis of the value of the deformation parameter A<b>1</b> associated with the first specific object or the value of the distance parameter A<b>2</b> may be adopted. The same applies to the origin setting processing illustrated in <figref idref="DRAWINGS">FIG. <b>26</b></figref> or the second distance parameter calculation processing illustrated in <figref idref="DRAWINGS">FIG. <b>27</b></figref>. Specifically, in a case where d<sub>L1</sub>(k+1)&#x3e;d<sub>L2</sub>(k+1), the second specific object functions as the &#x201c;specific object&#x201d; described in <figref idref="DRAWINGS">FIG. <b>26</b></figref> and <figref idref="DRAWINGS">FIG. <b>27</b></figref>, and in a case where d<sub>L1</sub>(k+1)&#x2264;d<sub>L2</sub>(k+1), the first specific object functions as the &#x201c;specific object&#x201d; described in <figref idref="DRAWINGS">FIG. <b>26</b></figref> and <figref idref="DRAWINGS">FIG. <b>27</b></figref>, and thus, the same effect can be attained while performing adjustment between two specific objects. Note that, such a modification example is preferable even in a case where the specific object is moved. For example, this is because in a case where the first specific object and/or the second specific object are an object that is moved, a distance between the first specific object and the second specific object is changed, and thus, the second object region described above may be generated.</p><p id="p-0367" num="0347">In addition, even in such a case, an average value or a weighted synthesis value may be used in a case where the position of the first specific object and the position of the second specific object simultaneously belong to the region within the viewing angle. In the case of weighting, for example, when the value &#x3b2; of the deformation parameter A<b>1</b> that is calculated on the basis of the first specific object is set to &#x3b2;<sub>A1</sub>&#x2032;(k+1), and the value &#x3b2; of the deformation parameter A<b>1</b> that is calculated on the basis of the second specific object is set to &#x3b2;<sub>A2</sub>&#x2033;(k+1), a synthesis value &#x3b2;<sub>com</sub>&#x2032;(k+1) after being updated may be calculated as described below, by using weighted coefficients w<b>3</b> and w<b>4</b>.</p><p id="p-0368" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b2;<sub>com</sub>&#x2032;(<i>k+</i>1)={<i>w</i>3&#xd7;&#x3b2;<sub>A1</sub>&#x2032;(<i>k+</i>1)+<i>w</i>4&#xd7;&#x3b2;<sub>A2</sub>&#x2033;(<i>k+</i>1)}/(<i>w</i>3+<i>w</i>4)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0369" num="0348">In this case, for example, the weighted coefficients w<b>3</b> and w<b>4</b> may be as described below.</p><p id="p-0370" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>3=1/<i>d</i><sub>L1</sub>(<i>k+</i>1), here, when <i>d</i><sub>L1</sub>(<i>k+</i>1)=0, &#x3b2;<sub>com</sub>&#x2032;(<i>k+</i>1)=&#x3b2;<sub>A1</sub>&#x2032;(<i>k+</i>1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0371" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>w</i>4=1/<i>d</i><sub>L2</sub>(<i>k+</i>1), here, when <i>d</i><sub>L2</sub>(<i>k+</i>1)=0, &#x3b2;<sub>com</sub>&#x2032;(<i>k+</i>1)=&#x3b2;<sub>A2</sub>&#x2033;(<i>k+</i>1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0372" num="0349">In addition, according to the operation example described with reference to <figref idref="DRAWINGS">FIG. <b>16</b></figref> to <figref idref="DRAWINGS">FIG. <b>28</b></figref>, even in a case where it is determined that the specific object is not positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, the value of the deformation parameter A<b>1</b> and/or the value of the distance parameter A<b>2</b> are changed on the basis of a relationship between the specific position and each of the values of the position parameters (X, Y) of the virtual camera <b>60</b>. That is, the value of the deformation parameter A<b>1</b> and/or the value of the distance parameter A<b>2</b> are different in a case where it is determined that the specific object is in the position of the virtual camera <b>60</b> (an example of a first position) in which each of the values of the position parameters (X, Y) corresponds to the specific position (for example, &#x201c;YES&#x201d; in step S<b>1700</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> and/or &#x201c;YES&#x201d; in step S<b>2300</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>) and in a case where it is determined that the specific object is in the position of the virtual camera <b>60</b> (an example of a second position) in which each of the values of the position parameters (X, Y) does not correspond to the specific position (for example, &#x201c;NO&#x201d; in step S<b>1700</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref> and/or &#x201c;NO&#x201d; in step S<b>2300</b> of <figref idref="DRAWINGS">FIG. <b>23</b></figref>). Accordingly, even in a case where the specific object is not positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, the field image can be further diversified. Note that, in a case where it is determined that the specific object is in the position of the virtual camera <b>60</b> (an example of the first position) in which each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated corresponds to the specific position (for example, &#x201c;YES&#x201d; in step S<b>1700</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>), the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated is set to a value associated with the specific position (step S<b>1702</b>), whereas in a case where it is determined that the specific object is in the position of the virtual camera <b>60</b> (an example of the second position) in which each of the values (X(k+1), Y(k+1)) of the position parameters (X, Y) after being updated does not correspond to the specific position (for example, &#x201c;NO&#x201d; in the step S<b>1700</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>), the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated is set to the normal value &#x3b3;<sub>0 </sub>or the interpolation value (step S<b>1712</b> and step S<b>1710</b>). Accordingly, equivalently, in the case of a specific position in which both of the value of the deformation parameter A<b>1</b> and the value of the distance parameter A<b>2</b> are associated, such as the specific positions A and B illustrated in <figref idref="DRAWINGS">FIG. <b>13</b></figref> and <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated may be set on the basis of the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated. For example, in a case where the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated is a value corresponding to a comparatively long distance (an example of a first distance), the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated may be set such that the deformation degree decreases, compared to a case where the value &#x3b3;(k+1) is a value corresponding to a comparatively short distance (an example of a second distance). In addition, as another embodiment, the value of the deformation parameter A<b>1</b> may be linked in accordance with the value of the distance parameter A<b>2</b>. For example, in a relationship of the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated=the normal value &#x3b2;<sub>0</sub>&#xd7;the normal value &#x3b3;<sub>0</sub>/the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated, the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated according to the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated may be calculated. In this case, when the value &#x3b3;(k+1) of the distance parameter A<b>2</b> after being updated is the value corresponding to the comparatively long distance (an example of the first distance), the value &#x3b2;(k+1) of the deformation parameter A<b>1</b> after being updated is linked such that the deformation degree decreases, compared to a case where the value &#x3b3;(k+1) is the value corresponding to the comparatively short distance (an example of the second distance). Accordingly, a storage region can be saved, compared to a case where each value of the deformation parameter A<b>1</b> and each value of the distance parameter A<b>2</b> are individually stored. In addition, the value of the deformation parameter A<b>1</b> can be similarly changed by changing the value of the distance parameter A<b>2</b> between the normal value &#x3b3;<sub>0 </sub>and the value of the distance parameter A<b>2</b> defined by the distance parameter data <b>14</b>A (for example, the value &#x3b3;<sub>1 </sub>and the like) through the interpolation value, as described above, and thus, a sense of discomfort due to a steep change in the values of both of the parameters can be simultaneously reduced.</p><p id="p-0373" num="0350">Next, a modification example of the embodiment described above will be described with reference to <figref idref="DRAWINGS">FIG. <b>30</b></figref>.</p><p id="p-0374" num="0351"><figref idref="DRAWINGS">FIG. <b>30</b></figref> is an example of a functional block diagram relevant to a drawing function of a server device <b>10</b>A according to a modification example. The server device <b>10</b>A according to the modification example is different from the server device <b>10</b> according to the embodiment described above in that the drawing processing unit <b>140</b> is replaced with a drawing processing unit <b>140</b>A.</p><p id="p-0375" num="0352">The drawing processing unit <b>140</b>A according to this modification example is different from the drawing processing unit <b>140</b> according to the embodiment described above in that the distance change unit <b>1421</b> is replaced with a zoom amount change unit <b>1421</b>A (an example of a parameter value change unit).</p><p id="p-0376" num="0353">The zoom amount change unit <b>1421</b>A changes the value of the optical parameter relevant to a zoom amount of the virtual camera <b>60</b>, such as a focal distance or a viewing angle. The zoom amount change unit <b>1421</b>A may change the value of the optical parameter such that the same effect as that of the distance change unit <b>1421</b> is attained. For example, the effect obtained by reducing the value of the distance parameter A<b>2</b> with the distance change unit <b>1421</b> may be attained by changing the value of the optical parameter with the zoom amount change unit <b>1421</b>A such that the zoom amount increases. Similarly, the effect obtained by increasing the value of the distance parameter A<b>2</b> with the distance change unit <b>1421</b> may be attained by changing the value of the optical parameter with the zoom amount change unit <b>1421</b>A such that the zoom amount decreases. As described above, the function of the distance change unit <b>1421</b> can be attained by the zoom amount change unit <b>1421</b>A.</p><p id="p-0377" num="0354">Note that, in another modification example, the distance change unit <b>1421</b> and the zoom amount change unit <b>1421</b>A may simultaneously function. In addition, only one of the first distance change unit <b>14211</b> and the second distance change unit <b>14212</b> of the distance change unit <b>1421</b> according to the embodiment described above may be attained by the zoom amount change unit <b>1421</b>A.</p><p id="p-0378" num="0355">As described above, each of the examples has been described, but the invention is not limited to specific examples, and various deformations and changes can be made within the scope according to the claims. In addition, all or a plurality of constituents of the examples described above can also be combined.</p><p id="p-0379" num="0356">For example, in the embodiment described above, processing of setting the value of the deformation parameter A<b>1</b>, the value of the distance parameter A<b>2</b>, and the origin position, on the basis of a position relationship between the region of the field object within the viewing angle of the virtual camera <b>60</b> and the specific object is executed in the deformation processing associated with the movement of the predetermined object (step S<b>1615</b> in <figref idref="DRAWINGS">FIG. <b>16</b></figref>), but instead thereof or in addition thereto, may be executed in the bending deformation processing of the field object after the rotation processing (step S<b>1619</b> in <figref idref="DRAWINGS">FIG. <b>16</b></figref>). In this case, the value of the deformation parameter A<b>1</b>, the value of the distance parameter A<b>2</b>, and the origin position may be similarly set on the basis of a position relationship between the region of the field object within the viewing angle of the virtual camera <b>60</b> and the specific object after the rotation processing.</p><p id="p-0380" num="0357">Specifically, for example, in a case where the virtual camera <b>60</b> is rotated by a predetermined angle for each processing cycle, in the rotation processing according to the revolution and/or the planetary rotation, there are a plurality of regions of the field object falling within the viewing angle of the virtual camera <b>60</b> at each predetermined angle for one lap (in the case of the rotation processing according to the attack angle, there are a plurality of regions according to a variable range), and hereinafter, the region will be referred to as a predetermined region. In a case where there is the region in which the specific object is positioned (the specific object region) (an example of a second predetermined region), in the plurality of predetermined regions for one lap, there is also the region in which the specific object is not positioned (an example of a first predetermined region). Note that, there may be no specific object region, in accordance with the position of the virtual camera <b>60</b>. In a case where the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed in the regions in which the specific object is not positioned, on the basis of the rotation processing according to the revolution and/or the planetary rotation, in a situation where there is the specific object region in the plurality of predetermined regions for one lap, the value of the deformation parameter A<b>1</b> may not be changed, whereas in a case where the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is transitioned between the region in which the specific object is not positioned and the specific object region, the value of the deformation parameter A<b>1</b> may be changed. In this case, when the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed in the regions in which the specific object is not positioned, on the basis of the rotation processing according to the revolution and/or the planetary rotation, the bending deformation of the field object <b>77</b> according to the changed visual line direction V (the bending deformation in the same deformation mode when viewed in the visual line direction V of the virtual camera <b>60</b>) is attained, and thus, the height H<b>1</b> of the horizon line HL is not changed. Accordingly, the height H<b>1</b> of the horizon line HL can be constantly retained while the virtual camera <b>6</b> follows even a curved movement of the first object <b>3</b>. Here, in the modification example, the height H<b>1</b> of the horizon line HL may be changed within a level at which the user does not feel a sense of discomfort. For example, the level at which the user does not feel a sense of discomfort may be a level corresponding to fine unevenness or the like that can be provided in the field object (that is, a shape slightly different from the shape of the field surface <b>70</b>). According to a change within such a level, the height H<b>1</b> of the horizon line HL is approximately the same.</p><p id="p-0381" num="0358">On the other hand, in a case where the value of the deformation parameter A<b>1</b> is not changed, and the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is transitioned between the region in which the specific object is not positioned and the specific object region, the deformation degree increases, and thus, as described above, the specific object can be effectively emphasized. Even in this case, as with the case described above, the value of the deformation parameter A<b>1</b> may be changed when the specific object is positioned in the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of the second range) and when the specific object is positioned in the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of the first range). In addition, the value of the deformation parameter A<b>1</b> may be changed (may increase) to the value &#x3b2;<sub>G</sub>* (an example of the first setting value) from the interpolation value (an example of the second setting value) that is greater than the normal value &#x3b2;<sub>0 </sub>(an example of the predetermined setting value), through one or more interpolation values (an example of the second setting value), as the position of the specific object is changed to the center portion from the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, on the basis of the rotation processing according to the revolution and/or the planetary rotation.</p><p id="p-0382" num="0359">In addition, in the embodiment described above, the processing of setting the value of the deformation parameter A<b>1</b>, the value of the distance parameter A<b>2</b>, and the origin position is executed in the deformation processing associated with the movement of the predetermined object (step S<b>1615</b> in <figref idref="DRAWINGS">FIG. <b>16</b></figref>), on the basis of the position relationship between the region of the field object within the viewing angle of the virtual camera <b>60</b> and the specific object, but instead thereof or in addition thereto, may be executed in the distance change processing that is not associated with the movement of the predetermined object (for example, distance change processing according to a distance change instruction from the user that is not included in the manipulation information) (not illustrated). This is because even in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is not changed, the value of the distance parameter A<b>2</b> is changed, and thus, a transition may be performed between a state in which the specific object is positioned in the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> and a state in a case where the specific object is not positioned. The same applies to the processing of the zoom amount change unit <b>1421</b>A according to the modification example described above (processing of changing the value of the optical parameter relevant to the zoom amount of the virtual camera <b>60</b>).</p><p id="p-0383" num="0360">Specifically, for example, in a case where the value of the distance parameter A<b>2</b> of the virtual camera <b>60</b> is changed by a predetermined distance for each processing cycle, in the distance change processing according to the distance change instruction from the user, there are a plurality of regions of the field object within the viewing angle of the virtual camera <b>60</b> according to a variable range of the distance, for each predetermined distance, and in a case where there is the region in which the specific object is positioned (the specific object region), in the plurality of regions, there is also the region in which the specific object is not positioned. Note that, there may be no specific object region, in accordance with the position of the virtual camera <b>60</b>. In a case where the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed in the regions in which the specific object is not positioned, on the basis of the distance change processing according to the distance change instruction, in a situation where there is the specific object region in the plurality of regions, the value of the deformation parameter A<b>1</b> may not be changed, whereas in a case where the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is transitioned between the region in which the specific object is not positioned and the specific object region, the value of the deformation parameter A<b>1</b> may be changed. In this case, in a case where the region of the field object falling within the viewing angle of the virtual camera <b>60</b> is changed to the region in which the specific object is positioned, on the basis of the distance change processing according to the distance change instruction, the deformation degree increases, and thus, as described above, the specific object can be effectively emphasized. Even in this case, as with the case described above, the value of the deformation parameter A<b>1</b> may be changed when the specific object is positioned in the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of the second range) and when the specific object is positioned in the center portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b> (an example of the first range). In addition, the value of the deformation parameter A<b>1</b> may be changed (may increase) to the value &#x3b2;<sub>G</sub>* (an example of the first setting value) from the interpolation value (an example of the second setting value) that is greater than the normal value &#x3b2;<sub>0 </sub>(an example of the predetermined setting value), through one or more interpolation values (an example of the second setting value), as the position of the specific object is changed to the center portion from the end portion of the region of the field object <b>77</b> within the viewing angle of the virtual camera <b>60</b>, on the basis of the distance change processing according to the distance change instruction.</p><p id="p-0384" num="0361">In addition, in the embodiment described above, the bending deformation of the field object is attained by performing the bending deformation with respect to the entire field object, but the invention is not limited thereto. For example, the bending deformation of the field object may be executed with respect to only the region of the entire field object falling within the viewing angle of the virtual camera <b>60</b> or only a part of a region including the region.</p><p id="p-0385" num="0362">In addition, in the embodiment described above, in a case where the predetermined object is moved, the values of various parameters (for example, the distance parameter A<b>2</b>, the direction parameter &#x3b8;, the attack angle parameter &#x3c8;, and the like) associated with each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> are the same every time when each of the values of the position parameters (X, Y) is the same. For example, in a case where the predetermined object is moved, and the predetermined object is positioned in one position, the same values of various parameters (for example, the distance parameter A<b>2</b>, the direction parameter &#x3b8;, the attack angle parameter &#x3c8;, and the like) are attained with respect to the one position. However, the invention is not limited thereto. For example, in a case where the predetermined object is moved, the values of various parameters (for example, the distance parameter A<b>2</b>, the direction parameter &#x3b8;, the attack angle parameter &#x3c8;, and the like) associated with each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> may be changed in accordance with the progress situation of the game or other factors (for example, the movement direction of the predetermined object, and the like) even when each of the values of the position parameters (X, Y) is the same. For example, in a case where the predetermined condition is established on the basis of the progress situation of the game or other factors, the value of the deformation parameter A<b>1</b> associated with the specific position A may be set to the value &#x3b2;<sub>1</sub>, and in the other case, the value may be set to the normal value &#x3b2;<sub>0</sub>. In addition, such a change, for example, may be executed in accordance with the occurrence of a predetermined event, a case where the second object that is moved enters the region of the field object falling within the viewing angle of the virtual camera <b>60</b> (is arranged within the screen), or a manual manipulation of the user for changing the viewing angle.</p><p id="p-0386" num="0363">In addition, in the embodiment described above, each of the specific positions such as the specific positions A and B is a fixed position on the field object, and may be a position that is movable on the field object. For example, a part of the specific position may be set in accordance with the position of a predetermined second object that is moved, in the second objects. In this case, one specific position may be a position having a predetermined relationship with respect to the position of one predetermined second object that is moved.</p><p id="p-0387" num="0364">In addition, in the embodiment described above, as a case where the interpolation processing range according to the specific position (hereinafter, the same applies to the interpolation processing range according to the specific object) is fixed without being dynamically changed, as described above, in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is positioned within the interpolation processing range, and the values of the distance parameter A<b>2</b> and the attack angle parameter &#x3c8; are the normal values &#x3b3;<sub>0 </sub>and &#x3c8;<sub>0</sub>, respectively, it may be set that the specific position according to the interpolation processing range is positioned in the region of the field object falling within the viewing angle of the virtual camera <b>60</b>, even at the time of the value of the arbitrary direction parameter &#x3b8;. For example, in <figref idref="DRAWINGS">FIG. <b>31</b></figref>, an interpolation processing range Rs(A) relevant to the specific position A is illustrated in the plan view, and in <figref idref="DRAWINGS">FIG. <b>32</b></figref>, a sectional view along line J<b>1</b>-J<b>1</b> of <figref idref="DRAWINGS">FIG. <b>31</b></figref> and a sectional view along line J<b>2</b>-J<b>2</b> are illustrated to overlap with each other. In this case, when each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> corresponds to a position P<b>260</b>, and the values of the distance parameter A<b>2</b> and the attack angle parameter &#x3c8; are the normal values &#x3b3;<sub>0 </sub>and &#x3c8;<sub>0</sub>, respectively, as illustrated in <figref idref="DRAWINGS">FIG. <b>32</b></figref>, the specific position A is positioned in the region of the field object falling within the viewing angle of the virtual camera <b>60</b> in a case where the value of the parameter &#x3b8; is &#x3b8;(P<b>260</b>) as illustrated in <figref idref="DRAWINGS">FIG. <b>31</b></figref>. Note that, the boundary line <b>6211</b> illustrated in <figref idref="DRAWINGS">FIG. <b>32</b></figref> is a boundary line on the upper side of the viewing angle <b>62</b> of the virtual camera <b>60</b> (the viewing angle when viewed in the direction perpendicular to the z direction) (refer to <figref idref="DRAWINGS">FIG. <b>5</b></figref>). Accordingly, in this case, the position P<b>260</b> belongs to the interpolation processing range Rs(A). On the other hand, when each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> corresponds to a position P<b>261</b>, and the values of the distance parameter A<b>2</b> and the attack angle parameter &#x3c8; are the normal values &#x3b3;<sub>0 </sub>and &#x3c8;<sub>0</sub>, respectively, as illustrated in <figref idref="DRAWINGS">FIG. <b>32</b></figref>, the specific position A is not positioned in the region of the field object falling within the viewing angle of the virtual camera <b>60</b> in a case where the value of the parameter &#x3b8; is &#x3b8;(P<b>261</b>) as illustrated in <figref idref="DRAWINGS">FIG. <b>31</b></figref>. Accordingly, in this case, the position P<b>260</b> does not belong to the interpolation processing range Rs(A). However, as another setting mode, it may be set that the interpolation processing range is dynamically changed on the basis of each of the values of the camera parameters at this time. For example, when each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is positioned within the interpolation processing range, and each of the values of the distance parameter A<b>2</b> and the attack angle parameter &#x3c8; after being updated (or before being updated) (can be values different from the normal values &#x3b3;<sub>0 </sub>and &#x3c8;<sub>0</sub>) is applied, an interpolation processing range according to one specific position may be dynamically set such that the one specific position is positioned in the region falling within the viewing angle of the virtual camera <b>60</b> even in the case of the value of the arbitrary direction parameter &#x3b8;. This is because even in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is out of one interpolation processing range, the values are positioned within the other interpolation processing range, and thus, the value of the distance parameter A<b>2</b> or the like may be an interpolation value that is not a normal value. Note that, here, a specification is assumed in which the revolution or the planetary rotation of the virtual camera <b>60</b> can be performed in an arbitrary position, but in a specification where the revolution or the planetary rotation of the virtual camera <b>60</b> can be performed only in a limited position or a specification where the revolution or the planetary rotation itself is not capable of being performed, &#x201c;even in the case of the value of the arbitrary direction parameter &#x3b8;&#x201d; described above may be replaced with &#x201c;in the case of the value of the direction parameter &#x3b8; at this time point&#x201d;.</p><p id="p-0388" num="0365">In addition, in a mode where the interpolation processing range increases as a change rate (a change amount per hour) of each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> increases, the width of the interpolation processing range may be dynamically changed in accordance with the change rate. Alternatively, on the basis of the similar concept, a predetermined margin may be set with respect to the interpolation processing range, regardless of the change rate. Accordingly, for example, a disadvantage that may occur in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is changed to each value corresponding to the specific position (or the specific object), in a state where the change rate (the change amount per hour) of each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is large (a sense of discomfort that can be felt by the user due to a comparatively steep change in the field image), can be reduced.</p><p id="p-0389" num="0366">In addition, in a case where the viewing angle of the virtual camera <b>60</b> is variable, in a mode where the interpolation processing range increases as the viewing angle decreases, the width of the interpolation processing range may be dynamically changed in accordance with the viewing angle of the virtual camera <b>60</b>. Accordingly, for example, a disadvantage that may occur in a case where each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> is changed to each value corresponding to the specific position (the specific object), in a state where the viewing angle of the virtual camera <b>60</b> is small (a sense of discomfort that can be felt by the user due to a comparatively steep change in the field image), can be reduced.</p><p id="p-0390" num="0367">In addition, in the embodiment described above, whether or not to execute interpolation processing is set on the basis of whether or not each of the values of the position parameters (X, Y) after being updated is positioned in the interpolation processing range, but the invention is not limited thereto. For example, equivalently, each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> after being updated may be positioned, the region of the field object falling within the viewing angle of the virtual camera <b>60</b> (that is, the region within the viewing angle described above) may be derived on the basis of each of the values of the distance parameter A<b>2</b> and the attack angle parameter &#x3c8; after being updated, and it may be determined whether or not the specific position or the specific object is positioned in the region within the viewing angle. In this case, when the specific position or the specific object is positioned in the region within the viewing angle, but each of the values of the position parameters (X, Y) of the virtual camera <b>60</b> after being updated does not correspond to the position of the specific position or the specific object, the interpolation processing may be executed.</p><p id="p-0391" num="0368">In addition, in each of the embodiments described above, the first movement processing unit <b>1420</b> may be omitted.</p><p id="p-0392" num="0369">Note that, the following appendices will be further disclosed regarding the examples described above.</p><heading id="h-0010" level="2">[Appendix 1]</heading><p id="p-0393" num="0370">An information processing device for drawing an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, in an expression viewed from a virtual camera arranged in the virtual space,</p><p id="p-0394" num="0371">the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis, and a specific object arranged on the field object, the device including:</p><p id="p-0395" num="0372">a change processing unit changing a region of the field object falling within a viewing angle of the virtual camera; and</p><p id="p-0396" num="0373">a deformation processing unit deforming the field object,</p><p id="p-0397" num="0374">in which in a case where the region is changed by the change processing unit, the deformation processing unit determines whether or not the specific object is positioned in the region after being changed, and makes a deformation mode of the field object different when it is determined that the specific object is positioned in the region after being changed and when it is determined that the specific object is not positioned in the region after being changed.</p><heading id="h-0011" level="2">[Appendix 2]</heading><p id="p-0398" num="0375">The information processing device according to appendix 1,</p><p id="p-0399" num="0376">in which in a case where the region is changed by the change processing unit, the deformation processing unit further determines whether the specific object is positioned within a first range in the region after being changed or the specific object is positioned within a second range adjacent to the first range in the region, and makes a deformation degree of the field object different when it is determined that the specific object is positioned within the first range and when it is determined that the specific object is positioned within the second range.</p><heading id="h-0012" level="2">[Appendix 3]</heading><p id="p-0400" num="0377">The information processing device according to appendix 2,</p><p id="p-0401" num="0378">in which the second range is positioned on both sides of the first range of the viewing angle of the virtual camera in a horizontal direction, and</p><p id="p-0402" num="0379">in a case where it is determined that the specific object is positioned within the first range, the deformation processing unit increases the deformation degree of the field object, compared to a case where it is determined that the specific object is positioned within the second range.</p><heading id="h-0013" level="2">[Appendix 4]</heading><p id="p-0403" num="0380">The information processing device according to appendix 1,</p><p id="p-0404" num="0381">in which in a case where a position of the specific object in the region after being changed is changed toward a center portion of the region after being changed from one end side in a horizontal direction, every time when the region is changed by the change processing unit, the deformation processing unit increases a deformation degree of the field object, every time when the region is changed by the change processing unit.</p><heading id="h-0014" level="2">[Appendix 5]</heading><p id="p-0405" num="0382">The information processing device according to appendix 1,</p><p id="p-0406" num="0383">in which the field object is shaped on the basis of a deformable basic surface, and</p><p id="p-0407" num="0384">in a case where a predetermined position is set to an origin on a plane including a visual line direction of the virtual camera and the third axis, an axis that passes through the origin and is parallel to the third axis is set to a Y axis, a direction toward an upper side with respect to the field object is set to a positive side of the Y axis, and an axis that passes through the origin and is perpendicular to the Y axis is set to an X axis, the deformation processing unit deforms the basic surface, in accordance with a function in which a value of a Y coordinate monotonically decreases linearly or non-linearly as an absolute value of a value of an X coordinate increases.</p><heading id="h-0015" level="2">[Appendix 6]</heading><p id="p-0408" num="0385">The information processing device according to appendix 5,</p><p id="p-0409" num="0386">in which the deformation processing unit deforms the basic surface, on the basis of a value of a deformation parameter that is changeable in accordance with a change in the region and sets a deformation degree of the field object, and</p><p id="p-0410" num="0387">the deformation parameter includes a coefficient that is assigned to a term for the value of the X coordinate, in the function.</p><heading id="h-0016" level="2">[Appendix 7]</heading><p id="p-0411" num="0388">The information processing device according to appendix 6,</p><p id="p-0412" num="0389">in which the region includes a predetermined object region in which the specific object is not positioned, a first object region in which the specific object is positioned in a center portion, and a second object region in which the specific object is positioned in an end portion,</p><p id="p-0413" num="0390">the value of the deformation parameter includes a predetermined setting value set in the predetermined object region, a first setting value set in the first object region, and a second setting value set in the second object region, and</p><p id="p-0414" num="0391">the first setting value attains the deformation degree of the field object that is greater than the second setting value, and the second setting value attains the deformation degree of the field object that is greater than the predetermined setting value.</p><heading id="h-0017" level="2">[Appendix 8]</heading><p id="p-0415" num="0392">The information processing device according to appendix 7,</p><p id="p-0416" num="0393">in which in a case where the region is changed between the predetermined object region and the first object region by the change processing unit through the second object region, the deformation processing unit changes the value of the deformation parameter to the first setting value or the predetermined setting value through the second setting value.</p><heading id="h-0018" level="2">[Appendix 9]</heading><p id="p-0417" num="0394">The information processing device according to appendix 6,</p><p id="p-0418" num="0395">in which the specific object includes a first specific object and a second specific object,</p><p id="p-0419" num="0396">the region includes a predetermined object region in which both of the first specific object and the second specific object are not positioned, and an object region in which both of the first specific object and the second specific object are positioned,</p><p id="p-0420" num="0397">the object region includes a first object region in which the first specific object is positioned in a center portion, a second object region in which both of the first specific object and the second specific object are not positioned in a center portion, and a third object region in which the second specific object is positioned in a center portion,</p><p id="p-0421" num="0398">the value of the deformation parameter includes a predetermined setting value set in the predetermined object region, a first setting value set in the first object region and the third object region, and a second setting value set in the second object region,</p><p id="p-0422" num="0399">the first setting value attains the deformation degree of the field object that is greater than the second setting value, and the second setting value attains the deformation degree of the field object that is greater than the predetermined setting value, and</p><p id="p-0423" num="0400">in a case where the region is changed between the predetermined object region and the first object region or the third object region by the change processing unit through the second object region, the deformation processing unit changes the value of the deformation parameter to the first setting value or the predetermined setting value through the second setting value.</p><heading id="h-0019" level="2">[Appendix 10]</heading><p id="p-0424" num="0401">The information processing device according to any one of appendices 5 to 9,</p><p id="p-0425" num="0402">in which in a case where it is determined that the specific object is positioned in the region after being changed, the deformation processing unit sets the predetermined position, on the basis of a position of the specific object.</p><heading id="h-0020" level="2">[Appendix 11]</heading><p id="p-0426" num="0403">The information processing device according to any one of appendices 1 to 10,</p><p id="p-0427" num="0404">in which the change processing unit includes a first movement processing unit relatively changing a position of the virtual camera with respect to the field object, in a direction intersecting with the visual line direction of the virtual camera, and</p><p id="p-0428" num="0405">the region is changed by the first movement processing unit changing the position of the virtual camera with respect to the field object.</p><heading id="h-0021" level="2">[Appendix 12]</heading><p id="p-0429" num="0406">The information processing device according to any one of appendices 5 to 10,</p><p id="p-0430" num="0407">in which the specific object is fixed with respect to the field object,</p><p id="p-0431" num="0408">the object further includes a predetermined object arranged with respect to the field object,</p><p id="p-0432" num="0409">the information processing device, further includes: a second movement processing unit relatively changing a position of the predetermined object with respect to the field object within the two-dimensional plane defined by the first axis and the second axis, and</p><p id="p-0433" num="0410">in a case where it is determined that the specific object is not positioned in the region after being changed, the deformation processing unit sets the predetermined position, on the basis of the position of the predetermined object.</p><heading id="h-0022" level="2">[Appendix 13]</heading><p id="p-0434" num="0411">The information processing device according to any one of appendices 1 to 12,</p><p id="p-0435" num="0412">in which in a case where it is determined that the specific object is not positioned in the region after being changed, the deformation processing unit further determines whether the position of the virtual camera with respect to the field object is in a first position or in a second position different from the first position in the direction intersecting with the visual line direction of the virtual camera, and makes the deformation degree of the field object different when it is determined that the position of the virtual camera with respect to the field object is in the first position and when it is determined that the position of the virtual camera with respect to the field object is in the second position.</p><heading id="h-0023" level="2">[Appendix 14]</heading><p id="p-0436" num="0413">The information processing device according to appendix 13, further including:</p><p id="p-0437" num="0414">a distance change unit making a distance between the virtual camera and the field object in the visual line direction of the virtual camera different when the position of the virtual camera with respect to the field object is in the first position and when the position of the virtual camera with respect to the field object is in the second position.</p><heading id="h-0024" level="2">[Appendix 15]</heading><p id="p-0438" num="0415">The information processing device according to any one of appendices 1 to 12,</p><p id="p-0439" num="0416">in which the change processing unit further includes a distance change unit changing a distance between the virtual camera and the field object in the visual line direction of the virtual camera, and</p><p id="p-0440" num="0417">the region is changed by the distance change unit changing a distance of the virtual camera with respect to the field object.</p><heading id="h-0025" level="2">[Appendix 16]</heading><p id="p-0441" num="0418">The information processing device according to appendix 15,</p><p id="p-0442" num="0419">in which in a case where it is determined that the specific object is not positioned in the region after being changed, the deformation processing unit further determines whether the distance of the virtual camera with respect to the field object is a first distance or a second distance different from the first distance, and makes the deformation degree of the field object different when it is determined that the distance of the virtual camera with respect to the field object is the first distance and when it is determined that the distance of the virtual camera with respect to the field object is the second distance.</p><heading id="h-0026" level="2">[Appendix 17]</heading><p id="p-0443" num="0420">The information processing device according to any one of appendices 1 to 16,</p><p id="p-0444" num="0421">in which the change processing unit further includes a rotation processing unit changing the visual line direction of the virtual camera with respect to the field object around the third axis, and</p><p id="p-0445" num="0422">the region is changed by the rotation processing unit changing the visual line direction of the virtual camera with respect to the field object.</p><heading id="h-0027" level="2">[Appendix 18]</heading><p id="p-0446" num="0423">The information processing device according to appendix 17,</p><p id="p-0447" num="0424">in which the region includes a plurality of predetermined regions, each of which sequentially falls within the viewing angle of the virtual camera, every time when the visual line direction is changed around the third axis by a predetermined angle by the rotation processing unit,</p><p id="p-0448" num="0425">the plurality of predetermined regions include a plurality of first predetermined regions in which the specific object is not positioned, and a second predetermined region in which the specific object is positioned, and</p><p id="p-0449" num="0426">in a case where the visual line direction is changed around the third axis by the rotation processing unit, the deformation processing unit determines whether or not the region is changed from one to another of the plurality of first predetermined regions, and performs bending deformation with respect to the field object such that a deformation mode is the same when viewed in the visual line direction before being changed and when viewed in the visual line direction after being changed in a case where it is determined that the region is changed from one to another of the plurality of first predetermined regions.</p><heading id="h-0028" level="2">[Appendix 19]</heading><p id="p-0450" num="0427">The information processing device according to any one of appendices 1 to 18,</p><p id="p-0451" num="0428">in which the object further includes a background object, and</p><p id="p-0452" num="0429">the information processing device further includes: a background processing unit changing a position of the background object with respect to the field object along a direction of the third axis in a case where it is determined that the deformation degree of the field object is changed by determining whether or not the deformation degree of the field object that is deformed by the deformation processing unit is changed.</p><heading id="h-0029" level="2">[Appendix 20]</heading><p id="p-0453" num="0430">An information processing method for drawing an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, in an expression viewed from a virtual camera arranged in the virtual space, which is executed by a computer,</p><p id="p-0454" num="0431">the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis, and a specific object arranged on the field object, the method including:</p><p id="p-0455" num="0432">determining whether or not the specific object is positioned in a region of the field object falling within a viewing angle of the virtual camera after being changed in a case where the region is changed;</p><p id="p-0456" num="0433">deforming the field object in a first deformation mode in a case where it is determined that the specific object is positioned in the region after being changed; and</p><p id="p-0457" num="0434">deforming the field object in a second deformation mode different from the first deformation mode in a case where it is determined that the specific object is not positioned in the region after being changed.</p><heading id="h-0030" level="2">[Appendix 21]</heading><p id="p-0458" num="0435">An information processing program for drawing an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, in an expression viewed from a virtual camera arranged in the virtual space,</p><p id="p-0459" num="0436">the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis, and a specific object arranged on the field object, the program allowing a computer to execute processing of:</p><p id="p-0460" num="0437">determining whether or not the specific object is positioned in a region of the field object falling within a viewing angle of the virtual camera after being changed in a case where the region is changed;</p><p id="p-0461" num="0438">deforming the field object in a first deformation mode in a case where it is determined that the specific object is positioned in the region after being changed; and</p><p id="p-0462" num="0439">deforming the field object in a second deformation mode different from the first deformation mode in a case where it is determined that the specific object is not positioned in the region after being changed.</p><heading id="h-0031" level="1">REFERENCE SIGNS LIST</heading><p id="p-0463" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0440"><b>1</b> game system</li>    <li id="ul0001-0002" num="0441"><b>3</b> first object</li>    <li id="ul0001-0003" num="0442"><b>10</b> server device</li>    <li id="ul0001-0004" num="0443"><b>11</b> server communication unit</li>    <li id="ul0001-0005" num="0444"><b>12</b> server storage unit</li>    <li id="ul0001-0006" num="0445"><b>13</b> server control unit</li>    <li id="ul0001-0007" num="0446"><b>14</b> horizontal passage</li>    <li id="ul0001-0008" num="0447"><b>15</b> vertical passage</li>    <li id="ul0001-0009" num="0448"><b>16</b> street plant object</li>    <li id="ul0001-0010" num="0449"><b>20</b> terminal device</li>    <li id="ul0001-0011" num="0450"><b>21</b> terminal communication unit</li>    <li id="ul0001-0012" num="0451"><b>22</b> terminal storage unit</li>    <li id="ul0001-0013" num="0452"><b>23</b> display unit</li>    <li id="ul0001-0014" num="0453"><b>24</b> input unit</li>    <li id="ul0001-0015" num="0454"><b>25</b> terminal control unit</li>    <li id="ul0001-0016" num="0455"><b>30</b> network</li>    <li id="ul0001-0017" num="0456"><b>60</b> virtual camera</li>    <li id="ul0001-0018" num="0457"><b>62</b> viewing angle</li>    <li id="ul0001-0019" num="0458"><b>70</b> field surface</li>    <li id="ul0001-0020" num="0459"><b>72</b> background surface</li>    <li id="ul0001-0021" num="0460"><b>77</b> field object</li>    <li id="ul0001-0022" num="0461"><b>130</b> drawing information storage unit</li>    <li id="ul0001-0023" num="0462"><b>132</b> manipulation information acquisition unit</li>    <li id="ul0001-0024" num="0463"><b>134</b> drawing data transmission unit</li>    <li id="ul0001-0025" num="0464"><b>140</b> drawing processing unit</li>    <li id="ul0001-0026" num="0465"><b>142</b> change processing unit</li>    <li id="ul0001-0027" num="0466"><b>1420</b> first movement processing unit</li>    <li id="ul0001-0028" num="0467"><b>1421</b> distance change unit</li>    <li id="ul0001-0029" num="0468"><b>14211</b> first distance change unit</li>    <li id="ul0001-0030" num="0469"><b>14212</b> second distance change unit</li>    <li id="ul0001-0031" num="0470"><b>1421</b>A zoom amount change unit</li>    <li id="ul0001-0032" num="0471"><b>1422</b> direction change unit</li>    <li id="ul0001-0033" num="0472"><b>1423</b> attack angle change unit</li>    <li id="ul0001-0034" num="0473"><b>1424</b> update reflection unit</li>    <li id="ul0001-0035" num="0474"><b>1425</b> rotation processing unit</li>    <li id="ul0001-0036" num="0475"><b>14251</b> revolution processing unit</li>    <li id="ul0001-0037" num="0476"><b>14252</b> planetary rotation processing unit</li>    <li id="ul0001-0038" num="0477"><b>14253</b> attack angle processing unit</li>    <li id="ul0001-0039" num="0478"><b>144</b> second movement processing unit</li>    <li id="ul0001-0040" num="0479"><b>145</b> deformation processing unit</li>    <li id="ul0001-0041" num="0480"><b>1451</b> first deformation parameter calculation unit</li>    <li id="ul0001-0042" num="0481"><b>1452</b> second deformation parameter calculation unit</li>    <li id="ul0001-0043" num="0482"><b>1453</b> deformation parameter adjustment unit</li>    <li id="ul0001-0044" num="0483"><b>1454</b> origin setting processing unit</li>    <li id="ul0001-0045" num="0484"><b>1455</b> deformation function application unit</li>    <li id="ul0001-0046" num="0485"><b>146</b> projection processing unit</li>    <li id="ul0001-0047" num="0486"><b>147</b> background processing unit</li>    <li id="ul0001-0048" num="0487"><b>148</b> drawing data generation unit</li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An information processing device configured to draw an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, said information processing device configured to draw the object in an expression viewed from a virtual camera arranged in the three-dimensional virtual space at a virtual camera position including a first axis coordinate, a second axis coordinate, and a third axis coordinate, and having a virtual camera orientation, the information processing device comprising:<claim-text>a storage unit configured to store the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis;</claim-text><claim-text>a change processing unit configured to change a region of the field object falling within a viewing angle of the virtual camera; and</claim-text><claim-text>a deformation processing unit configured to deform the field object,</claim-text><claim-text>wherein the information processing unit is configured to:</claim-text><claim-text>determine a visual line that is tangent to the field object and that extends through the virtual camera position; and</claim-text><claim-text>after receiving a request to change at least one of the virtual camera position or the virtual camera orientation, change a first deformation mode of the field object to a second deformation mode of the field object,</claim-text><claim-text>wherein the first deformation mode of the field object is a deformation mode of the field object determined prior to adjustment of the virtual camera, and wherein the second deformation mode of the field object is a new deformation mode of the field object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the request is a request to change the virtual camera position; and<claim-text>wherein the information processing unit is configured to determine the visual line tangent to the field object and extending through the virtual camera position after updating the virtual camera position based on the request.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The information processing device according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the information processing unit is configured to, after receiving the request to change the virtual camera position:<claim-text>determine whether an amount of change of the virtual camera position is a predetermined amount or more;</claim-text><claim-text>in a case where the amount of change of the virtual camera position is the predetermined amount or more, performing a step of adjusting a background object; and</claim-text><claim-text>in a case where the amount of change of the virtual camera position is less than the predetermined amount, skipping a step of adjusting a background object.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the request is a request to change the virtual camera orientation; and<claim-text>wherein the information processing unit is configured to change the first deformation mode of the field object to the second deformation mode of the field object after a step of executing rotation processing to determine a new virtual camera orientation.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The information processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of executing rotation processing to determine the new virtual camera orientation comprises:<claim-text>identifying a rotational axis defined within the three-dimensional virtual space, wherein the rotational axis is one of: a predetermined rotational axis or selected from a predetermined set of rotational axes; and</claim-text><claim-text>performing a rotation around the rotational axis.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The information processing device according to <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the information processing unit is configured to maintain the virtual camera at a predetermined distance from the rotational axis during rotation around the rotational axis.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The information processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of executing rotation processing to determine the new virtual camera orientation comprises:<claim-text>based on the request, rotating the virtual camera by a predetermined angle.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The information processing device according to <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the step of executing rotation processing to determine the new virtual camera orientation comprises:<claim-text>performing a plurality of rotations, each rotation comprising rotating the virtual camera by a predetermined angle;</claim-text><claim-text>wherein the information processing unit is configured to execute a plurality of processing cycles, and wherein the information processing unit is configured to perform one rotation in the plurality of rotations per processing cycle in the plurality of processing cycles.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the information processing unit is configured to, upon changing the first deformation mode of the field object to the second deformation mode of the field object, adjust a background object based on the change from the first deformation mode to the second deformation mode.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the request is a request to change the virtual camera orientation; and<claim-text>wherein the information processing unit is configured to, upon changing the first deformation mode of the field object to the second deformation mode of the field object, positionally move or rotationally move a background object based on the change from the first deformation mode to the second deformation mode.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the information processing unit is further configured to:<claim-text>after determination of the tangent line, identify an intersection point between the tangent line and the field object; and</claim-text><claim-text>define a horizon at the intersection point.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The information processing device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the information processing unit is configured to define the horizon at the intersection point by defining a first height of a horizon line; and<claim-text>wherein the information processing unit is configured to define a second height of the horizon line after changing the first deformation mode of the field object to the second deformation mode of the field object, and change a visible portion of a background object based on a change from the first height of the horizon line to the second height of the horizon line.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The information processing device according to <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the information processing unit is configured to change the visible portion of the background object by positionally moving the background object.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The information processing device according to <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein a direction of positional movement of the background object is based on a determination of whether the first height or second height is larger;<claim-text>wherein the information processing unit is configured to positionally move the background object in a first direction when the second height is adjusted to be smaller than the first height, wherein the first direction is defined as a vector parallel to the third axis and which extends from the intersection point away from the visible portion of the background object; and</claim-text><claim-text>wherein the information processing unit is configured to positionally move the background object in a second direction when the second height is adjusted to be larger than the first height, wherein the second direction is defined as a vector parallel to the third axis and which extends from the intersection point towards the visible portion of the background object.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The information processing device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the information processing unit is further configured to:<claim-text>define, within the three-dimensional virtual space, the position of a game object;</claim-text><claim-text>calculate, based on the horizon, whether or not the game object is visible at the position; and</claim-text><claim-text>in the case where the game object is not visible at the position, skipping a step of drawing the game object.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The information processing device according to <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the information processing unit is further configured to:<claim-text>determine a change in the position of the game object with respect to the horizon; and</claim-text><claim-text>recalculate whether or not the game object is visible at the position following the change in the position of the game object with respect to the horizon.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The information processing device according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>wherein the field object is shaped based on a shape of a deformable basic surface; and</claim-text><claim-text>wherein, for a two-dimensional coordinate system defined by a plane including two vectors, a first vector being a direction of a line of sight of the virtual camera and a second vector being a direction of the third axis:</claim-text><claim-text>a predetermined position is set as an origin of the two-dimensional coordinate system;</claim-text><claim-text>an axis passing through the origin and parallel to the third axis is identified as a Y axis of the two-dimensional coordinate system;</claim-text><claim-text>an upper side of the field object is oriented in a positive direction of the Y axis;</claim-text><claim-text>an axis passing through the origin and perpendicular to the third axis is identified as an X axis of the two-dimensional coordinate system; and</claim-text><claim-text>wherein the deformation processing unit is configured to deform the deformable basic surface based on a predetermined X-Y function, said predetermined X-Y function comprising monotonically decreasing a value of a Y coordinate of the deformable basic surface linearly or nonlinearly as a value of an X coordinate increases.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The information processing device according to <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the predetermined X-Y function includes a deformation parameter; and<claim-text>wherein the information processing unit is configured to change the first deformation mode of the field object to the second deformation mode of the field object by varying the deformation parameter.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. An information processing method, executed by a computer, for drawing an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, in an expression viewed from a virtual camera arranged in the virtual space in a virtual camera position including a first axis coordinate, a second axis coordinate, and a third axis coordinate, and having a virtual camera orientation, the method comprising:<claim-text>storing the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis;</claim-text><claim-text>changing a region of the field object falling within a viewing angle of the virtual camera;</claim-text><claim-text>receiving a request to change at least one of the virtual camera position or the virtual camera orientation;</claim-text><claim-text>determining a visual line that is tangent to the field object and that extends through the virtual camera position;</claim-text><claim-text>after receiving the request to change the at least one of the virtual camera position or the virtual camera orientation,</claim-text><claim-text>deforming the field object by changing a first deformation mode of the field object to a second deformation mode of the field object, wherein the first deformation mode of the field object is a deformation mode of the field object determined prior to adjustment of the virtual camera, and wherein the second deformation mode of the field object is a new deformation mode of the field object.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. A non-transitory computer-readable medium comprising an information processing program for drawing an object arranged in a three-dimensional virtual space defined by a first axis, a second axis, and a third axis that are orthogonal to each other, in an expression viewed from a virtual camera arranged in the virtual space in a virtual camera position including a first axis coordinate, a second axis coordinate, and a third axis coordinate, and having a virtual camera orientation, wherein the information processing program, when executed by a computer, causes the computer to carry out steps of:<claim-text>storing the object including a field object associated with a two-dimensional plane defined by the first axis and the second axis;</claim-text><claim-text>changing a region of the field object falling within a viewing angle of the virtual camera;</claim-text><claim-text>receiving a request to change at least one of the virtual camera position or the virtual camera orientation;</claim-text><claim-text>determining a visual line that is tangent to the field object and that extends through the virtual camera position;</claim-text><claim-text>after receiving the request to change the at least one of the virtual camera position or the virtual camera orientation,</claim-text><claim-text>deforming the field object by changing a first deformation mode of the field object to a second deformation mode of the field object, wherein the first deformation mode of the field object is a deformation mode of the field object determined prior to adjustment of the virtual camera, and wherein the second deformation mode of the field object is a new deformation mode of the field object.</claim-text></claim-text></claim></claims></us-patent-application>