<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005284A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005284</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17943458</doc-number><date>20220913</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>202111101658.8</doc-number><date>20210918</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>19</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>583</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>19147</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>30</main-group><subgroup>1916</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>583</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD FOR TRAINING IMAGE-TEXT MATCHING MODEL, COMPUTING DEVICE, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>BEIJING BAIDU NETCOM SCIENCE TECHNOLOGY CO., LTD.</orgname><address><city>BEIJING</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>HE</last-name><first-name>Feng</first-name><address><city>BEIJING</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>WANG</last-name><first-name>Qi</first-name><address><city>BEIJING</city><country>CN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>YANG</last-name><first-name>Hu</first-name><address><city>BEIJING</city><country>CN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>CHEN</last-name><first-name>Shuai</first-name><address><city>BEIJING</city><country>CN</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>FENG</last-name><first-name>Zhifan</first-name><address><city>BEIJING</city><country>CN</country></address></addressbook></inventor><inventor sequence="05" designation="us-only"><addressbook><last-name>CHAI</last-name><first-name>Chunguang</first-name><address><city>BEIJING</city><country>CN</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computer-implemented method is provided. The method includes: obtaining a sample text and a sample image corresponding to the sample text; labeling a true semantic tag for the sample text according to a first preset rule; obtaining a text feature representation of the sample text and a predicted semantic tag output by a text coding sub-model; obtaining an image feature representation of the sample image output by an image coding sub-model; calculating a first loss based on the true semantic tag and the predicted semantic tag; calculating a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image; adjusting parameters of the text coding sub-model based on the first loss and the contrast loss; and adjusting parameters of the image coding sub-model based on the contrast loss.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="215.73mm" wi="88.65mm" file="US20230005284A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="116.50mm" wi="145.12mm" file="US20230005284A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="227.16mm" wi="90.68mm" file="US20230005284A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="128.10mm" wi="151.38mm" file="US20230005284A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="145.63mm" wi="107.02mm" file="US20230005284A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="185.59mm" wi="139.28mm" file="US20230005284A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="93.30mm" wi="129.03mm" file="US20230005284A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to Chinese patent application No. 202111101658.8, filed on Sep. 18, 2021, the contents of which are hereby incorporated by reference in their entirety for all purposes.</p><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present disclosure relates to the field of artificial intelligence, in particular to the technical fields of knowledge mapping and deep learning, and specifically relates to a method training for an image-text matching model, a method for realizing image-text retrieval, a computing device and a computer readable storage medium.</p><heading id="h-0003" level="1">BACKGROUND</heading><p id="p-0004" num="0003">Artificial intelligence is a discipline of making computers to simulate certain thinking processes and intelligent behaviors of people (such as learning, reasoning, thinking, planning, etc.), involving both hardware-level technologies and software-level technologies. Artificial intelligence hardware technologies generally include technologies such as sensors, dedicated artificial intelligence chips, cloud computing, distributed storage, and big data processing. Artificial intelligence software technologies mainly include computer vision technology, speech recognition technology, natural language processing technology, machine learning/deep learning, big data processing technology, knowledge mapping technology and other major directions.</p><p id="p-0005" num="0004">With the development of Internet technology, the number of images on the Internet has been rising explosively. As an information carrier, images play an important role in people's daily life and work. People often need to retrieve desired images through texts.</p><p id="p-0006" num="0005">Approaches described in this section are not necessarily approaches that have been previously conceived or employed. Unless otherwise indicated, it should not be assumed that any of the approaches described in this section qualify as prior art merely by virtue of their inclusion in this section. Similarly, unless otherwise indicated, issues mentioned in this section should not be considered to be recognized in any prior art.</p><heading id="h-0004" level="1">SUMMARY</heading><p id="p-0007" num="0006">The present disclosure provides a method for training an image-text matching model, a computing device and a computer readable storage medium.</p><p id="p-0008" num="0007">According to one aspect of the present disclosure, a computer-implemented method is provided. The method includes: obtaining a sample text and a sample image corresponding to the sample text; labeling a true semantic tag for the sample text according to a first preset rule; inputting the sample text into a text coding sub-model of an image-text matching model, and obtaining a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model; inputting the sample image into an image coding sub-model of the image-text matching model, and obtaining an image feature representation of the sample image output by the image coding sub-model; calculating a first loss based on the true semantic tag and the predicted semantic tag; calculating a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image; adjusting one or more parameters of the text coding sub-model based on the first loss and the contrast loss; and adjusting one or more parameters of the image coding sub-model based on the contrast loss.</p><p id="p-0009" num="0008">According to another aspect of the present disclosure, a computing device is provided, and includes: one or more processors; and a memory storing one or more programs configured to be executed by the one or more processors, the one or more programs comprising instructions for performing operations comprising: obtaining a sample text and a sample image corresponding to the sample text; labeling a true semantic tag for the sample text according to a first preset rule; inputting the sample text into a text coding sub-model of an image-text matching model, and obtaining a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model; inputting the sample image into an image coding sub-model of the image-text matching model, and obtaining an image feature representation of the sample image output by the image coding sub-model; calculating a first loss function based on the true semantic tag and the predicted semantic tag; calculating a contrast loss function based on the text feature representation of the sample text and the image feature representation of the sample image; adjusting one or more parameters of the text coding sub-model based on the first loss function and the contrast loss function; and adjusting one or more parameters of the image coding sub-model based on the contrast loss.</p><p id="p-0010" num="0009">According to another aspect of the present disclosure, a non-transitory computer readable storage medium is provided. The non-transitory computer-readable storage medium storing one or more programs comprising instructions that, when executed by one or more processors of a computing device, cause the computing device to perform operations comprising: obtaining a sample text and a sample image corresponding to the sample text; labeling a true semantic tag for the sample text according to a first preset rule; inputting the sample text into a text coding sub-model of an image-text matching model, and obtaining a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model; inputting the sample image into an image coding sub-model of the image-text matching model, and obtaining an image feature representation of the sample image output by the image coding sub-model; calculating a first loss based on the true semantic tag and the predicted semantic tag; calculating a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image; adjusting one or more parameters of the text coding sub-model based on the first loss and the contrast loss; and adjusting one or more parameters of the image coding sub-model based on the contrast loss.</p><p id="p-0011" num="0010">It should be understood that, content described in this section is not intended to identify key or critical features of the embodiments of the present disclosure, nor is it used for limiting the scope of the present disclosure. Other features of the present disclosure will become readily understood through the following description.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTIONS OF THE DRAWINGS</heading><p id="p-0012" num="0011">The accompanying drawings exemplarily illustrate embodiments and constitute a part of the specification, and together with the written description of the specification serve to explain exemplary implementations of the embodiments. The shown embodiments are for illustrative purposes only and do not limit the scope of the claims. Throughout the drawings, the same reference numbers refer to similar but not necessarily identical elements.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a schematic diagram of an example system in which various methods described herein can be implemented according to some embodiments of the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flow chart of a method for training an image-text matching model according to some embodiments of the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a schematic diagram of a principle for training an image-text matching model according to some embodiments of the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates a flow chart of a method for realizing image-text retrieval by utilizing an image-text matching model according to some embodiments of the present disclosure;</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a structural block diagram of an apparatus for training an image-text matching model according to some embodiments of the present disclosure;</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a structural block diagram of an apparatus for realizing image-text retrieval by utilizing an image-text matching model according to some embodiments of the present disclosure; and</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates a structural block diagram of an exemplary electronic device that can be used for realizing some embodiments the present disclosure.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">DETAILED DESCRIPTION</heading><p id="p-0020" num="0019">Embodiments of the present disclosure are described below with reference to the accompanying drawings, which include various details of the embodiments of the present disclosure to facilitate understanding and should be considered as exemplary only. Accordingly, those of ordinary skill in the art should realize that various changes and modifications can be made to the embodiments described herein without departing from the scope of the present disclosure. Also, descriptions of well-known functions and constructions are omitted from the following description for clarity and conciseness.</p><p id="p-0021" num="0020">In the present disclosure, unless otherwise specified, the use of the terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc. to describe various elements is not intended to limit a positional relationship, timing relationship or importance relationship of these elements, and such terms are only used for distinguishing one element from another element. In some examples, a first element and a second element may refer to the same instance of the element, while in some cases they may refer to different instances based on the context of the description.</p><p id="p-0022" num="0021">Terms used in the description of the various described examples in the present disclosure are for the purpose of describing particular examples only and are not intended to be limiting. Unless the context clearly dictates otherwise, if the number of an element is not expressly limited, the element may be one or more. Furthermore, as used in the present disclosure, the term &#x201c;and/or&#x201d; covers any and all possible combinations of listed items.</p><p id="p-0023" num="0022">The embodiments of the present disclosure will be described in detail below with reference to the accompanying drawings.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a schematic diagram of an exemplary system <b>100</b> in which various methods described herein can be implemented according to an embodiment of the present disclosure. With reference to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system <b>100</b> includes one or more client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b> and <b>106</b>, a server <b>120</b>, and one or more communication networks <b>110</b> coupling the one or more client devices to the server <b>120</b>. The client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and <b>106</b> may be configured to execute one or more application programs.</p><p id="p-0025" num="0024">In the embodiment of the present disclosure, the server <b>120</b> may run one or more services or software applications that can execute a method for training an image-text matching model and a method for realizing image-text retrieval by utilizing the image-text matching model.</p><p id="p-0026" num="0025">In some embodiments, the server <b>120</b> may further provide other services or software applications including a non-virtual environment and a virtual environment. In some embodiments, these services may be provided as web-based services or cloud services, for example, be provided for a user of the client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and/or <b>106</b> under a software-as-a-service (SaaS) model.</p><p id="p-0027" num="0026">In a configuration shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the server <b>120</b> may include one or more components that realize a function executed by the server <b>120</b>. These components may include software components executable by one or more processors, hardware components, or a combination thereof. The user operating the client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and/or <b>106</b> may in turn utilize one or more client application programs to interact with the server <b>120</b> so as to utilize services provided by these components. It should be understood that a variety of different system configurations are possible, which may differ from the system <b>100</b>. Accordingly, <figref idref="DRAWINGS">FIG. <b>1</b></figref> is one example of a system for implementing the various methods described herein, and is not intended to be limiting.</p><p id="p-0028" num="0027">The user may use the client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and/or <b>106</b> to input a text to retrieve a corresponding image. The client devices may provide an interface that enables the user of the client devices to interact with the client devices. The client devices may also output information to the user via the interface. Although <figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts only six types of client devices, those of skill in the art will be able to understand that the present disclosure may support any quantity of client devices.</p><p id="p-0029" num="0028">The client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and/or <b>106</b> may include various types of computer devices, such as portable handheld devices, general purpose computers (such as personal computers and laptop computers), workstation computers, wearable devices, smart screen devices, self-service terminal devices, service robots, gaming systems, thin clients, various messaging devices, sensors or other sensing devices, etc. These computer devices may run various types and versions of software application programs and operating systems, such as MICROSOFT Windows, APPLE iOS, UNIX-like operating systems, and Linux or Linux-like operating systems (such as GOOGLE Chrome OS); or include various mobile operating systems, such as MICROSOFT Windows Mobile OS, iOS, Windows Phone, and Android. The portable handheld devices may include cellular phones, smart phones, tablet computers, personal digital assistants (PDAs), and the like. The wearable devices may include head mounted displays (such as smart glasses) and other devices. The gaming systems may include various handheld gaming devices, Internet-enabled gaming devices, and the like. The client devices can execute a variety of different application programs, such as various Internet-related application programs, communication application programs (e.g., e-mail application programs), and Short Message Service (SMS) application programs, and may use various communication protocols.</p><p id="p-0030" num="0029">A network <b>110</b> may be any type of network known to those of skill in the art that may support data communication by using any one of a variety of available protocols (including but not limited to TCP/IP, SNA, IPX, and the like). By way of example only, one or more networks <b>110</b> may be a local area network (LAN), an Ethernet-based network, a token ring, a wide area network (WAN), the Internet, a virtual network, a virtual private network (VPN), an intranet, an extranet, a public switched telephone network (PSTN), an infrared network, a wireless network (e.g., Bluetooth and WIFI) and/or any combination of these and/or other networks.</p><p id="p-0031" num="0030">The server <b>120</b> may include one or more general purpose computers, special purpose server computers (e.g., PC (personal computer) servers, UNIX servers, and midrange servers), blade servers, mainframe computers, server clusters, or any other suitable arrangement and/or combination. The server <b>120</b> may include one or more virtual machines running a virtual operating system, or other computing architecture involving virtualization (e.g., one or more flexible pools of logical storage devices that may be virtualized to maintain the server's virtual storage devices). In various embodiments, the server <b>120</b> may run one or more services or software applications that provide the functions described below.</p><p id="p-0032" num="0031">A computing unit in the server <b>120</b> may run one or more operating systems including any one of the operating systems described above, as well as any commercially available server operating systems. The server <b>120</b> may also run any one of a variety of additional server application programs and/or middle-tier application programs, including HTTP servers, FTP servers, CGI servers, JAVA servers, database servers, etc.</p><p id="p-0033" num="0032">In some embodiments, the server <b>120</b> may include one or more application programs, so as to analyze and consolidate data feeds and/or event updates received from the user of the client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and <b>106</b>. The server <b>120</b> may also include one or more application programs to display the data feeds and/or real-time events via one or more display devices of the client devices <b>101</b>, <b>102</b>, <b>103</b>, <b>104</b>, <b>105</b>, and <b>106</b>.</p><p id="p-0034" num="0033">In some implementation modes, the server <b>120</b> may be a server of a distributed system, or a server combined with a blockchain. The server <b>120</b> may also be a cloud server, or an intelligent cloud computing server or an intelligent cloud host with artificial intelligence technology. The cloud server is a host product in a cloud computing service system to solve the defects of high management difficulty and weak business expansibility in a traditional physical host and virtual private server (VPS) services.</p><p id="p-0035" num="0034">The system <b>100</b> may also include one or more databases <b>130</b>. In some embodiments, these databases may be used for storing data and other information. For example, one or more of the databases <b>130</b> may be used for storing information such as an audio file and an image file. Databases <b>130</b> may be resident in various locations. For example, a data storage library used by the server <b>120</b> may be located locally in the server <b>120</b>, or may be away from the server <b>120</b> and may communicate with the server <b>120</b> based on a network or a dedicated connection. The databases <b>130</b> may be of different types. In some embodiments, the data storage library used by the server <b>120</b> may be a database, for example, a relationship database. One or more of the databases may respond to a command and store, update and retrieve data of the databases and from the databases.</p><p id="p-0036" num="0035">In some embodiments, one or more of the databases <b>130</b> may also be used by an application program to store application program data. The databases used by the application program may be databases of different types, for example, a key-value storage library, an object storage library, or a regular storage library backed by a file system.</p><p id="p-0037" num="0036">The system <b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref> may be configured and operated in various manners, so that it is enabled to apply various methods and apparatus described according to the present disclosure.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a flow chart of a method for training an image-text matching model according to some embodiments of the present disclosure. <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a schematic diagram of a principle of training an image-text matching model according to some embodiments of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref> and <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the image-text matching model includes a text coding sub-model <b>310</b> and an image coding sub-model <b>320</b>. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the method <b>200</b> includes: step S<b>201</b>, a sample text <b>301</b> and a sample image <b>306</b> corresponding to the sample text <b>301</b> are obtained; step S<b>203</b>, a true semantic tag <b>304</b> of the sample text <b>301</b> is labeled according to a first preset rule; step S<b>205</b>, the sample text <b>301</b> is input into the text coding sub-model <b>310</b> so as to obtain text feature representation <b>311</b> of the sample text <b>301</b> and a predicted semantic tag <b>314</b> output by the text coding sub-model <b>310</b>; step S<b>207</b>, the sample image <b>306</b> is input into the image coding sub-model <b>320</b> so as to obtain image feature representation of the sample image output by the image coding sub-model <b>320</b>; step S<b>209</b>, a first loss is calculated based on the true semantic tag <b>304</b> and the predicted semantic tag <b>314</b>; step S<b>211</b>, a contrast loss is calculated based on the text feature representation <b>311</b> of the sample text and the image feature representation of the sample image; step S<b>213</b>, one or more parameters of the text coding sub-model <b>310</b> is adjusted at least based on the first loss and the contrast loss; and step S<b>215</b>, one or more parameters of the image coding sub-model <b>320</b> is adjusted based on the contrast loss. The image-text retrieval model trained by using the method <b>200</b> retains semantic information of the text itself, and learn the relationship between the text and the image better.</p><p id="p-0039" num="0038">The sample image <b>306</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> includes a positive sample image <b>302</b> that is matched with the sample text <b>301</b> and a negative sample image <b>303</b> that is not matched with the sample text <b>301</b>, which enables the model to learn a matching relationship between the sample text <b>301</b> and the positive sample image <b>302</b> as well as a non-matching relationship between the sample text <b>301</b> and the negative sample image <b>303</b>. It can be understood that, the model may be trained only by utilizing the positive sample image <b>302</b>.</p><p id="p-0040" num="0039">According to some embodiments of the present disclosure, at least one of the text coding sub-model <b>310</b> and the image coding sub-model <b>320</b> is a pre-training model.</p><p id="p-0041" num="0040">In some embodiments, a large amount of corpus is used to train a pre-trained model before performing a target task, so that the pre-trained model can obtain semantic and grammatical information, thereby enhancing performance of the target task. In some embodiments, when the text coding sub-model <b>310</b> is a pre-trained model, the text coding sub-model <b>310</b> may be pre-trained through a large amount of corpus, so that the text coding sub-model <b>310</b> can obtain semantic information. In an image-text matching task of the embodiment of the present disclosure, fine-tuning is performed on the image-text matching model based on the first loss and the contrast loss, which can achieve retention of a semantic information perception capability of the text coding sub-model <b>310</b>, and can improve a training speed and performance of the image-text matching model.</p><p id="p-0042" num="0041">According to some embodiments of the present disclosure, the text coding sub-model <b>310</b> may, without limitation to, adopt a coding structure based on transformer and use a [cls] vector to represent overall representation of the text. A process is as follows: v<sub>text</sub>=BERT (S), where s is the input sample text <b>301</b>, BERT is the coding structure used by the text coding sub-model <b>310</b>, and v<sub>text </sub>is the finally obtained text feature representation <b>311</b> of the sample text <b>301</b>.</p><p id="p-0043" num="0042">According to some embodiments of the present disclosure, the sample image <b>306</b> may be a single picture, or the sample image <b>306</b> may include a plurality of video frames of a sample video.</p><p id="p-0044" num="0043">According to some embodiments of the present disclosure, in a video-text (video and text) matching task, the sample image includes a plurality of frames in the sample video, and the plurality of video frames of the sample video may be processed through the pre-trained image coding sub-model <b>320</b> (for example, a resnet model) to obtain a sequence V<sub>video_feature</sub>=Resnet([img 1, img2, . . . , imgn]) of image features of the plurality of video frames, where img1, img2, . . . , imgn represent a sequence of the plurality of video frames. Then a video aggregation model may be used to aggregate the abovementioned sequence of the image features of the plurality of video frames into a vector. A process is as follows: a vector v<sub>video</sub>=Video_Encoder(v<sub>video_feature</sub>). The vector v<sub>video </sub>is the image feature representation of the sample image.</p><p id="p-0045" num="0044">In step S<b>201</b>, the sample text <b>301</b> and the sample image <b>306</b> are obtained. In some embodiments, the obtained sample text <b>301</b> may be, for example, &#x201c;a celebrity A and a celebrity B jointly participated in a program C&#x201d;, and the sample image <b>306</b> may include the plurality of video frames of the sample video. <figref idref="DRAWINGS">FIG. <b>3</b></figref> only exemplarily illustrates one frame in a positive sample video and one frame in a negative sample video.</p><p id="p-0046" num="0045">Step S<b>203</b>, the true semantic tag <b>304</b> of the sample text <b>301</b> is labeled according to the first preset rule.</p><p id="p-0047" num="0046">According to some embodiments of the present disclosure, step S<b>203</b> includes: mask labeling is performed on one or more target words in the sample text <b>301</b> to label each target word as a true semantic tag <b>304</b>. To still take the sample text <b>301</b> &#x201c;a celebrity A and a celebrity B j ointly participated in a program C&#x201d; as an example, mask labeling may be performed on &#x201c;program C&#x201d; in the sample text <b>301</b> &#x201c;a celebrity A and a celebrity B jointly participated in a program C&#x201d; to label &#x201c;program C&#x201d; as a true semantic tag <b>304</b>.</p><p id="p-0048" num="0047">Step S<b>205</b>, the sample text <b>301</b> is input into the text coding sub-model <b>310</b> to obtain the text feature representation <b>311</b> of the sample text <b>301</b> and the predicted semantic tag <b>314</b> output by the text coding sub-model <b>310</b>.</p><p id="p-0049" num="0048">To further take the sample text <b>301</b> &#x201c;a celebrity A and a celebrity B jointly participated in a program C&#x201d; as an example, a process of outputting the predicted semantic tag <b>314</b> is: according to &#x201c;a celebrity A and a celebrity B jointly participated in a XXX&#x201d;, a word corresponding to a position &#x201c;XXX&#x201d; is predicted, where &#x201c;XXX&#x201d; represents a location of mask labeling, and the true semantic tag is known to be &#x201c;program C&#x201d;. The model is trained to make the predicted semantic tag <b>314</b> output by the model to be the true semantic tag <b>304</b>. By introducing semantic information perception training, the semantic information perception ability of the text coding sub-model <b>310</b> may be retained during the image-text matching training process, and will not be weakened by interference of image information, so that the relationship between the text and the image can be better learned.</p><p id="p-0050" num="0049">Step S<b>207</b>, the sample image <b>306</b> is input into the image coding sub-model <b>320</b> so as to obtain the image feature representation of the sample image output by the image coding sub-model <b>320</b>. According to some embodiments of the present disclosure, an image feature v<sub>img </sub>(i.e. the image feature representation of the sample image) may be represented through a feature vector.</p><p id="p-0051" num="0050">Step S<b>209</b>, the first loss is calculated based on the true semantic tag <b>304</b> and the predicted semantic tag <b>314</b>. According to some embodiments of the present disclosure, in the image-text matching model <b>310</b>, the first loss may, for example, be: Loss<sub>1</sub>=&#x2212;log p(t<sub>i</sub>|S), where t<sub>i </sub>is the true semantic tag <b>304</b>, for example, &#x201c;program C&#x201d; in the above example, S is a text masked with the true semantic tag <b>304</b> in the sample text <b>301</b>, for example, &#x201c;a celebrity A and a celebrity B jointly participated in a XXX&#x201d; in the above image-text matching model, and p(t<sub>i</sub>|S) is a probability that the predicted semantic tag <b>314</b> output through the image-text matching model <b>310</b> and the true semantic tag <b>304</b> are the same under a condition that S is known. It can be seen from the formula that, the larger the probability p(t<sub>i</sub>|S) is, the smaller the first loss is. Therefore, by performing parameter adjustment on the image-text matching model <b>310</b> based on the first loss, the ability of the image-text matching model <b>310</b> in perceiving semantic information of the sample text <b>301</b> itself can be enhanced.</p><p id="p-0052" num="0051">Step S<b>211</b>, the contrast loss is calculated based on the text feature representation <b>311</b> of the sample text and the image feature representation of the sample image.</p><p id="p-0053" num="0052">According to some embodiments of the present disclosure, the sample image <b>306</b> includes the positive sample image <b>302</b> that is matched with the sample text <b>301</b> and the negative sample image <b>303</b> that is not matched with the sample text <b>301</b>, and the image feature representation includes positive sample image feature representation <b>312</b> and negative sample image feature representation <b>313</b>. Under this condition, calculating a contrast loss based on the text feature representation <b>311</b> of the sample text <b>301</b> and the image feature representation of the sample image includes: a contrast loss is calculated based on the text feature representation <b>311</b> of the sample text <b>301</b> as well as the positive sample image feature representation <b>312</b> and the negative sample image feature representation <b>313</b> of the sample image.</p><p id="p-0054" num="0053">With further reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, <figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates a schematic diagram of one sample image <b>306</b> according to an embodiment of the present disclosure. The sample image <b>306</b> includes the positive sample image <b>302</b>, and the positive sample image <b>302</b> is a poster picture of a variety show program &#x201c;program C&#x201d; and is an image that has been manually labeled to be matched with the sample text <b>301</b>; and the sample image <b>306</b> further includes the negative sample image <b>303</b>, and the negative sample image <b>303</b> is another picture and is an image that has been manually labeled to be not matched with the sample text <b>301</b>.</p><p id="p-0055" num="0054">According to some embodiments of the present disclosure, a similarity between the sample text <b>301</b> and the sample image may be measured by using cos similarity, so the similarity between the sample text <b>301</b> and the positive sample image <b>302</b> is sim(v<sub>text</sub>, v<sub>img</sub>)=CosSimilarity(v<sub>text</sub>, v<sub>img</sub>), the similarity between the sample text <b>301</b> and the negative sample image <b>303</b> is sim(v<sub>text</sub>, v<sub>&#x12b;mg</sub>)=CosSimilarity(v<sub>text</sub>, v<sub>&#x12b;mg</sub>), where the negative sample image <b>303</b> may be randomly selected, and the contrast loss is Loss<sub>cl_t2i</sub>=Max(|sim(v<sub>text</sub>, v<sub>&#x12b;mg</sub>)&#x2212;sim(v<sub>text</sub>, v<sub>img</sub>)+a|, 0), where a is a preset threshold and a is a positive integer. It can be seen from the formula of the contrast loss that, the model is trained to enable the similarity between the sample text <b>301</b> and the positive sample image <b>302</b> to be as large as possible, and to enable the similarity between the sample text <b>301</b> and the negative sample image <b>303</b> to be as small as possible.</p><p id="p-0056" num="0055">According to other embodiments of the present disclosure, the sample text <b>301</b> includes a positive sample text that is matched with the sample image and a negative sample text that is not matched with the sample image, and the text feature representation <b>311</b>includes positive example text feature representation and negative example text feature representation. Under this condition, calculating a contrast function based on the text feature representation <b>311</b> of the sample text <b>301</b> and the image feature representation of the sample image includes: a contrast function is calculated based on the positive example text feature representation and the negative example text feature representation of the sample text <b>301</b> as well as the image feature representation of the sample image.</p><p id="p-0057" num="0056">According to some embodiments of the present disclosure, a similarity between the sample image and a positive example of the sample text <b>301</b> is sim(v<sub>img</sub>, v<sub>text</sub>)=CosSimilarity(v<sub>img</sub>, v<sub><o ostyle="single">t</o>ext</sub>), a similarity between the sample image and the negative sample text is sim( v<sub>img</sub>, v<sub><o ostyle="single">t</o>ext</sub>)&#x2212;CosSimilarity(v<sub>img</sub>, v<sub><o ostyle="single">t</o>ext</sub>) and the contrast loss is Loss<sub>cl_i2t</sub>=Max(|sim(v<sub>img</sub>, v<sub><o ostyle="single">t</o>ext</sub>)&#x2212;sim(v<sub>img</sub>, v<sub>text</sub>)+a|, 0), where a is the preset threshold and a is a positive integer. It can be seen from the formula of the contrast loss that, the model is trained to enable the similarity between the sample image and the positive sample text to be as larger as possible, and to enable the similarity between the sample image and the negative sample text to be as small as possible.</p><p id="p-0058" num="0057">According to another embodiment of the present disclosure, for a certain pair of sample text <b>301</b> and positive sample image <b>302</b>, the positive sample image <b>302</b> that is matched with the sample text <b>301</b> and the negative sample image <b>303</b> that is not matched with the sample text <b>301</b> may be obtained based on the sample text <b>301</b> so as to obtain the contrast loss Loss<sub>cl_t2i</sub>=Max(|sim(v<sub>text</sub>, v<sub>&#x12b;mg</sub>)&#x2212;sim(v<sub>text</sub>, v<sub>img</sub>)+a|, 0). Meanwhile, the positive sample text that is matched with the sample image and the negative sample text that is not matched with the sample image are obtained based on the positive example image <b>302</b> so as to obtain the contrast loss Loss<sub>cl_i2t</sub>=Max(&#x2212;sim(v<sub>img</sub>, v<sub><o ostyle="single">t</o>ext</sub>)&#x2212;sim (v<sub>img</sub>, v<sub>text</sub>)+a|, 0).</p><p id="p-0059" num="0058">According to some embodiments, the above two technical solutions may be combined to calculate a total contrast loss Loss<sub>cl</sub>=Loss<sub>cl_t2i</sub>+Loss<sub>cl_i2t</sub>, and to perform parameter adjustment based on the total contrast loss. It can be seen from the formulas of the contrast loss that, model training can realize that the similarity of the sample image to the positive sample text is as large as possible while the similarity to the negative sample text is as small as possible and that the similarity of the sample text to the positive sample image <b>302</b> is as large as possible while the similarity to the negative sample image <b>303</b> is as small as possible.</p><p id="p-0060" num="0059">Step S<b>213</b>, the parameter of the text coding sub-model <b>310</b> is adjusted at least based on the first loss and the contrast loss.</p><p id="p-0061" num="0060">Step S<b>215</b>, a parameter of the image coding sub-model <b>320</b> is adjusted based on the contrast loss.</p><p id="p-0062" num="0061">Adjusting the parameter of the text coding sub-model <b>310</b> based on the first loss may enable the text coding sub-model <b>310</b> to retain text semantics. Adjusting the parameter of the text coding sub-model <b>310</b> and the parameter of the image coding sub-model <b>320</b> based on the contrast loss may enable a similarity between the text feature representation <b>311</b> of the sample text output by the text coding sub-model <b>310</b> and the image feature representation <b>312</b> of the positive sample image output by the image coding sub-model <b>320</b> to be larger.</p><p id="p-0063" num="0062">According to some embodiments of the present disclosure, output of the text coding sub-model <b>310</b> further includes a predicted attribute tag <b>315</b>, and the method further includes: a true attribute tag <b>305</b> of the sample text <b>301</b> is labeled according to a second preset rule; and a second loss is calculated based on the true attribute tag <b>305</b> and the predicted attribute tag <b>315</b>. Specifically, adjusting a parameter of the text coding sub-model <b>310</b> at least based on the first loss and the contrast loss includes: a parameter of the text coding sub-model <b>310</b> is adjusted based on the first loss, the second loss and the contrast loss.</p><p id="p-0064" num="0063">According to some embodiments of the present disclosure, labeling a true attribute tag <b>305</b> of the sample text <b>301</b> according to a second preset rule includes: a true attribute tag <b>305</b> of at least one entity word in the sample text <b>301</b> is labeled.</p><p id="p-0065" num="0064">According to some embodiments of the present disclosure, true attribute tags <b>305</b> of a plurality of entity words in the sample text <b>301</b> may also be labeled simultaneously and prediction may be performed on the plurality of true attribute tags <b>305</b>. In this way, through one sample text <b>301</b>, the text coding sub-model <b>310</b> may be enabled to repeatedly obtain attribute information of the entity words themselves, thus further enhancing an effect of the image-text matching model.</p><p id="p-0066" num="0065">For example, with further reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a true attribute tag <b>305</b> of &#x201c;celebrity A&#x201d; in the sample text <b>301</b> &#x201c;a celebrity A and a celebrity B jointly participated in a program C&#x201d; is &#x201c;actor&#x201d;. According to some embodiments of the present disclosure, a process of outputting a predicted attribute tag <b>315</b> is: a career attribute tag of &#x201c;celebrity A&#x201d; is predicted according to &#x201c;a celebrity A and a celebrity B jointly participated in a program C&#x201d;.</p><p id="p-0067" num="0066">In some embodiments the second loss may be but is not limited to Loss<sub>2</sub>=&#x2212;log p(attr<sub>i</sub>|S, t<sub>i</sub>), where attr<sub>i </sub>is a true value of t<sub>i </sub>attribute predicted by the text coding sub-model <b>310</b> after the sample text <b>301</b> S is given, i.e. the predicted attribute tag <b>315</b>. Through adding the second loss, the parameter of the text coding sub-model <b>310</b> is adjusted, which may enable the text coding sub-model <b>310</b> to obtain the attribute information of the words themselves, enhance semantic and grammatical information of the text coding sub-model <b>310</b>, and enable the model to better learn an association relationship between images and texts.</p><p id="p-0068" num="0067">According to another aspect of the present disclosure, a method <b>400</b> for realizing image-text retrieval by utilizing the image-text matching model obtained by training through using the above method <b>200</b> is further provided. The image-text matching model includes a text coding sub-model and an image coding sub-model, as shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. The method <b>400</b> includes: step S<b>401</b>, a to-be-retrieved text is input into the text coding sub-model so as to obtain text feature representation of the to-be-retrieved text output by the text coding sub-model; step S<b>403</b>, a similarity between the to-be-retrieved text and each image in an image database is calculated based on the text feature representation of the to-be-retrieved text and an image feature representation of the each image in the image database, wherein the image feature representation of the each image in the image database is obtained by utilizing the image coding sub-model; and step S<b>405</b>, at least one image that is matched with the to-be-retrieved text is determined from the image database based on the corresponding similarity.</p><p id="p-0069" num="0068">By utilizing the image-text retrieval method in the embodiment of the present disclosure, an effect and accuracy of image-text matching can be improved.</p><p id="p-0070" num="0069">According to another aspect of the present disclosure, an apparatus <b>500</b> for training an image-text matching model is further provided. The image-text matching model includes a text coding sub-model and an image coding sub-model. <figref idref="DRAWINGS">FIG. <b>5</b></figref> illustrates a structural block diagram of an apparatus <b>500</b> for training an image-text matching model according to an embodiment of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the apparatus <b>500</b> includes a first obtaining module <b>501</b>, a semantic labeling module <b>502</b>, a second obtaining module <b>503</b>, a third obtaining module <b>504</b>, a first calculating module <b>505</b>, a contrast loss calculating module <b>506</b>, a first parameter adjusting module <b>507</b>, and a second parameter adjusting module <b>508</b>.</p><p id="p-0071" num="0070">The first obtaining module <b>501</b> is configured to obtain a sample text and a sample image corresponding to the sample text.</p><p id="p-0072" num="0071">The semantic labeling module <b>502</b> is configured to label a true semantic tag for the sample text according to a first preset rule.</p><p id="p-0073" num="0072">The second obtaining module <b>503</b> is configured to input the sample text into the text coding sub-model, and obtain a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model.</p><p id="p-0074" num="0073">The third obtaining module <b>504</b> is configured to input the sample image into the image coding sub-model, and obtain an image feature representation of the sample image output by the image coding sub-model.</p><p id="p-0075" num="0074">The first calculating module <b>505</b> is configured to calculate a first loss based on the true semantic tag and the predicted semantic tag.</p><p id="p-0076" num="0075">The contrast loss calculating module <b>506</b> is configured to calculate a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image.</p><p id="p-0077" num="0076">The first parameter adjusting module <b>507</b> is configured to adjust one or more parameters of the text coding sub-model at least based on the first loss and the contrast loss.</p><p id="p-0078" num="0077">The second parameter adjusting module <b>508</b> is configured to adjust one or more parameters of the image coding sub-model based on the contrast loss.</p><p id="p-0079" num="0078">According to some embodiments of the present disclosure, output of the text coding sub-model further includes a predicted attribute tag, and the apparatus further includes an attribute labeling module <b>509</b> and a second calculating module <b>510</b>.</p><p id="p-0080" num="0079">The attribute labeling module <b>509</b> is configured to label a true attribute tag for the sample text according to a second preset rule.</p><p id="p-0081" num="0080">The second calculating module <b>510</b> is configured to calculate a second loss based on the true attribute tag and the predicted attribute tag, wherein the first parameter adjusting module is configured to adjust the parameters of the text coding sub-model based on the first loss, the second loss and the contrast loss.</p><p id="p-0082" num="0081">According to some embodiments of the present disclosure, the attribute labeling module <b>509</b> is configured to label a true attribute tag of at least one entity word in the sample text.</p><p id="p-0083" num="0082">According to some embodiments of the present disclosure, the semantic labeling module <b>502</b> is configured to perform mask labeling on one or more target words in the sample text so as to label each target word as a true semantic tag, wherein the target word is matched with the sample image.</p><p id="p-0084" num="0083">According to some embodiments of the present disclosure, the sample image includes a positive sample image that is matched with the sample text and a negative sample image that is not matched with the sample text, and the image feature representation includes positive sample image feature representation and negative sample image feature representation, wherein the contrast loss calculating module <b>506</b> is configured to calculate the contrast loss based on the text feature representation of the sample text as well as the positive sample image feature representation and the negative sample image feature representation of the sample image.</p><p id="p-0085" num="0084">According to some embodiments of the present disclosure, the sample text includes a positive sample text that is matched with the sample image and a negative sample text that is not matched with the sample image, and the text feature representation includes text positive example feature representation and text negative example feature representation, wherein the contrast loss calculating module <b>506</b> is configured to calculate the contrast loss based on the text positive example feature representation and the text negative example feature representation of the sample text as well as the image feature representation of the sample image.</p><p id="p-0086" num="0085">According to another aspect of the present disclosure, an apparatus <b>600</b> for realizing image-text retrieval by utilizing an image-text matching model obtained by training through the abovementioned training method is further provided. <figref idref="DRAWINGS">FIG. <b>6</b></figref> illustrates a structural block diagram of an apparatus <b>600</b> for realizing image-text retrieval by utilizing an image-text matching model according to an embodiment of the present disclosure. The image-text matching model includes a text coding sub-model and an image coding sub-model. As shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the apparatus <b>600</b> includes an obtaining module <b>601</b>, a calculating module <b>602</b>, and a determining module <b>603</b>.</p><p id="p-0087" num="0086">The obtaining module <b>601</b> is configured to input a to-be-retrieved text into the text coding sub-model, and obtain text feature representation of the to-be-retrieved text output by the text coding sub-model.</p><p id="p-0088" num="0087">The calculating module <b>602</b> is configured to calculate a similarity between the to-be-retrieved text and each image in an image database based on the text feature representation of the to-be-retrieved text and an image feature representation of each image in the image database, wherein the image feature representation of the each image in the image database is obtained by utilizing the image coding sub-model.</p><p id="p-0089" num="0088">The determining module <b>603</b> is configured to determine at least one image that is matched with the to-be-retrieved text from the image database based on the corresponding similarity.</p><p id="p-0090" num="0089">According to another aspect of the present disclosure, an electronic device is further provided, and includes: at least one processor; and a memory in communication connection with the at least one processor, wherein the memory stores an instruction executable by the at least one processor, and the instruction is executed by the at least one processor so as to enable the at least one processor to execute the abovementioned method.</p><p id="p-0091" num="0090">According to another aspect of the present disclosure, a non-transitory computer readable storage medium storing a computer instruction is further provided, wherein the computer instruction is used for enabling a computer to execute the abovementioned method.</p><p id="p-0092" num="0091">According to another aspect of the present disclosure, a computer program product is further provided, and includes a computer program, wherein the computer program, when executed by a processor, implements the abovementioned method.</p><p id="p-0093" num="0092">In the technical solution of the present disclosure, collection, storage, use, processing, transmission, provision and disclosure of the user's personal information involved are all in compliance with stipulations of relevant laws and regulations, and do not violate public order and good customs.</p><p id="p-0094" num="0093">With reference to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, a structural block diagram of an electronic device <b>700</b> that can serve as a server or a client of the present disclosure will be now described. The electronic device is an example of hardware devices that can be applied to various aspects of the present disclosure. The electronic device is intended to represent various forms of digital electronic computer devices, such as laptop computers, desktop computers, workstations, personal digital assistants, servers, blade servers, mainframe computers, and other suitable computers. The electronic device may also represent various forms of mobile devices, such as personal digital processors, cellular phones, smart phones, wearable devices, and other similar computing devices. Components shown herein, their connections and relationships, and their functions are exemplary only, and are not intended to limit implementations of the present disclosure described and/or claimed herein.</p><p id="p-0095" num="0094">As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the device <b>700</b> includes a computing unit <b>701</b>, which may execute various appropriate actions and processing according to a computer program stored in a read-only memory (ROM) <b>702</b> or a computer program loaded to a random access memory (RAM) <b>703</b> from a storage unit <b>708</b>. In the RAM <b>703</b>, various programs and data needed for operations of the device <b>700</b> may be further stored. The computing unit <b>701</b>, the ROM <b>702</b> and the RAM <b>703</b> are mutually connected through a bus <b>704</b>. An input/output (I/O) interface <b>705</b> is also connected to the bus <b>704</b>.</p><p id="p-0096" num="0095">A plurality of components in the device <b>700</b> are connected to the I/O interface <b>705</b>, including: an input unit <b>706</b>, an output unit <b>707</b>, the storage unit <b>708</b> and a communication unit <b>709</b>. The input unit<b>706</b> may be any type of device capable of inputting information to the device <b>700</b>. The input unit <b>706</b> may receive input number or character information and generate key signal input related to a user setting and/or function control of the electronic device and may include but is not limited to a mouse, a keyboard, a touch screen, a trackpad, a trackball, a joystick, a microphone and/or a remote-control unit. The output unit <b>707</b> may be any type of device capable of displaying information and may include but is not limited to a display, a speaker, a video/audio output terminal, a vibrator and/or a printer. The storage unit <b>708</b> may include but is not limited to a magnetic disk and a compact disc. The communication unit <b>709</b> may allow the device <b>700</b> to exchange information/data with other devices through a computer network, such as the Internet and/or various telecommunication networks, and may include but is not limited to a modem, a network card, an infrared communication device, a wireless communication transceiver and/or a chipset, for example, a Bluetooth&#x2122; device, a 1302.11 device, a WiFi device, a WiMax device, a cellular communication device and/or similar items.</p><p id="p-0097" num="0096">The computing unit <b>701</b> may be various general-purpose and/or special-purpose processing components with processing and computing capacity. Some examples of the computing unit <b>701</b> include but are not limited to a central processing unit (CPU), a graphics processing unit (GPU), various special-purpose artificial intelligence (AI) computing chips, various computing units for running a machine learning model algorithm, a digital signal processor (DSP), and any appropriate processor, controller, microcontroller and the like. The computing unit <b>701</b> executes each method and processing described above, for example, the method <b>200</b> or the method <b>400</b>. For example, in some embodiments, the method <b>200</b> or the method <b>400</b> may be realized as a computer software program, which is tangibly contained in a machine readable medium, for example, the storage unit <b>708</b>. In some embodiments, a part of or all of the computer program may be loaded and/or installed onto the device <b>700</b> via the ROM <b>702</b> and/or the communication unit <b>709</b>. When the computer program is loaded to the RAM <b>703</b> and is executed by the computing unit <b>701</b>, one or more steps of the method <b>200</b> or the method <b>400</b> described above can be executed. Alternatively, in other embodiments, the computing unit <b>701</b> may be configured to execute the method <b>200</b> or the method <b>400</b> in any other appropriate mode (for example, by means of firmware).</p><p id="p-0098" num="0097">Various implementations of the systems and technologies described above in this paper may be implemented in a digital electronic circuit system, an integrated circuit system, a field programmable gate array (FPGA), an application specific integrated circuit (ASIC), an application specific standard part (ASSP), a system on chip (SOC), a load programmable logic device (CPLD), computer hardware, firmware, software and/or their combinations. These various implementations may include: being implemented in one or more computer programs, wherein the one or more computer programs may be executed and/or interpreted on a programmable system including at least one programmable processor, and the programmable processor may be a special-purpose or general-purpose programmable processor, and may receive data and instructions from a storage system, at least one input apparatus, and at least one output apparatus, and transmit the data and the instructions to the storage system, the at least one input apparatus, and the at least one output apparatus.</p><p id="p-0099" num="0098">Program codes for implementing the methods of the present disclosure may be written in any combination of one or more programming languages. These program codes may be provided to processors or controllers of a general-purpose computer, a special-purpose computer or other programmable data processing apparatuses, so that when executed by the processors or controllers, the program codes enable the functions/operations specified in the flow diagrams and/or block diagrams to be implemented. The program codes may be executed completely on a machine, partially on the machine, partially on the machine and partially on a remote machine as a separate software package, or completely on the remote machine or server.</p><p id="p-0100" num="0099">In the context of the present disclosure, a machine readable medium may be a tangible medium that may contain or store a program for use by or in connection with an instruction execution system, apparatus or device. The machine readable medium may be a machine readable signal medium or a machine readable storage medium. The machine readable medium may include but not limited to an electronic, magnetic, optical, electromagnetic, infrared, or semiconductor system, apparatus or device, or any suitable combination of the above contents. More specific examples of the machine readable storage medium will include electrical connections based on one or more lines, a portable computer disk, a hard disk, a random access memory (RAM), a read only memory (ROM), an erasable programmable read only memory (EPROM or flash memory), an optical fiber, a portable compact disk read only memory (CD-ROM), an optical storage device, a magnetic storage device, or any suitable combination of the above contents.</p><p id="p-0101" num="0100">In order to provide interactions with users, the systems and techniques described herein may be implemented on a computer, and the computer has: a display apparatus for displaying information to the users (e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor); and a keyboard and a pointing device (e.g., a mouse or trackball), through which the users may provide input to the computer. Other types of apparatuses may further be used to provide interactions with users; for example, feedback provided to the users may be any form of sensory feedback (e.g., visual feedback, auditory feedback, or tactile feedback); an input from the users may be received in any form (including acoustic input, voice input or tactile input).</p><p id="p-0102" num="0101">The systems and techniques described herein may be implemented in a computing system including background components (e.g., as a data server), or a computing system including middleware components (e.g., an application server) or a computing system including front-end components (e.g., a user computer with a graphical user interface or a web browser through which a user may interact with the implementations of the systems and technologies described herein), or a computing system including any combination of such background components, middleware components, or front-end components. The components of the system may be interconnected by digital data communication (e.g., a communication network) in any form or medium. Examples of the communication network include: a local area network (LAN), a wide area network (WAN) and the Internet.</p><p id="p-0103" num="0102">A computer system may include a client and a server. The client and the server are generally away from each other and usually interact through a communication network. A relation between the client and the server is generated by running a computer program with a mutual client-server relation on a corresponding computer. The server may be a cloud server, or a server of a distributed system, or a server combined with a blockchain.</p><p id="p-0104" num="0103">It should be understood that steps can be reranked, added or deleted by using various forms of flows shown above. For example, all the steps recorded in the present disclosure can be executed in parallel, or in sequence or in different orders. As long as a desired result of the technical solutions disclosed by the present disclosure can be realized, no limitation is made herein.</p><p id="p-0105" num="0104">Though the embodiments or the examples of the present disclosure are already described with reference to the drawings, it should be understood that the above method, system or device is only an exemplary embodiment or example, and the scope of present disclosure is not limited by these embodiments or examples but is limited only by the scope of the authorized claims and their equivalents. Various elements in the embodiments or the examples may be omitted or replaced by their equivalent elements. Besides, all the steps may be executed in sequence different from a sequence described in the present disclosure. Furthermore, various elements in the embodiments or the examples may be combined in various modes. What counts is that with technology evolution, many elements described here can be replaced by equivalent elements appearing after the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented method, comprising:<claim-text>obtaining a sample text and a sample image corresponding to the sample text;</claim-text><claim-text>labeling a true semantic tag for the sample text according to a first preset rule;</claim-text><claim-text>inputting the sample text into a text coding sub-model of an image-text matching model, and obtaining a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model;</claim-text><claim-text>inputting the sample image into an image coding sub-model of the image-text matching model, and obtaining an image feature representation of the sample image output by the image coding sub-model;</claim-text><claim-text>calculating a first loss based on the true semantic tag and the predicted semantic tag;</claim-text><claim-text>calculating a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image;</claim-text><claim-text>adjusting one or more parameters of the text coding sub-model based on the first loss and the contrast loss; and</claim-text><claim-text>adjusting one or more parameters of the image coding sub-model based on the contrast loss.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein an output of the text coding sub-model further comprises a predicted attribute tag, the method further comprising:<claim-text>labeling a true attribute tag for the sample text according to a second preset rule; and</claim-text><claim-text>calculating a second loss based on the true attribute tag and the predicted attribute tag;</claim-text><claim-text>wherein the adjusting the one or more parameters of the text coding sub-model based on the first loss and the contrast loss comprises:<claim-text>adjusting the one or more parameters of the text coding sub-model based on the first loss, the second loss, and the contrast loss.</claim-text></claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method according to <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the labeling the true attribute tag of the sample text according to the second preset rule comprises:<claim-text>labeling a corresponding true attribute tag for each entity word of one or more entity words in the sample text.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein at least one of the text coding sub-model or the image coding sub-model is a pre-trained model.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the labeling the true semantic tag for the sample text according to the first preset rule comprises:<claim-text>performing mask labeling on one or more target words in the sample text and labeling each target word of the one or more target words as a true semantic tag.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sample image comprises a positive sample image that is matched with the sample text and a negative sample image that is not matched with the sample text, and the image feature representation comprises a positive sample image feature representation and a negative sample image feature representation;<claim-text>wherein the calculating the contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image comprises:<claim-text>calculating the contrast loss based on the text feature representation of the sample text, the positive sample image feature representation, and the negative sample image feature representation of the sample image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method according to <claim-ref idref="CLM-00001">claims 1</claim-ref>, wherein the sample text comprises a positive sample text that is matched with the sample image and a negative sample text that is not matched with the sample image, and the text feature representation comprises a positive sample text feature representation and a negative sample text feature representation;<claim-text>wherein the calculating the contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image comprises:<claim-text>calculating the contrast loss based on the positive sample text feature representation, the negative sample text feature representation of the sample text, and the image feature representation of the sample image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the sample image is a single picture, or the sample image comprises a plurality of video frames of a sample video.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image-text matching model that is trained is used to perform operations comprising:<claim-text>inputting a to-be-retrieved text into the text coding sub-model, and obtaining a text feature representation of the to-be-retrieved text output by the text coding sub-model;</claim-text><claim-text>calculating a similarity between the to-be-retrieved text and each image in an image database based on the text feature representation of the to-be-retrieved text and an image feature representation of the each image in the image database, wherein the image feature representation of the each image in the image database is obtained by utilizing the image coding sub-model; and</claim-text><claim-text>determining at least one image that is matched with the to-be-retrieved text from the image database based on the corresponding similarity.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A computing device, comprising:<claim-text>one or more processors; and</claim-text><claim-text>a memory storing one or more programs configured to be executed by the one or more processors, the one or more programs comprising instructions for performing operations comprising:<claim-text>obtaining a sample text and a sample image corresponding to the sample text;</claim-text><claim-text>labeling a true semantic tag for the sample text according to a first preset rule;</claim-text><claim-text>inputting the sample text into a text coding sub-model of an image-text matching model, and obtaining a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model;</claim-text><claim-text>inputting the sample image into an image coding sub-model of the image-text matching model, and obtaining an image feature representation of the sample image output by the image coding sub-model;</claim-text><claim-text>calculating a first loss based on the true semantic tag and the predicted semantic tag;</claim-text><claim-text>calculating a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image;</claim-text><claim-text>adjusting one or more parameters of the text coding sub-model based on the first loss and the contrast loss; and</claim-text><claim-text>adjusting one or more parameters of the image coding sub-model based on the contrast loss.</claim-text></claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein an output of the text coding sub-model further comprises a predicted attribute tag, and the operations comprising: labeling a true attribute tag for the sample text according to a second preset rule; and<claim-text>calculating a second loss based on the true attribute tag and the predicted attribute tag; and</claim-text><claim-text>wherein the adjusting the one or more parameters of the text coding sub-model at least based on the first loss and the contrast loss comprises:<claim-text>adjusting the one or more parameters of the text coding sub-model based on the first loss, the second loss, and the contrast loss.</claim-text></claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computing device according to <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the labeling the true attribute tag of the sample text according to the second preset rule comprises:<claim-text>labeling a corresponding true attribute tag for each entity word of one or more entity words in the sample text.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein at least one of the text coding sub-model or the image coding sub-model is a pre-trained model.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the labeling the true semantic tag for the sample text according to the first preset rule comprises:<claim-text>performing mask labeling on one or more target words in the sample text and labeling each target word of the one or more target words as a true semantic tag.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the sample image comprises a positive sample image that is matched with the sample text and a negative sample image that is not matched with the sample text, and the image feature representation comprises a positive sample image feature representation and a negative sample image feature representation;<claim-text>wherein the calculating the contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image comprises:<claim-text>calculating the contrast loss based on the text feature representation of the sample text, the positive sample image feature representation, and the negative sample image feature representation of the sample image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the sample text comprises a positive sample text that is matched with the sample image and a negative sample text that is not matched with the sample image, and the text feature representation comprises a positive sample text feature representation and a negative sample text feature representation;<claim-text>wherein the calculating the contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image comprises:<claim-text>calculating the contrast loss based on the positive sample text feature representation, the negative sample text feature representation of the sample text, and the image feature representation of the sample image.</claim-text></claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the sample image is a single picture, or the sample image comprises a plurality of video frames of a sample video.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The computing device according to <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the image-text matching model that is trained is used to perform operations comprising:<claim-text>inputting a to-be-retrieved text into the text coding sub-model, and obtaining a text feature representation of the to-be-retrieved text output by the text coding sub-model;</claim-text><claim-text>calculating a similarity between the to-be-retrieved text and each image in an image database based on the text feature representation of the to-be-retrieved text and an image feature representation of the each image in the image library, wherein the image feature representation of the each image in the image database is obtained by utilizing the image coding sub-model; and</claim-text><claim-text>determining at least one image that is matched with the to-be-retrieved text from the image database based on the corresponding similarity.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable storage medium storing one or more programs comprising instructions that, when executed by one or more processors of a computing device, cause the computing device to perform operations comprising:<claim-text>obtaining a sample text and a sample image corresponding to the sample text;</claim-text><claim-text>labeling a true semantic tag for the sample text according to a first preset rule;</claim-text><claim-text>inputting the sample text into a text coding sub-model of an image-text matching model, and obtaining a text feature representation of the sample text and a predicted semantic tag output by the text coding sub-model;</claim-text><claim-text>inputting the sample image into an image coding sub-model of the image-text matching model, and obtaining an image feature representation of the sample image output by the image coding sub-model;</claim-text><claim-text>calculating a first loss based on the true semantic tag and the predicted semantic tag;</claim-text><claim-text>calculating a contrast loss based on the text feature representation of the sample text and the image feature representation of the sample image;</claim-text><claim-text>adjusting one or more parameters of the text coding sub-model based on the first loss and the contrast loss; and</claim-text><claim-text>adjusting one or more parameters of the image coding sub-model based on the contrast loss.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The computer-readable storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein output of the text coding sub-model further comprises a predicted attribute tag, the operations further comprising:<claim-text>labeling a true attribute tag for the sample text according to a second preset rule; and</claim-text><claim-text>calculating a second loss based on the true attribute tag and the predicted attribute tag; and</claim-text><claim-text>wherein the adjusting the one or more parameters of the text coding sub-model at least based on the first loss and the contrast loss comprises:<claim-text>adjusting the one or more parameters of the text coding sub-model based on the first loss, the second loss, and the contrast loss.</claim-text></claim-text></claim-text></claim></claims></us-patent-application>