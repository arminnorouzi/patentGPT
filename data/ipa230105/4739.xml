<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004740A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004740</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364806</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00369</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>11</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HUMAN LYING POSTURE DETECTION METHOD AND MOBILE   MACHINE USING THE SAME</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UBTECH NORTH AMERICA RESEARCH AND DEVELOPMENT CENTER CORP</orgname><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UBTECH ROBOTICS CORP LTD</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Dong</last-name><first-name>Chuqiao</first-name><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Shao</last-name><first-name>Dan</first-name><address><city>San Gabriel</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Xiu</last-name><first-name>Zhen</first-name><address><city>Chino Hills</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Guo</last-name><first-name>Dejun</first-name><address><city>San Gabriel</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Tan</last-name><first-name>Huan</first-name><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Human lying posture detections are disclosed. A human lying on a bed is detected by obtaining an image through a depth camera, detecting objects in the image and marking the objects in the image using 2D bounding boxes by deep learning, determining the human being in a lying posture in response to a width and a height of the 2D bounding box of the human meeting a predetermined condition, detecting one or more skin areas in the image and generating skin area 2D bounding boxes to mark each of the one or more skin areas using a skin detection algorithm, and determining the human being in the lying posture in response to the skin area 2D bounding boxes and the 2D bounding box of the bed meeting a predetermined positional relationship.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="191.94mm" wi="111.25mm" file="US20230004740A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="194.39mm" wi="124.12mm" file="US20230004740A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="186.69mm" wi="110.91mm" orientation="landscape" file="US20230004740A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="167.13mm" wi="145.46mm" file="US20230004740A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="202.95mm" wi="133.01mm" file="US20230004740A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="183.39mm" wi="141.05mm" file="US20230004740A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="183.39mm" wi="141.05mm" file="US20230004740A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="180.26mm" wi="86.70mm" file="US20230004740A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="201.85mm" wi="113.28mm" file="US20230004740A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="125.73mm" wi="110.66mm" file="US20230004740A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="193.89mm" wi="150.54mm" file="US20230004740A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="215.31mm" wi="134.28mm" orientation="landscape" file="US20230004740A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">1. Technical Field</heading><p id="p-0002" num="0001">The present disclosure relates to human lying posture detections, and particularly to a human lying posture detection method and a mobile machine using the same.</p><heading id="h-0003" level="1">2. Description of Related Art</heading><p id="p-0003" num="0002">In healthcare industry, with the help of flourishing artificial intelligence (AI) techniques, robots already have many kinds of applications including mobility aid robots and medical robots. The mobility aid robots are often designed as devices like wheelchairs or walkers to assist walking and otherwise so as to improve the mobility of people with a mobility impairment.</p><p id="p-0004" num="0003">For realizing the functions like walking aids, a mobility aid robot is inevitable to have automatic navigation capability so as to assist the user in a more automatic and convenient way. And for realizing automatic navigation, it needs to detect its user for path planning, and even need to detect the user's posture so as to serve in a more appropriate manner accordingly.</p><p id="p-0005" num="0004">Skeleton-based posture detection is a popular technique for realizing human posture detection in robots, which detects the posture of a human according to the identified key points on an estimated skeleton of the human. In the case that there have enough identified key points, it will be effective and can detect accurately; otherwise, in the case that the key points can be identified are not enough because, for example, the body of the human is highly occluded by obstacles or cloths, it may even impossible to be realized. Especially when the human lies down in a bed and covered with a quilt, the quilt and even the bed may occlude the body and affect the effect of detection. Therefore, a method to detect human lying posture which is independent with the skeleton-based posture detection is need.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005">In order to more clearly illustrate the technical solutions in this embodiment, the drawings used in the embodiments or the description of the prior art will be briefly introduced below. In the drawing(s), like reference numerals designate corresponding parts throughout the figures. It should be understood that, the drawings in the following description are only examples of the present disclosure. For those skilled in the art, other drawings can be obtained based on these drawings without creative works.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of a scenario of detecting human posture using a mobile machine according to some embodiments of the present disclosure.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram of using a camera of the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> to detect the posture of a human.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic block diagram illustrating the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart of a human lying posture detection method according to some embodiments of the present disclosure.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of marking a human in an image captured by the camera of the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of marking a human and a bed in an image captured by the camera of the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a flow chart of an example of detecting skin areas in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>68</b></figref> is a schematic diagram of detecting skin areas in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a flow chart of an example of using skeleton-based posture detection in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a schematic diagram of a result of skeleton based posture detection corresponding to a lying human in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b>C</figref> is a schematic diagram of a result of skeleton based posture detection corresponding to a sitting human in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0018" num="0017">In order to make the objects, features and advantages of the present disclosure more obvious and easy to understand, the technical solutions in this embodiment will be clearly and completely described below with reference to the drawings. Apparently, the described embodiments are part of the embodiments of the present disclosure, not all of the embodiments. All other embodiments obtained by those skilled in the art based on the embodiments of the present disclosure without creative efforts are within the scope of the present disclosure.</p><p id="p-0019" num="0018">It is to be understood that, when used in the description and the appended claims of the present disclosure, the terms &#x201c;including&#x201d;, &#x201c;comprising&#x201d;, &#x201c;having&#x201d; and their variations indicate the presence of stated features, integers, steps, operations, elements and/or components, but do not preclude the presence or addition of one or a plurality of other features, integers, steps, operations, elements, components and/or combinations thereof.</p><p id="p-0020" num="0019">It is also to be understood that, the terminology used in the description of the present disclosure is only for the purpose of describing particular embodiments and is not intended to limit the present disclosure. As used in the description and the appended claims of the present disclosure, the singular forms &#x201c;one&#x201d;. &#x201c;a&#x201d;, and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise.</p><p id="p-0021" num="0020">It is also to be further understood that the term &#x201c;and/or&#x201d; used in the description and the appended claims of the present disclosure refers to any combination of one or more of the associated listed items and all possible combinations, and includes such combinations.</p><p id="p-0022" num="0021">In the present disclosure, the terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, and &#x201c;third&#x201d; are for descriptive purposes only, and are not to be comprehended as indicating or implying the relative importance or implicitly indicating the amount of technical features indicated. Thus, the feature limited by &#x201c;first&#x201d;, &#x201c;second&#x201d;, and &#x201c;third&#x201d; may include at least one of the feature either explicitly or implicitly. In the description of the present disclosure, the meaning of &#x201c;a plurality&#x201d; is at least two, for example, two, three, and the like, unless specifically defined otherwise.</p><p id="p-0023" num="0022">In the present disclosure, the descriptions of &#x201c;one embodiment&#x201d;, &#x201c;some embodiments&#x201d; or the like described in the specification mean that one or more embodiments of the present disclosure can include particular features, structures, or characteristics which are related to the descriptions of the descripted embodiments. Therefore, the sentences &#x201c;in one embodiment&#x201d;. &#x201c;in some embodiments&#x201d;, &#x201c;in other embodiments&#x201d;, &#x201c;in other embodiments&#x201d; and the like that appear in different places of the specification do not mean that descripted embodiments should be referred by all other embodiments, but instead be referred by &#x201c;one or more but not all other embodiments&#x201d; unless otherwise specifically emphasized.</p><p id="p-0024" num="0023">The present disclosure relates to mobile machine navigating. As used herein, the term &#x201c;human&#x201d; refers to the most populous and widespread species of primates in the earth. A human has a body including a head, a neck, a trunk, arms, hands, legs and feet. The term &#x201c;posture&#x201d; refers to a human position such as standing, sitting, and lying, and the term &#x201c;lying posture&#x201d; refers to a kind of human position in which the body is supported along its length by the surface underneath. The term &#x201c;bed&#x201d; refers to an object, which may be raised and allows a human to rest on. A bed may include typical objects such as pillows and blankets on there. The bed may also be, for example, a couch, a bench, and hammock. The term &#x201c;detection&#x201d; refers to a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (e.g., humans and furniture) in digital images. The term &#x201c;mobile machine&#x201d; refers to a machine such as a mobile robot or a vehicle that has the capability to move around in its environment. The term &#x201c;trajectory planning&#x201d; refers to find a sequence of valid configurations that moves a mobile machine from the source to the destination and is parametrized by time, where &#x201c;trajectory&#x201d; denotes a sequence of poses with time stamp (cf. &#x201c;path&#x201d; denotes a sequence of poses or position without time stamp). The term &#x201c;pose&#x201d; refers to position (e.g., x and y coordinates on x and y axes) and posture (e.g., a yaw angle along z axis). The term &#x201c;navigation&#x201d; refers to the process of monitoring and controlling the movement of a mobile robot from one place to another, and the term &#x201c;collision avoidance&#x201d; refers to prevent or reduce the severity of a collision. The term &#x201c;sensor&#x201d; refers to a device, module, machine, or subsystem such as ambient light sensor and image sensor (e.g., camera) whose purpose is to detect events or changes in its environment and send the information to other electronics (e.g., processor).</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a schematic diagram of a scenario of detecting human posture using a mobile machine <b>100</b> according to some embodiments of the present disclosure; and <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a schematic diagram of using a camera C of the mobile machine <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> to detect the posture of a human. As shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> and <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the mobile machine <b>100</b> that is navigated in its environment (e.g., a room) detects the posture of the human, that is, a user U on a bed C. The bed O may be a furniture (e.g., a bench). The mobile machine <b>100</b> is a mobile robot (e.g., a mobility aid robot), which includes the camera C and wheels E. The camera C may be disposed toward a forward direction D<sub>f </sub>which the mobile machine <b>100</b> straightly moves such that lens of the camera C straightly face toward the forward direction D<sub>f</sub>. The camera C has a camera coordinate system, and the coordinates of the mobile machine <b>100</b> are consistent with the coordinates of the camera C. In the camera coordinate system, the x-axis is consistent with the forward direction D<sub>f</sub>, the y-axis is parallel to the horizon, and the z-axis is perpendicular to the horizon. A field of view V of the camera C covers both the user U and the bed O. The height (e.g., 1 meter) of the camera C on the mobile machine <b>100</b> may be changed according to actual needs (e.g., larger height to have the larger field of view V and smaller height to have the smaller field of view V), and the pitch angle of the camera C with respect to the floor may also be changed according to actual needs (e.g., larger pitch angle to have the nearer field of view V and smaller pitch angle to have the farer field of view V). Based on the height and the pitch angle of the camera C, a relative position of the user U near the mobile machine <b>100</b> can be obtained, and whether the user U is lying down or the bed O at which the user U is lying can also be determined. It should be noted that, the mobile machine <b>100</b> is only one example of mobile machine, and the mobile machine <b>100</b> may have more, fewer, or different parts than shown in above or below (e.g., have legs rather than the wheels E), or may have a different configuration or arrangement of the parts (e.g., have the camera C dispose on the top of the mobile machine <b>100</b>). In other embodiments, the mobility machine <b>100</b> may be another kind of mobile machine such as a vehicle.</p><p id="p-0026" num="0025">In some embodiments, the mobile machine <b>100</b> is navigated in the environment while dangerous situations such as collisions and unsafe conditions (e.g., falling, extreme temperature, radiation, and exposure) may be prevented. In this indoor navigation, the mobile machine <b>100</b> is navigated from a starting point (e.g., the location where the mobile machine <b>100</b> originally locates) to a destination (e.g., the location of the goal of navigation which is indicated by the user U or the navigation/operation system of the mobile machine <b>100</b>), and obstacles (e.g., walls, furniture, humans, pets, and garbage) may be avoided so as to prevent the above-mentioned dangerous situations. The trajectory (e.g., trajectory T) for the mobile machine <b>100</b> to move from the starting point to the destination has to be planned so as to move the mobile machine <b>100</b> according to the trajectory. The trajectory includes a sequence of poses (e.g., poses S<sub>n-1</sub>-S<sub>n </sub>of trajectory T). It should be noted that, the starting point and the destination only refer to the locations of the mobile machine <b>100</b>, rather than the real beginning and end of the trajectory (the real beginning and end of a trajectory should be a pose, respectively). In some embodiments, for realizing the navigation of the mobile machine <b>100</b>, the map for the environment has to be built, the current position of the mobile machine <b>100</b> in the environment may have to be determined (using, for example, the IMU <b>1331</b>), and trajectories may be planned based on the built map and the determined current position of the mobile machine <b>100</b>. The desired pose S<sub>d </sub>is the last of the sequence of poses S in a trajectory T (only shown partially in the figure), that is, the end of the trajectory T. The trajectory T is planned according to, for example, a shortest path in the built map to the user U. In addition, the collision avoidance to obstacles in the built map (e.g., walls and furniture) or that detected in real time (e.g., humans and pets) may also be considered when planning, so as to accurately and safely navigate the mobile machine <b>100</b>.</p><p id="p-0027" num="0026">In some embodiments, the navigation of the mobile machine <b>100</b> may be actuated through the mobile machine <b>100</b> itself (e.g., a control interface on the mobile machine <b>100</b>) or a control device such as a remote control, a smart phone, a tablet computer, a notebook computer, a desktop computer, or other electronic device by, for example, providing a request for the navigation of the mobile machine <b>100</b>. The mobile machine <b>100</b> and the control device may communicate over a network which may include, for example, the Internet, intranet, extranet, local area network (LAN), wide area network (WAN), wired network, wireless networks (e.g., Wi-Fi network, Bluetooth network, and mobile network), or other suitable networks, or any combination of two or more such networks.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic block diagram illustrating the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. The mobile machine <b>100</b> may include a processing unit <b>110</b>, a storage unit <b>120</b>, and a control unit <b>130</b> that communicate over one or more communication buses or signal lines L. It should be noted that, the mobile machine <b>100</b> is only one example of mobile machine, and the mobile machine <b>100</b> may have more or fewer components (e.g., unit, subunits, and modules) than shown in above or below, may combine two or more components, or may have a different configuration or arrangement of the components. The processing unit <b>110</b> executes various (sets ot) instructions stored in the storage unit <b>120</b> that may be in form of software programs to perform various functions for the mobile machine <b>100</b> and to process related data, which may include one or more processors (e.g., CPU). The storage unit <b>120</b> may include one or more memories (e.g., high-speed random access memory (RAM) and non-transitory memory), one or more memory controllers, and one or more non-transitory computer readable storage media (e.g., solid-state drive (SSD) or hard disk drive). The control unit <b>130</b> may include various controllers (e.g., camera controller, display controller, and physical button controller) and peripherals interface for coupling the input and output peripheral of the mobile machine <b>100</b>, for example, external port (e.g., USB), wireless communication circuit (e.g., RF communication circuit), audio circuit (e.g., speaker circuit), sensor (e.g., inertial measurement unit (IMU)), and the like, to the processing unit <b>110</b> and the storage unit <b>120</b>. In some embodiments, the storage unit <b>120</b> may include a navigation module <b>121</b> for implementing navigation functions (e.g., map building and trajectory planning) related to the navigation (and trajectory planning) of the mobile machine <b>100</b>, which may be stored in the one or more memories (and the one or more non-transitory computer readable storage media).</p><p id="p-0029" num="0028">The navigation module <b>121</b> in the storage unit <b>120</b> of the mobile machine <b>100</b> may be a software module (of the operation system of the mobile machine <b>100</b>), which has instructions I<sub>n </sub>(e.g., instructions for actuating motor(s) <b>1321</b> of the wheels E of the mobile machine <b>100</b> to move the mobile machine <b>100</b>) for implementing the navigation of the mobile machine <b>100</b>, a map builder <b>1211</b>, and trajectory planner(s) <b>1212</b>. The map builder <b>1211</b> may be a software module having instructions I<sub>b </sub>for building map for the mobile machine <b>100</b>. The trajectory planner(s) <b>1212</b> may be software module(s) having instructions I<sub>p </sub>for planning trajectories for the mobile machine <b>100</b>. The trajectory planner(s) <b>1212</b> may include a global trajectory planner for planning global trajectories (e.g., trajectory T) for the mobile machine <b>100</b> and a local trajectory planner for planning local trajectories (e.g., the part of the trajectory T in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>) for the mobile machine <b>100</b>. The global trajectory planner may be, for example, a trajectory planner based on Dijkstra's algorithm, which plans global trajectories based on map(s) built by the map builder <b>1211</b> through, for example, simultaneous localization and mapping (SLAM). The local trajectory planner may be, for example, a trajectory planner based on TEB (timed elastic band) algorithm, which plans local trajectories based on the global trajectory P<sub>g</sub>, and other data collected by the mobile machine <b>100</b>. For example, images may be collected through the camera C of the mobile machine <b>100</b>, and the collected images may be analyzed so as to identify obstacles, so that the local trajectory can be planned with reference to the identified obstacles, and the obstacles can be avoided by moving the mobile machine <b>100</b> according to the planned local trajectory.</p><p id="p-0030" num="0029">Each of the map builder <b>1211</b> and the trajectory planner(s) <b>1212</b> may be a submodule separated from the instructions I<sub>n </sub>or other submodules of the navigation module <b>121</b>, or a part of the instructions I<sub>n </sub>for implementing the navigation of the mobile machine <b>100</b>. The trajectory planner(s) <b>1212</b> may further have data (e.g., input/output data and temporary data) related to the trajectory planning of the mobile machine <b>100</b> which may be stored in the one or more memories and accessed by the processing unit <b>110</b>. In some embodiments, each of the trajectory planner(s) <b>1212</b> may be a module in the storage unit <b>120</b> that is separated from the navigation module <b>121</b>.</p><p id="p-0031" num="0030">In some embodiments, the instructions I<sub>n </sub>may include instructions for implementing collision avoidance of the mobile machine <b>100</b> (e.g., obstacle detection and trajectory replanning). In addition, the global trajectory planner may replan the global trajectory(s) (i.e., plan new global trajectory(s)) to detour in response to, for example, the original global trajectory(s) being blocked (e.g., blocked by an unexpected obstacle) or inadequate for collision avoidance (e.g., impossible to avoid a detected obstacle when adopted). In other embodiments, the navigation module <b>121</b> may be a navigation unit communicating with the processing unit <b>110</b>, the storage unit <b>120</b>, and the control unit <b>130</b> over the one or more communication buses or signal lines L and may further include one or more memories (e.g., high-speed random access memory (RAM) and non-transitory memory) for storing the instructions I<sub>n</sub>, the map builder <b>1211</b>, and the trajectory planner(s) <b>1212</b>, and one or more processors (e.g., MPU and MCU) for executing the stored instructions I<sub>n</sub>, I<sub>b </sub>and I<sub>p </sub>to implement the navigation of the mobile machine <b>100</b>.</p><p id="p-0032" num="0031">The mobile machine <b>100</b> may further include a communication subunit <b>131</b> and an actuation subunit <b>132</b>. The communication subunit <b>131</b> and the actuation subunit <b>132</b> communicate with the control unit <b>130</b> over one or more communication buses or signal lines that may be the same or at least partially different from the above-mentioned one or more communication buses or signal lines L. The communication subunit <b>131</b> is coupled to communication interfaces of the mobile machine <b>100</b>, for example, network interface(s) <b>1311</b> for the mobile machine <b>100</b> to communicate with the control device via the network, I/O interface(s) <b>1312</b> (e.g., a physical button), and the like. The actuation subunit <b>132</b> is coupled to component(s)/device(s) for implementing the motions of the mobile machine <b>100</b> by, for example, actuating motor(s) <b>1321</b> of the wheels E and/or joints of the mobile machine <b>100</b>. The communication subunit <b>131</b> may include controllers for the above-mentioned communication interfaces of the mobile machine <b>100</b>, and the actuation subunit <b>132</b> may include controller(s) for the above-mentioned component(s)/device(s) for implementing the motions of the mobile machine <b>100</b>. In other embodiments, the communication subunit <b>131</b> and/or actuation subunit <b>132</b> may just abstract component for representing the logical relationships between the components of the mobile machine <b>100</b>.</p><p id="p-0033" num="0032">The mobile machine <b>100</b> may further include a sensor subunit <b>133</b> which may include a set of sensor(s) and related controller(s), for example, the camera C, and an IMU <b>1331</b> (or an accelerometer and a gyroscope), for detecting the environment in which it is located to realize its navigation. The camera C is a depth camera such as an RGB-D camera. The sensor subunit <b>133</b> communicates with the control unit <b>130</b> over one or more communication buses or signal lines that may be the same or at least partially different from the above-mentioned one or more communication buses or signal lines L. In other embodiments, in the case that the navigation module <b>121</b> is the above-mentioned navigation unit, the sensor subunit <b>133</b> may communicate with the navigation unit over one or more communication buses or signal lines that may be the same or at least partially different from the above-mentioned one or more communication buses or signal lines L. In addition, the sensor subunit <b>133</b> may just abstract component for representing the logical relationships between the components of the mobile machine <b>100</b>.</p><p id="p-0034" num="0033">In some embodiments, the map builder <b>1211</b>, the trajectory planner(s) <b>1212</b>, the sensor subunit <b>133</b>, and the motor(s) <b>1321</b> (and the wheels E and/or joints of the mobile machine <b>100</b> coupled to the motor(s) <b>1321</b>) jointly compose a (navigation) system which implements map building, (global and local) trajectory planning, and motor actuating so as to realize the navigation of the mobile machine <b>100</b>. In addition, the various components shown in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> may be implemented in hardware, software or a combination of both hardware and software. Two or more of the processing unit <b>110</b>, the storage unit <b>120</b>, the control unit <b>130</b>, the navigation module <b>121</b>, and other units/subunits/modules may be implemented on a single chip or a circuit. In other embodiments, at least a part of them may be implemented on separate chips or circuits.</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow chart of a human lying posture detection method according to some embodiments of the present disclosure. In some embodiments, the human lying posture detection method maybe implemented in the mobile machine <b>100</b> by, for example, storing (sets of) the instructions I<sub>n </sub>corresponding to the human lying posture detection method as the navigation module <b>121</b> in the storage unit <b>120</b> and executing the stored instructions I<sub>n </sub>through the processing unit <b>110</b>, and then the mobile machine <b>100</b> can detect using the camera C so as to determine whether the user U is lying down or not. The human lying posture detection method may be performed in response to, for example, a request for detecting the posture of the user U from, for example, (the navigation/operation system of) the mobile machine <b>100</b> itself or the control device, then it may also be re-performed, for example, in every predetermined time interval (e.g., 1 second) to re-determine whether the user U is lying down or not, thereby detecting the change of the posture of the user U. Accordingly, at step <b>3110</b>, image(s) I may be obtained through the camera C. Since the camera C is a depth camera (e.g., an RGB-D camera), the image(s) I captured by the camera C include pixel values that represent distance. A plurality of images I may be obtained so as to select one image I (e.g., the image I that meets a certain quality) for use.</p><p id="p-0036" num="0035">At step <b>3120</b>, objects (e.g., a human and a bed) in the image I are detected and marked using 2D bounding boxes by deep learning. A 2D (two dimensional) bounding box (BBox) B<sub>1 </sub>of human is generated to mark the human, that is, the user U. and a 2D bounding box B<sub>2 </sub>of bed is generated to mark the bed O. The 2D bounding box B<sub>1 </sub>is output by a deep learning model. The deep learning model may be a computer model based on, for example, YOLO (you only look once) algorithm, which may be trained using the labeled data with respect to the detection of human. <figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic diagram of marking a human (i.e., the user U) in the image I captured by the camera C of the mobile machine <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. In the image I of the upper part of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the bed O and the user U and lying thereon are included. In the image I of the lower part of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the 2D bounding box B<sub>1 </sub>is a rectangular area in the image I that is shown as a rectangular box with a dashed line frame to mark the user U. Since the entirety of the body (i.e., a head, a neck, a trunk, two arms, two hands, two legs and two feet) of the user U is shown in the image I, the 2D bounding box B<sub>1 </sub>marks the entirety of the body of the user U in the image I. In other embodiments, the 2D bounding box B<sub>1 </sub>may be an area of another shape (e.g., an irregular shape which fits the user U) in the image I that is shown as a box of another shape (e.g., an irregular shape which fits the user U) with a frame of other form (e.g., solid line frame). The 2D bounding box B<sub>2 </sub>for bed may be output by the above-mentioned deep learning model that is further trained based on the algorithm of YOLO using the labeled data with respect to the detection of bed. In other embodiment, the 2D bounding box B<sub>2 </sub>for bed may be output by another deep learning model. <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of marking a human (i.e., the user U) and a bed (i.e., the bed O) in the image I captured by the camera C of the mobile machine <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. In the image I of the upper part of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, since a quilt Q is covered on the body of the user U, only a part of the body (i.e., the head, the neck, a part of the trunk, the two arms, and the two hands) of the user U is shown in the image I, and the 2D bounding box B<sub>2 </sub>of bed only marks the part of the body of the user U in the image I.</p><p id="p-0037" num="0036">At step <b>3130</b>, a determination is made whether or not there is a user U in the image I. The user U in the image I is detected using the above-mentioned deep learning model. The deep learning model is trained by using a large set of labeled data with respect to the detection of human (e.g., a data set of more than 10,000 images of humans in various scenes), and neural network architectures that contain a plurality of layers, so as to learn to perform classification tasks directly from the inputted image I, thereby detecting the user U in the image I. If it is determined that there is a user U in the image I, step <b>3140</b> will be performed; otherwise, step <b>3160</b> will be performed.</p><p id="p-0038" num="0037">At step <b>3140</b>, a determination is made whether or not a width of the 2D bounding box B<sub>1 </sub>and a height of the 2D bounding box B<sub>1 </sub>meet the predetermined condition. The predetermined condition may be that the ratio of the width to the height greater than a predetermined ratio (e.g., 5:1), which represents that the width is greater than the height for more than predetermined times (e.g., 5 times). If it is determined that the width and the height of the 2D bounding box B<sub>1 </sub>meet the predetermined condition (i.e., the width of the 2D bounding box B<sub>1 </sub>is greater than the height of the 2D bounding box B<sub>1 </sub>for more than the predetermined times), step <b>3150</b> will be performed; otherwise, step <b>3160</b> will be performed. For instance, in the case that the predetermined condition is the ratio of 5:1, in the image I of the lower part of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, because the width W<sub>1 </sub>of the 2D bounding box B<sub>1 </sub>is greater than the height H<sub>1 </sub>of the 2D bounding box B<sub>1 </sub>for more than 5 times, it will determine that the width and the height of the 2D bounding box B<sub>1 </sub>meet the predetermined condition. As shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, because the width W<sub>2 </sub>of the 2D bounding box B<sub>2 </sub>of bed is greater than the height H<sub>2 </sub>of the 2D bounding box B<sub>2 </sub>for less than 5 times, it will determine the width W<sub>2 </sub>and the height H<sub>2 </sub>of the second 2D bounding box B<sub>2 </sub>not meet the predetermined condition, hence step <b>3160</b> will be performed. In some embodiments, the width of the first 2D bounding box B<sub>1 </sub>is represented by an eigen vector of the first 2D bounding box B<sub>1 </sub>in a horizontal direction DX, and the height of the first 2D bounding box B<sub>1 </sub>is represented by another eigen vector of the first 2D bounding box B<sub>1 </sub>in a vertical direction D<sub>v</sub>. At step <b>3150</b>, the human (i.e., the user U) is determined as in a lying posture, which means that the user U is lying down.</p><p id="p-0039" num="0038">At step <b>3160</b>, a determination is made whether or not there is a bed O in the image I. The bed O in the image I is detected using the above-mentioned deep learning model. The deep learning model is trained by using a large set of labeled data with respect to the detection of bed (e.g., a data set of more than 10,000 images of beds in various scenes), and neural network architectures that contain a plurality of layers, so as to learn to perform classification tasks directly from the inputted image I, thereby detecting the bed O in the image I. If it is determined that there is a bed O in the image I, step <b>3170</b> will be performed; otherwise, step <b>3200</b> will be performed. In the image I of the lower part of <figref idref="DRAWINGS">FIG. <b>5</b></figref>, although the entirety of the bed O (i.e., a seat, a back, and legs) of the user U is shown in the image I, for detecting the lying posture of the user U that may be on the bed O in the subsequent step(s) of the human lying posture detection method, the 2D bounding box B<sub>2 </sub>may only mark an upper part (i.e., the seat and the back) of the bed O in the image I. The 2D bounding box B<sub>2 </sub>is a rectangular area in the image I that is shown as a rectangular box with a dashed line frame to mark the user U. In other embodiments, the 2D bounding box B<sub>2 </sub>may be an area of another shape (e.g., an irregular shape which fits the bed O) in the image I that is shown as a box of the shape with a frame of other form (e.g., solid line frame).</p><p id="p-0040" num="0039">At step <b>3170</b>, a determination is made whether or not there are skin areas A in the image I. The skin areas A may be detected from all pixels of the image I. The skin areas A in the image I are detected using a skin detection algorithm which may be a CV (computer vision) algorithm, and in-house developed architectures that contain a plurality of layers, so as to learn to perform classification tasks directly from the inputted image I, thereby detecting the skin areas A in the image I. <figref idref="DRAWINGS">FIG. <b>6</b>A</figref> is a flow chart of an example of detecting the skin areas A in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>; and <figref idref="DRAWINGS">FIG. <b>6</b>B</figref> is a schematic diagram of detecting the skin areas A in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. Steps <b>3171</b>-<b>3174</b> implements the above-mentioned skin detection algorithm. Accordingly, at step <b>3171</b>, a texture-based segmentation is performed on all the pixels within the image to obtain textural segments S<sub>t</sub>. The texture-based segmentation may be performed based on the threshold of pixels value in the HSV (hue, saturation, value) color map for human skin. In the texture-based segmentation T of the upper part of <figref idref="DRAWINGS">FIG. <b>613</b></figref>, each textural segment S<sub>t </sub>represents an area of the potential pixels for human skin. At step <b>3172</b>, a region-based segmentation is performed on the textural segments S<sub>t </sub>to obtain confident segments S<sub>c</sub>. The confident segments S<sub>e </sub>of human skin is obtained by calculating the similarity between segment textural segment S<sub>t</sub>. The confident segments S<sub>c </sub>are generally smaller than that the textural segments S<sub>t</sub>. In the region-based segmentation R of the middle part of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, the confident segments S<sub>c </sub>are smaller than that the corresponding textural segments S<sub>t</sub>. At step <b>3173</b>, the skin areas A is obtained by growing the confident segments S<sub>c</sub>. The skin area A may be obtained by taking the corresponding confident segment S<sub>c </sub>as the seed and applying a watershed algorithm to grow the confident segments S<sub>c </sub>into the skin area A. At step <b>3174</b>, a determination is made whether or not there are skin areas A in the image I. It may determine that there are skin areas A in the image I if at least two skin areas A have been obtained. If it is determined that there are skin areas A in the image I, step <b>3180</b> will be performed; otherwise, step <b>3200</b> will be performed.</p><p id="p-0041" num="0040">At step <b>3180</b>, skin area 2D bounding boxes B<sub>3 </sub>are generated using the above-mentioned skin detection algorithm so as to mark the skin areas A. In the image I of the lower part of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, the skin area 2D bounding box B<sub>3 </sub>is a rectangular area in the image I that is shown as a rectangular box with a dashed line frame to mark the user U. In other embodiments, the skin area 2D bounding boxes B<sub>1 </sub>may be an area of another shape (e.g., an irregular shape which fits the skin area A) in the image I that is shown as a box of the shape with a frame of other form (e.g., solid line frame). At step <b>3190</b>, a determination is made whether or not the skin area 2D bounding boxes B<sub>3 </sub>corresponding to the skin areas A and the 2D bounding box B<sub>2 </sub>meet a predetermined positional relationship. In some embodiments, the predetermined positional relationship is that at least a part of each skin area 2D bounding box B<sub>3 </sub>corresponding to all the skin areas A is within the 2D bounding box B<sub>2</sub>, that is, all the skin area 2D bounding boxes B<sub>3 </sub>are within the 2D bounding box B<sub>2</sub>, or each of the skin area 2D bounding boxes B<sub>3 </sub>is entirely or has a part within the 2D bounding box B<sub>2</sub>, which covers the case that a part of the body (e.g., the head, the arm, the hand, the leg, or the foot) of the user U strengths beyond the edge of the bed O.</p><p id="p-0042" num="0041">If it is determined that the skin area 2D bounding boxes B<sub>3 </sub>corresponding to the skin areas A and the 2D bounding box B<sub>2 </sub>meet the predetermined positional relationship, step <b>3150</b> will be performed; otherwise, step <b>3200</b> will be performed. For instance, in the image I of the lower part of <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, because the entirety of the skin area 2D bounding boxes B<sub>3 </sub>corresponding to the skin areas A of the hands of the user U and a part of the skin area 2D bounding boxes B<sub>3 </sub>corresponding to the skin areas A of the face of the user U are within the 2D bounding box B<sub>2</sub>, it will be determined that the skin area 2D bounding boxes B<sub>3 </sub>corresponding to the skin areas A and the 2D bounding box B<sub>2 </sub>meet the predetermined positional relationship, and step <b>3150</b> will be performed. At step <b>3200</b>, the human (i.e., the user U) is determined as not in the lying posture. In some embodiments, in the human lying posture detection method, a time-window may be added for filtering out the invalid results to realize a more accurate and robust detection by, for example, determining the user U as in the lying posture after a plurality of adjacent frames (i.e., the images I) within the time-window are obtained and the user U in all the images I are all determined as in the lying posture. It should be noted that the size of the time-window can be defined according to actual needs (e.g., the preference of the user).</p><p id="p-0043" num="0042">In some embodiments, skeleton-based posture detection may also be used in the human lying posture detection method, which can provide a more accurate lying posture detection when enough key points can be detected, and may further used to provide a more comprehensive human posture detection (e.g., providing a human posture detection that further detects standing posture and sitting posture of the user U in addition to lying posture of the user U). <figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a flow chart of an example of using skeleton-based posture detection in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>3</b></figref>; <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a schematic diagram of a result of skeleton based posture detection corresponding to a lying human in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>; and <figref idref="DRAWINGS">FIG. <b>7</b>C</figref> is a schematic diagram of a result of skeleton based posture detection corresponding to a sitting human in the human lying posture detection method of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>. Step <b>3211</b> and step <b>3212</b> that are for performing skeleton-based posture detection may be performed after it is determined that there is a human in the image I (step <b>3130</b>).</p><p id="p-0044" num="0043">Accordingly, at step <b>3211</b>, key points P on a body of the human (i.e., the user U) are identified to obtain positions of the key points P on an estimated skeleton B of the human. In the image I of the upper part of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, since the entirety of the body of the user U who is lying down is shown in the image I, the positions of the key points P on the estimated skeleton B of the entirety of the body of the user U are obtained. In the image I of the lower part of <figref idref="DRAWINGS">FIG. <b>71</b>B</figref>, since only a part of the body (i.e., the head, the neck, a part of the trunk, the two arms and the two hands) of the user U who is lying down and covered by the quilt Q is shown in the image I, only the positions of the key points P on the estimated skeleton B of the part of the body of the user U are obtained. In the image I of <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, since the entirety of the body of the user U who is sitting is shown in the image I, the positions of the key points P on the estimated skeleton B of the entirety of the body of the user U are obtained. At step <b>3212</b>, a determination is made whether or not an inclination angle &#x3b8; (e.g., the angle &#x3b8;<sub>1 </sub>in the image I of the upper part of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> and the angle &#x3b8;<sub>2 </sub>in the image I of <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>) of an upper part of the body (i.e., the upper body angle) of the human (i.e., the user U) is larger than a predetermined angle. The upper part of the body may be the trunk. The predetermined angle (e.g., 60&#xb0;) is the smallest inclination angle &#x3b8; of the upper part of the body when the user U is in the lying posture. The inclination angle &#x3b8; of the upper part of the body of the user U is determined based on the positions of the key points P on the estimated skeleton B of the human by, for example, estimating an axis X (e.g., the axis X<sub>1 </sub>in the image I of the upper part of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref> and the axis X<sub>2 </sub>in the image I of <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>) of the upper part of the body based on the positions of the key points P corresponding to the neck and the hip of the body and taking an included angle between the axis X and the z axis of the coordinate system of the camera C as the inclination angle &#x3b8;. If it is determined that the inclination angle &#x3b8; of the upper part of the body of the user U is larger than the predetermined angle, step <b>3150</b> will be performed; otherwise, step <b>3160</b> or step <b>3200</b> will be performed. For instance, in the case that the predetermined angle is 60&#x2032;, in the image I of the upper part of <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, because the angle &#x3b8;<sub>1 </sub>of the upper part of the body is larger than 60&#xb0;, step <b>3150</b> will be performed; and in the image I of <figref idref="DRAWINGS">FIG. <b>7</b>C</figref>, because the angle &#x3b8;<sub>2 </sub>(which is 0&#xb0; because of overlapping with the z axis of the coordinate system of the camera C) of the upper part of the body is smaller than 60&#xb0;, step <b>3160</b> will be performed.</p><p id="p-0045" num="0044">The human lying posture detection method combines the detection of furniture and human skin, which is capable of detecting the lying posture of the user U in the case that the key points can be identified are not enough. The human lying posture detection method can be realized in a real-time manner while only a few computation resources are need, and is economic and efficient because only a depth camera rather than a plurality of sensors are need for detection. In the case that the mobile machine <b>100</b> is a mobility aid robot, it can realize the human lying posture detection method to detect the lying posture of the user U and choose a suitable way to interact with the user U accordingly. For example, when the user U is an old person who is detected as having lied down in bed, the mobile machine <b>100</b> can ask the user U to sit first before providing further aids.</p><p id="p-0046" num="0045">It can be understood by those skilled in the art that, all or part of the method in the above-mentioned embodiment(s) can be implemented by one or more computer programs to instruct related hardware. In addition, the one or more programs can be stored in a non-transitory computer readable storage medium. When the one or more programs are executed, all or part of the corresponding method in the above-mentioned embodiment(s) is performed. Any reference to a storage, a memory, a database or other medium may include non-transitory and/or transitory memory. Non-transitory memory may include read only memory (ROM), programmable ROM (PROM), electrically programmable ROM (EPROM), electrically erasable programmable ROM (EEPROM), flash memory, solid-state drive (SSD), or the like. Volatile memory may include random access memory (RAM), external cache memory, or the like.</p><p id="p-0047" num="0046">The processing unit <b>110</b> (and the above-mentioned processor) may include central processing unit (CPU), or be other general purpose processor, digital signal processor (DSP), application specific integrated circuit (ASIC), field-programmable gate array (FPGA), or be other programmable logic device, discrete gate, transistor logic device, and discrete hardware component. The general purpose processor may be microprocessor, or the processor may also be any conventional processor. The storage unit <b>120</b> (and the above-mentioned memory) may include internal storage unit such as hard disk and internal memory. The storage unit <b>120</b> may also include external storage device such as plug-in hard disk, smart media card (SMC), secure digital (SD) card, and flash card.</p><p id="p-0048" num="0047">The exemplificative units/modules and methods/steps described in the embodiments may be implemented through software, hardware, or a combination of software and hardware. Whether these functions are implemented through software or hardware depends on the specific application and design constraints of the technical schemes. The above-mentioned human lying posture detection method and mobile machine <b>100</b> may be implemented in other manners. For example, the division of units/modules is merely a logical functional division, and other division manner may be used in actual implementations, that is, multiple units/modules may be combined or be integrated into another system, or some of the features may be ignored or not performed. In addition, the above-mentioned mutual coupling/connection may be direct coupling/connection or communication connection, and may also be indirect coupling/connection or communication connection through some interfaces/devices, and may also be electrical, mechanical or in other forms.</p><p id="p-0049" num="0048">The above-mentioned embodiments are merely intended for describing but not for limiting the technical schemes of the present disclosure. Although the present disclosure is described in detail with reference to the above-mentioned embodiments, the technical schemes in each of the above-mentioned embodiments may still be modified, or some of the technical features may be equivalently replaced, so that these modifications or replacements do not make the essence of the corresponding technical schemes depart from the spirit and scope of the technical schemes of each of the embodiments of the present disclosure, and should be included within the scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computerized detection method, comprising:<claim-text>obtaining, through a depth camera, at least an image;</claim-text><claim-text>detecting objects in the image, and marking the objects in the image using 2D bounding boxes, by deep learning;</claim-text><claim-text>in response to one of the objects in the image being detected as a human, determining the human being in a lying posture in response to a width and a height of the 2D bounding box of the human meeting a predetermined condition;</claim-text><claim-text>in response to one of the objects in the image not being detected as the human or the width and height of the 2D bounding box of the human not meeting the predetermined condition, and another one of the objects in the image being detected as a bed, detecting one or more skin areas in the image and generating skin area 2D bounding boxes to mark each of the one or more skin areas using a skin detection algorithm; and</claim-text><claim-text>determining the human being in the lying posture in response to the skin area 2D bounding boxes and the 2D bounding box of the bed meeting a predetermined positional relationship.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the detecting the one or more skin areas in the image and generating skin area 2D bounding boxes to mark each of the one or more skin areas using the skin detection algorithm comprises:<claim-text>performing a texture-based segmentation on all the pixels within the image to obtain one or more textural segments;</claim-text><claim-text>performing a region-based segmentation on the one or more textural segments to obtain one or more confident segments;</claim-text><claim-text>obtaining the one or more skin areas by growing the one or more confident segments; and</claim-text><claim-text>generating the skin area 2D bounding box to mark each of the one or more skin areas using the skin detection algorithm.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the predetermined positional relationship comprises at least a part of each of the skin area 2D bounding boxes corresponding to all the one or more skin areas being within the 2D bounding box of the bed.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the width of the 2D bounding box of the human is represented by an eigen vector of the 2D bounding box of the human in a horizontal direction, and the height of the 2D bounding box of the human is represented by another eigen vector of the 2D bounding box of the human in a vertical direction.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, before the determining the human being in the lying posture, further comprising:<claim-text>in response to one of the objects in the image being detected as the human, identifying key points on a body of the human to obtain positions of the key points on an estimated skeleton of the human;</claim-text><claim-text>determining an inclination angle of an upper part of the body of the human determined based on the positions of the key points on the estimated skeleton of the human; and</claim-text><claim-text>determining the human as in the lying posture in response to the inclination angle of the upper part of the body of the human being larger than a predetermined angle; and</claim-text><claim-text>in response to the inclination angle of the upper part of the body of the human being not larger than the predetermined angle, determining the human as in the lying posture in response to the width and the height of the 2D bounding box of the human meeting the predetermined condition.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, comprising:<claim-text>in response to none of the objects in the image being detected as the bed, having not detected the one or more skin areas, or the skin area 2D bounding box corresponding to the one or more skin areas and the 2D bounding box of the bed not meeting the predetermined positional relationship, determining the human as not in the lying posture.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the bed is one of a bed, a couch, a bench, and hammock.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A mobile machine, comprising:<claim-text>a depth camera;</claim-text><claim-text>one or more processors; and</claim-text><claim-text>a memory storing one or more programs configured to be executed by the one or more processors, wherein the one or more programs include instructions to:</claim-text><claim-text>obtain, through the depth camera, at least an image;</claim-text><claim-text>detect objects in the image, and mark the objects in the image using 2D bounding boxes, by deep learning;</claim-text><claim-text>in response to one of the objects in the image being detected as a human, determine the human being in a lying posture in response to a width and a height of the 2D bounding box of the human meeting a predetermined condition;</claim-text><claim-text>in response to one of the objects in the image not being detected as the human or the width and height of the 2D bounding box of the human not meeting the predetermined condition, and another one of the objects in the image being detected as a bed, detect one or more skin areas in the image and generate skin area 2D bounding boxes to mark each of the one or more skin areas using a skin detection algorithm; and</claim-text><claim-text>determine the human being in the lying posture in response to the skin area 2D bounding boxes and the 2D bounding box of the bed meeting a predetermined positional relationship.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The mobile machine of <claim-ref idref="CLM-00008">claim 8</claim-ref>, the detecting the one or more skin areas in the image and generating skin area 2D bounding boxes to mark each of the one or more skin areas using the skin detection algorithm comprises:<claim-text>performing a texture-based segmentation on all the pixels within the image to obtain one or more textural segments;</claim-text><claim-text>performing a region-based segmentation on the one or more textural segments to obtain one or more confident segments;</claim-text><claim-text>obtaining the one or more skin areas by growing the one or more confident segments; and</claim-text><claim-text>generating the skin area 2D bounding box to mark each of the one or more skin areas using the skin detection algorithm.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The mobile machine of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the predetermined positional relationship comprises at least a part of each of the skin area 2D bounding boxes corresponding to all the one or more skin areas being within the 2D bounding box of the bed.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The mobile machine of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the width of the 2D bounding box of the human is represented by an eigen vector of the 2D bounding box of the human in a horizontal direction, and the height of the 2D bounding box of the human is represented by another eigen vector of the 2D bounding box of the human in a vertical direction.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The mobile machine of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more programs further include instructions to:<claim-text>in response to one of the objects in the image being detected as the human, identify key points on a body of the human to obtain positions of the key points on an estimated skeleton of the human;</claim-text><claim-text>determine an inclination angle of an upper part of the body of the human determined based on the positions of the key points on the estimated skeleton of the human; and</claim-text><claim-text>determine the human as in the lying posture in response to the inclination angle of the upper part of the body of the human being larger than a predetermined angle; and</claim-text><claim-text>in response to the inclination angle of the upper part of the body of the human being not larger than the predetermined angle, determine the human as in the lying posture in response to the width and the height of the 2D bounding box of the human meeting the predetermined condition.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The mobile machine of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the one or more programs further include instructions to:<claim-text>in response to none of the objects in the image being detected as the bed, having not detected the one or more skin areas, or the skin area 2D bounding box corresponding to the one or more skin areas and the 2D bounding box of the bed not meeting the predetermined positional relationship, determine the human as not in the lying posture.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The mobile machine of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the bed is one of a bed, a couch, a bench, and hammock.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer readable storage medium storing one or more programs, wherein the one or more programs comprise instructions, which when executed by a mobile machine having a depth camera, cause the mobile machine to:<claim-text>obtain, through the depth camera, at least an image;</claim-text><claim-text>detect objects in the image, and mark the objects in the image using 2D bounding boxes, by deep learning;</claim-text><claim-text>in response to one of the objects in the image being detected as a human, determine the human being in a lying posture in response to a width and a height of the 2D bounding box of the human meeting a predetermined condition;</claim-text><claim-text>in response to one of the objects in the image not being detected as the human or the width and height of the 2D bounding box of the human not meeting the predetermined condition, and another one of the objects in the image being detected as a bed, detect one or more skin areas in the image and generate skin area 2D bounding boxes to mark each of the one or more skin areas using a skin detection algorithm; and</claim-text><claim-text>determine the human being in the lying posture in response to the skin area 2D bounding boxes and the 2D bounding box of the bed meeting a predetermined positional relationship.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, the detecting the one or more skin areas in the image and generating skin area 2D bounding boxes to mark each of the one or more skin areas using the skin detection algorithm comprises:<claim-text>performing a texture-based segmentation on all the pixels within the image to obtain one or more textural segments;</claim-text><claim-text>performing a region-based segmentation on the one or more textural segments to obtain one or more confident segments;</claim-text><claim-text>obtaining the one or more skin areas by growing the one or more confident segments; and</claim-text><claim-text>generating the skin area 2D bounding box to mark each of the one or more skin areas using the skin detection algorithm.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the predetermined positional relationship comprises at least a part of each of the skin area 2D bounding boxes corresponding to all the one or more skin areas being within the 2D bounding box of the bed.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the width of the 2D bounding box of the human is represented by an eigen vector of the 2D bounding box of the human in a horizontal direction, and the height of the 2D bounding box of the human is represented by another eigen vector of the 2D bounding box of the human in a vertical direction.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more programs further comprise instructions causing the mobile machine to:<claim-text>in response to one of the objects in the image being detected as the human, identify key points on a body of the human to obtain positions of the key points on an estimated skeleton of the human;</claim-text><claim-text>determine an inclination angle of an upper part of the body of the human determined based on the positions of the key points on the estimated skeleton of the human; and</claim-text><claim-text>determine the human as in the lying posture in response to the inclination angle of the upper part of the body of the human being larger than a predetermined angle; and</claim-text><claim-text>in response to the inclination angle of the upper part of the body of the human being not larger than the predetermined angle, determine the human as in the lying posture in response to the width and the height of the 2D bounding box of the human meeting the predetermined condition.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the one or more programs further comprise instructions causing the mobile machine to:<claim-text>in response to none of the objects in the image being detected as the bed, having not detected the one or more skin areas, or the skin area 2D bounding box corresponding to the one or more skin areas and the 2D bounding box of the bed not meeting the predetermined positional relationship, determine the human as not in the lying posture.</claim-text></claim-text></claim></claims></us-patent-application>