<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005114A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005114</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17771007</doc-number><date>20201013</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>KR</country><doc-number>10-2019-0130542</doc-number><date>20191021</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>77</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>50</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>3</main-group><subgroup>4053</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>002</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>5</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>7715</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20081</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20076</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20084</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30201</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20212</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE RESTORATION METHOD AND APPARATUS</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>PIXTREE Inc.</orgname><address><city>Seoul</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SHIN</last-name><first-name>Jaeseob</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>RYOO</last-name><first-name>Sungul</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>SON</last-name><first-name>Sehoon</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Hyeongduck</first-name><address><city>Suwon-gu, Gyeonggi-do</city><country>KR</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>KIM</last-name><first-name>Hyosong</first-name><address><city>Seoul</city><country>KR</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/KR2020/013937</doc-number><date>20201013</date></document-id><us-371c12-date><date>20220421</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present embodiment provides an image restoration method and apparatus which generate independent different restoration models by performing learning for each of different resolutions, receive a distorted image, and apply a restoration model corresponding to the resolution of the distorted image among the independent different restoration models to restore the distorted image into an improved upscaled image centering on a restoration target object within the distorted image.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="69.17mm" wi="148.67mm" file="US20230005114A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="175.68mm" wi="150.71mm" file="US20230005114A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="206.84mm" wi="155.19mm" file="US20230005114A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="228.60mm" wi="160.78mm" file="US20230005114A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="233.93mm" wi="155.87mm" file="US20230005114A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure relates to an image restoration method and apparatus.</p><heading id="h-0002" level="1">BACKGROUND ART</heading><p id="p-0003" num="0002">The content described below merely provides background information related to the present embodiment and does not constitute the conventional art.</p><p id="p-0004" num="0003">In general, a technology for restoring a low-resolution image to a high-resolution image is divided depending on the number of input images used for restoration or a restoration technology. It is divided into a single image super-resolution image restoration technology and a continuous image super-resolution image restoration technology depending on the number of input images.</p><p id="p-0005" num="0004">In general, the single image super-resolution image restoration technology has a faster processing speed than the continuous image super-resolution image restoration technology, but the quality of image restoration is low because information required for restoration is insufficient.</p><p id="p-0006" num="0005">Since the continuous image super-resolution image restoration technology uses various features extracted from a plurality of consecutively acquired images, the quality of the restored image is superior to that of the single image super-resolution image restoration technology, but the algorithm is complex and the amount of computation is large so that real-time processing is difficult.</p><p id="p-0007" num="0006">Depending on the restoration technology, there are a technique using an interpolation method, a technique using edge information, a technique using frequency characteristics, and a technique using machine learning such as deep learning. The technique using the interpolation method has a fast processing speed, but has a disadvantage in that the edge portions are blurred.</p><p id="p-0008" num="0007">The technique using edge information not only has a high speed, but also can restore an image while maintaining the sharpness of edges, but has a disadvantage in that it may include a visually noticeable restoration error when the edge direction is incorrectly estimated.</p><p id="p-0009" num="0008">The technique using the frequency characteristics can restore the image while maintaining the sharpness of the edges as in the technique using edge information by using the high frequency component, but has a disadvantage in that ringing artifacts near the boundary line occur. Finally, the technique using machine learning such as example-based or deep learning has the most excellent quality of restored images, but its processing speed is very slow.</p><p id="p-0010" num="0009">As described above, the continuous image super-resolution image restoration technology among the various existing high-resolution image restoration technologies may be applied to fields that require a digital zoom function using the existing interpolation method, and provides superior quality of images compared to an interpolation method-based image restoration technology. However, in the existing super-resolution image restoration technology, the technology applicable to electro-optical equipment requiring limited resources and real-time processing is limited due to the complex amount of computation.</p><p id="p-0011" num="0010">The existing single image super-resolution image restoration technology capable of real-time processing has a problem in that performance is greatly deteriorated compared to the continuous image super-resolution image restoration technology when image magnification is required to a high magnification of 2 times or more.</p><heading id="h-0003" level="1">DISCLOSURE</heading><heading id="h-0004" level="1">Technical Problem</heading><p id="p-0012" num="0011">An object of the present embodiment is to provide an image restoration method and apparatus which generate independent different restoration models by performing learning for each of different resolutions, receive a distorted image, and apply a restoration model corresponding to the resolution of the distorted image among the independent different restoration models to restore the distorted image into an improved upscaled image centering on a restoration target object within the distorted image.</p><heading id="h-0005" level="1">Technical Solution</heading><p id="p-0013" num="0012">According to an aspect of the present embodiment, there is provided an image restoration apparatus characterized by including a receiver for receiving an image and a processor for processing the image through convolution operation and non-linearization, wherein the processor includes: a learning unit that receives a plurality of images having different resolutions and performs learning for each of the different resolutions to learn an independent restoration model for each resolution; a selection unit that, when receiving a distorted observed image y, analyzes the resolution of the distorted observed image y to select a suitable restoration model; and an image restoration unit that generates a restored image x{circumflex over (&#x2003;)} in which the observed image is restored by using a restoration model corresponding to the resolution of the observed image y among independent restoration models that are different for each resolution.</p><p id="p-0014" num="0013">According to another aspect of the present embodiment, there is provided an image restoration method characterized by comprising: a learning process of receiving a plurality of images having different resolutions and performing learning for each of the different resolutions to learn an independent restoration model for each resolution; a selection process of selecting a suitable restoration model by analyzing the resolution of the distorted observed image y when the distorted observed image y is received; and an image restoration process of generating a restored image x{circumflex over (&#x2003;)} in which the observed image is restored by using a restoration model corresponding to the resolution of the observed image y among independent restoration models that are different for each resolution.</p><heading id="h-0006" level="1">Advantageous Effects</heading><p id="p-0015" num="0014">According to the present embodiment as described above, there is an effect capable of restoring the distorted image into an improved upscaled image centering on a restoration target object within the distorted image by performing learning for each of the different resolutions to generate independent different restoration models, receiving a distorted image, and applying a restoration model corresponding to the resolution of the distorted image among the independent different restoration models.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0007" level="1">DESCRIPTION OF DRAWINGS</heading><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a view showing an input image and an output image according to the present embodiment.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining an image restoration process according to the present embodiment.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view for explaining an observed image and a restored image according to the present embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view showing the distortion of observed images according to the present embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view showing a restored image for each observed image according to the present embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view showing an original image, an observed image, and a generated fake image of an input image according to the present embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> are views showing an image restoration apparatus according to the present embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0008" level="1">EXPLANATION OF REFERENCE NUMERALS</heading><p id="p-0023" num="0000"><ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0022"><b>700</b>: Image restoration apparatus</li>        <li id="ul0002-0002" num="0023"><b>710</b>: Learning unit</li>        <li id="ul0002-0003" num="0024"><b>712</b>: generator (G) <b>714</b>: Discriminator (D)</li>        <li id="ul0002-0004" num="0025"><b>720</b>: Image restoration unit</li>    </ul>    </li></ul></p><heading id="h-0009" level="1">MODE FOR INVENTION</heading><p id="p-0024" num="0026">Hereinafter, the present embodiment will be described in detail with reference to the accompanying drawings.</p><p id="p-0025" num="0027"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a view showing an input image and an output image according to the present embodiment.</p><p id="p-0026" num="0028">The image restoration apparatus <b>700</b> according to the present embodiment applies Generative Adversarial Networks (GAN) to the distorted image and outputs it as an upscaled image.</p><p id="p-0027" num="0029">The image restoration apparatus <b>700</b> restores the distorted image into an upscaled image by improving centering on the restoration target object within the distorted image.</p><p id="p-0028" num="0030">As shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, when a 64&#xd7;64 distorted image is restored, it may be restored to the conventional extent shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, but the image restoration apparatus <b>700</b> according to the present embodiment may restore the distorted image to a level of upscaled image as shown in <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0029" num="0031"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a view for explaining an image restoration process according to the present embodiment.</p><p id="p-0030" num="0032">When the original image (clean image x) exists, distortion may occur in the original image x due to various causes. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, unknown distortions (caused by blur, noise, compression, etc.) occur in the original image x so that the observed image y is distorted.</p><p id="p-0031" num="0033">The image restoration apparatus <b>700</b> generates a restored image x{circumflex over (&#x2003;)} restored by applying a deep learning technique to the observed image y.</p><p id="p-0032" num="0034">A generator <b>712</b> in Generative Adversarial Networks (GAN) generates a restoration upscaled image (64&#xd7;64) using a CNN based on an input image (32&#xd7;32).</p><p id="p-0033" num="0035">The discriminator <b>714</b> in the GAN determines whether the upscaled image output from the generator (G) <b>712</b> is an original real image or a generated fake image. The discriminator (D) <b>714</b> in the GAN reverse-propagates the score that has determined whether the upscaled image output from the generator (G) <b>712</b> is an original (real) image or a generated fake image, and GAN performs learning to generate a generated fake upscaled image like the original (real) one in order to increase the score for determining whether it is an original (real) image or a generated fake image.</p><p id="p-0034" num="0036">The relationship between the observed image y and the original image x may be expressed by [Equation 1].</p><p id="p-0035" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>P</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>&#x2062;</mo>       <mrow>        <semantics definitionURL="">         <mo>&#x2758;</mo>         <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>        </semantics>        <mi>y</mi>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mo>=</mo>     <mrow>      <mfrac>       <mrow>        <mrow>         <mi>P</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <mi>y</mi>          <mo>&#x2062;</mo>          <mrow>           <semantics definitionURL="">            <mo>&#x2758;</mo>            <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>           </semantics>           <mi>x</mi>          </mrow>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#x2062;</mo>        <mrow>         <mi>P</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mi>x</mi>         <mo>)</mo>        </mrow>       </mrow>       <mrow>        <mi>P</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mi>y</mi>        <mo>)</mo>       </mrow>      </mfrac>      <mo>&#x221d;</mo>      <mrow>       <mrow>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mrow>          <semantics definitionURL="">           <mo>&#x2758;</mo>           <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>          </semantics>          <mi>x</mi>         </mrow>        </mrow>        <mo>)</mo>       </mrow>       <mo>&#x2062;</mo>       <mrow>        <mi>P</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mi>x</mi>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>1</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0036" num="0037">(y|x) obtains an observed image y that is a distorted image from the original image x so that there may be a likelihood of modeling the confirmed distortion knowledge such as blur, noise, compression, or the like.</p><p id="p-0037" num="0038">P(x) is a prior term, and knowledge of the original image x may be modeled.</p><p id="p-0038" num="0039">The discriminator (D) <b>714</b> itself models probability distributions for p(x) and determines whether it is an original (real) image or a generated fake image based on how similar the distances between the probability distributions are. The discriminator (D) <b>714</b> itself represents the prior term.</p><p id="p-0039" num="0040">In general, when it is assumed that a 32&#xd7;32 image is modeled as a 64&#xd7;64 image or a 64&#xd7;64 image is modeled twice as a 128&#xd7;128 image, the discriminator (D) inputs an image with twice lower resolution, and learns based on an image with twice higher resolution as the correct answer (output).</p><p id="p-0040" num="0041">However, in this learning method, since the discriminator (D) itself models the probability distribution for the original image x (target image), if all images are set as the correct answer (output), the correct answer (output) for the 32&#xd7;32 image is output as a 64&#xd7;64 image, but the correct answer (output) for the 64&#xd7;64 image has a problem that not only a 128&#xd7;128 image is output, but also a 64&#xd7;64 image is output as an original (real) image.</p><p id="p-0041" num="0042">The image restoration apparatus <b>700</b> according to the present embodiment generates models independent from each other for each original image x (target image) and outputs the most suitable image.</p><p id="p-0042" num="0043"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a view for explaining an observed image and a restored image according to the present embodiment.</p><p id="p-0043" num="0044">Since the image restoration apparatus <b>700</b> according to the present embodiment models knowledge of blur, noise, compression, etc., the image compression technique obtains a distorted observed image y as a result of compression in a state in which random noise is added from the original image x and the blur kernel is convolved.</p><p id="p-0044" num="0045">The observed image y has the same value as in [Equation 2] due to distortion occurrence.</p><p id="p-0045" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>&#x3b3;=JPEG<sub>q</sub>((<i>X+N</i><sub>&#x3c3;;c</sub>)&#x2297;<i>K</i><sub>S</sub>)&#x2003;&#x2003;[Equation 2]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0046" num="0046">N<sub>&#x3c3;;c</sub>: AWGN with channel dependent noise level</p><p id="p-0047" num="0047">K<sub>S</sub>: blur kernel with size s</p><p id="p-0048" num="0048">JPEG<sub>q</sub>: JPEG compression with quality q</p><p id="p-0049" num="0049">In other words, N<sub>&#x3c3;;c </sub>means a noise level dependent on channel c. K<sub>S </sub>refers to blur kernel for size s. JPEG<sub>q </sub>means that JPEG compression is performed with compression quality q. Here, JPEG compression is an example and the present disclosure is not limited to a specific image/video compression technique.</p><p id="p-0050" num="0050">The discriminator (D) <b>714</b> according to the present embodiment acquires a distorted image y and then performs learning based on a distorted observed image y and an original image x. That is, the image restoration apparatus <b>700</b> receives the distorted observed image y as an input, sets the original image x as the correct answer (output), and performs learning.</p><p id="p-0051" num="0051">The image restoration apparatus <b>700</b> according to the present embodiment acquires the distorted observed image y, and then generates a restored image x{circumflex over (&#x2003;)} based on the distorted observed image y and the original image x.</p><p id="p-0052" num="0052">The image restoration apparatus <b>700</b> according to the present embodiment uses [Equation 3] in order to generate the restored image x{circumflex over (&#x2003;)}.</p><p id="p-0053" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mtable>     <mtr>      <mtd>       <mrow>        <mover>         <mi>x</mi>         <mo>^</mo>        </mover>        <mo>=</mo>        <malignmark/>        <mrow>         <mi>arg</mi>         <mtext>  </mtext>         <munder>          <mi>max</mi>          <mi>x</mi>         </munder>         <mtext>   </mtext>         <mrow>          <mi>P</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mrow>           <mi>y</mi>           <mo>&#x2062;</mo>           <mrow>            <semantics definitionURL="">             <mo>&#x2758;</mo>             <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>            </semantics>            <mi>x</mi>           </mrow>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <mi>P</mi>          <mo>&#x2061;</mo>          <mo>(</mo>          <mi>x</mi>          <mo>)</mo>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mo>=</mo>        <malignmark/>        <mrow>         <mrow>          <mi>arg</mi>          <mtext>  </mtext>          <munder>           <mi>min</mi>           <mi>x</mi>          </munder>         </mrow>         <mo>-</mo>         <mrow>          <mi>log</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mrow>           <mi>P</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mrow>            <mi>y</mi>            <mo>&#x2062;</mo>            <mrow>             <semantics definitionURL="">              <mo>&#x2758;</mo>              <annotation encoding="Mathematica">"\[LeftBracketingBar]"</annotation>             </semantics>             <mi>x</mi>            </mrow>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>-</mo>         <mrow>          <mi>log</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mrow>           <mi>P</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mi>x</mi>           <mo>)</mo>          </mrow>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mo>=</mo>        <malignmark/>        <mrow>         <mrow>          <mi>arg</mi>          <mo>&#x2062;</mo>          <mtext>  </mtext>          <munder>           <mrow>            <mi>min</mi>            <mo>&#x2062;</mo>            <mtext>  </mtext>            <mi>E</mi>           </mrow>           <mi>&#x3b8;</mi>          </munder>          <mo>&#x2062;</mo>          <mrow>           <mo>(</mo>           <mrow>            <mi>x</mi>            <mo>,</mo>            <mrow>             <msub>              <mi>G</mi>              <mi>&#x3b8;</mi>             </msub>             <mo>(</mo>             <mi>y</mi>             <mo>)</mo>            </mrow>           </mrow>           <mo>)</mo>          </mrow>         </mrow>         <mo>+</mo>         <mrow>          <mi>&#x3bb;</mi>          <mo>&#x2062;</mo>          <mrow>           <mi>R</mi>           <mo>&#x2061;</mo>           <mo>(</mo>           <mi>x</mi>           <mo>)</mo>          </mrow>         </mrow>        </mrow>       </mrow>      </mtd>     </mtr>    </mtable>   </mtd>   <mtd>    <mrow>     <mo>[</mo>     <mrow>      <mi>Equation</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mn>3</mn>     </mrow>     <mo>]</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00002-2" num="00002.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>E</mi>     <mo>=</mo>     <msubsup>      <mrow>       <mo>&#xf605;</mo>       <mrow>        <mrow>         <mi>F</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mi>x</mi>         <mo>)</mo>        </mrow>        <mo>-</mo>        <mrow>         <mi>F</mi>         <mo>&#x2061;</mo>         <mo>(</mo>         <mrow>          <msub>           <mi>G</mi>           <mi>&#x3b8;</mi>          </msub>          <mo>(</mo>          <mi>y</mi>          <mo>)</mo>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>&#xf606;</mo>      </mrow>      <mn>2</mn>      <mn>2</mn>     </msubsup>    </mrow>   </mtd>   <mtd>    <mrow>     <mi>R</mi>     <mo>=</mo>     <mrow>      <mrow>       <mo>-</mo>       <mi>log</mi>      </mrow>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <msub>        <mi>D</mi>        <mrow>         <mi>&#x3c6;</mi>         <mo>;</mo>         <mi>x</mi>        </mrow>       </msub>       <mo>(</mo>       <mrow>        <msub>         <mi>G</mi>         <mi>&#x3b8;</mi>        </msub>        <mo>&#x2062;</mo>        <mrow>         <mo>(</mo>         <mi>y</mi>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0054" num="0053">F: (5,4)-th convolution map of object recognition network</p><p id="p-0055" num="0054">G<sub>&#x3b8;</sub>: generator network (EDSR) with parameter</p><p id="p-0056" num="0055">D<sub>&#x3c6;;x</sub>: discriminator network with parameter &#x3c6; given prior x</p><p id="p-0057" num="0056">P(y|x) means a probability distribution modeling the distorted observed image y.</p><p id="p-0058" num="0057">P(x) means a probability distribution of the original image x.</p><p id="p-0059" num="0058">The restored image x{circumflex over (&#x2003;)} has a value x for making the probability distribution P(y|x) modeling the distorted observed image y and the probability distribution P(x) of the original image x a maximum value.</p><p id="p-0060" num="0059">The restored image x{circumflex over (&#x2003;)} has a value x for making the probability distribution P(y|x) modeling the distorted observed image y taking &#x2212;log and the probability distribution P(x) of the original image x taking &#x2212;log a minimum value.</p><p id="p-0061" num="0060">R(x) denotes a score that has determined whether the upscaled image output from the generator (G) <b>712</b> is an original (real) image or a generated fake image by the discriminator (D) <b>714</b> in the GAN.</p><p id="p-0062" num="0061">G<sub>&#x3b8;</sub>(y) denotes a fake image generated by the generator (G) <b>712</b>.</p><p id="p-0063" num="0062">E denotes a value obtained by squaring the difference between the original image x and G<sub>&#x3b8;</sub>(y).</p><p id="p-0064" num="0063">F refers to a network learned for object recognition.</p><p id="p-0065" num="0064">As described in [Equation 1], the image restoration apparatus <b>700</b> can know that the model is separated and learned by Likelihood term and Prior term.</p><p id="p-0066" num="0065">The image restoration apparatus <b>700</b> learns an independent model G depending on whether the target image as the correct answer is a 64&#xd7;64 image, a 128&#xd7;128 image, a 2 k image, a 4 k image, or an 8 k image according to the prior term. That is, the image restoration apparatus <b>700</b> should be provided with an independent model G for each resolution.</p><p id="p-0067" num="0066">Since the image restoration apparatus <b>700</b> according to the present embodiment is mathematically modeled as in [Equation 1], learning is performed separately for each resolution. The image restoration apparatus <b>700</b> may model the distorted observed image y as in [Equation 2].</p><p id="p-0068" num="0067">The image restoration apparatus <b>700</b> minimizes only the main features of a restoration target object by additionally implementing an object recognition network when minimizing the data term between the distorted observed image y and the correct answer image. Here, the object recognition network F extracts the features of the restoration target object while reducing the size of the convolutional map. For example, in an application example in which a target object to be restored is a face image, a face recognition network may be implemented to perform learning in a direction to minimize the difference between main features of the face.</p><p id="p-0069" num="0068">The image restoration apparatus <b>700</b> calculates a score that has determined whether the upscaled image output from the generator (G) <b>712</b> is an original (real) image or a generated fake image by the discriminator (D) <b>714</b> in the GAN.</p><p id="p-0070" num="0069">The discriminator (D) <b>714</b> determines whether the upscaled image output from the generator (G) is an original (real) image or a generated fake image based on learned information. Thereafter, the discriminator (D) <b>714</b> may be improved, which enables the learned information to be used without information on the correct answer as a criterion for determining whether it is an original (real) image or a generated fake image.</p><p id="p-0071" num="0070">Thereafter, the image restoration apparatus <b>700</b> has a multi-scale structure in order to integrate the above-described process.</p><p id="p-0072" num="0071">When an arbitrary distorted (degraded) image having various resolutions is input, the image restoration apparatus <b>700</b> upscales the distorted image to an upscaled image having a preset resolution (512&#xd7;512). The image restoration apparatus <b>700</b> downscales (32&#xd7;32, 64&#xd7;64, 128&#xd7;128) the upscaled image (512&#xd7;512) to images back to preset multiples (2 times, 4 times).</p><p id="p-0073" num="0072">Thereafter, the image restoration apparatus <b>700</b> generates data of different scales. The result is obtained by passing the upscaled image (512&#xd7;512) through a network learned as an input.</p><p id="p-0074" num="0073">The image restoration apparatus <b>700</b> passes the upscaled image (512&#xd7;512) through the network learned as an input and generates images downscaled (32&#xd7;32, 64&#xd7;64, 128&#xd7;128) back to the preset multiples (2 times, 4 times). The image restoration apparatus <b>700</b> generates the smallest image (32&#xd7;32) first when performing downscaling. The image restoration apparatus <b>700</b> converts the smallest downscaled image (32&#xd7;32) into an upscaled image (64&#xd7;64).</p><p id="p-0075" num="0074">The image restoration apparatus <b>700</b> generates an upscaled image (128&#xd7;128) by combining the upscaled image (64&#xd7;64) with the downscaled image (64&#xd7;64). The image restoration apparatus <b>700</b> generates an upscaled image (256&#xd7;256) by combining the upscaled image (128&#xd7;128) with the downscaled image (128&#xd7;128). The image restoration apparatus <b>700</b> generates an upscaled image (512&#xd7;512) by combining the upscaled image (256&#xd7;256) with the downscaled image (256&#xd7;256). The image restoration apparatus <b>700</b> always seeks to obtain constantly good results regardless of different scales and different distortions.</p><p id="p-0076" num="0075">The image restoration apparatus <b>700</b> generates an observed image y which has the original image x and is distorted, and restores the distorted observed image y to generate a restored image x{circumflex over (&#x2003;)}. The image restoration apparatus <b>700</b> repeatedly performs upscaling an input image to 512&#xd7;512, generating the input image while lowering the resolution of the upscaled image to &#xbd;, &#xbc;, &#x215b;, etc., and inputting the generated input image.</p><p id="p-0077" num="0076"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a view showing the distortion of observed images according to the present embodiment.</p><p id="p-0078" num="0077">As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the observed image y may be distorted in various forms with respect to the original image x. In other words, distortion as in the observed image y shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> may occur due to a noise image, a blur image, a resize image, a compressed image, and a noise image and a compressed image with respect to the original image x.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a view showing a restored image for each observed image according to the present embodiment.</p><p id="p-0080" num="0079">The results of generating a restored image x{circumflex over (&#x2003;)} in the image restoration apparatus <b>700</b> by receiving various types of distorted observed images y shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref> are as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0081" num="0080">In other words, when the image restoration apparatus <b>700</b> receives and restores various types of distorted observed images y, a restored image is generated for each of the noise image, the blur image, the resized image, the compressed image, and the noise image and the compressed image as shown in <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0082" num="0081"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a view showing an original image, an observed image, and a generated fake image of an input image according to the present embodiment.</p><p id="p-0083" num="0082">In the process of generating a restored image in which an input image is restored by the image restoration apparatus <b>700</b> as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>, the input image input to the generator (G) <b>712</b>, the CNN image generated by the generator (G) <b>712</b>, and the Ground-Truth image input to the discriminator (D) <b>714</b> are as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. The discriminator (D) <b>714</b> in the image restoration apparatus <b>700</b> calculates a score which has determined whether the upscaled image output from the generator (G) <b>712</b> is an original (real) image or a generated fake image.</p><p id="p-0084" num="0083">The discriminator (D) <b>714</b> in the image restoration apparatus <b>700</b> determines whether the upscaled image output from the generator G is an original (real) image or a generated fake image based on the learned information. The image restoration apparatus <b>700</b> transfers the correct answer image (Ground-Truth) and the fake image (Relative-GAN) generated in the generator G together as a criterion for determining whether it is an original (real) image or a generated fake image by improving the performance of the discriminator (D) <b>714</b>.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> are views showing an image restoration apparatus according to the present embodiment.</p><p id="p-0086" num="0085">The image restoration apparatus <b>700</b> according to the present embodiment includes a learning unit <b>710</b> and an image restoration unit <b>720</b>. Components included in the image restoration apparatus <b>700</b> are not necessarily limited thereto.</p><p id="p-0087" num="0086">The respective components included in the image restoration apparatus <b>700</b> may be connected to a communication path connecting a software module or a hardware module inside the apparatus so that they can operate organically with each other. These components communicate using one or more communication buses or signal lines.</p><p id="p-0088" num="0087">Each component of the image restoration apparatus <b>700</b> shown in <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> means a unit that processes at least one function or operation, and may be implemented by a software module, a hardware module, or a combination of software and hardware.</p><p id="p-0089" num="0088">The learning unit <b>710</b> receives a plurality of images having different resolutions and performs learning for different resolutions to generate independent restoration models respectively that are different for each resolution.</p><p id="p-0090" num="0089">The learning unit <b>710</b> includes a generator (G) <b>712</b> and a discriminator (D) <b>714</b>.</p><p id="p-0091" num="0090">The generator (G) <b>712</b> generates a restored image obtained by upscaling the resolution of the input image by a preset resolution.</p><p id="p-0092" num="0091">The generator (G) <b>712</b> generates an upscaled image obtained by upscaling the input image to an image having a preset resolution. The generator (G) <b>712</b> repeats inputting the upscaled image as an input image while lowering the resolution to have a resolution of a preset ratio.</p><p id="p-0093" num="0092">The discriminator (D) <b>714</b> receives the generated fake image and the original image x, and calculates a score that has determined whether the generated fake image is the original image or the generated fake image. The discriminator (D) <b>714</b> generates a restoration model for each resolution based on the score.</p><p id="p-0094" num="0093">When the image restoration unit <b>720</b> receives a distorted observed image y, it generates a restored image x{circumflex over (&#x2003;)} in which the observed image is restored by using a restoration model corresponding to the resolution of the observed image y among independent restoration models that are different for each resolution.</p><p id="p-0095" num="0094">The observed image y has a value in which a blur kernel (K<sub>S</sub>) for size and JPEG compression quality (JPEG<sub>q</sub>) are applied to a channel-dependent noise level (N<sub>&#x3c3;;c</sub>) value added to the original image x.</p><p id="p-0096" num="0095">The image restoration unit <b>720</b> generates a restored image x{circumflex over (&#x2003;)} allowing to have a value for making the probability distribution P(y|x) modeling the distorted observed image y and the probability distribution P(x) of the original image x a maximum value.</p><p id="p-0097" num="0096">The image restoration unit <b>720</b> generates a restored image x{circumflex over (&#x2003;)} allowing to have a value for making the probability distribution P(y|x) modeling the distorted observed image y taking &#x2212;log and the probability distribution P(x) of the original image x taking &#x2212;log a minimum value.</p><p id="p-0098" num="0097">The image restoration unit <b>720</b> generates a restored image x{circumflex over (&#x2003;)} from three of the original image (x), the fake image G<sub>&#x3b8;</sub>(y) generated by the generator (G) <b>712</b>, and a parameter that minimizes E, a value obtained by squaring a difference between the original image x and the generated fake image G<sub>&#x3b8;</sub>(y), based on the score R(x) that has determined whether the image generated by the generator (G) <b>712</b> is an original (real) image or a generated fake image.</p><p id="p-0099" num="0098">When the image restoration unit <b>720</b> generates the restored image x{circumflex over (&#x2003;)}, it applies a value obtained by extracting only the main features of the restoration target object while reducing the size of the convolution map by applying the object recognition network additionally implemented in order to minimize the data term between the distorted observed image y and the original image x.</p><p id="p-0100" num="0099">The above description is merely exemplarily explaining the technical spirit of the present embodiment, and various modifications and variations will be possible without departing from the essential features of the present embodiment by those skilled in the art to which the present embodiment belongs. Accordingly, the present embodiments are intended to explain rather than limit the technical spirit of the present embodiment, and the scope of the technical spirit of the present embodiment is not limited by these embodiments. The protection scope of the present embodiment should be interpreted by the following claims, and all technical spirits within the scope equivalent thereto should be interpreted as being included in the scope of the present embodiment.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005114A1-20230105-M00001.NB"><img id="EMI-M00001" he="6.01mm" wi="76.20mm" file="US20230005114A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002 MATH-US-00002-2" nb-file="US20230005114A1-20230105-M00002.NB"><img id="EMI-M00002" he="18.37mm" wi="76.20mm" file="US20230005114A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image restoration apparatus characterized by including:<claim-text>a receiver for receiving an image; and</claim-text><claim-text>a processor for processing the image through convolution operation and non-linearization,</claim-text><claim-text>wherein the processor includes:</claim-text><claim-text>a learning unit that receives a plurality of images having different resolutions and performs learning for each of the different resolutions to learn an independent restoration model for each resolution;</claim-text><claim-text>a selection unit that, when receiving a distorted observed image y, analyzes the resolution of the distorted observed image y to select a suitable restoration model; and</claim-text><claim-text>an image restoration unit that generates a restored image x{circumflex over (&#x2003;)} in which the observed image is restored by using a restoration model corresponding to the resolution of the observed image y among independent restoration models that are different for each resolution.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The image restoration apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the learning unit is characterized by including:<claim-text>a generator for generating a fake image that increases the resolution of the input image by a preset resolution;</claim-text><claim-text>a discriminator for receiving the generated fake image and the original image x, determining whether the generated fake image is an original (real) image or a fake image to calculate a score, and generating the restoration model for each resolution based on the calculated score.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The image restoration apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the observed image y is characterized in that a channel-dependent noise, various intensities of blur, and image/video compression are applied to the original image x.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The image restoration apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image restoration unit is characterized by generating the restored image x{circumflex over (&#x2003;)} allowing to have a value for making the probability distribution P(y|x) modeling the distorted observed image y and the probability distribution P(x) of the original image x a maximum value.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The image restoration apparatus of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the image restoration unit is characterized by generating the restored image x{circumflex over (&#x2003;)} allowing to have a value for making the probability distribution P(y|x) modeling the distorted observed image y taking &#x2212;log and the probability distribution P(x) of the original image x taking &#x2212;log a minimum value.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The image restoration apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the image restoration unit is characterized by generating the restored image x{circumflex over (&#x2003;)} that minimizes a difference between the original image x and the generated fake image G<sub>&#x3b8;</sub>(y) based on the score R(x) that has calculated whether a fake image generated by the generator is an original (real) image or a generated fake image by the discriminator.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The image restoration apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the discriminator is characterized by generating the restored image x{circumflex over (&#x2003;)} allowing a difference between the corresponding scores to be minimized by relatively comparing the score that has calculated whether the fake image generated by the generator is an original (real) image or a generated fake image and the score that has calculated whether the original image is an original (real) image or a generated fake image.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The image restoration apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the learning unit is characterized by learning a difference between the main features of the restoration target object extracted from the convolution map by passing through an object recognition network model in order to minimize a data term between the distorted observed image y and the original image x when the restored image x{circumflex over (&#x2003;)} is generated.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The image restoration apparatus of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the generator is characterized in that it repeats generating an upscaled image obtained by upscaling the input image to an image having a preset resolution, and inputting the upscaled image as an input image while lowering the resolution to have a resolution of a preset ratio.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. An image restoration method characterized by comprising:<claim-text>a learning process of receiving a plurality of images having different resolutions and performing learning for each of the different resolutions to learn an independent restoration model for each resolution;</claim-text><claim-text>a selection process of selecting a suitable restoration model by analyzing the resolution of the distorted observed image y when the distorted observed image y is received; and</claim-text><claim-text>an image restoration process of generating a restored image x{circumflex over (&#x2003;)} in which the observed image is restored by using a restoration model corresponding to the resolution of the observed image y among independent restoration models that are different for each resolution.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The image restoration method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the learning process is characterized by including:<claim-text>a generation process of generating a fake image that increases the resolution of the input image by a preset resolution; and</claim-text><claim-text>a discrimination process of receiving the generated fake image and the original image x, determining whether the generated fake image is an original image or a generated fake image to calculate a score, and generating the restoration model for each resolution based on the calculated score.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The image restoration method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the image restoration process is characterized by generating the restored image x{circumflex over (&#x2003;)} allowing to have a value for making the probability distribution P(y|x) modeling the distorted observed image y and the probability distribution P(x) of the original image x a maximum value.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The image restoration method of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the image restoration process is characterized by generating the restored image x{circumflex over (&#x2003;)} allowing to have a value for making the probability distribution P(y|x) modeling the distorted observed image y taking &#x2212;log and the probability distribution P(x) of the original image x taking &#x2212;log a minimum value.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The image restoration method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the image restoration process is characterized by generating the restored image x{circumflex over (&#x2003;)} that minimizes a difference between the original image x and the generated fake image G<sub>&#x3b8;</sub>(y) based on the score R(x) that has calculated whether a fake image generated in the generation process is an original (real) image or a generated fake image by the discriminator.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The image restoration method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the discrimination process is characterized by generating the restored image x{circumflex over (&#x2003;)} allowing a difference between the corresponding scores to be minimized by relatively comparing the score that has calculated whether the fake image generated in the generation process is an original (real) image or a generated fake image and the score that has calculated whether the original image is an original (real) image or a generated fake image.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The image restoration method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the learning process is characterized by learning a difference between the main features of the restoration target object extracted from the convolution map by passing through an object recognition network model in order to minimize a data term between the distorted observed image y and the original image x when the restored image x{circumflex over (&#x2003;)} is generated.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The image restoration method of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the generation process is characterized in that it repeats generating an upscaled image obtained by upscaling the input image to an image having a preset resolution, and inputting the upscaled image as an input image while lowering the resolution to have a resolution of a preset ratio.</claim-text></claim></claims></us-patent-application>