<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005239A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005239</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17782521</doc-number><date>20200414</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>CN</country><doc-number>201911222644.4</doc-number><date>20191203</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>16</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>60</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>20</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>70</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>25</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>761</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>40</main-group><subgroup>169</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>2201</main-group><subgroup>07</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10144</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30168</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">IMAGE CAPTURING METHOD AND DEVICE, APPARATUS, AND STORAGE MEDIUM</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ZHEJIANG UNIVIEW TECHNOLOGIES CO., LTD.</orgname><address><city>Zhejiang</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>XU</last-name><first-name>Qiong</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>ZHANG</last-name><first-name>Wenping</first-name><address><city>Zhejiang</city><country>CN</country></address></addressbook></inventor></inventors></us-parties><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/CN2020/084589</doc-number><date>20200414</date></document-id><us-371c12-date><date>20220603</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Provided are an image capturing method and apparatus, a device and a storage medium. The method includes: at a new acquisition moment, predicting a predicted projection area position of a target object in a current captured image on an image sensor and estimated exposure brightness information of the target object in the predicted projection area position; adjusting, according to a type of the target object and the estimated exposure brightness information, an exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives; and acquiring a new captured image at the new acquisition moment according to the adjusted exposure parameter, where both the new captured image and the current captured image include the target object.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="113.45mm" wi="158.75mm" file="US20230005239A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="96.94mm" wi="172.04mm" file="US20230005239A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="170.26mm" wi="172.04mm" file="US20230005239A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="236.56mm" wi="172.21mm" file="US20230005239A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="215.39mm" wi="167.13mm" file="US20230005239A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="221.66mm" wi="167.56mm" file="US20230005239A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="73.15mm" wi="106.00mm" file="US20230005239A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><p id="p-0002" num="0001">This application claims priority to Chinese Patent Application No. 201911222644.4 filed with the China National Intellectual Property Administration (CNIPA) on Dec. 3, 2019, the disclosure of which is incorporated herein by reference in its entirety.</p><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present application relates to the field of monitoring technology, for example, an image capturing method and apparatus, a device and a storage medium.</p><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0004" num="0003">With the development of monitoring technology, video surveillance applications have an increasing demand for acquisition and recognition of captured targets such as a human face, a human body and a license plate.</p><p id="p-0005" num="0004">However, in an actual scenario, if an acquired image includes multiple target objects such as the human face, the human body and the license plate, it cannot be ensured that different target objects meet capture requirements at the same time. That is, generally only one of the target objects can be guaranteed to achieve a required capture effect, and it cannot be ensured that all the multiple target objects achieve required capture effects.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">Embodiments of the present application provide an image capturing method and apparatus, a device and a storage medium, so as to ensure that multiple target objects can all achieve desired capturing effects in a capturing process.</p><p id="p-0007" num="0006">Embodiments of the present application provide an image capturing method including the steps below.</p><p id="p-0008" num="0007">A predicted projection area position of a target object in a current captured image on an image sensor and at a new acquisition moment is predicted, and estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment is predicted.</p><p id="p-0009" num="0008">An exposure parameter of the target object in the predicted projection area position is adjusted according to a type of the target object and the estimated exposure brightness information when the new acquisition moment arrives.</p><p id="p-0010" num="0009">A new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter, where both the new captured image and the current captured image include the target object.</p><p id="p-0011" num="0010">Embodiments of the present application further provide an image capturing apparatus including an exposure brightness determination module, an exposure parameter adjustment module and an image exposure capture module.</p><p id="p-0012" num="0011">The exposure brightness determination module is configured to predict a predicted projection area position of a target object in a current captured image on an image sensor and at a new acquisition moment and predict estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment.</p><p id="p-0013" num="0012">The exposure parameter adjustment module is configured to adjust, according to a type of the target object and the estimated exposure brightness information, an exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</p><p id="p-0014" num="0013">The image exposure capture module is configured to acquire a new captured image at the new acquisition moment according to the adjusted exposure parameter, where both the new captured image and the current captured image include the target object.</p><p id="p-0015" num="0014">Embodiment of the present application further provides an electronic device including one or more processors and a storage apparatus.</p><p id="p-0016" num="0015">The storage apparatus is configured to store one or more programs.</p><p id="p-0017" num="0016">The one or more programs are executed by the one or more processors to cause the one or more processors to implement the image capturing method provided in any embodiment of the present application.</p><p id="p-0018" num="0017">Embodiment of the present application further provides a computer-readable storage medium configured to store a computer program, and the computer program, when executed by a processor, implements the image capturing method provided in any embodiment of the present application.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF DRAWINGS</heading><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of an image capturing method according to an embodiment of the present application;</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of another image capturing method according to an embodiment of the present application;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of the determination of a predicted projection area position according to an embodiment of the present application;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of another image capturing method according to an embodiment of the present application;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of projection areas on an image sensor according to an embodiment of the present application;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a photometry area according to an embodiment of the present application;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of another image capturing method according to an embodiment of the present application;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram of an image capturing apparatus according to an embodiment of the present application; and</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a structural diagram of an electronic device according to an embodiment of the present application.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">The present application is described below in conjunction with drawings and embodiments. The embodiments described herein are intended to explain the present application and not to limit the present application. For ease of description, only part, not all, of structures related to the present application are illustrated in the drawings.</p><p id="p-0029" num="0028">Before example embodiments are discussed, it is to be noted that some example embodiments are described as processing or methods depicted in flowcharts. Although multiple operations (or steps) are described as sequential processing in the flowcharts, many of the operations (or steps) may be implemented concurrently, coincidently or simultaneously. Additionally, the sequence of the multiple operations may be rearranged. The processing may be terminated when the operations are completed, but the processing may further have additional steps that are not included in the drawings. The processing may correspond to a method, a function, a procedure, a subroutine, a subprogram or the like.</p><p id="p-0030" num="0029">To understand the technical solutions of the present application, the content related to exposure capture in an actual scenario is analyzed here, so as to find the defects of the exposure capture. Generally, a human face and a human body belong to a target object easy to darken, and a license plate belongs to a target object easy to brighten. If the human face, the human body or the license plate appears alone in a captured scene, the brightness of the human face, the human body or the license plate in a captured image can be guaranteed to meet a requirement through the photometry of the human face, the human body or the license plate. However, generally the human face, the human body or the license plate hardly appears alone in the captured image. If the human face, the human body and the license plate appear simultaneously in the captured scene, regardless of any type of photometry used, the brightness effect of only one target object can be ensured, and the brightness effects of all multiple target objects cannot be ensured.</p><p id="p-0031" num="0030">In conjunction with the above analysis of exposure capture, an image capturing method and apparatus, a device and a storage medium are described below through the following embodiments and optional technical solutions in these embodiments.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a flowchart of an image capturing method according to an embodiment of the present application. The embodiment of the present application may be applied to the case where a target object in a captured scene is captured, for example, the case where multiple target objects in the captured scene are captured. The method may be performed by an image capturing apparatus, and the image capturing apparatus may be implemented by software and/or hardware and integrated on any electronic device having a network communication function. For example, the electronic device includes, but is not limited to, an electronic capture device, an electronic police device and the like. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the image capturing method provided in the embodiment of the present application includes the steps below.</p><p id="p-0033" num="0032">In S<b>110</b>, at a new acquisition moment, a predicted projection area position of a target object in a current captured image on an image sensor and estimated exposure brightness information of the target object in the predicted projection area position are determined.</p><p id="p-0034" num="0033">In this embodiment, the current captured image is a captured image obtained at a current acquisition moment. The new acquisition moment refers to an acquisition moment after the current acquisition moment. The electronic device includes the image sensor, and an optical image of an external target object is generated through a lens assembly and projected onto the image sensor. The image sensor converts the projected optical image into an electrical signal that is converted into the captured image after a series of signal processing. Considering that the target object in the current captured image is mobile rather than stationary in most cases, a projection area position of the target object on the image sensor included in the electronic device also changes in the case where the electronic device is fixed. Therefore, a projection area position of the target object on the image sensor and at the new acquisition moment needs to be determined as the predicted projection area position. Considering that the current acquisition moment is a moment that has already occurred and the new acquisition moment is a moment that is about to arrive and has not yet occurred, the predicted projection area position is a projection area position obtained through prediction.</p><p id="p-0035" num="0034">In this embodiment, in an exposure adjustment mode, the current captured image includes the target object, and the target object, for example, may include a human face, a human body, a vehicle body, a license plate and the like. In an actual scenario, for example, in the daytime, the license plate is easily overexposed in a strong front light scenario and the license plate and the human face both have very low brightness in a strong backlight scenario; and at night, the human face has very low brightness and the license plate is easily overexposed. When the optical image of the target object is projected onto the image sensor, the image sensor performs exposure. Considering that multiple types of target object are exposed with different difficulties under the same illumination condition or under different illumination conditions, the exposure brightness of the target object in the projection area position on the image sensor is different under different conditions. Therefore, the estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment needs to be automatically determined. The estimated exposure brightness information includes estimated exposure brightness of the target object in the current captured image in the predicted projection area position and at the new acquisition moment. The exposure brightness refers to the brightness of the target object in the projection area position after the optical image of the target object is projected to the projection area position on the image sensor and exposed.</p><p id="p-0036" num="0035">In an optional implementation of this embodiment, the image capturing method in this embodiment further includes the steps below.</p><p id="p-0037" num="0036">The acquired current captured image is detected in a capturing process; and if it is detected that the current captured image includes the target object, it is determined that the image is captured in the exposure adjustment mode to adjust an exposure parameter.</p><p id="p-0038" num="0037">In this implementation, if the target object does not appear in the captured scene in the capturing process, the case of overexposure or very low brightness generally does not occur in an exposure capturing process. Therefore, the current captured image obtained at the current acquisition moment needs to be acquired in real time and detected. If it is detected that the current captured image includes the target object, it indicates that the target object appears in the captured scene, and the exposure adjustment mode continues being used or a normal exposure mode is switched to the exposure adjustment mode so that an image is captured in the exposure adjustment mode for the exposure parameter to be adjusted. If it is detected that the current captured image does not include the target object, it indicates that the target object disappears from the captured scene, and the normal exposure mode continues being used or the exposure adjustment mode is switched to the normal exposure mode so that the image is captured in the normal exposure mode. For example, when it is detected that the target object appears in the captured scene, the exposure adjustment mode is switched to; when it is detected that the target object disappears from the captured scene, the normal exposure mode is switched back to. The target object is detected in real time, and an exposure mode is switched in real time as the target object appears and disappears.</p><p id="p-0039" num="0038">With the above scheme, adaptive exposure can be performed according to whether the target object appears in the captured scene, and the exposure mode can be automatically switched according to whether the target object appears in the captured scene so that captured images acquired in the presence and absence of the target object can both achieve best effects as much as possible. Meanwhile, considering that the exposure adjustment mode is more complicated than the normal exposure mode in terms of data processing, the target object is detected so that the exposure adjustment mode can be switched to the normal exposure mode in time in the absence of the target object, thereby reducing the complexity of exposure capture to a certain extent.</p><p id="p-0040" num="0039">In this implementation, optionally, in the exposure adjustment mode, if it is detected that the current captured image does not include the target object, the current captured image acquired subsequently continues being detected; if it is determined within a preset time that the target object is still not detected in the current captured image acquired subsequently, the exposure adjustment mode is switched to the normal exposure mode. The above optional implementation has the following advantage: considering that the target object in the current captured image may temporarily disappear and appear again after a while, the above implementation can avoid frequent switching between the exposure adjustment mode and the normal exposure mode and the waste of processing resources.</p><p id="p-0041" num="0040">For example, in the exposure adjustment mode, when the target object disappears from the captured scene, the electronic device continues detecting the captured scene for a preset time period Tn. If it is detected within the preset time period Tn that the target object still does not appear in the captured scene, the exposure mode is switched from the exposure adjustment mode to the normal exposure mode so that the normal exposure mode can ensure the overall effect of the captured scene in the absence of the target object. If it is detected within the preset time period Tn that the target object appears again in the captured scene, the exposure adjustment mode is maintained, and the exposure parameter continues being adjusted in the exposure adjustment mode. The above is cycled so that all-weather adaptive exposure capture is achieved.</p><p id="p-0042" num="0041">In S<b>120</b>, the exposure parameter of the target object in the predicted projection area position is adjusted according to a type of the target object in the current captured image and the estimated exposure brightness information when the new acquisition moment arrives.</p><p id="p-0043" num="0042">In this embodiment, the current captured image may include one or more target objects, and each target object corresponds to one predicted projection area position at the new acquisition moment. If the current captured image includes multiple target objects, separate statistics needs to be performed on the estimated exposure brightness information of each target object in the predicted projection area position and at the new acquisition moment, and exposure parameters used in different predicted projection area positions on the image sensor need to be separately adjusted so that different exposure parameters can be assigned to different areas of the image sensor.</p><p id="p-0044" num="0043">In this embodiment, different target objects belong to different types, and different types of target object are exposed with different difficulties under the same illumination condition. For example, at night, the human face has very low brightness and the license plate is easily overexposed. Therefore, the exposure parameter of the target object in the predicted projection area position and at the new acquisition moment needs to be adjusted in conjunction with the type of the target object in the current captured image and the estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment. The exposure parameter of the target object in the predicted projection area position and at the new acquisition moment refers to an exposure parameter to be used by the image sensor in the predicted projection area position and at the new acquisition moment before the exposure parameter is adjusted. Optionally, the type of the target object may be a type such as the license plate, the vehicle body, the human face and the human body.</p><p id="p-0045" num="0044">With the above technical solution, different exposure parameters can be set for different areas of the image sensor according to the type of the target object and the estimated exposure brightness information of the target object, so as to ensure that exposure capture can be performed at the new acquisition moment by using the adjusted exposure parameter and that different target objects can be exposed by using adapted exposure parameters. It can be seen that the scheme in this embodiment can dynamically and independently adjust exposure parameters of different target objects in projection areas of the image sensor, so as to achieve independent exposure. The scheme may be applied in any time period regardless of the daytime and the night. As long as the exposure of the target objects needs to be adjusted, different projection areas corresponding to different target objects are dynamically determined and exposure parameters in the different projection areas are independently adjusted, so as to achieve independent exposure and ensure that different target objects in a captured image can all reach appropriate capturing brightness.</p><p id="p-0046" num="0045">In S<b>130</b>, a new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter.</p><p id="p-0047" num="0046">In this embodiment, both the new captured image and the current captured image include the target object.</p><p id="p-0048" num="0047">In this embodiment, when the new acquisition moment arrives, the exposure may be performed according to the adjusted exposure parameter in the prediction projection area position, and the new captured image is acquired.</p><p id="p-0049" num="0048">An image capturing scheme is provided in the embodiment of the present application. With the scheme in this embodiment, projection areas of different target objects on the image sensor and at a subsequent acquisition moment can be automatically detected, and an appropriate exposure parameter can be assigned to a projection area where each target object is located according to exposure brightness and to-be-reached target exposure brightness of the projection area where the target object is located so that it is ensured that multiple target objects can all reach desired capturing brightness in the capturing process and it can be ensured that a target object easy to darken or a target object easy to brighten can reach desired brightness.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart of another image capturing method according to an embodiment of the present application. Based on the preceding embodiments, S<b>110</b> in the above embodiment is described in the embodiment of the present application. The embodiment of the present application may be combined with an optional solution in one or more of the preceding embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the image capturing method provided in this embodiment includes the steps below.</p><p id="p-0051" num="0050">In S<b>210</b>, a current projection area position of a target object in a current captured image on an image sensor and at a current acquisition moment is determined.</p><p id="p-0052" num="0051">In this embodiment, the step in which the current projection area position of the target object in the current captured image on the image sensor and at the current acquisition moment is determined includes: determining a projection area position of an optical image of the target object on the image sensor when the optical image of the target object in the current captured image is projected onto the image sensor at the current acquisition moment, and recoding the projection area position as the current projection area position. Since the current acquisition moment has already occurred, the current projection area position is a projection area position actually calculated based on the current captured image rather than a predicted projection area position. A time sequence is distinguished by the current acquisition moment and a new acquisition moment.</p><p id="p-0053" num="0052">In S<b>220</b>, a predicted projection area position of the target object in the current captured image on the image sensor and at the new acquisition moment is determined.</p><p id="p-0054" num="0053">In this embodiment, the step in which the predicted projection area position of the target object in the current captured image on the image sensor and at the new acquisition moment is determined includes: predicting a projection area position of the optical image of the target object on the image sensor when the optical image of the target object in the current captured image is projected onto the image sensor at the new acquisition moment, and recoding the projection area position as the predicted projection area position.</p><p id="p-0055" num="0054">In an optional implementation of this embodiment, the step in which the predicted projection area position of the target object in the current captured image on the image sensor and at the new acquisition moment is determined includes steps A<b>10</b> to A<b>30</b>.</p><p id="p-0056" num="0055">In step A<b>10</b>, a current image position of the target object in the current captured image and at the current acquisition moment is determined.</p><p id="p-0057" num="0056">In this implementation, the current captured image includes the target object. In this case, an image position of the target object in the current captured image may be determined through a target recognition algorithm, that is, the current image position of the target object in the current captured image and at the current acquisition moment is obtained.</p><p id="p-0058" num="0057">In step A<b>20</b>, a motion of the target object is predicted according to the current image position, and a predicted image position of the target object in the current captured image and at the new acquisition moment is determined.</p><p id="p-0059" num="0058">In this implementation, the current image position is a position of the target object in the current captured image and at the current acquisition moment. However, since the target object is generally mobile, motion prediction can be performed on the target object in the current captured image, so as to determine which position in the current captured image the target object is about to move to at the new acquisition moment, that is, the predicted image position of the target object in the current captured image and at the new acquisition moment can be obtained. Optionally, with the current image position of the target object as a starting point, prediction is performed according to a motion direction and a motion speed of the target object in the current captured image, so as to obtain which position in the current captured image the target object is about to move to at the new acquisition moment.</p><p id="p-0060" num="0059">In step A<b>30</b>, the predicted projection area position of the target object on the image sensor and at the new acquisition moment is determined according to the predicted image position.</p><p id="p-0061" num="0060">In this implementation, an imaging plane of the image sensor and a captured image may use the same coordinate division information. If the imaging plane of the image sensor and the captured image use the same coordinate division information, the predicted image position may be directly used as the predicted projection area position of the target object on the image sensor. If the imaging plane of the image sensor and the captured image use different coordinate division information, the predicted projection area position of the target object on the image sensor needs to be determined according to the predicted image position and preset position mapping information. Optionally, the predicted image position may be matched with the preset position mapping information so as to determine a predicted projection area position of the optical image of the target object on the imaging plane of the image sensor and at the new acquisition moment.</p><p id="p-0062" num="0061">In this implementation, the captured image where the target object is located is divided into M&#xd7;N area blocks, and the imaging plane of the image sensor where the target object is located is divided into m&#xd7;n area blocks so that an image position of the target object in the captured image and a projection area position of the same target object on the imaging plane of the image sensor have a certain position mapping relationship that is pre-stored. On this basis, position mapping information between the image position and the projection area position of the target object is x=f<sub>1 </sub>(x0)=rounding</p><p id="p-0063" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mo>(</mo>  <mrow>   <mrow>    <mfrac>     <mrow>      <mi>x</mi>      <mo>&#x2062;</mo>      <mn>0</mn>     </mrow>     <mi>M</mi>    </mfrac>    <mo>&#xd7;</mo>    <mi>m</mi>   </mrow>   <mo>+</mo>   <mrow>    <mn>0</mn>    <mo>.</mo>    <mn>5</mn>   </mrow>  </mrow>  <mo>)</mo> </mrow></math></maths></p><p id="p-0064" num="0000">and y=f<sub>2 </sub>(y0)=rounding</p><p id="p-0065" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <mo>(</mo>   <mrow>    <mrow>     <mfrac>      <mrow>       <mi>y</mi>       <mo>&#x2062;</mo>       <mn>0</mn>      </mrow>      <mi>N</mi>     </mfrac>     <mo>&#xd7;</mo>     <mi>n</mi>    </mrow>    <mo>+</mo>    <mrow>     <mn>0</mn>     <mo>.</mo>     <mn>5</mn>    </mrow>   </mrow>   <mo>)</mo>  </mrow>  <mo>.</mo> </mrow></math></maths></p><p id="p-0066" num="0000">A coordinate (x0, y0) denotes the abscissa and ordinate of the image position of the target object in the captured image, and a coordinate (x, y) denotes the abscissa and ordinate of the projection area position of the target object on the image sensor. To ensure the accuracy of coordinate conversion, a constant of 0.5 is used here.</p><p id="p-0067" num="0062">For example, <figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic diagram of the determination of a predicted projection area position according to an embodiment of the present application. Referring to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, in the captured image, coordinate information of the current image position of the target object (a human face) in the current captured image and at the current acquisition moment is denoted as X(x1, y1) and Y(x2, y2). After the motion prediction is performed on the target object, it can be obtained that coordinate information of the predicted image position of the target object in the current captured image and at the new acquisition moment is denoted as X&#x2032;(x3, y3) and Y&#x2032;(x4, y4). After the predicted image position, X&#x2032;(x3, y3) and Y&#x2032;(x4, y4), of the target object is determined, coordinate information of the predicted projection area position of the target object on the image sensor and at the new acquisition moment may be obtained according to a position conversion relationship included in the position mapping information, where the coordinate information of the predicted projection area position is denoted as X&#x2032;(f<sub>1 </sub>(x3), f<sub>2</sub>(y3)) and Y&#x2032;(f<sub>1 </sub>(x4), f<sub>2</sub>(y4)).</p><p id="p-0068" num="0063">In an optional implementation of this embodiment, the step in which the current projection area position of the target object in the current captured image on the image sensor and at the current acquisition moment is determined includes steps B<b>10</b> and B<b>20</b>.</p><p id="p-0069" num="0064">In step B<b>10</b>, the current image position of the target object in the current captured image and at the current acquisition moment is determined, where the current captured image includes the target object.</p><p id="p-0070" num="0065">In step B<b>20</b>, the current projection area position of the target object on the image sensor and at the current acquisition moment is determined according to the current image position.</p><p id="p-0071" num="0066">In this embodiment, optionally, the current image position may be matched with the preset position mapping information so as to determine a current projection area position of the optical image of the target object on the imaging plane of the image sensor and at the current acquisition moment.</p><p id="p-0072" num="0067">For example, still using the target object in <figref idref="DRAWINGS">FIG. <b>3</b></figref> as an example, after the current image position, X(x1, y1) and Y(x2, y2), of the target object is determined, coordinate information of the current projection area position of the target object on the image sensor and at the current acquisition moment may be obtained according to the position conversion relationship included in the position mapping information, where the coordinate information of the current projection area position is denoted as X(f<sub>1 </sub>(x1), f<sub>2</sub>(y1)) and Y(f<sub>1</sub>(x2), f<sub>2</sub>(y2)).</p><p id="p-0073" num="0068">In S<b>230</b>, current exposure brightness information of the target object in the current projection area position and at the current acquisition moment and current exposure brightness information of the target object in the predicted projection area position and at the current acquisition moment are determined.</p><p id="p-0074" num="0069">In this embodiment, the optical image of the target object in the current captured image is projected onto the image sensor at the current acquisition moment, and the image sensor exposes the optical image of the target object according to an exposure parameter. After the target object is exposed, the brightness of the target object in the current projection area position on the image sensor is recorded as the current exposure brightness information of the target object in the current projection area position and at the current acquisition moment. The current exposure brightness information of the target object in the predicted projection area position and at the current acquisition moment may be obtained in the same calculation manner.</p><p id="p-0075" num="0070">For example, it is assumed that the coordinates of the current projection area position of the target object at the current acquisition moment are X(f<sub>1 </sub>(x1), f<sub>2</sub>(y1)) and Y(f<sub>1 </sub>(x2), f<sub>2</sub>(y2)) and the coordinates of the predicted projection area position of the target object at the current acquisition moment are X&#x2032;(f<sub>1 </sub>(x3), f<sub>2</sub>(y3)) and Y&#x2032;(f<sub>1 </sub>(x4), f<sub>2</sub>(y4)). Then, the current exposure brightness information of the target object in the current projection area position and at the current acquisition moment is</p><p id="p-0076" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>L</mi>    <mrow>     <mi>current</mi>     <mo>&#x2062;</mo>     <mn>1</mn>    </mrow>   </msub>   <mo>=</mo>   <mfrac>    <mrow>     <msubsup>      <mi>&#x3a3;</mi>      <mrow>       <mi>x</mi>       <mo>=</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>1</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <msub>        <mi>f</mi>        <mn>1</mn>       </msub>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>&#x2062;</mo>        <mn>2</mn>       </mrow>       <mo>)</mo>      </mrow>     </msubsup>     <mo>&#x2062;</mo>     <msubsup>      <mi>&#x3a3;</mi>      <mrow>       <mi>y</mi>       <mo>=</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>2</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <msub>        <mi>f</mi>        <mn>2</mn>       </msub>       <mo>(</mo>       <mrow>        <mi>y</mi>        <mo>&#x2062;</mo>        <mn>2</mn>       </mrow>       <mo>)</mo>      </mrow>     </msubsup>     <mo>&#x2062;</mo>     <mn>1</mn>     <mo>&#x2062;</mo>     <mrow>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mrow>        <msub>         <mi>f</mi>         <mn>1</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <mn>2</mn>        </mrow>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>1</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#xd7;</mo>     <mrow>      <mo>(</mo>      <mrow>       <mrow>        <msub>         <mi>f</mi>         <mn>2</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mn>2</mn>        </mrow>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>2</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mfrac>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0077" num="0000">and the current exposure brightness information of the target object in the predicted projection area position and at the current acquisition moment is</p><p id="p-0078" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <msub>   <mi>L</mi>   <mrow>    <mi>current</mi>    <mo>&#x2062;</mo>    <mn>2</mn>   </mrow>  </msub>  <mo>=</mo>  <mrow>   <mfrac>    <mrow>     <msubsup>      <mi>&#x3a3;</mi>      <mrow>       <mi>x</mi>       <mo>=</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>1</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <msub>        <mi>f</mi>        <mn>1</mn>       </msub>       <mo>(</mo>       <mrow>        <mi>x</mi>        <mo>&#x2062;</mo>        <mn>4</mn>       </mrow>       <mo>)</mo>      </mrow>     </msubsup>     <mo>&#x2062;</mo>     <msubsup>      <mi>&#x3a3;</mi>      <mrow>       <mi>y</mi>       <mo>=</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>2</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mrow>       <msub>        <mi>f</mi>        <mn>2</mn>       </msub>       <mo>(</mo>       <mrow>        <mi>y</mi>        <mo>&#x2062;</mo>        <mn>4</mn>       </mrow>       <mo>)</mo>      </mrow>     </msubsup>     <mo>&#x2062;</mo>     <mn>1</mn>     <mo>&#x2062;</mo>     <mrow>      <mo>(</mo>      <mrow>       <mi>x</mi>       <mo>,</mo>       <mi>y</mi>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mrow>     <mrow>      <mo>(</mo>      <mrow>       <mrow>        <msub>         <mi>f</mi>         <mn>1</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <mn>4</mn>        </mrow>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>1</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>x</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mo>&#xd7;</mo>     <mrow>      <mo>(</mo>      <mrow>       <mrow>        <msub>         <mi>f</mi>         <mn>2</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mn>4</mn>        </mrow>        <mo>)</mo>       </mrow>       <mo>-</mo>       <mrow>        <msub>         <mi>f</mi>         <mn>2</mn>        </msub>        <mo>(</mo>        <mrow>         <mi>y</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>    </mrow>   </mfrac>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0079" num="0000">l(x, y) denotes brightness in a projection area position whose coordinate is (x, y) on the image sensor at the current acquisition moment.</p><p id="p-0080" num="0071">In S<b>240</b>, estimated exposure brightness information is determined according to the current exposure brightness information in the current projection area position and the current exposure brightness information in the predicted projection area position.</p><p id="p-0081" num="0072">In this embodiment, after the current exposure brightness information of the target object in the current projection area position and at the current acquisition moment and the current exposure brightness information of the target object in the predicted projection area position and at the current acquisition moment are obtained, a weighted average calculation is performed according to a preset weight ratio, and an obtained weighted average brightness value is used as the estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment. For example, the weighted average brightness value is L=m1&#xd7;L<sub>current1</sub>+m2&#xd7;L<sub>current2</sub>, where m1+m2=1.</p><p id="p-0082" num="0073">The above implementation has the following advantage: considering that the predicted projection area position obtained through prediction is not necessarily very accurate and may have a certain error, when exposure brightness of the target object in the predicted projection area position is estimated, reference cannot be made to only brightness in the predicted projection area position and also needs to be made to brightness of the target object in the current projection area position on the image sensor, and accurate exposure brightness of the target object in the predicted projection area position and at the new acquisition moment can be obtained only in conjunction with brightness in two projection area positions.</p><p id="p-0083" num="0074">In S<b>250</b>, an exposure parameter of the target object in the predicted projection area position is adjusted according to a type of the target object in the current captured image and the estimated exposure brightness information when the new acquisition moment arrives.</p><p id="p-0084" num="0075">In S<b>260</b>, a new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter.</p><p id="p-0085" num="0076">An image capturing scheme is provided in the embodiment of the present application. With the scheme in this embodiment, projection areas of different target objects on the image sensor and at a subsequent acquisition moment can be automatically detected, and an appropriate exposure parameter can be assigned to a projection area where each target object is located according to exposure brightness and to-be-reached target exposure brightness of the projection area where the target object is located so that it is ensured that multiple target objects can all reach desired capturing brightness in a capturing process and it can be ensured that a target object easy to darken or a target object easy to brighten can reach desired brightness. Meanwhile, when estimated exposure brightness of the target object in a predicted projection area is determined, current exposure brightness of the target object in the current projection area position and the predicted projection area position is fully considered so that the obtained estimated exposure brightness approximates better to the exposure brightness in the predicted projection area position in an actual scenario.</p><p id="p-0086" num="0077"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart of another image capturing method according to an embodiment of the present application. Based on the preceding embodiments, S<b>120</b> and S<b>250</b> in the above embodiments are described in the embodiment of the present application. The embodiment of the present application may be combined with an optional solution in one or more of the preceding embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the image capturing method in this embodiment includes the steps below.</p><p id="p-0087" num="0078">In S<b>410</b>, a predicted projection area position of a target object in a current captured image on an image sensor and at a new acquisition moment is determined, and estimated exposure brightness information of the target object in the current captured image in the predicted projection area position and at the new acquisition moment is determined.</p><p id="p-0088" num="0079">In S<b>420</b>, target exposure brightness information associated with a type of the target object in the captured image is determined.</p><p id="p-0089" num="0080">In this embodiment, the current captured image may include multiple target objects, and each target object corresponds to one projection area on the image sensor. Thus, the image sensor includes multiple projection areas. Different types of target object require that different target exposure brightness be satisfied. Therefore, when an exposure adjustment is separately performed on each target object, the target exposure brightness information associated with the type of each target object needs to be determined so that whether to adjust an exposure parameter is determined according to the target exposure brightness information.</p><p id="p-0090" num="0081">In S<b>430</b>, the exposure parameter of the target object in the current captured image in the predicted projection area position is adjusted according to the target exposure brightness information and the estimated exposure brightness information when the new acquisition moment arrives.</p><p id="p-0091" num="0082">In this embodiment, for each target object in the current captured image, brightness difference information between the target exposure brightness information associated with the type of the target object and the estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment is determined. If it is detected that the brightness difference information does not satisfy preset difference information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives is adjusted according to the brightness difference information. If it is detected that the brightness difference information satisfies the preset difference information, the exposure parameter of the target object in the predicted projection area position is not adjusted and the original exposure parameter is maintained.</p><p id="p-0092" num="0083">For example, <figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of projection areas on an image sensor according to an embodiment of the present application. Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, projection areas of all target objects in the current captured image on the image sensor include R1, R2 and R3, and the exposure brightness of multiple target objects in their corresponding projection area positions is denoted as Lr1, Lr2 and Lr3, separately. Lr1, Lr2 and Lr3 are compared with target exposure brightness T1 associated with Lr1, target exposure brightness T2 associated with Lr2 and target exposure brightness T3 associated with Lr3, respectively. Then, automatic exposure (AE) is adjusted in real time and corresponding exposure parameters are delivered to multiple projection areas on the image sensor until the exposure brightness of the target objects in the projection area positions reaches desired target exposure brightness values.</p><p id="p-0093" num="0084">In this embodiment, optionally, the step in which the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives is adjusted according to the brightness difference information includes: determining exposure amount adjustment information of the target object in the predicted projection area position according to an exposure brightness difference value included in the brightness difference information; and adjusting the exposure parameter of the target object in the predicted projection area position on the image sensor is adjusted according to the exposure amount adjustment information when the new acquisition moment arrives. For example, if the exposure brightness of the target object is lower than the target exposure brightness, T1=Lr1+&#x3b4; and the exposure parameter in the predicted projection area position needs to be adjusted through an increase of an exposure amount; if the exposure brightness of the target object is higher than the target exposure brightness, T1=Lr1&#x2212;&#x3b4; and the exposure parameter in the predicted projection area position needs to be adjusted through a decrease of the exposure amount; where &#x3b4; denotes a calculated increase or decrease of the exposure amount.</p><p id="p-0094" num="0085">In S<b>440</b>, a new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter.</p><p id="p-0095" num="0086">In an optional implementation of this embodiment, before the new captured image is acquired according to the adjusted exposure parameter, the image capturing method in this embodiment further includes the steps C<b>10</b> and C<b>20</b> below.</p><p id="p-0096" num="0087">In step C<b>10</b>, the projection area of the target object on the image sensor is removed from a photometry area, and photometry is performed on the photometry area after removal.</p><p id="p-0097" num="0088">In step C<b>20</b>, exposure amount information of the photometry area is determined according to a photometry result, and an exposure parameter used by a non-target object in a projection area on the image sensor and at the new acquisition moment is adjusted according to the exposure amount information of the photometry area.</p><p id="p-0098" num="0089">In this implementation, <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of a photometry area according to an embodiment of the present application. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, for the projection area of the non-target object on the image sensor, the projection area of the target object on the image sensor is removed for statistics, and the photometry area and a weight are consistent with those in the absence of the target object. That is, information in the projection area of the target object on the image sensor is subtracted from the photometry area, and the information in the projection area of the target object on the image sensor is also subtracted from a non-photometry area. Then, the brightness of the whole image is calculated according to the weight, and the exposure parameter of the non-target object in the projection area on the image sensor is provided. Weighted average brightness of the non-target object in the projection area on the image sensor is expressed as follows:</p><p id="p-0099" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mrow>   <mrow>    <mi>L</mi>    <mo>&#x2062;</mo>    <mi>r</mi>    <mo>&#x2062;</mo>    <mn>0</mn>   </mrow>   <mo>=</mo>   <mfrac>    <mtable>     <mtr>      <mtd>       <mrow>        <mrow>         <msubsup>          <mi>&#x3a3;</mi>          <mrow>           <mi>p</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>P</mi>         </msubsup>         <mo>&#x2062;</mo>         <msubsup>          <mi>&#x3a3;</mi>          <mrow>           <mi>q</mi>           <mo>=</mo>           <mn>1</mn>          </mrow>          <mi>Q</mi>         </msubsup>         <mo>&#x2062;</mo>         <mi>pix</mi>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mi>p</mi>           <mo>,</mo>           <mi>q</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#xd7;</mo>         <mi>&#x3c9;</mi>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mi>p</mi>           <mo>,</mo>           <mi>q</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>-</mo>       </mrow>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <msubsup>         <mi>&#x3a3;</mi>         <mrow>          <mi>i</mi>          <mo>=</mo>          <mn>1</mn>         </mrow>         <mi>n</mi>        </msubsup>        <mo>&#x2062;</mo>        <msubsup>         <mi>&#x3a3;</mi>         <mrow>          <mi>p</mi>          <mo>=</mo>          <msub>           <mi>x</mi>           <mrow>            <mo>(</mo>            <mrow>             <mrow>              <mn>2</mn>              <mo>&#xd7;</mo>              <mi>i</mi>             </mrow>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>          </msub>         </mrow>         <msub>          <mi>x</mi>          <mrow>           <mo>(</mo>           <mrow>            <mn>2</mn>            <mo>&#xd7;</mo>            <mi>i</mi>           </mrow>           <mo>)</mo>          </mrow>         </msub>        </msubsup>        <mo>&#x2062;</mo>        <msubsup>         <mi>&#x3a3;</mi>         <mrow>          <mi>q</mi>          <mo>=</mo>          <msub>           <mi>y</mi>           <mrow>            <mo>(</mo>            <mrow>             <mrow>              <mn>2</mn>              <mo>&#xd7;</mo>              <mi>i</mi>             </mrow>             <mo>-</mo>             <mn>1</mn>            </mrow>            <mo>)</mo>           </mrow>          </msub>         </mrow>         <msub>          <mi>y</mi>          <mrow>           <mo>(</mo>           <mrow>            <mn>2</mn>            <mo>&#xd7;</mo>            <mi>i</mi>           </mrow>           <mo>)</mo>          </mrow>         </msub>        </msubsup>        <mo>&#x2062;</mo>        <mi>pix</mi>        <mo>&#x2062;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mi>p</mi>          <mo>,</mo>          <mi>q</mi>         </mrow>         <mo>)</mo>        </mrow>        <mo>&#xd7;</mo>        <mi>&#x3c9;</mi>        <mo>&#x2062;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mi>p</mi>          <mo>,</mo>          <mi>q</mi>         </mrow>         <mo>)</mo>        </mrow>       </mrow>      </mtd>     </mtr>    </mtable>    <mrow>     <mrow>      <msubsup>       <mi>&#x3a3;</mi>       <mrow>        <mi>p</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>P</mi>      </msubsup>      <mo>&#x2062;</mo>      <msubsup>       <mi>&#x3a3;</mi>       <mrow>        <mi>q</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>Q</mi>      </msubsup>      <mo>&#x2062;</mo>      <mrow>       <mi>&#x3c9;</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>p</mi>        <mo>,</mo>        <mi>q</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>-</mo>     <mrow>      <msubsup>       <mi>&#x3a3;</mi>       <mrow>        <mi>i</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mi>n</mi>      </msubsup>      <mo>&#x2062;</mo>      <msubsup>       <mi>&#x3a3;</mi>       <mrow>        <mi>p</mi>        <mo>=</mo>        <msub>         <mi>x</mi>         <mrow>          <mo>(</mo>          <mrow>           <mrow>            <mn>2</mn>            <mo>&#xd7;</mo>            <mi>i</mi>           </mrow>           <mo>-</mo>           <mn>1</mn>          </mrow>          <mo>)</mo>         </mrow>        </msub>       </mrow>       <msub>        <mi>x</mi>        <mrow>         <mo>(</mo>         <mrow>          <mn>2</mn>          <mo>&#xd7;</mo>          <mi>i</mi>         </mrow>         <mo>)</mo>        </mrow>       </msub>      </msubsup>      <mo>&#x2062;</mo>      <msubsup>       <mi>&#x3a3;</mi>       <mrow>        <mi>q</mi>        <mo>=</mo>        <msub>         <mi>y</mi>         <mrow>          <mo>(</mo>          <mrow>           <mrow>            <mn>2</mn>            <mo>&#xd7;</mo>            <mi>i</mi>           </mrow>           <mo>-</mo>           <mn>1</mn>          </mrow>          <mo>)</mo>         </mrow>        </msub>       </mrow>       <msub>        <mi>y</mi>        <mrow>         <mo>(</mo>         <mrow>          <mn>2</mn>          <mo>&#xd7;</mo>          <mi>i</mi>         </mrow>         <mo>)</mo>        </mrow>       </msub>      </msubsup>      <mo>&#x2062;</mo>      <mrow>       <mi>&#x3c9;</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>p</mi>        <mo>,</mo>        <mi>q</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mfrac>  </mrow>  <mo>;</mo> </mrow></math></maths></p><p id="p-0100" num="0090">where &#x3c9;(p, q) denotes a weight of a pixel (p, q) in a captured image or on an imaging plane of the image sensor. With pix(p, q)=shutter(p, q)&#xd7;gain(p, q) known, the exposure parameter of the non-target object in the projection area on the image sensor after stable exposure may be obtained according to ABS(Lr0&#x2212;L0)&#x3c;&#x3b4;1, and the exposure parameter is denotes as a shutter parameter shutter0 and a gain parameter gain0, where pix denotes a pixel, Lr0 denotes the weighted average brightness of the non-target object in the projection area on the image sensor, L0 denotes a target brightness value of the non-target object in the projection area on the image sensor and is a constant, &#x3b4;1 denotes an error range, P and Q denote a width and a height of the captured image, respectively, and n denotes the number of target objects in the captured image.</p><p id="p-0101" num="0091">In this implementation, considering the stability of exposure and the degree of natural transition in brightness between the projection area of the target object on the image sensor and the projection area of the adjacent non-target object on the image sensor, an adjustment range of the exposure parameter of the target object in the projection area on the image sensor is limited. Thus, an exposure parameter gain_i for exposure adjustment of the target object in the projection area on the image sensor is obtained as follows:</p><p id="p-0102" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>ABS(<i>Lri&#x2212;Li</i>)&#x3c;&#x3b4;2,gain&#x2208;[gain0&#x2212;&#x3b5;1,gain0+&#x3b5;1],<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0103" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>shutter &#x2208;[shutter0&#x2212;&#x3b5;2,shutter0+&#x3b5;2];<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0104" num="0092">where gain &#x2208; [gain0&#x2212;&#x3b5;1, gain0+&#x3b5;1] denotes a range of a gain parameter, shutter &#x2208; [shutter0&#x2212;&#x3b5;2, shutter0+&#x3b5;2] denotes a range of a shutter parameter, Lri denotes average brightness of an i-th object in a projection area (for example, current exposure brightness of the target object and the weighted average brightness of the non-target object), and Li denotes a target brightness value of the i-th object in the projection area and is a constant.</p><p id="p-0105" num="0093">In this implementation, the exposure parameter may be adjusted in the following manner: a shutter and another exposure parameter of the target object in the prediction projection area on the image sensor are adjusted. Projection areas of non-target objects on the image sensor are mostly the background and include no target object, and an exposure principle that a shutter is slowed down and a gain is as small as possible is preferentially selected for the adjustment of the exposure parameter.</p><p id="p-0106" num="0094">In this implementation, the projection areas of multiple target objects on the image sensor and the projection area of the non-target object on the image sensor are dynamically and independently exposed, the independent exposure process may be applied in any time period regardless of the daytime and the night. As long as the exposure of the target object needs to be adjusted, each area is determined and the exposure is adjusted dynamically. Additionally, the projection areas of the target objects on the image sensor may be several specified areas where brightness statistics and exposure are the same as those described above, and a human face, a human body, a license plate, a vehicle body or another target object may be in the areas.</p><p id="p-0107" num="0095">An image capturing scheme is provided in the embodiment of the present application. With the scheme in this embodiment, projection areas of different target objects on the image sensor and at a subsequent acquisition moment can be automatically detected, and an appropriate exposure parameter can be assigned to the projection area where each target object is located according to exposure brightness and to-be-reached target exposure brightness of the projection area where the target object is located so that it is ensured that multiple target objects can all reach desired capturing brightness in a capturing process and, in particular, it can be ensured that a target object easy to darken or a target object easy to brighten can reach desired brightness.</p><p id="p-0108" num="0096"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a flowchart of another image capturing method according to an embodiment of the present application. The embodiment of the present application is described based on the preceding embodiments, and the embodiment of the present application may be combined with an optional solution in one or more of the preceding embodiments. As shown in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the image capturing method provided in the embodiment of the present application includes the steps below.</p><p id="p-0109" num="0097">In S<b>710</b>, at a new acquisition moment, a predicted projection area position of a target object in a current captured image on an image sensor and estimated exposure brightness information of the target object in the predicted projection area position are determined.</p><p id="p-0110" num="0098">In S<b>720</b>, an exposure parameter of the target object in the predicted projection area position is adjusted according to a type of the target object and the estimated exposure brightness information when the new acquisition moment arrives.</p><p id="p-0111" num="0099">In S<b>730</b>, a new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter.</p><p id="p-0112" num="0100">In S<b>740</b>, at least one of an image quality or a feature similarity of the target object included in the new captured image is determined.</p><p id="p-0113" num="0101">In this embodiment, after exposure capture is performed according to the adjusted exposure parameter and the new captured image is acquired, online evaluation may be performed on the image quality of the new captured image, so as to determine an image quality effect of the target object in the new captured image. If the target object is a human face, the image quality of the human face included in the new captured image may be evaluated and determined by at least one of: a human face brightness evaluation value Yl, a human face sharpness evaluation value Ts, a human face noise evaluation value Nn or a human face skin color evaluation value C. If the image quality is evaluated using the above four criteria and the weights of the above four criteria are W1, W2, W3 and W4, separately, the image quality of the human face in the captured image is FaceValue=Y1&#xd7;W1+Ts&#xd7;W2+Nn&#xd7;W3+C&#xd7;W4. If the target object is a license plate, the image quality of the license plate included in the new captured image may be evaluated and determined by at least one of: a license plate brightness evaluation value Ylc, a license plate sharpness evaluation value Tsc, a license plate noise evaluation value Nnc or a license plate color evaluation value Cc4. If the image quality is evaluated using the above four criteria and the weights of the above four criteria are Wc1, Wc2, Wc3 and Wc4, separately, the image quality of the license plate in the captured image is Car=Y1c&#xd7;Wc1+Tsc&#xd7;Wc2+Nnc&#xd7;Wc3+Cc&#xd7;Wc4.</p><p id="p-0114" num="0102">In this embodiment, after the new captured image is acquired, the feature similarity of the new captured image may also be determined, so as to determine a similarity of the target object in the new captured image. Optionally, the feature similarity of the new captured image is calculated through artificial intelligence (AI). Using the evaluation of the image quality of the human face as an example, the image quality of the human face and the feature similarity of the human face are evaluated. The process is described below.</p><p id="p-0115" num="0103">(1) Human Face Brightness Evaluation</p><p id="p-0116" num="0104">Average brightness of an image I of the human face is calculated as follows:</p><p id="p-0117" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mi>L</mi>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <mi>P</mi>     <mo>&#xd7;</mo>     <mi>Q</mi>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <mrow>    <msubsup>     <mo>&#x2211;</mo>     <mrow>      <mi>p</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>P</mi>    </msubsup>    <mrow>     <msubsup>      <mo>&#x2211;</mo>      <mrow>       <mi>q</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>Q</mi>     </msubsup>     <mrow>      <mi>Gray</mi>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mrow>        <mi>p</mi>        <mo>,</mo>        <mi>q</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>    </mrow>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0118" num="0105">where Gray(p, q) denotes a grayscale value of a pixel (p, q), and P and Q denote a width and a height of the image of the human face, respectively.</p><p id="p-0119" num="0106">A human face brightness evaluation function is as follows:</p><p id="p-0120" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mrow>   <mi>Y</mi>   <mo>&#x2062;</mo>   <mn>1</mn>  </mrow>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mtable>        <mtr>         <mtd>          <mtable>           <mtr>            <mtd>             <mtable>              <mtr>               <mtd>                <mrow>                 <mrow>                  <mfrac>                   <mrow>                    <mi>L</mi>                    <mo>-</mo>                    <mrow>                     <mi>t</mi>                     <mo>&#x2062;</mo>                     <mn>1</mn>                    </mrow>                   </mrow>                   <mrow>                    <mrow>                     <mi>t</mi>                     <mo>&#x2062;</mo>                     <mn>2</mn>                    </mrow>                    <mo>-</mo>                    <mrow>                     <mi>t</mi>                     <mo>&#x2062;</mo>                     <mn>1</mn>                    </mrow>                   </mrow>                  </mfrac>                  <mo>&#xd7;</mo>                  <mn>1</mn>                  <mo>&#x2062;</mo>                  <mn>00</mn>                 </mrow>                 <mo>,</mo>                 <mrow>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>1</mn>                  </mrow>                  <mo>&#x2264;</mo>                  <mi>L</mi>                  <mo>&#x3c;</mo>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>2</mn>                  </mrow>                 </mrow>                </mrow>               </mtd>              </mtr>              <mtr>               <mtd>                <mrow>                 <mn>100</mn>                 <mo>,</mo>                 <mrow>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>2</mn>                  </mrow>                  <mo>&#x2264;</mo>                  <mi>L</mi>                  <mo>&#x3c;</mo>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>3</mn>                  </mrow>                 </mrow>                </mrow>               </mtd>              </mtr>             </mtable>            </mtd>           </mtr>           <mtr>            <mtd>             <mrow>              <mrow>               <mn>100</mn>               <mo>-</mo>               <mrow>                <mfrac>                 <mrow>                  <mi>L</mi>                  <mo>-</mo>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>3</mn>                  </mrow>                 </mrow>                 <mrow>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>4</mn>                  </mrow>                  <mo>-</mo>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>3</mn>                  </mrow>                 </mrow>                </mfrac>                <mo>&#xd7;</mo>                <mn>60</mn>               </mrow>              </mrow>              <mo>,</mo>              <mrow>               <mrow>                <mi>t</mi>                <mo>&#x2062;</mo>                <mn>3</mn>               </mrow>               <mo>&#x2264;</mo>               <mi>L</mi>               <mo>&#x3c;</mo>               <mrow>                <mi>t</mi>                <mo>&#x2062;</mo>                <mn>4</mn>               </mrow>              </mrow>             </mrow>            </mtd>           </mtr>          </mtable>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mn>40</mn>           <mo>,</mo>           <mrow>            <mrow>             <mi>t</mi>             <mo>&#x2062;</mo>             <mn>4</mn>            </mrow>            <mo>&#x2264;</mo>            <mi>L</mi>            <mo>&#x3c;</mo>            <mrow>             <mi>t</mi>             <mo>&#x2062;</mo>             <mn>5</mn>            </mrow>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>      </mtd>     </mtr>     <mtr>      <mtd>       <mpadded width="-2.64706em" lspace="2.64706em" depth="0ex" height="0ex">        <mrow>         <mn>0</mn>         <mo>,</mo>         <mi>other</mi>        </mrow>       </mpadded>      </mtd>     </mtr>    </mtable>    <mo>;</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0121" num="0107">where t1=40, t2=110, t3=140, t4=180 and t5=195 denote boundary thresholds and may be adjusted according to actual requirements.</p><p id="p-0122" num="0108">(2) Human Face Sharpness Evaluation</p><p id="p-0123" num="0109">A fuzzy image of the human face in the new captured image is obtained by using a low-pass filter and the fuzzy image of the human face is subtracted from an original image of the human face, so as to obtain an edge image of the human face for representing sharpness of the human face. Assuming that the original image of the human face is I and the low-pass filter is F, the edge image of the human face is I<sub>1</sub>=I&#x2212;I&#xdf;F.</p><p id="p-0124" num="0110">Thus, an average value of the sharpness of the human face may be obtained as follows:</p><p id="p-0125" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mi>S</mi>  <mo>=</mo>  <mrow>   <mfrac>    <mn>1</mn>    <mrow>     <mi>P</mi>     <mo>&#xd7;</mo>     <mi>Q</mi>    </mrow>   </mfrac>   <mo>&#x2062;</mo>   <msubsup>    <mi>&#x3a3;</mi>    <mrow>     <mi>p</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>    <mi>P</mi>   </msubsup>   <mo>&#x2062;</mo>   <msubsup>    <mi>&#x3a3;</mi>    <mrow>     <mi>q</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>    <mi>Q</mi>   </msubsup>   <mo>&#x2062;</mo>   <mrow>    <mrow>     <msub>      <mi>I</mi>      <mn>1</mn>     </msub>     <mo>(</mo>     <mrow>      <mi>p</mi>      <mo>,</mo>      <mi>q</mi>     </mrow>     <mo>)</mo>    </mrow>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0126" num="0111">A human face sharpness evaluation function is as follows:</p><p id="p-0127" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mi>Ts</mi>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mtable>    <mtr>     <mtd>      <mrow>       <mrow>        <mfrac>         <mrow>          <mi>S</mi>          <mo>-</mo>          <mrow>           <mi>t</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>         </mrow>         <mrow>          <mrow>           <mi>t</mi>           <mo>&#x2062;</mo>           <mn>2</mn>          </mrow>          <mo>-</mo>          <mrow>           <mi>t</mi>           <mo>&#x2062;</mo>           <mn>1</mn>          </mrow>         </mrow>        </mfrac>        <mo>&#xd7;</mo>        <mn>1</mn>        <mo>&#x2062;</mo>        <mn>00</mn>       </mrow>       <mo>,</mo>       <mrow>        <mrow>         <mi>t</mi>         <mo>&#x2062;</mo>         <mn>1</mn>        </mrow>        <mo>&#x2264;</mo>        <mi>S</mi>        <mo>&#x3c;</mo>        <mrow>         <mi>t</mi>         <mo>&#x2062;</mo>         <mn>2</mn>        </mrow>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mrow>        <mn>1</mn>        <mo>&#x2062;</mo>        <mn>00</mn>       </mrow>       <mo>,</mo>       <mrow>        <mrow>         <mi>t</mi>         <mo>&#x2062;</mo>         <mn>2</mn>        </mrow>        <mo>&#x2264;</mo>        <mi>S</mi>        <mo>&#x3c;</mo>        <mrow>         <mi>t</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mrow>        <mrow>         <mn>1</mn>         <mo>&#x2062;</mo>         <mn>0</mn>         <mo>&#x2062;</mo>         <mn>0</mn>        </mrow>        <mo>-</mo>        <mrow>         <mfrac>          <mrow>           <mi>S</mi>           <mo>-</mo>           <mrow>            <mi>t</mi>            <mo>&#x2062;</mo>            <mn>3</mn>           </mrow>          </mrow>          <mrow>           <mrow>            <mi>t</mi>            <mo>&#x2062;</mo>            <mn>4</mn>           </mrow>           <mo>-</mo>           <mrow>            <mi>t</mi>            <mo>&#x2062;</mo>            <mn>3</mn>           </mrow>          </mrow>         </mfrac>         <mo>&#xd7;</mo>         <mn>1</mn>         <mo>&#x2062;</mo>         <mn>00</mn>        </mrow>       </mrow>       <mo>,</mo>       <mrow>        <mrow>         <mi>t</mi>         <mo>&#x2062;</mo>         <mn>3</mn>        </mrow>        <mo>&#x2264;</mo>        <mi>S</mi>        <mo>&#x3c;</mo>        <mrow>         <mi>t</mi>         <mo>&#x2062;</mo>         <mn>4</mn>        </mrow>       </mrow>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mpadded width="-4.05556em" lspace="4.05556em" depth="0ex" height="0ex">       <mrow>        <mn>0</mn>        <mo>,</mo>        <mtext>&#x205f;</mtext>        <mi>other</mi>       </mrow>      </mpadded>     </mtd>    </mtr>   </mtable>  </mrow> </mrow></math></maths></p><p id="p-0128" num="0112">where t1, t2, t3, and t4 denote boundary thresholds and may be adjusted according to actual requirements.</p><p id="p-0129" num="0113">(3) Human Face Noise Evaluation</p><p id="p-0130" num="0114">A noise-reduced image of the human face is obtained using a smoothing filter and the noise-reduced image of the human face is subtracted from the original image, so as to obtain a noise image. Assuming that the original image is I and the smoothing filter is G, the noise image is I<sub>2</sub>=I&#x2212;I &#xdf;G.</p><p id="p-0131" num="0115">Thus, an average value of noise of the human face may be obtained as follows:</p><p id="p-0132" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <mrow>   <mi>N</mi>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mrow>      <mi>P</mi>      <mo>&#xd7;</mo>      <mi>Q</mi>     </mrow>    </mfrac>    <mo>&#x2062;</mo>    <msubsup>     <mi>&#x3a3;</mi>     <mrow>      <mi>p</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>P</mi>    </msubsup>    <mo>&#x2062;</mo>    <msubsup>     <mi>&#x3a3;</mi>     <mrow>      <mi>q</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>Q</mi>    </msubsup>    <mo>&#x2062;</mo>    <mrow>     <msub>      <mi>I</mi>      <mn>2</mn>     </msub>     <mo>(</mo>     <mrow>      <mi>p</mi>      <mo>,</mo>      <mi>q</mi>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mrow>  <mo>;</mo> </mrow></math></maths></p><p id="p-0133" num="0116">where p, q denotes a subscript of a pixel in the image, and P and Q denote the width and the height of the image of the human face, respectively.</p><p id="p-0134" num="0117">A human face noise evaluation function is as follows:</p><p id="p-0135" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mrow>  <mi>Nn</mi>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mtable>        <mtr>         <mtd>          <mtable>           <mtr>            <mtd>             <mrow>              <mn>100</mn>              <mo>,</mo>              <mrow>               <mi>N</mi>               <mo>&#x2264;</mo>               <mrow>                <mi>t</mi>                <mo>&#x2062;</mo>                <mn>1</mn>               </mrow>              </mrow>             </mrow>            </mtd>           </mtr>           <mtr>            <mtd>             <mrow>              <mrow>               <mn>100</mn>               <mo>-</mo>               <mrow>                <mfrac>                 <mrow>                  <mi>N</mi>                  <mo>-</mo>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>1</mn>                  </mrow>                 </mrow>                 <mrow>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>2</mn>                  </mrow>                  <mo>-</mo>                  <mrow>                   <mi>t</mi>                   <mo>&#x2062;</mo>                   <mn>1</mn>                  </mrow>                 </mrow>                </mfrac>                <mo>&#xd7;</mo>                <mn>100</mn>               </mrow>              </mrow>              <mo>,</mo>              <mrow>               <mrow>                <mi>t</mi>                <mo>&#x2062;</mo>                <mn>1</mn>               </mrow>               <mo>&#x2264;</mo>               <mi>N</mi>               <mo>&#x3c;</mo>               <mrow>                <mi>t</mi>                <mo>&#x2062;</mo>                <mn>2</mn>               </mrow>              </mrow>             </mrow>            </mtd>           </mtr>          </mtable>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <mfrac>             <mrow>              <mo>(</mo>              <mrow>               <mi>N</mi>               <mo>-</mo>               <mrow>                <mi>t</mi>                <mo>&#x2062;</mo>                <mn>2</mn>               </mrow>              </mrow>              <mo>)</mo>             </mrow>             <mrow>              <mrow>               <mi>t</mi>               <mo>&#x2062;</mo>               <mn>3</mn>              </mrow>              <mo>-</mo>              <mrow>               <mi>t</mi>               <mo>&#x2062;</mo>               <mn>2</mn>              </mrow>             </mrow>            </mfrac>            <mo>&#xd7;</mo>            <mn>100</mn>           </mrow>           <mo>,</mo>           <mrow>            <mrow>             <mi>t</mi>             <mo>&#x2062;</mo>             <mn>2</mn>            </mrow>            <mo>&#x2264;</mo>            <mi>N</mi>            <mo>&#x3c;</mo>            <mrow>             <mi>t</mi>             <mo>&#x2062;</mo>             <mn>3</mn>            </mrow>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mn>0</mn>        <mo>,</mo>        <mi>other</mi>       </mrow>      </mtd>     </mtr>    </mtable>    <mo>;</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0136" num="0118">where t1=1, t2=3 and t3=5 denote boundary thresholds and may be adjusted according to actual requirements.</p><p id="p-0137" num="0119">(4) Human Face Skin Color Evaluation</p><p id="p-0138" num="0120">Information about U and V channels of the human face in the new captured image is extracted, so as to obtain an average difference in a color gamut as</p><p id="p-0139" num="0000"><maths id="MATH-US-00012" num="00012"><math overflow="scroll"> <mrow>  <mfrac>   <mn>1</mn>   <mrow>    <mi>P</mi>    <mo>&#xd7;</mo>    <mi>Q</mi>   </mrow>  </mfrac>  <mo>&#x2062;</mo>  <msubsup>   <mi>&#x3a3;</mi>   <mrow>    <mi>p</mi>    <mo>=</mo>    <mn>1</mn>   </mrow>   <mi>P</mi>  </msubsup>  <mo>&#x2062;</mo>  <mrow>   <mrow>    <msubsup>     <mi>&#x3a3;</mi>     <mrow>      <mi>q</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>Q</mi>    </msubsup>    <mo>(</mo>    <mrow>     <mrow>      <mi>U</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>p</mi>       <mo>,</mo>       <mi>q</mi>      </mrow>      <mo>)</mo>     </mrow>     <mo>-</mo>     <mrow>      <mi>V</mi>      <mo>&#x2061;</mo>      <mo>(</mo>      <mrow>       <mi>p</mi>       <mo>,</mo>       <mi>q</mi>      </mrow>      <mo>)</mo>     </mrow>    </mrow>    <mo>)</mo>   </mrow>   <mo>.</mo>  </mrow> </mrow></math></maths></p><p id="p-0140" num="0121">A human face skin color evaluation function is as follows:</p><p id="p-0141" num="0000"><maths id="MATH-US-00013" num="00013"><math overflow="scroll"> <mrow>  <mi>Cf</mi>  <mo>=</mo>  <mrow>   <mo>{</mo>   <mrow>    <mtable>     <mtr>      <mtd>       <mtable>        <mtr>         <mtd>          <mtable>           <mtr>            <mtd>             <mrow>              <mrow>               <mn>100</mn>               <mo>&#xd7;</mo>               <mrow>                <mo maxsize="1">(</mo>                <msup>                 <mi>e</mi>                 <mrow>                  <mo>-</mo>                  <mfrac>                   <msup>                    <mrow>                     <mo>(</mo>                     <mrow>                      <mi>f</mi>                      <mo>-</mo>                      <mi>A</mi>                     </mrow>                     <mo>)</mo>                    </mrow>                    <mn>2</mn>                   </msup>                   <mrow>                    <mn>2</mn>                    <mo>&#xd7;</mo>                    <mrow>                     <mo>(</mo>                     <msup>                      <mrow>                       <mo>(</mo>                       <mfrac>                        <mrow>                         <mi>m</mi>                         <mo>&#x2062;</mo>                         <mn>1</mn>                        </mrow>                        <mn>2</mn>                       </mfrac>                       <mo>)</mo>                      </mrow>                      <mn>2</mn>                     </msup>                     <mo>)</mo>                    </mrow>                   </mrow>                  </mfrac>                 </mrow>                </msup>                <mo maxsize="1">)</mo>               </mrow>              </mrow>              <mo>,</mo>              <mrow>               <mi>f</mi>               <mo>&#x2264;</mo>               <mrow>                <mi>A</mi>                <mo>-</mo>                <mn>1</mn>               </mrow>              </mrow>             </mrow>            </mtd>           </mtr>           <mtr>            <mtd>             <mrow>              <mn>100</mn>              <mo>,</mo>              <mrow>               <mrow>                <mi>A</mi>                <mo>-</mo>                <mn>1</mn>               </mrow>               <mo>&#x2264;</mo>               <mi>f</mi>               <mo>&#x3c;</mo>               <mi>B</mi>              </mrow>             </mrow>            </mtd>           </mtr>          </mtable>         </mtd>        </mtr>        <mtr>         <mtd>          <mrow>           <mrow>            <mn>100</mn>            <mo>&#xd7;</mo>            <mrow>             <mo>(</mo>             <msup>              <mi>e</mi>              <mrow>               <mo>-</mo>               <mfrac>                <msup>                 <mrow>                  <mo>(</mo>                  <mrow>                   <mi>f</mi>                   <mo>-</mo>                   <mi>B</mi>                  </mrow>                  <mo>)</mo>                 </mrow>                 <mn>2</mn>                </msup>                <mrow>                 <mn>2</mn>                 <mo>&#xd7;</mo>                 <mrow>                  <mo>(</mo>                  <mrow>                   <mi>m</mi>                   <mo>&#x2062;</mo>                   <msup>                    <mn>1</mn>                    <mn>2</mn>                   </msup>                  </mrow>                  <mo>)</mo>                 </mrow>                </mrow>               </mfrac>              </mrow>             </msup>             <mo>)</mo>            </mrow>           </mrow>           <mo>,</mo>           <mrow>            <mi>B</mi>            <mo>&#x2264;</mo>            <mi>f</mi>            <mo>&#x3c;</mo>            <mrow>             <mi>B</mi>             <mo>+</mo>             <mn>2</mn>            </mrow>           </mrow>          </mrow>         </mtd>        </mtr>       </mtable>      </mtd>     </mtr>     <mtr>      <mtd>       <mrow>        <mrow>         <mrow>          <mo>-</mo>          <mi>k</mi>         </mrow>         <mo>&#xd7;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mi>f</mi>           <mo>-</mo>           <mi>B</mi>           <mo>-</mo>           <mn>2</mn>          </mrow>          <mo>)</mo>         </mrow>        </mrow>        <mo>,</mo>        <mi>other</mi>       </mrow>      </mtd>     </mtr>    </mtable>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0142" num="0122">If an average sum value of the color gamut is denoted as</p><p id="p-0143" num="0000"><maths id="MATH-US-00014" num="00014"><math overflow="scroll"> <mrow>  <mrow>   <mi>X</mi>   <mo>=</mo>   <mrow>    <mfrac>     <mn>1</mn>     <mrow>      <mi>P</mi>      <mo>&#xd7;</mo>      <mi>Q</mi>     </mrow>    </mfrac>    <mo>&#x2062;</mo>    <msubsup>     <mi>&#x3a3;</mi>     <mrow>      <mi>p</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>P</mi>    </msubsup>    <mo>&#x2062;</mo>    <mrow>     <msubsup>      <mi>&#x3a3;</mi>      <mrow>       <mi>q</mi>       <mo>=</mo>       <mn>1</mn>      </mrow>      <mi>Q</mi>     </msubsup>     <mo>(</mo>     <mrow>      <mrow>       <mi>U</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>p</mi>        <mo>,</mo>        <mi>q</mi>       </mrow>       <mo>)</mo>      </mrow>      <mo>+</mo>      <mrow>       <mi>V</mi>       <mo>&#x2061;</mo>       <mo>(</mo>       <mrow>        <mi>p</mi>        <mo>,</mo>        <mi>q</mi>       </mrow>       <mo>)</mo>      </mrow>     </mrow>     <mo>)</mo>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0144" num="0000">then A=m1&#x2212;((X&#x3e;3)&#xd7;(X&#x2212;m1))&#xd7;0.5, where m1=3, B=5, and k=0.7, the parameters in the formula may be adjusted according to actual requirements.</p><p id="p-0145" num="0123">In this case, an evaluation value of the image quality of the human face is FaceValue=Y1&#xd7;W1+Ts&#xd7;W2+Nn&#xd7;W3+Cf&#xd7;W4. W1+W2+W3+W4=1.</p><p id="p-0146" num="0124">The feature similarity of the human face is evaluated below.</p><p id="p-0147" num="0125">A Euclidean distance between feature coordinate points in a feature point set H of the human face in the new captured image is calculated so as to obtain a distance matrix DS.</p><p id="p-0148" num="0126">It is assumed that the feature point set H of the human face including n points in a two-dimensional space is H={(xi, yi)}, where i=1, 2, . . . , n, and (xi, yi) denotes a coordinate of an i-th feature point.</p><p id="p-0149" num="0127">The distance matrix of H is defined as DSi,j={&#x221a;{square root over ((xi&#x2212;xj)<sup>2</sup>+(yi&#x2212;yj)<sup>2</sup>)}}, where i=1, 2, . . . , n, and j=1, 2, . . . , n.</p><p id="p-0150" num="0128">The following elements in DS constitute a feature vector Tf, where Tf={di,j|i&#x2265;j; i=1, 2, . . . , n; j=1, 2, . . . , n}, where di,j denotes a distance between the i-th feature point and a j-th feature point.</p><p id="p-0151" num="0129">An average absolute error of two vectors is calculated and then normalized, so as to obtain the similarity of the human face.</p><p id="p-0152" num="0130">It can be seen that through the above process, the evaluation value of the image quality of the human face and an evaluation value of the feature similarity of the human face can be obtained, separately. Whether a capturing effect of the target object in the new captured image can meet a requirement of human eyes is determined according to the evaluation value of the image quality, and whether the capturing effect of the target object in the new captured image can meet an AI similarity recognition requirement is determined according to the evaluation value of the feature similarity of the human face.</p><p id="p-0153" num="0131">In S<b>750</b>, an image parameter of the new captured image is adjusted according to the image quality and/or the feature similarity so as to obtain the adjusted new captured image.</p><p id="p-0154" num="0132">In this embodiment, after the image quality and the feature similarity are obtained, whether target requirements are met may be determined according to the image quality and the feature similarity, so as to perform targeted adjustment on the image parameter of the new captured image. After the adjustment, real-time evaluation is performed and then the image parameter is adjusted, where the similarity is evaluated synchronously. In this manner, the image parameter is cyclically adjusted until the evaluation value of the image quality and the evaluation value of the feature similarity both meet the target requirements so that the adjusted new captured image can be obtained.</p><p id="p-0155" num="0133">An image capturing scheme is provided in the embodiment of the present application. With the scheme in this embodiment, projection areas of different target objects on the image sensor and at a subsequent acquisition moment can be automatically detected, and an appropriate exposure parameter can be assigned to a projection area where each target object is located according to exposure brightness and to-be-reached target exposure brightness of the projection area where the target object is located so that it is ensured that multiple target objects can all reach desired capturing brightness in a capturing process and, in particular, it can be ensured that a target object easy to darken or a target object easy to brighten can reach desired brightness. Meanwhile, the online evaluation is performed on the image quality, exposure is adjusted in real time, and ISP-related processing is performed at the same time, so as to ensure that the image quality of the target object can meet both an AI recognition requirement and a visual requirement of human eyes.</p><p id="p-0156" num="0134"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a block diagram of an image capturing apparatus according to an embodiment of the present application. The embodiment of the present application may be applied to the case where a target object in a captured scene is captured, for example, the case where multiple target objects in the captured scene are captured. The apparatus may be implemented by software and/or hardware and integrated on any electronic device having a network communication function. For example, the electronic device includes, but is not limited to, an electronic capture device, an electronic police device and the like. As shown in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, the image capturing apparatus provided in the embodiment of the present application includes an exposure brightness determination module <b>810</b>, an exposure parameter adjustment module <b>820</b> and an image exposure capture module <b>830</b>.</p><p id="p-0157" num="0135">The exposure brightness determination module <b>810</b> is configured to determine a predicted projection area position of a target object in a current captured image on an image sensor and at a new acquisition moment and determine estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment. The exposure parameter adjustment module <b>820</b> is configured to adjust, according to a type of the target object and the estimated exposure brightness information, an exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives. The image exposure capture module <b>830</b> is configured to acquire a new captured image at the new acquisition moment according to the adjusted exposure parameter, where both the new captured image and the current captured image include the target object.</p><p id="p-0158" num="0136">Based on the preceding embodiments, optionally, the exposure brightness determination module <b>810</b> is configured to perform the steps below.</p><p id="p-0159" num="0137">A current projection area position of the target object on the image sensor and at a current acquisition moment is determined. Current exposure brightness information of the target object in the current projection area position and at the current acquisition moment and current exposure brightness information of the target object in the predicted projection area position and at the current acquisition moment are determined. The estimated exposure brightness information is determined according to the current exposure brightness information in the current projection area position and the current exposure brightness information in the predicted projection area position.</p><p id="p-0160" num="0138">Based on the preceding embodiments, optionally, the exposure brightness determination module <b>810</b> is configured to perform the steps below.</p><p id="p-0161" num="0139">A current image position of the target object in the current captured image and at the current acquisition moment is determined. A motion of the target object is predicted according to the current image position and a predicted image position of the target object in the current captured image and at the new acquisition moment is determined. The predicted projection area position of the target object on the image sensor is determined according to the predicted image position.</p><p id="p-0162" num="0140">The exposure brightness determination module <b>810</b> is configured to perform the step below.</p><p id="p-0163" num="0141">The current projection area position of the target object on the image sensor and at the current acquisition moment is determined according to the current image position.</p><p id="p-0164" num="0142">Based on the preceding embodiments, optionally, the exposure parameter adjustment module <b>820</b> includes a target brightness determination unit and an exposure parameter adjustment unit.</p><p id="p-0165" num="0143">The target brightness determination unit is configured to determine target exposure brightness information associated with the type of the target object.</p><p id="p-0166" num="0144">The exposure parameter adjustment unit is configured to adjust, according to the target exposure brightness information and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</p><p id="p-0167" num="0145">Based on the preceding embodiments, optionally, the exposure parameter adjustment unit is configured to perform the steps below.</p><p id="p-0168" num="0146">Brightness difference information between the target exposure brightness information and the estimated exposure brightness information is determined. If it is detected that the brightness difference information does not satisfy preset difference information, the exposure parameter of the target object in the predicted projection area position is adjusted according to the brightness difference information when the new acquisition moment arrives.</p><p id="p-0169" num="0147">Based on the preceding embodiments, optionally, the apparatus further includes a captured image analysis module <b>840</b> and a captured image processing module <b>850</b>.</p><p id="p-0170" num="0148">The captured image analysis module <b>840</b> is configured to determine an image quality and/or a feature similarity of the target object included in the new captured image.</p><p id="p-0171" num="0149">The captured image processing module <b>850</b> is configured to adjust an image parameter of the new captured image according to the image quality and/or the feature similarity to obtain the adjusted new captured image.</p><p id="p-0172" num="0150">The image capturing apparatus provided in the embodiment of the present application may perform the image capturing method provided in any one of the preceding embodiments of the present application and has functions and effects corresponding to the image capturing method performed. For a process, reference is made to related operations in the image capturing method in the preceding embodiments.</p><p id="p-0173" num="0151"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a structural diagram of an electronic device according to an embodiment of the present application. As shown in <figref idref="DRAWINGS">FIG. <b>9</b></figref>, the electronic device provided in the embodiment of the present application includes one or more processors <b>910</b> and a storage apparatus <b>920</b>. One or more processors <b>910</b> may be disposed in the electronic device, where one processor <b>910</b> is shown as an example in <figref idref="DRAWINGS">FIG. <b>9</b></figref>. The storage apparatus <b>920</b> is configured to store one or more programs, where the one or more programs are executed by the one or more processors <b>910</b> to cause the one or more processors <b>910</b> to implement the image capturing method described in any one of the embodiments of the present application.</p><p id="p-0174" num="0152">The electronic device may further include an input apparatus <b>930</b> and an output apparatus <b>940</b>.</p><p id="p-0175" num="0153">The processor <b>910</b>, the storage apparatus <b>920</b>, the input apparatus <b>930</b> and the output apparatus <b>940</b> in the electronic device may be connected via a bus or in other manners, where the connection via a bus is shown as an example in <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0176" num="0154">As a computer-readable storage medium, the storage apparatus <b>920</b> in the electronic device may be configured to store one or more programs. The programs may be software programs, computer-executable programs and modules, such as program instructions/modules corresponding to the image capturing method provided in the embodiments of the present application. The processor <b>910</b> executes software programs, instructions and modules stored in the storage apparatus <b>920</b> to perform various function applications and data processing of the electronic device, that is, to implement the image capturing method in the preceding method embodiments.</p><p id="p-0177" num="0155">The storage apparatus <b>920</b> may include a program storage area and a data storage area, where the program storage area may store an operating system and an application program required by at least one function, and the data storage area may store the data created according to the use of the electronic device. Additionally, the storage apparatus <b>920</b> may include a high-speed random-access memory and may also include a non-volatile memory, such as at least one magnetic disk memory, a flash memory or another non-volatile solid-state memory. In some examples, the storage apparatus <b>920</b> may include memories, the memories are remotely disposed relative to the processor <b>910</b>, and these remote memories may be connected to the device via a network. Examples of the preceding network include, but are not limited to, the Internet, an intranet, a local area network, a mobile communication network and a combination thereof.</p><p id="p-0178" num="0156">The input apparatus <b>930</b> may be configured to receive inputted digital or character information and generate key signal input related to user settings and function control of the electronic device. The output apparatus <b>940</b> may include a display device such as a display screen.</p><p id="p-0179" num="0157">When executed by the one or more processors <b>910</b>, the one or more programs included in the above electronic device perform the operations below.</p><p id="p-0180" num="0158">A predicted projection area position of a target object in a current captured image on an image sensor and at a new acquisition moment is determined, and estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment is determined. An exposure parameter of the target object in the predicted projection area position is adjusted according to a type of the target object and the estimated exposure brightness information when the new acquisition moment arrives. A new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter, where both the new captured image and the current captured image include the target object.</p><p id="p-0181" num="0159">When executed by the one or more processors <b>910</b>, the one or more programs included in the above electronic device may also perform related operations in the image capturing method provided in any one of the embodiments of the present application.</p><p id="p-0182" num="0160">An embodiment of the present application provides a computer-readable storage medium, the storage medium is configured to store a computer program, and the computer program, when executed by a processor, is used for performing an image capturing method. The method includes the steps below.</p><p id="p-0183" num="0161">A predicted projection area position of a target object in a current captured image on an image sensor and at a new acquisition moment is determined, and estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment is determined. An exposure parameter of the target object in the predicted projection area position is adjusted according to a type of the target object and the estimated exposure brightness information when the new acquisition moment arrives. A new captured image is acquired at the new acquisition moment according to the adjusted exposure parameter, where both the new captured image and the current captured image include the target object.</p><p id="p-0184" num="0162">Optionally, when executed by the processor, the program may also be used for performing the image capturing method provided in any one of the embodiments of the present application.</p><p id="p-0185" num="0163">A computer storage medium in the embodiment of the present application may adopt any combination of one or more computer-readable media. The computer-readable media may be computer-readable signal media or computer-readable storage media. A computer-readable storage medium, for example, may be, but is not limited to, an electronic, magnetic, optical, electromagnetic, infrared or semiconductor system, apparatus or device or any combination thereof. Examples (a non-exhaustive list) of the computer-readable storage medium include an electrical connection having one or more wires, a portable computer magnetic disk, a hard disk, a random-access memory (RAM), a read-only memory (ROM), an erasable programmable read-only memory (EPROM), a flash memory, an optical fiber, a portable compact disc read-only memory (CD-ROM), an optical storage device, a magnetic storage device or any suitable combination thereof. The computer-readable storage medium may be any tangible medium including or storing a program. The program may be used by or used in conjunction with an instruction execution system, apparatus or device.</p><p id="p-0186" num="0164">A computer-readable signal medium may include a data signal propagated in a baseband or as part of a carrier. Computer-readable program codes are carried in the data signal. The data signal propagated in this manner may be in multiple forms and includes, but is not limited to, an electromagnetic signal, an optical signal or any suitable combination thereof. The computer-readable signal medium may also be any computer-readable medium except the computer-readable storage medium. The computer-readable medium may send, propagate or transmit a program used by or used in conjunction with an instruction execution system, apparatus or device.</p><p id="p-0187" num="0165">Program codes included in computer-readable media may be transmitted by using any suitable medium, including, but not limited to, a wireless medium, a wired medium, an optical cable, a radio frequency (RF) and the like or any suitable combination thereof.</p><p id="p-0188" num="0166">Computer program codes for performing the operations of the present application may be written in one or more programming languages or a combination thereof, the programming languages including object-oriented programming languages such as Java, Smalltalk and C++ and further including conventional procedural programming languages such as C programming language or similar programming languages. Program codes may be executed entirely on a user computer, partly on a user computer, as a stand-alone software package, partly on a user computer and partly on a remote computer, or entirely on a remote computer or a server. In the case where the remote computer is involved, the remote computer may be connected to the user computer via any type of network including a local area network (LAN) or a wide area network (WAN) or may be connected to an external computer (for example, via the Internet provided by an Internet service provider).</p><p id="p-0189" num="0167">In the description of the present application, the description of reference terms such as &#x201c;one embodiment&#x201d;, &#x201c;some embodiments&#x201d;, &#x201c;an example&#x201d;, &#x201c;a specific example&#x201d; or &#x201c;some examples&#x201d; means that a feature, structure, material or characteristic described in conjunction with the embodiment or the example is included in at least one embodiment or example of the present application. In the specification, the illustrative description of the preceding terms does not necessarily refer to the same embodiment or example. Moreover, described features, structures, materials or characteristics may be combined in an appropriate manner in any one or more embodiments or examples.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005239A1-20230105-M00001.NB"><img id="EMI-M00001" he="5.67mm" wi="76.20mm" file="US20230005239A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005239A1-20230105-M00002.NB"><img id="EMI-M00002" he="5.67mm" wi="76.20mm" file="US20230005239A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230005239A1-20230105-M00003.NB"><img id="EMI-M00003" he="7.37mm" wi="76.20mm" file="US20230005239A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005239A1-20230105-M00004.NB"><img id="EMI-M00004" he="7.37mm" wi="76.20mm" file="US20230005239A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005239A1-20230105-M00005.NB"><img id="EMI-M00005" he="12.70mm" wi="76.20mm" file="US20230005239A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005239A1-20230105-M00006.NB"><img id="EMI-M00006" he="6.01mm" wi="76.20mm" file="US20230005239A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005239A1-20230105-M00007.NB"><img id="EMI-M00007" he="21.51mm" wi="76.20mm" file="US20230005239A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005239A1-20230105-M00008.NB"><img id="EMI-M00008" he="6.01mm" wi="76.20mm" file="US20230005239A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230005239A1-20230105-M00009.NB"><img id="EMI-M00009" he="18.37mm" wi="76.20mm" file="US20230005239A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230005239A1-20230105-M00010.NB"><img id="EMI-M00010" he="6.01mm" wi="76.20mm" file="US20230005239A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011" nb-file="US20230005239A1-20230105-M00011.NB"><img id="EMI-M00011" he="18.37mm" wi="76.20mm" file="US20230005239A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00012" nb-file="US20230005239A1-20230105-M00012.NB"><img id="EMI-M00012" he="6.01mm" wi="76.20mm" file="US20230005239A1-20230105-M00012.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00013" nb-file="US20230005239A1-20230105-M00013.NB"><img id="EMI-M00013" he="20.83mm" wi="76.20mm" file="US20230005239A1-20230105-M00013.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00014" nb-file="US20230005239A1-20230105-M00014.NB"><img id="EMI-M00014" he="6.01mm" wi="76.20mm" file="US20230005239A1-20230105-M00014.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. An image capturing method, comprising:<claim-text>at a new acquisition moment, predicting a predicted projection area position of a target object in a current captured image on an image sensor and estimated exposure brightness information of the target object in the predicted projection area position;</claim-text><claim-text>adjusting, according to a type of the target object and the estimated exposure brightness information, an exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives; and</claim-text><claim-text>acquiring a new captured image at the new acquisition moment according to the adjusted exposure parameter, wherein both the new captured image and the current captured image comprise the target object.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein predicting the estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment comprises:<claim-text>determining a current projection area position of the target object on the image sensor and at a current acquisition moment;</claim-text><claim-text>at the current acquisition moment, determining first current exposure brightness information of the target object in the current projection area position and second current exposure brightness information of the target object in the predicted projection area position; and</claim-text><claim-text>determining the estimated exposure brightness information according to the first current exposure brightness information and the second current exposure brightness information.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein<claim-text>predicting the predicted projection area position of the target object in the current captured image on the image sensor and at the new acquisition moment comprises: determining a current image position of the target object in the current captured image and at the current acquisition moment; predicting a motion of the target object according to the current image position and determining a predicted image position of the target object in the current captured image and at the new acquisition moment; and determining the predicted projection area position of the target object on the image sensor at the new acquisition moment according to the predicted image position; and</claim-text><claim-text>determining the current projection area position of the target object on the image sensor and at the current acquisition moment comprises: determining the current projection area position of the target object on the image sensor and at the current acquisition moment according to the current image position.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein adjusting, according to the type of the target object and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives comprises:<claim-text>determining target exposure brightness information associated with the type of the target object; and</claim-text><claim-text>adjusting, according to the target exposure brightness information and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein adjusting, according to the target exposure brightness information and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives comprises:<claim-text>determining brightness difference information between the target exposure brightness information and the estimated exposure brightness information; and</claim-text><claim-text>in response to detecting that the brightness difference information does not satisfy preset difference information, adjusting, according to the brightness difference information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>determining at least one of an image quality or a feature similarity of the target object comprised in the new captured image; and</claim-text><claim-text>adjusting an image parameter of the new captured image according to at least one of the determined image quality or the determined feature similarity to obtain an adjusted new captured image.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. An image capturing apparatus, comprising:<claim-text>at least one processor; and</claim-text><claim-text>a storage apparatus configured to store at least one program;</claim-text><claim-text>wherein the at least one program is executed by the at least one processor to cause the at least one processor to implement the steps in an exposure brightness determination module, an exposure parameter adjustment module and an image exposure capture module; wherein</claim-text><claim-text>the exposure brightness determination module is configured to at a new acquisition moment, predict a predicted projection area position of a target object in a current captured image on an image sensor and estimated exposure brightness information of the target object in the predicted projection area position;</claim-text><claim-text>the exposure parameter adjustment module is configured to adjust, according to a type of the target object and the estimated exposure brightness information, an exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives; and</claim-text><claim-text>the image exposure capture module is configured to acquire a new captured image at the new acquisition moment according to the adjusted exposure parameter, wherein both the new captured image and the current captured image comprise the target object.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the exposure brightness determination module is configured to:<claim-text>determine a current projection area position of the target object on the image sensor and at a current acquisition moment;</claim-text><claim-text>at the current acquisition moment, determine first current exposure brightness information of the target object in the current projection area position and second current exposure brightness information of the target object in the predicted projection area position; and</claim-text><claim-text>determine the estimated exposure brightness information according to the first current exposure brightness information and the second current exposure brightness information.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. (canceled)</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A non-transitory computer-readable storage medium, wherein the storage medium is configured to store a computer program, and the computer program, when executed by a processor, implements:<claim-text>at a new acquisition moment, predicting a predicted projection area position of a target object in a current captured image on an image sensor and estimated exposure brightness information of the target object in the predicted projection area position;</claim-text><claim-text>adjusting, according to a type of the target object and the estimated exposure brightness information, an exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives; and</claim-text><claim-text>acquiring a new captured image at the new acquisition moment according to the adjusted exposure parameter, wherein both the new captured image and the current captured image comprise the target object.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The apparatus of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein<claim-text>the exposure brightness determination module is configured to: determine a current image position of the target object in the current captured image and at the current acquisition moment; predict a motion of the target object according to the current image position and determine a predicted image position of the target object in the current captured image and at the new acquisition moment; and determine the predicted projection area position of the target object on the image sensor at the new acquisition moment according to the predicted image position; and</claim-text><claim-text>the exposure brightness determination module is further configured to: determine the current projection area position of the target object on the image sensor and at the current acquisition moment according to the current image position.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the exposure parameter adjustment module comprises:<claim-text>a target brightness determination unit configured to determine target exposure brightness information associated with the type of the target object; and</claim-text><claim-text>an exposure parameter adjustment unit configured to adjust, according to the target exposure brightness information and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The apparatus of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the exposure parameter adjustment unit is configured to:<claim-text>determine brightness difference information between the target exposure brightness information and the estimated exposure brightness information; and</claim-text><claim-text>in response to detecting that the brightness difference information does not satisfy preset difference information, adjust, according to the brightness difference information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The apparatus of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>a captured image analysis module configured to determine at least one of an image quality or a feature similarity of the target object comprised in the new captured image; and</claim-text><claim-text>a captured image processing module configured to adjust an image parameter of the new captured image according to at least one of the determined image quality or the determined feature similarity to obtain an adjusted new captured image.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein predicting the estimated exposure brightness information of the target object in the predicted projection area position and at the new acquisition moment comprises:<claim-text>determining a current projection area position of the target object on the image sensor and at a current acquisition moment;</claim-text><claim-text>at the current acquisition moment, determining first current exposure brightness information of the target object in the current projection area position and second current exposure brightness information of the target object in the predicted projection area position; and</claim-text><claim-text>determining the estimated exposure brightness information according to the first current exposure brightness information and the second current exposure brightness information.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein<claim-text>predicting the predicted projection area position of the target object in the current captured image on the image sensor and at the new acquisition moment comprises: determining a current image position of the target object in the current captured image and at the current acquisition moment; predicting a motion of the target object according to the current image position and determining a predicted image position of the target object in the current captured image and at the new acquisition moment; and determining the predicted projection area position of the target object on the image sensor at the new acquisition moment according to the predicted image position; and</claim-text><claim-text>determining the current projection area position of the target object on the image sensor and at the current acquisition moment comprises: determining the current projection area position of the target object on the image sensor and at the current acquisition moment according to the current image position.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein adjusting, according to the type of the target object and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives comprises:<claim-text>determining target exposure brightness information associated with the type of the target object; and</claim-text><claim-text>adjusting, according to the target exposure brightness information and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein adjusting, according to the target exposure brightness information and the estimated exposure brightness information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives comprises:<claim-text>determining brightness difference information between the target exposure brightness information and the estimated exposure brightness information; and</claim-text><claim-text>in response to detecting that the brightness difference information does not satisfy preset difference information, adjusting, according to the brightness difference information, the exposure parameter of the target object in the predicted projection area position when the new acquisition moment arrives.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable storage medium of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the computer program further implements:<claim-text>determining at least one of an image quality or a feature similarity of the target object comprised in the new captured image; and</claim-text><claim-text>adjusting an image parameter of the new captured image according to at least one of the determined image quality or the determined feature similarity to obtain an adjusted new captured image.</claim-text></claim-text></claim></claims></us-patent-application>