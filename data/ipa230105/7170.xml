<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007171A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007171</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17744889</doc-number><date>20220516</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>225</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>33</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23238</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2259</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>332</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">WIDE AREA IMAGING SYSTEM AND METHOD</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17067118</doc-number><date>20201009</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11336824</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17744889</doc-number></document-id></child-doc></relation></continuation><division><relation><parent-doc><document-id><country>US</country><doc-number>16179459</doc-number><date>20181102</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10805533</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17067118</doc-number></document-id></child-doc></relation></division><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15794681</doc-number><date>20171026</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10122920</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16179459</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14070934</doc-number><date>20131104</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>9813618</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>15794681</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>61722120</doc-number><date>20121102</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Diversified Innovations Fund, LLLP</orgname><address><city>Scottsdale</city><state>AZ</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>GRIFFIS</last-name><first-name>Andrew J.</first-name><address><city>Tucson</city><state>AZ</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>POWELL</last-name><first-name>Michael B.</first-name><address><city>Tucson</city><state>AZ</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>HOWELL</last-name><first-name>Mark J.</first-name><address><city>Phoenix</city><state>AZ</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present invention provides a new and useful paradigm in wide area imaging, in which wide area imaging is provided by a step/dwell/image capture process and system to capture images and produce from the captured images a wide area image. The image capture is by a sensor that has a predetermined image field and provides image capture at a predetermined frame capture rate, and by a motorized step and dwell sequence of the sensor, where image capture is during a dwell, and the step and dwell sequence of the sensor is synchronized with the image capture rate of the sensor.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="185.84mm" wi="152.74mm" file="US20230007171A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="187.88mm" wi="154.86mm" file="US20230007171A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="188.98mm" wi="155.87mm" file="US20230007171A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="141.73mm" wi="155.87mm" file="US20230007171A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="216.66mm" wi="149.69mm" file="US20230007171A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="133.77mm" wi="145.54mm" file="US20230007171A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="93.39mm" wi="127.85mm" file="US20230007171A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="162.90mm" wi="155.53mm" file="US20230007171A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">RELATED APPLICATION/CLAIM OF PRIORITY</heading><p id="p-0002" num="0001">This application is a continuation of copending U.S. application Ser. No. 17/067,118 filed Oct. 9, 2020, which, in turn, is a divisional of U.S. application Ser. No. 16/179,459 filed Nov. 2, 2018, now U.S. Pat. No. 10,805,533, which, in turn, is a continuation of U.S. application Ser. No. 15/794,681 filed Oct. 26, 2017, now U.S. Pat. No. 10,122,920, which, in turn, is a continuation of U.S. application Ser. No. 14/070,934 filed Nov. 4, 2013, now U.S. Pat. No. 9,813,618, and which claims the priority of Provisional Application No. 61/722,120, filed Nov. 2, 2012. Each of the above-identified patent applications are incorporated by reference herein.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">The invention relates to the field of wide area, or, when imaged regions are all contiguous, panoramic imaging. The invention has particular utility for commercial security and surveillance but is readily used for other applications that require a low cost wide field of view device, particularly mid-wave and long-wave thermal infrared (MWIR, LWIR respectively).</p><p id="p-0004" num="0003">There are at least three ways to obtain a wide area image, including a 360 degree panorama, with a focal plane array (FPA): 1) use multiple copies of the FPA each having its own lens and field of view, and assign each of the FPAs to a particular segment of the wide area; 2) use projection optics to map pixels around the periphery to a common FPA, typically imaging the periphery onto an annulus within the FPA; 3) move or scan the FPA and lens so as to use a limited field of view device (i.e., &#x3c;360 degrees) to collect images in sequence that, when viewed together completely image the wide area, or in the case of a 360 degree panorama, comprise a 360 degree field of view.</p><p id="p-0005" num="0004">The choice of one method over another is often driven by the tradeoff between cost, frame rate (images per second) and sensitivity (amount of energy needed for a good image). Presently, reliable and relatively low cost MWIR and LWIR FPAs, e.g., uncooled microbolometers, are readily available. These are sometimes used in scenario (1) above, but require many FPA's, thereby making this scenario costly and less reliable for commercial applications. When these devices are used for scenario (3), they are inefficient in the use of pixels and also require an expensive peripheral window, both of which push cost upwards. The use of uncooled microbolometer FPAs continuously scanning a wide area can provide a low cost means of imaging, but the imaging speed is too low for real-time applications, owing to the low sensitivity, e.g., D*, of such thermal devices (as compared to cooled photonic devices) and the typically low sampling rate required by pixel thermal mass constraints. Thus even with a relatively low cost technology (uncooled microbolometers) it is yet relatively expensive to obtain a 360 degree field of view.</p><p id="p-0006" num="0005">Thus it is typical to use a relatively expensive sensor technology, e.g., cooled HgCdTe or something comparable, but use only a minimal size FPA, e.g., a few columns and a few hundred rows, so as to minimize the amount of expensive detector material and yet afford the opportunity to generate many resultant image pixels by using a simple, low cost mechanism&#x2014;a continuous scan, constant velocity rotary mechanism. Typically, the hundreds of rows are used to image the vertical dimension and the few columns are used to capture the horizontal, rotating, dimension and many columns are joined (&#x201c;stitched&#x201d;) together. Furthermore, TDI (time delay and integration) techniques can be used to integrate many &#x201c;looks&#x201d; per column together so as to accumulate signal and overcome any shortfall in sensitivity, even while mitigating the effects of motion induced blur.</p><p id="p-0007" num="0006">The resultant wide area imaging system can be very effective, but even with a good tradeoff in the choice of sensor technology and the number of pixels used, the overall solution has a relatively low mean time to failure which makes it expensive to maintain and the systems components are quite expensive, often much more expensive than the aforementioned multiple-FPA approach for uncooled microbolometers devices.</p><p id="p-0008" num="0007">The objective of the invention then, is to produce a time series of wide area images, i.e., having a field of view greater than that of the sensor by itself, so that object detection and related data reduction activities ensue. A traditional panorama involves a set of adjacent images that have some overlap that permits edges to be merged seamlessly into a single composite image. Such a panorama is within the capability of the invention, but only sometimes generated by the invention. This is due to the desire to minimize time spent on image areas that have little interest, leaving more time (and therefore higher rates of imaging or coverage) for image areas that have elevated interest. For instance, in a perimeter surveillance scenario where a fence line is to be monitored, the area adjacent to, but outside, the fence, is typically of more interest than areas inside the fence, so an imaging system would provide higher performance for detecting activity outside the fence if more time could be spent imaging those regions. This desire for a high degree of flexibility and control in the distribution of the imagery generated leads to a requirement for computer control of both timing of sensor imaging and the motion profile. Because the motion of the sensor is under computer control, e.g., a direct drive servo motor driven by a real time motor controller with encoder feedback, in the preferred embodiment, the sensor motion profile can be arbitrarily determined&#x2014;there is no pattern that cannot be commanded and generated on demand, adaptively when needed, within the range of velocities and positions enabled by specific choices of motor, amplifier, encoder and mass/inertia of the sensor assembly. The preferred embodiment uses a direct drive servo for the precision control it provides, but also for the longevity, consistency and reliability provided by the absence of gears, pulleys or otherwise friction-based mechanical assemblies. Such assemblies can be made to work with the invention, but will not be as broadly optimal as the direct drive servo motor implementation.</p><heading id="h-0003" level="1">SUMMARY OF THE PRESENT INVENTION</heading><p id="p-0009" num="0008">The present invention provides a new and useful paradigm in wide area imaging, which overcomes the types of issues discussed above.</p><p id="p-0010" num="0009">According to the present invention, wide area imaging, including panoramic imaging, is provided by a step/dwell/image capture process/system to capture images and produce from the captured images a wide area image. The image capture is by a sensor that has a predetermined image field and provides image capture at a predetermined frame capture rate, and by a processor controlled motorized step and dwell sequence of the sensor, where image capture is during a dwell, and the step and dwell sequence of the sensor is synchronized with the image capture rate of the sensor.</p><p id="p-0011" num="0010">In a preferred form of the system and method of the present invention, the step/dwell/image capture sequence is under processor control that is interfaced to a servo motor to selectively control the sensor position in a manner that is related to the step/dwell/image capture sequence.</p><p id="p-0012" num="0011">For this reason, the term, step, represents a variable move: the size of the step, speed of the step and timing of the step are completely within the ability of the controlling processor to specify upon demand, at any time. Thus a step is not a fixed quantity for the invention; it is a movement under control of a processor and associated electronic motor controller (e.g., <figref idref="DRAWINGS">FIG. <b>1</b></figref>).</p><p id="p-0013" num="0012">In addition, in a preferred aspect of the present invention, the sensor is located on a moveable platform, and movements of the platform that affect the wide area image produced from the captured image are measured and used to provide image compensation that is related to such movements of the platform. Moreover, when a subject, e.g. a human, is identified in the wide area image, the subject can be hailed or notified that its presence has been detected.</p><p id="p-0014" num="0013">The preferred wide area imaging system and method can also have one or more of the following features;</p><p id="p-0015" num="0014">a. The wide area system and method may include a processor that uses motion and/or object detection on a captured image or the wide area image to extract and localize objects of interest in the wide area image.</p><p id="p-0016" num="0015">b. The wide area system and method may be configured to provide variable step and dwell sequences of the sensor, to produce the wide area image using a processor, without modifying the movable platform mechanism.</p><p id="p-0017" num="0016">c. The wide area system and method may be configured to provide variable step and dwell sequences of the sensor, to enable the sensor to localize the sensor on selected image fields. Thus, if the system is being manually or automatically monitored, and the monitor observes an object of particular interest, the sensor can be localized on that object.</p><p id="p-0018" num="0017">d. The wide area system and method can have (or be controlled by) a processor configured to use measures of object detection or recognition or identification, probability, e.g., successive hypothesis testing, of detected objects in the wide area image to determine the step and dwell sequences of the sensor.</p><p id="p-0019" num="0018">e. In the wide area system and method the sensor can be configured to produce image capture in a manner that is useful in producing high density imagery, e.g., images having on the order of a million pixels or more, from or as the wide area image.</p><p id="p-0020" num="0019">f. The wide area system and method can be coupled to a control center (a location separate from the location of the invention from which a person could control and observe the invention operation) via an interface and configured to allow the control center access to subsets of the captured images, via the interface.</p><p id="p-0021" num="0020">g. The wide area system and method can have a sensor configured with one of the following sensing techniques: infrared, visible or other wavelengths to form an image using sensor pixels, including the collection of multiple wavelength images at the same time (multiple FPAs).</p><p id="p-0022" num="0021">h. The step/dwell/image capture sequence can be configured to synchronize the initiation of image capture by the sensor to a position to which the sensor is selectively moved under servo motor control.</p><p id="p-0023" num="0022">i. the step/dwell/image capture sequence can be configured to synchronize movement of the sensor to a selected position to the timing with which the sensor is controlled to initiate image capture.</p><p id="p-0024" num="0023">It should also be noted that the present invention overcomes the difficulties described above by using a low cost sensor with sufficient pixels for the application but overcomes the limitations of sensitivity and speed by way of a digitally controlled servo system (that controls the motorized step and dwell sequence) that accommodates the need for variable or programmable integration times while providing motion adequate to form a wide area image quickly and continuously, and to selectively control or vary the wide area image in real time.</p><p id="p-0025" num="0024">Additional features of the present invention will be apparent from the following detailed description and the accompanying drawings</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows the architecture of the invention that includes the imaging, actuation and data management subsystems while also indicating the network centric nature of the invention and its data products.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example geometry for the invention packaging, wherein the upper portion contains the imaging sensor and optics, ancillary sensor(s) (as needed), the middle section contains the motor, bearings, and support for power and data in the upper portion, while the lower portion contains the motion control, data processing, and ancillary sensor, e.g., inertial sensor, electronics with integral heat dissipation elements if needed;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example geometry for the invention packaging for the upper portion of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, wherein the upper portion contains the imaging sensor and optics, ancillary sensor(s), e.g., inertial, and a hailing device e.g., a laser, along with the requisite surroundings including wavelength appropriate windows for optical elements;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example installation of the invention mounted atop a pole and connecting to site infrastructure through a cabinet at the base of the pole;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of actuation and imaging timing where a step-dwell-step (etc) sequence is described to account for the sensor motion, sensor integration and readout of image data respectively;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an example of a sensor packaged with a lens and identifying the FPA and associated FPA/lens field of view;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows an example of actuation and imaging timing where a step-dwell-step (etc) sequence is described to account for the sensor motion, sensor integration and readout of image data respectively;</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0033" num="0032">As discussed above, the present invention relates to a new and useful paradigm in wide area imaging, including panoramic imaging, in which the wide area imaging is provided by a step/dwell/image capture, and as needed image stitch, process to capture images and produce from the captured images a wide area image. The image capture is by a sensor that has a predetermined image field and provides image capture at a predetermined frame capture rate, and by a motorized step and dwell sequence of the sensor, where image capture is during a dwell period or interval, and the step and dwell sequence of the sensor is synchronized with the image capture rate of the sensor. In addition, an image stitch is processor controlled and designed to combine images captured by the sensor in a predetermined manner to produce the wide area image.</p><p id="p-0034" num="0033">The principles of the present invention are described herein in connection with one exemplary system and method, and from that description the manner in which the principles of the present invention can be applied to various types of wide area imaging systems and methods will be apparent to those in the art.</p><p id="p-0035" num="0034">In this application, &#x201c;panorama&#x201d; or &#x201c;panoramic imaging&#x201d; means a scene that is taken in and produced by a step/dwell/image capture/image sequence, and that scene may be a full <b>360</b> degree scene or less than a <b>360</b> degree scene, or any wide angle scene, so long as the scene is taken in and produced by a step/dwell/image capture sequence.</p><p id="p-0036" num="0035">The system of the invention is comprised of two major subsystems (<figref idref="DRAWINGS">FIG. <b>1</b></figref>): sensors and sensor controllers (that are part of an imaging system); actuation, data and systems control (which are part of a data management and control system), which when used in conjunction with one another enable the low cost wide area imaging system and method of the present invention.</p><p id="p-0037" num="0036">The imaging is provided by a sensor that provides image capture of a predetermined field of view by means of lens and a focal plane array (FPA), an actuation subsystem for moving the sensor by a step and dwell sequence, and an additional processing subsystem that is processor controlled for processing the captured image by analog/digital readout electronics, digital image processing electronics and a network interface. This processing subsystem manages the FPA, provides image stitching when needed (whereby images captured by the sensor during the step and dwell sequence are effectively stitched or mosaicked together) provides integration and readout of image data from the sensor, correction of images for gain and offset artifacts, subsequent buffering and combining of images into a single wide area image (which is sometimes a panorama) over the course of one motion sequence, or sequence of steps or moves, of the sensor and processing of images for object detection, tracking and localization, along with compression of images prior to storage and/or transmission within a network. These functions can be co-located or embedded near the FPA electronics or distributed across a network, e.g., with wide area image processing occurring remotely if necessary.</p><p id="p-0038" num="0037">Wide area images formed by the invention are suitable for detection and tracking of objects and humans; the constituent images comprising the wide area can also be used individually for such detection and tracking, as befits the particular situation. This detection and tracking can be performed near the FPA elements or at a remote location via a network connection. Knowing the geometry of the system in relation to the environment, detection data products can be geodetically localized and time stamped for use in security and surveillance applications.</p><p id="p-0039" num="0038">The actuation subsystem is comprised of an amplifier/controller that closes a feedback loop around the controlled motor, or in the preferred embodiment, an output axis of the imaging system, this axis being e.g. a horizontal axis of rotation, but not limited to, and the axis being controlled for motion profiles. An encoder <b>104</b> is used to indicate the position of the device used to accomplish rotation with the attached servo motor. The servo motor is in turn driven by the output of the amplifier/controller <b>108</b>.</p><p id="p-0040" num="0039">Synchronization of the two subassemblies is derived from the FPA video control signals so that the timing of motion and readout functions can be precisely controlled. Timing is adjusted in advance or during operation so as to place image formation (integration) at points of zero or near zero motion. The preferred embodiment matches the FPA integration (and readout, if necessary) time to the maximum time of dwell so that maximum time is available for step motion; however, other considerations may produce a different timing scheme in order to be optimal for a particular installation site. Further, since the control of the system timing for motion and/or image capture is entirely electronic and under computer control, the image capture timing can be slaved to the motion timing, or vice versa.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>1</b></figref> illustrates a system and method architecture for the invention with an imaging subsystem, a data management and control system, and a bearing/mechanism/interconnect <b>103</b> between the imaging system and the data management and control system. A typical sequence of interplay between the two subsystems is as follows: a) move sensor (e.g., sensor <b>100</b> in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) to new position synchronously with video and control to minimum velocity, b) integrate image captured by sensor (done by the data management and control system), read out FPA pixel levels and digitize them (done by the data management and control system), c) form 2D image array with pixels, compensate image data for rotation, translation, position errors and image distortion, grayscale errors and compute the GPS coordinates of the relative N/E/S/W optical axes (centers), e) place pixel data in output (wide area) image (e.g., grid the data) and with interpolation as needed to accommodate overlap and holidays (done by the data management and control system), f) produce output wide area image (done by the data management and control system), g) process image or wide area constituent images locally or transmit image and related data products to a remote location for storage/processing. Motion to a new position can proceed once image integration, i.e., exposure, has completed, or when readout of the FPA pixel levels has completed, when typical uncooled microbolometers are used as sensors, as these use near-continuous exposure.</p><p id="p-0042" num="0041">The system and method thus described according to <figref idref="DRAWINGS">FIG. <b>1</b></figref> can, by virtue of its imagery, also accomplish the following incremental actions locally or remotely (e.g., a remote server): a) object detection within corrected and gridded wide area image data, b) object tracking between sequential wide area images, c) streaming of wide area image data as whole images or as subsets/sub-images selected by, for example, a virtual joystick, d) storing of detection and tracking data products locally or remotely for subsequent spatio-temporal data analysis, use detection and tracking information to direct laser illumination to humans being tracked so as to hail and warn.</p><p id="p-0043" num="0042">In particular, <figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a moving/rotating subassembly (above dashed line) with at least one sensor <b>100</b> having an FPA and lens producing a predetermined field of view that captures an image of that field of view, a hailing device <b>101</b>, e.g., a laser that can be used to signal to detected objects that they have been detected, ancillary sensors <b>109</b>, e.g., inertial or temperature, such as are helpful for optimizing imaging and motion performance, and a controller <b>102</b> that manages the signals, power and interfaces generally to the electronics within the subassembly.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>1</b></figref> further shows a middle subassembly <b>103</b> that furnishes the bearing surfaces (for rotation), its associated mechanism (if needed) and the electrical interconnection facility for signals and power. This bearing/mechanism/interconnect <b>103</b> subassembly interfaces mounting points, power and data to a lower subassembly (beneath the dotted line). This lower subassembly contains a system controller <b>106</b> that enables communication and control of data products, operational settings or parameters, and system behavior generally. The system controller manages the motion profiles through a motor controller <b>108</b> and associated motor and encoder <b>104</b> elements, in the preferred embodiment (closed loop, servo motion control). The sensor data from imaging and ancillary devices are relayed or directly connected to a data processor <b>105</b>. In the preferred embodiment, wide area generation and object detection are accomplished in this data processor <b>105</b>; however, it is anticipated that the invention will in some cases only preprocess data at the data processor <b>105</b> and distribute the remaining processing tasks to devices available at another node of the network by way of the network interface <b>107</b>.</p><p id="p-0045" num="0044">In <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the system controller <b>106</b> also makes use of ancillary sensor <b>110</b> data, e.g., inertial sensors for measuring the motion of the moving elements of the invention, such that motion control can be enhanced or image processing optimized by virtue of measuring residual or unintended motion. The system controller <b>106</b> thus manages the flow of data and monitors the system, serving as the primary communicator for the system, that communication taking place through a network interface <b>107</b> in the preferred embodiment. Is it anticipated that other interfaces of the system will be used when a network is not available or circumstances of location or use dictate a non networked configuration.</p><p id="p-0046" num="0045">The actuation capability of the invention implied by the agility required for streaming panoramic images and the architecture of <figref idref="DRAWINGS">FIG. <b>1</b></figref> enables several additional capabilities, namely: a) super resolution by virtue of forming a wide area image, or in some cases a panorama, of less than <b>360</b> degrees and using non integral pixel step sizes so as to capture images at inter-pixel locations, b) adaptive panorama generation wherein increased resolution is obtained in regions of higher priority, said priority being determined by the nature of detected objects, occlusions (e.g., due to a mounting structure), or by other cueing information provided by historical panorama data or other networked sensors in communication with the invention, c) higher rate wide field of view imagery wherein the motions normally employed to transit a horizontal motion sequence are used to move within a smaller region but delivering this smaller wide area image at higher data rate so, e.g., a 320&#xd7;240 imager capable of 30 Hz video could be used to deliver 640&#xd7;240 imagery at 15 Hz.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates an example packaging/configuration for the invention such as would be expected for implementing the architecture of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The upper housing package <b>200</b> houses the imaging optics, FPA, laser, ancillary sensors and associated electronics, while providing windows <b>201</b> that protect internal elements without impeding the transmission of light at sensor wavelengths. In the middle section <b>202</b> actuation, e.g., rotation, is powered using an integral servo motor and encoder, e.g., direct drive, in the preferred embodiment. Alternate actuation is compatible with the invention generally but may not provide the motion performance of a direct drive servo motor approach. The figure illustrates the use of convective cooling fins <b>203</b> as well, though these are not generally required for the invention to operate. Supporting electronics and ancillary sensors associated with the lower section of the invention can be included in implementations of the invention so as to measure and compensate for undesirable motion of the support structure, e.g., a pole, upon which the invention is mounted. These electronics/elements are shown as part of the inner profile of lower housing <b>204</b>. In the preferred embodiment, the power and data are brought into the upper housing by a slip ring or equivalent device, though alternate approaches are envisioned, including those disclosed in U.S. application Ser. No. 12/436,687 pertaining to the transmission of power and data into a moving apparatus wirelessly, and that application is incorporated by reference herein. In the preferred embodiment, the upper housing attaches to the lower by a central column and associated bearing, and a central/concentric motor is used in conjunction with the amplifier/controller to rotate the upper housing.</p><p id="p-0048" num="0047">Further, in <figref idref="DRAWINGS">FIG. <b>2</b></figref> the lower housing <b>204</b> contains the amp/controller, the motor, encoder (preferably attached to the upper housing so as to monitor the actual load), the image digitizer/processor, any required ancillary sensors e.g., inertial sensors, GPS receiver (can be in same assembly), and also the power supply electronics, here shown packaged <b>205</b> for insertion into a pole structure inner diameter. Communications to and from the base can be managed by the main power/data connector on the side of the lower housing, though many communications configurations are compatible with the invention.</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIG. <b>3</b></figref> shows an expanded view of the upper section of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, illustrating a configuration having multiple sensors and a laser packaged within a single enclosure <b>300</b>. Thus, for instance, a narrow field of view thermal sensor <b>305</b>, a wide field of view thermal sensor <b>306</b>, a wide field of view visible light sensor <b>301</b> and a visible, e.g., green, laser <b>303</b>, can be used to simultaneously form wide area images at two resolutions in the thermal infrared waveband <b>305</b> <b>306</b> while also imaging the same wide area with visible light <b>301</b> (assuming daytime or illuminated conditions), while responding to object detections, e.g., intruders, using the laser <b>303</b>. For each waveband of light, suitable transparent windows <b>302</b> are used to allow visibility of the scene to be imaged while protecting optics and electronics needed to obtain the corresponding imagery, the signals and power for sensors and electronics being provisioned by a slip ring or equivalent sealed rotary pass through <b>307</b>. The windows <b>302</b> are sealed to prevent ingress of moisture and/or dirt, and the lower flange <b>304</b> is likewise constructed such that an air tight seal can be accomplished, enabling a purge gas, e.g., nitrogen, to be used to enhance the resilience of the invention across diverse environmental conditions.</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>3</b></figref> thus shows sensors operating at two wavebands: thermal infrared (approximately 8000-12000 nanometers) and visible light (approximately 400-700 nanometers). The figure also illustrates the use of more than one field of view per wavelength; in the case of the thermal infrared both a narrow and wide are shown, corresponding to a higher and lower pixel density per imaging area, respectively. The diversity of wavelengths and fields of view is not limited by the invention, other than by constraints placed on the physical spaces allowed for sensors and the associated mechanisms required to sufficiently control the requisite motion. Thus, while <figref idref="DRAWINGS">FIG. <b>3</b></figref> shows sensors stacked vertically and opposed horizontally (180 degrees separation between the visible and the thermal sensors, for instance), sensors could be distributed only horizontally, or only vertically, stacked more densely than shown, etc.</p><p id="p-0051" num="0050"><figref idref="DRAWINGS">FIG. <b>4</b></figref> shows an example installation of the components of <figref idref="DRAWINGS">FIG. <b>2</b></figref> atop a pole for a remote surveillance application. In <figref idref="DRAWINGS">FIG. <b>4</b></figref>, the wide area imaging assembly <b>400</b> is mounted atop a pole such that the lower subassemblies (see <figref idref="DRAWINGS">FIG. <b>204</b></figref>) mount into the inner surface of the pole, where electrical connections and thermal mating surfaces, if needed, can be found. The pole structure <b>401</b> extends above ground level <b>403</b>, and provides support for an enclosure <b>402</b> that can be used for site electrical junctions, battery backup, network and power monitoring, etc. The pole is anchored into the earth using reinforced concrete <b>404</b>. The configuration of <figref idref="DRAWINGS">FIG. <b>4</b></figref> is the preferred embodiment when infrastructure at remote locations is not sufficient to support the invention; however, diverse mounting configurations are anticipated for the invention, including mounting directly to buildings and also mounting on mobile/moving platforms, e.g., a fixed mount or telescoping mast installed in a vehicle.</p><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an example of the timing required to acquire images by sequentially stepping, dwelling and capturing image data; this figure shows plots of normalized position, velocity (dots) and image capture sync (on vertical axis) versus time <b>503</b> (horizontal axis). A low slope region for the position indicates the minimization of motion; this rotary position <b>501</b> is shown for a system such as that shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. The low position slope or velocity <b>500</b> having low value <b>502</b> indicates a time for integration and image capture, as the FPA is stationary. The image capture sync signal <b>504</b> transitions <b>505</b> to an active state when the FPA is stationary such that the process of integrating, reading out and digitizing an image can begin. The high velocity periods <b>500</b> correspond to motion as can be seen in the position data. Using digital and analog electronics and computer control, the sensor motion and video acquisition process is controlled such that motion is synchronous with image acquisition timing. By synchronizing thus, images free of blur can be obtained at high rate, enabling a wide area image to be formed using video rate imagery and at sufficient frequency to support security and surveillance applications, e.g., human detection and tracking. Further, since the sensor motion and image capture are computer controlled and adaptable, arbitrarily long dwells can occur, such as intimated by the longer low motion state <b>504</b> in the figure. Similarly, the position pattern can be altered from a regular sequence to an arbitrary, including randomized pattern, all under computer control while operating and generating wide area imagery.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows an assembly of an FPA <b>602</b> with lens and associated package <b>600</b> and with its corresponding predetermined field of view <b>601</b>. The FPA is shown exposed for the sake of clarity; however, these devices are typically not exposed but are packaged within sealed enclosures, including vacuum enclosures in the case of some uncooled thermal imaging FPAs.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>7</b></figref> illustrates in simplified fashion how a wide area image can be generated with the invention; as such the figure shows the sensor assembly of <figref idref="DRAWINGS">FIG. <b>6</b></figref> in the context of a wide area imaging motion pattern. In this simplified illustration, a sensor/lens assembly <b>702</b> is imaging with its predetermined field of view <b>701</b> in a rightward facing position, but once an image has been captured as this position, will advance <b>703</b> to a downward facing position <b>704</b>, where a successive image will be captured. This process of motion and image capture at distinct positions is then repeated until the entire panorama <b>700</b>, e.g., 360 degrees in this example, is traversed. In this example, a large gap occurs between the rightward and downward positions; this is typically not desired for a panorama made of adjacent constituent images, but is often the type of motion used for the more generate wide area imaging scenario where the priority of the imaged regions will determine the motion profile used and, thereby, the continuity of the constituent images comprising the wide area image.</p><p id="p-0055" num="0054">As seen from the foregoing description, the system and method of the invention uses a motorized step/dwell/image capture, and when necessary, image stitch, approach to enable M&#xd7;N focal plane arrays, M&#x2265;1, N&#x2265;1, to acquire images M&#x2032;&#xd7;N&#x2032; having fields of view wider than the stationary imaging system (i.e. the sensor) can obtain, e.g., M&#x2032;&#x2265;M, N&#x2032;&#x2265;N. The system and method preferably uses inertial sensors, e.g., MEMS gyro/accelerometer device(s), to measure platform motion so as to stabilize the image to be produced using electro-mechanical means (e.g., adaptive step/dwell), image processing methods, or combinations thereof to compensate the output images for the observed inertial motion.</p><p id="p-0056" num="0055">An example of wide area imaging that leads to a panorama will help to illustrate the invention. The example uses a 640&#xd7;480 pixel uncooled micro bolometer sensor having a lens that provides a 35 degree horizontal field of view and a 26 degree vertical field of view. The average rate of rotation in the horizontal direction (so as to afford panoramic fields of view) is 360 degrees per second; thus, the wide area panoramic image generated once per second will be 26 degrees high and 360 degrees wide.</p><p id="p-0057" num="0056">The difference between the invention and a typical panoramic imager that uses scanning is that the scan velocity, while having a constant mean value over one second intervals, is highly variable during the transit through 360 degrees in the horizontal direction. In fact, the velocity is periodic and can extend to beyond 360 degrees per second, depending on the details of the video timing and the format of the sensor. For this example, if we assume nearly instantaneous transitions from zero to full velocity, we can approximate the motion by a 50% duty cycle velocity profile having two states: 0 degrees per second and 720 degrees per second. Thus, the average velocity is 360 degrees per second as desired. This motion then is a start-stop-start-stop-etc. sequence (while the example is adequate for illustrating the principles, in practice the motion is more complicated, see <figref idref="DRAWINGS">FIG. <b>5</b></figref>, for example). The image data is collected during the &#x201c;stop&#x201d; portion so as to minimize blur; and the motion is accomplished during the &#x201c;start&#x201d; so as to change the field of view and over the course of 360 degrees rotation, accumulate the 640&#xd7;480 images needed to make up a 360 degree panoramic view. If there is no overlap between adjacent images in the start-stop-start-stop sequence, then only 360/35=10+ images would be needed per second, each image comprising 35 degrees of the panorama. However, if overlap is to be used, which is the preferred embodiment when a 360 degree panorama is desired as the wide area image, then at least 11 images would be needed. For example, a 5% overlap (2.5% each side of each 640&#xd7;480 image) would only require the 11 images per second (non integral frame rates not being allowed), but would provide some border regions intra-image to facilitate the image manipulations to accommodate spatial distortions, grayscale discontinuities at the edges, and automatic gain control effects&#x2014;all of which need to be removed from the images being joined together to form the panoramic scene.</p><p id="p-0058" num="0057">Thus, using a low cost micro bolometer and servo motor technology, a high speed, high quality, wide area digital imaging system can be obtained at low cost. This forms the basis for the invention, which is readily expanded to include image processing features, geodetic registration and many resultant temporal and spatial data services.</p><p id="p-0059" num="0058">Encoding position states or information in imagery for offline or networked wide area image generation is contemplated for the invention. Since the information relating to the state of motion is readily available to the processor(s) that handles or compresses raw (pre panoramic) imagery, the position and motion state variables can be embedded into the images so as to remove the need for image processing in selecting images for wide area images. So, for example, the time and position corresponding to the image readout completion time, or an equivalent timing parameter, could be embedded in an image. This inclusion of information can be accommodated in, e.g., jpeg headers, or other metadata locations used for digital imagery.</p><heading id="h-0006" level="1">ADDITIONAL FEATURES OF THE INVENTION</heading><p id="p-0060" num="0059">The scanning nature of the invention encourages the use of multiple-pass and sequential-pass algorithms for detecting and tracking, since there is little chance of detected objects ever actually exiting the field of view (flight and travel outside the resolution limits being some exceptions). For example, successive hypothesis test algorithms for detecting can be used to optimize the use of scanning resources for optimum resolution and resultant probability of detection. For instance, in order to maximize scan rate, it may be advisable to reduce the nominal panorama resolution or image frame rate while in a detection mode. In this case, if objects of potential interest are identified during the rapid scan mode, successive passes at increased resolution for finite regions of interest could be used to further test the hypothesis that &#x201c;interesting&#x201d; objects are there and merit further attention. Used in this fashion, the average and peak frame rates of the wide area imager can be traded against detection performance as a function of time, location, object priority, or location priority any of which may serve as proxies for an event or object of interest.</p><p id="p-0061" num="0060"><figref idref="DRAWINGS">FIG. <b>1</b></figref> implies a plurality of sensors and or other devices in the moving portion of the invention. Thus, wide area images at multiple wavelengths can be obtained at the same time, on an individual frame basis and/or on a wide area image basis; the former case corresponding to multiple sensors stacked vertically according to the example geometry of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the latter case corresponding to the opposing geometry (facing 180 degrees away from each other) of the visible wavelength and thermal wavelength sensors of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, a geometry that can be extended in the invention to include arbitrary angular distributions of sensors of one or more wavelengths. <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates this by showing a system that uses two thermal infrared sensors, one visible light sensors and visible light laser. This can be further extended to include other wavebands and suggests the use of a ranging device such as a LIDAR (light detection and ranging) subsystem, or acoustic hailing devices as an alternate to a laser.</p><p id="p-0062" num="0061">The invention is well suited to cueing itself, as suggested by successive hypothesis testing. However, it is also very useful for cueing other devices, e.g., nearby or adjacent high resolution pan/tilt/zoom cameras that can obtain continuous high rate high resolution video as track updates are provided by the invention, which is able to do so owing to its continuously updated wide area view of peripheral scenes.</p><p id="p-0063" num="0062">The invention is also useful when simpler devices, e.g., motion detectors, seismic, or acoustic sensors, are used to indicate areas of unknown activity. In this scenario a successive hypothesis test scenario could be operated (or not, if simpler methods suffice) so as to maximize the wide area image update rate while still &#x201c;keeping an eye on&#x201d; areas that suggest ongoing activity, such as would be indicated with a simple motion detector placed near the periphery of the invention or in an adjacent occluded area, etc.</p><p id="p-0064" num="0063">The hailing feature of the invention has been illustrated using a laser for visual communication of detection. An additional medium that can be used for audible communication of detection is hypersonic audio, wherein an audible signal is carried by a ultrasonic acoustic wave. Such a transmitter is anticipated to be used with the system and can be placed on the moving or stationary elements of the invention. In the former case, the transducers are integrated into the apparatus of <figref idref="DRAWINGS">FIG. <b>3</b></figref> along with such additional electronics and/or actuation as are required to provide ultrasonic wave steering in the direction perpendicular to the plane of rotation of the wide area imaging sensor(s). For such a hypersonic hailing device, audible messages of a prerecorded nature could be transmitted and communicated to a detected object or person and/or a live human operator could generate audible messages by way of the network interface (<b>107</b> see <figref idref="DRAWINGS">FIG. <b>1</b></figref>) and internal electronics used to convert those audio signals to hypersonic signals.</p><p id="p-0065" num="0064">The use of a laser for hailing, in the preferred embodiment, uses modulation of the laser output (or conversely, modulation of the velocity and/or timing of the motion profile) to control the intensity of light as a function of range and also to shape the apparent (to the object/person) distribution of light in the direction of wide area image formation (horizontal, as shown in the example of <figref idref="DRAWINGS">FIGS. <b>2</b>, <b>3</b></figref>). Further, the preferred embodiment uses structured light optics to constrain the geometry of the emitted laser light to, for instance, a vertical line that has an angular extent encompassing the vertical field of view for the invention. Other geometries are compatible with the invention and suitability of a geometry of structured light will vary as the wide area image orientation/application varies.</p><p id="p-0066" num="0065">When a laser is used as the hailing device or is otherwise installed in the invention, and when a sensor capable of imaging the light from the laser (so both sensor and laser are in the same wave band) has the projected laser light within its field of view, e.g., the example of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the laser can also be used to illuminate objects being imaged with the laser when ambient light is not sufficient for an image, e.g., for a color sensor at night. In this case, the invention adjusts its motion pattern and image data collection scheme to suit the size of the imaged region and the structure of the projected laser illumination. For instance, if a vertical line is projected from the laser of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, the invention motion pattern would restrict its motion to the area occupied by the object to be imaged and step/dwell past the object with the laser line, collecting and combining, e.g., shifting and stacking, successive image capture frames until the object is imaged in its entirety. By doing this at high speed the object will have moved very little and an image of the object will have been acquired, in spite of the lack of ambient light.</p><p id="p-0067" num="0066">The invention is capable of adjusting its motion profile or pattern as a function of detections that occur within the field of view, e.g., the entire wide area, often 360 degrees. When multiple objects, e.g., intruders, are detected, the preferred embodiment will adapt the timing and dwell time within the wide area image region to minimize latency on objects of greatest interest, e.g., those closest to assets of high value and/or risk of loss, while maximizing the respective probability of detection for the detected objects of interest.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows that multiple fields of view corresponding to multiple sensors, e.g., FPA/lens assemblies, per invention are contemplated for the invention. In order to minimize the number of unique lens assemblies present, thereby reducing the number of components in the invention, it is anticipated that unconventional, e.g., anamorphic, lenses will be used for imaging sensor fields of view onto the sensor FPA. Such a lens will accomplish a mapping of pixels in the field of view onto the FPA such that the probability of detecting an object, e.g., human, is maximally flat as a function of distance from the invention. For example, if detecting a human requires at least 10 resolution elements, e.g., pixels, per major axis of the imaged human in order to be reliably detected, then the optimum unconventional lens would furnish at least 10 pixels of resolution at the maximum range and no more than 10 pixels of resolution at the minimum range. The image thus formed on the FPA would be highly distorted as compared to normal human vision; however, it would maximize the use of FPA pixels for detecting across the entire field of view such that maximum value is extracted for detecting objects of interest with the invention. Images thus generated could still be viewed by humans, but would be corrected for the nonstandard geometry prior to displaying, for a more intuitive view for a human operator of the invention.</p><p id="p-0069" num="0068">In using the invention to hail an object or person, e.g., using a laser or acoustic means, the communication is from the invention to the object. However, it is anticipated that the invention will be used for communication from the object to the invention as well, either directly or by way of other nearby instances of the invention or comparable resources. That is, the invention is expected to be used to detect not only the presence of an object, but what its state is, e.g., its pose, orientation, or distribution. Thus the invention will detect gestures of predetermined nature so that the invention will classify the gesture, e.g., stationary human with arms extended 45 degrees to the ground could be assigned the meaning, &#x201c;distressed&#x201d;, or arms extended horizontally may indicate a &#x201c;friend&#x201d; classification instead of &#x201c;foe&#x201d;.</p><p id="p-0070" num="0069">Classifying friend versus foe is further enabled by the invention through the use of wearable badges or patches that are resolved by the invention sensor(s) such that a unique individual or class identifier is detected and associated with an object or human. In the case of a thermal infrared sensor, the badge could contain, for example, a pattern of high and low emissivity surface coatings, i.e., a pattern of relative apparent physical temperature, that is imaged by a sensor and that functions equivalently to a bar code pattern used in electronic item identification. Such a thermal infrared compatible badge could, for example, be enabled using polycarbonate heat patch material, or other active thermally emissive materials. Other wavelengths and badge constructions are readily conceived and are contemplated for the invention.</p><p id="p-0071" num="0070">The ability to measure the apparent physical temperature of objects imaged by a sensor in the invention is readily extended to measuring the actual physical temperature, such that temperature is quantified, e.g., degrees Celcius, by either calibrating sensors prior to installation, conducting a field calibration after installation of the invention, or using objects in the field of view to automatically discern and approximate the physical temperature, given ambient conditions. Such a sensor can measure the physical temperature of objects such that thermal abnormalities are detected, e.g., excessive heating or risk/presence of fire. The preferred embodiment is a sensor that has been calibrated prior to installation and which has calibration periodically updated.</p><p id="p-0072" num="0071">The invention is further contemplated for use in asset management, whether as part of a security and surveillance application or not. In such a use of the invention, a catalogue of assets is generated by the invention in a sequence of steps: 1) generate a wide area image of the scene containing assets, 2) detect stationary objects, 3) classify stationary objects, 4) estimate sizes of stationary objects using image data and/or other sensor data, 5) (optionally) present objects to an operator for confirmation and valuation in priority or currency, e.g., dollars, or compute a value parameter for objects based on predetermined criteria, 6) store object information, including estimated size and location, e.g., GIS data, into a database that serves as a reference for future object comparison, e.g., displaced object detection/tracking for damage assessment, and/or to objectively determine the absence of an object in the case of theft or other movement of the object or an ensemble of objects of interest. The invention used in this way would then make use of its scan pattern to prioritize the invention operating parameters for motion and sensor data acquisition such that higher value assets are emphasized and more robustly protected than lower value assets.</p><p id="p-0073" num="0072">Techniques for the foregoing asset tracking use of the invention are well known to those skilled in the art of computer vision technology. For instance the detection and classification of stationary objects is readily accomplished to a high degree of accuracy using imaged edge detection techniques such as the Canny method or the Berkeley natural boundary detector, in combination with Contour Segment Networks, to name only one approach to the problem of object detection. The preferred embodiment uses a visible wavelength sensor for the initial asset tracking task of object detection and classification and uses this to inform processing at other wavelengths, as the highest density of pixels and the presence of multiple colors can both be helpful. However, other wave bands can be used independently and, in some cases will yield superior results to that of the visible bands.</p><p id="p-0074" num="0073">The estimation of size for the asset tracking use of the invention is applicable to more than just the problem of asset tracking as described. The estimation of size is helpful for detecting and tracking Consequently, the invention will be used to estimate the size of objects imaged with its sensors. The size can be estimated using a priori information and an estimate of the class of an object, such class being provided by one of the well-known supervised object classification algorithms. However, the error rate of an object class based size estimate, e.g., if object is a house, then an average house size is used, is likely to be unsatisfactory for some applications. The invention enables a more robust estimate of object size by making use of stereo imaging techniques and the known geometry of sensors and optics used in the invention. The finite field of view of the invention sensor(s), combined with the presence of physical separation between optical axes, enables an object to be imaged with a sensor in the invention at two different locations of the sensor, these locations having a finite distance between them perpendicular to the line between the FPA and the object. This finite distance along a perpendicular, or baseline, exists in two forms in the invention as shown in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. A baseline between the two thermal sensors <b>305</b> <b>306</b> exists in the vertical direction; a baseline also exists for any sensor <b>301</b> <b>305</b> <b>306</b> with itself between two angular positions, e.g., 0 to 360 degrees. These baselines, when combined with images of the same object captured by sensor data from the two ends of the baseline, enable a stereo image estimate of range to be made for features and/or objects imaged by the sensor. The techniques for doing so with imagery are well known to those skilled in the art of computer vision and are readily available at public repositories, e.g., OpenCV or equivalent.</p><p id="p-0075" num="0074">A related facility for the invention to that of asset tracking is that of automatically estimating areas in a panorama that are of little interest, such that the automated detection of objects is not susceptible to clutter from those regions, and so that, to the extent possible, those regions are de-emphasized in the estimating of optimal scan patterns for the motion profile used within the invention to produce a wide area image. Predetermined or inferred criteria for assessing the level of interest can be used, and interest can also be determined adaptively, e.g., by observing the frequency of events of interest such as vehicle traffic and/or by observing the presence or absence of object features and/or by using known landscape features to classify imaged regions, such as sky (blue). The computer vision application area addressed by this invention feature is that of gist recognition and is well known to those skilled in the art of computer vision.</p><p id="p-0076" num="0075">When the invention is used in conjunction with other surveillance systems, including other instances of the invention, it is helpful to those responsible for the surveillance to be able to seamlessly detect and track objects from one system to the other. Thus the invention contemplates the handing off of a detected object from one system to another by virtue of sharing the location, e.g., GPS coordinates, and nature, e.g., sensor data characteristics in image or other more compact form, between successive systems viewing the object as it moves out of the viewing range of one and into the viewing range of another.</p><p id="p-0077" num="0076">The sensors illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> are optical in nature. However, longer wavelengths, such as those employed in radio frequency (RF, including microwave and similar order of magnitude wavelengths), can be accommodated as sensors in the wide area image generation as well. The simplest implementation of such an RF panorama sensor would be to use a commercial radar module having a beam that radiates the same region as that of the optical assemblies of <figref idref="DRAWINGS">FIG. <b>3</b></figref>, i.e., having the same or similar field of view, and sample the radar at the same time as a sensor, or continuously, provided the radar pulse repetition frequency (PRF) is sufficiently high and the ambiguous range is outside the region of interest, for example. In this case a panorama having at least a single pixel, but not more than the number of range bins, in the vertical direction would result, and having pixels in the panorama direction of at least the number of stationary locations (see <figref idref="DRAWINGS">FIG. <b>4</b></figref>) but not exceeding the product of the PRF and the time per panorama sweep (360 degrees in the case of a complete 360 degree panorama). A more robust, as concerns antenna side lobes and related detection clutter for the RF implementation in the invention would use an array, e.g., conformal and integrated into the invention upper housing (<b>200</b>, <figref idref="DRAWINGS">FIG. <b>2</b></figref>), having elements distributed both vertically and horizontally such that resolution in the vertical and horizontal dimensions is achieved and antenna gain in each dimension enables lowering of side lobes and improvement of directivity. Further, if the elements are implemented with phase controls, a phased array steering of the central lobe of the resultant antenna pattern can be accomplished to further control and enhance the directivity. Finally, using the motion profile needed for wide area image generation, an RF sensor, e.g., coherent radar, a synthetic aperture radar (SAR) implementation could be integrated into the geometry/package of <figref idref="DRAWINGS">FIG. <b>2</b></figref> such that RF wavelength sensor are available as wide area images instead of, or in conjunction with, shorter wavelength, e.g., optical wavelengths less than 20000 nanometers, sensor data as described above.</p><p id="p-0078" num="0077">Sensors are also contemplated for use with the invention that are integrated along side of the invention or, in some sense, in close proximity. Examples of this are scanning radars having fixed/mounted arrays, e.g., mounted to the pole beneath the invention shown in <b>401</b> <figref idref="DRAWINGS">FIG. <b>4</b></figref>, and hypersonic/acoustic devices or arrays that can be electrically or mechanically directed to objects of interest that are detected by the invention.</p><p id="p-0079" num="0078">The system and method of the invention enable sensor scans of a predetermined field of view, at different angles, by a step/dwell/image capture sequence under processor controlled servo motor that provides coverage of a wide field of regard with an inexpensive sensor (e.g. uncooled microbolometer). The system and method utilizes a relatively small number of pixels at a relatively low net frame rate (e.g., maximal thermal infrared pixels and image capture rate for a field of regard, at low system and operating cost (vs lots of FPAs or cooled cameras)</p><p id="p-0080" num="0079">The imaging system and method utilize a) an image sensor with a known/particular field of view, attached to b) a motion control platform, such that the image sensor can be moved automatically to cover a wider &#x201c;field of regard&#x201d; than the (immediate/nominal/optical) field of view of the sensor, with the motion controlled to step through a sequence of 2 or more positions quickly and then dwell in each position only long enough to take a still image, repeating the sequence (or a computed variation of the sequence) continually, preferably under the control of a servo drive, or more specifically direct drive.</p><p id="p-0081" num="0080">The system and method can utilize a plurality of sensors, each potentially of different spectrum or FOV. The sensors can be sensors with variable fields of view (FOV), e.g., zoom optics, with the data management and control system choosing an FOV based on predetermined or dynamic strategy. The system and method can be implemented on a moving platform as well, and can provide for stitching images together to produce single composite or panoramic images of up to 360 degrees coverage.</p><p id="p-0082" num="0081">The system and method of the present invention enables automatic object or motion detection against individual images, wide area images or panoramas, provides variable step and dwell sequences, step and dwell synchronized to camera capture, sensor fields of regard may or may not be contiguous, and may or may not be 360 degrees. The system and method provides &#x201c;motion and/or object detection&#x201d;, and utilizes a servo drive, as opposed, e.g., to a mechanical indexing scheme. A preferred embodiment would use a direct drive motor.</p><p id="p-0083" num="0082">The system and method of the present invention provides an automatic imaging system, with a step and dwell sequence for the sensor, and covers a wider field of regard than the field of view of the sensor (FPA and lens), and utilizes time and synchronization to get wide coverage and repetition in short time (e.g. real time). The data management and control system is autonomous; it chooses/changes its priority of where and how long to look and with what dynamically, so that, knowing the necessary integration time, dwell time can be varied accordingly, and imaging can be concentrated on a particular region of interest. Image stitching may be useful in some situations, but is not a requirement. The sensor platform, in its preferred embodiment, has one moving (bearing) part, so that there is relatively little wear or associated service, and the system and method enables changing the step and dwell steps and image capture, on the fly under software control.</p><p id="p-0084" num="0083">Thus, as seen from the foregoing description, according to the present invention, wide area imaging is provided by a step/dwell/image capture process/system to capture images and produce from the captured images a wide area image as maximal speed (minimum time per wide area image). The image capture is by a sensor that has a predetermined image field and provides image capture at a predetermined frame capture rate, and by a processor controlled motorized step and dwell sequence of the sensor, where image capture is during a dwell, and the step and dwell sequence of the sensor is synchronized with the image capture rate of the sensor. In a preferred form of the system and method of the present invention, the step/dwell/image capture sequence is under processor control that is interfaced to a servo motor to selectively control the sensor position in a manner that is related to the step/dwell/image capture sequence.</p><p id="p-0085" num="0084">Detection products can be derived from either a wide area image or the individual image frames used to form the panorama. The advantage of using an individual image frame, e.g., a single 640&#xd7;480 in the example cited above, is that there is no loss of information due to overlap and the signal processing gain corresponding to multiple observations of an event is available in an obvious way (often this will accumulate as the square root of the number of observations). In the preferred embodiment, image processing occurs in both individual image frames and in wide area (which can included stitched) imagery when image overlap exists between individual frames, such that partial images (caused by the bisection of an object with a single frame boundary) and multiple observances can be accommodated without loss of performance.</p><p id="p-0086" num="0085">In a preferred aspect of the present invention, the sensor is located on a moveable platform, and movements of the platform that affect the wide area image produced from the captured image are measured and used to provide image compensation that is related to such movements of the platform. Moreover, when a subject, e.g. a human, is identified in the wide area image, the subject can be hailed or notified that its presence has been detected.</p><p id="p-0087" num="0086">The preferred wide area imaging system and method can also have one or more of the following features; (a) the wide area imaging system and method may include a processor that uses object detection on a captured image or the wide area image to extract and localize objects of interest in the wide area image; (b) the wide area system and method may be configured to provide variable step and dwell sequences of the sensor, to produce the wide area image; (c) The wide area system and method may be configured to provide variable step and dwell sequences of the sensor, to enable the sensor to localize the sensor on selected image fields; thus, if the system is being manually or automatically monitored, and the monitor observes an object of particular interest, the sensor can be localized on that object; (d) the wide area system and method can have (or be controlled by) a processor configured to use successive hypothesis testing of detected objects in the wide area image to determine the step and dwell sequences of the sensor; (e) in the wide area system and method the sensor can be configured to produce image capture in a manner that is useful in producing high density imagery in the wide area image; (f) the wide area imaging system and method can be coupled to a control center via an interface and configured to allow the control center access to subsets of the captured images (including stitched subsets), via the interface; (g) the wide area imaging system and method can have a sensor configured with one of the following sensing techniques: infrared, visible or other wavelengths to form an image using sensor pixels, including the collection of multiple wavelength images at the same time (multiple FPAs); (h) the step/dwell/image capture sequence can be configured to synchronize the initiation of image capture by the sensor to a position to which the sensor is selectively moved under servo motor control; and (i) the step/dwell/image capture sequence can be configured to synchronize movement of the sensor to a selected position to the timing with which the sensor is controlled to initiate image capture.</p><p id="p-0088" num="0087">The principles of the present invention have been described herein in connection with one exemplary system and method, and from that description the manner in which the principles of the present invention can be applied to various types of wide area imaging systems and methods will be apparent to those in the art.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A wide area imaging system that uses a step/dwell/image capture sequence to capture images and produce from the captured images a wide area image, wherein image capture is by a sensor that has a predetermined image field and provides image capture at a predetermined frame capture rate, wherein image capture is by a processor controlled motorized step and dwell sequence of the sensor, where image capture is processor controlled during a dwell step, and wherein the step and dwell sequence of the sensor is synchronized with the image capture rate of the sensor.</claim-text></claim></claims></us-patent-application>