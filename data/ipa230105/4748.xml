<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004749A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004749</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17899539</doc-number><date>20220830</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>62</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>16</class><subclass>B</subclass><main-group>40</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>907</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>75</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6218</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6256</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>08</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190201</date></cpc-version-indicator><section>G</section><class>16</class><subclass>B</subclass><main-group>40</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6267</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>04</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>907</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6222</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6232</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6262</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>6277</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>628</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>3</main-group><subgroup>084</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>7</main-group><subgroup>005</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>751</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>10</main-group><subgroup>82</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>N</subclass><main-group>5</main-group><subgroup>046</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">DEEP NEURAL NETWORK-BASED SEQUENCING</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16826168</doc-number><date>20200321</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11436429</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17899539</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62821602</doc-number><date>20190321</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62821618</doc-number><date>20190321</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62821681</doc-number><date>20190321</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62821724</doc-number><date>20190321</date></document-id></us-provisional-application><us-provisional-application><document-id><country>US</country><doc-number>62821766</doc-number><date>20190321</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>ILLUMINA, INC.</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>JAGANATHAN</last-name><first-name>Kishore</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>DUTTA</last-name><first-name>Anindita</first-name><address><city>Brisbane</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>KASHEFHAGHIGHI</last-name><first-name>Dorna</first-name><address><city>Menlo Park</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>GOBBEL</last-name><first-name>John Randall</first-name><address><city>Brisbane</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>KIA</last-name><first-name>Amirali</first-name><address><city>San Mateo</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>ILLUMINA, INC.</orgname><role>02</role><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system, a method and a non-transitory computer readable storage medium for base calling are described. The base calling method includes processing through a neural network first image data comprising images of clusters and their surrounding background captured by a sequencing system for one or more sequencing cycles of a sequencing run. The base calling method further includes producing a base call for one or more of the clusters of the one or more sequencing cycles of the sequencing run.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="231.31mm" wi="155.36mm" file="US20230004749A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="239.61mm" wi="157.40mm" file="US20230004749A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="212.51mm" wi="159.85mm" orientation="landscape" file="US20230004749A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="226.65mm" wi="160.95mm" orientation="landscape" file="US20230004749A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="145.97mm" wi="169.50mm" orientation="landscape" file="US20230004749A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="113.20mm" wi="164.25mm" orientation="landscape" file="US20230004749A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="180.76mm" wi="170.69mm" orientation="landscape" file="US20230004749A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="149.10mm" wi="167.47mm" orientation="landscape" file="US20230004749A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="211.24mm" wi="170.52mm" orientation="landscape" file="US20230004749A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="236.73mm" wi="174.24mm" file="US20230004749A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="200.41mm" wi="173.65mm" orientation="landscape" file="US20230004749A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="227.84mm" wi="152.57mm" orientation="landscape" file="US20230004749A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="114.22mm" wi="157.56mm" orientation="landscape" file="US20230004749A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="225.89mm" wi="174.16mm" orientation="landscape" file="US20230004749A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="236.73mm" wi="174.16mm" orientation="landscape" file="US20230004749A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="236.73mm" wi="174.16mm" orientation="landscape" file="US20230004749A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="212.43mm" wi="174.50mm" orientation="landscape" file="US20230004749A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="229.53mm" wi="173.40mm" orientation="landscape" file="US20230004749A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="235.97mm" wi="171.87mm" orientation="landscape" file="US20230004749A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="240.20mm" wi="171.79mm" file="US20230004749A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="244.52mm" wi="171.11mm" orientation="landscape" file="US20230004749A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="244.52mm" wi="171.11mm" orientation="landscape" file="US20230004749A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="223.10mm" wi="149.10mm" orientation="landscape" file="US20230004749A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="244.69mm" wi="172.04mm" orientation="landscape" file="US20230004749A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="233.68mm" wi="168.32mm" orientation="landscape" file="US20230004749A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="243.50mm" wi="168.32mm" orientation="landscape" file="US20230004749A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="240.88mm" wi="168.32mm" orientation="landscape" file="US20230004749A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="248.07mm" wi="174.50mm" orientation="landscape" file="US20230004749A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="245.19mm" wi="173.99mm" orientation="landscape" file="US20230004749A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="238.00mm" wi="149.69mm" orientation="landscape" file="US20230004749A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="243.92mm" wi="172.80mm" orientation="landscape" file="US20230004749A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="246.21mm" wi="171.53mm" orientation="landscape" file="US20230004749A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="241.30mm" wi="174.33mm" orientation="landscape" file="US20230004749A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="244.01mm" wi="174.33mm" orientation="landscape" file="US20230004749A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00034" num="00034"><img id="EMI-D00034" he="240.62mm" wi="166.71mm" file="US20230004749A1-20230105-D00034.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00035" num="00035"><img id="EMI-D00035" he="228.85mm" wi="170.86mm" orientation="landscape" file="US20230004749A1-20230105-D00035.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00036" num="00036"><img id="EMI-D00036" he="204.39mm" wi="169.93mm" orientation="landscape" file="US20230004749A1-20230105-D00036.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00037" num="00037"><img id="EMI-D00037" he="205.23mm" wi="168.66mm" orientation="landscape" file="US20230004749A1-20230105-D00037.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00038" num="00038"><img id="EMI-D00038" he="238.17mm" wi="151.30mm" orientation="landscape" file="US20230004749A1-20230105-D00038.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00039" num="00039"><img id="EMI-D00039" he="238.51mm" wi="151.30mm" orientation="landscape" file="US20230004749A1-20230105-D00039.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00040" num="00040"><img id="EMI-D00040" he="230.21mm" wi="153.25mm" orientation="landscape" file="US20230004749A1-20230105-D00040.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00041" num="00041"><img id="EMI-D00041" he="194.99mm" wi="167.72mm" orientation="landscape" file="US20230004749A1-20230105-D00041.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00042" num="00042"><img id="EMI-D00042" he="187.88mm" wi="169.50mm" orientation="landscape" file="US20230004749A1-20230105-D00042.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00043" num="00043"><img id="EMI-D00043" he="190.84mm" wi="169.50mm" orientation="landscape" file="US20230004749A1-20230105-D00043.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00044" num="00044"><img id="EMI-D00044" he="229.70mm" wi="170.69mm" orientation="landscape" file="US20230004749A1-20230105-D00044.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00045" num="00045"><img id="EMI-D00045" he="231.22mm" wi="165.95mm" orientation="landscape" file="US20230004749A1-20230105-D00045.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00046" num="00046"><img id="EMI-D00046" he="171.79mm" wi="169.50mm" orientation="landscape" file="US20230004749A1-20230105-D00046.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00047" num="00047"><img id="EMI-D00047" he="228.60mm" wi="165.95mm" orientation="landscape" file="US20230004749A1-20230105-D00047.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00048" num="00048"><img id="EMI-D00048" he="223.01mm" wi="163.66mm" orientation="landscape" file="US20230004749A1-20230105-D00048.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00049" num="00049"><img id="EMI-D00049" he="247.99mm" wi="173.06mm" orientation="landscape" file="US20230004749A1-20230105-D00049.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00050" num="00050"><img id="EMI-D00050" he="247.99mm" wi="173.06mm" orientation="landscape" file="US20230004749A1-20230105-D00050.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00051" num="00051"><img id="EMI-D00051" he="230.29mm" wi="169.50mm" orientation="landscape" file="US20230004749A1-20230105-D00051.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00052" num="00052"><img id="EMI-D00052" he="236.22mm" wi="171.53mm" orientation="landscape" file="US20230004749A1-20230105-D00052.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00053" num="00053"><img id="EMI-D00053" he="240.79mm" wi="166.45mm" orientation="landscape" file="US20230004749A1-20230105-D00053.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00054" num="00054"><img id="EMI-D00054" he="197.19mm" wi="162.64mm" orientation="landscape" file="US20230004749A1-20230105-D00054.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00055" num="00055"><img id="EMI-D00055" he="197.19mm" wi="163.75mm" orientation="landscape" file="US20230004749A1-20230105-D00055.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00056" num="00056"><img id="EMI-D00056" he="213.19mm" wi="159.60mm" orientation="landscape" file="US20230004749A1-20230105-D00056.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00057" num="00057"><img id="EMI-D00057" he="237.66mm" wi="168.91mm" orientation="landscape" file="US20230004749A1-20230105-D00057.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00058" num="00058"><img id="EMI-D00058" he="247.99mm" wi="173.06mm" orientation="landscape" file="US20230004749A1-20230105-D00058.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00059" num="00059"><img id="EMI-D00059" he="217.25mm" wi="170.69mm" orientation="landscape" file="US20230004749A1-20230105-D00059.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00060" num="00060"><img id="EMI-D00060" he="240.20mm" wi="135.72mm" file="US20230004749A1-20230105-D00060.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00061" num="00061"><img id="EMI-D00061" he="240.79mm" wi="171.62mm" orientation="landscape" file="US20230004749A1-20230105-D00061.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00062" num="00062"><img id="EMI-D00062" he="235.97mm" wi="171.53mm" orientation="landscape" file="US20230004749A1-20230105-D00062.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00063" num="00063"><img id="EMI-D00063" he="223.27mm" wi="145.71mm" file="US20230004749A1-20230105-D00063.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00064" num="00064"><img id="EMI-D00064" he="241.47mm" wi="149.10mm" orientation="landscape" file="US20230004749A1-20230105-D00064.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00065" num="00065"><img id="EMI-D00065" he="227.67mm" wi="173.23mm" orientation="landscape" file="US20230004749A1-20230105-D00065.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00066" num="00066"><img id="EMI-D00066" he="239.61mm" wi="172.21mm" orientation="landscape" file="US20230004749A1-20230105-D00066.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00067" num="00067"><img id="EMI-D00067" he="244.43mm" wi="172.21mm" orientation="landscape" file="US20230004749A1-20230105-D00067.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00068" num="00068"><img id="EMI-D00068" he="242.23mm" wi="171.70mm" orientation="landscape" file="US20230004749A1-20230105-D00068.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00069" num="00069"><img id="EMI-D00069" he="204.05mm" wi="164.59mm" orientation="landscape" file="US20230004749A1-20230105-D00069.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00070" num="00070"><img id="EMI-D00070" he="216.75mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00070.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00071" num="00071"><img id="EMI-D00071" he="216.75mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00071.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00072" num="00072"><img id="EMI-D00072" he="216.32mm" wi="148.59mm" orientation="landscape" file="US20230004749A1-20230105-D00072.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00073" num="00073"><img id="EMI-D00073" he="233.85mm" wi="174.41mm" orientation="landscape" file="US20230004749A1-20230105-D00073.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00074" num="00074"><img id="EMI-D00074" he="221.32mm" wi="165.18mm" orientation="landscape" file="US20230004749A1-20230105-D00074.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00075" num="00075"><img id="EMI-D00075" he="231.90mm" wi="148.76mm" orientation="landscape" file="US20230004749A1-20230105-D00075.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00076" num="00076"><img id="EMI-D00076" he="230.63mm" wi="167.72mm" orientation="landscape" file="US20230004749A1-20230105-D00076.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00077" num="00077"><img id="EMI-D00077" he="217.25mm" wi="151.55mm" orientation="landscape" file="US20230004749A1-20230105-D00077.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00078" num="00078"><img id="EMI-D00078" he="209.80mm" wi="170.01mm" orientation="landscape" file="US20230004749A1-20230105-D00078.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00079" num="00079"><img id="EMI-D00079" he="235.88mm" wi="169.33mm" orientation="landscape" file="US20230004749A1-20230105-D00079.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00080" num="00080"><img id="EMI-D00080" he="223.69mm" wi="170.69mm" orientation="landscape" file="US20230004749A1-20230105-D00080.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00081" num="00081"><img id="EMI-D00081" he="121.24mm" wi="155.79mm" orientation="landscape" file="US20230004749A1-20230105-D00081.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00082" num="00082"><img id="EMI-D00082" he="237.32mm" wi="168.66mm" orientation="landscape" file="US20230004749A1-20230105-D00082.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00083" num="00083"><img id="EMI-D00083" he="235.88mm" wi="172.72mm" orientation="landscape" file="US20230004749A1-20230105-D00083.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00084" num="00084"><img id="EMI-D00084" he="243.92mm" wi="156.04mm" orientation="landscape" file="US20230004749A1-20230105-D00084.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00085" num="00085"><img id="EMI-D00085" he="246.72mm" wi="171.79mm" orientation="landscape" file="US20230004749A1-20230105-D00085.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00086" num="00086"><img id="EMI-D00086" he="232.66mm" wi="171.53mm" orientation="landscape" file="US20230004749A1-20230105-D00086.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00087" num="00087"><img id="EMI-D00087" he="236.22mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00087.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00088" num="00088"><img id="EMI-D00088" he="241.47mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00088.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00089" num="00089"><img id="EMI-D00089" he="214.46mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00089.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00090" num="00090"><img id="EMI-D00090" he="244.43mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00090.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00091" num="00091"><img id="EMI-D00091" he="241.47mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00091.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00092" num="00092"><img id="EMI-D00092" he="214.55mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00092.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00093" num="00093"><img id="EMI-D00093" he="247.31mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00093.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00094" num="00094"><img id="EMI-D00094" he="247.23mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00094.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00095" num="00095"><img id="EMI-D00095" he="214.80mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00095.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00096" num="00096"><img id="EMI-D00096" he="249.34mm" wi="170.77mm" orientation="landscape" file="US20230004749A1-20230105-D00096.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00097" num="00097"><img id="EMI-D00097" he="246.13mm" wi="174.41mm" orientation="landscape" file="US20230004749A1-20230105-D00097.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00098" num="00098"><img id="EMI-D00098" he="206.25mm" wi="172.80mm" orientation="landscape" file="US20230004749A1-20230105-D00098.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00099" num="00099"><img id="EMI-D00099" he="221.23mm" wi="172.80mm" orientation="landscape" file="US20230004749A1-20230105-D00099.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00100" num="00100"><img id="EMI-D00100" he="224.96mm" wi="170.77mm" orientation="landscape" file="US20230004749A1-20230105-D00100.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00101" num="00101"><img id="EMI-D00101" he="230.63mm" wi="169.25mm" orientation="landscape" file="US20230004749A1-20230105-D00101.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00102" num="00102"><img id="EMI-D00102" he="226.31mm" wi="170.26mm" orientation="landscape" file="US20230004749A1-20230105-D00102.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00103" num="00103"><img id="EMI-D00103" he="246.04mm" wi="174.33mm" file="US20230004749A1-20230105-D00103.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00104" num="00104"><img id="EMI-D00104" he="242.23mm" wi="174.33mm" orientation="landscape" file="US20230004749A1-20230105-D00104.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00105" num="00105"><img id="EMI-D00105" he="243.08mm" wi="174.24mm" orientation="landscape" file="US20230004749A1-20230105-D00105.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00106" num="00106"><img id="EMI-D00106" he="242.23mm" wi="174.24mm" orientation="landscape" file="US20230004749A1-20230105-D00106.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00107" num="00107"><img id="EMI-D00107" he="245.79mm" wi="174.67mm" orientation="landscape" file="US20230004749A1-20230105-D00107.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00108" num="00108"><img id="EMI-D00108" he="239.69mm" wi="175.09mm" orientation="landscape" file="US20230004749A1-20230105-D00108.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00109" num="00109"><img id="EMI-D00109" he="239.95mm" wi="155.28mm" file="US20230004749A1-20230105-D00109.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00110" num="00110"><img id="EMI-D00110" he="239.69mm" wi="170.94mm" file="US20230004749A1-20230105-D00110.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00111" num="00111"><img id="EMI-D00111" he="242.23mm" wi="170.94mm" file="US20230004749A1-20230105-D00111.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00112" num="00112"><img id="EMI-D00112" he="219.37mm" wi="172.30mm" orientation="landscape" file="US20230004749A1-20230105-D00112.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00113" num="00113"><img id="EMI-D00113" he="238.59mm" wi="172.55mm" orientation="landscape" file="US20230004749A1-20230105-D00113.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00114" num="00114"><img id="EMI-D00114" he="184.91mm" wi="163.58mm" orientation="landscape" file="US20230004749A1-20230105-D00114.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00115" num="00115"><img id="EMI-D00115" he="246.46mm" wi="173.31mm" orientation="landscape" file="US20230004749A1-20230105-D00115.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00116" num="00116"><img id="EMI-D00116" he="246.04mm" wi="173.31mm" orientation="landscape" file="US20230004749A1-20230105-D00116.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00117" num="00117"><img id="EMI-D00117" he="245.87mm" wi="173.31mm" orientation="landscape" file="US20230004749A1-20230105-D00117.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00118" num="00118"><img id="EMI-D00118" he="241.38mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00118.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00119" num="00119"><img id="EMI-D00119" he="224.62mm" wi="169.08mm" orientation="landscape" file="US20230004749A1-20230105-D00119.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00120" num="00120"><img id="EMI-D00120" he="226.31mm" wi="156.38mm" orientation="landscape" file="US20230004749A1-20230105-D00120.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00121" num="00121"><img id="EMI-D00121" he="241.89mm" wi="147.07mm" file="US20230004749A1-20230105-D00121.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00122" num="00122"><img id="EMI-D00122" he="245.62mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00122.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00123" num="00123"><img id="EMI-D00123" he="236.64mm" wi="155.70mm" file="US20230004749A1-20230105-D00123.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00124" num="00124"><img id="EMI-D00124" he="246.80mm" wi="174.50mm" file="US20230004749A1-20230105-D00124.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00125" num="00125"><img id="EMI-D00125" he="245.36mm" wi="172.89mm" file="US20230004749A1-20230105-D00125.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00126" num="00126"><img id="EMI-D00126" he="228.43mm" wi="168.66mm" orientation="landscape" file="US20230004749A1-20230105-D00126.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00127" num="00127"><img id="EMI-D00127" he="203.03mm" wi="167.30mm" orientation="landscape" file="US20230004749A1-20230105-D00127.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00128" num="00128"><img id="EMI-D00128" he="230.29mm" wi="173.23mm" orientation="landscape" file="US20230004749A1-20230105-D00128.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00129" num="00129"><img id="EMI-D00129" he="229.62mm" wi="168.91mm" orientation="landscape" file="US20230004749A1-20230105-D00129.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00130" num="00130"><img id="EMI-D00130" he="229.62mm" wi="167.05mm" orientation="landscape" file="US20230004749A1-20230105-D00130.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00131" num="00131"><img id="EMI-D00131" he="216.07mm" wi="171.70mm" orientation="landscape" file="US20230004749A1-20230105-D00131.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00132" num="00132"><img id="EMI-D00132" he="233.09mm" wi="167.56mm" orientation="landscape" file="US20230004749A1-20230105-D00132.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00133" num="00133"><img id="EMI-D00133" he="215.48mm" wi="155.11mm" orientation="landscape" file="US20230004749A1-20230105-D00133.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00134" num="00134"><img id="EMI-D00134" he="224.71mm" wi="161.12mm" orientation="landscape" file="US20230004749A1-20230105-D00134.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00135" num="00135"><img id="EMI-D00135" he="224.54mm" wi="160.36mm" orientation="landscape" file="US20230004749A1-20230105-D00135.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00136" num="00136"><img id="EMI-D00136" he="235.63mm" wi="167.22mm" orientation="landscape" file="US20230004749A1-20230105-D00136.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00137" num="00137"><img id="EMI-D00137" he="164.51mm" wi="168.40mm" orientation="landscape" file="US20230004749A1-20230105-D00137.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00138" num="00138"><img id="EMI-D00138" he="222.08mm" wi="170.10mm" orientation="landscape" file="US20230004749A1-20230105-D00138.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00139" num="00139"><img id="EMI-D00139" he="186.69mm" wi="176.02mm" orientation="landscape" file="US20230004749A1-20230105-D00139.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00140" num="00140"><img id="EMI-D00140" he="238.08mm" wi="174.07mm" orientation="landscape" file="US20230004749A1-20230105-D00140.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00141" num="00141"><img id="EMI-D00141" he="209.04mm" wi="155.96mm" orientation="landscape" file="US20230004749A1-20230105-D00141.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00142" num="00142"><img id="EMI-D00142" he="210.65mm" wi="157.65mm" orientation="landscape" file="US20230004749A1-20230105-D00142.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00143" num="00143"><img id="EMI-D00143" he="226.65mm" wi="159.68mm" orientation="landscape" file="US20230004749A1-20230105-D00143.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00144" num="00144"><img id="EMI-D00144" he="210.74mm" wi="164.08mm" orientation="landscape" file="US20230004749A1-20230105-D00144.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00145" num="00145"><img id="EMI-D00145" he="236.14mm" wi="171.96mm" orientation="landscape" file="US20230004749A1-20230105-D00145.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00146" num="00146"><img id="EMI-D00146" he="218.78mm" wi="166.96mm" orientation="landscape" file="US20230004749A1-20230105-D00146.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00147" num="00147"><img id="EMI-D00147" he="236.14mm" wi="171.11mm" orientation="landscape" file="US20230004749A1-20230105-D00147.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00148" num="00148"><img id="EMI-D00148" he="242.06mm" wi="171.37mm" orientation="landscape" file="US20230004749A1-20230105-D00148.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00149" num="00149"><img id="EMI-D00149" he="245.70mm" wi="173.82mm" file="US20230004749A1-20230105-D00149.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00150" num="00150"><img id="EMI-D00150" he="231.06mm" wi="171.37mm" orientation="landscape" file="US20230004749A1-20230105-D00150.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00151" num="00151"><img id="EMI-D00151" he="238.51mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00151.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00152" num="00152"><img id="EMI-D00152" he="231.22mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00152.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00153" num="00153"><img id="EMI-D00153" he="206.33mm" wi="171.28mm" orientation="landscape" file="US20230004749A1-20230105-D00153.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00154" num="00154"><img id="EMI-D00154" he="226.99mm" wi="173.23mm" orientation="landscape" file="US20230004749A1-20230105-D00154.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00155" num="00155"><img id="EMI-D00155" he="237.57mm" wi="159.94mm" file="US20230004749A1-20230105-D00155.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00156" num="00156"><img id="EMI-D00156" he="237.57mm" wi="164.25mm" file="US20230004749A1-20230105-D00156.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00157" num="00157"><img id="EMI-D00157" he="240.96mm" wi="174.33mm" orientation="landscape" file="US20230004749A1-20230105-D00157.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00158" num="00158"><img id="EMI-D00158" he="241.05mm" wi="174.33mm" orientation="landscape" file="US20230004749A1-20230105-D00158.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00159" num="00159"><img id="EMI-D00159" he="244.94mm" wi="173.65mm" orientation="landscape" file="US20230004749A1-20230105-D00159.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00160" num="00160"><img id="EMI-D00160" he="244.94mm" wi="173.65mm" orientation="landscape" file="US20230004749A1-20230105-D00160.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00161" num="00161"><img id="EMI-D00161" he="241.05mm" wi="171.79mm" orientation="landscape" file="US20230004749A1-20230105-D00161.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00162" num="00162"><img id="EMI-D00162" he="246.21mm" wi="174.33mm" orientation="landscape" file="US20230004749A1-20230105-D00162.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00163" num="00163"><img id="EMI-D00163" he="237.91mm" wi="173.82mm" orientation="landscape" file="US20230004749A1-20230105-D00163.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00164" num="00164"><img id="EMI-D00164" he="246.63mm" wi="167.98mm" orientation="landscape" file="US20230004749A1-20230105-D00164.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00165" num="00165"><img id="EMI-D00165" he="247.23mm" wi="149.86mm" orientation="landscape" file="US20230004749A1-20230105-D00165.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00166" num="00166"><img id="EMI-D00166" he="232.75mm" wi="173.23mm" orientation="landscape" file="US20230004749A1-20230105-D00166.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00167" num="00167"><img id="EMI-D00167" he="205.99mm" wi="166.62mm" orientation="landscape" file="US20230004749A1-20230105-D00167.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00168" num="00168"><img id="EMI-D00168" he="226.91mm" wi="156.80mm" orientation="landscape" file="US20230004749A1-20230105-D00168.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00169" num="00169"><img id="EMI-D00169" he="224.71mm" wi="158.75mm" orientation="landscape" file="US20230004749A1-20230105-D00169.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">PRIORITY APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of U.S. Nonprovisional patent application Ser. No. 16/826,168, entitled &#x201c;Artificial Intelligence-Based Sequencing,&#x201d; filed 21 Mar. 2020 (Attorney Docket No. ILLM 1008-20/IP-1752-US), which in turn claims priority to or the benefit of the following applications:</p><p id="p-0003" num="0002">U.S. Provisional Patent Application No. 62/821,602, entitled &#x201c;Training Data Generation for Artificial Intelligence-Based Sequencing,&#x201d; filed 21 Mar. 2019 (Attorney Docket No. ILLM 1008-1/IP-1693-PRV);</p><p id="p-0004" num="0003">U.S. Provisional Patent Application No. 62/821,618, entitled &#x201c;Artificial Intelligence-Based Generation of Sequencing Metadata,&#x201d; filed 21 Mar. 2019 (Attorney Docket No. ILLM 1008-3/IP-1741-PRV);</p><p id="p-0005" num="0004">U.S. Provisional Patent Application No. 62/821,681, entitled &#x201c;Artificial Intelligence-Based Base Calling,&#x201d; filed 21 Mar. 2019 (Attorney Docket No. ILLM 1008-4/IP-1744-PRV);</p><p id="p-0006" num="0005">U.S. Provisional Patent Application No. 62/821,724, entitled &#x201c;Artificial Intelligence-Based Quality Scoring,&#x201d; filed 21 Mar. 2019 (Attorney Docket No. ILLM 1008-7/IP-1747-PRV);</p><p id="p-0007" num="0006">U.S. Provisional Patent Application No. 62/821,766, entitled &#x201c;Artificial Intelligence-Based Sequencing,&#x201d; filed 21 Mar. 2019 (Attorney Docket No. ILLM 1008-9/IP-1752-PRV);</p><heading id="h-0002" level="1">US Non-Provisional Applications</heading><p id="p-0008" num="0007">U.S. patent application Ser. No. 16/825,987, entitled &#x201c;Training Data Generation for Artificial Intelligence-Based Sequencing,&#x201d; (Attorney Docket No. ILLM 1008-16/IP-1693-US) filed on Mar. 20, 2020;</p><p id="p-0009" num="0008">U.S. patent application Ser. No. 16/825,991, entitled &#x201c;Artificial Intelligence-Based Generation of Sequencing Metadata,&#x201d; (Attorney Docket No. ILLM 1008-17/IP-1741-US) filed on Mar. 20, 2020;</p><p id="p-0010" num="0009">U.S. patent application Ser. No. 16/826,126, entitled &#x201c;Artificial Intelligence-Based Base Calling,&#x201d; (Attorney Docket No. ILLM 1008-18/IP-1744-US) filed on Mar. 20, 2020; and</p><p id="p-0011" num="0010">U.S. patent application Ser. No. 16/826,134, entitled &#x201c;Artificial Intelligence-Based Quality Scoring,&#x201d; (Attorney Docket No. ILLM 1008-19/IP-1747-US) filed on Mar. 20, 2020.</p><heading id="h-0003" level="1">PCT Applications</heading><p id="p-0012" num="0011">PCT Patent Application No. PCT/US2020/024090, titled &#x201c;Training Data Generation for Artificial Intelligence-Based Sequencing,&#x201d; (Attorney Docket No. ILLM 1008-21/IP-1693-PCT) filed on Mar. 21, 2020, subsequently published as PCT Publication No. WO 2020/191389 A1;</p><p id="p-0013" num="0012">PCT Patent Application No. PCT/US2020/024087, titled &#x201c;Artificial Intelligence-Based Generation of Sequencing Metadata,&#x201d; (Attorney Docket No. ILLM 1008-22/IP-1741-PCT) filed on Mar. 21, 2020, subsequently published as PCT Publication No. WO 2020/205296 A1;</p><p id="p-0014" num="0013">PCT Patent Application No. PCT/US2020/024088, titled &#x201c;Artificial Intelligence-Based Base Calling,&#x201d; (Attorney Docket No. ILLM 1008-23/IP-1744-PCT) filed on Mar. 21, 2020, subsequently published as PCT Publication No. WO 2020/191387 A1;</p><p id="p-0015" num="0014">PCT Patent Application No. PCT/US2020/024091, titled &#x201c;Artificial Intelligence-Based Quality Scoring,&#x201d; (Attorney Docket No. ILLM 1008-24/IP-1747-PCT) filed on Mar. 21, 2020, subsequently published as PCT Publication No. WO 2020/191390 A2;</p><p id="p-0016" num="0015">PCT Patent Application No. PCT/US2020/024092, titled &#x201c;Artificial Intelligence-Based Sequencing,&#x201d; (Attorney Docket No. ILLM 1008-25/IP-1752-PCT) filed on Mar. 22, 2020, subsequently published as PCT Publication No. WO 2020/191391 A3.</p><p id="p-0017" num="0016">The priority applications are hereby incorporated by reference for all purposes as if fully set forth herein.</p><heading id="h-0004" level="1">INCORPORATIONS</heading><p id="p-0018" num="0017">The following are incorporated by reference for all purposes as if fully set forth herein:</p><p id="p-0019" num="0018">U.S. Provisional Patent Application No. 62/849,091, entitled, &#x201c;Systems and Devices for Characterization and Performance Analysis of Pixel-Based Sequencing,&#x201d; filed May 16, 2019 (Attorney Docket No. ILLM 1011-1/IP-1750-PRV);</p><p id="p-0020" num="0019">U.S. Provisional Patent Application No. 62/849,132, entitled, &#x201c;Base Calling Using Convolutions,&#x201d; filed May 16, 2019 (Attorney Docket No. ILLM 1011-2/IP-1750-PR2);</p><p id="p-0021" num="0020">U.S. Provisional Patent Application No. 62/849,133, entitled, &#x201c;Base Calling Using Compact Convolutions,&#x201d; filed May 16, 2019 (Attorney Docket No. ILLM 1011-3/IP-1750-PR3);</p><p id="p-0022" num="0021">U.S. Provisional Patent Application No. 62/979,384, entitled, &#x201c;Artificial Intelligence-Based Base Calling of Index Sequences,&#x201d; filed Feb. 20, 2020 (Attorney Docket No. ILLM 1015-1/IP-1857-PRV);</p><p id="p-0023" num="0022">U.S. Provisional Patent Application No. 62/979,414, entitled, &#x201c;Artificial Intelligence-Based Many-To-Many Base Calling,&#x201d; filed Feb. 20, 2020 (Attorney Docket No. ILLM 1016-1/IP-1858-PRV);</p><p id="p-0024" num="0023">U.S. Provisional Patent Application No. 62/979,385, entitled, &#x201c;Knowledge Distillation-Based Compression of Artificial Intelligence-Based Base Caller,&#x201d; filed Feb. 20, 2020 (Attorney Docket No. ILLM 1017-1/IP-1859-PRV);</p><p id="p-0025" num="0024">U.S. Provisional Patent Application No. 62/979,412, entitled, &#x201c;Multi-Cycle Cluster Based Real Time Analysis System,&#x201d; filed Feb. 20, 2020 (Attorney Docket No. ILLM 1020-1/IP-1866-PRV);</p><p id="p-0026" num="0025">U.S. Provisional Patent Application No. 62/979,411, entitled, &#x201c;Data Compression for Artificial Intelligence-Based Base Calling,&#x201d; filed Feb. 20, 2020 (Attorney Docket No. ILLM 1029-1/IP-1964-PRV);</p><p id="p-0027" num="0026">U.S. Provisional Patent Application No. 62/979,399, entitled, &#x201c;Squeezing Layer for Artificial Intelligence-Based Base Calling,&#x201d; filed Feb. 20, 2020 (Attorney Docket No. ILLM 1030-1/IP-1982-PRV);</p><p id="p-0028" num="0027">Liu P, Hemani A, Paul K, Weis C, Jung M, Wehn N. 3D-Stacked Many-Core Architecture for Biological Sequence Analysis Problems. Int J Parallel Prog. 2017; 45(6):1420-60;</p><p id="p-0029" num="0028">Z. Wu, K. Hammad, R. Mittmann, S. Magierowski, E. Ghafar-Zadeh, and X. Zhong, &#x201c;FPGA-Based DNA Basecalling Hardware Acceleration,&#x201d; in Proc. IEEE 61st Int. Midwest Symp. Circuits Syst., August 2018, pp. 1098-1101;</p><p id="p-0030" num="0029">Z. Wu, K. Hammad, E. Ghafar-Zadeh, and S. Magierowski, &#x201c;FPGA-Accelerated 3rd Generation DNA Sequencing,&#x201d; in IEEE Transactions on Biomedical Circuits and Systems, Volume 14, Issue 1, February 2020, pp. 65-74;</p><p id="p-0031" num="0030">Prabhakar et al., &#x201c;Plasticine: A Reconfigurable Architecture for Parallel Patterns,&#x201d; ISCA 17, Jun. 24-28, 2017, Toronto, ON, Canada;</p><p id="p-0032" num="0031">M. Lin, Q. Chen, and S. Yan, &#x201c;Network in Network,&#x201d; in Proc. of ICLR, 2014;</p><p id="p-0033" num="0032">L. Sifre, &#x201c;Rigid-motion Scattering for Image Classification, Ph.D. thesis, 2014;</p><p id="p-0034" num="0033">L. Sifre and S. Mallat, &#x201c;Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination,&#x201d; in Proc. of CVPR, 2013;</p><p id="p-0035" num="0034">F. Chollet, &#x201c;Xception: Deep Learning with Depthwise Separable Convolutions,&#x201d; in Proc. of CVPR, 2017;</p><p id="p-0036" num="0035">X. Zhang, X. Zhou, M. Lin, and J. Sun, &#x201c;ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices,&#x201d; in arXiv: 1707.01083, 2017;</p><p id="p-0037" num="0036">K. He, X. Zhang, S. Ren, and J. Sun, &#x201c;Deep Residual Learning for Image Recognition,&#x201d; in Proc. of CVPR, 2016;</p><p id="p-0038" num="0037">S. Xie, R. Girshick, P. Dollar, Z. Tu, and K. He, &#x201c;Aggregated Residual Transformations for Deep Neural Networks,&#x201d; in Proc. of CVPR, 2017;</p><p id="p-0039" num="0038">A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, &#x201c;Mobilenets: Efficient Convolutional Neural Networks for Mobile Vision Applications,&#x201d; in arXiv: 1704.04861, 2017;</p><p id="p-0040" num="0039">M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L. Chen, &#x201c;MobileNetV2: Inverted Residuals and Linear Bottlenecks,&#x201d; in arXiv: 1801.04381v3, 2018;</p><p id="p-0041" num="0040">Z. Qin, Z. Zhang, X. Chen, and Y. Peng, &#x201c;FD-MobileNet: Improved MobileNet with a Fast Downsampling Strategy,&#x201d; in arXiv: 1802.03750, 2018;</p><p id="p-0042" num="0041">Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. CoRR, abs/1706.05587, 2017;</p><p id="p-0043" num="0042">J. Huang, V. Rathod, C. Sun, M. Zhu, A. Korattikara, A. Fathi, I. Fischer, Z. Wojna, Y. Song, S. Guadarrama, et al. Speed/accuracy trade-offs for modern convolutional object detectors. arXiv preprint arXiv: 1611.10012, 2016;</p><p id="p-0044" num="0043">S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, &#x201c;WAVENET: A GENERATIVE MODEL FOR RAW AUDIO,&#x201d; arXiv: 1609.03499, 2016;</p><p id="p-0045" num="0044">S. O. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, Y. Kang, X. Li, J. Miller, A. Ng, J. Raiman, S. Sengupta and M. Shoeybi, &#x201c;DEEP VOICE: REAL-TIME NEURAL TEXT-TO-SPEECH,&#x201d; arXiv: 1702.07825, 2017;</p><p id="p-0046" num="0045">F. Yu and V. Koltun, &#x201c;MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS,&#x201d; arXiv: 1511.07122, 2016;</p><p id="p-0047" num="0046">K. He, X. Zhang, S. Ren, and J. Sun, &#x201c;DEEP RESIDUAL LEARNING FOR IMAGE RECOGNITION,&#x201d; arXiv: 1512.03385, 2015;</p><p id="p-0048" num="0047">R. K. Srivastava, K. Greff, and J. Schmidhuber, &#x201c;HIGHWAY NETWORKS,&#x201d; arXiv: 1505.00387, 2015;</p><p id="p-0049" num="0048">G. Huang, Z. Liu, L. van der Maaten and K. Q. Weinberger, &#x201c;DENSELY CONNECTED CONVOLUTIONAL NETWORKS,&#x201d; arXiv: 1608.06993, 2017;</p><p id="p-0050" num="0049">C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, &#x201c;GOING DEEPER WITH CONVOLUTIONS,&#x201d; arXiv: 1409.4842, 2014;</p><p id="p-0051" num="0050">S. Ioffe and C. Szegedy, &#x201c;BATCH NORMALIZATION: ACCELERATING DEEP NETWORK TRAINING BY REDUCING INTERNAL COVARIATE SHIFT,&#x201d; arXiv: 1502.03167, 2015;</p><p id="p-0052" num="0051">J. M. Wolterink, T. Leiner, M. A. Viergever, and I. Isgum, &#x201c;DILATED CONVOLUTIONAL NEURAL NETWORKS FOR CARDIOVASCULAR MR SEGMENTATION IN CONGENITAL HEART DISEASE,&#x201d; arXiv: 1704.03669, 2017;</p><p id="p-0053" num="0052">L. C. Piqueras, &#x201c;AUTOREGRESSIVE MODEL BASED ON A DEEP CONVOLUTIONAL NEURAL NETWORK FOR AUDIO GENERATION,&#x201d; Tampere University of Technology, 2016;</p><p id="p-0054" num="0053">J. Wu, &#x201c;Introduction to Convolutional Neural Networks,&#x201d; Nanjing University, 2017;</p><p id="p-0055" num="0054">&#x201c;Illumina CMOS Chip and One-Channel SBS Chemistry&#x201d;, Illumina, Inc. 2018, 2 pages;</p><p id="p-0056" num="0055">&#x201c;skikit-image/peak.py at master&#x201d;, GitHub, 5 pages, [retrieved on 2018-11-16]. Retrieved from the Internet &#x3c;URL: (https://)github.com/scikit-image/scikit-image/blob/master/skimage/feature/peak.py#L25&#x3e;;</p><p id="p-0057" num="0056">&#x201c;3.3.9.11. Watershed and random walker for segmentation&#x201d;, Scipy lecture notes, 2 pages, [retrieved on 2018-11-13]. Retrieved from the Internet &#x3c;URL: (http://)scipy-lectures.org/packages/scikit-image/auto_examples/plot_segmentations.html&#x3e;;</p><p id="p-0058" num="0057">Mordvintsev, Alexander and Revision, Abid K., &#x201c;Image Segmentation with Watershed Algorithm&#x201d;, Revision 43532856, 2013, 6 pages [retrieved on 2018-11-13]. Retrieved from the Internet &#x3c;URL: (https://)opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_watershed/py_watershed.html&#x3e;;</p><p id="p-0059" num="0058">Mzur, &#x201c;Watershed.py&#x201d;, 25 Oct. 2017, 3 pages, [retrieved on 2018-11-13]. Retrieved from the Internet &#x3c;URL: (https://)github.com/mzur/watershed/blob/master/Watershed.py&#x3e;;</p><p id="p-0060" num="0059">Thakur, Pratibha, et. al. &#x201c;A Survey of Image Segmentation Techniques&#x201d;, International Journal of Research in Computer Applications and Robotics, Vol. 2, Issue. 4, April 2014, Pg.: 158-165;</p><p id="p-0061" num="0060">Long, Jonathan, et. al., &#x201c;Fully Convolutional Networks for Semantic Segmentation&#x201d;, IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol 39, Issue 4, 1 Apr. 2017, 10 pages;</p><p id="p-0062" num="0061">Ronneberger, Olaf, et al., &#x201c;U-net: Convolutional networks for biomedical image segmentation.&#x201d; In International Conference on Medical image computing and computer-assisted intervention, 18 May 2015, 8 pages;</p><p id="p-0063" num="0062">Xie, W., et. al., &#x201c;Microscopy cell counting and detection with fully convolutional regression networks&#x201d;, Computer methods in biomechanics and biomedical engineering: Imaging &#x26; Visualization, 6(3), pp. 283-292, 2018;</p><p id="p-0064" num="0063">Xie, Yuanpu, et al., &#x201c;Beyond classification: structured regression for robust cell detection using convolutional neural network&#x201d;, International Conference on Medical Image Computing and Computer Assisted Intervention. October 2015, 12 pages;</p><p id="p-0065" num="0064">Snuverink, I. A. F., &#x201c;Deep Learning for Pixelwise Classification of Hyperspectral Images&#x201d;, Master of Science Thesis, Delft University of Technology, 23 Nov. 2017, 19 pages;</p><p id="p-0066" num="0065">Shevchenko, A., &#x201c;Keras weighted categorical_crossentropy&#x201d;, 1 page, [retrieved on 2019-01-15]. Retrieved from the Internet &#x3c;URL: (https://)gist.github.com/skeeet/cad06d584548fb45eece1d4e28cfa98b&#x3e;;</p><p id="p-0067" num="0066">van den Assem, D.C.F., &#x201c;Predicting periodic and chaotic signals using Wavenets&#x201d;, Master of Science Thesis, Delft University of Technology, 18 Aug. 2017, pages 3-38;</p><p id="p-0068" num="0067">I. J. Goodfellow, D. Warde-Farley, M. Mirza, A. Courville, and Y. Bengio, &#x201c;CONVOLUTIONAL NETWORKS&#x201d;, Deep Learning, MIT Press, 2016; and</p><p id="p-0069" num="0068">J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, and G. Wang, &#x201c;RECENT ADVANCES IN CONVOLUTIONAL NEURAL NETWORKS,&#x201d; arXiv: 1512.07108, 2017.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0005" level="1">FIELD OF THE TECHNOLOGY DISCLOSED</heading><p id="p-0070" num="0069">The technology disclosed relates to artificial intelligence type computers and digital data processing systems and corresponding data processing methods and products for emulation of intelligence (i.e., knowledge based systems, reasoning systems, and knowledge acquisition systems); and including systems for reasoning with uncertainty (e.g., fuzzy logic systems), adaptive systems, machine learning systems, and artificial neural networks. In particular, the technology disclosed relates to using deep neural networks such as deep convolutional neural networks for analyzing data.</p><heading id="h-0006" level="1">BACKGROUND</heading><p id="p-0071" num="0070">The subject matter discussed in this section should not be assumed to be prior art merely as a result of its mention in this section. Similarly, a problem mentioned in this section or associated with the subject matter provided as background should not be assumed to have been previously recognized in the prior art. The subject matter in this section merely represents different approaches, which in and of themselves can also correspond to implementations of the claimed technology.</p><p id="p-0072" num="0071">Deep neural networks are a type of artificial neural networks that use multiple nonlinear and complex transforming layers to successively model high-level features. Deep neural networks provide feedback via backpropagation which carries the difference between observed and predicted output to adjust parameters. Deep neural networks have evolved with the availability of large training datasets, the power of parallel and distributed computing, and sophisticated training algorithms. Deep neural networks have facilitated major advances in numerous domains such as computer vision, speech recognition, and natural language processing.</p><p id="p-0073" num="0072">Convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are components of deep neural networks. Convolutional neural networks have succeeded particularly in image recognition with an architecture that comprises convolution layers, nonlinear layers, and pooling layers. Recurrent neural networks are designed to utilize sequential information of input data with cyclic connections among building blocks like perceptrons, long short-term memory units, and gated recurrent units. In addition, many other emergent deep neural networks have been proposed for limited contexts, such as deep spatio-temporal neural networks, multi-dimensional recurrent neural networks, and convolutional auto-encoders.</p><p id="p-0074" num="0073">The goal of training deep neural networks is optimization of the weight parameters in each layer, which gradually combines simpler features into complex features so that the most suitable hierarchical representations can be learned from data. A single cycle of the optimization process is organized as follows. First, given a training dataset, the forward pass sequentially computes the output in each layer and propagates the function signals forward through the network. In the final output layer, an objective loss function measures error between the inferenced outputs and the given labels. To minimize the training error, the backward pass uses the chain rule to backpropagate error signals and compute gradients with respect to all weights throughout the neural network. Finally, the weight parameters are updated using optimization algorithms based on stochastic gradient descent. Whereas batch gradient descent performs parameter updates for each complete dataset, stochastic gradient descent provides stochastic approximations by performing the updates for each small set of data examples. Several optimization algorithms stem from stochastic gradient descent. For example, the Adagrad and Adam training algorithms perform stochastic gradient descent while adaptively modifying learning rates based on update frequency and moments of the gradients for each parameter, respectively.</p><p id="p-0075" num="0074">Another core element in the training of deep neural networks is regularization, which refers to strategies intended to avoid overfitting and thus achieve good generalization performance. For example, weight decay adds a penalty term to the objective loss function so that weight parameters converge to smaller absolute values. Dropout randomly removes hidden units from neural networks during training and can be considered an ensemble of possible subnetworks. To enhance the capabilities of dropout, a new activation function, maxout, and a variant of dropout for recurrent neural networks called rnnDrop have been proposed. Furthermore, batch normalization provides a new regularization method through normalization of scalar features for each activation within a mini-batch and learning each mean and variance as parameters.</p><p id="p-0076" num="0075">Given that sequenced data are multi- and high-dimensional, deep neural networks have great promise for bioinformatics research because of their broad applicability and enhanced prediction power. Convolutional neural networks have been adapted to solve sequence-based problems in genomics such as motif discovery, pathogenic variant identification, and gene expression inference. Convolutional neural networks use a weight-sharing strategy that is especially useful for studying DNA because it can capture sequence motifs, which are short, recurring local patterns in DNA that are presumed to have significant biological functions. A hallmark of convolutional neural networks is the use of convolution filters.</p><p id="p-0077" num="0076">Unlike traditional classification approaches that are based on elaborately-designed and manually-crafted features, convolution filters perform adaptive learning of features, analogous to a process of mapping raw input data to the informative representation of knowledge. In this sense, the convolution filters serve as a series of motif scanners, since a set of such filters is capable of recognizing relevant patterns in the input and updating themselves during the training procedure. Recurrent neural networks can capture long-range dependencies in sequential data of varying lengths, such as protein or DNA sequences.</p><p id="p-0078" num="0077">Therefore, an opportunity arises to use a principled deep learning-based framework for template generation and base calling.</p><p id="p-0079" num="0078">In the era of high-throughput technology, amassing the highest yield of interpretable data at the lowest cost per effort remains a significant challenge. Cluster-based methods of nucleic acid sequencing, such as those that utilize bridge amplification for cluster formation, have made a valuable contribution toward the goal of increasing the throughput of nucleic acid sequencing. These cluster-based methods rely on sequencing a dense population of nucleic acids immobilized on a solid support, and typically involve the use of image analysis software to deconvolve optical signals generated in the course of simultaneously sequencing multiple clusters situated at distinct locations on a solid support.</p><p id="p-0080" num="0079">However, such solid-phase nucleic acid cluster-based sequencing technologies still face considerable obstacles that limit the amount of throughput that can be achieved. For example, in cluster-based sequencing methods, determining the nucleic acid sequences of two or more clusters that are physically too close to one another to be resolved spatially, or that in fact physically overlap on the solid support, can pose an obstacle. For example, current image analysis software can require valuable time and computational resources for determining from which of two overlapping clusters an optical signal has emanated. As a consequence, compromises are inevitable for a variety of detection platforms with respect to the quantity and/or quality of nucleic acid sequence information that can be obtained.</p><p id="p-0081" num="0080">High density nucleic acid cluster-based genomics methods extend to other areas of genome analysis as well. For example, nucleic acid cluster-based genomics can be used in sequencing applications, diagnostics and screening, gene expression analysis, epigenetic analysis, genetic analysis of polymorphisms, and the like. Each of these nucleic acid cluster-based genomics technologies, too, is limited when there is an inability to resolve data generated from closely proximate or spatially overlapping nucleic acid clusters.</p><p id="p-0082" num="0081">Clearly there remains a need for increasing the quality and quantity of nucleic acid sequencing data that can be obtained rapidly and cost-effectively for a wide variety of uses, including for genomics (e.g., for genome characterization of any and all animal, plant, microbial or other biological species or populations), pharmacogenomics, transcriptomics, diagnostics, prognostics, biomedical risk assessment, clinical and research genetics, personalized medicine, drug efficacy and drug interactions assessments, veterinary medicine, agriculture, evolutionary and biodiversity studies, aquaculture, forestry, oceanography, ecological and environmental management, and other purposes.</p><p id="p-0083" num="0082">The technology disclosed provides neural network-based methods and systems that address these and similar needs, including increasing the level of throughput in high-throughput nucleic acid sequencing technologies, and offers other related advantages.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0007" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0084" num="0083">The patent or application file contains at least one drawing executed in color. Copies of this patent or patent application publication with color drawing(s) will be provided by the Office upon request and payment of the necessary fee.</p><p id="p-0085" num="0084">The color drawings also may be available in PAIR via the Supplemental Content tab. In the drawings, like reference characters generally refer to like parts throughout the different views. Also, the drawings are not necessarily to scale, with an emphasis instead generally being placed upon illustrating the principles of the technology disclosed. In the following description, various implementations of the technology disclosed are described with reference to the following drawings, in which:</p><p id="p-0086" num="0085"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows one implementation of a processing pipeline that determines cluster metadata using subpixel base calling.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts one implementation of a flow cell that contains clusters in its tiles.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates one example of the Illumina GA-IIx flow cell with eight lanes.</p><p id="p-0089" num="0088"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an image set of sequencing images for four-channel chemistry, i.e., the image set has four sequencing images, captured using four different wavelength bands (image/imaging channel) in the pixel domain.</p><p id="p-0090" num="0089"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is one implementation of dividing a sequencing image into subpixels (or subpixel regions).</p><p id="p-0091" num="0090"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows preliminary center coordinates of the clusters identified by the base caller during the subpixel base calling.</p><p id="p-0092" num="0091"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts one implementation of merging subpixel base calls produced over the plurality of sequencing cycles to generate the so-called &#x201c;cluster maps&#x201d; that contain the cluster metadata.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>illustrates one example of a cluster map generated by the merging of the subpixel base calls.</p><p id="p-0094" num="0093"><figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>depicts one implementation of subpixel base calling.</p><p id="p-0095" num="0094"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows another example of a cluster map that identifies cluster metadata.</p><p id="p-0096" num="0095"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows how a center of mass (COM) of a disjointed region in a cluster map is calculated.</p><p id="p-0097" num="0096"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts one implementation of calculation of a weighted decay factor based on the Euclidean distance from a subpixel in a disjointed region to the COM of the disjointed region.</p><p id="p-0098" num="0097"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates one implementation of an example ground truth decay map derived from an example cluster map produced by the subpixel base calling.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates one implementation of deriving a ternary map from a cluster map.</p><p id="p-0100" num="0099"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates one implementation of deriving a binary map from a cluster map.</p><p id="p-0101" num="0100"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram that shows one implementation of generating training data that is used to train the neural network-based template generator and the neural network-based base caller.</p><p id="p-0102" num="0101"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows characteristics of the disclosed training examples used to train the neural network-based template generator and the neural network-based base caller.</p><p id="p-0103" num="0102"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates one implementation of processing input image data through the disclosed neural network-based template generator and generating an output value for each unit in an array. In one implementation, the array is a decay map. In another implementation, the array is a ternary map. In yet another implementation, the array is a binary map.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows one implementation of post-processing techniques that are applied to the decay map, the ternary map, or the binary map produced by the neural network-based template generator to derive cluster metadata, including cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries.</p><p id="p-0105" num="0104"><figref idref="DRAWINGS">FIG. <b>19</b></figref> depicts one implementation of extracting cluster intensity in the pixel domain.</p><p id="p-0106" num="0105"><figref idref="DRAWINGS">FIG. <b>20</b></figref> illustrates one implementation of extracting cluster intensity in the subpixel domain.</p><p id="p-0107" num="0106"><figref idref="DRAWINGS">FIG. <b>21</b><i>a </i></figref>shows three different implementations of the neural network-based template generator.</p><p id="p-0108" num="0107"><figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>depicts one implementation of the input image data that is fed as input to the neural network-based template generator <b>1512</b>. The input image data comprises a series of image sets with sequencing images that are generated during a certain number of initial sequences cycles of a sequencing run.</p><p id="p-0109" num="0108"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows one implementation of extracting patches from the series of image sets in <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>to produce a series of &#x201c;down-sized&#x201d; image sets that form the input image data.</p><p id="p-0110" num="0109"><figref idref="DRAWINGS">FIG. <b>23</b></figref> depicts one implementation of upsampling the series of image sets in <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>to produce a series of &#x201c;upsampled&#x201d; image sets that forms the input image data.</p><p id="p-0111" num="0110"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows one implementation of extracting patches from the series of upsampled image sets in <figref idref="DRAWINGS">FIG. <b>23</b></figref> to produce a series of &#x201c;upsampled and down-sized&#x201d; image sets that form the input image data.</p><p id="p-0112" num="0111"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates one implementation of an overall example process of generating ground truth data for training the neural network-based template generator.</p><p id="p-0113" num="0112"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates one implementation of the regression model.</p><p id="p-0114" num="0113"><figref idref="DRAWINGS">FIG. <b>27</b></figref> depicts one implementation of generating a ground truth decay map from a cluster map. The ground truth decay map is used as ground truth data for training the regression model.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is one implementation of training the regression model using a backpropagation-based gradient update technique.</p><p id="p-0116" num="0115"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is one implementation of template generation by the regression model during inference.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>30</b></figref> illustrates one implementation of subjecting the decay map to post-processing to identify cluster metadata.</p><p id="p-0118" num="0117"><figref idref="DRAWINGS">FIG. <b>31</b></figref> depicts one implementation of a watershed segmentation technique identifying non-overlapping groups of contiguous cluster/cluster interior subpixels that characterize the clusters.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a table that shows an example U-Net architecture of the regression model.</p><p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. <b>33</b></figref> illustrates different approaches of extracting cluster intensity using cluster shape information identified in a template image.</p><p id="p-0121" num="0120"><figref idref="DRAWINGS">FIG. <b>34</b></figref> shows different approaches of base calling using the outputs of the regression model.</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. <b>35</b></figref> illustrates the difference in base calling performance when the RTA base caller uses ground truth center of mass (COM) location as the cluster center, as opposed to using a non-COM location as the cluster center. The results show that using COM improves base calling.</p><p id="p-0123" num="0122"><figref idref="DRAWINGS">FIG. <b>36</b></figref> shows, on the left, an example decay map produced the regression model. On the right, <figref idref="DRAWINGS">FIG. <b>36</b></figref> also shows an example ground truth decay map that the regression model approximates during the training.</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>37</b></figref> portrays one implementation of the peak locator identifying cluster centers in the decay map by detecting peaks.</p><p id="p-0125" num="0124"><figref idref="DRAWINGS">FIG. <b>38</b></figref> compares peaks detected by the peak locator in a decay map produced by the regression model with peaks in a corresponding ground truth decay map.</p><p id="p-0126" num="0125"><figref idref="DRAWINGS">FIG. <b>39</b></figref> illustrates performance of the regression model using precision and recall statistics.</p><p id="p-0127" num="0126"><figref idref="DRAWINGS">FIG. <b>40</b></figref> compares performance of the regression model with the RTA base caller for 20 pM library concentration (normal run).</p><p id="p-0128" num="0127"><figref idref="DRAWINGS">FIG. <b>41</b></figref> compares performance of the regression model with the RTA base caller for 30 pM library concentration (dense run).</p><p id="p-0129" num="0128"><figref idref="DRAWINGS">FIG. <b>42</b></figref> compares number of non-duplicate proper read pairs, i.e., the number of paired reads that do not have both reads aligned inwards within a reasonable distance detected by the regression model versus the same detected by the RTA base caller.</p><p id="p-0130" num="0129"><figref idref="DRAWINGS">FIG. <b>43</b></figref> shows, on the right, a first decay map produced by the regression model. On the left, <figref idref="DRAWINGS">FIG. <b>43</b></figref> shows a second decay map produced by the regression model.</p><p id="p-0131" num="0130"><figref idref="DRAWINGS">FIG. <b>44</b></figref> compares performance of the regression model with the RTA base caller for 40 pM library concentration (highly dense run).</p><p id="p-0132" num="0131"><figref idref="DRAWINGS">FIG. <b>45</b></figref> shows, on the left, a first decay map produced by the regression model. On the right, <figref idref="DRAWINGS">FIG. <b>45</b></figref> shows the results of the thresholding, the peak locating, and the watershed segmentation technique applied to the first decay map.</p><p id="p-0133" num="0132"><figref idref="DRAWINGS">FIG. <b>46</b></figref> illustrates one implementation of the binary classification model.</p><p id="p-0134" num="0133"><figref idref="DRAWINGS">FIG. <b>47</b></figref> is one implementation of training the binary classification model using a backpropagation-based gradient update technique that involves softmax scores.</p><p id="p-0135" num="0134"><figref idref="DRAWINGS">FIG. <b>48</b></figref> is another implementation of training the binary classification model using a backpropagation-based gradient update technique that involves sigmoid scores.</p><p id="p-0136" num="0135"><figref idref="DRAWINGS">FIG. <b>49</b></figref> illustrates another implementation of the input image data fed to the binary classification model and the corresponding class labels used to train the binary classification model.</p><p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. <b>50</b></figref> is one implementation of template generation by the binary classification model during inference.</p><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIG. <b>51</b></figref> illustrates one implementation of subjecting the binary map to peak detection to identify cluster centers.</p><p id="p-0139" num="0138"><figref idref="DRAWINGS">FIG. <b>52</b><i>a </i></figref>shows, on the left, an example binary map produced by the binary classification model. On the right, <figref idref="DRAWINGS">FIG. <b>52</b><i>a </i></figref>also shows an example ground truth binary map that the binary classification model approximates during the training.</p><p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. <b>52</b><i>b </i></figref>illustrates performance of the binary classification model using a precision statistic.</p><p id="p-0141" num="0140"><figref idref="DRAWINGS">FIG. <b>53</b></figref> is a table that shows an example architecture of the binary classification model.</p><p id="p-0142" num="0141"><figref idref="DRAWINGS">FIG. <b>54</b></figref> illustrates one implementation of the ternary classification model.</p><p id="p-0143" num="0142"><figref idref="DRAWINGS">FIG. <b>55</b></figref> is one implementation of training the ternary classification model using a backpropagation-based gradient update technique.</p><p id="p-0144" num="0143"><figref idref="DRAWINGS">FIG. <b>56</b></figref> illustrates another implementation of the input image data fed to the ternary classification model and the corresponding class labels used to train the ternary classification model.</p><p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. <b>57</b></figref> is a table that shows an example architecture of the ternary classification model.</p><p id="p-0146" num="0145"><figref idref="DRAWINGS">FIG. <b>58</b></figref> is one implementation of template generation by the ternary classification model during inference.</p><p id="p-0147" num="0146"><figref idref="DRAWINGS">FIG. <b>59</b></figref> shows a ternary map produced by the ternary classification model.</p><p id="p-0148" num="0147"><figref idref="DRAWINGS">FIG. <b>60</b></figref> depicts an array of units produced by the ternary classification model <b>5400</b>, along with the unit-wise output values.</p><p id="p-0149" num="0148"><figref idref="DRAWINGS">FIG. <b>61</b></figref> shows one implementation of subjecting the ternary map to post-processing to identify cluster centers, cluster background, and cluster interior.</p><p id="p-0150" num="0149"><figref idref="DRAWINGS">FIG. <b>62</b><i>a </i></figref>shows example predictions of the ternary classification model.</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>62</b><i>b </i></figref>illustrates other example predictions of the ternary classification model.</p><p id="p-0152" num="0151"><figref idref="DRAWINGS">FIG. <b>62</b><i>c </i></figref>shows yet other example predictions of the ternary classification model.</p><p id="p-0153" num="0152"><figref idref="DRAWINGS">FIG. <b>63</b></figref> depicts one implementation of deriving the cluster centers and cluster shapes from the output of the ternary classification model in <figref idref="DRAWINGS">FIG. <b>62</b></figref><i>a. </i></p><p id="p-0154" num="0153"><figref idref="DRAWINGS">FIG. <b>64</b></figref> compares base calling performance of the binary classification model, the regression model, and the RTA base caller.</p><p id="p-0155" num="0154"><figref idref="DRAWINGS">FIG. <b>65</b></figref> compares the performance of the ternary classification model with that of the RTA base caller under three contexts, five sequencing metrics, and two run densities.</p><p id="p-0156" num="0155"><figref idref="DRAWINGS">FIG. <b>66</b></figref> compares the performance of the regression model with that of the RTA base caller under the three contexts, the five sequencing metrics, and the two run densities discussed in <figref idref="DRAWINGS">FIG. <b>65</b></figref>.</p><p id="p-0157" num="0156"><figref idref="DRAWINGS">FIG. <b>67</b></figref> focuses on the penultimate layer of the neural network-based template generator.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>68</b></figref> visualizes what the penultimate layer of the neural network-based template generator has learned as a result of the backpropagation-based gradient update training. The illustrated implementation visualizes twenty-four out of the thirty-two trained convolution filters of the penultimate layer depicted in <figref idref="DRAWINGS">FIG. <b>67</b></figref>.</p><p id="p-0159" num="0158"><figref idref="DRAWINGS">FIG. <b>69</b></figref> overlays cluster center predictions of the binary classification model (in blue) onto those of the RTA base caller (in pink).</p><p id="p-0160" num="0159"><figref idref="DRAWINGS">FIG. <b>70</b></figref> overlays cluster center predictions made by the RTA base caller (in pink) onto visualization of the trained convolution filters of the penultimate layer of the binary classification model.</p><p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. <b>71</b></figref> illustrates one implementation of training data used to train the neural network-based template generator.</p><p id="p-0162" num="0161"><figref idref="DRAWINGS">FIG. <b>72</b></figref> is one implementation of using beads for image registration based on cluster center predictions of the neural network-based template generator.</p><p id="p-0163" num="0162"><figref idref="DRAWINGS">FIG. <b>73</b></figref> illustrates one implementation of cluster statistics of clusters identified by the neural network-based template generator.</p><p id="p-0164" num="0163"><figref idref="DRAWINGS">FIG. <b>74</b></figref> shows how the neural network-based template generator's ability to distinguish between adjacent clusters improves when the number of initial sequencing cycles for which the input image data is used increases from five to seven.</p><p id="p-0165" num="0164"><figref idref="DRAWINGS">FIG. <b>75</b></figref> illustrates the difference in base calling performance when a RTA base caller uses ground truth center of mass (COM) location as the cluster center, as opposed to when a non-COM location is used as the cluster center.</p><p id="p-0166" num="0165"><figref idref="DRAWINGS">FIG. <b>76</b></figref> portrays the performance of the neural network-based template generator on extra detected clusters.</p><p id="p-0167" num="0166"><figref idref="DRAWINGS">FIG. <b>77</b></figref> shows different datasets used for training the neural network-based template generator.</p><p id="p-0168" num="0167"><figref idref="DRAWINGS">FIG. <b>78</b></figref> shows the processing stages used by the RTA base caller for base calling, according to one implementation.</p><p id="p-0169" num="0168"><figref idref="DRAWINGS">FIG. <b>79</b></figref> illustrates one implementation of base calling using the disclosed neural network-based base caller.</p><p id="p-0170" num="0169"><figref idref="DRAWINGS">FIG. <b>80</b></figref> is one implementation of transforming, from subpixel domain to pixel domain, location/position information of cluster centers identified from the output of the neural network-based template generator.</p><p id="p-0171" num="0170"><figref idref="DRAWINGS">FIG. <b>81</b></figref> is one implementation of using cycle-specific and image channel-specific transformations to derive the so-called &#x201c;transformed cluster centers&#x201d; from the reference cluster centers.</p><p id="p-0172" num="0171"><figref idref="DRAWINGS">FIG. <b>82</b></figref> illustrates an image patch that is part of the input data fed to the neural network-based base caller.</p><p id="p-0173" num="0172"><figref idref="DRAWINGS">FIG. <b>83</b></figref> depicts one implementation of determining distance values for a distance channel when a single target cluster is being base called by the neural network-based base caller.</p><p id="p-0174" num="0173"><figref idref="DRAWINGS">FIG. <b>84</b></figref> shows one implementation of pixel-wise encoding the distance values that are calculated between the pixels and the target cluster.</p><p id="p-0175" num="0174"><figref idref="DRAWINGS">FIG. <b>85</b><i>a </i></figref>depicts one implementation of determining distance values for a distance channel when multiple target clusters are being simultaneously base called by the neural network-based base caller.</p><p id="p-0176" num="0175"><figref idref="DRAWINGS">FIG. <b>85</b><i>b </i></figref>shows, for each of the target clusters, some nearest pixels determined based on the pixel center-to-nearest cluster center distances.</p><p id="p-0177" num="0176"><figref idref="DRAWINGS">FIG. <b>86</b></figref> shows one implementation of pixel-wise encoding the minimum distance values that are calculated between the pixels and the nearest one of the clusters.</p><p id="p-0178" num="0177"><figref idref="DRAWINGS">FIG. <b>87</b></figref> illustrates one implementation using pixel-to-cluster classification/attribution/categorization, referred to herein as &#x201c;cluster shape data&#x201d;.</p><p id="p-0179" num="0178"><figref idref="DRAWINGS">FIG. <b>88</b></figref> shows one implementation of calculating the distance values using the cluster shape data.</p><p id="p-0180" num="0179"><figref idref="DRAWINGS">FIG. <b>89</b></figref> shows one implementation of pixel-wise encoding the distance values that are calculated between the pixels and the assigned clusters.</p><p id="p-0181" num="0180"><figref idref="DRAWINGS">FIG. <b>90</b></figref> illustrates one implementation of the specialized architecture of the neural network-based base caller that is used to segregate processing of data for different sequencing cycles.</p><p id="p-0182" num="0181"><figref idref="DRAWINGS">FIG. <b>91</b></figref> depicts one implementation of segregated convolutions.</p><p id="p-0183" num="0182"><figref idref="DRAWINGS">FIG. <b>92</b><i>a </i></figref>depicts one implementation of combinatory convolutions.</p><p id="p-0184" num="0183"><figref idref="DRAWINGS">FIG. <b>92</b><i>b </i></figref>depicts another implementation of the combinatory convolutions.</p><p id="p-0185" num="0184"><figref idref="DRAWINGS">FIG. <b>93</b></figref> shows one implementation of convolution layers of the neural network-based base caller in which each convolution layer has a bank of convolution filters.</p><p id="p-0186" num="0185"><figref idref="DRAWINGS">FIG. <b>94</b></figref> depicts two configurations of the scaling channel that supplements the image channels.</p><p id="p-0187" num="0186"><figref idref="DRAWINGS">FIG. <b>95</b><i>a </i></figref>illustrates one implementation of input data for a single sequencing cycle that produces a red image and a green image.</p><p id="p-0188" num="0187"><figref idref="DRAWINGS">FIG. <b>95</b><i>b </i></figref>illustrates one implementation of the distance channels supplying additive bias that is incorporated in the feature maps generated from the image channels.</p><p id="p-0189" num="0188"><figref idref="DRAWINGS">FIGS. <b>96</b><i>a</i>, <b>96</b><i>b</i>, and <b>96</b><i>c </i></figref>depict one implementation of base calling a single target cluster.</p><p id="p-0190" num="0189"><figref idref="DRAWINGS">FIG. <b>97</b></figref> shows one implementation of simultaneously base calling multiple target clusters.</p><p id="p-0191" num="0190"><figref idref="DRAWINGS">FIG. <b>98</b></figref> shows one implementation of simultaneously base calling multiple target clusters at a plurality of successive sequencing cycles, thereby simultaneously producing a base call sequence for each of the multiple target clusters.</p><p id="p-0192" num="0191"><figref idref="DRAWINGS">FIG. <b>99</b></figref> illustrates the dimensionality diagram for the single cluster base calling implementation.</p><p id="p-0193" num="0192"><figref idref="DRAWINGS">FIG. <b>100</b></figref> illustrates the dimensionality diagram for the multiple clusters, single sequencing cycle base calling implementation.</p><p id="p-0194" num="0193"><figref idref="DRAWINGS">FIG. <b>101</b></figref> illustrates the dimensionality diagram for the multiple clusters, multiple sequencing cycles base calling implementation.</p><p id="p-0195" num="0194"><figref idref="DRAWINGS">FIG. <b>102</b><i>a </i></figref>depicts an example arrayed input configuration the multi-cycle input data.</p><p id="p-0196" num="0195"><figref idref="DRAWINGS">FIG. <b>102</b><i>b </i></figref>shows an example stacked input configuration the multi-cycle input data.</p><p id="p-0197" num="0196"><figref idref="DRAWINGS">FIG. <b>103</b><i>a </i></figref>depicts one implementation of reframing pixels of an image patch to center a center of a target cluster being base called in a center pixel.</p><p id="p-0198" num="0197"><figref idref="DRAWINGS">FIG. <b>103</b><i>b </i></figref>depicts another example reframed/shifted image patch in which (i) the center of the center pixel coincides with the center of the target cluster and (ii) the non-center pixels are equidistant from the center of the target cluster.</p><p id="p-0199" num="0198"><figref idref="DRAWINGS">FIG. <b>104</b></figref> shows one implementation of base calling a single target cluster at a current sequencing cycle using a standard convolution neural network and the reframed input.</p><p id="p-0200" num="0199"><figref idref="DRAWINGS">FIG. <b>105</b></figref> shows one implementation of base calling multiple target clusters at the current sequencing cycle using the standard convolution neural network and the aligned input.</p><p id="p-0201" num="0200"><figref idref="DRAWINGS">FIG. <b>106</b></figref> shows one implementation of base calling multiple target clusters at a plurality of sequencing cycles using the standard convolution neural network and the aligned input.</p><p id="p-0202" num="0201"><figref idref="DRAWINGS">FIG. <b>107</b></figref> shows one implementation of training the neural network-based base caller.</p><p id="p-0203" num="0202"><figref idref="DRAWINGS">FIG. <b>108</b><i>a </i></figref>depicts one implementation of a hybrid neural network that is used as the neural network-based base caller.</p><p id="p-0204" num="0203"><figref idref="DRAWINGS">FIG. <b>108</b><i>b </i></figref>shows one implementation of 3D convolutions used by the recurrent module of the hybrid neural network to produce the current hidden state representations.</p><p id="p-0205" num="0204"><figref idref="DRAWINGS">FIG. <b>109</b></figref> illustrates one implementation of processing, through a cascade of convolution layers of the convolution module, per-cycle input data for a single sequencing cycle among the series oft sequencing cycles to be base called.</p><p id="p-0206" num="0205"><figref idref="DRAWINGS">FIG. <b>110</b></figref> depicts one implementation of mixing the single sequencing cycle's per-cycle input data with its corresponding convolved representations produced by the cascade of convolution layers of the convolution module.</p><p id="p-0207" num="0206"><figref idref="DRAWINGS">FIG. <b>111</b></figref> shows one implementation of arranging flattened mixed representations of successive sequencing cycles as a stack.</p><p id="p-0208" num="0207"><figref idref="DRAWINGS">FIG. <b>112</b><i>a </i></figref>illustrates one implementation of subjecting the stack of <figref idref="DRAWINGS">FIG. <b>111</b></figref> to recurrent application of 3D convolutions in forward and backward directions and producing base calls for each of the clusters at each of the t sequencing cycles in the series.</p><p id="p-0209" num="0208"><figref idref="DRAWINGS">FIG. <b>112</b><i>b </i></figref>shows one implementation of processing a 3D input volume x(t), which comprises groups of flattened mixed representations, through an input gate, an activation gate, a forget gate, and an output gate of a long short-term memory (LSTM) network that applies the 3D convolutions. The LS&#x2122; network is part of the recurrent module of the hybrid neural network.</p><p id="p-0210" num="0209"><figref idref="DRAWINGS">FIG. <b>113</b></figref> shows one implementation of balancing trinucleotides (3-mers) in the training data used to train the neural network-based base caller.</p><p id="p-0211" num="0210"><figref idref="DRAWINGS">FIG. <b>114</b></figref> compares base calling accuracy of the RTA base caller against the neural network-based base caller.</p><p id="p-0212" num="0211"><figref idref="DRAWINGS">FIG. <b>115</b></figref> compares tile-to-tile generalization of the RTA base caller with that of the neural network-based base caller on a same tile.</p><p id="p-0213" num="0212"><figref idref="DRAWINGS">FIG. <b>116</b></figref> compares tile-to-tile generalization of the RTA base caller with that of the neural network-based base caller on a same tile and on different tiles.</p><p id="p-0214" num="0213"><figref idref="DRAWINGS">FIG. <b>117</b></figref> also compares tile-to-tile generalization of the RTA base caller with that of the neural network-based base caller on different tiles.</p><p id="p-0215" num="0214"><figref idref="DRAWINGS">FIG. <b>118</b></figref> shows how different sizes of the image patches fed as input to the neural network-based base caller effect the base calling accuracy.</p><p id="p-0216" num="0215"><figref idref="DRAWINGS">FIGS. <b>119</b>, <b>120</b>, <b>121</b>, and <b>122</b></figref> show lane-to-lane generalization of the neural network-based base caller on training data from <i>A. baumanni </i>and <i>E. coli. </i></p><p id="p-0217" num="0216"><figref idref="DRAWINGS">FIG. <b>123</b></figref> depicts an error profile for the lane-to-lane generalization discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>119</b>, <b>120</b>, <b>121</b>, and <b>122</b></figref>.</p><p id="p-0218" num="0217"><figref idref="DRAWINGS">FIG. <b>124</b></figref> attributes the source of the error detected by the error profile of <figref idref="DRAWINGS">FIG. <b>123</b></figref> to low cluster intensity in the green channel.</p><p id="p-0219" num="0218"><figref idref="DRAWINGS">FIG. <b>125</b></figref> compares error profiles of the RTA base caller and the neural network-based base caller for two sequencing runs (Read <b>1</b> and Read <b>2</b>).</p><p id="p-0220" num="0219"><figref idref="DRAWINGS">FIG. <b>126</b><i>a </i></figref>shows run-to-run generalization of the neural network-based base caller on four different instruments.</p><p id="p-0221" num="0220"><figref idref="DRAWINGS">FIG. <b>126</b><i>b </i></figref>shows run-to-run generalization of the neural network-based base caller on four different runs executed on a same instrument.</p><p id="p-0222" num="0221"><figref idref="DRAWINGS">FIG. <b>127</b></figref> shows the genome statistics of the training data used to train the neural network-based base caller.</p><p id="p-0223" num="0222"><figref idref="DRAWINGS">FIG. <b>128</b></figref> shows the genome context of the training data used to train the neural network-based base caller.</p><p id="p-0224" num="0223"><figref idref="DRAWINGS">FIG. <b>129</b></figref> shows the base calling accuracy of the neural network-based base caller in base calling long reads (e.g., 2&#xd7;250).</p><p id="p-0225" num="0224"><figref idref="DRAWINGS">FIG. <b>130</b></figref> illustrates one implementation of how the neural network-based base caller attends to the central cluster pixel(s) and its neighboring pixels across image patches.</p><p id="p-0226" num="0225"><figref idref="DRAWINGS">FIG. <b>131</b></figref> shows various hardware components and configurations used to train and run the neural network-based base caller, according to one implementation. In other implementations, different hardware components and configurations are used.</p><p id="p-0227" num="0226"><figref idref="DRAWINGS">FIG. <b>132</b></figref> shows various sequencing tasks that can be performed using the neural network-based base caller.</p><p id="p-0228" num="0227"><figref idref="DRAWINGS">FIG. <b>133</b></figref> is a scatter plot visualized by t-Distributed Stochastic Neighbor Embedding (t-SNE) and portrays base calling results of the neural network-based base caller.</p><p id="p-0229" num="0228"><figref idref="DRAWINGS">FIG. <b>134</b></figref> illustrates one implementation of selecting the base call confidence probabilities made by the neural network-based base caller for quality scoring.</p><p id="p-0230" num="0229"><figref idref="DRAWINGS">FIG. <b>135</b></figref> shows one implementation of the neural network-based quality scoring.</p><p id="p-0231" num="0230"><figref idref="DRAWINGS">FIGS. <b>136</b><i>a</i>-<b>136</b><i>b </i></figref>depict one implementation of correspondence between the quality scores and the base call confidence predictions made by the neural network-based base caller.</p><p id="p-0232" num="0231"><figref idref="DRAWINGS">FIG. <b>137</b></figref> shows one implementation of inferring quality scores from base call confidence predictions made by the neural network-based base caller during inference.</p><p id="p-0233" num="0232"><figref idref="DRAWINGS">FIG. <b>138</b></figref> shows one implementation of training the neural network-based quality scorer to process input data derived from the sequencing images and directly produce quality indications.</p><p id="p-0234" num="0233"><figref idref="DRAWINGS">FIG. <b>139</b></figref> shows one implementation of directly producing quality indications as outputs of the neural network-based quality scorer during inference.</p><p id="p-0235" num="0234"><figref idref="DRAWINGS">FIG. <b>140</b></figref> depicts one implementation of using lossless transformation to generate transformed data that can be fed as input to the neural network-based template generator, the neural network-based base caller, and the neural network-based quality scorer.</p><p id="p-0236" num="0235"><figref idref="DRAWINGS">FIG. <b>141</b></figref> illustrates one implementation of integrating the neural network-based template generator with the neural network-based base caller using area weighting factoring.</p><p id="p-0237" num="0236"><figref idref="DRAWINGS">FIG. <b>142</b></figref> illustrates another implementation of integrating the neural network-based template generator with the neural network-based base caller using upsampling and background masking.</p><p id="p-0238" num="0237"><figref idref="DRAWINGS">FIG. <b>143</b></figref> depicts one example of area weighting factoring <b>14300</b> for contribution from only a single cluster per pixel.</p><p id="p-0239" num="0238"><figref idref="DRAWINGS">FIG. <b>144</b></figref> depicts one example of area weighting factoring for contributions from multiple clusters per pixel.</p><p id="p-0240" num="0239"><figref idref="DRAWINGS">FIG. <b>145</b></figref> depicts one example of using interpolation for upsampling and background masking.</p><p id="p-0241" num="0240"><figref idref="DRAWINGS">FIG. <b>146</b></figref> depicts one example of using subpixel count weighting for upsampling and background masking.</p><p id="p-0242" num="0241"><figref idref="DRAWINGS">FIGS. <b>147</b>A and <b>147</b>B</figref> depict one implementation of a sequencing system. The sequencing system comprises a configurable processor.</p><p id="p-0243" num="0242"><figref idref="DRAWINGS">FIG. <b>147</b>C</figref> is a simplified block diagram of a system for analysis of sensor data from the sequencing system, such as base call sensor outputs.</p><p id="p-0244" num="0243"><figref idref="DRAWINGS">FIG. <b>148</b>A</figref> is a simplified diagram showing aspects of the base calling operation, including functions of a runtime program executed by a host processor.</p><p id="p-0245" num="0244"><figref idref="DRAWINGS">FIG. <b>148</b>B</figref> is a simplified diagram of a configuration of a configurable processor such as the one depicted in <figref idref="DRAWINGS">FIG. <b>147</b>C</figref>.</p><p id="p-0246" num="0245"><figref idref="DRAWINGS">FIG. <b>149</b></figref> is a computer system that can be used by the sequencing system of <figref idref="DRAWINGS">FIG. <b>147</b>A</figref> to implement the technology disclosed herein.</p><p id="p-0247" num="0246"><figref idref="DRAWINGS">FIG. <b>150</b></figref> shows different implementations of data pre-processing, which can include data normalization and data augmentation.</p><p id="p-0248" num="0247"><figref idref="DRAWINGS">FIG. <b>151</b></figref> shows that the data normalization technique (DeepRTA (norm)) and the data augmentation technique (DeepRTA (augment)) of <figref idref="DRAWINGS">FIG. <b>150</b></figref> reduce the base calling error percentage when the neural network-based base caller is trained on bacterial data and tested on human data, where the bacterial data and the human data share the same assay (e.g., both contain intronic data).</p><p id="p-0249" num="0248"><figref idref="DRAWINGS">FIG. <b>152</b></figref> shows that the data normalization technique (DeepRTA (norm)) and the data augmentation technique (DeepRTA (augment)) of <figref idref="DRAWINGS">FIG. <b>151</b></figref> reduce the base calling error percentage when the neural network-based base caller is trained on non-exonic data (e.g., intronic data) and tested on exonic data.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0008" level="1">DETAILED DESCRIPTION</heading><p id="p-0250" num="0249">The following discussion is presented to enable any person skilled in the art to make and use the technology disclosed, and is provided in the context of a particular application and its requirements. Various modifications to the disclosed implementations will be readily apparent to those skilled in the art, and the general principles defined herein may be applied to other implementations and applications without departing from the spirit and scope of the technology disclosed. Thus, the technology disclosed is not intended to be limited to the implementations shown, but is to be accorded the widest scope consistent with the principles and features disclosed herein.</p><heading id="h-0009" level="1">INTRODUCTION</heading><p id="p-0251" num="0250">Base calling from digital images is massively parallel and computationally intensive. This presents numerous technical challenges that we identify before introducing our new technology.</p><p id="p-0252" num="0251">The signal from an image set being evaluated is increasingly faint as classification of bases proceeds in cycles, especially over increasingly long strands of bases. The signal-to-noise ratio decreases as base classification extends over the length of a strand, so reliability decreases. Updated estimates of reliability are expected as the estimated reliability of base classification changes.</p><p id="p-0253" num="0252">Digital images are captured from amplified clusters of sample strands. Samples are amplified by duplicating strands using a variety of physical structures and chemistries. During sequencing by synthesis, tags are chemically attached in cycles and stimulated to glow. Digital sensors collect photons from the tags that are read out of pixels to produce images.</p><p id="p-0254" num="0253">Interpreting digital images to classify bases requires resolving positional uncertainty, handicapped by limited image resolution. At a greater resolution than collected during base calling, it is apparent imaged clusters have irregular shapes and indeterminate center positions. Cluster positions are not mechanically regulated, so cluster centers are not aligned with pixel centers. A pixel center can be the integer coordinate assigned to a pixel. In other implementations, it can be the top-left corner of the pixel. In yet other implementations, it can be the centroid or center-of-mass of the pixel. Amplification does not produce uniform cluster shapes. Distribution of cluster signals in the digital image is, therefore, a statistical distribution rather than a regular pattern. We call this positional uncertainty.</p><p id="p-0255" num="0254">One of the signal classes may produce no detectable signal and be classified at a particular position based on a &#x201c;dark&#x201d; signal. Thus, templates are necessary for classification during dark cycles. Production of templates resolves initial positional uncertainty using multiple imaging cycles to avoid missing dark signals.</p><p id="p-0256" num="0255">Trade-offs in image sensor size, magnification, and stepper design lead to pixel sizes that are relatively large, that are too large to treat cluster centers as coincident with sensor pixel centers. This disclosure uses pixel in two senses. The physical, sensor pixel is a region of an optical sensor that reports detected photons. A logical pixel, simply referred to as a pixel, is data corresponding to at least one physical pixel, data read from the sensor pixel. The pixel can be subdivided or &#x201c;up sampled&#x201d; into sub pixels, such as 4&#xd7;4 sub pixels. To take into account the possibility that all the photons are hitting one side of the physical pixel and not the opposite side, values can be assigned to sub pixels by interpolation, such as bilinear interpolation or area weighting. Interpolation or bilinear interpolation also is applied when pixels are re-framed by applying an affine transformation to data from physical pixels.</p><p id="p-0257" num="0256">Larger physical pixels are more sensitive to faint signals than smaller pixels. While digital sensors improve with time, the physical limitation of collector surface area is unavoidable. Taking design trade-offs into consideration, legacy systems have been designed to collect and analyze image data from a three-by-three patch of sensor pixels, with the center of the cluster somewhere in the center pixel of the patch.</p><p id="p-0258" num="0257">High resolution sensors capture only part of an imaged media at a time. The sensor is stepped over the imaged media to cover the whole field. Thousands of digital images can be collected during one processing cycle.</p><p id="p-0259" num="0258">Sensor and illumination design are combined to distinguish among at least four illumination response values that are used to classify bases. If a traditional RGB camera with a Bayer color filter array were used, four sensor pixels would be combined into a single RGB value. This would reduce the effective sensor resolution by four-fold. Alternatively, multiple images can be collected at a single position using different illumination wavelengths and/or different filters rotated into position between the imaged media and the sensor. The number of images required to distinguish among four base classifications varies between systems. Some systems use one image with four intensity levels for different classes of bases. Other systems use two images with different illumination wavelengths (red and green, for instance) and/or filters with a sort of truth table to classify bases. Systems also can use four images with different illumination wavelengths and/or filters tuned to specific base classes.</p><p id="p-0260" num="0259">Massively parallel processing of digital images is practically necessary to align and combine relatively short strands, on the order of 30 to 2000 base pairs, into longer sequences, potentially millions or even billions of bases in length. Redundant samples are desirable over an imaged media, so a part of a sequence may be covered by dozens of sample reads. Millions or at least hundreds of thousands of sample clusters are imaged from a single imaged media. Massively parallel processing of so many clusters has increased in sequencing capacity while decreasing cost.</p><p id="p-0261" num="0260">The capacity for sequencing has increased at a pace that rivals Moore's law. While the first sequencing cost billions of dollars, in 2018 services such as Illumina&#x2122; are delivering results for hundred(s) of dollars. As sequencing goes mainstream and unit prices drop, less computing power is available for classification, which increases the challenge of near real time classification. With these technical challenges in mind, we turn to the technology disclosed.</p><p id="p-0262" num="0261">The technology disclosed improves processing during both template generation to resolve positional uncertainty and during base classification of clusters at resolved positions. Applying the technology disclosed, less expensive hardware can be used to reduce the cost of machines. Near real time analysis can become cost effective, reducing the lag between image collection and base classification.</p><p id="p-0263" num="0262">The technology disclosed can use upsampled images produced by interpolating sensor pixels into subpixels and then producing templates that resolve positional uncertainty. A resulting subpixel is submitted to a base caller for classification that treats the subpixel as if it were at the center of a cluster. Clusters are determined from groups of adjoining subpixels that repeatedly receive the same base classification. This aspect of the technology leverages existing base calling technology to determine shapes of clusters and to hyper-locate cluster centers with a subpixel resolution.</p><p id="p-0264" num="0263">Another aspect of the technology disclosed is to create ground truth, training data sets that pair images with confidently determined cluster centers and/or cluster shapes. Deep learning systems and other machine learning approaches require substantial training sets. Human curated data is expensive to compile. The technology disclosed can be used to leverage existing classifiers, in a non-standard mode of operation, to generate large sets of confidently classified training data without intervention or the expense of a human curator. The training data correlates raw images with cluster centers and/or cluster shapes available from existing classifiers, in a non-standard mode of operation, such as CNN-based deep learning systems, which can then directly process image sequences. One training image can be rotated and reflected to produce additional, equally valid examples. Training examples can focus on regions of a predetermined size within an overall image. The context evaluated during base calling determines the size of example training regions, rather than the size of an image from or overall imaged media.</p><p id="p-0265" num="0264">The technology disclosed can produce different types of maps, usable as training data or as templates for base classification, which correlate cluster centers and/or cluster shapes with digital images. First, a subpixel can be classified as a cluster center, thereby localizing a cluster center within a physical sensor pixel. Second, a cluster center can be calculated as the centroid of a cluster shape. This location can be reported with a selected numeric precision. Third, a cluster center can be reported with surrounding subpixels in a decay map, either at subpixel or pixel resolution. A decay map reduces weight given to photons detected in regions as separation of the regions from the cluster center increase, attenuating signals from more distant positions. Fourth, binary or ternary classifications can be applied to subpixels or pixels in clusters of adjoining regions. In binary classification, a region is classified as belonging to a cluster center or as background. In ternary classification, the third class type is assigned to the region that contains the cluster interior, but not the cluster center. Subpixel classification of cluster center locations could be substituted for real valued cluster center coordinates within a larger optical pixel.</p><p id="p-0266" num="0265">The alternative styles of maps can initially be produced as ground truth data sets, or, with training, they can be produced using a neural network. For instance, clusters can be depicted as disjoint regions of adjoining subpixels with appropriate classifications. Intensity mapped clusters from a neural network can be post-processed by a peak detector filter, to calculate cluster centers, if the centers have not already been determined. Applying a so-called watershed analysis, abutting regions can be assigned to separate clusters. When produced by a neural network inference engine, the maps can be used as templates for evaluating a sequence of digital images and classifying bases over cycles of base calling.</p><p id="p-0267" num="0266">When bases are classified in sequences of digital images, the neural network processes multiple image channels in a current cycle together with image channels of past and future cycles. In a cluster, some of the strands may run ahead or behind the main course of synthesis, which out-of-phase tagging is known as pre-phasing or phasing. Given the low rates of pre-phasing and post-phasing observed empirically, nearly all of the noise in the signal resulting from pre-phasing and post-phasing can be handled by a neural network that processes digital images in current, past and future cycles, in just three cycles.</p><p id="p-0268" num="0267">Among digital image channels in the current cycle, careful registration to align images within a cycle contributes strongly to accurate base classification. A combination of wavelengths and non-coincident illumination sources, among other sources of error, produces a small, correctable difference in measured cluster center locations. A general affine transformation, with translation, rotation and scaling, can be used to bring the cluster centers across an image tile into precise alignment. An affine transformation can be used to reframe image data and to resolve offsets for cluster centers.</p><p id="p-0269" num="0268">Refraining image data means interpolating image data, typically by applying an affine transformation. Reframing can put a cluster center of interest in the middle of the center pixel of a pixel patch. Or, it can align an image with a template, to overcome jitter and other discrepancies during image collection. Reframing involves adjusting intensity values of all pixels in the pixel patch. Bi-linear and bi-cubic interpolation and weighted area adjustments are alternative strategies.</p><p id="p-0270" num="0269">In some implementations, cluster center coordinates can be fed to a neural network as an additional image channel.</p><p id="p-0271" num="0270">Distance signals also can contribute to base classification. Several types of distance signals reflect separation of regions from cluster centers. The strongest optical signal is deemed to coincide with the cluster center. The optical signal along the cluster perimeter sometimes includes a stray signal from a nearby cluster. Classification has been observed to be more accurate when contribution of signal component is attenuated according to its separation from the cluster center. Distance signals that work include a single cluster distance channel, a multi-cluster distance channel, and a multi-cluster shape-based distance channel. A single cluster distance channel applies to a patch with a cluster center in the center pixel. Then, distance of all regions in the patch is a distance from the cluster center in the center pixel. Pixels that do not belong to same cluster as the center pixel can be flagged as background, instead of given a calculated distance. A multi-cluster distance channel pre-calculates distance of each region to the closest cluster center. This has the potential of connecting a region to the wrong cluster center, but that potential is low. A multi-cluster shape-based distance channel associates regions (sub-pixels or pixels) through adjoining regions to a pixel center that produces a same base classification. At some computational expense, this avoids the possibility of measuring a distance to the wrong pixel. The multi-cluster and multi-cluster shape-based approaches to distance signals have the advantage of being subject to pre-calculation and use with multiple clusters in an image.</p><p id="p-0272" num="0271">Shape information can be used by a neural network to separate signal from noise, to improve the signal-to-noise ratio. In the discussion above, several approaches to region classification and to supplying distance channel information were identified. In any of the approaches, regions can be marked as background, as not being part of a cluster, to define cluster edges. A neural network can be trained to take advantage of the resulting information about irregular cluster shapes. Distance information and background classification can be combined or used separately. Separating signals from abutting clusters will be increasingly important as cluster density increases.</p><p id="p-0273" num="0272">One direction for increasing the scale of parallel processing is to increase cluster density on the imaged media. Increasing density has the downside of increasing background noise when reading a cluster that has an adjacent neighbor. Using shape data, instead of an arbitrary patch (e.g., of 3&#xd7;3 pixels), for instance, helps maintain signal separation as cluster density increases.</p><p id="p-0274" num="0273">Applying one aspect of the technology disclosed, base classification scores also can be leveraged to predict quality. The technology disclosed includes correlating classification scores, directly or through a prediction model, with traditional Sanger or Phred quality Q-scores. Scores such as Q20, Q30 or Q40 are logarithmically related to base classification error probabilities, by Q=&#x2212;10 log<sub>10</sub>P. Correlation of class scores with Q scores can be performed using a multi-output neural network or multi-variate regression analysis. An advantage of real time calculation of quality scores, during base classification, is that a flawed sequencing run can be terminated early. Applicant has found that occasional (rare) decisions to terminate runs can be made one-eighth to one-quarter of the way through the analysis sequence. A decision to terminate can be made after 50 cycles or after 25 to 75 cycles. In a sequential process that would otherwise run 300 to 1000 cycles, early termination results in substantial resource savings.</p><p id="p-0275" num="0274">Specialized convolutional neural network (CNN) architectures can be used to classify bases over multiple cycles. One specialization involves segregation among digital image channels during initial layers of processing. Convolution filters stacks can be structured to segregate processing among cycles, preventing cross-talk between digital image sets from different cycles. The motivation for segregating processing among cycles is that images taken at different cycles have residual registration error and are thus misaligned and have random translational offsets with respect to each other. This occurs due to the finite accuracy of the movements of the sensor's motion stage and also because images taken in different frequency channels have different optical paths and wavelengths.</p><p id="p-0276" num="0275">The motivation for using image sets from successive cycles is that the contribution of pre-phasing and post-phasing to signals in a particular cycle is a second order contribution. It follows that it can be helpful for the convolutional neural network to structurally segregate lower layer convolution of digital image sets among image collection cycles.</p><p id="p-0277" num="0276">The convolutional neural network structure also can be specialized in handling information about clustering. Templates for cluster centers and/or shapes provide additional information, which the convolutional neural network combines with the digital image data. The cluster center classification and distance data can be applied repeatedly across cycles.</p><p id="p-0278" num="0277">The convolutional neural network can be structured to classify multiple clusters in an image field. When multiple clusters are classified, the distance channel for a pixel or subpixel can more compactly contain distance information relative to either the closest cluster center or to the adjoining cluster center, to which a pixel or subpixel belongs. Alternatively, a large distance vector could be supplied for each pixel or subpixel, or at least for each one that contains a cluster center, which gives complete distance information from a cluster center to all other pixels that are context for the given pixel.</p><p id="p-0279" num="0278">Some combinations of template generation with base calling can use variations on area weighting to supplant a distance channel. The discussion now turns to how output of the template generator can be used directly, in lieu of a distance channel.</p><p id="p-0280" num="0279">We discuss three considerations that impact direct application of template images to pixel value modification: whether image sets are processed in the pixel or subpixel domain; in either domain, how area weights are calculated; and in the subpixel domain, applying a template image as mask to modify interpolated intensity values.</p><p id="p-0281" num="0280">Performing base classification in the pixel domain has the advantage of not calling for an increase in calculations, such as 16 fold, which results from upsampling. In the pixel domain, even the top layer of convolutions may have sufficient cluster density to justify performing calculations that would not be harvested, instead of adding logic to cancel unneeded calculations. We begin with examples in the pixel domain of directly using template image data without a distance channel.</p><p id="p-0282" num="0281">In some implementations, classification focuses on a particular cluster. In these instances, pixels on the perimeter of a cluster may have different modified intensity values, depending on which adjoining cluster is the focus of classification. The template image in the subpixel domain can indicate that an overlap pixel contributes intensity value to two different clusters. We refer to optical pixel as an &#x201c;overlap pixel&#x201d; when two or more adjacent or abutting clusters both overlap the pixel; both contribute to the intensity reading from the optical pixel. Watershed analysis, named after separating rain flows into different watersheds at a ridge line, can be applied to separate even abutting clusters. When data is received for classification on a cluster-by-cluster basis, the template image can be used to modify intensity data for overlap pixels along the perimeter of clusters. The overlap pixels can have different modified intensities, depending on which cluster is the focus of classification.</p><p id="p-0283" num="0282">The modified intensity of a pixel can be reduced based on subpixel contribution in the overlap pixel to a home cluster (i.e., the cluster to which the pixel belongs or the cluster whose intensity emissions the pixel primarily depicts), as opposed to an away cluster (i.e., the non-home cluster whose intensity emissions the pixel depicts). Suppose that 5 subpixels are part of the home cluster and 2 subpixels are part of the away cluster. Then, 7 subpixels contribute intensity to the home or away cluster. During focus on the home cluster, in one implementation the overlap pixel is reduced in intensity by 7/16, because 7 of the 16 subpixels contribute intensity to the home or away cluster. In another implementation, intensity is reduced by 5/16, based on the area of subpixels contributing to the home cluster divided by the total number of subpixels. In a third implementation, intensity is reduced by 5/7, based on the area of subpixels contributing to the home cluster divided by the total area of contributing subpixels. The latter two calculations change when the focus turns to the away cluster, producing fractions with &#x201c;2&#x201d; in the numerator.</p><p id="p-0284" num="0283">Of course, further reduction in intensity can be applied if a distance channel is being considered along with a subpixel map of cluster shapes.</p><p id="p-0285" num="0284">Once the pixel intensities for a cluster that is the focus of classification have been modified using the template image, the modified pixel values are convolved through layers of a neural network-based classifier to produce modified images. The modified images are used to classify bases in successive sequencing cycles.</p><p id="p-0286" num="0285">Alternatively, classification in the pixel domain can proceed in parallel for all pixels or all clusters in a chunk of an image. Only one modification of a pixel value can be applied in this scenario to assure reusability of intermediate calculations. Any of the fractions given above can be used to modify pixel intensity, depending on whether a smaller or larger attenuation of intensity is desired.</p><p id="p-0287" num="0286">Once the pixel intensities for the image chunk have been modified using the template image, pixels and surrounding context can be convolved through layers of a neural network-based classifier to produce modified images. Performing convolutions on an image chunk allows reuse of intermediate calculations among pixels that have shared context. The modified images are used to classify bases in successive sequencing cycles.</p><p id="p-0288" num="0287">This description can be paralleled for application of area weights in the subpixel domain. The parallel is that weights can be calculated for individual subpixels. The weights can, but do not need to, be the same for different subpixel parts of an optical pixel. Repeating the scenario above of home and away clusters, with 5 and 2 subpixels of the overlap pixel, respectively, the assignment of intensity to a subpixel belonging to the home cluster can be 7/16, 5/16 or 5/7 of the pixel intensity. Again, further reduction in intensity can be applied if a distance channel is being considered along with a subpixel map of cluster shapes.</p><p id="p-0289" num="0288">Once the pixel intensities for the image chunk have been modified using the template image, subpixels and surrounding context can be convolved through layers of a neural network-based classifier to produce modified images. Performing convolutions on an image chunk allows reuse of intermediate calculations among subpixels that have shared context. The modified images are used to classify bases in successive sequencing cycles.</p><p id="p-0290" num="0289">Another alternative is to apply the template image as a binary mask, in the subpixel domain, to image data interpolated into the subpixel domain. The template image can either be arranged to require a background pixel between clusters or to allow subpixels from different clusters to abut. The template image can be applied as a mask. The mask determines whether an interpolated pixel keeps the value assigned by interpolation or receives a background value (e.g., zero), if it is classified in the template image as background.</p><p id="p-0291" num="0290">Again, once the pixel intensities for the image chunk have been masked using the template image, subpixels and surrounding context can be convolved through layers of a neural network-based classifier to produce modified images. Performing convolutions on an image chunk allows reuse of intermediate calculations among subpixels that have shared context. The modified images are used to classify bases in successive sequencing cycles.</p><p id="p-0292" num="0291">Features of the technology disclosed are combinable to classify an arbitrary number of clusters within a shared context, reusing intermediate calculations. At optical pixel resolution, in one implementation, about ten percent of pixels hold cluster centers to be classified. In legacy systems, three by three optical pixels were grouped for analysis as potential signal contributors for a cluster center, given observation of irregularly shaped clusters. Even one 3-by-3 filter away from the top convolution layer, cluster densities are likely to roll up into pixels at cluster centers optical signals from substantially more than half of the optical pixels. Only at super sampled resolution does cluster center density for the top convolution layer drop below one percent.</p><p id="p-0293" num="0292">Shared context is substantial in some implementations. For instance, 15-by-15 optical pixel context may contribute to accurate base classification. An equivalent 4&#xd7; up sampled context would be 60-by-60 sub pixels. This extent of context helps the neural network recognize impacts of non-uniform illumination and background during imaging.</p><p id="p-0294" num="0293">The technology disclosed uses small filters at a lower convolution layer to combine cluster boundaries in template input with boundaries detected in digital image input. Cluster boundaries help the neural network separate signal from background conditions and normalize image processing against the background.</p><p id="p-0295" num="0294">The technology disclosed substantially reuses intermediate calculations. Suppose that 20 to 25 cluster centers appear within a context area of 15-by-15 optical pixels. Then, first layer convolutions stand to be reused 20 to 25 times in blockwise convolution roll-ups. The reuse factor is reduced layer-by-layer until the penultimate layer, which is the first time that the reuse factor at optical resolution drops below 1&#xd7;.</p><p id="p-0296" num="0295">Blockwise roll-up training and inference from multiple convolution layers applies successive roll-ups to a block of pixels or sub pixels. Around a block perimeter, there is an overlap zone in which data used during roll-up of a first data block overlaps with and can be reused for a second block of roll-ups. Within the block, in a center area surrounded by the overlap zone, are pixel values and intermediate calculations that can be rolled up and that can be reused. With an overlap zone, convolution results that progressively reduce the size of a context field, for instance from 15-by-15 to 13-by-13 by application of a 3-by-3 filter, can be written into the same memory block that holds the values convolved, conserving memory without impairing reuse of underlying calculations within the block. With larger blocks, sharing intermediate calculations in the overlap zone, requires less resources. With smaller blocks, it can be possible to calculate multiple blocks in parallel, to share the intermediate calculations in the overlap zones.</p><p id="p-0297" num="0296">Larger filters and dilations would reduce the number of convolution layers, which may be speed calculation without impairing classification, after lower convolution layers have reacted to cluster boundaries in the template and/or digital image data.</p><p id="p-0298" num="0297">The input channels for template data can be chosen to make the template structure consistent with classifying multiple cluster centers in a digital image field. Two alternatives described above do not satisfy this consistency criteria: reframing and distance mapping over an entire context. Refraining places the center of just one cluster in the center of an optical pixel. Better for classifying multiple clusters is supplying center offsets for pixels classified as holding cluster centers.</p><p id="p-0299" num="0298">Distance mapping, if provided, is difficult to perform across a whole context area unless every pixel has its own distance map over a whole context Simpler distance maps provide the useful consistency for classifying multiple clusters from a digital image input block.</p><p id="p-0300" num="0299">A neural network can learn from classification in a template of pixels or sub pixels at the boundary of a cluster, so a distance channel can be supplanted by a template that supplies binary or ternary classification, accompanied by a cluster center offset channel. When used, a distance map can give a distance of a pixel from a cluster center to which the pixel (or subpixel) belongs. Or the distance map can give a distance to the closest cluster center. The distance map can encode binary classification with a flag value assigned to background pixels or it can be a separate channel from pixel classification. Combined with cluster center offsets, the distance map can encode ternary classification. In some implementations, particularly ones that encode pixel classifications with one or two bits, it may be desirable, at least during development, to use separate channels for pixel classification and for distance.</p><p id="p-0301" num="0300">The technology disclosed can include reduction of calculations to save some calculation resources in upper layers. The cluster center offset channel or a ternary classification map can be used to identify centers of pixel convolutions that do not contribute to an ultimate classification of a pixel center. In many hardware/software implementations, performing a lookup during inference and skipping a convolution roll up can be more efficient in upper layer(s) than performing even nine multiplies and eight adds to apply a 3-by-3 filter. In custom hardware that pipelines calculations for parallel execution, every pixel can be classified within the pipeline. Then, the cluster center map can be used after the final convolution to harvest results for only pixels that coincide with cluster centers, because an ultimate classification is only desired for those pixels. Again, in the optical pixel domain, at currently observed cluster densities, rolled up calculations for about ten percent of the pixels would be harvested. In a 4&#xd7; up sampled domain, more layers could benefit from skipped convolutions, on some hardware, because less than one percent of the sub pixel classifications in the top layer would be harvested.</p><heading id="h-0010" level="2">Neural Network-Based Template Generation</heading><p id="p-0302" num="0301">The first step of template generation is determining cluster metadata. Cluster metadata identifies spatial distribution of clusters, including their centers, shapes, sizes, background, and/or boundaries.</p><p id="p-0303" num="0302">Determining Cluster Metadata</p><p id="p-0304" num="0303"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows one implementation of a processing pipeline that determines cluster metadata using subpixel base calling.</p><p id="p-0305" num="0304"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts one implementation of a flow cell that contains clusters in its tiles. The flow cell is partitioned into lanes. The lanes are further partitioned into non-overlapping regions called &#x201c;tiles&#x201d;. During the sequencing procedure, the clusters and their surrounding background on the tiles are imaged.</p><p id="p-0306" num="0305"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example Illumina GA-IIx&#x2122; flow cell with eight lanes. <figref idref="DRAWINGS">FIG. <b>3</b></figref> also shows a zoom-in on one tile and its clusters and their surrounding background.</p><p id="p-0307" num="0306"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts an image set of sequencing images for four-channel chemistry, i.e., the image set has four sequencing images, captured using four different wavelength bands (image/imaging channel) in the pixel domain. Each image in the image set covers a tile of a flow cell and depicts intensity emissions of clusters on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. In one implementation, each imaged channel corresponds to one of a plurality of filter wavelength bands. In another implementation, each imaged channel corresponds to one of a plurality of imaging events at a sequencing cycle. In yet another implementation, each imaged channel corresponds to a combination of illumination with a specific laser and imaging through a specific optical filter. The intensity emissions of a cluster comprise signals detected from an analyte that can be used to classify a base associated with the analyte. For example, the intensity emissions may be signals indicative of photons emitted by tags that are chemically attached to an analyte during a cycle when the tags are stimulated and that may be detected by one or more digital sensors, as described above.</p><p id="p-0308" num="0307"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is one implementation of dividing a sequencing image into subpixels (or subpixel regions). In the illustrated implementation, quarter (0.25) subpixels are used, which results in each pixel in the sequencing image being divided into sixteen subpixels. Given that the illustrated sequencing image has a resolution of 20&#xd7;20 pixels, i.e., 400 pixels, the division produces 6400 subpixels. Each of the subpixels is treated by a base caller as a region center for subpixel base calling. In some implementations, this base caller does not use neural network-based processing. In other implementations, this base caller is a neural network-based base caller.</p><p id="p-0309" num="0308">For a given sequencing cycle and a particular subpixel, the base caller is configured with logic to produce a base call for the given sequencing cycle particular subpixel by performing image processing steps and extracting intensity data for the subpixel from the corresponding image set of the sequencing cycle. This is done for each of the subpixels and for each of a plurality of sequencing cycles. Experiments have also been carried out with quarter subpixel division of 1800&#xd7;1800 pixel resolution tile images of the Illumina MiSeq sequencer. Subpixel base calling was performed for fifty sequencing cycles and for ten tiles of a lane.</p><p id="p-0310" num="0309"><figref idref="DRAWINGS">FIG. <b>6</b></figref> shows preliminary center coordinates of the clusters identified by the base caller during the subpixel base calling. <figref idref="DRAWINGS">FIG. <b>6</b></figref> also shows &#x201c;origin subpixels&#x201d; or &#x201c;center subpixels&#x201d; that contain the preliminary center coordinates.</p><p id="p-0311" num="0310"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts one example of merging subpixel base calls produced over the plurality of sequencing cycles to generate the so-called &#x201c;cluster maps&#x201d; that contain the cluster metadata. In the illustrated implementation, the subpixel base calls are merged using a breadth-first search approach.</p><p id="p-0312" num="0311"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>illustrates one example of a cluster map generated by the merging of the subpixel base calls. <figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>depicts one example of subpixel base calling. <figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>also shows one implementation of analyzing subpixel-wise base call sequences produced from the subpixel base calling to generate a cluster map.</p><p id="p-0313" num="0312">Sequencing Images</p><p id="p-0314" num="0313">Cluster metadata determination involves analyzing image data produced by a sequencing instrument <b>102</b> (e.g., Illumina's iSeq, HiSeqX, HiSeq3000, HiSeq4000, HiSeq2500, NovaSeq 6000, NextSeq, NextSeqDx, MiSeq and MiSeqDx). The following discussion outlines how the image data is generated and what it depicts, in accordance with one implementation.</p><p id="p-0315" num="0314">Base calling is the process in which the raw signal of the sequencing instrument <b>102</b>, i.e., intensity data extracted from images, is decoded into DNA sequences and quality scores. In one implementation, the Illumina platforms employ cyclic reversible termination (CRT) chemistry for base calling. The process relies on growing nascent DNA strands complementary to template DNA strands with modified nucleotides, while tracking the emitted signal of each newly added nucleotide. The modified nucleotides have a 3&#x2032; removable block that anchors a fluorophore signal of the nucleotide type.</p><p id="p-0316" num="0315">Sequencing occurs in repetitive cycles, each comprising three steps: (a) extension of a nascent strand by adding a modified nucleotide; (b) excitation of the fluorophores using one or more lasers of the optical system <b>104</b> and imaging through different filters of the optical system <b>104</b>, yielding sequencing images <b>108</b>; and (c) cleavage of the fluorophores and removal of the 3&#x2032; block in preparation for the next sequencing cycle. Incorporation and imaging cycles are repeated up to a designated number of sequencing cycles, defining the read length of all clusters. Using this approach, each cycle interrogates a new position along the template strands.</p><p id="p-0317" num="0316">The tremendous power of the Illumina platforms stems from their ability to simultaneously execute and sense millions or even billions clusters undergoing CRT reactions. The sequencing process occurs in a flow cell <b>202</b>&#x2014;a small glass slide that holds the input DNA fragments during the sequencing process. The flow cell <b>202</b> is connected to the high-throughput optical system <b>104</b>, which comprises microscopic imaging, excitation lasers, and fluorescence filters. The flow cell <b>202</b> comprises multiple chambers called lanes <b>204</b>. The lanes <b>204</b> are physically separated from each other and may contain different tagged sequencing libraries, distinguishable without sample cross contamination. The imaging device <b>106</b> (e.g., a solid-state imager such as a charge-coupled device (CCD) or a complementary metal-oxide-semiconductor (CMOS) sensor) takes snapshots at multiple locations along the lanes <b>204</b> in a series of non-overlapping regions called tiles <b>206</b>.</p><p id="p-0318" num="0317">For example, there are a hundred tiles per lane in Illumina Genome Analyzer II and sixty-eight tiles per lane in Illumina HiSeq2000. A tile <b>206</b> holds hundreds of thousands to millions of clusters. An image generated from a tile with clusters shown as bright spots is shown at <b>208</b>. A cluster <b>302</b> comprises approximately one thousand identical copies of a template molecule, though clusters vary in size and shape. The clusters are grown from the template molecule, prior to the sequencing run, by bridge amplification of the input library. The purpose of the amplification and cluster growth is to increase the intensity of the emitted signal since the imaging device <b>106</b> cannot reliably sense a single fluorophore. However, the physical distance of the DNA fragments within a cluster <b>302</b> is small, so the imaging device <b>106</b> perceives the cluster of fragments as a single spot <b>302</b>.</p><p id="p-0319" num="0318">The output of a sequencing run is the sequencing images <b>108</b>, each depicting intensity emissions of clusters on the tile in the pixel domain for a specific combination of lane, tile, sequencing cycle, and fluorophore (<b>208</b>A, <b>208</b>C, <b>208</b>T, <b>208</b>G).</p><p id="p-0320" num="0319">In one implementation, a biosensor comprises an array of light sensors. A light sensor is configured to sense information from a corresponding pixel area (e.g., a reaction site/well/nanowell) on the detection surface of the biosensor. An analyte disposed in a pixel area is said to be associated with the pixel area, i.e., the associated analyte. At a sequencing cycle, the light sensor corresponding to the pixel area is configured to detect/capture/sense emissions/photons from the associated analyte and, in response, generate a pixel signal for each imaged channel. In one implementation, each imaged channel corresponds to one of a plurality of filter wavelength bands. In another implementation, each imaged channel corresponds to one of a plurality of imaging events at a sequencing cycle. In yet another implementation, each imaged channel corresponds to a combination of illumination with a specific laser and imaging through a specific optical filter.</p><p id="p-0321" num="0320">Pixel signals from the light sensors are communicated to a signal processor coupled to the biosensor (e.g., via a communication port). For each sequencing cycle and each imaged channel, the signal processor produces an image whose pixels respectively depict/contain/denote/represent/characterize pixel signals obtained from the corresponding light sensors. This way, a pixel in the image corresponds to: (i) a light sensor of the biosensor that generated the pixel signal depicted by the pixel, (ii) an associated analyte whose emissions were detected by the corresponding light sensor and converted into the pixel signal, and (iii) a pixel area on the detection surface of the biosensor that holds the associated analyte.</p><p id="p-0322" num="0321">Consider, for example, that a sequencing run uses two different imaged channels: a red channel and a green channel. Then, at each sequencing cycle, the signal processor produces a red image and a green image. This way, for a series of k sequencing cycles of the sequencing run, a sequence with k pairs of red and green images is produced as output.</p><p id="p-0323" num="0322">Pixels in the red and green images (i.e., different imaged channels) have one-to-one correspondence within a sequencing cycle. This means that corresponding pixels in a pair of the red and green images depict intensity data for the same associated analyte, albeit in different imaged channels. Similarly, pixels across the pairs of red and green images have one-to-one correspondence between the sequencing cycles. This means that corresponding pixels in different pairs of the red and green images depict intensity data for the same associated analyte, albeit for different acquisition events/timesteps (sequencing cycles) of the sequencing run.</p><p id="p-0324" num="0323">Corresponding pixels in the red and green images (i.e., different imaged channels) can be considered a pixel of a &#x201c;per-cycle image&#x201d; that expresses intensity data in a first red channel and a second green channel. A per-cycle image whose pixels depict pixel signals for a subset of the pixel areas, i.e., a region (tile) of the detection surface of the biosensor, is called a &#x201c;per-cycle tile image.&#x201d; A patch extracted from a per-cycle tile image is called a &#x201c;per-cycle image patch.&#x201d; In one implementation, the patch extraction is performed by an input preparer.</p><p id="p-0325" num="0324">The image data comprises a sequence of per-cycle image patches generated for a series of k sequencing cycles of a sequencing run. The pixels in the per-cycle image patches contain intensity data for associated analytes and the intensity data is obtained for one or more imaged channels (e.g., a red channel and a green channel) by corresponding light sensors configured to detect emissions from the associated analytes. In one implementation, when a single target cluster is to be base called, the per-cycle image patches are centered at a center pixel that contains intensity data for a target associated analyte and non-center pixels in the per-cycle image patches contain intensity data for associated analytes adjacent to the target associated analyte. In one implementation, the image data is prepared by an input preparer.</p><p id="p-0326" num="0325">Subpixel Base Calling</p><p id="p-0327" num="0326">The technology disclosed accesses a series of image sets generated during a sequencing run. The image sets comprise the sequencing images <b>108</b>. Each image set in the series is captured during a respective sequencing cycle of the sequencing run. Each image (or sequencing image) in the series captures clusters on a tile of a flow cell and their surrounding background.</p><p id="p-0328" num="0327">In one implementation, the sequencing run utilizes four-channel chemistry and each image set has four images. In another implementation, the sequencing run utilizes two-channel chemistry and each image set has two images. In yet another implementation, the sequencing run utilizes one-channel chemistry and each image set has two images. In yet other implementations, each image set has only one image.</p><p id="p-0329" num="0328">The sequencing images <b>108</b> in the pixel domain are first converted into the subpixel domain by a subpixel addresser <b>110</b> to produce sequencing images <b>112</b> in the subpixel domain. In one implementation, each pixel in the sequencing images <b>108</b> is divided into sixteen subpixels <b>502</b>. Thus, in one implementation, the subpixels <b>502</b> are quarter subpixels. In another implementation, the subpixels <b>502</b> are half subpixels. As a result, each of the sequencing images <b>112</b> in the subpixel domain has a plurality of subpixels <b>502</b>.</p><p id="p-0330" num="0329">The subpixels are then separately fed as input to a base caller <b>114</b> to obtain, from the base caller <b>114</b>, a base call classifying each of the subpixels as one of four bases (A, C, T, and G). This produces a base call sequence <b>116</b> for each of the subpixels across a plurality of sequencing cycles of the sequencing run. In one implementation, the subpixels <b>502</b> are identified to the base caller <b>114</b> based on their integer or non-integer coordinates. By tracking the emission signal from the subpixels <b>502</b> across image sets generated during the plurality of sequencing cycles, the base caller <b>114</b> recovers the underlying DNA sequence for each subpixel. An example of this is illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref><i>b. </i></p><p id="p-0331" num="0330">In other implementations, the technology disclosed obtains, from the base caller <b>114</b>, the base call classifying each of the subpixels as one of five bases (A, C, T, G, and N). In such implementations, N base call denotes an undecided base call, usually due to low levels of extracted intensity.</p><p id="p-0332" num="0331">Some examples of the base caller <b>114</b> include non-neural network-based Illumina offerings such as the RTA (Real Time Analysis), the Firecrest program of the Genome Analyzer Analysis Pipeline, the WAR (Integrated Primary Analysis and Reporting) machine, and the OLB (Off-Line Basecaller). For example, the base caller <b>114</b> produces the base call sequences by interpolating intensity of the subpixels, including at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage. These techniques are described in detail in Appendix entitled &#x201c;Intensity Extraction Methods&#x201d;.</p><p id="p-0333" num="0332">In other implementations, the base caller <b>114</b> can be a neural network-based base caller, such as the neural network-based base caller <b>1514</b> disclosed herein.</p><p id="p-0334" num="0333">The subpixel-wise base call sequences <b>116</b> are then fed as input to a searcher <b>118</b>. The searcher <b>118</b> searches for substantially matching base call sequences of contiguous subpixels. Base call sequences of contiguous subpixels are &#x201c;substantially matching&#x201d; when a predetermined portion of base calls match on an ordinal position-wise basis (e.g., &#x3e;=41 matches in 45 cycles, &#x3c;=4 mismatches in 45 cycles, &#x3c;=4 mismatches in 50 cycles, or &#x3c;=2 mismatches in 34 cycles).</p><p id="p-0335" num="0334">The searcher <b>118</b> then generates a cluster map <b>802</b> that identifies clusters as disjointed regions, e.g., <b>804</b><i>a</i>-<i>d</i>, of contiguous subpixels that share a substantially matching base call sequence. This application uses &#x201c;disjointed&#x201d;, &#x201c;disjoint&#x201d;, and &#x201c;non-overlapping&#x201d; interchangeably. The search involves base calling the subpixels that contain parts of clusters to allow linking the called subpixels to contiguous subpixels with which they share a substantially matching base call sequence. In some implementations, the searcher <b>118</b> requires that at least some of the disjointed regions have a predetermined minimum number of subpixels (e.g., more than 4, 6, or 10 subpixels) to be processed as a cluster.</p><p id="p-0336" num="0335">In some implementations, the base caller <b>114</b> also identifies preliminary center coordinates of the clusters. Subpixels that contain the preliminary center coordinates are referred to as origin subpixels. Some example preliminary center coordinates (<b>604</b><i>a</i>-<i>c</i>) identified by the base caller <b>114</b> and corresponding origin subpixels (<b>606</b><i>a</i>-<i>c</i>) are shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. However, identification of the origin subpixels (preliminary center coordinates of the clusters) is not needed, as explained below. In some implementations, the searcher <b>118</b> uses breadth-first search for identifying substantially matching base call sequences of the subpixels by beginning with the origin subpixels <b>606</b><i>a</i>-<i>c </i>and continuing with successively contiguous non-origin subpixels <b>702</b><i>a</i>-<i>c</i>. This again is optional, as explained below.</p><p id="p-0337" num="0336">Cluster Map</p><p id="p-0338" num="0337"><figref idref="DRAWINGS">FIG. <b>8</b><i>a </i></figref>illustrates one example of a cluster map <b>802</b> generated by the merging of the subpixel base calls. The cluster map identifies a plurality of disjointed regions (depicted in various colors in <figref idref="DRAWINGS">FIG. <b>8</b><i>a</i></figref>). Each disjointed region comprises a non-overlapping group of contiguous subpixels that represents a respective cluster on a tile (from whose sequencing images and for which the cluster map is generated via the subpixel base calling). The region between the disjointed regions represents the background on the tile. The subpixels in the background region are called &#x201c;background subpixels&#x201d;. The subpixels in the disjointed regions are called &#x201c;cluster subpixels&#x201d; or &#x201c;cluster interior subpixels&#x201d;. In this discussion, origin subpixels are those subpixels in which preliminary center cluster coordinates determined by the RTA or another base caller, are located.</p><p id="p-0339" num="0338">The origin subpixels contain the preliminary center cluster coordinates. This means that the area covered by an origin subpixel includes a coordinate location that coincides with a preliminary center cluster coordinate location. Since the cluster map <b>802</b> is an image of logical subpixels, the origin subpixels are some of the subpixels in the cluster map.</p><p id="p-0340" num="0339">The search to identify clusters with substantially matching base call sequences of the subpixels does not need to begin with identification of the origin subpixels (preliminary center coordinates of the clusters) because the search can be done for all the subpixels and can start from any subpixel (e.g., 0,0 subpixel or any random subpixel). Thus, since each subpixel is evaluated to determine whether it shares a substantially matching base call sequence with another contiguous subpixel, the search does not depend on origin subpixels; the search can start with any subpixel.</p><p id="p-0341" num="0340">Irrespective of whether origin subpixels are used or not, certain clusters are identified that do not contain the origin subpixels (preliminary center coordinates of the clusters) predicted by the base caller <b>114</b>. Some examples of clusters identified by the merging of the subpixel base calls and not containing an origin subpixel are clusters <b>812</b><i>a</i>, <b>812</b><i>b</i>, <b>812</b><i>c</i>, <b>812</b><i>d</i>, and <b>812</b><i>e </i>in <figref idref="DRAWINGS">FIG. <b>8</b><i>a</i></figref>. Thus, the technology disclosed identifies additional or extra clusters for which the centers may not have been identified by the base caller <b>114</b>. Therefore, use of the base caller <b>114</b> for identification of origin subpixels (preliminary center coordinates of the clusters) is optional and not essential for the search of substantially matching base call sequences of contiguous subpixels.</p><p id="p-0342" num="0341">In one implementation, first, the origin subpixels (preliminary center coordinates of the clusters) identified by the base caller <b>114</b> are used to identify a first set of clusters (by identification of substantially matching base call sequences of contiguous subpixels). Then, subpixels that are not part of the first set of clusters are used to identify a second set of clusters (by identification of substantially matching base call sequences of contiguous subpixels). This allows the technology disclosed to identify additional or extra clusters for which the centers are not identified by the base caller <b>114</b>. Finally, subpixels that are not part of the first and second sets of clusters are identified as background subpixels.</p><p id="p-0343" num="0342"><figref idref="DRAWINGS">FIG. <b>8</b><i>b </i></figref>depicts one example of subpixel base calling. In <figref idref="DRAWINGS">FIG. <b>8</b><i>b</i></figref>, each sequencing cycle has an image set with four distinct images (i.e., A, C, T, G images) captured using four different wavelength bands (image/imaging channel) and four different fluorescent dyes (one for each base).</p><p id="p-0344" num="0343">In this example, pixels in images are divided into sixteen subpixels. Subpixels are then separately base called at each sequencing cycle by the base caller <b>114</b>. To base call a given subpixel at a particular sequencing cycle, the base caller <b>114</b> uses intensities of the given subpixel in each of the four A, C, T, G images. For example, intensities in image regions covered by subpixel 1 in each of the each of the four A, C, T, G images of cycle 1 are used to base call subpixel 1 at cycle 1. For subpixel 1, these image regions include top-left one-sixteenth area of the respective top-left pixels in each of the four A, C, T, G images of cycle 1. Similarly, intensities in image regions covered by subpixel m in each of the each of the four A, C, T, G images of cycle n are used to base call subpixel m at cycle n. For subpixel m, these image regions include bottom-right one-sixteenth area of the respective bottom-right pixels in each of the four A, C, T, G images of cycle 1.</p><p id="p-0345" num="0344">This process produces subpixel-wise base call sequences <b>116</b> across the plurality of sequencing cycles. Then, the searcher <b>118</b> evaluates pairs of contiguous subpixels to determine whether they have a substantially matching base call sequence. If yes, then the pair of subpixels is stored in the cluster map <b>802</b> as belonging to a same cluster in a disjointed region. If no, then the pair of subpixels is stored in the cluster map <b>802</b> as not belonging to a same disjointed region. The cluster map <b>802</b> therefore identifies contiguous sets of sub-pixels for which the base calls for the sub-pixels substantially match across a plurality of cycles. Cluster map <b>802</b> therefore uses information from multiple cycles to provide a plurality of clusters with a high confidence that each cluster of the plurality of clusters provides sequence data for a single DNA strand.</p><p id="p-0346" num="0345">A cluster metadata generator <b>122</b> then processes the cluster map <b>802</b> to determine cluster metadata, including determining spatial distribution of clusters, including their centers (<b>810</b><i>a</i>), shapes, sizes, background, and/or boundaries based on the disjointed regions (<figref idref="DRAWINGS">FIG. <b>9</b></figref>).</p><p id="p-0347" num="0346">In some implementations, the cluster metadata generator <b>122</b> identifies as background those subpixels in the cluster map <b>802</b> that do not belong to any of the disjointed regions and therefore do not contribute to any clusters. Such subpixels are referred to as background subpixels <b>806</b><i>a</i>-<i>c. </i></p><p id="p-0348" num="0347">In some implementations, the cluster map <b>802</b> identifies cluster boundary portions <b>808</b><i>a</i>-<i>c </i>between two contiguous subpixels whose base call sequences do not substantially match.</p><p id="p-0349" num="0348">The cluster map is stored in memory (e.g., cluster maps data store <b>120</b>) for use as ground truth for training a classifier such as the neural network-based template generator <b>1512</b> and the neural network-based base caller <b>1514</b>. The cluster metadata can also be stored in memory (e.g., cluster metadata data store <b>124</b>).</p><p id="p-0350" num="0349"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows another example of a cluster map that identifies cluster metadata, including spatial distribution of the clusters, along with cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries.</p><p id="p-0351" num="0350">Center of Mass (COM)</p><p id="p-0352" num="0351"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows how a center of mass (COM) of a disjointed region in a cluster map is calculated. The COM can be used as the &#x201c;revised&#x201d; or &#x201c;improved&#x201d; center of the corresponding cluster in downstream processing.</p><p id="p-0353" num="0352">In some implementations, a center of mass generator <b>1004</b>, on a cluster-by-cluster basis, determines hyperlocated center coordinates <b>1006</b> of the clusters by calculating centers of mass of the disjointed regions of the cluster map as an average of coordinates of respective contiguous subpixels forming the disjointed regions. It then stores the hyperlocated center coordinates of the clusters in the memory on the cluster-by-cluster basis for use as ground truth for training the classifier.</p><p id="p-0354" num="0353">In some implementations, a subpixel categorizer, on the cluster-by-cluster basis, identifies centers of mass subpixels <b>1008</b> in the disjointed regions <b>804</b><i>a</i>-<i>d </i>of the cluster map <b>802</b> at the hyperlocated center coordinates <b>1006</b> of the clusters.</p><p id="p-0355" num="0354">In other implementations, the cluster map is upsampled using interpolation. The upsampled cluster map is stored in the memory for use as ground truth for training the classifier.</p><p id="p-0356" num="0355">Decay Factor &#x26; Decay Map</p><p id="p-0357" num="0356"><figref idref="DRAWINGS">FIG. <b>11</b></figref> depicts one implementation of calculation of a weighted decay factor for a subpixel based on the Euclidean distance from the subpixel to the center of mass (COM) of the disjointed region to which the subpixel belongs. In the illustrated implementation, the weighted decay factor gives the highest value to the subpixel containing the COM and decreases for subpixels further away from the COM. The weighted decay factor is used to derive a ground truth decay map <b>1204</b> from a cluster map generated from the subpixel base calling discussed above. The ground truth decay map <b>1204</b> contains an array of units and assigns at least one output value to each unit in the array. In some implementations, the units are subpixels and each subpixel is assigned an output value based on the weighted decay factor. The ground truth decay map <b>1204</b> is then used as ground truth for training the disclosed neural network-based template generator <b>1512</b>. In some implementations, information from the ground truth decay map <b>1204</b> is also used to prepare input for the disclosed neural network-based base caller <b>1514</b>.</p><p id="p-0358" num="0357"><figref idref="DRAWINGS">FIG. <b>12</b></figref> illustrates one implementation of an example ground truth decay map <b>1204</b> derived from an example cluster map produced by the subpixel base calling as discussed above. In some implementations, in the upsampled cluster map, on the cluster-by-cluster basis, a value is assigned to each contiguous subpixel in the disjointed regions based on a decay factor <b>1102</b> that is proportional to distance <b>1106</b> of an contiguous subpixel from a center of mass subpixel <b>1104</b> in a disjointed region to which the contiguous subpixel belongs.</p><p id="p-0359" num="0358"><figref idref="DRAWINGS">FIG. <b>12</b></figref> depicts a ground truth decay map <b>1204</b>. In one implementation, the subpixel value is an intensity value normalized between zero and one. In another implementation, in the upsampled cluster map, a same predetermined value is assigned to all the subpixels identified as the background. In some implementations, the predetermined value is a zero intensity value.</p><p id="p-0360" num="0359">In some implementations, the ground truth decay map <b>1204</b> is generated by a ground truth decay map generator <b>1202</b> from the upsampled cluster map that expresses the contiguous subpixels in the disjointed regions and the subpixels identified as the background based on their assigned values. The ground truth decay map <b>1204</b> is stored in the memory for use as ground truth for training the classifier. In one implementation, each subpixel in the ground truth decay map <b>1204</b> has a value normalized between zero and one.</p><p id="p-0361" num="0360">Ternary (Three Class) Map</p><p id="p-0362" num="0361"><figref idref="DRAWINGS">FIG. <b>13</b></figref> illustrates one implementation of deriving a ground truth ternary map <b>1304</b> from a cluster map. The ground truth ternary map <b>1304</b> contains an array of units and assigns at least one output value to each unit in the array. By name, ternary map implementations of the ground truth ternary map <b>1304</b> assign three output values to each unit in the array, such that, for each unit, a first output value corresponds to a classification label or score for a background class, a second output value corresponds to a classification label or score for a cluster center class, and a third output value corresponds to a classification label or score for a cluster/cluster interior class. The ground truth ternary map <b>1304</b> is used as ground truth data for training the neural network-based template generator <b>1512</b>. In some implementations, information from the ground truth ternary map <b>1304</b> is also used to prepare input for the neural network-based base caller <b>1514</b>.</p><p id="p-0363" num="0362"><figref idref="DRAWINGS">FIG. <b>13</b></figref> depicts an example ground truth ternary map <b>1304</b>. In another implementation, in the upsampled cluster map, the contiguous subpixels in the disjointed regions are categorized on the cluster-by-cluster basis by a ground truth ternary map generator <b>1302</b>, as cluster interior subpixels belonging to a same cluster, the centers of mass subpixels as cluster center subpixels, and as background subpixels the subpixels not belonging to any cluster. In some implementations, the categorizations are stored in the ground truth ternary map <b>1304</b>. These categorizations and the ground truth ternary map <b>1304</b> are stored in the memory for use as ground truth for training the classifier.</p><p id="p-0364" num="0363">In other implementations, on the cluster-by-cluster basis, coordinates of the cluster interior subpixels, the cluster center subpixels, and the background subpixels are stored in the memory for use as ground truth for training the classifier. Then, the coordinates are downscaled by a factor used to upsample the cluster map. Then, on the cluster-by-cluster basis, the downscaled coordinates are stored in the memory for use as ground truth for training the classifier.</p><p id="p-0365" num="0364">In yet other implementations, the ground truth ternary map generator <b>1302</b> uses the cluster maps to generate the ternary ground truth data <b>1304</b> from the upsampled cluster map. The ternary ground truth data <b>1304</b> labels the background subpixels as belonging to a background class, the cluster center subpixels as belonging to a cluster center class, and the cluster interior subpixels as belonging to a cluster interior class. In some visualization implementations, color coding can be used to depict and distinguish the different class labels. The ternary ground truth data <b>1304</b> is stored in the memory for use as ground truth for training the classifier.</p><p id="p-0366" num="0365">Binary (Two Class) Map</p><p id="p-0367" num="0366"><figref idref="DRAWINGS">FIG. <b>14</b></figref> illustrates one implementation of deriving a ground truth binary map <b>1404</b> from a cluster map. The binary map <b>1404</b> contains an array of units and assigns at least one output value to each unit in the array. By name, the binary map assigns two output values to each unit in the array, such that, for each unit, a first output value corresponds to a classification label or score for a cluster center class and a second output value corresponds to a classification label or score for a non-center class. The binary map is used as ground truth data for training the neural network-based template generator <b>1512</b>. In some implementations, information from the binary map is also used to prepare input for the neural network-based base caller <b>1514</b>.</p><p id="p-0368" num="0367"><figref idref="DRAWINGS">FIG. <b>14</b></figref> depicts a ground truth binary map <b>1404</b>. The ground truth binary map generator <b>1402</b> uses the cluster maps <b>120</b> to generate the binary ground truth data <b>1404</b> from the upsampled cluster maps. The binary ground truth data <b>1404</b> labels the cluster center subpixels as belonging to a cluster center class and labels all other subpixels as belonging to a non-center class. The binary ground truth data <b>1404</b> is stored in the memory for use as ground truth for training the classifier.</p><p id="p-0369" num="0368">In some implementations, the technology disclosed generates cluster maps <b>120</b> for a plurality of tiles of the flow cell, stores the cluster maps in memory, and determines spatial distribution of clusters in the tiles based on the cluster maps <b>120</b>, including their shapes and sizes. Then, the technology disclosed, in the upsampled cluster maps <b>120</b> of the clusters in the tiles, categorizes, on a cluster-by-cluster basis, subpixels as cluster interior subpixels belonging to a same cluster, cluster center subpixels, and background subpixels. The technology disclosed then stores the categorizations in the memory for use as ground truth for training the classifier, and stores, on the cluster-by-cluster basis across the tiles, coordinates of the cluster interior subpixels, the cluster center subpixels, and the background subpixels in the memory for use as ground truth for training the classifier. The technology disclosed then downscales the coordinates by the factor used to upsample the cluster map and stores, on the cluster-by-cluster basis across the tiles, the downscaled coordinates in the memory for use as ground truth for training the classifier.</p><p id="p-0370" num="0369">In some implementations, the flow cell has at least one patterned surface with an array of wells that occupy the clusters. In such implementations, based on the determined shapes and sizes of the clusters, the technology disclosed determines: (1) which ones of the wells are substantially occupied by at least one cluster, (2) which ones of the wells are minimally occupied, and (3) which ones of the wells are co-occupied by multiple clusters. This allows for determining respective metadata of multiple clusters that co-occupy a same well, i.e., centers, shapes, and sizes of two or more clusters that share a same well.</p><p id="p-0371" num="0370">In some implementations, the solid support on which samples are amplified into clusters comprises a patterned surface. A &#x201c;patterned surface&#x201d; refers to an arrangement of different regions in or on an exposed layer of a solid support. For example, one or more of the regions can be features where one or more amplification primers are present. The features can be separated by interstitial regions where amplification primers are not present. In some implementations, the pattern can be an x-y format of features that are in rows and columns. In some implementations, the pattern can be a repeating arrangement of features and/or interstitial regions. In some implementations, the pattern can be a random arrangement of features and/or interstitial regions. Exemplary patterned surfaces that can be used in the methods and compositions set forth herein are described in U.S. Pat. Nos. 8,778,849, 9,079,148, 8,778,848, and US Pub. No. 2014/0243224, each of which is incorporated herein by reference.</p><p id="p-0372" num="0371">In some implementations, the solid support comprises an array of wells or depressions in a surface. This may be fabricated as is generally known in the art using a variety of techniques, including, but not limited to, photolithography, stamping techniques, molding techniques and microetching techniques. As will be appreciated by those in the art, the technique used will depend on the composition and shape of the array substrate.</p><p id="p-0373" num="0372">The features in a patterned surface can be wells in an array of wells (e.g. microwells or nanowells) on glass, silicon, plastic or other suitable solid supports with patterned, covalently-linked gel such as poly(N-(5-azidoacetamidylpentyfiacrylamide-co-acrylamide) (PAZAM, see, for example, US Pub. No. 2013/184796, WO 2016/066586, and WO 2015-002813, each of which is incorporated herein by reference in its entirety). The process creates gel pads used for sequencing that can be stable over sequencing runs with a large number of cycles. The covalent linking of the polymer to the wells is helpful for maintaining the gel in the structured features throughout the lifetime of the structured substrate during a variety of uses. However in many implementations, the gel need not be covalently linked to the wells. For example, in some conditions silane free acrylamide (SFA, see, for example, U.S. Pat. No. 8,563,477, which is incorporated herein by reference in its entirety) which is not covalently attached to any part of the structured substrate, can be used as the gel material.</p><p id="p-0374" num="0373">In particular implementations, a structured substrate can be made by patterning a solid support material with wells (e.g. microwells or nanowells), coating the patterned support with a gel material (e.g. PAZAM, SFA or chemically modified variants thereof, such as the azidolyzed version of SFA (azido-SFA)) and polishing the gel coated support, for example via chemical or mechanical polishing, thereby retaining gel in the wells but removing or inactivating substantially all of the gel from the interstitial regions on the surface of the structured substrate between the wells. Primer nucleic acids can be attached to gel material. A solution of target nucleic acids (e.g. a fragmented human genome) can then be contacted with the polished substrate such that individual target nucleic acids will seed individual wells via interactions with primers attached to the gel material; however, the target nucleic acids will not occupy the interstitial regions due to absence or inactivity of the gel material. Amplification of the target nucleic acids will be confined to the wells since absence or inactivity of gel in the interstitial regions prevents outward migration of the growing nucleic acid colony. The process is conveniently manufacturable, being scalable and utilizing micro- or nano-fabrication methods.</p><p id="p-0375" num="0374">The term &#x201c;flow cell&#x201d; as used herein refers to a chamber comprising a solid surface across which one or more fluid reagents can be flowed. Examples of flow cells and related fluidic systems and detection platforms that can be readily used in the methods of the present disclosure are described, for example, in Bentley et al., Nature 456:53-59 (2008), WO 04/018497; U.S. Pat. No. 7,057,026; WO 91/06678; WO 07/123744; U.S. Pat. Nos. 7,329,492; 7,211,414; 7,315,019; 7,405,281, and US 2008/0108082, each of which is incorporated herein by reference.</p><p id="p-0376" num="0375">Throughout this disclosure, the terms &#x201c;P5&#x201d; and &#x201c;P7&#x201d; are used when referring to amplification primers. It will be understood that any suitable amplification primers can be used in the methods presented herein, and that the use of P5 and P7 are exemplary implementations only. Uses of amplification primers such as P5 and P7 on flow cells is known in the art, as exemplified by the disclosures of WO 2007/010251, WO 2006/064199, WO 2005/065814, WO 2015/106941, WO 1998/044151, and WO 2000/018957, each of which is incorporated by reference in its entirety. For example, any suitable forward amplification primer, whether immobilized or in solution, can be useful in the methods presented herein for hybridization to a complementary sequence and amplification of a sequence. Similarly, any suitable reverse amplification primer, whether immobilized or in solution, can be useful in the methods presented herein for hybridization to a complementary sequence and amplification of a sequence. One of skill in the art will understand how to design and use primer sequences that are suitable for capture, and amplification of nucleic acids as presented herein.</p><p id="p-0377" num="0376">In some implementations, the flow cell has at least one nonpatterned surface and the clusters are unevenly scattered over the nonpatterned surface.</p><p id="p-0378" num="0377">In some implementations, density of the clusters ranges from about 100,000 clusters/mm<sup>2 </sup>to about 1,000,000 clusters/1 mm<sup>2</sup>. In other implementations, density of the clusters ranges from about 1,000,000 clusters/mm<sup>2 </sup>to about 10,000,000 clusters/1 mm<sup>2</sup>.</p><p id="p-0379" num="0378">In one implementation, the preliminary center coordinates of the clusters determined by the base caller are defined in a template image of the tile. In some implementations, a pixel resolution, an image coordinate system, and measurement scales of the image coordinate system are same for the template image and the images.</p><p id="p-0380" num="0379">In another implementation, the technology disclosed relates to determining metadata about clusters on a tile of a flow cell. First, the technology disclosed accesses (1) a set of images of the tile captured during a sequencing run and (2) preliminary center coordinates of the clusters determined by a base caller.</p><p id="p-0381" num="0380">Then, for each image set, the technology disclosed obtains a base call classifying, as one of four bases, (1) origin subpixels that contain the preliminary center coordinates and (2) a predetermined neighborhood of contiguous subpixels that are successively contiguous to respective ones of the origin subpixels. This produces a base call sequence for each of the origin subpixels and for each of the predetermined neighborhood of contiguous subpixels. The predetermined neighborhood of contiguous subpixels can be a m&#xd7;n subpixel patch centered at subpixels containing the origin subpixels. In one implementation, the subpixel patch is 3&#xd7;3 subpixels. In other implementations, it the image patch can be of any size, such as 5&#xd7;5, 15&#xd7;15, 20&#xd7;20, and so on. In other implementations, the predetermined neighborhood of contiguous subpixels can be a n-connected subpixel neighborhood centered at subpixels containing the origin subpixels.</p><p id="p-0382" num="0381">In one implementation, the technology disclosed identifies as background those subpixels in the cluster map that do not belong to any of the disjointed regions.</p><p id="p-0383" num="0382">Then, the technology disclosed generates a cluster map that identifies the clusters as disjointed regions of contiguous subpixels that: (a) are successively contiguous to at least some of the respective ones of the origin subpixels and (b) share a substantially matching base call sequence of the one of four bases with the at least some of the respective ones of the origin subpixels.</p><p id="p-0384" num="0383">The technology disclosed then stores the cluster map in memory and determines the shapes and the sizes of the clusters based on the disjointed regions in the cluster map. In other implementations, centers of the clusters are also determined.</p><p id="p-0385" num="0384">Generating Training Data for Template Generator</p><p id="p-0386" num="0385"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram that shows one implementation of generating training data that is used to train the neural network-based template generator <b>1512</b> and the neural network-based base caller <b>1514</b>.</p><p id="p-0387" num="0386"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows characteristics of the disclosed training examples used to train the neural network-based template generator <b>1512</b> and the neural network-based base caller <b>1514</b>. Each training example corresponds to a tile and is labelled with a corresponding ground truth data representation. In some implementations, the ground truth data representation is a ground truth mask or a ground truth map that identifies the ground truth cluster metadata in the form of the ground truth decay map <b>1204</b>, the ground truth ternary map <b>1304</b>, or the ground truth binary map <b>1404</b>. In some implementation, multiple training examples correspond to a same tile.</p><p id="p-0388" num="0387">In one implementation, the technology disclosed relates to generating training data <b>1504</b> for neural network-based template generation and base calling. First, the technology disclosed accesses a multitude of images <b>108</b> of a flow cell <b>202</b> captured over a plurality of cycles of a sequencing run. The flow cell <b>202</b> has a plurality of tiles. In the multitude of images <b>108</b>, each of the tiles has a sequence of image sets generated over the plurality of cycles. Each image in the sequence of image sets <b>108</b> depicts intensity emissions of clusters <b>302</b> and their surrounding background <b>304</b> on a particular one of the tiles at a particular one the cycles.</p><p id="p-0389" num="0388">Then, a training set constructor <b>1502</b> constructs a training set <b>1504</b> that has a plurality of training examples. As shown in <figref idref="DRAWINGS">FIG. <b>16</b></figref>, each training example corresponds to a particular one of the tiles and includes image data from at least some image sets in the sequence of image sets <b>1602</b> of the particular one of the tiles. In one implementation, the image data includes images in at least some image sets in the sequence of image sets <b>1602</b> of the particular one of the tiles. For example, the images can have a resolution of 1800&#xd7;1800. In other implementations, it can be any resolution such as 100&#xd7;100, 3000&#xd7;3000, 10000&#xd7;10000, and so on. In yet other implementations, the image data includes at least one image patch from each of the images. In one implementation, the image patch covers a portion of the particular one of the tiles. In one example, the image patch can have a resolution of 20&#xd7;20. In other implementations, the image patch can have any resolution, such as 50&#xd7;50, 70&#xd7;70, 90&#xd7;90, 100&#xd7;100, 3000&#xd7;3000, 10000&#xd7;10000, and so on.</p><p id="p-0390" num="0389">In some implementations, the image data includes an upsampled representation of the image patch. The upsampled representation can have a resolution of 80&#xd7;80, for example. In other implementations, the upsampled representation can have any resolution, such as 50&#xd7;50, 70&#xd7;70, 90&#xd7;90, 100&#xd7;100, 3000&#xd7;3000, 10000&#xd7;10000, and so on.</p><p id="p-0391" num="0390">In some implementations, multiple training examples correspond to a same particular one of the tiles and respectively include as image data different image patches from each image in each of at least some image sets in a sequence of image sets <b>1602</b> of the same particular one of the tiles. In such implementations, at least some of the different image patches overlap with each other.</p><p id="p-0392" num="0391">Then, a ground truth generator <b>1506</b> generates at least one ground truth data representation for each of the training examples. The ground truth data representation identifies at least one of spatial distribution of clusters and their surrounding background on the particular one of the tiles whose intensity emissions are depicted by the image data, including at least one of cluster shapes, cluster sizes, and/or cluster boundaries, and/or centers of the clusters.</p><p id="p-0393" num="0392">In one implementation, the ground truth data representation identifies the clusters as disjointed regions of contiguous subpixels, the centers of the clusters as centers of mass subpixels within respective ones of the disjointed regions, and their surrounding background as subpixels that do not belong to any of the disjointed regions.</p><p id="p-0394" num="0393">In one implementation, the ground truth data representation has an upsampled resolution of 80&#xd7;80. In other implementations, the ground truth data representation can have any resolution, such as 50&#xd7;50, 70&#xd7;70, 90&#xd7;90, 100&#xd7;100, 3000&#xd7;3000, 10000&#xd7;10000, and so on.</p><p id="p-0395" num="0394">In one implementation, the ground truth data representation identifies each subpixel as either being a cluster center or a non-center. In another implementation, the ground truth data representation identifies each subpixel as either being cluster interior, cluster center, or surrounding background.</p><p id="p-0396" num="0395">In some implementations, the technology disclosed stores, in memory, the training examples in the training set <b>1504</b> and associated ground truth data <b>1508</b> as the training data <b>1504</b> for training the neural network-based template generator <b>1512</b> and the neural network-based base caller <b>1514</b>. The training is operationalized by trainer <b>1510</b>.</p><p id="p-0397" num="0396">In some implementations, the technology disclosed generates the training data for a variety of flow cells, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, and cluster densities.</p><p id="p-0398" num="0397">Neural Network-Based Template Generator</p><p id="p-0399" num="0398">In an inference or production implementation, the technology disclosed uses peak detection and segmentation to determine cluster metadata. The technology disclosed processes input image data <b>1702</b> derived from a series of image sets <b>1602</b> through a neural network <b>1706</b> to generate an alternative representation <b>1708</b> of the input image data <b>1702</b>. For example, an image set can be for a particular sequencing cycle and include four images, one for each image channel A, C, T, and G. Then, for a sequencing run with fifty sequencing cycles, there will be fifty such image sets, i.e., a total of 200 images. When arranged temporally, fifty image sets with four images-per image set would form the series of image sets <b>1602</b>. In some implementations, image patches of a certain size are extracted from each image in the fifty image sets, forming fifty image patch sets with four image patches-per image patch set and, in one implementation, this is the input image data <b>1702</b>. In other implementations, the input image data <b>1702</b> comprises image patch sets with four image patches-per image patch set for fewer than the fifty sequencing cycles, i.e., just one, two, three, fifteen, twenty sequencing cycles.</p><p id="p-0400" num="0399"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates one implementation of processing input image data <b>1702</b> through the neural network-based template generator <b>1512</b> and generating an output value for each unit in an array. In one implementation, the array is a decay map <b>1716</b>. In another implementation, the array is a ternary map <b>1718</b>. In yet another implementation, the array is a binary map <b>1720</b>. The array may therefore represent one or more properties of each of a plurality of locations represented in the input image data <b>1702</b>.</p><p id="p-0401" num="0400">Different than training the template generator using structures in earlier figures, including the ground truth decay map <b>1204</b>, the ground truth ternary map <b>1304</b>, and the ground truth binary <b>1404</b>, the decay map <b>1716</b>, the ternary map <b>1718</b>, and/or the binary map <b>1720</b> are generated by forward propagation of the trained neural network-based template generator <b>1512</b>. The forward propagation can be during training or during inference. During the training, due to the backward propagation-based gradient update, the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> (i.e., cumulatively the output <b>1714</b>) progressively match or approach the ground truth decay map <b>1204</b>, the ground truth ternary map <b>1304</b>, and the ground truth binary map <b>1404</b>, respectively.</p><p id="p-0402" num="0401">The size of the image array analyzed during inference depends on the size of the input image data <b>1702</b> (e.g., be the same or an upscaled or downscaled version), according to one implementation. Each unit can represent a pixel, a subpixel, or a superpixel. The unit-wise output values of an array can characterize/represent/denote the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>. In some implementations, the input image data <b>1702</b> is also an array of units in the pixel, subpixel, or superpixel resolution. In such an implementation, the neural network-based template generator <b>1512</b> uses semantic segmentation techniques to produce an output value for each unit in the input array. Additional details about the input image data <b>1702</b> can be found in <figref idref="DRAWINGS">FIGS. <b>21</b><i>b</i></figref>, <b>22</b>, <b>23</b>, and <b>24</b> and their discussion.</p><p id="p-0403" num="0402">In some implementations, the neural network-based template generator <b>1512</b> is a fully convolutional network, such as the one described in J. Long, E. Shelhamer, and T. Darrell, &#x201c;Fully convolutional networks for semantic segmentation,&#x201d; in CVPR, (2015), which is incorporated herein by reference. In other implementations, the neural network-based template generator <b>1512</b> is a U-Net network with skip connections between the decoder and the encoder between the decoder and the encoder, such as the one described in Ronneberger O, Fischer P, Brox T., &#x201c;U-net: Convolutional networks for biomedical image segmentation,&#x201d; Med. Image Comput. Comput. Assist. Interv. (2015), available at: http://link.springer.com/chapter/10.1007/978-3-319-24574-428, which is incorporated herein by reference. The U-Net architecture resembles an autoencoder with two main sub-structures: 1) an encoder, which takes an input image and reduces its spatial resolution through multiple convolutional layers to create a representation encoding. 2) A decoder, which takes the representation encoding and increases spatial resolution back to produce a reconstructed image as output. The U-Net introduces two innovations to this architecture: First, the objective function is set to reconstruct a segmentation mask using a loss function; and second, the convolutional layers of the encoder are connected to the corresponding layers of the same resolution in the decoder using skip connections. In yet further implementations, the neural network-based template generator <b>1512</b> is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network. In such an implementation, the encoder subnetwork includes a hierarchy of encoders and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps. Additional details about segmentation networks can be found in Appendix entitled &#x201c;Segmentation Networks&#x201d;.</p><p id="p-0404" num="0403">In one implementation, the neural network-based template generator <b>1512</b> is a convolutional neural network. In another implementation, the neural network-based template generator <b>1512</b> is a recurrent neural network. In yet another implementation, the neural network-based template generator <b>1512</b> is a residual neural network with residual bocks and residual connections. In a further implementation, the neural network-based template generator <b>1512</b> is a combination of a convolutional neural network and a recurrent neural network.</p><p id="p-0405" num="0404">One skilled in the art will appreciate that the neural network-based template generator <b>1512</b> (i.e., the neural network <b>1706</b> and/or the output layer <b>1710</b>) can use various padding and striding configurations. It can use different output functions (e.g., classification or regression) and may or may not include one or more fully-connected layers. It can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. It can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. It can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous SGD. It can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LS&#x2122; or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tan h)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><p id="p-0406" num="0405">In some implementations, each image in the sequence of image sets <b>1602</b> covers a tile and depicts intensity emissions of clusters on a tile and their surrounding background captured for a particular imaging channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on a flow cell. In one implementation, the input image data <b>1702</b> includes at least one image patch from each of the images in the sequence of image sets <b>1602</b>. In such an implementation, the image patch covers a portion of the tile. In one example, the image patch has a resolution of 20&#xd7;20. In other cases, the resolution of the image patch can range from 20&#xd7;20 to 10000&#xd7;10000. In another implementation, the input image data <b>1702</b> includes an upsampled, subpixel resolution representation of the image patch from each of the images in the sequence of image sets <b>1602</b>. In one example, the upsampled, subpixel representation has a resolution of 80&#xd7;80. In other cases, the resolution of the upsampled, subpixel representation can range from 80&#xd7;80 to 10000&#xd7;10000.</p><p id="p-0407" num="0406">The input image data <b>1702</b> has an array of units <b>1704</b> that depicts clusters and their surrounding background. For example, an image set can be for a particular sequencing cycle and include four images, one for each image channel A, C, T, and G. Then, for a sequencing run with fifty sequencing cycles, there will be fifty such image sets, i.e., a total of 200 images. When arranged temporally, fifty image sets with four images-per image set would form the series of image sets <b>1602</b>. In some implementations, image patches of a certain size are extracted from each image in the fifty image sets, forming fifty image patch sets with four image patches-per image patch set and, in one implementation, this is the input image data <b>1702</b>. In other implementations, the input image data <b>1702</b> comprises image patch sets with four image patches-per image patch set for fewer than the fifty sequencing cycles, i.e., just one, two, three, fifteen, twenty sequencing cycles. The alternative representation is a feature map. The feature map can be a convolved feature or convolved representation when the neural network is a convolutional neural network. The feature map can be a hidden state feature or hidden state representation when the neural network is a recurrent neural network.</p><p id="p-0408" num="0407">Then, the technology disclosed processes the alternative representation <b>1708</b> through an output layer <b>1710</b> to generate an output <b>1714</b> that has an output value <b>1712</b> for each unit in the array <b>1704</b>. The output layer can be a classification layer such as softmax or sigmoid that produces unit-wise output values. In one implementation, the output layer is a ReLU layer or any other activation function layer that produces unit-wise output values.</p><p id="p-0409" num="0408">In one implementation, the units in the input image data <b>1702</b> are pixels and therefore pixel-wise output values <b>1712</b> are produced in the output <b>1714</b>. In another implementation, the units in the input image data <b>1702</b> are subpixels and therefore subpixel-wise output values <b>1712</b> are produced in the output <b>1714</b>. In yet another implementation, the units in the input image data <b>1702</b> are superpixels and therefore superpixel-wise output values <b>1712</b> are produced in the output <b>1714</b>.</p><p id="p-0410" num="0409">Deriving Cluster Metadata from Decay Map, Ternary Map, and/or Binary Map</p><p id="p-0411" num="0410"><figref idref="DRAWINGS">FIG. <b>18</b></figref> shows one implementation of post-processing techniques that are applied to the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> produced by the neural network-based template generator <b>1512</b> to derive cluster metadata, including cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries. In some implementations, the post-processing techniques are applied by a post-processor <b>1814</b> that further comprises a thresholder <b>1802</b>, a peak locator <b>1806</b>, and a segmenter <b>1810</b>.</p><p id="p-0412" num="0411">The input to the thresholder <b>1802</b> is the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> produced by template generator <b>1512</b>, such as the disclosed neural network-based template generator. In one implementation, the thresholder <b>1802</b> applies thresholding on the values in the decay map, the ternary map, or the binary map to identify background units <b>1804</b> (i.e., subpixels characterizing non-cluster background).) and non-background units. Said differently, once the output <b>1714</b> is produced, the thresholder <b>1802</b> thresholds output values of the units <b>1712</b> and classifies, or can reclassify a first subset of the units <b>1712</b> as &#x201c;background units&#x201d; <b>1804</b> depicting the surrounding background of the clusters and &#x201c;non-background units&#x201d; depicting units that potentially belong to clusters. The threshold value applied by the thresholder <b>1802</b> can be preset.</p><p id="p-0413" num="0412">The input to the peak locator <b>1806</b> is also the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> produced by the neural network-based template generator <b>1512</b>. In one implementation, the peak locator <b>1806</b> applies peak detection on the values in the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> to identify center units <b>1808</b> (i.e., center subpixels characterizing cluster centers). Said differently, the peak locator <b>1806</b> processes the output values of the units <b>1712</b> in the output <b>1714</b> and classifies a second subset of the units <b>1712</b> as &#x201c;center units&#x201d; <b>1808</b> containing centers of the clusters. In some implementations, the centers of the clusters detected by the peak locator <b>1806</b> are also the centers of mass of the clusters. The center units <b>1808</b> are then provided to the segmenter <b>1810</b>. Additional details about the peak locator <b>1806</b> can be found in the Appendix entitled &#x201c;Peak Detection&#x201d;.</p><p id="p-0414" num="0413">The thresholding and the peak detection can be done in parallel or one after the other. That is, they are not dependent on each other.</p><p id="p-0415" num="0414">The input to the segmenter <b>1810</b> is also the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> produced by the neural network-based template generator <b>1512</b>. Additional supplemental input to the segmenter <b>1810</b> comprises the thresholded units (background, non-background) <b>1804</b> identified by the thresholder <b>1802</b> and the center units <b>1808</b> identified by the peak locator <b>1806</b>. The segmenter <b>1810</b> uses the background, non-background <b>1804</b> and the center units <b>1808</b> to identify disjointed regions <b>1812</b> (i.e., non-overlapping groups of contiguous cluster/cluster interior subpixels characterizing clusters). Said differently, the segmenter <b>1810</b> processes the output values of the units <b>1712</b> in the output <b>1714</b> and uses the background, non-background units <b>1804</b> and the center units <b>1808</b> to determine shapes <b>1812</b> of the clusters as non-overlapping regions of contiguous units separated by the background units <b>1804</b> and centered at the center units <b>1808</b>. The output of the segmenter <b>1810</b> is cluster metadata <b>1812</b>. The cluster metadata <b>1812</b> identifies cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries.</p><p id="p-0416" num="0415">In one implementation, the segmenter <b>1810</b> begins with the center units <b>1808</b> and determines, for each center unit, a group of successively contiguous units that depict a same cluster whose center of mass is contained in the center unit. In one implementation, the segmenter <b>1810</b> uses a so-called &#x201c;watershed&#x201d; segmentation technique to subdivide contiguous clusters into multiple adjoining clusters at a valley in intensity. Additional details about the watershed segmentation technique and other segmentation techniques can be found in Appendix entitled &#x201c;Watershed Segmentation&#x201d;.</p><p id="p-0417" num="0416">In one implementation, the output values of the units <b>1712</b> in the output <b>1714</b> are continuous values, such as the one encoded in the ground truth decay map <b>1204</b>. In another implementation, the output values are softmax scores, such as the one encoded in the ground truth ternary map <b>1304</b> and the ground truth binary map <b>1404</b>. In the ground truth decay map <b>1204</b>, according to one implementation, the contiguous units in the respective ones of the non-overlapping regions have output values weighted according to distance of a contiguous unit from a center unit in a non-overlapping region to which the contiguous unit belongs. In such an implementation, the center units have highest output values within the respective ones of the non-overlapping regions. As discussed above, during the training, due to the backward propagation-based gradient update, the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> (i.e., cumulatively the output <b>1714</b>) progressively match or approach the ground truth decay map <b>1204</b>, the ground truth ternary map <b>1304</b>, and the ground truth binary map <b>1404</b>, respectively.</p><p id="p-0418" num="0417">Pixel Domain&#x2014;Intensity Extraction from Irregular Cluster Shapes</p><p id="p-0419" num="0418">The discussion now turns to how cluster shapes determined by the technology disclosed can be used to extract intensity of the clusters. Since clusters typically have irregular shapes and contours, the technology disclosed can be used to identify which subpixels contribute to the irregularly shaped disjointed/non-overlapping regions that represent the cluster shapes.</p><p id="p-0420" num="0419"><figref idref="DRAWINGS">FIG. <b>19</b></figref> depicts one implementation of extracting cluster intensity in the pixel domain. &#x201c;Template image&#x201d; or &#x201c;template&#x201d; can refer to a data structure that contains or identifies the cluster metadata <b>1812</b> derived from the decay map <b>1716</b>, the ternary map <b>1718</b>, and/or the binary map <b>1718</b>. The cluster metadata <b>1812</b> identifies cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries.</p><p id="p-0421" num="0420">In some implementations, the template image is in the upsampled, subpixel domain to distinguish the cluster boundaries at a fine-grained level. However, the sequencing images <b>108</b>, which contain the cluster and background intensity data, are typically in the pixel domain. Thus, the technology disclosed proposes two approaches to use the cluster shape information encoded in the template image in the upsampled, subpixel resolution to extract intensities of the irregularly shaped clusters from the optical, pixel-resolution sequencing images. In the first approach, depicted in <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the non-overlapping groups of contiguous subpixels identified in the template image are located in the pixel resolution sequencing images and their intensities extracted via interpolation. Additional details about this intensity extraction technique can be found in <figref idref="DRAWINGS">FIG. <b>33</b></figref> and its discussion.</p><p id="p-0422" num="0421">In one implementation, when the non-overlapping regions have irregular contours and the units are subpixels, the cluster intensity <b>1912</b> of a given cluster is determined by an intensity extractor <b>1902</b> as follows.</p><p id="p-0423" num="0422">First, a subpixel locator <b>1904</b> identifies subpixels that contribute to the cluster intensity of the given cluster based on a corresponding non-overlapping region of contiguous subpixels that identifies a shape of the given cluster.</p><p id="p-0424" num="0423">Then, the subpixel locator <b>1904</b> locates the identified subpixels in one or more optical, pixel-resolution images <b>1918</b> generated for one or more imaging channels at a current sequencing cycle. In one implementation, integer or non-integer coordinates (e.g., floating points) are located in the optical, pixel-resolution images, after a downscaling based on a downscaling factor that matches an upsampling factor used to create the subpixel domain.</p><p id="p-0425" num="0424">Then, an interpolator and subpixel intensity combiner <b>1906</b>, intensities of the identified subpixels in the images processed, combines the interpolated intensities, and normalizes the combined interpolated intensities to produce a per-image cluster intensity for the given cluster in each of the images. The normalization is performed by a normalizer <b>1908</b> and is based on a normalization factor. In one implementation, the normalization factor is a number of the identified subpixels. This is done to normalize/account for different cluster sizes and uneven illuminations that clusters receive depending on their location on the flow cell.</p><p id="p-0426" num="0425">Finally, a cross-channel subpixel intensity accumulator <b>1910</b> combines the per-image cluster intensity for each of the images to determine the cluster intensity <b>1912</b> of the given cluster at the current sequencing cycle.</p><p id="p-0427" num="0426">Then, the given cluster is base called based on the cluster intensity <b>1912</b> at the current sequencing cycle by any one of the base callers discussed in this application, yielding base calls <b>1916</b>.</p><p id="p-0428" num="0427">In some implementations though, when the cluster sizes are large enough, the output of the neural network-based base caller <b>1514</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the optical, pixel domain. Accordingly, in such implementations, the template image is also in the optical, pixel domain.</p><p id="p-0429" num="0428">Subpixel Domain&#x2014;Intensity Extraction from Irregular Cluster Shapes</p><p id="p-0430" num="0429"><figref idref="DRAWINGS">FIG. <b>20</b></figref> depicts the second approach of extracting cluster intensity in the subpixel domain. In this second approach, the sequencing images in the optical, pixel-resolution are upsampled into the subpixel resolution. This results in correspondence between the &#x201c;cluster shape depicting subpixels&#x201d; in the template image and the &#x201c;cluster intensity depicting subpixels&#x201d; in the upsampled sequencing images. The cluster intensity is then extracted based on the correspondence. Additional details about this intensity extraction technique can be found in <figref idref="DRAWINGS">FIG. <b>33</b></figref> and its discussion.</p><p id="p-0431" num="0430">In one implementation, when the non-overlapping regions have irregular contours and the units are subpixels, the cluster intensity <b>2012</b> of a given cluster is determined by an intensity extractor <b>2002</b> as follows.</p><p id="p-0432" num="0431">First, a subpixel locator <b>2004</b> identifies subpixels that contribute to the cluster intensity of the given cluster based on a corresponding non-overlapping region of contiguous subpixels that identifies a shape of the given cluster.</p><p id="p-0433" num="0432">Then, the subpixel locator <b>2004</b> locates the identified subpixels in one or more subpixel resolution images <b>2018</b> upsampled from corresponding optical, pixel-resolution images <b>1918</b> generated for one or more imaging channels at a current sequencing cycle. The upsampling can be performed by nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage. These techniques are described in detail in Appendix entitled &#x201c;Intensity Extraction Methods&#x201d;. The template image can, in some implementations, serve as a mask for intensity extraction.</p><p id="p-0434" num="0433">Then, a subpixel intensity combiner <b>2006</b>, in each of the upsampled images, combines intensities of the identified subpixels and normalizes the combined intensities to produce a per-image cluster intensity for the given cluster in each of the upsampled images. The normalization is performed by a normalizer <b>2008</b> and is based on a normalization factor. In one implementation, the normalization factor is a number of the identified subpixels. This is done to normalize/account for different cluster sizes and uneven illuminations that clusters receive depending on their location on the flow cell.</p><p id="p-0435" num="0434">Finally, a cross-channel, subpixel-intensity accumulator <b>2010</b> combines the per-image cluster intensity for each of the upsampled images to determine the cluster intensity <b>2012</b> of the given cluster at the current sequencing cycle.</p><p id="p-0436" num="0435">Then, the given cluster is base called based on the cluster intensity <b>2012</b> at the current sequencing cycle by any one of the base callers discussed in this application, yielding base calls <b>2016</b>.</p><p id="p-0437" num="0436">Types of Neural Network-Based Template Generators</p><p id="p-0438" num="0437">The discussion now turns to details of three different implementations of the neural network-based template generator <b>1512</b>. There are shown in <figref idref="DRAWINGS">FIG. <b>21</b><i>a </i></figref>and include: (1) the decay map-based template generator <b>2600</b> (also called the regression model), (2) the binary map-based template generator <b>4600</b> (also called the binary classification model), and (3) the ternary map-based template generator <b>5400</b> (also called the ternary classification model).</p><p id="p-0439" num="0438">In one implementation, the regression model <b>2600</b> is a fully convolutional network. In another implementation, the regression model <b>2600</b> is a U-Net network with skip connections between the decoder and the encoder. In one implementation, the binary classification model <b>4600</b> is a fully convolutional network. In another implementation, the binary classification model <b>4600</b> is a U-Net network with skip connections between the decoder and the encoder. In one implementation, the ternary classification model <b>5400</b> is a fully convolutional network. In another implementation, the ternary classification model <b>5400</b> is a U-Net network with skip connections between the decoder and the encoder.</p><p id="p-0440" num="0439">Input Image Data</p><p id="p-0441" num="0440"><figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>depicts one implementation of the input image data <b>1702</b> that is fed as input to the neural network-based template generator <b>1512</b>. The input image data <b>1702</b> comprises a series of image sets <b>2100</b> with the sequencing images <b>108</b> that are generated during a certain number of initial sequences cycles of a sequencing run (e.g., the first 2 to 7 sequencing cycles).</p><p id="p-0442" num="0441">In some implementations, intensities of the sequencing images <b>108</b> are corrected for background and/or aligned with each other using affine transformation. In one implementation, the sequencing run utilizes four-channel chemistry and each image set has four images. In another implementation, the sequencing run utilizes two-channel chemistry and each image set has two images. In yet another implementation, the sequencing run utilizes one-channel chemistry and each image set has two images. In yet other implementations, each image set has only one image. These and other different implementations are described in Appendices 6 and 9.</p><p id="p-0443" num="0442">Each image <b>2116</b> in the series of image sets <b>2100</b> covers a tile <b>2104</b> of a flow cell <b>2102</b> and depicts intensity emissions of clusters <b>2106</b> on the tile <b>2104</b> and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of the sequencing run. In one example, for cycle t1, the image set includes four images <b>2112</b>A, <b>2112</b>C, <b>2112</b>T, and <b>2112</b>G: one image for each base A, C, T, and G labeled with a corresponding fluorescent dye and imaged in a corresponding wavelength band (image/imaging channel).</p><p id="p-0444" num="0443">For illustration purposes, in image <b>2112</b>G, <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>depicts cluster intensity emissions as <b>2108</b> and background intensity emissions as <b>2110</b>. In another example, for cycle tn, the image set also includes four images <b>2114</b>A, <b>2114</b>C, <b>2114</b>T, and <b>2114</b>G: one image for each base A, C, T, and G labeled with a corresponding fluorescent dye and imaged in a corresponding wavelength band (image/imaging channel). Also for illustration purposes, in image <b>2114</b>A, <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>depicts cluster intensity emissions as <b>2118</b> and, in image <b>2114</b>T, depicts background intensity emissions as <b>2120</b>.</p><p id="p-0445" num="0444">Non-Image Data</p><p id="p-0446" num="0445">The input image data <b>1702</b> is encoded using intensity channels (also called imaged channels). For each of the c images obtained from the sequencer for a particular sequencing cycle, a separate imaged channel is used to encode its intensity signal data. Consider, for example, that the sequencing run uses the 2-channel chemistry which produces a red image and a green image at each sequencing cycle. In such a case, the input data <b>2632</b> comprises (i) a first red imaged channel with w&#xd7;h pixels that depict intensity emissions of the one or more clusters and their surrounding background captured in the red image and (ii) a second green imaged channel with w&#xd7;h pixels that depict intensity emissions of the one or more clusters and their surrounding background captured in the green image.</p><p id="p-0447" num="0446">In another implementation, image data is not used as input to the neural network-based template generator <b>1512</b> or the neural network-based base caller <b>1514</b>. Instead, the input to the neural network-based template generator <b>1512</b> and the neural network-based base caller <b>1514</b> is based on pH changes induced by the release of hydrogen ions during molecule extension. The pH changes are detected and converted to a voltage change that is proportional to the number of bases incorporated (e.g., in the case of Ion Torrent).</p><p id="p-0448" num="0447">In yet another implementation, the input to the neural network-based template generator <b>1512</b> and the neural network-based base caller <b>1514</b> is constructed from nanopore sensing that uses biosensors to measure the disruption in current as an analyte passes through a nanopore or near its aperture while determining the identity of the base. For example, the Oxford Nanopore Technologies (ONT) sequencing is based on the following concept: pass a single strand of DNA (or RNA) through a membrane via a nanopore and apply a voltage difference across the membrane. The nucleotides present in the pore will affect the pore's electrical resistance, so current measurements over time can indicate the sequence of DNA bases passing through the pore. This electrical current signal (the &#x2018;squiggle&#x2019; due to its appearance when plotted) is the raw data gathered by an ONT sequencer. These measurements are stored as 16-bit integer data acquisition (DAC) values, taken at 4 kHz frequency (for example). With a DNA strand velocity of &#x2dc;450 base pairs per second, this gives approximately nine raw observations per base on average. This signal is then processed to identify breaks in the open pore signal corresponding to individual reads. These stretches of raw signal are base called&#x2014;the process of converting DAC values into a sequence of DNA bases. In some implementations, the input data <b>2632</b> comprises normalized or scaled DAC values.</p><p id="p-0449" num="0448">Patch Extraction</p><p id="p-0450" num="0449"><figref idref="DRAWINGS">FIG. <b>22</b></figref> shows one implementation of extracting patches from the series of image sets <b>2100</b> in <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>to produce a series of &#x201c;down-sized&#x201d; image sets that form the input image data <b>1702</b>. In the illustrated implementation, the sequencing images <b>108</b> in the series of image sets <b>2100</b> are of size L&#xd7;L (e.g., 2000&#xd7;2000). In other implementations, L is any number ranging from 1 and 10,000.</p><p id="p-0451" num="0450">In one implementation, a patch extractor <b>2202</b> extracts patches from the sequencing images <b>108</b> in the series of image sets <b>2100</b> and produces a series of down-sized image sets <b>2206</b>, <b>2208</b>, <b>2210</b>, and <b>2212</b>. Each image in the series of down-sized image sets is a patch of size M&#xd7;M (e.g., 20&#xd7;20) that is extracted from a corresponding sequencing image in the series of image sets <b>2100</b>. The size of the patches can be preset. In other implementations, M is any number ranging from 1 and 1000.</p><p id="p-0452" num="0451">In <figref idref="DRAWINGS">FIG. <b>22</b></figref>, four example series of down-sized image sets are shown. The first example series of down-sized image sets <b>2206</b> is extracted from coordinates 0,0 to 20,20 in the sequencing images <b>108</b> in the series of image sets <b>2100</b>. The second example series of down-sized image sets <b>2208</b> is extracted from coordinates 20,20 to 40,40 in the sequencing images <b>108</b> in the series of image sets <b>2100</b>. The third example series of down-sized image sets <b>2210</b> is extracted from coordinates 40,40 to 60,60 in the sequencing images <b>108</b> in the series of image sets <b>2100</b>. The fourth example series of down-sized image sets <b>2212</b> is extracted from coordinates 60,60 to 80,80 in the sequencing images <b>108</b> in the series of image sets <b>2100</b>.</p><p id="p-0453" num="0452">In some implementations, the series of down-sized image sets form the input image data <b>1702</b> that is fed as input to the neural network-based template generator <b>1512</b>. Multiple series of down-sized image sets can be simultaneously fed as an input batch and a separate output can be produced for each series in the input batch.</p><p id="p-0454" num="0453">Upsampling</p><p id="p-0455" num="0454"><figref idref="DRAWINGS">FIG. <b>23</b></figref> depicts one implementation of upsampling the series of image sets <b>2100</b> in <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>to produce a series of &#x201c;upsampled&#x201d; image sets <b>2300</b> that forms the input image data <b>1702</b>.</p><p id="p-0456" num="0455">In one implementation, an upsampler <b>2302</b> uses interpolation (e.g., bicubic interpolation) to upsample the sequencing images <b>108</b> in the series of image sets <b>2100</b> by an upsampling factor (e.g., 4&#xd7;) and the series of upsampled image sets <b>2300</b>.</p><p id="p-0457" num="0456">In the illustrated implementation, the sequencing images <b>108</b> in the series of image sets <b>2100</b> are of size L&#xd7;L (e.g., 2000&#xd7;2000) and are upsampled by an upsampling factor of four to produce upsampled images of size U&#xd7;U (e.g., 8000&#xd7;8000) in the series of upsampled image sets <b>2300</b>.</p><p id="p-0458" num="0457">In one implementation, the sequencing images <b>108</b> in the series of image sets <b>2100</b> are fed directly to the neural network-based template generator <b>1512</b> and the upsampling is performed by an initial layer of the neural network-based template generator <b>1512</b>. That is, the upsampler <b>2302</b> is part of the neural network-based template generator <b>1512</b> and operates as its first layer that upsamples the sequencing images <b>108</b> in the series of image sets <b>2100</b> and produces the series of upsampled image sets <b>2300</b>.</p><p id="p-0459" num="0458">In some implementations, the series of upsampled image sets <b>2300</b> forms the input image data <b>1702</b> that is fed as input to the neural network-based template generator <b>1512</b>.</p><p id="p-0460" num="0459"><figref idref="DRAWINGS">FIG. <b>24</b></figref> shows one implementation of extracting patches from the series of upsampled image sets <b>2300</b> in <figref idref="DRAWINGS">FIG. <b>23</b></figref> to produce a series of &#x201c;upsampled and down-sized&#x201d; image sets <b>2406</b>, <b>2408</b>, <b>2410</b>, and <b>2412</b> that form the input image data <b>1702</b>.</p><p id="p-0461" num="0460">In one implementation, the patch extractor <b>2202</b> extracts patches from the upsampled images in the series of upsampled image sets <b>2300</b> and produces series of upsampled and down-sized image sets <b>2406</b>, <b>2408</b>, <b>2410</b>, and <b>2412</b>. Each upsampled image in the series of upsampled and down-sized image sets is a patch of size M&#xd7;M (e.g., 80&#xd7;80) that is extracted from a corresponding upsampled image in the series of upsampled image sets <b>2300</b>. The size of the patches can be preset. In other implementations, M is any number ranging from 1 and 1000.</p><p id="p-0462" num="0461">In <figref idref="DRAWINGS">FIG. <b>24</b></figref>, four example series of upsampled and down-sized image sets are shown. The first example series of upsampled and down-sized image sets <b>2406</b> is extracted from coordinates 0,0 to 80,80 in the upsampled images in the series of upsampled image sets <b>2300</b>. The second example series of upsampled and down-sized image sets <b>2408</b> is extracted from coordinates 80,80 to 160,160 in the upsampled images in the series of upsampled image sets <b>2300</b>. The third example series of upsampled and down-sized image sets <b>2410</b> is extracted from coordinates 160,160 to 240,240 in the upsampled images in the series of upsampled image sets <b>2300</b>. The fourth example series of upsampled and down-sized image sets <b>2412</b> is extracted from coordinates 240,240 to 320,320 in the upsampled images in the series of upsampled image sets <b>2300</b>.</p><p id="p-0463" num="0462">In some implementations, the series of upsampled and down-sized image sets form the input image data <b>1702</b> that is fed as input to the neural network-based template generator <b>1512</b>. Multiple series of upsampled and down-sized image sets can be simultaneously fed as an input batch and a separate output can be produced for each series in the input batch.</p><p id="p-0464" num="0463">Output</p><p id="p-0465" num="0464">The three models are trained to produce different outputs. This is achieved by using different types of ground truth data representations as training labels. The regression model <b>2600</b> is trained to produce output that characterizes/represents/denotes a so-called &#x201c;decay map&#x201d; <b>1716</b>. The binary classification model <b>4600</b> is trained to produce output that characterizes/represents/denotes a so-called &#x201c;binary map&#x201d; <b>1720</b>. The ternary classification model <b>5400</b> is trained to produce output that characterizes/represents/denotes a so-called &#x201c;ternary map&#x201d; <b>1718</b>.</p><p id="p-0466" num="0465">The output <b>1714</b> of each type of model comprises an array of units <b>1712</b>. The units <b>1712</b> can be pixels, subpixels, or superpixels. The output of each type of model includes unit-wise output values, such that the output values of an array of units together characterize/represent/denote the decay map <b>1716</b> in the case of the regression model <b>2600</b>, the binary map <b>1720</b> in the case of the binary classification model <b>4600</b>, and the ternary map <b>1718</b> in the case of the ternary classification model <b>5400</b>. More details follow.</p><p id="p-0467" num="0466">Ground Truth Data Generation</p><p id="p-0468" num="0467"><figref idref="DRAWINGS">FIG. <b>25</b></figref> illustrates one implementation of an overall example process of generating ground truth data for training the neural network-based template generator <b>1512</b>. For the regression model <b>2600</b>, the ground truth data can be the decay map <b>1204</b>. For the binary classification model <b>4600</b>, the ground truth data can be the binary map <b>1404</b>. For the ternary classification model <b>5400</b>, the ground truth data can be the ternary map <b>1304</b>. The ground truth data is generated from the cluster metadata. The cluster metadata is generated by the cluster metadata generator <b>122</b>. The ground truth data is generated by the ground truth data generator <b>1506</b>.</p><p id="p-0469" num="0468">In the illustrated implementation, the ground truth data is generated for tile A that is on lane A of flow cell A. The ground truth data is generated from the sequencing images <b>108</b> of tile A captured during sequencing run A. The sequencing images <b>108</b> of tile A are in the pixel domain. In one example involving 4-channel chemistry that generates four sequencing images per sequencing cycle, two hundred sequencing images <b>108</b> for fifty sequencing cycles are accessed. Each of the two hundred sequencing images <b>108</b> depicts intensity emissions of clusters on tile A and their surrounding background captured in a particular image channel at a particular sequencing cycle.</p><p id="p-0470" num="0469">The subpixel addresser <b>110</b> converts the sequencing images <b>108</b> into the subpixel domain (e.g., by dividing each pixel into a plurality of subpixels) and produces sequencing images <b>112</b> in the subpixel domain.</p><p id="p-0471" num="0470">The base caller <b>114</b> (e.g., RTA) then processes the sequencing images <b>112</b> in the subpixel domain and produces a base call for each subpixel and for each of the fifty sequencing cycles. This is referred to herein as &#x201c;subpixel base calling&#x201d;.</p><p id="p-0472" num="0471">The subpixel base calls <b>116</b> are then merged to produce, for each subpixel, a base call sequence across the fifty sequencing cycles. Each subpixel's base call sequence has fifty base calls, i.e., one base call for each of the fifty sequencing cycles.</p><p id="p-0473" num="0472">The searcher <b>118</b> evaluates base call sequences of contiguous subpixels on a pair-wise basis. The search involves evaluating each subpixel to determine with which of its contiguous subpixels it shares a substantially matching base call sequence. Base call sequences of contiguous subpixels are &#x201c;substantially matching&#x201d; when a predetermined portion of base calls match on an ordinal position-wise basis (e.g., &#x3e;=41 matches in 45 cycles, &#x3c;=4 mismatches in 45 cycles, &#x3c;=4 mismatches in 50 cycles, or &#x3c;=2 mismatches in 34 cycles).</p><p id="p-0474" num="0473">In some implementations, the base caller <b>114</b> also identifies preliminary center coordinates of the clusters. Subpixels that contain the preliminary center coordinates are referred to as center or origin subpixels. Some example preliminary center coordinates (<b>604</b><i>a</i>-<i>c</i>) identified by the base caller <b>114</b> and corresponding origin subpixels (<b>606</b><i>a</i>-<i>c</i>) are shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>. However, identification of the origin subpixels (preliminary center coordinates of the clusters) is not needed, as explained below. In some implementations, the searcher <b>118</b> uses a breadth-first search for identifying substantially matching base call sequences of the subpixels by beginning with the origin subpixels <b>606</b><i>a</i>-<i>c </i>and continuing with successively contiguous non-origin subpixels <b>702</b><i>a</i>-<i>c</i>. This again is optional, as explained below.</p><p id="p-0475" num="0474">The search for substantially matching base call sequences of the subpixels does not need identification of the origin subpixels (preliminary center coordinates of the clusters) because the search can be done for all the subpixels and the search does not have to start from the origin subpixels and instead can start from any subpixel (e.g., <b>0</b>,<b>0</b> subpixel or any random subpixel). Thus, since each subpixel is evaluated to determine whether it shares a substantially matching base call sequence with another contiguous subpixel, the search does not have to utilize the origin subpixels and can start with any subpixel.</p><p id="p-0476" num="0475">Irrespective of whether origin subpixels are used or not, certain clusters are identified that do not contain the origin subpixels (preliminary center coordinates of the clusters) predicted by the base caller <b>114</b>. Some examples of clusters identified by the merging of the subpixel base calls and not containing an origin subpixel are clusters <b>812</b><i>a</i>, <b>812</b><i>b</i>, <b>812</b><i>c</i>, <b>812</b><i>d</i>, and <b>812</b><i>e </i>in <figref idref="DRAWINGS">FIG. <b>8</b><i>a</i></figref>. Therefore, use of the base caller <b>114</b> for identification of origin subpixels (preliminary center coordinates of the clusters) is optional and not essential for the search of substantially matching base call sequences of the subpixels.</p><p id="p-0477" num="0476">The searcher <b>118</b>: (1) identifies contiguous subpixels with substantially matching base call sequences as so-called &#x201c;disjointed regions&#x201d;, (2) further evaluates base call sequences of those subpixels that do not belong to any of the disjointed regions already identified at (1) to yield additional disjointed regions, and (3) then identifies background subpixels as those subpixels that do not belong to any of the disjointed regions already identified at (1) and (2). Action (2) allows the technology disclosed to identify additional or extra clusters for which the centers are not identified by the base caller <b>114</b>.</p><p id="p-0478" num="0477">The results of the searcher <b>118</b> are encoded in a so-called &#x201c;cluster map&#x201d; of tile A and stored in the cluster map data store <b>120</b>. In the cluster map, each of the clusters on tile A are identified by a respective disjointed region of contiguous subpixels, with background subpixels separating the disjointed regions to identify the surrounding background on tile A.</p><p id="p-0479" num="0478">The center of mass (COM) calculator <b>1004</b> determines a center for each of the clusters on tile A by calculating a COM of each of the disjointed regions as an average of coordinates of respective contiguous subpixels forming the disjointed regions. The centers of mass of the clusters are stored as COM data <b>2502</b>.</p><p id="p-0480" num="0479">A subpixel categorizer <b>2504</b> uses the cluster map and the COM data <b>2502</b> to produce subpixel categorizations <b>2506</b>. The subpixel categorizations <b>2506</b> classify subpixels in the cluster map as (1) backgrounds subpixels, (2) COM subpixels (one COM subpixel for each disjointed region containing the COM of the respective disjointed region), and (3) cluster/cluster interior subpixels forming the respective disjointed regions. That is, each subpixel in the cluster map is assigned one of the three categories.</p><p id="p-0481" num="0480">Based on the subpixel categorizations <b>2506</b>, in some implementations, (i) the ground truth decay map <b>1204</b> is produced by the ground truth decay map generator <b>1202</b>, (ii) the ground truth binary map <b>1304</b> is produced by the ground truth binary map generator <b>1302</b>, and (iii) the ground truth ternary map <b>1404</b> is produced by the ground truth ternary map generator <b>1402</b>.</p><p id="p-0482" num="0481">1. Regression Model</p><p id="p-0483" num="0482"><figref idref="DRAWINGS">FIG. <b>26</b></figref> illustrates one implementation of the regression model <b>2600</b>. In the illustrated implementation, the regression model <b>2600</b> is a fully convolutional network <b>2602</b> that processes the input image data <b>1702</b> through an encoder subnetwork and a corresponding decoder subnetwork. The encoder subnetwork includes a hierarchy of encoders. The decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to a full input resolution decay map <b>1716</b>. In another implementation, the regression model <b>2600</b> is a U-Net network <b>2604</b> with skip connections between the decoder and the encoder. Additional details about the segmentation networks can be found in the Appendix entitled &#x201c;Segmentation Networks&#x201d;.</p><p id="p-0484" num="0483">Decay Map</p><p id="p-0485" num="0484"><figref idref="DRAWINGS">FIG. <b>27</b></figref> depicts one implementation of generating a ground truth decay map <b>1204</b> from a cluster map <b>2702</b>. The ground truth decay map <b>1204</b> is used as ground truth data for training the regression model <b>2600</b>. In the ground truth decay map <b>1204</b>, the ground truth decay map generator <b>1202</b> assigns a weighted decay value to each contiguous subpixel in the disjointed regions based on a weighted decay factor. The weighted decay value is proportional to Euclidean distance of a contiguous subpixel from a center of mass (COM) subpixel in a disjointed region to which the contiguous subpixel belongs, such that the weighted decay value is highest (e.g., <b>1</b> or <b>100</b>) for the COM subpixel and decreases for subpixels further away from the COM subpixel. In some implementations, the weighted decay value is multiplied by a preset factor, such as 100.</p><p id="p-0486" num="0485">Further, the ground truth decay map generator <b>1202</b> assigns all background subpixels a same predetermine value (e.g., a minimalist background value).</p><p id="p-0487" num="0486">The ground truth decay map <b>1204</b> expresses the contiguous subpixels in the disjointed regions and the background subpixels based on the assigned values. The ground truth decay map <b>1204</b> also stores the assigned values in an array of units, with each unit in the array representing a corresponding subpixel in the input.</p><p id="p-0488" num="0487">Training</p><p id="p-0489" num="0488"><figref idref="DRAWINGS">FIG. <b>28</b></figref> is one implementation of training <b>2800</b> the regression model <b>2600</b> using a backpropagation-based gradient update technique that modifies parameters of the regression model <b>2600</b> until the decay map <b>1716</b> produced by the regression model <b>2600</b> as training output during the training <b>2800</b> progressively approaches or matches the ground truth decay map <b>1204</b>.</p><p id="p-0490" num="0489">The training <b>2800</b> includes iteratively optimizing a loss function that minimizes error <b>2807</b> between the decay map <b>1716</b> and the ground truth decay map <b>1204</b>, and updating parameters of the regression model <b>2600</b> based on the error <b>2807</b>. In one implementation, the loss function is mean squared error and the error is minimized on a subpixel-by-subpixel basis between weighted decay values of corresponding subpixels in the decay map <b>1716</b> and the ground truth decay map <b>1204</b>.</p><p id="p-0491" num="0490">The training <b>2800</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>2808</b> and backward propagation <b>2810</b>, including parallelization techniques such as batching. The training data <b>1504</b> includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The training data <b>1504</b> is annotated with ground truth labels by an annotator <b>2806</b>. The training <b>2800</b> is operationalized by the trainer <b>1510</b> using a stochastic gradient update algorithm such as ADAM.</p><p id="p-0492" num="0491">Inference</p><p id="p-0493" num="0492"><figref idref="DRAWINGS">FIG. <b>29</b></figref> is one implementation of template generation by the regression model <b>2600</b> during inference <b>2900</b> in which the decay map <b>1716</b> is produced by the regression model <b>2600</b> as the inference output during the inference <b>2900</b>. One example of the decay map <b>1716</b> is disclosed in the Appendix titled &#x201c;Regression_Model_Sample_Ouput&#x201d;. The Appendix includes unit-wise weighted decay output values <b>2910</b> that together represent the decay map <b>1716</b>.</p><p id="p-0494" num="0493">The inference <b>2900</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>2904</b>, including parallelization techniques such as batching. The inference <b>2900</b> is performed on inference data <b>2908</b> that includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The inference <b>2900</b> is operationalized by a tester <b>2906</b>.</p><p id="p-0495" num="0494">Watershed Segmentation</p><p id="p-0496" num="0495"><figref idref="DRAWINGS">FIG. <b>30</b></figref> illustrates one implementation of subjecting the decay map <b>1716</b> to (i) thresholding to identify background subpixels characterizing cluster background and to (ii) peak detection to identify center subpixels characterizing cluster centers. The thresholding is performed by the thresholder <b>1802</b> that uses a local threshold binary to produce binarized output. The peak detection is performed by the peak locator <b>1806</b> to identify the cluster centers. Additional details about the peak locator can be found in the Appendix entitled &#x201c;Peak Detection&#x201d;.</p><p id="p-0497" num="0496"><figref idref="DRAWINGS">FIG. <b>31</b></figref> depicts one implementation of a watershed segmentation technique that takes as input the background subpixels and the center subpixels respectively identified by the thresholder <b>1802</b> and the peak locator <b>1806</b>, finds valleys in intensity between adjoining clusters, and outputs non-overlapping groups of contiguous cluster/cluster interior subpixels characterizing the clusters. Additional details about the watershed segmentation technique can be found in the Appendix entitled &#x201c;Watershed Segmentation&#x201d;.</p><p id="p-0498" num="0497">In one implementation, a watershed segmenter <b>3102</b> takes as input (1) negativized output values <b>2910</b> in the decay map <b>1716</b>, (2) binarized output of the thresholder <b>1802</b>, and (3) cluster centers identified by the peak locator <b>1806</b>. Then, based on the input, the watershed segmenter <b>3102</b> produces output <b>3104</b>. In the output <b>3104</b>, each cluster center is identified as a unique set/group of subpixels that belong to the cluster center (as long as the subpixels are &#x201c;1&#x201d; in the binary output, i.e., not background subpixels). Further, the clusters are filtered based on containing at least four subpixels. The watershed segmenter <b>3102</b> can be part of the segmenter <b>1810</b>, which in turn is part of the post-processor <b>1814</b>.</p><p id="p-0499" num="0498">Network Architecture</p><p id="p-0500" num="0499"><figref idref="DRAWINGS">FIG. <b>32</b></figref> is a table that shows an example U-Net architecture of the regression model <b>2600</b>, along with details of the layers of the regression model <b>2600</b>, dimensionality of the output of the layers, magnitude of the model parameters, and interconnections between the layers. Similar details are disclosed in the file titled &#x201c;Regression_Model_Example_Architecture&#x201d;, which is submitted as an appendix to this application.</p><p id="p-0501" num="0500">Cluster Intensity Extraction</p><p id="p-0502" num="0501"><figref idref="DRAWINGS">FIG. <b>33</b></figref> illustrates different approaches of extracting cluster intensity using cluster shape information identified in a template image. As discussed above, the template image identifies the cluster shape information in the upsampled, subpixel resolution. However, the cluster intensity information is in the sequencing images <b>108</b>, which are typically in the optical, pixel-resolution.</p><p id="p-0503" num="0502">According to a first approach, coordinates of the subpixels are located in the sequencing images <b>108</b> and their respective intensities extracted using bilinear interpolation and normalized based on a count of the subpixels that contribute to a cluster.</p><p id="p-0504" num="0503">The second approach uses a weighted area coverage technique to modulate the intensity of a pixel according to a number of subpixels that contribute to the pixel. Here too, the modulated pixel intensity is normalized by a subpixel count parameter.</p><p id="p-0505" num="0504">The third approach upsamples the sequencing images into the subpixel domain using bicubic interpolation, sums the intensity of the upsampled pixels belonging to a cluster, and normalizes the summed intensity based on a count of the upsampled pixels that belong to the cluster.</p><heading id="h-0011" level="1">Experimental Results and Observations</heading><p id="p-0506" num="0505"><figref idref="DRAWINGS">FIG. <b>34</b></figref> shows different approaches of base calling using the outputs of the regression model <b>2600</b>. In the first approach, the cluster centers identified from the output of the neural network-based template generator <b>1512</b> in the template image are fed to a base caller (e.g., Illumina's Real-Time Analysis software, referred to herein as &#x201c;RTA base caller&#x201d;) for base calling.</p><p id="p-0507" num="0506">In the second approach, instead of the cluster centers, the cluster intensities extracted from the sequencing images based on the cluster shape information in the template image are fed to the RTA base caller for base calling.</p><p id="p-0508" num="0507"><figref idref="DRAWINGS">FIG. <b>35</b></figref> illustrates the difference in base calling performance when the RTA base caller uses ground truth center of mass (COM) location as the cluster center, as opposed to using a non-COM location as the cluster center. The results show that using COM improves base calling.</p><heading id="h-0012" level="1">Example Model Outputs</heading><p id="p-0509" num="0508"><figref idref="DRAWINGS">FIG. <b>36</b></figref> shows, on the left, an example decay map <b>1716</b> produced by the regression model <b>2600</b>. On the right, <figref idref="DRAWINGS">FIG. <b>36</b></figref> also shows an example ground truth decay map <b>1204</b> that the regression model <b>2600</b> approximates during the training.</p><p id="p-0510" num="0509">Both the decay map <b>1716</b> and the ground truth decay map <b>1204</b> depict clusters as disjointed regions of contiguous subpixels, the centers of the clusters as center subpixels at centers of mass of the respective ones of the disjointed regions, and their surrounding background as background subpixels not belonging to any of the disjointed regions.</p><p id="p-0511" num="0510">Also, the contiguous subpixels in the respective ones of the disjointed regions have values weighted according to distance of a contiguous subpixel from a center subpixel in a disjointed region to which the contiguous subpixel belongs. In one implementation, the center subpixels have the highest values within the respective ones of the disjointed regions. In one implementation, the background subpixels all have a same minimalist background value within a decay map.</p><p id="p-0512" num="0511"><figref idref="DRAWINGS">FIG. <b>37</b></figref> portrays one implementation of the peak locator <b>1806</b> identifying cluster centers in a decay map by detecting peaks <b>3702</b>. Additional details about the peak locator can be found in the Appendix entitled &#x201c;Peak Detection&#x201d;.</p><p id="p-0513" num="0512"><figref idref="DRAWINGS">FIG. <b>38</b></figref> compares peaks detected by the peak locator <b>1806</b> in the decay map <b>1716</b> produced by the regression model <b>2600</b> with peaks in a corresponding ground truth decay map <b>1204</b>. The red markers are peaks predicted by the regression model <b>2600</b> as cluster centers and the green markers are the ground truth centers of mass of the clusters.</p><heading id="h-0013" level="1">More Experimental Results and Observations</heading><p id="p-0514" num="0513"><figref idref="DRAWINGS">FIG. <b>39</b></figref> illustrates performance of the regression model <b>2600</b> using precision and recall statistics. The precision and recall statistics demonstrate that the regression model <b>2600</b> is good at recovering all identified cluster centers.</p><p id="p-0515" num="0514"><figref idref="DRAWINGS">FIG. <b>40</b></figref> compares performance of the regression model <b>2600</b> with the RTA base caller for 20 pM library concentration (normal run).</p><p id="p-0516" num="0515">Outperforming the RTA base caller, the regression model <b>2600</b> identifies 34,323 (4.46%) more clusters in a higher cluster density environment (i.e., 988,884 clusters).</p><p id="p-0517" num="0516"><figref idref="DRAWINGS">FIG. <b>40</b></figref> also shows results for other sequencing metrics such as number of clusters that pass the chastity filter (&#x201c;% PF&#x201d; (pass-filter)), number of aligned reads (&#x201c;% Aligned&#x201d;), number of duplicate reads (&#x201c;% Duplicate&#x201d;), number of reads mismatching the reference sequence for all reads aligned to the reference sequence (&#x201c;% Mismatch&#x201d;), bases called with quality score 30 and above (&#x201c;% Q30 bases&#x201d;), and so on.</p><p id="p-0518" num="0517"><figref idref="DRAWINGS">FIG. <b>41</b></figref> compares performance of the regression model <b>2600</b> with the RTA base caller for 30 pM library concentration (dense run). Outperforming the RTA base caller, the regression model <b>2600</b> identifies 34,323 (6.27%) more clusters in a much higher cluster density environment (i.e., 1,351,588 clusters).</p><p id="p-0519" num="0518"><figref idref="DRAWINGS">FIG. <b>41</b></figref> also shows results for other sequencing metrics such as number of clusters that pass the chastity filter (&#x201c;% PF&#x201d; (pass-filter)), number of aligned reads (&#x201c;% Aligned&#x201d;), number of duplicate reads (&#x201c;% Duplicate&#x201d;), number of reads mismatching the reference sequence for all reads aligned to the reference sequence (&#x201c;% Mismatch&#x201d;), bases called with quality score 30 and above (&#x201c;% Q30 bases&#x201d;), and so on.</p><p id="p-0520" num="0519"><figref idref="DRAWINGS">FIG. <b>42</b></figref> compares number of non-duplicate (unique or deduplicated) proper read pairs, i.e., the number of paired reads that have both reads aligned inwards within a reasonable distance detected by the regression model <b>2600</b> versus the same detected by the RTA base caller. The comparison is made both for the 20 pM normal run and the 30 pM dense run.</p><p id="p-0521" num="0520">More importantly, <figref idref="DRAWINGS">FIG. <b>42</b></figref> shows that the disclosed neural network-based template generators are able to detect more clusters in fewer sequencing cycles of input to template generation than the RTA base caller. In just four sequencing cycles, the regression model <b>2600</b> identifies 11% more non-duplicate proper read pairs than the RTA base caller during the 20 pM normal run and 33% more non-duplicate proper read pairs than the RTA base caller during the 30 pM dense run. In just seven sequencing cycles, the regression model <b>2600</b> identifies 4.5% more non-duplicate proper read pairs than the RTA base caller during the 20 pM normal run and 6.3% more non-duplicate proper read pairs than the RTA base caller during the 30 pM dense run.</p><p id="p-0522" num="0521"><figref idref="DRAWINGS">FIG. <b>43</b></figref> shows, on the right, a first decay map produced by the regression model <b>2600</b>. The first decay map identifies clusters and their surrounding background imaged during the 20 pM normal run, along with their spatial distribution depicting cluster shapes, cluster sizes, and cluster centers.</p><p id="p-0523" num="0522">On the left, <figref idref="DRAWINGS">FIG. <b>43</b></figref> shows a second decay map produced by the regression model <b>2600</b>. The second decay map identifies clusters and their surrounding background imaged during the 30 pM dense run, along with their spatial distribution depicting cluster shapes, cluster sizes, and cluster centers.</p><p id="p-0524" num="0523"><figref idref="DRAWINGS">FIG. <b>44</b></figref> compares performance of the regression model <b>2600</b> with the RTA base caller for 40 pM library concentration (highly dense run). The regression model <b>2600</b> produced 89,441,688 more aligned bases than the RTA base caller in a much higher cluster density environment (i.e., 1,509,395 clusters).</p><p id="p-0525" num="0524"><figref idref="DRAWINGS">FIG. <b>44</b></figref> also shows results for other sequencing metrics such as number of clusters that pass the chastity filter (&#x201c;% PF&#x201d; (pass-filter)), number of aligned reads (&#x201c;% Aligned&#x201d;), number of duplicate reads (&#x201c;% Duplicate&#x201d;), number of reads mismatching the reference sequence for all reads aligned to the reference sequence (&#x201c;% Mismatch&#x201d;), bases called with a quality score 30 and above (&#x201c;% Q30 bases&#x201d;), and so on.</p><heading id="h-0014" level="1">More Example Model Outputs</heading><p id="p-0526" num="0525"><figref idref="DRAWINGS">FIG. <b>45</b></figref> shows, on the left, a first decay map produced by the regression model <b>2600</b>. The first decay map identifies clusters and their surrounding background imaged during the 40 pM normal run, along with their spatial distribution depicting cluster shapes, cluster sizes, and cluster centers.</p><p id="p-0527" num="0526">On the right, <figref idref="DRAWINGS">FIG. <b>45</b></figref> shows the results of the thresholding and the peak locating applied to the first decay map to distinguish the respective clusters from each other and from the background and to identify their respective cluster centers. In some implementations, intensities of the respective clusters are identified and a chastity filter (or passing filter) applied to reduce the mismatch rate.</p><p id="p-0528" num="0527">2. Binary Classification Model</p><p id="p-0529" num="0528"><figref idref="DRAWINGS">FIG. <b>46</b></figref> illustrates one implementation of the binary classification model <b>4600</b>. In the illustrated implementation, the binary classification model <b>4600</b> is a deep fully convolutional segmentation neural network that processes the input image data <b>1702</b> through an encoder subnetwork and a corresponding decoder subnetwork. The encoder subnetwork includes a hierarchy of encoders. The decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to a full input resolution binary map <b>1720</b>. In another implementation, the binary classification model <b>4600</b> is a U-Net network with skip connections between the decoder and the encoder. Additional details about the segmentation networks can be found in the Appendix entitled &#x201c;Segmentation Networks&#x201d;.</p><p id="p-0530" num="0529">Binary Map</p><p id="p-0531" num="0530">The final output layer of the binary classification model <b>4600</b> is a unit-wise classification layer that produces a classification label for each unit in an output array. In some implementations, the unit-wise classification layer is a subpixel-wise classification layer that produces a softmax classification score distribution for each subpixel in the binary map <b>1720</b> across two classes, namely, a cluster center class and a non-cluster class, and the classification label for a given subpixel is determined from the corresponding softmax classification score distribution.</p><p id="p-0532" num="0531">In other implementations, the unit-wise classification layer is a subpixel-wise classification layer that produces a sigmoid classification score for each subpixel in the binary map <b>1720</b>, such that the activation of a unit is interpreted as the probability that the unit belongs to the first class and, conversely, one minus the activation gives the probability that it belongs to the second class.</p><p id="p-0533" num="0532">The binary map <b>1720</b> expresses each subpixel based on the predicted classification scores. The binary map <b>1720</b> also stores the predicted value classification scores in an array of units, with each unit in the array representing a corresponding subpixel in the input.</p><p id="p-0534" num="0533">Training</p><p id="p-0535" num="0534"><figref idref="DRAWINGS">FIG. <b>47</b></figref> is one implementation of training <b>4700</b> the binary classification model <b>4600</b> using a backpropagation-based gradient update technique that modifies parameters of the binary classification model <b>4600</b> until the binary map <b>1720</b> of the binary classification model <b>4600</b> progressively approaches or matches the ground truth binary map <b>1404</b>.</p><p id="p-0536" num="0535">In the illustrated implementation, the final output layer of the binary classification model <b>4600</b> is a softmax-based subpixel-wise classification layer. In softmax implementations, the ground truth binary map generator <b>1402</b> assigns each ground truth subpixel either (i) a cluster center value pair (e.g., [1, 0]) or (ii) a non-center value pair (e.g., [0, 1]).</p><p id="p-0537" num="0536">In the cluster center value pair [1, 0], a first value [1] represents the cluster center class label and a second value [0] represents the non-center class label. In the non-center value pair [0, 1], a first value [0] represents the cluster center class label and a second value [1] represents the non-center class label.</p><p id="p-0538" num="0537">The ground truth binary map <b>1404</b> expresses each subpixel based on the assigned value pair/value. The ground truth binary map <b>1404</b> also stores the assigned value pairs/values in an array of units, with each unit in the array representing a corresponding subpixel in the input.</p><p id="p-0539" num="0538">The training includes iteratively optimizing a loss function that minimizes error <b>4706</b> (e.g., softmax error) between the binary map <b>1720</b> and the ground truth binary map <b>1404</b>, and updating parameters of the binary classification model <b>4600</b> based on the error <b>4706</b>.</p><p id="p-0540" num="0539">In one implementation, the loss function is a custom-weighted binary cross-entropy loss and the error <b>4706</b> is minimized on a subpixel-by-subpixel basis between predicted classification scores (e.g., softmax scores) and labelled class scores (e.g., softmax scores) of corresponding subpixels in the binary map <b>1720</b> and the ground truth binary map <b>1404</b>, as shown in <figref idref="DRAWINGS">FIG. <b>47</b></figref>.</p><p id="p-0541" num="0540">The custom-weighted loss function gives more weight to the COM subpixels, such that the cross-entropy loss is multiplied by a corresponding reward (or penalty) weight specified in a reward (or penalty) matrix whenever a COM subpixel is misclassified. Additional details about the custom-weighted loss function can be found in the Appendix entitled &#x201c;Custom-Weighted Loss Function&#x201d;.</p><p id="p-0542" num="0541">The training <b>4700</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>4708</b> and backward propagation <b>4710</b>, including parallelization techniques such as batching. The training data <b>1504</b> includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The training data <b>1504</b> is annotated with ground truth labels by the annotator <b>2806</b>. The training <b>2800</b> is operationalized by the trainer <b>1510</b> using a stochastic gradient update algorithm such as ADAM.</p><p id="p-0543" num="0542"><figref idref="DRAWINGS">FIG. <b>48</b></figref> is another implementation of training <b>4800</b> the binary classification model <b>4600</b>, in which the final output layer of the binary classification model <b>4600</b> is a sigmoid-based subpixel-wise classification layer.</p><p id="p-0544" num="0543">In sigmoid implementations, the ground truth binary map generator <b>1302</b> assigns each ground truth subpixel either (i) a cluster center value (e.g., [1]) or (ii) a non-center value (e.g., [0]). The COM subpixels are assigned the cluster center value pair/value and all other subpixels are assigned the non-center value pair/value.</p><p id="p-0545" num="0544">With the cluster center value, values above a threshold intermediate value between 0 and 1 (e.g., values above 0.5) represent the center class label. With the non-center value, values below a threshold intermediate value between 0 and 1 (e.g., values below 0.5) represent the non-center class label.</p><p id="p-0546" num="0545">The ground truth binary map <b>1404</b> expresses each subpixel based on the assigned value pair/value. The ground truth binary map <b>1404</b> also stores the assigned value pairs/values in an array of units, with each unit in the array representing a corresponding subpixel in the input.</p><p id="p-0547" num="0546">The training includes iteratively optimizing a loss function that minimizes error <b>4806</b> (e.g., sigmoid error) between the binary map <b>1720</b> and the ground truth binary map <b>1404</b>, and updating parameters of the binary classification model <b>4600</b> based on the error <b>4806</b>.</p><p id="p-0548" num="0547">In one implementation, the loss function is a custom-weighted binary cross-entropy loss and the error <b>4806</b> is minimized on a subpixel-by-subpixel basis between predicted scores (e.g., sigmoid scores) and labelled scores (e.g., sigmoid scores) of corresponding subpixels in the binary map <b>1720</b> and the ground truth binary map <b>1404</b>, as shown in <figref idref="DRAWINGS">FIG. <b>48</b></figref>.</p><p id="p-0549" num="0548">The custom-weighted loss function gives more weight to the COM subpixels, such that the cross-entropy loss is multiplied by a corresponding reward (or penalty) weight specified in a reward (or penalty) matrix whenever a COM subpixel is misclassified. Additional details about the custom-weighted loss function can be found in the Appendix entitled &#x201c;Custom-Weighted Loss Function&#x201d;.</p><p id="p-0550" num="0549">The training <b>4800</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>4808</b> and backward propagation <b>4810</b>, including parallelization techniques such as batching. The training data <b>1504</b> includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The training data <b>1504</b> is annotated with ground truth labels by the annotator <b>2806</b>. The training <b>2800</b> is operationalized by the trainer <b>1510</b> using a stochastic gradient update algorithm such as ADAM.</p><p id="p-0551" num="0550"><figref idref="DRAWINGS">FIG. <b>49</b></figref> illustrates another implementation of the input image data <b>1702</b> fed to the binary classification model <b>4600</b> and the corresponding class labels <b>4904</b> used to train the binary classification model <b>4600</b>.</p><p id="p-0552" num="0551">In the illustrated implementation, the input image data <b>1702</b> comprises a series of upsampled and down-sized image sets <b>4902</b>. The class labels <b>4904</b> comprise two classes: (1) &#x201c;no cluster center&#x201d; and (2) &#x201c;cluster center&#x201d;, which are distinguished using different output values. That is, (1) the light green units/subpixels <b>4906</b> represent subpixels that are predicted by the binary classification model <b>4600</b> to not contain the cluster centers and (2) the dark green subpixels <b>4908</b> represent units/subpixels that are predicted by the binary classification model <b>4600</b> to contain the cluster centers.</p><p id="p-0553" num="0552">Inference</p><p id="p-0554" num="0553"><figref idref="DRAWINGS">FIG. <b>50</b></figref> is one implementation of template generation by the binary classification model <b>4600</b> during inference <b>5000</b> in which the binary map <b>1720</b> is produced by the binary classification model <b>4600</b> as the inference output during the inference <b>5000</b>. One example of the binary map <b>1720</b> includes unit-wise binary classification scores <b>5010</b> that together represent the binary map <b>1720</b>. In the softmax applications, the binary map <b>1720</b> has a first array <b>5002</b><i>a </i>of unit-wise classification scores for the non-center class and a second array <b>5002</b><i>b </i>of unit-wise classification scores for the cluster center class.</p><p id="p-0555" num="0554">The inference <b>5000</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>5004</b>, including parallelization techniques such as batching. The inference <b>5000</b> is performed on inference data <b>2908</b> that includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The inference <b>5000</b> is operationalized by the tester <b>2906</b>.</p><p id="p-0556" num="0555">In some implementations, the binary map <b>1720</b> is subjected to post-processing techniques discussed above, such as thresholding, peak detection, and/or watershed segmentation to generate cluster metadata.</p><p id="p-0557" num="0556">Peak Detection</p><p id="p-0558" num="0557"><figref idref="DRAWINGS">FIG. <b>51</b></figref> depicts one implementation of subjecting the binary map <b>1720</b> to peak detection to identify cluster centers. As discussed above, the binary map <b>1720</b> is an array of units that classifies each subpixel based on the predicted classification scores, with each unit in the array representing a corresponding subpixel in the input. The classification scores can be softmax scores or sigmoid scores.</p><p id="p-0559" num="0558">In the softmax applications, the binary map <b>1720</b> includes two arrays: (1) a first array <b>5002</b><i>a </i>of unit-wise classification scores for the non-center class and (2) a second array <b>5002</b><i>b </i>of unit-wise classification scores for the cluster center class. In both the arrays, each unit represents a corresponding subpixel in the input.</p><p id="p-0560" num="0559">To determine which subpixels in the input contain the cluster centers and which do not contain the cluster centers, the peak locator <b>1806</b> applies peak detection on the units in the binary map <b>1720</b>. The peak detection identifies those units that have classification scores (e.g., softmax/sigmoid scores) above a preset threshold. The identified units are inferred as the cluster centers and their corresponding subpixels in the input are determined to contain the cluster centers and stored as cluster center subpixels in a subpixel classifications data store <b>5102</b>. Additional details about the peak locator <b>1806</b> can be found in the Appendix entitled &#x201c;Peak Detection&#x201d;.</p><p id="p-0561" num="0560">The remaining units and their corresponding subpixels in the input are determined to not contain the cluster centers and stored as non-center subpixels in the subpixel classifications data store <b>5102</b>.</p><p id="p-0562" num="0561">In some implementations, prior to applying the peak detection, those units that have classification scores below a certain background threshold (e.g., <b>0</b>.<b>3</b>) are set to zero. In some implementations, such units and their corresponding subpixels in the input are inferred to denote the background surrounding the clusters and stored as background subpixels in the subpixel classifications data store <b>5102</b>. In other implementations, such units can be considered noise and ignored.</p><heading id="h-0015" level="1">Example Model Outputs</heading><p id="p-0563" num="0562"><figref idref="DRAWINGS">FIG. <b>52</b><i>a </i></figref>shows, on the left, an example binary map produced by the binary classification model <b>4600</b>. On the right, <figref idref="DRAWINGS">FIG. <b>52</b><i>a </i></figref>also shows an example ground truth binary map that the binary classification model <b>4600</b> approximates during the training. The binary map has a plurality of subpixels and classifies each subpixel as either a cluster center or a non-center. Similarly, the ground truth binary map has a plurality of subpixels and classifies each subpixel as either a cluster center or a non-center.</p><heading id="h-0016" level="1">Experimental Results and Observations</heading><p id="p-0564" num="0563"><figref idref="DRAWINGS">FIG. <b>52</b><i>b </i></figref>illustrates performance of the binary classification model <b>4600</b> using recall and precision statistics. Applying these statistics, the binary classification model <b>4600</b> outperforms the RTA base caller.</p><p id="p-0565" num="0564">Network Architecture</p><p id="p-0566" num="0565"><figref idref="DRAWINGS">FIG. <b>53</b></figref> is a table that shows an example architecture of the binary classification model <b>4600</b>, along with details of the layers of the binary classification model <b>4600</b>, dimensionality of the output of the layers, magnitude of the model parameters, and interconnections between the layers. Similar details are disclosed in the Appendix titled &#x201c;Binary_Classification_Model_Example_Architecture&#x201d;.</p><p id="p-0567" num="0566">3. Ternary (Three Class) Classification Model</p><p id="p-0568" num="0567"><figref idref="DRAWINGS">FIG. <b>54</b></figref> illustrates one implementation of the ternary classification model <b>5400</b>. In the illustrated implementation, the ternary classification model <b>5400</b> is a deep fully convolutional segmentation neural network that processes the input image data <b>1702</b> through an encoder subnetwork and a corresponding decoder subnetwork. The encoder subnetwork includes a hierarchy of encoders. The decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to a full input resolution ternary map <b>1718</b>. In another implementation, the ternary classification model <b>5400</b> is a U-Net network with skip connections between the decoder and the encoder. Additional details about the segmentation networks can be found in the Appendix entitled &#x201c;Segmentation Networks&#x201d;.</p><p id="p-0569" num="0568">Ternary Map</p><p id="p-0570" num="0569">The final output layer of the ternary classification model <b>5400</b> is a unit-wise classification layer that produces a classification label for each unit in an output array. In some implementations, the unit-wise classification layer is a subpixel-wise classification layer that produces a softmax classification score distribution for each subpixel in the ternary map <b>1718</b> across three classes, namely, a background class, a cluster center class, and a cluster/cluster interior class, and the classification label for a given subpixel is determined from the corresponding softmax classification score distribution.</p><p id="p-0571" num="0570">The ternary map <b>1718</b> expresses each subpixel based on the predicted classification scores. The ternary map <b>1718</b> also stores the predicted value classification scores in an array of units, with each unit in the array representing a corresponding subpixel in the input.</p><p id="p-0572" num="0571">Training</p><p id="p-0573" num="0572"><figref idref="DRAWINGS">FIG. <b>55</b></figref> is one implementation of training <b>5500</b> the ternary classification model <b>5400</b> using a backpropagation-based gradient update technique that modifies parameters of the ternary classification model <b>5400</b> until the ternary map <b>1718</b> of the ternary classification model <b>5400</b> progressively approaches or matches training ground truth ternary maps <b>1304</b>.</p><p id="p-0574" num="0573">In the illustrated implementation, the final output layer of the ternary classification model <b>5400</b> is a softmax-based subpixel-wise classification layer. In softmax implementations, the ground truth ternary map generator <b>1402</b> assigns each ground truth subpixel either (i) a background value triplet (e.g., [<b>1</b>, <b>0</b>, <b>0</b>]), (ii) a cluster center value triplet (e.g., [<b>0</b>, <b>1</b>, <b>0</b>]), or (iii) a cluster/cluster interior value triplet (e.g., [<b>0</b>, <b>0</b>, <b>1</b>]).</p><p id="p-0575" num="0574">The background subpixels are assigned the background value triplet. The center of mass (COM) subpixels are assigned the cluster center value triplet. The cluster/cluster interior subpixels are assigned the cluster/cluster interior value triplet.</p><p id="p-0576" num="0575">In the background value triplet [1, 0, 0], a first value [1] represents the background class label, a second value [0] represents the cluster center label, and a third value [0] represents the cluster/cluster interior class label.</p><p id="p-0577" num="0576">In the cluster center value triplet [0, 1, 0], a first value [0] represents the background class label, a second value [1] represents the cluster center label, and a third value [0] represents the cluster/cluster interior class label.</p><p id="p-0578" num="0577">In the cluster/cluster interior value triplet [0, 0, 1], a first value [0] represents the background class label, a second value [0] represents the cluster center label, and a third value [1] represents the cluster/cluster interior class label.</p><p id="p-0579" num="0578">The ground truth ternary map <b>1304</b> expresses each subpixel based on the assigned value triplet. The ground truth ternary map <b>1304</b> also stores the assigned triplets in an array of units, with each unit in the array representing a corresponding subpixel in the input.</p><p id="p-0580" num="0579">The training includes iteratively optimizing a loss function that minimizes error <b>5506</b> (e.g., softmax error) between the ternary map <b>1718</b> and the ground truth ternary map <b>1304</b>, and updating parameters of the ternary classification model <b>5400</b> based on the error <b>5506</b>.</p><p id="p-0581" num="0580">In one implementation, the loss function is a custom-weighted categorical cross-entropy loss and the error <b>5506</b> is minimized on a subpixel-by-subpixel basis between predicted classification scores (e.g., softmax scores) and labelled class scores (e.g., softmax scores) of corresponding subpixels in the ternary map <b>1718</b> and the ground truth ternary map <b>1304</b>, as shown in <figref idref="DRAWINGS">FIG. <b>54</b></figref>.</p><p id="p-0582" num="0581">The custom-weighted loss function gives more weight to the COM subpixels, such that the cross-entropy loss is multiplied by a corresponding reward (or penalty) weight specified in a reward (or penalty) matrix whenever a COM subpixel is misclassified. Additional details about the custom-weighted loss function can be found in the Appendix entitled &#x201c;Custom-Weighted Loss Function&#x201d;.</p><p id="p-0583" num="0582">The training <b>5500</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>5508</b> and backward propagation <b>5510</b>, including parallelization techniques such as batching. The training data <b>1504</b> includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The training data <b>1504</b> is annotated with ground truth labels by the annotator <b>2806</b>. The training <b>5500</b> is operationalized by the trainer <b>1510</b> using a stochastic gradient update algorithm such as ADAM.</p><p id="p-0584" num="0583"><figref idref="DRAWINGS">FIG. <b>56</b></figref> illustrates one implementation of input image data <b>1702</b> fed to the ternary classification model <b>5400</b> and the corresponding class labels used to train the ternary classification model <b>5400</b>.</p><p id="p-0585" num="0584">In the illustrated implementation, the input image data <b>1702</b> comprises a series of upsampled and down-sized image sets <b>5602</b>. The class labels <b>5604</b> comprise three classes: (1) &#x201c;background class&#x201d;, (2) &#x201c;cluster center class&#x201d;, and (3) &#x201c;cluster interior class&#x201d;, which are distinguished using different output values. For example, some of these different output values can be visually represented as follows: (1) the grey units/subpixels <b>5606</b> represent subpixels that are predicted by the ternary classification model <b>5400</b> to be the background, (2) the dark green units/subpixels <b>5608</b> represent subpixels that are predicted by the ternary classification model <b>5400</b> to contain the cluster centers, and (3) the light green subpixels <b>5610</b> represent subpixels that are predicted by the ternary classification model <b>5400</b> to contain the interior of the clusters.</p><p id="p-0586" num="0585">Network Architecture</p><p id="p-0587" num="0586"><figref idref="DRAWINGS">FIG. <b>57</b></figref> is a table that shows an example architecture of the ternary classification model <b>5400</b>, along with details of the layers of the ternary classification model <b>5400</b>, dimensionality of the output of the layers, magnitude of the model parameters, and interconnections between the layers. Similar details are disclosed in the Appendix titled &#x201c;Ternary_Classification_Model_Example_Architecture&#x201d;.</p><p id="p-0588" num="0587">Inference</p><p id="p-0589" num="0588"><figref idref="DRAWINGS">FIG. <b>58</b></figref> is one implementation of template generation by the ternary classification model <b>5400</b> during inference <b>5800</b> in which the ternary map <b>1718</b> is produced by the ternary classification model <b>5400</b> as the inference output during the inference <b>5800</b>. One example of the ternary map <b>1718</b> is disclosed in the Appendix titled &#x201c;Ternary_Classification_Model_Sample_Ouput&#x201d;. The Appendix includes unit-wise binary classification scores <b>5810</b> that together represent the ternary map <b>1718</b>. In the softmax applications, the Appendix has a first array <b>5802</b><i>a </i>of unit-wise classification scores for the background class, a second array <b>5802</b><i>b </i>of unit-wise classification scores for the cluster center class, and a third array <b>5802</b><i>c </i>of unit-wise classification scores for the cluster/cluster interior class.</p><p id="p-0590" num="0589">The inference <b>5800</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>5804</b>, including parallelization techniques such as batching. The inference <b>5800</b> is performed on inference data <b>2908</b> that includes, as the input image data <b>1702</b>, a series of upsampled and down-sized image sets. The inference <b>5000</b> is operationalized by the tester <b>2906</b>.</p><p id="p-0591" num="0590">In some implementations, the ternary map <b>1718</b> is produced by the ternary classification model <b>5400</b> using post-processing techniques discussed above, such as thresholding, peak detection, and/or watershed segmentation.</p><p id="p-0592" num="0591"><figref idref="DRAWINGS">FIG. <b>59</b></figref> graphically portrays the ternary map <b>1718</b> produced by the ternary classification model <b>5400</b> in which each subpixel has a three-way softmax classification score distribution for the three corresponding classes, namely, the background class <b>5906</b>, the cluster center class <b>5902</b>, and the cluster/cluster interior class <b>5904</b>.</p><p id="p-0593" num="0592"><figref idref="DRAWINGS">FIG. <b>60</b></figref> depicts an array of units produced by the ternary classification model <b>5400</b>, along with the unit-wise output values. As depicted, each unit has three output values for the three corresponding classes, namely, the background class <b>5906</b>, the cluster center class <b>5902</b>, and the cluster/cluster interior class <b>5904</b>. For each classification (column-wise), each unit is assigned the class that has the highest output value, as indicated by the class in parenthesis under each unit. In some implementations, the output values <b>6002</b>, <b>6004</b>, and <b>6006</b> are analyzed for each of the respective classes <b>5906</b>, <b>5902</b>, and <b>5904</b> (row-wise).</p><p id="p-0594" num="0593">Peak Detection &#x26; Watershed Segmentation</p><p id="p-0595" num="0594"><figref idref="DRAWINGS">FIG. <b>61</b></figref> shows one implementation of subjecting the ternary map <b>1718</b> to post-processing to identify cluster centers, cluster background, and cluster interior. As discussed above, the ternary map <b>1718</b> is an array of units that classifies each subpixel based on the predicted classification scores, with each unit in the array representing a corresponding subpixel in the input. The classification scores can be softmax scores.</p><p id="p-0596" num="0595">In the softmax applications, the ternary map <b>1718</b> includes three arrays: (1) a first array <b>5802</b><i>a </i>of unit-wise classification scores for the background class, (2) a second array <b>5802</b><i>b </i>of unit-wise classification scores for the cluster center class, and (3) a third array <b>5802</b><i>c </i>of unit-wise classification scores for the cluster interior class. In all three arrays, each unit represents a corresponding subpixel in the input.</p><p id="p-0597" num="0596">To determine which subpixels in the input contain the cluster centers, which contain the interior of the clusters, and which contain the background, the peak locator <b>1806</b> applies peak detection on softmax values in the ternary map <b>1718</b> for the cluster center class <b>5802</b><i>b</i>. The peak detection identifies those units that have classification scores (e.g., softmax scores) above a preset threshold. The identified units are inferred as the cluster centers and their corresponding subpixels in the input are determined to contain the cluster centers and stored as cluster center subpixels in a subpixel classifications and segmentations data store <b>6102</b>. Additional details about the peak locator <b>1806</b> can be found in the Appendix entitled &#x201c;Peak Detection&#x201d;.</p><p id="p-0598" num="0597">In some implementations, prior to applying the peak detection, those units that have classification scores below a certain noise threshold (e.g., <b>0</b>.<b>3</b>) are set to zero. Such units can be considered noise and ignored.</p><p id="p-0599" num="0598">Also, units that have classification scores for the background class <b>5802</b><i>a </i>above a certain background threshold (e.g., equal to or greater than 0.5) and their corresponding subpixels in the input are inferred to denote the background surrounding the clusters and stored as background subpixels in the subpixel classifications and segmentations data store <b>6102</b>.</p><p id="p-0600" num="0599">Then, the watershed segmentation algorithm, operationalized by the watershed segmenter <b>3102</b>, is used to determine the shapes of the clusters. In some implementations, the background units/subpixels are used as a mask by the watershed segmentation algorithm. Classification scores of the unit/subpixels inferred as the cluster centers and the cluster interior are summed to produce so-called &#x201c;cluster labels&#x201d;. The cluster centers are used as watershed markers, for separation by intensity valleys by the watershed segmentation algorithm.</p><p id="p-0601" num="0600">In one implementation, negativized cluster labels are provided as an input image to the watershed segmenter <b>3102</b> that performs segmentation and produces the cluster shapes as disjointed regions of contiguous cluster interior subpixels separated by the background subpixels. Furthermore, each disjointed region includes a corresponding cluster center subpixel. In some implementations, the corresponding cluster center subpixel is the center of the disjointed region to which it belongs. In other implementations, centers of mass (COM) of the disjointed regions are calculated based on the underlying location coordinates and stored as new centers of the clusters.</p><p id="p-0602" num="0601">The outputs of the watershed segmenter <b>3102</b> are stored in the subpixel classifications and segmentations data store <b>6102</b>. Additional details about the watershed segmentation algorithm and other segmentation algorithms can be found in Appendix entitled &#x201c;Watershed Segmentation&#x201d;.</p><p id="p-0603" num="0602">Example outputs of the peak locator <b>1806</b> and the watershed segmenter <b>3102</b> are shown in <figref idref="DRAWINGS">FIGS. <b>62</b><i>a</i>, <b>62</b><i>b</i></figref>, <b>63</b>, and <b>64</b>.</p><heading id="h-0017" level="1">Example Model Outputs</heading><p id="p-0604" num="0603"><figref idref="DRAWINGS">FIG. <b>62</b><i>a </i></figref>shows example predictions of the ternary classification model <b>5400</b>. <figref idref="DRAWINGS">FIG. <b>62</b><i>a </i></figref>shows four maps and each map has an array of units. The first map <b>6202</b> (left most) shows each unit's output values for the cluster center class <b>5802</b><i>b</i>. The second map <b>6204</b> shows each unit's output values for the cluster/cluster interior class <b>5802</b><i>c</i>. The third map <b>6206</b> (right most) shows each unit's output values for the background class <b>5802</b><i>a</i>. The fourth map <b>6208</b> (bottom) is a binary mask of ground truth ternary map <b>6008</b> that assigns each unit the class label that has the highest output value.</p><p id="p-0605" num="0604"><figref idref="DRAWINGS">FIG. <b>62</b><i>b </i></figref>illustrates other example predictions of the ternary classification model <b>5400</b>. <figref idref="DRAWINGS">FIG. <b>62</b><i>b </i></figref>shows four maps and each map has an array of units. The first map <b>6212</b> (bottom left most) shows each unit's output values for the cluster/cluster interior class. The second map <b>6214</b> shows each unit's output values for the cluster center class. The third map <b>6216</b> (bottom right most) shows each unit's output values for the background class. The fourth map (top) <b>6210</b> is the ground truth ternary map that assigns each unit the class label that has the highest output value.</p><p id="p-0606" num="0605"><figref idref="DRAWINGS">FIG. <b>62</b><i>c </i></figref>shows yet other example predictions of the ternary classification model <b>5400</b>. <figref idref="DRAWINGS">FIG. <b>64</b></figref> shows four maps and each map has an array of units. The first map <b>6220</b> (bottom left most) shows each unit's output values for the cluster/cluster interior class. The second map <b>6222</b> shows each unit's output values for the cluster center class. The third map <b>6224</b> (bottom right most) shows each unit's output values for the background class. The fourth map <b>6218</b> (<i>top</i>) is the ground truth ternary map that assigns each unit the class label that has the highest output value.</p><p id="p-0607" num="0606"><figref idref="DRAWINGS">FIG. <b>63</b></figref> depicts one implementation of deriving the cluster centers and cluster shapes from the output of the ternary classification model <b>5400</b> in <figref idref="DRAWINGS">FIG. <b>62</b><i>a </i></figref>by subjecting the output to post-processing. The post-processing (e.g., peak locating, watershed segmentation) generates cluster shape data and other metadata, which is identified in the cluster map <b>6310</b>.</p><heading id="h-0018" level="1">Experimental Results and Observations</heading><p id="p-0608" num="0607"><figref idref="DRAWINGS">FIG. <b>64</b></figref> compares performance of the binary classification model <b>4600</b>, the regression model <b>2600</b>, and the RTA base caller. The performance is evaluated using a variety of sequencing metrics. One metric is the total number of clusters detected (&#x201c;#clusters&#x201d;), which can be measured by the number of unique cluster centers that are detected. Another metric is the number of detected clusters that pass the chastity filter (&#x201c;% PF&#x201d; (pass-filter)). During cycles 1-25 of a sequencing run, the chastity filter removes the least reliable clusters from the image extraction results. Clusters &#x201c;pass filter&#x201d; if no more than one base call has a chastity value below 0.6 in the first 25 cycles. Chastity is defined as the ratio of the brightest base intensity divided by the sum of the brightest and the second brightest base intensities. This metric goes beyond the quantity of the detected clusters and also conveys their quality, i.e., how many of the detected clusters can be used for accurate base calling and downstream secondary and ternary analysis such as variant calling and variant pathogenicity annotation.</p><p id="p-0609" num="0608">Other metrics that measure how good the detected clusters are for downstream analysis include the number of aligned reads produced from the detected clusters (&#x201c;% Aligned&#x201d;), the number of duplicate reads produced from the detected clusters (&#x201c;% Duplicate&#x201d;), the number of reads produced from the detected clusters mismatching the reference sequence for all reads aligned to the reference sequence (&#x201c;% Mismatch&#x201d;), the number of reads produced from the detected clusters whose portions do not match well to the reference sequence on either side and thus are ignored for the alignment (&#x201c;% soft clipped&#x201d;), the number of bases called for the detected clusters with quality score 30 and above (&#x201c;% Q30 bases&#x201d;), the number of paired reads produced from the detected clusters that have both reads aligned inwards within a reasonable distance (&#x201c;total proper read pairs&#x201d;), and the number of unique or deduplicated proper read pairs produced from the detected clusters (&#x201c;non-duplicate proper read pairs&#x201d;).</p><p id="p-0610" num="0609">As shown in <figref idref="DRAWINGS">FIG. <b>64</b></figref>, both the binary classification model <b>4600</b> and the regression model <b>2600</b> outperform the RTA base caller at template generation on most of the metrics.</p><p id="p-0611" num="0610"><figref idref="DRAWINGS">FIG. <b>65</b></figref> compares the performance of the ternary classification model <b>5400</b> with that of the RTA base caller under three contexts, five sequencing metrics, and two run densities.</p><p id="p-0612" num="0611">In the first context called &#x201c;RTA&#x201d;, the cluster centers are detected by the RTA base caller, the intensity extraction from the clusters is done by the RTA base caller, and the clusters are also base called using the RTA base caller. In the second context called &#x201c;RTA IE&#x201d;, the cluster centers are detected by the ternary classification model <b>5400</b>; however, the intensity extraction from the clusters is done by the RTA base caller and the clusters are also base called using the RTA base caller. In the third context called &#x201c;Self IE&#x201d;, the cluster centers are detected by the ternary classification model <b>5400</b> and the intensity extraction from the clusters is done using the cluster shape-based intensity extraction techniques disclosed herein (note that the cluster shape information is generated by the ternary classification model <b>5400</b>); but the clusters are base called using the RTA base caller.</p><p id="p-0613" num="0612">The performance is compared between the ternary classification model <b>5400</b> and the RTA base caller along five metrics: (1) the total number of clusters detected (&#x201c;#clusters&#x201d;), (2) the number of detected clusters that pass the chastity filter (&#x201c;#PF&#x201d;), (3) the number of unique or deduplicated proper read pairs produced from the detected clusters (&#x201c;#nondup proper read pairs&#x201d;), (4) the rate of mismatches between a sequence read produced from the detected clusters and a reference sequence after alignment (&#x201c;% Mismatch rate&#x201d;), and (5) bases called for the detected clusters with quality score 30 and above (&#x201c;% Q30&#x201d;).</p><p id="p-0614" num="0613">The performance is compared between the ternary classification model <b>5400</b> and the RTA base caller under the three contexts and the five metrics for two types of sequencing runs: (1) a normal run with 20 pM library concentration and (2) a dense run with 30 pM library concentration.</p><p id="p-0615" num="0614">As shown in <figref idref="DRAWINGS">FIG. <b>65</b></figref>, the ternary classification model <b>5400</b> outperforms the RTA base caller on all the metrics.</p><p id="p-0616" num="0615">Under the same three contexts, five metrics, and two run densities, <figref idref="DRAWINGS">FIG. <b>66</b></figref> shows that the regression model <b>2600</b> outperforms the RTA base caller on all the metrics.</p><p id="p-0617" num="0616"><figref idref="DRAWINGS">FIG. <b>67</b></figref> focuses on the penultimate layer <b>6702</b> of the neural network-based template generator <b>1512</b>.</p><p id="p-0618" num="0617"><figref idref="DRAWINGS">FIG. <b>68</b></figref> visualizes what the penultimate layer <b>6702</b> of the neural network-based template generator <b>1512</b> has learned as a result of the backpropagation-based gradient update training. The illustrated implementation visualizes twenty-four out of the thirty-two convolution filters of the penultimate layer <b>6702</b> overlaid on the ground truth cluster shapes. As shown in <figref idref="DRAWINGS">FIG. <b>68</b></figref>, the penultimate layer <b>6702</b> has learned the cluster metadata, including spatial distribution of the clusters such as cluster centers, cluster shapes, cluster sizes, cluster background, and cluster boundaries.</p><p id="p-0619" num="0618"><figref idref="DRAWINGS">FIG. <b>69</b></figref> overlays cluster center predictions of the binary classification model <b>4600</b> (in blue) onto those of the RTA base caller (in pink). The predictions are made on sequencing image data from the Illumina NextSeq sequencer.</p><p id="p-0620" num="0619"><figref idref="DRAWINGS">FIG. <b>70</b></figref> overlays cluster center predictions made by the RTA base caller (in pink) onto visualization of the trained convolution filters of the penultimate layer of the binary classification model <b>4600</b>. These convolution filters are learned as a result of training on sequencing image data from the Illumina NextSeq sequencer.</p><p id="p-0621" num="0620"><figref idref="DRAWINGS">FIG. <b>71</b></figref> illustrates one implementation of training data used to train the neural network-based template generator <b>1512</b>. In this implementation, the training data is obtained from dense flow cells that produce data with storm probe images. In another implementation, the training data is obtained from dense flow cells that produce data with fewer bridge amplification cycles.</p><p id="p-0622" num="0621"><figref idref="DRAWINGS">FIG. <b>72</b></figref> is one implementation of using beads for image registration based on cluster center predictions of the neural network-based template generator <b>1512</b>.</p><p id="p-0623" num="0622"><figref idref="DRAWINGS">FIG. <b>73</b></figref> illustrates one implementation of cluster statistics of clusters identified by the neural network-based template generator <b>1512</b>. The cluster statistics include cluster size based on number of contributive subpixels and GC-content.</p><p id="p-0624" num="0623"><figref idref="DRAWINGS">FIG. <b>74</b></figref> shows how the neural network-based template generator <b>1512</b>'s ability to distinguish between adjacent clusters improves when the number of initial sequencing cycles for which the input image data <b>1702</b> is used increases from five to seven. For five sequencing cycles, a single cluster is identified by a single disjointed region of contiguous subpixels. For seven sequencing cycles, the single cluster is segmented into two adjacent clusters, each having their own disjointed regions of contiguous subpixels.</p><p id="p-0625" num="0624"><figref idref="DRAWINGS">FIG. <b>75</b></figref> illustrates the difference in base calling performance when a RTA base caller uses ground truth center of mass (COM) location as the cluster center, as opposed to when a non-COM location is used as the cluster center.</p><p id="p-0626" num="0625"><figref idref="DRAWINGS">FIG. <b>76</b></figref> portrays the performance of the neural network-based template generator <b>1512</b> on extra detected clusters.</p><p id="p-0627" num="0626"><figref idref="DRAWINGS">FIG. <b>77</b></figref> shows different datasets used for training the neural network-based template generator <b>1512</b>.</p><p id="p-0628" num="0627"><figref idref="DRAWINGS">FIG. <b>78</b></figref> shows the processing stages used by the RTA base caller for base calling, according to one implementation. <figref idref="DRAWINGS">FIG. <b>78</b></figref> also shows the processing stages used by the disclosed neural network-based base caller for base calling, according to two implementations. As shown in <figref idref="DRAWINGS">FIG. <b>78</b></figref>, the neural network-based base caller <b>1514</b> can streamline the base calling process by obviating many of the processing stages used by the RTA base caller. The streamlining improves base calling accuracy and scale. In a first implementation of the neural network-based base caller <b>1514</b>, it performs base calling using location/position information of cluster centers identified from the output of the neural network-based template generator <b>1512</b>. In a second implementation, the neural network-based base caller <b>1514</b> does not use the location/position information of the cluster centers for base calling. The second implementation is used when a patterned flow cell design is used for cluster generation. The patterned flow cell contains nanowells that are precisely positioned relative to known fiducial locations and provide prearranged cluster distribution on the patterned flow cell. In other implementations, the neural network-based base caller <b>1514</b> base calls clusters generated on random flow cells.</p><p id="p-0629" num="0628">Neural Network-Based Base Calling</p><p id="p-0630" num="0629">The discussion now turns to the neural network-based base calling in which a neural network is trained to map sequencing images to base calls. The discussion is organized as follows. First, the inputs to the neural network are described. Then, the structure and form of the neural network are described. Finally, the outputs of the neural network are described.</p><p id="p-0631" num="0630">Input</p><p id="p-0632" num="0631"><figref idref="DRAWINGS">FIG. <b>79</b></figref> illustrates one implementation of base calling using the neural network <b>7906</b>.</p><p id="p-0633" num="0632">Main Input: Image Channels</p><p id="p-0634" num="0633">The main input to the neural network <b>7906</b> is image data <b>7902</b>. The image data <b>7902</b> is derived from the sequencing images <b>108</b> produced by the sequencer <b>102</b> during a sequencing run. In one implementation, the image data <b>7902</b> comprises n&#xd7;n image patches extracted from the sequencing images <b>102</b>, where n is any number ranging from 1 and 10,000. The sequencing run produces m image(s) per sequencing cycle for corresponding m image channels, and an image patch is extracted from each of them image(s) to prepare the image data for a particular sequencing cycle. In different implementations such as 4-, 2-, and 1-channel chemistries, m is 4 or 2. In other implementations, m is 1, 3, or greater than 4. The image data <b>7902</b> is in the optical, pixel domain in some implementations, and in the upsampled, subpixel domain in other implementations.</p><p id="p-0635" num="0634">The image data <b>7902</b> comprises data for multiple sequencing cycles (e.g., a current sequencing cycle, one or more preceding sequencing cycles, and one or more successive sequencing cycles). In one implementation, the image data <b>7902</b> comprises data for three sequencing cycles, such that data for a current (time t) sequencing cycle to be base called is accompanied with (i) data for a left flanking/context/previous/preceding/prior (time t&#x2212;1) sequencing cycle and (ii) data for a right flanking/context/next/successive/subsequent (time t+1) sequencing cycle. In other implementations, the image data <b>7902</b> comprises data for a single sequencing cycle.</p><p id="p-0636" num="0635">The image data <b>7902</b> depicts intensity emissions of one or more clusters and their surrounding background. In one implementation, when a single target cluster is to be base called, the image patches are extracted from the sequencing images <b>108</b> in such a way that each image patch contains the center of the target cluster in its center pixel, a concept referred to herein as the &#x201c;target cluster-centered patch extraction&#x201d;.</p><p id="p-0637" num="0636">The image data <b>7902</b> is encoded in the input data <b>7904</b> using intensity channels (also called image channels). For each of them images obtained from the sequencer <b>102</b> for a particular sequencing cycle, a separate image channel is used to encode its intensity data. Consider, for example, that the sequencing run uses the 2-channel chemistry which produces a red image and a green image at each sequencing cycle, then the input data <b>7904</b> comprises (i) a first red image channel with n&#xd7;n pixels that depict intensity emissions of the one or more clusters and their surrounding background captured in the red image and (ii) a second green image channel with n&#xd7;n pixels that depict intensity emissions of the one or more clusters and their surrounding background captured in the green image.</p><p id="p-0638" num="0637">Supplemental Input: Distance Channels</p><p id="p-0639" num="0638">The image data <b>7902</b> is accompanied with supplemental distance data (also called distance channels). Distance channels supply additive bias that is incorporated in the feature maps generated from the image channels. This additive bias contributes to base calling accuracy because it is based on pixel center-to-cluster center(s) distances, which are pixel-wise encoded in the distance channels.</p><p id="p-0640" num="0639">In a &#x201c;single target cluster&#x201d; base calling implementation, for each image channel (image patch) in the input data <b>7904</b>, a supplemental distance channel identifies distances of its pixels' centers from the center of a target cluster containing its center pixel and to be base called. The distance channel thereby indicates respective distances of pixels of an image patch from a center pixel of the image patch.</p><p id="p-0641" num="0640">In a &#x201c;multi-cluster&#x201d; base calling implementation, for each image channel (image patch) in the input data <b>7904</b>, a supplemental distance channel identifies each pixel's center-to-center distance from a nearest one of the clusters selected based on center-to-center distances between the pixel and each of the clusters.</p><p id="p-0642" num="0641">In a &#x201c;multi-cluster shape-based&#x201d; base calling implementation, for each image channel (image patch) in the input data <b>7904</b>, a supplemental distance channel identifies each cluster pixel's center-to-center distance from an assigned cluster selected based on classifying each cluster pixel to only one cluster.</p><p id="p-0643" num="0642">Supplemental Input: Scaling Channel</p><p id="p-0644" num="0643">The image data <b>7902</b> is accompanied with supplemental scaling data (also called scaling channel) that accounts for different cluster sizes and uneven illumination conditions. Scaling channel also supplies additive bias that is incorporated in the feature maps generated from the image channels. This additive bias contributes to base calling accuracy because it is based on mean intensities of central cluster pixel(s), which are pixel-wise encoded in the scaling channel.</p><p id="p-0645" num="0644">Supplemental Input: Cluster Center Coordinates</p><p id="p-0646" num="0645">In some implementations, the location/position information <b>7916</b> (e.g., x-y coordinates) of cluster center(s) identified from the output of the neural network-based template generator <b>1512</b> is fed as supplemental input to the neural network <b>7906</b>.</p><p id="p-0647" num="0646">Supplemental Input: Cluster Attribution Information</p><p id="p-0648" num="0647">In some implementations, the neural network <b>7906</b> receives, as supplemental input, cluster attribution information that classifies which pixels or subpixels are: background pixels or subpixels, cluster center pixels or subpixels, and cluster/cluster interior pixels or subpixels depicting/contributing to/belonging to a same cluster. In other implementations, the decay map, the binary map, and/or the ternary map or a variation of those is fed as supplemental input to the neural network <b>7906</b>.</p><p id="p-0649" num="0648">Pre-Processing: Intensity Modification</p><p id="p-0650" num="0649">In some implementations, the input data <b>7904</b> does not contain the distance channels, but instead the neural network <b>7906</b> receives, as input, modified image data that is modified based on the output of the neural network-based template generator <b>1512</b>, i.e., the decay map, the binary map, and/or the ternary map. In such implementations, the intensities of the image data <b>7902</b> are modified to account for the absence distance channels.</p><p id="p-0651" num="0650">In other implementations, the image data <b>7902</b> is subjected to one or more lossless transformation operations (e.g., convolutions, deconvolutions, Fourier transforms) and the resulting modified image data is fed as input to the neural network <b>7906</b>.</p><p id="p-0652" num="0651">Network Structure and Form</p><p id="p-0653" num="0652">The neural network <b>7906</b> is also referred to herein as the &#x201c;neural network-based base caller&#x201d; <b>1514</b>. In one implementation, the neural network-based base caller <b>1514</b> is a multilayer perceptron (MLP). In another implementation, the neural network-based base caller <b>1514</b> is a feedforward neural network. In yet another implementation, the neural network-based base caller <b>1514</b> is a fully-connected neural network. In a further implementation, the neural network-based base caller <b>1514</b> is a fully convolutional neural network. In yet further implementation, the neural network-based base caller <b>1514</b> is a semantic segmentation neural network.</p><p id="p-0654" num="0653">In one implementation, the neural network-based base caller <b>1514</b> is a convolutional neural network (CNN) with a plurality of convolution layers. In another implementation, it is a recurrent neural network (RNN) such as a long short-term memory network (LSTM), bi-directional LS&#x2122; (Bi-LS&#x2122;), or a gated recurrent unit (GRU). In yet another implementation, it includes both a CNN and a RNN.</p><p id="p-0655" num="0654">In yet other implementations, the neural network-based base caller <b>1514</b> can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. It can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. It can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous SGD. It can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LS&#x2122; or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tan h)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><p id="p-0656" num="0655">The neural network-based base caller <b>1514</b> processes the input data <b>7904</b> and produces an alternative representation <b>7908</b> of the input data <b>7904</b>. The alternative representation <b>7908</b> is a convolved representation in some implementations and a hidden representation in other implementations. The alternative representation <b>7908</b> is then processed by an output layer <b>7910</b> to produce an output <b>7912</b>. The output <b>7912</b> is used to produce the base call(s), as discussed below.</p><p id="p-0657" num="0656">Output</p><p id="p-0658" num="0657">In one implementation, the neural network-based base caller <b>1514</b> outputs a base call for a single target cluster for a particular sequencing cycle. In another implementation, it outputs a base call for each target cluster in a plurality of target clusters for the particular sequencing cycle. In yet another implementation, it outputs a base call for each target cluster in a plurality of target clusters for each sequencing cycle in a plurality of sequencing cycles, thereby producing a base call sequence for each target cluster.</p><p id="p-0659" num="0658">Distance Channel Calculation</p><p id="p-0660" num="0659">The discussion now turns to how appropriate location/position information (e.g., x-y coordinates) of cluster center(s) is obtained for use in calculating distance values of the distance channels.</p><p id="p-0661" num="0660">Downscaling of Coordinates</p><p id="p-0662" num="0661"><figref idref="DRAWINGS">FIG. <b>80</b></figref> is one implementation of transforming, from subpixel domain to pixel domain, location/position information of cluster centers identified from the output of the neural network-based template generator <b>1512</b>.</p><p id="p-0663" num="0662">Cluster center location/position information is used for the neural network-based base calling at least (i) to construct the input data by extracting image patches from the sequencing images <b>108</b> that contain the centers of target clusters to be base called in their center pixels, (ii) to construct the distance channel that identifies distances of an image patch's pixels' centers from the center of a target cluster contained its center pixel, and/or (iii) as supplemental input <b>7916</b> to the neural network-based base caller <b>1514</b>.</p><p id="p-0664" num="0663">In some implementations, the cluster center location/position information is identified from the output of the neural network-based template generator <b>1512</b> in the upsampled, subpixel resolution. However, in some implementations, the neural network-based base caller <b>1514</b> operates on image data that is in optical, pixel-resolution. Therefore, in one implementation, the cluster center location/position information is transformed into the pixel domain by downscaling coordinates of the cluster centers by the same upsampling factor used to upsample image data fed as input to the neural network-based template generator <b>1512</b>.</p><p id="p-0665" num="0664">Consider, for example, that the image patches data fed as input to the neural network-based template generator <b>1512</b> are derived by upsampling sequencing images <b>108</b> from some initial sequencing cycles by an upsampling factor f. Then, in one implementation, the coordinates of the cluster centers <b>8002</b>, produced by the neural network-based template generator <b>1512</b> by the post-processor <b>1814</b> and stored in the template/template image <b>8004</b>, are divided by f(the divisor). These downscaled cluster center coordinates are referred to herein as the &#x201c;reference cluster centers&#x201d; <b>8008</b> and stored in the template/template image <b>8004</b>. In one implementation, the downscaling is performed by a downscaler <b>8006</b>.</p><p id="p-0666" num="0665">Transformation of Coordinates</p><p id="p-0667" num="0666"><figref idref="DRAWINGS">FIG. <b>81</b></figref> is one implementation of using cycle-specific and image channel-specific transformations to derive the so-called &#x201c;transformed cluster centers&#x201d; <b>8104</b> from the reference cluster centers <b>8008</b>. The motivation for doing so is discussed first.</p><p id="p-0668" num="0667">Sequencing images taken at different sequencing cycles are misaligned and have random translational offsets with respect to each other. This occurs due to the finite accuracy of the movements of the sensor's motion stage and also because images taken in different image/frequency channels have different optical paths and wavelengths. Consequently, an offset exists between the reference cluster centers and locations/positions of the cluster centers in the sequencing images. This offset varies between images captured at different sequencing cycles and within images captured at a same sequencing cycle in different image channels.</p><p id="p-0669" num="0668">To account for this offset, cycle-specific and image channel-specific transformations are applied to the reference cluster centers to produce respective transformed cluster centers for image patches of each sequencing cycle. The cycle-specific and image channel-specific transformations are derived by an image registration process that uses image correlation to determine a full six-parameter affine transformation (e.g., translation, rotation, scaling, shear, right reflection, left reflection) or a Procrustes transformation (e.g., translation, rotation, scaling, optionally extended to aspect ratio), additional details of which can be found in Appendices 1, 2, 3, and 4.</p><p id="p-0670" num="0669">Consider, for example, that the reference cluster centers for four cluster centers are (x<sub>1</sub>, y<sub>1</sub>); (x<sub>2</sub>, y<sub>2</sub>); (x<sub>3</sub>, y<sub>3</sub>); (x<sub>4</sub>, y<sub>4</sub>) and the sequencing run uses 2-channel chemistry in which a red image and a green image are produced at each sequencing cycle. Then, for example sequencing cycle 3, the cycle-specific and image channel-specific transformations are {&#x3b1;<sub>r</sub><sup>3</sup>, &#x3b2;<sub>r</sub><sup>3</sup>, X<sub>r</sub><sup>3</sup>, &#x3b4;<sub>r</sub><sup>3</sup>, &#x3f5;<sub>r</sub><sup>3</sup>, &#x3d5;<sub>r</sub><sup>3</sup>} for the red image and {&#x3b1;<sub>g</sub><sup>3</sup>, &#x3b2;<sub>g</sub><sup>3</sup>, X<sub>g</sub><sup>3</sup>, &#x3b4;<sub>g</sub><sup>3</sup>, &#x3f5;<sub>g</sub><sup>3</sup>, &#x3d5;<sub>g</sub><sup>3</sup>} for the green image.</p><p id="p-0671" num="0670">Similarly, for example sequencing cycle 9, the cycle-specific and image channel-specific transformations are {&#x3b1;<sub>r</sub><sup>9</sup>, &#x3b2;<sub>r</sub><sup>9</sup>, X<sub>r</sub><sup>9</sup>, &#x3b4;<sub>r</sub><sup>9</sup>, &#x3f5;<sub>r</sub><sup>9</sup>, &#x3d5;<sub>r</sub><sup>9</sup>} for the red image and {&#x3b1;<sub>g</sub><sup>9</sup>, &#x3b2;<sub>g</sub><sup>9</sup>, X<sub>g</sub><sup>9</sup>, &#x3b4;<sub>g</sub><sup>9</sup>, &#x3f5;<sub>g</sub><sup>9</sup>, &#x3d5;<sub>g</sub><sup>9</sup>} for the green image.</p><p id="p-0672" num="0671">Then, the transformed cluster centers for the red image of sequencing cycle 3 ({circumflex over (x)}<sub>1</sub>, &#x177;<sub>1</sub>); ({circumflex over (x)}<sub>2</sub>, &#x177;<sub>2</sub>); ({circumflex over (x)}<sub>3</sub>, &#x177;<sub>3</sub>); ({circumflex over (x)}<sub>4</sub>, &#x177;<sub>4</sub>) are derived by applying the transformation {&#x3b1;<sub>r</sub><sup>3</sup>, &#x3b2;<sub>r</sub><sup>3</sup>, X<sub>r</sub><sup>3</sup>, &#x3b4;<sub>r</sub><sup>3</sup>, &#x3b5;<sub>r</sub><sup>3</sup>, &#x3c6;<sub>r</sub><sup>3</sup>} to the reference cluster centers (x<sub>1</sub>, y<sub>1</sub>); (x<sub>2</sub>, y<sub>2</sub>), (x<sub>3</sub>, y<sub>3</sub>); (x<sub>4</sub>, y<sub>4</sub>), and the transformed cluster centers for the green image of sequencing cycle 3 (x&#x306;<sub>1</sub>, y&#x306;<sub>1</sub>); (x&#x306;<sub>2</sub>, y&#x306;<sub>2</sub>); (x&#x306;<sub>3</sub>, y&#x306;<sub>3</sub>); (x&#x306;<sub>4</sub>, y&#x306;<sub>4</sub>) are derived by applying the transformation {&#x3b1;<sub>g</sub><sup>3</sup>, &#x3b2;<sub>g</sub><sup>3</sup>, X<sub>g</sub><sup>3</sup>, &#x3b4;<sub>g</sub><sup>3</sup>, &#x3b5;<sub>g</sub><sup>3</sup>, &#x3c6;<sub>g</sub><sup>3</sup>} to the reference cluster centers (x<sub>1</sub>, y<sub>1</sub>); (x<sub>2</sub>, y<sub>2</sub>); (x<sub>3</sub>, y<sub>3</sub>); (x<sub>4</sub>, y<sub>4</sub>).</p><p id="p-0673" num="0672">Similarly, the transformed cluster centers for the red image of sequencing cycle 9 ({right arrow over (x)}<sub>1</sub>, {right arrow over (y)}<sub>1</sub>); ({right arrow over (x)}<sub>2</sub>, {right arrow over (y)}<sub>2</sub>), ({right arrow over (x)}<sub>3</sub>, {right arrow over (y)}<sub>3</sub>), ({right arrow over (x)}<sub>4</sub>, {right arrow over (y)}<sub>4</sub>) are derived by applying the transformation {&#x3b1;<sub>r</sub><sup>9</sup>, &#x3b2;<sub>r</sub><sup>9</sup>, X<sub>r</sub><sup>9</sup>, &#x3b4;<sub>r</sub><sup>9</sup>, &#x3b5;<sub>r</sub><sup>9</sup>, &#x3c6;<sub>r</sub><sup>9</sup>} to the reference cluster centers (x<sub>1</sub>, y<sub>1</sub>); (x<sub>2</sub>, y<sub>2</sub>); (x<sub>3</sub>, y<sub>3</sub>); (x<sub>4</sub>, y<sub>4</sub>), and the transformed cluster centers for the green image of sequencing cycle 9 ({tilde over (x)}<sub>1</sub>, {tilde over (y)}<sub>1</sub>); ({tilde over (x)}<sub>2</sub>, {tilde over (y)}<sub>2</sub>); ({tilde over (x)}<sub>3</sub>, {tilde over (y)}<sub>3</sub>); ({tilde over (x)}<sub>4</sub>, {tilde over (y)}<sub>4</sub>) are derived by applying the transformation {&#x3b1;<sub>g</sub><sup>9</sup>, &#x3b2;<sup>9</sup><sub>g</sub>, X<sub>g</sub><sup>9</sup>, &#x3b4;<sub>g</sub><sup>9</sup>, &#x3b5;<sub>g</sub><sup>9</sup>, &#x3c6;<sub>g</sub><sup>9</sup>} to the reference cluster centers (x<sub>1</sub>, y<sub>1</sub>); (x<sub>2</sub>, y<sub>2</sub>); (x<sub>3</sub>, y<sub>3</sub>); (x<sub>4</sub>, y<sub>4</sub>).</p><p id="p-0674" num="0673">In one implementation, the transformations are performed by a transformer <b>8102</b>.</p><p id="p-0675" num="0674">The transformed cluster centers <b>8104</b> are the stored in the template/template image <b>8004</b> and respectively used (i) to do the patch extraction from corresponding sequencing images <b>108</b> (e.g., by a patch extractor <b>8106</b>), (ii) in the distance formula (d=&#x221a;{square root over ((x<sub>2</sub>&#x2212;x<sub>1</sub>)<sup>2</sup>+(y<sub>2</sub>&#x2212;y<sub>1</sub>)<sup>2</sup>)}) to calculate the distance channels for corresponding image patches, and (iii) as supplemental input to the neural network-based base caller <b>1514</b> for the corresponding sequencing cycle being base called. In other implementations, a different distance formula can be used such as distance squared, e{circumflex over (&#x2003;)}-distance, and e{circumflex over (&#x2003;)}-distance squared.</p><p id="p-0676" num="0675">Image Patch</p><p id="p-0677" num="0676"><figref idref="DRAWINGS">FIG. <b>82</b></figref> illustrates an image patch <b>8202</b> that is part of the input data fed to the neural network-based base caller <b>1514</b>. The input data includes a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. Each per-cycle image patch set in the sequence has an image patch for a respective one of one or more image channels.</p><p id="p-0678" num="0677">Consider, for example, that the sequencing run uses the 2-channel chemistry which produces a red image and a green image at each sequencing cycle, and the input data comprises data spanning a series of three sequencing cycles of the sequencing run: a current (time t) sequencing cycle to be base called, a previous (time t&#x2212;1) sequencing cycle, and a next (time t+1) sequencing cycle.</p><p id="p-0679" num="0678">Then, the input data comprises the following sequence of per-cycle image patch sets: a current cycle image patch set with a current red image patch and a current green image patch respectively extracted from the red and green sequencing images captured at the current sequencing cycle, a previous cycle image patch set with a previous red image patch and a previous green image patch respectively extracted from the red and green sequencing images captured at the previous sequencing cycle, and a next cycle image patch set with a next red image patch and a next green image patch respectively extracted from the red and green sequencing images captured at the next sequencing cycle.</p><p id="p-0680" num="0679">The size of each image patch can be n&#xd7;n, where n can be any number ranging from 1 and 10,000. Each image patch can be in the optical, pixel domain or in the upsampled, subpixel domain. In the implementation illustrated in <figref idref="DRAWINGS">FIG. <b>82</b></figref>, the extracted image page <b>8202</b> has pixel intensity data for pixels that cover/depict a plurality of clusters 1-m and their surrounding background. Also, in the illustrated implementation, the image patch <b>8202</b> is extracted in such a way that is contains in its center pixel the center of a target cluster being base called.</p><p id="p-0681" num="0680">In <figref idref="DRAWINGS">FIG. <b>82</b></figref>, the pixel centers are depicted by a black rectangle and have integer location/position coordinates, and the cluster centers are depicted by a purple circle and have floating-point location/position coordinates.</p><p id="p-0682" num="0681">Distance Calculation for a Single Target Cluster</p><p id="p-0683" num="0682"><figref idref="DRAWINGS">FIG. <b>83</b></figref> depicts one implementation of determining distance values <b>8302</b> for a distance channel when a single target cluster is being base called by the neural network-based base caller <b>1514</b>. The center of the target cluster is contained in the center pixels of the image patches that are fed as input to the neural network-based base caller <b>1514</b>. The distance values are calculated on a pixel-by-pixel basis, such that, for each pixel, the distance between its center and the center of the target cluster is determined. Accordingly, a distance value is calculated for each pixel in each of the image patches that are part of the input data.</p><p id="p-0684" num="0683"><figref idref="DRAWINGS">FIG. <b>83</b></figref> shows three distance values dl, dc, and dn for a particular image patch. In one implementation, the distance values <b>8302</b> are calculated using the following distance formula: d=&#x221a;{square root over ((x<sub>2</sub>&#x2212;x<sub>1</sub>)<sup>2</sup>+(y<sub>2</sub>&#x2212;y<sub>1</sub>)<sup>2</sup>)}, which operates on the transformed cluster centers <b>8104</b>. In other implementations, a different distance formula can be used such as distance squared, e{circumflex over (&#x2003;)}-distance, and e{circumflex over (&#x2003;)}-distance squared.</p><p id="p-0685" num="0684">In other implementations, when the image patches are in the upsampled, subpixel resolution, the distance values <b>8302</b> are calculated in the subpixel domain.</p><p id="p-0686" num="0685">Thus, in the single target cluster base calling implementation, the distance channels are calculated only with respect to the target cluster being base called.</p><p id="p-0687" num="0686"><figref idref="DRAWINGS">FIG. <b>84</b></figref> shows one implementation of pixel-wise encoding <b>8402</b> the distance values <b>8302</b> that are calculated between the pixels and the target cluster. In one implementation, in the input data, the distance values <b>8302</b>, as part of the distance channel, supplement each corresponding image channel (image patch) as &#x201c;pixel distance data&#x201d;. Returning to the example of a red image and a green image being generated per-sequencing cycle, the input data comprises a red distance channel and a green distance channel that supplement the red image channel and the green image channel as pixel distance data, respectively.</p><p id="p-0688" num="0687">In other implementations, when the image patches are in the upsampled, subpixel resolution, the distance channels are encoded on a subpixel-by-subpixel basis.</p><p id="p-0689" num="0688">Distance Calculation for Multiple Target Clusters</p><p id="p-0690" num="0689"><figref idref="DRAWINGS">FIG. <b>85</b><i>a </i></figref>depicts one implementation of determining distance values <b>8502</b> for a distance channel when multiple target clusters 1-m are being simultaneously base called by the neural network-based base caller <b>1514</b>. The distance values are calculated on a pixel-by-pixel basis, such that, for each pixel, the distance between its center and respective centers of each of the multiple clusters 1-m is determined and the minimum distance value (in red) is assigned to the pixel.</p><p id="p-0691" num="0690">Accordingly, the distance channel identifies each pixel's center-to-center distance from a nearest one of the clusters selected based on center-to-center distances between the pixel and each of the clusters. In the illustrated implementation, <figref idref="DRAWINGS">FIG. <b>85</b><i>a </i></figref>shows pixel center-to-cluster center distances for two pixels and four cluster centers. Pixel <b>1</b> is nearest to cluster 1 and pixel n is nearest to cluster 3.</p><p id="p-0692" num="0691">In one implementation, the distance values <b>8502</b> are calculated using the following distance formula: &#x221a;{square root over (d=(x<sub>2</sub>&#x2212;x<sub>1</sub>)<sup>2</sup>+(y<sub>2</sub>&#x2212;y<sub>1</sub>)<sup>2</sup>)}, which operates on the transformed cluster centers <b>8104</b>. In other implementations, a different distance formula can be used such as distance squared, e{circumflex over (&#x2003;)}-distance, and e{circumflex over (&#x2003;)}-distance squared.</p><p id="p-0693" num="0692">In other implementations, when the image patches are in the upsampled, subpixel resolution, the distance values <b>8502</b> are calculated in the subpixel domain.</p><p id="p-0694" num="0693">Thus, in the multi-cluster base calling implementation, the distance channels are calculated with respect to the nearest cluster from among a plurality of clusters.</p><p id="p-0695" num="0694"><figref idref="DRAWINGS">FIG. <b>85</b><i>b </i></figref>shows, for each of the target clusters 1-m, some nearest pixels determined based on the pixel center-to-nearest cluster center distances <b>8504</b> (d1, d2, d23, d29, d24, d32, dn, d13, d14, and etc.).</p><p id="p-0696" num="0695"><figref idref="DRAWINGS">FIG. <b>86</b></figref> shows one implementation of pixel-wise encoding <b>8602</b> the minimum distance values that are calculated between the pixels and the nearest one of the clusters. In other implementations, when the image patches are in the upsampled, subpixel resolution, the distance channels are encoded on a subpixel-by-subpixel basis.</p><p id="p-0697" num="0696">Distance Calculation for Multiple Target Clusters based on Cluster Shapes</p><p id="p-0698" num="0697"><figref idref="DRAWINGS">FIG. <b>87</b></figref> illustrates one implementation using pixel-to-cluster classification/attribution/categorization <b>8702</b>, referred to herein as &#x201c;cluster shape data&#x201d; or &#x201c;cluster shape information&#x201d;, to determine cluster distance values <b>8802</b> for a distance channel when multiple target clusters 1-m are being simultaneously base called by the neural network-based base caller <b>1514</b>. First, what follows is a brief review of how the cluster shape data is generated.</p><p id="p-0699" num="0698">As discussed above, the output of the neural network-based template generator <b>1512</b> is used to classify the pixels as: background pixels, center pixels, and cluster/cluster interior pixels depicting/contributing to/belonging to a same cluster. This pixel-to-cluster classification information is used to attribute each pixel to only one cluster, irrespective of the distances between the pixel centers and the cluster centers, and is stored as the cluster shape data.</p><p id="p-0700" num="0699">In the implementation illustrated in <figref idref="DRAWINGS">FIG. <b>87</b></figref>, background pixels are colored in grey, pixels belonging to cluster 1 are colored in yellow (cluster 1 pixels), pixels belonging to cluster 2 are colored in green (cluster 2 pixels), pixels belonging to cluster 3 are colored in red (cluster 3 pixels), and pixels belonging to cluster m are colored in blue (cluster m pixels).</p><p id="p-0701" num="0700"><figref idref="DRAWINGS">FIG. <b>88</b></figref> shows one implementation of calculating the distance values <b>8802</b> using the cluster shape data. First, we explain why distance information calculated without accounting for cluster shapes is prone to error. We then explain how the cluster shape data overcomes this limitation.</p><p id="p-0702" num="0701">In the &#x201c;multi-cluster&#x201d; base calling implementation that does not use cluster shape data (<figref idref="DRAWINGS">FIGS. <b>85</b><i>a</i>-<i>b </i></figref>and <b>86</b>), the center-to-center distance value for a pixel is calculated with respect to the nearest cluster from among a plurality of clusters. Now, consider the scenario when a pixel that belongs to cluster A is further away from the center of cluster A but nearer to the center of cluster B. In such a case, without the cluster shape data, the pixel is assigned a distance value that is calculated with respect to cluster B (to which it does not belong), instead of being assigned a distance value vis-a-vis cluster A (to which it truly belongs).</p><p id="p-0703" num="0702">The &#x201c;multi-cluster shape-based&#x201d; base calling implementation avoids this by using the true pixel-to-cluster mapping, as defined in the raw image data and produced by the neural network-based template generator <b>1512</b>.</p><p id="p-0704" num="0703">Contrast between the two implementations can be seen with regards to pixels <b>34</b> and <b>35</b>. In <figref idref="DRAWINGS">FIG. <b>85</b><i>b</i></figref>, distance values of pixels <b>34</b> and <b>35</b> are calculated with respect to the nearest center of cluster 3, without accounting for the cluster shape data. However, in <figref idref="DRAWINGS">FIG. <b>88</b></figref>, based on the cluster shape data, distance values <b>8802</b> of pixels <b>34</b> and <b>35</b> are calculated with respect to cluster 2 (to which they actually belong).</p><p id="p-0705" num="0704">In <figref idref="DRAWINGS">FIG. <b>88</b></figref>, the cluster pixels depict cluster intensities and the background pixels depict background intensities. The cluster distance values identify each cluster pixel's center-to-center distance from an assigned one of the clusters selected based on classifying each cluster pixel to only one of the clusters. In some implementations, the background pixels are assigned a predetermined background distance value, such as 0 or 0.1, or some other minimum value.</p><p id="p-0706" num="0705">In one implementation, as discussed above, the cluster distance values <b>8802</b> are calculated using the following distance formula: &#x221a;{square root over (d=(x<sub>2</sub>&#x2212;x<sub>1</sub>)<sup>2</sup>+(y<sub>2</sub>&#x2212;y<sub>1</sub>)<sup>2</sup>)}, which operates on the transformed cluster centers <b>8104</b>. In other implementations, a different distance formula can be used such as distance squared, e{circumflex over (&#x2003;)}-distance, and e{circumflex over (&#x2003;)}-distance squared.</p><p id="p-0707" num="0706">In other implementations, when the image patches are in the upsampled, subpixel resolution, the cluster distance values <b>8802</b> are calculated in the subpixel domain and the cluster and background attribution <b>8702</b> occurs on a subpixel-by-subpixel basis.</p><p id="p-0708" num="0707">Thus, in the multi-cluster shape-based base calling implementation, the distance channels are calculated with respect to an assigned cluster from among a plurality of clusters. The assigned cluster is selected based on classifying each cluster pixel to only one of the clusters in accordance with the true pixel-to-cluster mapping defined in the raw image data.</p><p id="p-0709" num="0708"><figref idref="DRAWINGS">FIG. <b>89</b></figref> shows one implementation of pixel-wise encoding the distance values <b>8702</b> that are calculated between the pixels and the assigned clusters. In other implementations, when the image patches are in the upsampled, subpixel resolution, the distance channels are encoded on a subpixel-by-subpixel basis.</p><p id="p-0710" num="0709">Deep learning is a powerful machine learning technique that uses many-layered neural networks. One particularly successful network structure in computer vision and image processing domains is the convolutional neural network (CNN), where each layer performs a feed-forward convolutional transformations from an input tensor (an image-like, multi-dimensional dense array) to an output tensor of different shape. CNNs are particularly suited for image-like input due the spatial coherence of images and the advent of general purpose graphics processing units (GPUs) which make training fast on arrays up to 3- or 4-D. Exploiting these image-like properties leads to superior empirical performance compared to other learning methods such as support vector machine (SVM) or multi-layer perceptron (MLP).</p><p id="p-0711" num="0710">We introduce a specialized architecture that augments a standard CNN to handle both image data and supplemental distance and scaling data. More details follow.</p><p id="p-0712" num="0711">Specialized Architecture</p><p id="p-0713" num="0712"><figref idref="DRAWINGS">FIG. <b>90</b></figref> illustrates one implementation of the specialized architecture of the neural network-based base caller <b>1514</b> that is used to segregate processing of data for different sequencing cycles. The motivation for using the specialized architecture is described first.</p><p id="p-0714" num="0713">As discussed above, the neural network-based base caller <b>1514</b> processes data for a current sequencing cycle, one or more preceding sequencing cycles, and one or more successive sequencing cycles. Data for additional sequencing cycles provides sequence-specific context. The neural network-based base caller <b>1514</b> learns the sequence-specific context during training and base call them. Furthermore, data for pre and post sequencing cycles provides second order contribution of pre-phasing and phasing signals to the current sequencing cycle.</p><p id="p-0715" num="0714">Spatial Convolution Layers</p><p id="p-0716" num="0715">However, as discussed above, images captured at different sequencing cycles and in different image channels are misaligned and have residual registration error with respect to each other. To account for this misalignment, the specialized architecture comprises spatial convolution layers that do not mix information between sequencing cycles and only mix information within a sequencing cycle.</p><p id="p-0717" num="0716">Spatial convolution layers use so-called &#x201c;segregated convolutions&#x201d; that operationalize the segregation by independently processing data for each of a plurality of sequencing cycles through a &#x201c;dedicated, non-shared&#x201d; sequence of convolutions. The segregated convolutions convolve over data and resulting feature maps of only a given sequencing cycle, i.e., intra-cycle, without convolving over data and resulting feature maps of any other sequencing cycle.</p><p id="p-0718" num="0717">Consider, for example, that the input data comprises (i) current data for a current (time t) sequencing cycle to be base called, (ii) previous data for a previous (time t&#x2212;1) sequencing cycle, and (iii) next data for a next (time t+1) sequencing cycle. The specialized architecture then initiates three separate data processing pipelines (or convolution pipelines), namely, a current data processing pipeline, a previous data processing pipeline, and a next data processing pipeline. The current data processing pipeline receives as input the current data for the current (time t) sequencing cycle and independently processes it through a plurality of spatial convolution layers to produce a so-called &#x201c;current spatially convolved representation&#x201d; as the output of a final spatial convolution layer. The previous data processing pipeline receives as input the previous data for the previous (time t&#x2212;1) sequencing cycle and independently processes it through the plurality of spatial convolution layers to produce a so-called &#x201c;previous spatially convolved representation&#x201d; as the output of the final spatial convolution layer. The next data processing pipeline receives as input the next data for the next (time t+1) sequencing cycle and independently processes it through the plurality of spatial convolution layers to produce a so-called &#x201c;next spatially convolved representation&#x201d; as the output of the final spatial convolution layer.</p><p id="p-0719" num="0718">In some implementations, the current, previous, and next processing pipelines are executed in parallel.</p><p id="p-0720" num="0719">In some implementations, the spatial convolution layers are part of a spatial convolutional network (or subnetwork) within the specialized architecture.</p><p id="p-0721" num="0720">Temporal Convolution Layers</p><p id="p-0722" num="0721">The neural network-based base caller <b>1514</b> further comprises temporal convolution layers that mix information between sequencing cycles, i.e., inter-cycles. The temporal convolution layers receive their inputs from the spatial convolutional network and operate on the spatially convolved representations produced by the final spatial convolution layer for the respective data processing pipelines.</p><p id="p-0723" num="0722">The inter-cycle operability freedom of the temporal convolution layers emanates from the fact that the misalignment property, which exists in the image data fed as input to the spatial convolutional network, is purged out from the spatially convolved representations by the cascade of segregated convolutions performed by the sequence of spatial convolution layers.</p><p id="p-0724" num="0723">Temporal convolution layers use so-called &#x201c;combinatory convolutions&#x201d; that groupwise convolve over input channels in successive inputs on a sliding window basis. In one implementation, the successive inputs are successive outputs produced by a previous spatial convolution layer or a previous temporal convolution layer.</p><p id="p-0725" num="0724">In some implementations, the temporal convolution layers are part of a temporal convolutional network (or subnetwork) within the specialized architecture. The temporal convolutional network receives its inputs from the spatial convolutional network. In one implementation, a first temporal convolution layer of the temporal convolutional network groupwise combines the spatially convolved representations between the sequencing cycles. In another implementation, subsequent temporal convolution layers of the temporal convolutional network combine successive outputs of previous temporal convolution layers.</p><p id="p-0726" num="0725">The output of the final temporal convolution layer is fed to an output layer that produces an output. The output is used to base call one or more clusters at one or more sequencing cycles.</p><p id="p-0727" num="0726">What follows is a more detailed discussion of the segregated and combinatory convolutions.</p><p id="p-0728" num="0727">Segregated Convolutions</p><p id="p-0729" num="0728">During a forward propagation, the specialized architecture processes information from a plurality of inputs in two stages. In the first stage, segregation convolutions are used to prevent mixing of information between the inputs. In the second stage, combinatory convolutions are used to mix information between the inputs. The results from the second stage are used to make a single inference for the plurality of inputs.</p><p id="p-0730" num="0729">This is different than the batch mode technique where a convolution layer processes multiple inputs in a batch at the same time and makes a corresponding inference for each input in the batch. In contrast, the specialized architecture maps the plurality of inputs to the single inference. The single inference can comprise more than one prediction, such as a classification score for each of the four bases (A, C, T, and G).</p><p id="p-0731" num="0730">In one implementation, the inputs have temporal ordering such that each input is generated at a different time step and has a plurality of input channels. For example, the plurality of inputs can include the following three inputs: a current input generated by a current sequencing cycle at time step (t), a previous input generated by a previous sequencing cycle at time step (t&#x2212;1), and a next input generated by a next sequencing cycle at time step (t+1). In another implementation, each input is respectively derived from the current, previous, and next inputs by one or more previous convolution layers and includes k feature maps.</p><p id="p-0732" num="0731">In one implementation, each input can include the following five input channels: a red image channel (in red), a red distance channel (in yellow), a green image channel (in green), a green distance channel (in purple), and a scaling channel (in blue). In another implementation, each input can include k feature maps produced by a previous convolution layer and each feature map is treated as an input channel.</p><p id="p-0733" num="0732"><figref idref="DRAWINGS">FIG. <b>91</b></figref> depicts one implementation of the segregated convolutions. Segregated convolutions process the plurality of inputs at once by applying a convolution filter to each input in parallel. With the segregated convolutions, the convolution filter combines input channels in a same input and does not combine input channels in different inputs. In one implementation, a same convolution filter is applied to each input in parallel. In another implementation, a different convolution filter is applied to each input in parallel. In some implementations, each spatial convolution layer comprises a bank of k convolution filters, each of which applies to each input in parallel.</p><p id="p-0734" num="0733">Combinatory Convolutions</p><p id="p-0735" num="0734">Combinatory convolutions mix information between different inputs by grouping corresponding input channels of the different inputs and applying a convolution filter to each group. The grouping of the corresponding input channels and application of the convolution filter occurs on a sliding window basis. In this context, a window spans two or more successive input channels representing, for instance, outputs for two successive sequencing cycles. Since the window is a sliding window, most input channels are used in two or more windows.</p><p id="p-0736" num="0735">In some implementations, the different inputs originate from an output sequence produced by a preceding spatial or temporal convolution layer. In the output sequence, the different inputs are arranged as successive outputs and therefore viewed by a next temporal convolution layer as successive inputs. Then, in the next temporal convolution layer, the combinatory convolutions apply the convolution filter to groups of corresponding input channels in the successive inputs.</p><p id="p-0737" num="0736">In one implementation, the successive inputs have temporal ordering such that a current input is generated by a current sequencing cycle at time step (t), a previous input is generated by a previous sequencing cycle at time step (t&#x2212;1), and a next input is generated by a next sequencing cycle at time step (t+1). In another implementation, each successive input is respectively derived from the current, previous, and next inputs by one or more previous convolution layers and includes k feature maps.</p><p id="p-0738" num="0737">In one implementation, each input can include the following five input channels: a red image channel (in red), a red distance channel (in yellow), a green image channel (in green), a green distance channel (in purple), and a scaling channel (in blue). In another implementation, each input can include k feature maps produced by a previous convolution layer and each feature map is treated as an input channel.</p><p id="p-0739" num="0738">The depth B of the convolution filter is dependent upon the number of successive inputs whose corresponding input channels are groupwise convolved by the convolution filter on a sliding window basis. In other words, the depth B is equal to the number of successive inputs in each sliding window and the group size.</p><p id="p-0740" num="0739">In <figref idref="DRAWINGS">FIG. <b>92</b><i>a</i></figref>, corresponding input channels from two successive inputs are combined in each sliding window, and therefore B=2. In <figref idref="DRAWINGS">FIG. <b>92</b><i>b</i></figref>, corresponding input channels from three successive inputs are combined in each sliding window, and therefore B=3.</p><p id="p-0741" num="0740">In one implementation, the sliding windows share a same convolution filter. In another implementation, a different convolution filter is used for each sliding window. In some implementations, each temporal convolution layer comprises a bank of k convolution filters, each of which applies to the successive inputs on a sliding window basis.</p><p id="p-0742" num="0741">Filter Banks</p><p id="p-0743" num="0742"><figref idref="DRAWINGS">FIG. <b>93</b></figref> shows one implementation of convolution layers of the neural network-based base caller <b>1514</b> in which each convolution layer has a bank of convolution filters. In <figref idref="DRAWINGS">FIG. <b>93</b></figref>, five convolution layers are shown, each of which has a bank of 64 convolution filters. In some implementations, each spatial convolution layer has a bank of k convolution filters, where k can be any number such as 1, 2, 8, 64, 128, 256, and so on. In some implementations, each temporal convolution layer has a bank of k convolution filters, where k can be any number such as 1, 2, 8, 64, 128, 256, and so on.</p><p id="p-0744" num="0743">The discussion now turns to the supplemental scaling channel and how it is calculated.</p><p id="p-0745" num="0744">Scaling Channel</p><p id="p-0746" num="0745"><figref idref="DRAWINGS">FIG. <b>94</b></figref> depicts two configurations of the scaling channel that supplements the image channels. The scaling channel is pixel-wise encoded in the input data that is fed to the neural network-based base caller <b>1514</b>. Different cluster sizes and uneven illumination conditions result in a wide range of cluster intensities being extracted. The additive bias supplied by the scaling channel makes cluster intensities comparable across clusters. In other implementations, when the image patches are in the upsampled, subpixel resolution, the scaling channel is encoded on a subpixel-by-subpixel basis.</p><p id="p-0747" num="0746">When a single target cluster is being base called, the scaling channel assigns a same scaling value to all the pixels. When multiple target clusters are being simultaneously base called, the scaling channels assign different scaling values to groups of pixels based on the cluster shape data.</p><p id="p-0748" num="0747">Scaling channel <b>9410</b> has a same scaling value (s1) for all the pixels. Scaling value (s1) is based on a mean intensity of the center pixel that contains the center of the target cluster. In one implementation, the mean intensity is calculated by averaging intensity values of the center pixel observe during two or more preceding sequencing cycles that produced an A and a T base call for the target cluster.</p><p id="p-0749" num="0748">Scaling channel <b>9408</b> has different scaling values (s1, s2, s3, sm) for respective pixel groups attributed to corresponding clusters based on the cluster shape data. Each pixel group includes a central cluster pixel that contains a center of the corresponding cluster. Scaling value for a particular pixel group is based on the mean intensity of its central cluster pixel. In one implementation, the mean intensity is calculated by averaging intensity values of the central cluster pixel observe during two or more preceding sequencing cycles that produced an A and a T base call for the corresponding cluster.</p><p id="p-0750" num="0749">In some implementations, the background pixels are assigned a background scaling value (sb), which can be 0 or 0.1, or some other minimum value.</p><p id="p-0751" num="0750">In one implementation, the scaling channels <b>9406</b> and their scaling values are determined by an intensity scaler <b>9404</b>. The intensity scaler <b>9404</b> uses cluster intensity data <b>9402</b> from preceding sequencing cycles to calculate the mean intensities.</p><p id="p-0752" num="0751">In other implementations, the supplemental scaling channel can be provided as input in a different way, such as prior to or to the last layer of the neural network-based base caller <b>1514</b>, prior to or to the one or more intermediate layers of the neural network-based base caller <b>1514</b>, and as a single value instead of encoding it pixel-wise to match the image size.</p><p id="p-0753" num="0752">The discussion now turns to the input data that is fed to the neural network-based base caller <b>1514</b></p><p id="p-0754" num="0753">Input Data: Image Channels, Distance Channels, and Scaling Channel</p><p id="p-0755" num="0754"><figref idref="DRAWINGS">FIG. <b>95</b><i>a </i></figref>illustrates one implementation of input data <b>9500</b> for a single sequencing cycle that produces a red image and a green image. The input data <b>9500</b> comprises the following:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0755">Red intensity data <b>9502</b> (in red) for pixels in an image patch extracted from the red image. The red intensity data <b>9502</b> is encoded in a red image channel.</li>        <li id="ul0002-0002" num="0756">Red distance data <b>9504</b> (in yellow) that pixel-wise supplements the red intensity data <b>9502</b>. The red distance data <b>9504</b> is encoded in a red distance channel.</li>        <li id="ul0002-0003" num="0757">Green intensity data <b>9506</b> (in green) for pixels in an image patch extracted from the green image. The green intensity data <b>9506</b> is encoded in a green image channel.</li>        <li id="ul0002-0004" num="0758">Green distance data <b>9508</b> (in purple) that pixel-wise supplements the green intensity data <b>9506</b>. The green distance data <b>9508</b> is encoded in a green distance channel.</li>        <li id="ul0002-0005" num="0759">Scaling data <b>9510</b> (in blue) that pixel-wise supplements the red intensity data <b>9502</b> and the green intensity data <b>9506</b>. The scaling data <b>9510</b> is encoded in a scaling channel.</li>    </ul>    </li></ul></p><p id="p-0756" num="0760">In other implementations, the input data can include fewer or greater number of image channels and supplemental distance channels. In one example, for a sequencing run that uses 4-channel chemistry, the input data comprises four image channels for each sequencing cycle and four supplemental distance channels.</p><p id="p-0757" num="0761">The discussion now turns to how the distance channels and the scaling channel contribute to base calling accuracy.</p><p id="p-0758" num="0762">Additive Biasing</p><p id="p-0759" num="0763"><figref idref="DRAWINGS">FIG. <b>95</b><i>b </i></figref>illustrates one implementation of the distance channels supplying additive bias that is incorporated in the feature maps generated from the image channels. This additive bias contributes to base calling accuracy because it is based on pixel center-to-cluster center(s) distances, which are pixel-wise encoded in the distance channels.</p><p id="p-0760" num="0764">On average, around 3&#xd7;3 pixels comprise one cluster. Density at the center of a cluster is expected to be higher than at the fringe because the cluster grows outwards from a substantially central location. Perimeter cluster pixels can contain conflicting signals from nearby clusters. Therefore, the central cluster pixel is considered the maximum intensity region and serves as a beacon that reliably identifies the cluster.</p><p id="p-0761" num="0765">An image patch's pixels depict intensity emissions of a plurality of clusters (e.g., 10 to 200 clusters) and their surround background. Additional clusters incorporate information from a wider radius and contribute to base call prediction by discerning the underlying base whose intensity emissions are depicted in the image patch. In other words, intensity emissions from a group of clusters cumulatively create an intensity pattern that can be assigned to a discrete base (A, C, T, or G).</p><p id="p-0762" num="0766">We observe that explicitly communicating to the convolution filters distance of each pixel from the cluster center(s) in the supplemental distance channels results in higher base calling accuracy. The distance channels convey to the convolution filters which pixels contain the cluster centers and which pixels are farther away from the cluster centers. The convolution filters use this information to assign a sequencing signal to its proper source cluster by attending to (a) the central cluster pixels, their neighboring pixels, and feature maps derived from them more than (b) the perimeter cluster pixels, background pixels, and feature maps derived from them. In one example of the attending, the distance channels supply positive additive biases that are incorporated in feature maps resulting from (a), but supply negative additive biases that are incorporated in feature maps resulting from (b).</p><p id="p-0763" num="0767">The distance channels have the same dimensionality as the image channels. This allows the convolution filters to separately evaluate the image channels and the distance channels within a local receptive field and coherently combine the evaluations.</p><p id="p-0764" num="0768">When a single target cluster is being base called, the distance channels identify only one central cluster pixel at the center of the image patches. When multiple target clusters are being simultaneously base called, the distance channels identify multiple central cluster pixels distributed across the image patches.</p><p id="p-0765" num="0769">A &#x201c;single cluster&#x201d; distance channel applies to an image patch that contains the center of a single target cluster to be base called in its center pixel. The single cluster distance channel includes center-to-center distance of each pixel in the image patch to the single target cluster. In this implementation, the image patch also includes additional clusters that are adjacent to the single target cluster, but the additional clusters are not base called.</p><p id="p-0766" num="0770">A &#x201c;multi-cluster&#x201d; distance channel applies to an image patch that contains the centers of multiple target clusters to be base called in its respective central cluster pixels. The multi-cluster distance channel includes center-to-center distance of each pixel in the image patch to the nearest cluster from among the multiple target clusters. This has the potential of measuring a center-to-center distance to the wrong cluster, but that potential is low.</p><p id="p-0767" num="0771">A &#x201c;multi-cluster shape-based&#x201d; distance channel applies to an image patch that contains the centers of multiple target clusters to be base called in its respective central cluster pixels and for which pixel-to-cluster attribution information is known. The multi-cluster distance channel includes center-to-center distance of each cluster pixel in the image patch to the cluster to which it belongs or is attributed to from among the multiple target clusters. Background pixels can be flagged as background, instead of given a calculated distance.</p><p id="p-0768" num="0772"><figref idref="DRAWINGS">FIG. <b>95</b><i>b </i></figref>also illustrates one implementation of the scaling channel supplying additive bias that is incorporated in the feature maps generated from the image channels. This additive bias contributes to base calling accuracy because it is based on mean intensities of central cluster pixel(s), which are pixel-wise encoded in the scaling channel. The discussion about additive biasing in the context of the distance channels analogously applies to the scaling channel.</p><heading id="h-0019" level="1">Example of Additive Biasing</heading><p id="p-0769" num="0773"><figref idref="DRAWINGS">FIG. <b>95</b><i>b </i></figref>further shows an example of how the additive biases are derived from the distance and scaling channels and incorporated into the features maps generated from the image channels.</p><p id="p-0770" num="0774">In <figref idref="DRAWINGS">FIG. <b>95</b><i>b</i></figref>, convolution filter i <b>9514</b> evaluates a local receptive field <b>9512</b> (in magenta) across the two image channels <b>9502</b> and <b>9506</b>, the two distance channels <b>9504</b> and <b>9508</b>, and the scaling channel <b>9510</b>. Because the distance and scaling channels are separately encoded, the additive biasing occurs when the intermediate outputs <b>9516</b><i>a</i>-<i>e </i>of each of the channel-specific convolution kernels (or feature detectors) <b>9516</b><i>a</i>-<i>e </i>(plus bias <b>9516</b><i>f</i>) are channel-wise accumulated <b>9518</b> as the final output/feature map element <b>9520</b> for the local receptive field <b>9512</b>. In this example, the additive biases supplied by the two distance channels <b>9504</b> and <b>9508</b> are the intermediate outputs <b>9516</b><i>b </i>and <b>9516</b><i>d</i>, respectively. The additive bias supplied by the scaling channel <b>9510</b> is the intermediate output <b>9516</b><i>e. </i></p><p id="p-0771" num="0775">The additive biasing guides the feature map compilation process by putting greater emphasis on those features in the image channels that are considered more important and reliable for base calling, i.e., pixel intensities of central cluster pixels and their neighboring pixels. During training, backpropagation of gradients computed from comparison to the ground truth base calls updates weights of the convolution kernels to produce stronger activations for central cluster pixels and their neighboring pixels.</p><p id="p-0772" num="0776">Consider, for example, that a pixel in the group of adjacent pixels covered by the local receptive field <b>9512</b> contains a cluster center, then the distance channels <b>9504</b> and <b>9508</b> reflect the proximity of the pixels to the cluster center. As a result, when the intensity intermediate outputs <b>9516</b><i>a </i>and <b>9516</b><i>c </i>are merged with the distance channel additive biases <b>9516</b><i>b </i>and <b>9516</b><i>d </i>at the channelwise accumulation <b>9518</b>, what results is a positively biased convolved representation <b>9520</b> of the pixels.</p><p id="p-0773" num="0777">In contrast, if the pixels covered by the local receptive field <b>9512</b> are not near a cluster center, then the distance channels <b>9504</b> and <b>9508</b> reflect their separation from the cluster center. As a result, when the intensity intermediate outputs <b>9516</b><i>a </i>and <b>9516</b><i>c </i>are merged with the distance channel additive biases <b>9516</b><i>b </i>and <b>9516</b><i>d </i>at the channelwise accumulation <b>9518</b>, what results is a negatively biased convolved representation <b>9520</b> of the pixels.</p><p id="p-0774" num="0778">Similarly, the scaling channel additive bias <b>9516</b><i>e </i>derived from the scaling channel <b>9510</b> can positively or negatively bias the convolved representation <b>9520</b> of the pixels.</p><p id="p-0775" num="0779">For clarity's sake, <figref idref="DRAWINGS">FIG. <b>95</b><i>b </i></figref>shows application of a single convolution filter i <b>9514</b> on the input data <b>9500</b> for a single sequencing cycle. One skilled in the art will appreciate that the discussion can be extended to multiple convolution filters (e.g., a filter bank of k filters, where k can be 8, 16, 32, 64, 128, 256, and so on), to multiple convolutional layers (e.g., multiple spatial and temporal convolution layers), and multiple sequencing cycles (e.g., t, t+1, t&#x2212;1).</p><p id="p-0776" num="0780">In other implementations, the distance and scaling channels, instead of being separately encoded, are directly applied to the image channels to generate modulated pixel multiplication) since the distance and scaling channels and the image channels have the same dimensionality. In further implementations, weights of the convolution kernels are determined based on the distance and image channels so as to detect most important features in the image channels during the elementwise multiplication. In yet other implementations, instead of being fed to a first layer, the distance and scaling channels are provided as auxiliary input to downstream layers and/or networks (e.g., to a fully-connected network or a classification layer). In yet further implementations, the distance and scaling channels are fed to the first layer and re-fed to the downstream layers and/or networks (e.g., via a residual connection).</p><p id="p-0777" num="0781">The discussion above is for 2D input data with k input channels. The extension to 3D input will be appreciated by one skilled in the art. Briefly, volumetric input is a 4D tensor with dimensions k&#xd7;l&#xd7;w&#xd7;h, with l being the additional dimension, length. Each individual kernel is a 4D tensor swept in a 4D tensor, resulting in a 3D tensor (the channel dimension is collapsed because it is not swept across).</p><p id="p-0778" num="0782">In other implementations, when the input data <b>9500</b> is in the upsampled, subpixel resolution, the distance and scaling channels are separately encoded on a subpixel-by-subpixel basis and the additive biasing occurs at the subpixel level.</p><p id="p-0779" num="0783">Base Calling Using the Specialized Architecture and the Input Data</p><p id="p-0780" num="0784">The discussion now turns to how the specialized architecture and the input data are used for the neural network-based base calling.</p><p id="p-0781" num="0785">Single Cluster Base Calling</p><p id="p-0782" num="0786"><figref idref="DRAWINGS">FIGS. <b>96</b><i>a</i>, <b>96</b><i>b</i>, and <b>96</b><i>c </i></figref>depict one implementation of base calling a single target cluster. The specialized architecture processes the input data for three sequencing cycles, namely, a current (time t) sequencing cycle to be base called, a previous (time t&#x2212;1) sequencing cycle, and a next (time t+1) sequencing cycle and produces a base call for the single target cluster at the current (time t) sequencing cycle.</p><p id="p-0783" num="0787"><figref idref="DRAWINGS">FIGS. <b>96</b><i>a </i>and <b>96</b><i>b </i></figref>show the spatial convolution layers. <figref idref="DRAWINGS">FIG. <b>96</b><i>c </i></figref>shows the temporal convolution layers, along with some other non-convolution layers. In <figref idref="DRAWINGS">FIGS. <b>96</b><i>a </i>and <b>96</b><i>b</i></figref>, vertical dotted lines demarcate spatial convolution layers from the feature maps and horizontal dashdotted lines demarcate the three convolution pipelines corresponding to the three sequencing cycles.</p><p id="p-0784" num="0788">For each sequencing cycle, the input data includes a tensor of dimensionality n&#xd7;n&#xd7;m (e.g., the input tensor <b>9500</b> in <figref idref="DRAWINGS">FIG. <b>95</b><i>a</i></figref>), where n represents the width and height of a square tensor and in represents the number of input channels, making the dimensionality of the input data for the three cycles n&#xd7;n&#xd7;m&#xd7;t.</p><p id="p-0785" num="0789">Here, each per-cycle tensor contains, in the center pixel of its image channels, a center of the single target cluster. It also depicts intensity emissions of the single target cluster, of some adjacent clusters, and of their surrounding background captured in each of the image channels at a particular sequencing cycle. In <figref idref="DRAWINGS">FIG. <b>96</b><i>a</i></figref>, two example image channels are depicted, namely, the red image channel and the green image channel.</p><p id="p-0786" num="0790">Each per-cycle tensor also includes distance channels that supplement corresponding image channels (e.g., a red distance channel and a green distance channel). The distance channels identify center-to-center distance of each pixel in the corresponding image channels to the single target cluster. Each per-cycle tensor further includes a scaling channel that pixel-wise scales intensity values in each of the image channels.</p><p id="p-0787" num="0791">The specialized architecture has five spatial convolution layers and two temporal convolution layers. Each spatial convolution layer applies segregated convolutions using a bank of k convolution filters of dimensionality j&#xd7;j&#xd7;&#x2202;, where j represents the width and height of a square filter and &#x2202; represents its depth. Each temporal convolution layer applies combinatory convolutions using a bank of k convolution filters of dimensionality j&#xd7;j&#xd7;a, where j represents the width and height of a square filter and a represents its depth.</p><p id="p-0788" num="0792">The specialized architecture has pre-classification layers (e.g., a flatten layer and a dense layer) and an output layer (e.g., a softmax classification layer). The pre-classification layers prepare the input for the output layer. The output layer produces the base call for the single target cluster at the current (time t) sequencing cycle.</p><p id="p-0789" num="0793">Consistently Reducing Spatial Dimensionality</p><p id="p-0790" num="0794"><figref idref="DRAWINGS">FIGS. <b>96</b><i>a</i>, <b>96</b><i>b</i>, and <b>96</b><i>c </i></figref>also show the resulting feature maps (convolved representations or intermediate convolved representations or convolved features or activation maps) produced by the convolution filters. Starting from the per-cycle tensors, the spatial dimensionality of the resulting feature maps reduces by a constant step size from one convolution layer to the next, a concept referred to herein as the &#x201c;consistently reducing spatial dimensionality&#x201d;. In <figref idref="DRAWINGS">FIGS. <b>96</b><i>a</i>, <b>96</b><i>b</i>, and <b>96</b><i>c</i></figref>, an example constant step size of two is used for the consistently reducing spatial dimensionality.</p><p id="p-0791" num="0795">The consistently reducing spatial dimensionality is expressed by the following formulation: &#x201c;current feature map spatial dimensionality=previous feature map spatial dimensionality&#x2212;convolution filter spatial dimensionality+1&#x201d;. The consistently reducing spatial dimensionality causes the convolution filters to progressively narrow the focus of attention on the central cluster pixels and their neighboring pixels and generate feature maps with features that capture local dependencies among the central cluster pixels and their neighboring pixels. This in turn helps with accurately base calling the clusters whose centers are contained in the central cluster pixels.</p><p id="p-0792" num="0796">The segregated convolutions of the five spatial convolution layers prevent mixing of information between the three sequencing cycles and maintain the three separate convolution pipelines.</p><p id="p-0793" num="0797">The combinatory convolutions of the two temporal convolution layers mix information between the three sequencing cycles. The first temporal convolution layer convolves over the next and current spatially convolved representations respectively produced for the next and current sequencing cycles by a final spatial convolution layer. This yields a first temporal output. The first temporal convolution layer also convolves over the current and previous spatially convolved representations respectively produced for the current and previous sequencing cycles by the final spatial convolution layer. This yields a second temporal output. The second temporal convolution layer convolves over the first and second temporal outputs and produces a final temporal output.</p><p id="p-0794" num="0798">In some implementations, the final temporal output is fed to the flatten layer to produce a flattened output. The flattened output is then fed to the dense layer to produce a dense output. The dense output is processed by the output layer to produce the base call for the single target cluster at the current (time t) sequencing cycle.</p><p id="p-0795" num="0799">In some implementations, the output layer produces likelihoods (classification scores) of a base incorporated in the single target cluster at the current sequencing cycle being A, C, T, and G, and classifies the base as A, C, T, or G based on the likelihoods (e.g., the base with the maximum likelihood is selected, such the base A in <figref idref="DRAWINGS">FIG. <b>96</b><i>a</i></figref>). In such implementations, the likelihoods are exponentially normalized scores produced by a softmax classification layer and sum to unity.</p><p id="p-0796" num="0800">In some implementations, the output layer derives an output pair for the single target cluster. The output pair identifies a class label of a base incorporated in the single target cluster at the current sequencing cycle being A, C, T, or G, and base calls the single target cluster based on the class label. In one implementation, a class label of 1, 0 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 1, 1 identifies a T base, and a class label of 0, 0 identifies a G base. In another implementation, a class label of 1, 1 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 0.5, 0.5 identifies a T base, and a class label of 0, 0 identifies a G base. In yet another implementation, a class label of 1, 0 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 0.5, 0.5 identifies a T base, and a class label of 0, 0 identifies a G base. In yet further implementation, a class label of 1, 2 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 1, 1 identifies a T base, and a class label of 0, 0 identifies a G base.</p><p id="p-0797" num="0801">In some implementations, the output layer derives a class label for the single target cluster that identifies a base incorporated in the single target cluster at the current sequencing cycle being A, C, T, or G, and base calls the single target cluster based on the class label. In one implementation, a class label of 0.33 identifies an A base, a class label of 0.66 identifies a C base, a class label of 1 identifies a T base, and a class label of 0 identifies a G base. In another implementation, a class label of 0.50 identifies an A base, a class label of 0.75 identifies a C base, a class label of 1 identifies a T base, and a class label of 0.25 identifies a G base.</p><p id="p-0798" num="0802">In some implementations, the output layer derives a single output value, compares the single output value against class value ranges corresponding to bases A, C, T, and G, based on the comparison, assigns the single output value to a particular class value range, and base calls the single target cluster based on the assignment. In one implementation, the single output value is derived using a sigmoid function and the single output value ranges from 0 to 1. In another implementation, a class value range of 0-0.25 represents an A base, a class value range of 0.25-0.50 represents a C base, a class value range of 0.50-0.75 represents a T base, and a class value range of 0.75-1 represents a G base.</p><p id="p-0799" num="0803">One skilled in the art will appreciate that, in other implementations, the specialized architecture can process input data for fewer or greater number of sequencing cycles and can comprise fewer or greater number of spatial and temporal convolution layers. Also, the dimensionality of the input data, the per-cycle tensors in the input data, the convolution filters, the resulting feature maps, and the output can be different. Also, the number of convolution filters in a convolution layer can be different. It can use different padding and striding configurations. It can use a different classification function (e.g., sigmoid or regression) and may or may not include a fully-connected layer. It can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. It can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. It can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous SGD. It can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LSTM or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tan h)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><p id="p-0800" num="0804">Having described single cluster base calling, the discussion now turns to multiple clusters base calling.</p><p id="p-0801" num="0805">Multiple Clusters Base Calling</p><p id="p-0802" num="0806">Depending on the size of the input data and cluster density on the flow cell, anywhere between ten to three hundred thousand clusters are simultaneously base called by the neural network-based base caller <b>1514</b> on a per-input basis. Extending this to the data parallelism and/or model parallelism strategies implemented on parallel processors, using a batch or mini-batch of size ten results in hundred to three million clusters being simultaneously base called on a per-batch basis or per-mini-batch basis.</p><p id="p-0803" num="0807">Depending on the sequencing configuration (e.g., cluster density, number of tiles on the flow cell), a tile includes twenty thousand to three hundred thousand clusters. In another implementation, Illumina's NovaSeq sequencer has up to four million clusters per tile. Therefore, a sequencing image of the tile (tile image) can depict intensity emissions from twenty thousand to three hundred thousand clusters and their surrounding background. So, in one implementation, using input data which includes the entire tile image results in three hundred thousand clusters being simultaneously base called on a per-input basis. In another implementation, using image patches of size 15&#xd7;15 pixels in the input data results in less than hundred clusters being simultaneously base called on a per-input basis. One skilled in the art will appreciate that these numbers can vary depending on the sequencing configuration, the parallelism strategy, the details of the architecture (e.g., based on optimal architecture hyperparameters), and available compute.</p><p id="p-0804" num="0808"><figref idref="DRAWINGS">FIG. <b>97</b></figref> shows one implementation of simultaneously base calling multiple target clusters. The input data has three tensors for the three sequencing cycles discussed above. Each per-cycle tensor (e.g., the input tensor <b>9500</b> in <figref idref="DRAWINGS">FIG. <b>95</b><i>a</i></figref>) depicts intensity emissions of multiple target clusters to be base called and their surrounding background captured in each of the image channels at a particular sequencing cycle. In other implementations, some additional adjacent clusters, which are not base called, are also included for context.</p><p id="p-0805" num="0809">In the multi-cluster base calling implementation, each per-cycle tensor includes distance channels that supplement corresponding image channels (e.g., a red distance channel and a green distance channel). The distance channels identify center-to-center distance of each pixel in the corresponding image channels to the nearest cluster from among the multiple target clusters.</p><p id="p-0806" num="0810">In the multi-cluster shape-based base calling implementation, each per-cycle tensor includes distance channels that supplement corresponding image channels (e.g., a red distance channel and a green distance channel). The distance channels identify center-to-center distance of each cluster pixel in the corresponding image channels to the cluster to which it belongs or is attributed to from among the multiple target clusters.</p><p id="p-0807" num="0811">Each per-cycle tensor further includes a scaling channel that pixel-wise scales intensity values in each of the image channels.</p><p id="p-0808" num="0812">In <figref idref="DRAWINGS">FIG. <b>97</b></figref>, the spatial dimensionality of each per-cycle tensor is great than that shown in <figref idref="DRAWINGS">FIG. <b>96</b><i>a</i></figref>. That is, in the single target cluster base calling implementation in <figref idref="DRAWINGS">FIG. <b>96</b><i>a</i></figref>, the spatial dimensionality of each per-cycle tensor is 15&#xd7;15, whereas in the multiple cluster base calling implementation in <figref idref="DRAWINGS">FIG. <b>97</b></figref>, the spatial dimensionality of each per-cycle tensor is 114&#xd7;114. Having greater amount of pixelated data that depicts intensity emissions of additional clusters improves the accuracy of base calls simultaneously predicted for the multiple clusters, according to some implementations.</p><p id="p-0809" num="0813">Avoiding Redundant Convolutions</p><p id="p-0810" num="0814">Furthermore, the image channels in each per-cycle tensor are obtained from the image patches extracted from the sequencing images. In some implementations, there are overlapping pixels between extracted image patches that are spatially contiguous (e.g., left, right, top, and bottom contiguous). Accordingly, in one implementation, the overlapping pixels are not subjected to redundant convolutions and results from a prior convolution are reused in later instances when the overlapping pixels are part of the subsequent inputs.</p><p id="p-0811" num="0815">Consider, for example, that a first image patch of size n&#xd7;n pixels is extracted from a sequencing image and a second image patch of size m&#xd7;m pixels is also extracted from the same sequencing image, such that the first and second image patches are spatially contiguous and share an overlapping region of o&#xd7;o pixels. Further consider that the o&#xd7;o pixels are convolved as part of the first image patch to produce a first convolved representation that is stored in memory. Then, when the second image patch is convolved, the o&#xd7;o pixels are not convolved again and instead the first convolved representation is retrieved from memory and reused. In some implementations, n=m. In other implementations, they are not equal.</p><p id="p-0812" num="0816">The input data is then processed through the spatial and temporal convolution layers of the specialized architecture to produce a final temporal output of dimensionality w&#xd7;w&#xd7;k. Here too, under the consistently reducing spatial dimensionality phenomenon, the spatial dimensionality is reduced by a constant step size of two at each convolution layer. That is, starting with a n&#xd7;n spatial dimensionality of the input data, a w&#xd7;w spatial dimensionality of the final temporal output is derived.</p><p id="p-0813" num="0817">Then, based on the final temporal output of spatial dimensionality w&#xd7;w, an output layer produces a base call for each unit in the w&#xd7;w set of units. In one implementation, the output layer is a softmax layer that produces four-way classification scores for the four bases (A, C, T, and G) on a unit-by-unit basis. That is, each unit in the w&#xd7;w set of units is assigned a base call based on the maximum classification score in a corresponding softmax quadruple, as depicted in <figref idref="DRAWINGS">FIG. <b>97</b></figref>. In some implementations, the w&#xd7;w set of units is derived as a result of processing the final temporal output through a flatten layer and a dense layer to produce a flattened output and a dense output, respectively. In such implementations, the flattened output has w&#xd7;w&#xd7;k elements and the dense output has w&#xd7;w elements that form the w&#xd7;w set of units.</p><p id="p-0814" num="0818">Base calls for the multiple target clusters are obtained by identifying which of the base called units in the w&#xd7;w set of units coincide with or correspond to central cluster pixels, i.e., pixels in the input data that contain the respective centers of the multiple target clusters. A given target cluster is assigned the base call of the unit that coincides with or corresponds to the pixel that contains the center of the given target cluster. In other words, base calls of units that do not coincide with or correspond to the central cluster pixels are filtered out. This functionality is operationalized by a base call filtering layer, which is part of the specialized architecture in some implementations, or implemented as a post-processing module in other implementations.</p><p id="p-0815" num="0819">In other implementations, base calls for the multiple target clusters are obtained by identifying which groups of base called units in the w&#xd7;w set of units cover a same cluster, i.e., identifying pixel groups in the input data that depict a same cluster. Then, for each cluster and its corresponding pixel group, an average of classification scores (softmax probabilities) of the respective four base classes (A, C, T, and G) is calculated across pixels in the pixel group and the base class that has the highest average classification score is selected for base calling the cluster.</p><p id="p-0816" num="0820">During training, in some implementations, the ground truth comparison and error computation occurs only for those units that coincide with or correspond to the central cluster pixels, such that their predicted base calls are evaluated against the correct base calls identified as ground truth labels.</p><p id="p-0817" num="0821">Having described multiple clusters base calling, the discussion now turns to multiple clusters and multiple cycles base calling.</p><p id="p-0818" num="0822">Multiple Clusters and Multiple Cycles Base Calling</p><p id="p-0819" num="0823"><figref idref="DRAWINGS">FIG. <b>98</b></figref> shows one implementation of simultaneously base calling multiple target clusters at a plurality of successive sequencing cycles, thereby simultaneously producing a base call sequence for each of the multiple target clusters.</p><p id="p-0820" num="0824">In the single and multiple base calling implementations discussed above, base call at one sequencing cycle (the current (time t) sequencing cycle) is predicted using data for three sequencing cycles (the current (time t), the previous/left flanking (time t&#x2212;1), and the next/right flanking (time t+1) sequencing cycles), where the right and left flanking sequencing cycles provide sequence-specific context for base triplet motifs and second order contribution of pre-phasing and phasing signals. This relationship is expressed by the following formulation: &#x201c;number of sequencing cycles for which data is included in the input data (t)=number of sequencing cycles being base called (y)+number of right and left flanking sequencing cycles (x).&#x201d;</p><p id="p-0821" num="0825">In <figref idref="DRAWINGS">FIG. <b>98</b></figref>, the input data includes t per-cycle tensors for t sequencing cycles, making its dimensionality n&#xd7;n&#xd7;m&#xd7;t, where n=114, m=5, and t=15. In other implementations, these dimensionalities are different. Of the t sequencing cycles, the t<sup>th </sup>sequencing cycle and the first sequencing cycle serve as right and left flanking contexts x, and y sequencing cycles between them are base called. Thus, y=13, x=2, and t=y+x. Each per-cycle tensor includes image channels, corresponding distance channels, and a scaling channel, such as the input tensor <b>9500</b> in <figref idref="DRAWINGS">FIG. <b>95</b></figref><i>a. </i></p><p id="p-0822" num="0826">The input data with t per-cycle tensors is then processed through the spatial and temporal convolution layers of the specialized architecture to produce y final temporal outputs, each of which corresponds to a respective one of they sequencing cycles being base called. Each of they final temporal outputs has a dimensionality of w&#xd7;w&#xd7;k. Here too, under the consistently reducing spatial dimensionality phenomenon, the spatial dimensionality is reduced by a constant step size of two at each convolution layer. That is, starting with a n&#xd7;n spatial dimensionality of the input data, a w&#xd7;w spatial dimensionality of each of they final temporal outputs is derived.</p><p id="p-0823" num="0827">Then, each of they final temporal outputs is processed in parallel by an output layer. For each of they final temporal outputs, the output layer produces a base call for each unit in the w&#xd7;w set of units. In one implementation, the output layer is a softmax layer that produces four-way classification scores for the four bases (A, C, T, and G) on a unit-by-unit basis. That is, each unit in the w&#xd7;w set of units is assigned a base call based on the maximum classification score in a corresponding softmax quadruple, as depicted in <figref idref="DRAWINGS">FIG. <b>97</b></figref>. In some implementations, the w&#xd7;w set of units is derived for each of they final temporal outputs as a result of respectively processing the later through a flatten layer and a dense layer to produce corresponding flattened outputs and dense outputs. In such implementations, each flattened output has w&#xd7;w&#xd7;k elements and each dense output has w&#xd7;w elements that form the w&#xd7;w set of units.</p><p id="p-0824" num="0828">For each of they sequencing cycles, base calls for the multiple target clusters are obtained by identifying which of the base called units in the corresponding w&#xd7;w set of units coincide with or correspond to central cluster pixels, i.e., pixels in the input data that contain the respective centers of the multiple target clusters. A given target cluster is assigned the base call of the unit that coincides with or corresponds to the pixel that contains the center of the given target cluster. In other words, base calls of units that do not coincide with or correspond to the central cluster pixels are filtered out. This functionality is operationalized by a base call filtering layer, which is part of the specialized architecture in some implementations, or implemented as a post-processing module in other implementations.</p><p id="p-0825" num="0829">During training, in some implementations, the ground truth comparison and error computation occurs only for those units that coincide with or correspond to the central cluster pixels, such that their predicted base calls are evaluated against the correct base calls identified as ground truth labels.</p><p id="p-0826" num="0830">On a per-input basis, what results is a base call for each of the multiple target clusters at each of they sequencing cycles, i.e., a base call sequence of lengthy for each of the multiple target clusters. In other implementations, y is 20, 30, 50, 150, 300, and so on. One skilled in the art will appreciate that these numbers can vary depending on the sequencing configuration, the parallelism strategy, the details of the architecture (e.g., based on optimal architecture hyperparameters), and available compute.</p><p id="p-0827" num="0831">End-to-End Dimensionality Diagrams</p><p id="p-0828" num="0832">The following discussion uses dimensionality diagrams to illustrate different implementations of underlying data dimensionality changes involved in producing base calls from image data, together with dimensionality of data operators that effectuate the said data dimensionality changes.</p><p id="p-0829" num="0833">In <figref idref="DRAWINGS">FIGS. <b>99</b>, <b>100</b>, and <b>101</b></figref>, rectangles represent data operators like spatial and temporal convolution layers and softmax classification layer, and rounded corner rectangles represent data (e.g., feature maps) produced by the data operators.</p><p id="p-0830" num="0834"><figref idref="DRAWINGS">FIG. <b>99</b></figref> illustrates the dimensionality diagram <b>9900</b> for the single cluster base calling implementation. Note that the &#x201c;cycle dimension&#x201d; of the input is three and continues to be that for the resulting feature maps up until the first temporal convolution layer. Cycle dimension of three presents the three sequencing cycles, and its continuity represents that feature maps for the three sequencing cycles are separately generated and convolved upon and no features are mixed between the three sequencing cycles. The segregated convolution pipelines are effectuated by the depth-wise segregated convolution filters of the spatial convolution layers. Note that the &#x201c;depth dimensionality&#x201d; of the depth-wise segregated convolution filters of the spatial convolution layers is one. This is what enables the depth-wise segregated convolution filters to convolve over data and resulting feature maps of only a given sequencing cycle, i.e., intra-cycle, and prevents them from convolving over data and resulting feature maps of any other sequencing cycle.</p><p id="p-0831" num="0835">In contrast, note that the depth dimensionality of the depth-wise combinatory convolution filters of the temporal convolution layers is two. This is what enables the depth-wise combinatory convolution filters to groupwise convolve over resulting features maps from multiple sequencing cycles and mix features between the sequencing cycles.</p><p id="p-0832" num="0836">Also note the consistent reduction in the &#x201c;spatial dimensionality&#x201d; by a constant step size of two.</p><p id="p-0833" num="0837">Further, a vector with four elements is exponentially normalized by the softmax layer to produce classification scores (i.e., confidence scores, probabilities, likelihoods, softmax scores) for the four bases (A, C, T, and G). The base with the highest (maximum) softmax score is assigned to the single target cluster being base called at the current sequencing cycle.</p><p id="p-0834" num="0838">One skilled in the art will appreciate that, in other implementations, the illustrated dimensionalities can vary depending on the sequencing configuration, the parallelism strategy, the details of the architecture (e.g., based on optimal architecture hyperparameters), and available compute.</p><p id="p-0835" num="0839"><figref idref="DRAWINGS">FIG. <b>100</b></figref> illustrates the dimensionality diagram <b>10000</b> for the multiple clusters, single sequencing cycle base calling implementation. The above discussion about the cycle, depth, and spatial dimensionality with respect to the single cluster base calling applies to this implementation.</p><p id="p-0836" num="0840">Here, the softmax layer operates independently on each of the 10,000 units and produces a respective quadruple of softmax scores for each of the 10,000 units. The quadruple corresponds to the four bases (A, C, T, and G). In some implementations, the 10,000 units are derived from the transformation of 64,0000 flattened units to 10,000 dense units.</p><p id="p-0837" num="0841">Then, from the softmax score quadruple of each of the 10,000 units, the base with the highest softmax score in each quadruple is assigned to a respective one of the 10,000 units.</p><p id="p-0838" num="0842">Then, of the 10,000 units, those 2500 units are selected which correspond the 2,500 central cluster pixels containing respective centers of the 2,500 target clusters being simultaneously base called at the current sequencing cycle. The bases assigned to the selected 2,500 units are in turn assigned to the corresponding ones of the 2,500 target clusters.</p><p id="p-0839" num="0843">One skilled in the art will appreciate that, in other implementations, the illustrated dimensionalities can vary depending on the sequencing configuration, the parallelism strategy, the details of the architecture (e.g., based on optimal architecture hyperparameters), and available compute.</p><p id="p-0840" num="0844"><figref idref="DRAWINGS">FIG. <b>101</b></figref> illustrates the dimensionality diagram <b>10100</b> for the multiple clusters, multiple sequencing cycles base calling implementation. The above discussion about the cycle, depth, and spatial dimensionality with respect to the single cluster base calling applies to this implementation.</p><p id="p-0841" num="0845">Further, the above discussion about the softmax-based base call classification with respect to the multiple clusters base calling applies here too. However, here, the softmax-based base call classification of the 2,500 target clusters occurs in parallel for each of the thirteen sequencing cycles base called, thereby simultaneously producing thirteen base calls for each of the 2,500 target clusters.</p><p id="p-0842" num="0846">One skilled in the art will appreciate that, in other implementations, the illustrated dimensionalities can vary depending on the sequencing configuration, the parallelism strategy, the details of the architecture (e.g., based on optimal architecture hyperparameters), and available compute.</p><p id="p-0843" num="0847">Arrayed Input v/s Stacked Input</p><p id="p-0844" num="0848">The discussion now turns to the two configurations in which the multi-cycle input data to the neural network-based caller can be arranged. The first configuration is called &#x201c;arrayed input&#x201d; and the second configuration is called &#x201c;stacked input&#x201d;. The arrayed input is shown in <figref idref="DRAWINGS">FIG. <b>102</b><i>a </i></figref>and is discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>96</b><i>a </i></figref>to <b>101</b>. The arrayed input encodes each sequencing cycle's input in a separate column/block because image patches in the per-cycle inputs are misaligned with respect to each other due to residual registration error. The specialized architecture is used with the arrayed input to segregate processing of each of the separate columns/blocks. Also, the distance channels are calculated using the transformed cluster centers to account for the misalignments between image patches in a cycle and between image patches across cycles.</p><p id="p-0845" num="0849">In contrast, the stacked input, shown in <figref idref="DRAWINGS">FIG. <b>102</b><i>b</i></figref>, encodes the inputs from different sequencing cycles in a single column/block. In one implementation, this obviates the need of using the specialized architecture because the image patches in the stacked input are aligned with each other through affine transformation and intensity interpolation, which eliminate the inter-cycle and intra-cycle residual registration error. In some implementations, the stacked input has a common scaling channel for all the inputs.</p><p id="p-0846" num="0850">In another implementation, intensity interpolation is used to reframe or shift the image patches such that the center of the center pixel of each image patch coincides with the center of the single target cluster being base called. This obviates the need of using the supplemental distance channels because all the non-center pixels are equidistant from the center of the single target cluster. Stacked input without the distance channels is referred to herein as the &#x201c;reframed input&#x201d; and is illustrated in <figref idref="DRAWINGS">FIG. <b>104</b></figref>.</p><p id="p-0847" num="0851">However, the reframing may not be feasible with base calling implementations involving multiple clusters because there the image patches contain multiple central cluster pixels that are base called. Stacked input without the distance channels and without the reframing is referred to herein as the &#x201c;aligned input&#x201d; and is illustrated in <figref idref="DRAWINGS">FIGS. <b>105</b> and <b>106</b></figref>. Aligned input may be used when calculation of the distance channels is not desired (e.g., due to compute limitations) and reframing is not feasible.</p><p id="p-0848" num="0852">The following section discusses various base calling implementations that do not use the specialized architecture and the supplemental distance channels, and instead using standard convolution layers and filters.</p><p id="p-0849" num="0853">Reframed Input: Aliened Image Patches without the Distance Channels</p><p id="p-0850" num="0854"><figref idref="DRAWINGS">FIG. <b>103</b><i>a </i></figref>depicts one implementation of reframing <b>10300</b><i>a </i>pixels of an image patch <b>10302</b> to center a center of a target cluster being base called in a center pixel. The center of the target cluster (in purple) falls within the center pixel of the image patch <b>10302</b>, but is at an offset (in red) from the center pixel's center, as depicted in <figref idref="DRAWINGS">FIG. <b>10300</b></figref><i>a. </i></p><p id="p-0851" num="0855">To eliminate the offset, a reframer <b>10304</b> shifts the image patch <b>10302</b> by interpolating intensity of the pixels to compensate for the refraining and produces a reframed/shifted image patch <b>10306</b>. In the shifted image patch <b>10306</b>, the center of the center pixel coincides with the center of the target cluster. Also, the non-center pixels are equidistant from the center of the target cluster. The interpolation can be performed by nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage. These techniques are described in detail in Appendix entitled &#x201c;Intensity Extraction Methods&#x201d;.</p><p id="p-0852" num="0856"><figref idref="DRAWINGS">FIG. <b>103</b><i>b </i></figref>depicts another example reframed/shifted image patch <b>10300</b><i>b </i>in which (i) the center of the center pixel coincides with the center of the target cluster and (ii) the non-center pixels are equidistant from the center of the target cluster. These two factors obviate the need of providing a supplemental distance channel because all the non-center pixels have the same degree of proximity to the center of the target cluster.</p><p id="p-0853" num="0857"><figref idref="DRAWINGS">FIG. <b>104</b></figref> shows one implementation of base calling a single target cluster at a current sequencing cycle using a standard convolution neural network and the reframed input. In the illustrated implementation, the reframed input includes a current image patch set for a current (t) sequencing cycle being base called, a previous image patch set for a previous (t&#x2212;1) sequencing cycle, and a next image patch set for a next (t+1) sequencing cycle. Each image patch set has an image patch for a respective one of one or more image channels. <figref idref="DRAWINGS">FIG. <b>104</b></figref> depicts two image channels, a red channel and a green channel. Each image patch has pixel intensity data for pixels covering a target cluster being base called, some adjacent clusters, and their surrounding background. The reframed input also includes a common scaling channel.</p><p id="p-0854" num="0858">The reframed input does not include any distance channels because the image patches are reframed or shifted to center at the center the target cluster, as explained above with respect to <figref idref="DRAWINGS">FIGS. <b>103</b><i>a</i>-<i>b</i></figref>. Also, the image patches are aligned with each other to remove inter-cycle and intra-cycle residual registration error. In one implementation, this is done using affine transformation and intensity interpolation, additional details of which can be found in Appendices 1, 2, 3, and 4. These factors obviate the need of using the specialized architecture, and instead a standard convolutional neural network is used with the reframed input.</p><p id="p-0855" num="0859">In the illustrated implementation, the standard convolutional neural network <b>10400</b> includes seven standard convolution layers that use standard convolution filters. This means that there are no segregated convolution pipelines to prevent mixing of data between the sequencing cycles (since the data is aligned and can be mixed). In some implementations, the consistently reducing spatial dimensionality phenomenon is used to teach the standard convolution filters to attend to the central cluster center and its neighboring pixels more than to other pixels.</p><p id="p-0856" num="0860">The reframed input is then processed through the standard convolution layers to produce a final convolved representation. Based on the final convolved representation, the base call for the target cluster at the current sequencing cycle is obtained in the similar fashion using flatten, dense, and classification layers as discussed above with respect to <figref idref="DRAWINGS">FIG. <b>96</b></figref><i>c. </i></p><p id="p-0857" num="0861">In some implementations, the process is iterated over a plurality of sequencing cycles to produce a sequence of base calls for the target cluster.</p><p id="p-0858" num="0862">In other implementations, the process is iterated over a plurality of sequencing cycles for a plurality of target clusters to produce a sequence of base calls for each target cluster in the plurality of target clusters.</p><p id="p-0859" num="0863">Aliened Input: Aligned Image Patches without the Distance Channels and the Reframing</p><p id="p-0860" num="0864"><figref idref="DRAWINGS">FIG. <b>105</b></figref> shows one implementation of base calling multiple target clusters at the current sequencing cycle using the standard convolution neural network and the aligned input. The reframing is not feasible here because the image patches contain multiple central cluster pixels that are being base called. As a result, the image patches in the aligned input are not reframed. Further, the supplemental distance channels are not included due to compute considerations, according to one implementation.</p><p id="p-0861" num="0865">The aligned input is then processed through the standard convolution layers to produce a final convolved representation. Based on the final convolved representation, a base call for each of the target clusters is obtained at the current sequencing cycle in the similar fashion using flatten (optional), dense (optional), classification, and base call filtering layers as discussed above with respect to <figref idref="DRAWINGS">FIG. <b>97</b></figref>.</p><p id="p-0862" num="0866"><figref idref="DRAWINGS">FIG. <b>106</b></figref> shows one implementation of base calling multiple target clusters at a plurality of sequencing cycles using the standard convolution neural network and the aligned input. The aligned input is processed through the standard convolution layers to produce a final convolved representation for each of they sequencing cycles being base called. Based on they final convolved representations, a base call for each of the target clusters is obtained for each of they sequencing cycles being base called in the similar fashion using flatten (optional), dense (optional), classification, and base call filtering layers as discussed above with respect to <figref idref="DRAWINGS">FIG. <b>98</b></figref>.</p><p id="p-0863" num="0867">One skilled in the art will appreciate that, in other implementations, the standard convolutional neural network can process reframed input for fewer or greater number of sequencing cycles and can comprise fewer or greater number of standard convolution layers. Also, the dimensionality of the reframed input, the per-cycle tensors in the reframed input, the convolution filters, the resulting feature maps, and the output can be different. Also, the number of convolution filters in a convolution layer can be different. It can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. It can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. It can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous SGD. It can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LS&#x2122; or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tan h)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><p id="p-0864" num="0868">Training</p><p id="p-0865" num="0869"><figref idref="DRAWINGS">FIG. <b>107</b></figref> shows one implementation of training <b>10700</b> the neural network-based base caller <b>1514</b>. With both the specialized and standard architectures, the neural network-based base caller <b>1514</b> is trained using a backpropagation-based gradient update technique that compares the predicted base calls <b>10704</b> against the correct base calls <b>10708</b> and computes an error <b>10706</b> based on the comparison. The error <b>10706</b> is then used to calculate gradients, which are applied to the weights and parameters of the neural network-based base caller <b>1514</b> during backward propagation <b>10710</b>. The training <b>10700</b> is operationalized by the trainer <b>1510</b> using a stochastic gradient update algorithm such as ADAM.</p><p id="p-0866" num="0870">The trainer <b>1510</b> uses training data <b>10702</b> (derived from the sequencing images <b>108</b>) to train the neural network-based base caller <b>1514</b> over thousands and millions of iterations of the forward propagation <b>10712</b> that produces the predicted base calls <b>10704</b> and the backward propagation <b>10710</b> that updates the weights and parameters based on the error <b>10706</b>. Additional details about the training <b>10700</b> can be found in Appendix entitled &#x201c;Deep Learning Tools&#x201d;.</p><p id="p-0867" num="0871">CNN&#x2014;RNN-Based Base Caller</p><p id="p-0868" num="0872">Hybrid Neural Network</p><p id="p-0869" num="0873"><figref idref="DRAWINGS">FIG. <b>108</b><i>a </i></figref>depicts one implementation of a hybrid neural network <b>10800</b><i>a </i>that is used as the neural network-based base caller <b>1514</b>. The hybrid neural network <b>10800</b><i>a </i>comprises at least one convolution module <b>10804</b> (or convolutional neural network (CNN)) and at least one recurrent module <b>10808</b> (or recurrent neural network (RNN)). The recurrent module <b>10808</b> uses and/or receives inputs from the convolution module <b>10804</b>.</p><p id="p-0870" num="0874">The convolution module <b>10804</b> processes input data <b>10802</b> through one or more convolution layers and produces convolution output <b>10806</b>. In one implementation, the input data <b>10802</b> includes only image channels or image data as the main input, as discussed above in the Section entitled &#x201c;Input&#x201d;. The image data fed to the hybrid neural network <b>10800</b><i>a </i>can be the same as the image data <b>7902</b> described above.</p><p id="p-0871" num="0875">In another implementation, the input data <b>10802</b>, in addition to the image channels or the image data, also includes supplemental channels such as the distance channels, the scaling channel, the cluster center coordinates, and/or cluster attribution information, as discussed above in the Section entitled &#x201c;Input&#x201d;.</p><p id="p-0872" num="0876">The image data (i.e., the input data <b>10802</b>) depicts intensity emissions of one or more clusters and their surrounding background. The convolution module <b>10804</b> processes the image data for a series of sequencing cycles of a sequencing run through the convolution layers and produces one or more convolved representations of the image data (i.e., the convolved output <b>10806</b>).</p><p id="p-0873" num="0877">The series of sequencing cycles can include image data for t sequencing cycles that are to be base called, where t is any number between 1 and 1000. We observe accurate base calling results when t is between fifteen and twenty-one.</p><p id="p-0874" num="0878">The recurrent module <b>10810</b> convolves the convolved output <b>10806</b> and produces recurrent output <b>10810</b>. In particular, the recurrent module <b>10810</b> produces current hidden state representations (i.e., the recurrent output <b>10810</b>) based on convolving the convolved representations and previous hidden state representations.</p><p id="p-0875" num="0879">In one implementation, the recurrent module <b>10810</b> applies three-dimensional (3D) convolutions to the convolved representations and previous hidden state representations and produces the current hidden state representations, mathematically formulated as:</p><p id="p-0876" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>h</i><sub>t</sub><i>=W</i>1<sub>3DCONV</sub><i>V</i><sub>t</sub><i>+W</i>2<sub>3DCONV</sub><i>h</i><sub>t-1</sub>, where<?in-line-formulae description="In-line Formulae" end="tail"?><ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0880">h<sub>t </sub>represents a current hidden state representation produced at a current time step t,</li>    <li id="ul0003-0002" num="0881">V<sub>t </sub>represents a set or group of convolved representations that form an input volume at a current sliding window at the current time step t,</li>    <li id="ul0003-0003" num="0882">W1<sub>3DCONV </sub>represents weights of a first 3D convolution filter applied to V<sub>t</sub>,</li>    <li id="ul0003-0004" num="0883">h<sub>t-1 </sub>represents a previous hidden state representation produced at a previous time step t&#x2212;1, and</li>    <li id="ul0003-0005" num="0884">W2<sub>3DCONV </sub>represents weights of a second 3D convolution filter applied to h<sub>t-1</sub>.</li></ul></p><p id="p-0877" num="0885">In some implementations, W1<sub>3DCONV </sub>and W2<sub>3DCONV </sub>are the same because the weights are shared.</p><p id="p-0878" num="0886">An output module <b>10812</b> then produces base calls <b>10814</b> based on the recurrent output <b>10810</b>. In some implementations, the output module <b>10812</b> comprises one or more fully-connected layers and a classification layer (e.g., softmax). In such implementations, the current hidden state representations are processed through the fully-connected layers and the outputs of the fully-connected layers are processed through the classification layer to produce the base calls <b>10814</b>.</p><p id="p-0879" num="0887">The base calls <b>10814</b> include a base call for at least one of the clusters and for at least one of the sequencing cycles. In some implementations, the base calls <b>10814</b> include a base call for each of the clusters and for each of sequencing cycles. So, for example, when the input data <b>10802</b> includes image data for twenty-five clusters and for fifteen sequencing cycles, the base calls <b>10802</b> include a base call sequence of fifteen base calls for each of the twenty-five clusters.</p><p id="p-0880" num="0888">3D Convolutions</p><p id="p-0881" num="0889"><figref idref="DRAWINGS">FIG. <b>108</b><i>b </i></figref>shows one implementation of 3D convolutions <b>10800</b><i>b </i>used by the recurrent module <b>10810</b> of the hybrid neural network <b>10800</b><i>b </i>to produce the current hidden state representations.</p><p id="p-0882" num="0890">A 3D convolution is a mathematical operation where each voxel present in the input volume is multiplied by a voxel in the equivalent position of the convolution kernel. At the end, the sum of the results is added to the output volume. In <figref idref="DRAWINGS">FIG. <b>108</b><i>b</i></figref>, it is possible to observe the representation of the 3D convolution operation, where the voxels <b>10816</b><i>a </i>highlighted in the input <b>10816</b> are multiplied with their respective voxels in the kernel <b>10818</b>. After these calculations, their sum <b>10820</b><i>a </i>is added to the output <b>10820</b>.</p><p id="p-0883" num="0891">Since the coordinates of the input volume are given by (x, y, z) and the convolution kernel has size (P, Q, R), the 3D convolution operation can be mathematically defined as:</p><p id="p-0884" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mi>O</mi>    <mrow>     <mi>x</mi>     <mo>&#x2062;</mo>     <mi>y</mi>     <mo>&#x2062;</mo>     <mi>z</mi>    </mrow>   </msub>   <mo>=</mo>   <mrow>    <munderover>     <mo>&#x2211;</mo>     <mrow>      <mi>p</mi>      <mo>=</mo>      <mn>0</mn>     </mrow>     <mrow>      <mi>P</mi>      <mo>-</mo>      <mn>1</mn>     </mrow>    </munderover>    <mrow>     <munderover>      <mo>&#x2211;</mo>      <mrow>       <mi>q</mi>       <mo>=</mo>       <mn>0</mn>      </mrow>      <mrow>       <mi>Q</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>     </munderover>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>r</mi>        <mo>=</mo>        <mn>0</mn>       </mrow>       <mrow>        <mi>R</mi>        <mo>-</mo>        <mn>1</mn>       </mrow>      </munderover>      <mrow>       <msub>        <mi>K</mi>        <mrow>         <mi>p</mi>         <mo>&#x2062;</mo>         <mi>q</mi>         <mo>&#x2062;</mo>         <mi>r</mi>        </mrow>       </msub>       <mo>&#x2062;</mo>       <msub>        <mi>I</mi>        <mrow>         <mrow>          <mo>(</mo>          <mrow>           <mi>x</mi>           <mo>+</mo>           <mi>p</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mi>y</mi>           <mo>+</mo>           <mi>q</mi>          </mrow>          <mo>)</mo>         </mrow>         <mo>&#x2062;</mo>         <mrow>          <mo>(</mo>          <mrow>           <mi>z</mi>           <mo>+</mo>           <mi>r</mi>          </mrow>          <mo>)</mo>         </mrow>        </mrow>       </msub>      </mrow>     </mrow>    </mrow>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0885" num="0000">where<ul id="ul0004" list-style="none">    <li id="ul0004-0001" num="0892">O is the result of the convolution,</li>    <li id="ul0004-0002" num="0893">I is the input volume,</li>    <li id="ul0004-0003" num="0894">K is the convolution kernel, and</li>    <li id="ul0004-0004" num="0895">(p, q, r) are the coordinates of K.</li></ul></p><p id="p-0886" num="0896">The bias term is omitted from the above equation to improve clarity.</p><p id="p-0887" num="0897">3D convolutions, in addition to extracting spatial information from matrices like 2D convolutions, extract information present between consecutive matrices. This allows them to map both spatial information of 3D objects and temporal information of a set of sequential images.</p><p id="p-0888" num="0898">Convolution Module</p><p id="p-0889" num="0899"><figref idref="DRAWINGS">FIG. <b>109</b></figref> illustrates one implementation of processing, through a cascade of convolution layers <b>10900</b> of the convolution module <b>10804</b>, per-cycle input data <b>10902</b> for a single sequencing cycle among the series oft sequencing cycles to be base called.</p><p id="p-0890" num="0900">The convolution module <b>10804</b> separately processes each per-cycle input data in a sequence of per-cycle input data through the cascade of convolution layers <b>10900</b>. The sequence of per-cycle input data is generated for a series oft sequencing cycles of a sequencing run that are to be base called, where t is any number between 1 and 1000. So, for example, when the series includes fifteen sequencing cycles, the sequence of per-cycle input data comprises fifteen different per-cycle input data.</p><p id="p-0891" num="0901">In one implementation, each per-cycle input data includes only image channels (e.g., a red channel and a green channel) or image data (e.g., the image data <b>7902</b> described above). The image channels or the image data depict intensity emissions of one or more clusters and their surrounding background captured at a respective sequencing cycle in the series. In another implementation, each per-cycle input data, in addition to the image channels or the image data, also includes supplemental channels such as the distance channels and the scaling channel (e.g., the input data <b>9500</b> described above).</p><p id="p-0892" num="0902">In the illustrated implementation, the per-cycle input data <b>10902</b> includes two image channels, namely, a red channel and a green channel, for the single sequencing cycle among the series oft sequencing cycles to be base called. Each image channel is encoded in an image patch of size 15&#xd7;15. The convolution module <b>10804</b> comprises five convolution layers. Each convolution layer has a bank of twenty-five convolution filters of size 3&#xd7;3. Further, the convolution filters use so-called SAME padding that preserves the height and width of the input images or tensors. With the SAME padding, a padding is added to the input features such that the output feature map has the same size as the input features. In contrast, so-called VALID padding means no padding.</p><p id="p-0893" num="0903">The first convolution layer <b>10904</b> processes the per-cycle input data <b>10902</b> and produces a first convolved representation <b>10906</b> of size 15&#xd7;15&#xd7;25. The second convolution layer <b>10908</b> processes the first convolved representation <b>10906</b> and produces a second convolved representation <b>10910</b> of size 15&#xd7;15&#xd7;25. The third convolution layer <b>10912</b> processes the second convolved representation <b>10910</b> and produces a third convolved representation <b>10914</b> of size 15&#xd7;15&#xd7;25. The fourth convolution layer <b>10916</b> processes the third convolved representation <b>10914</b> and produces a fourth convolved representation <b>10918</b> of size 15&#xd7;15&#xd7;25. The fifth convolution layer <b>10920</b> processes the fourth convolved representation <b>10918</b> and produces a fifth convolved representation <b>10922</b> of size 15&#xd7;15&#xd7;25. Note that the SAME padding preserves the spatial dimensions of the resulting convolved representations (e.g., 15&#xd7;15). In some implementations, the number of convolution filters in the convolution layers are a power of two, such as 2, 4, 16, 32, 64, 128, 256, 512, and 1024.</p><p id="p-0894" num="0904">As convolutions become deeper, information can be lost. To account for this, in some implementations, we use skip connections (1) to reintroduce the original per-cycle input data and (2) to combine low-level spatial features extracted by earlier convolution layers with high-level spatial features extracted by later convolution layers. We observe that this improves base calling accuracy.</p><p id="p-0895" num="0905"><figref idref="DRAWINGS">FIG. <b>110</b></figref> depicts one implementation of mixing <b>11000</b> the single sequencing cycle's per-cycle input data <b>10902</b> with its corresponding convolved representations <b>10906</b>, <b>10910</b>, <b>10914</b>, <b>10918</b>, and <b>10922</b> produced by the cascade of convolution layers <b>10900</b> of the convolution module <b>10804</b>. The convolved representations <b>10906</b>, <b>10910</b>, <b>10914</b>, <b>10918</b>, and <b>10922</b> are concatenated to form a sequence of convolved representations <b>11004</b>, which in turn is concatenated with the per-cycle input data <b>10902</b> to produce a mixed representation <b>11006</b>. In other implementations, summation is used instead of concatenation. Also, the mixing <b>11000</b> is operationalized by the mixer <b>11002</b>.</p><p id="p-0896" num="0906">A flattener <b>11008</b> then flattens the mixed representation <b>11006</b> and produces a per-cycle flattened mixed representation <b>11010</b>. In some implementations, the flattened mixed representation <b>11010</b> is a high dimensional vector or two-dimensional (2D) array that shares at least one dimension size with the per-cycle input data <b>10902</b> and the convolved representations <b>10906</b>, <b>10910</b>, <b>10914</b>, <b>10918</b>, and <b>10922</b> (e.g., 15&#xd7;1905, i.e., same row-wise dimension). This induces symmetry in the data that facilitates feature extraction in downstream 3D convolutions.</p><p id="p-0897" num="0907"><figref idref="DRAWINGS">FIGS. <b>109</b> and <b>110</b></figref> illustrate processing of the per-cycle image data <b>10902</b> for the single sequencing cycle among the series of t sequencing cycles to be base called. The convolution module <b>10804</b> separately processes respective per-cycle image data for each of the t sequencing cycles and produces a respective per-cycle flattened mixed presentation for each of the t sequencing cycles.</p><p id="p-0898" num="0908">Stacking</p><p id="p-0899" num="0909"><figref idref="DRAWINGS">FIG. <b>111</b></figref> shows one implementation of arranging flattened mixed representations of successive sequencing cycles as a stack <b>11100</b>. In the illustrated implementation, fifteen flattened mixed representations <b>10904</b><i>a </i>to <b>10904</b><i>o </i>for fifteen sequencing cycles are stacked in the stack <b>11100</b>. Stack <b>11100</b> is a 3D input volume that makes available features from both spatial and temporal dimensions (i.e., multiple sequencing cycles) in a same receptive field of a 3D convolution filter. The stacking is operationalized by the stacker <b>11102</b>. In other implementations, stack <b>11100</b> can be a tensor of any dimensionality (e.g., 1D, 2D, 4D, 5D, etc.).</p><p id="p-0900" num="0910">Recurrent Module</p><p id="p-0901" num="0911">We use recurrent processing to capture long-term dependencies in the sequencing data and, in particular, to account for second order contributions in cross-cycle sequencing images from pre-phasing and phasing. Recurrent processing is used for analysis of sequential data because of the usage of time steps. A current hidden state representation at a current time step is a function of (i) the previous hidden state representation from a previous time step and (ii) the current input at the current time step.</p><p id="p-0902" num="0912">The recurrent module <b>10808</b> subjects the stack <b>11100</b> to recurrent application of 3D convolutions (i.e., recurrent processing <b>11200</b>) in forward and backward directions and produces base calls for each of the clusters at each of the t sequencing cycles in the series. The 3D convolutions are used to extract spatio-temporal features from a subset of the flattened mixed representations in the stack <b>11100</b> on a sliding window basis. Each sliding window (w) corresponds to a respective sequencing cycle and is highlighted in <figref idref="DRAWINGS">FIG. <b>112</b><i>a </i></figref>in orange. In some implementations, w is parameterized to be 1, 2, 3, 5, 7, 9, 15, 21, etc., depending on the total number of sequencing cycles being simultaneously base called. In one implementation, w is a fraction of the total number of sequencing cycles being simultaneously base called.</p><p id="p-0903" num="0913">So, for example, consider that each sliding window contains three successive flattened mixed representations from the stack <b>11100</b> that comprises the fifteen flattened mixed representations <b>10904</b><i>a </i>to <b>10904</b><i>o</i>. Then, the first three flattened mixed representations <b>10904</b><i>a </i>to <b>10904</b><i>c </i>in the first sliding window correspond to the first sequencing cycle, the next three flattened mixed representations <b>10904</b><i>b </i>to <b>10904</b><i>d </i>in the second sliding window correspond to the second sequencing cycle, and so on. In some implementations, padding is used to encode adequate number of flattened mixed representations in the final sliding window corresponding to the final sequencing cycle, starting with the final flattened mixed representation <b>10904</b><i>o. </i></p><p id="p-0904" num="0914">At each time step, the recurrent module <b>10808</b> accepts (1) the current input x(t) and (2) the previous hidden state representation h(t&#x2212;1) and computes the current hidden state representation h(t). The current input x(t) includes only a subset of the flattened mixed representations from the stack <b>11100</b> that fall within the current sliding window ((w), in orange). Therefore, each current input x(t), at each time step, is a 3D volume of a plurality of flattened mixed representations (e.g., 1, 2, 3, 5, 7, 9, 15, or 21 flattened mixed representations, depending on w). For example, when (i) a single flattened mixed representation is two-dimensional (2D) with dimensions 15&#xd7;1905 and (ii) w is 7, then each current input x(t), at each time step, is a 3D volume with dimensions 15&#xd7;1905&#xd7;7.</p><p id="p-0905" num="0915">The recurrent module <b>10808</b> applies a first 3D convolution (W1<sub>3DCONV</sub>) to the current input x(t) and a second 3D convolution (W2<sub>3DCONV</sub>) to the previous hidden state representation h(t&#x2212;1) to produce the current hidden state representation h(t). In some implementations, W1<sub>3DCONV </sub>and W2<sub>3DCONV </sub>are the same because the weights are shared.</p><p id="p-0906" num="0916">Gated Processing</p><p id="p-0907" num="0917">In one implementation, the recurrent module <b>10808</b> processes the current input x(t) and the previous hidden state representation h(t&#x2212;1) through a gated network such as long short-term memory (LSTM) network or gated recurrent unit (GRU) network. For example, in the LSTM implementation, the current input x(t), along with the previous hidden state representation h(t&#x2212;1), is processed through each of the four gates of an LSTM unit: input gate, activation gate, forget gate, and output gate. This is illustrated in <figref idref="DRAWINGS">FIG. <b>112</b><i>b</i></figref>, which shows one implementation of processing <b>11200</b><i>b </i>the current input x(t) and the previous hidden state representation h(t&#x2212;1) through an LSTM unit that applies 3D convolutions to the current input x(t) and the previous hidden state representation h(t&#x2212;1) and produces the current hidden state representation h(t) as output. In such an implementation, the weights of the input, activation, forget, and output gates apply 3D convolutions.</p><p id="p-0908" num="0918">In some implementations, the gated units (LSTM or GRU) do not use the non-linearity/squashing functions like hyperbolic tangent and sigmoid.</p><p id="p-0909" num="0919">In one implementation, the current input x(t), the previous hidden state representation h(t&#x2212;1), and the current hidden state representation h(t) are all 3D volume with same dimensionality and are processed through or produced by the input, activation, forget, and output gates as 3D volume.</p><p id="p-0910" num="0920">In one implementation, the 3D convolutions of the recurrent module <b>10808</b> use a bank of twenty-five convolution filters of size 3&#xd7;3, along with the SAME padding. In some implementations, the size of the convolution filters is 5&#xd7;5. In some implementations, the number of convolution filters used by the recurrent module <b>10808</b> are factorized by a power of two, such as 2, 4, 16, 32, 64, 128, 256, 512, and 1024.</p><p id="p-0911" num="0921">Bi-Directional Processing</p><p id="p-0912" num="0922">The recurrent module <b>10808</b> first processes the stack <b>11100</b> from the beginning to the end (top-down) on the sliding window basis and produces a sequence of current hidden state representations (vectors) for the forward traversal {right arrow over (h)}<sub>t</sub>=3DCONV(x<sub>t</sub>+{right arrow over (h)}<sub>t-1</sub>).</p><p id="p-0913" num="0923">The recurrent module <b>10808</b> then processes the stack <b>11100</b> from the end to the beginning (bottom-up) on the sliding window basis and produces a sequence of current hidden state representations (vectors) for the backward/reverse traversal</p><p id="p-0914" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00001" he="3.22mm" wi="1.78mm" file="US20230004749A1-20230105-P00001.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>t</sub>=3DCONV(<i>x</i><sub>t</sub>+<img id="CUSTOM-CHARACTER-00002" he="3.22mm" wi="1.78mm" file="US20230004749A1-20230105-P00002.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>t-1</sub>).<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0915" num="0924">In some implementations, for both the directions, at each time step, the processing uses the gates of an LSTM or a GRU. For example, at each time step, a forward current input x(t) is processed through the input, activation, forget, and output gates of an LSTM unit to produce a forward current hidden state representation {right arrow over (h)}<sub>t </sub>and a backward current input x(t) is processed through the input, activation, forget, and output gates of another LSTM unit to produce a backward current hidden state representation <img id="CUSTOM-CHARACTER-00003" he="3.22mm" wi="1.78mm" file="US20230004749A1-20230105-P00003.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>t</sub>.</p><p id="p-0916" num="0925">Then, for each time step/sliding window/sequencing cycle, the recurrent module <b>10808</b> combines (concatenates or sums or averages) the corresponding forward and backward current hidden state representations and produces a combined hidden state representation</p><p id="p-0917" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><img id="CUSTOM-CHARACTER-00004" he="3.22mm" wi="1.44mm" file="US20230004749A1-20230105-P00004.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>t</sub>=[<i>{right arrow over (h)}</i><sub>t</sub>;<img id="CUSTOM-CHARACTER-00005" he="3.22mm" wi="1.78mm" file="US20230004749A1-20230105-P00005.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>t</sub>]<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0918" num="0926">The combined hidden representation <img id="CUSTOM-CHARACTER-00006" he="3.22mm" wi="1.44mm" file="US20230004749A1-20230105-P00006.TIF" alt="custom-character" img-content="character" img-format="tif"/><sub>t </sub>is then processed through one or more fully-connected networks to produce a dense representation. The dense representation is then processed through a softmax layer to produce likelihoods of bases incorporated in each of the clusters at a given sequencing cycle being A, C, T, and G. The bases are classified as A, C, T, or G based on the likelihoods. This is done for each of the t sequencing cycles in the series (or each time step/sliding window), either in parallel or sequentially.</p><p id="p-0919" num="0927">One skilled in the art will appreciate that, in other implementations, the hybrid architecture can process input data for fewer or greater number of sequencing cycles and can comprise fewer or greater number of convolution and recurrent layers. Also, the dimensionality of the input data, the current and previous hidden representations, the convolution filters, the resulting feature maps, and the output can be different. Also, the number of convolution filters in a convolution layer can be different. It can use different padding and striding configurations. It can use a different classification function (e.g., sigmoid or regression) and may or may not include a fully-connected layer. It can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. It can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. It can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous SGD. It can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LSTM or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tan h)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><heading id="h-0020" level="1">Experimental Results and Observations</heading><p id="p-0920" num="0928"><figref idref="DRAWINGS">FIG. <b>113</b></figref> shows one implementation of balancing trinucleotides (3-mers) in the training data used to train the neural network-based base caller <b>1514</b>. Balancing results in very little learning of statistics about genome in the training data and in turn improves generalization. Heat map <b>11302</b> shows balanced 3-mers in the training data for a first organism called &#x201c;<i>A. baumanni</i>&#x201d;. Heap map <b>11304</b> shows balanced 3-mers in the training data for a second organism called &#x201c;<i>E. coli&#x201d;. </i></p><p id="p-0921" num="0929"><figref idref="DRAWINGS">FIG. <b>114</b></figref> compares base calling accuracy of the RTA base caller against the neural network-based base caller <b>1514</b>. As illustrated in <figref idref="DRAWINGS">FIG. <b>114</b></figref>, the RTA base caller has a higher error percentage in two sequencing runs (Read: 1 and Read: 2). That is, the neural network-based base caller <b>1514</b> outperforms the RTA base caller in both the sequencing runs.</p><p id="p-0922" num="0930"><figref idref="DRAWINGS">FIG. <b>115</b></figref> compares tile-to-tile generalization of the RTA base caller with that of the neural network-based base caller <b>1514</b> on a same tile. That is, with the neural network-based base caller <b>1514</b>, the inference (testing) is performed on data for the same tile whose data is used in the training.</p><p id="p-0923" num="0931"><figref idref="DRAWINGS">FIG. <b>116</b></figref> compares tile-to-tile generalization of the RTA base caller with that of the neural network-based base caller <b>1514</b> on a same tile and on different tiles. That is, the neural network-based base caller <b>1514</b> is trained on data for clusters on a first tile, but performs inference on data from clusters on a second tile. In the same tile implementation, the neural network-based base caller <b>1514</b> is trained on data from clusters on tile five and tested on data from clusters on tile five. In the different tile implementation, the neural network-based base caller <b>1514</b> is trained on data from clusters on tile ten and tested on data from clusters on tile five.</p><p id="p-0924" num="0932"><figref idref="DRAWINGS">FIG. <b>117</b></figref> also compares tile-to-tile generalization of the RTA base caller with that of the neural network-based base caller <b>1514</b> on different tiles. In the different tile implementations, the neural network-based base caller <b>1514</b> is once trained on data from clusters on tile ten and tested on data from clusters on tile five, and then trained on data from clusters on tile twenty and tested on data from clusters on tile five.</p><p id="p-0925" num="0933"><figref idref="DRAWINGS">FIG. <b>118</b></figref> shows how different sizes of the image patches fed as input to the neural network-based base caller <b>1514</b> effect the base calling accuracy. In both sequencing runs (Read: 1 and Read: 2), the error percentage decreases as the patch size increases from 3&#xd7;3 to 11&#xd7;11. That is, the neural network-based base caller <b>1514</b> produces more accurate base calls with larger image patches. In some implementations, base calling accuracy is balanced against compute efficiency by using image patches that are not larger than 100&#xd7;100 pixels. In other implementations, image patches as large as 3000&#xd7;3000 pixels (and larger) are used.</p><p id="p-0926" num="0934"><figref idref="DRAWINGS">FIGS. <b>119</b>, <b>120</b>, <b>121</b>, and <b>122</b></figref> show lane-to-lane generalization of the neural network-based base caller <b>1514</b> on training data from <i>A. baumanni </i>and <i>E. coli. </i></p><p id="p-0927" num="0935">Turning to <figref idref="DRAWINGS">FIG. <b>120</b></figref>, in one implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on a first lane of a flow cell and tested on <i>A. baumanni </i>data from clusters on both the first and second lanes of the flow cell. In another implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the first lane and tested on the <i>A. baumanni </i>data from clusters on both the first and second lanes. In yet another implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on the second lane and tested on the <i>A. baumanni </i>data from clusters on both the first and second lanes. In yet further implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the second lane and tested on the <i>A. baumanni </i>data from clusters on both the first and second lanes.</p><p id="p-0928" num="0936">In one implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on a first lane of a flow cell and tested on <i>E. coli </i>data from clusters on both the first and second lanes of the flow cell. In another implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the first lane and tested on the <i>E. coli </i>data from clusters on both the first and second lanes. In yet another implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on the second lane and tested on the <i>E. coli </i>data from clusters on the first lane. In yet further implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the second lane and tested on the <i>E. coli </i>data from clusters on both the first and second lanes.</p><p id="p-0929" num="0937">In <figref idref="DRAWINGS">FIG. <b>120</b></figref>, the base calling accuracy (measured by the error percentage) is shown for each of these implementations for two sequencing runs (e.g., Read: 1 and Read: 2).</p><p id="p-0930" num="0938">Turning to <figref idref="DRAWINGS">FIG. <b>121</b></figref>, in one implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on a first lane of a flow cell and tested on <i>A. baumanni </i>data from clusters on the first lane. In another implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the first lane and tested on the <i>A. baumanni </i>data from clusters on the first lane. In yet another implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on the second lane and tested on the <i>A. baumanni </i>data from clusters on the first lane. In yet further implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the second lane and tested on the <i>A. baumanni </i>data from clusters on the first lane.</p><p id="p-0931" num="0939">In one implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on a first lane of a flow cell and tested on <i>E. coli </i>data from clusters on the first lane. In another implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the first lane and tested on the <i>E. coli </i>data from clusters on the first lane. In yet another implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on the second lane and tested on the <i>E. coli </i>data from clusters on the first lane. In yet further implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the second lane and tested on the <i>E. coli </i>data from clusters on the first lane.</p><p id="p-0932" num="0940">In <figref idref="DRAWINGS">FIG. <b>121</b></figref>, the base calling accuracy (measured by the error percentage) is shown for each of these implementations for two sequencing runs (e.g., Read: 1 and Read: 2). Comparing <figref idref="DRAWINGS">FIG. <b>120</b></figref> with <figref idref="DRAWINGS">FIG. <b>121</b></figref>, it can be seen that the implementations covered by the later result in an error reduction between fifty to eighty percent.</p><p id="p-0933" num="0941">Turning to <figref idref="DRAWINGS">FIG. <b>122</b></figref>, in one implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on a first lane of a flow cell and tested on <i>A. baumanni </i>data from clusters on the second lane. In another implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the first lane and tested on the <i>A. baumanni </i>data from clusters on the second lane. In yet another implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on the second lane and tested on the <i>A. baumanni </i>data from clusters on the first lane. In second first lane. In yet further implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the second lane and tested on the <i>A. baumanni </i>data from clusters on the second lane.</p><p id="p-0934" num="0942">In one implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on a first lane of a flow cell and tested on <i>E. coli </i>data from clusters on the second lane. In another implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the first lane and tested on the <i>E. coli </i>data from clusters on the second lane. In yet another implementation, the neural network-based base caller <b>1514</b> is trained on <i>E. coli </i>data from clusters on the second lane and tested on the <i>E. coli </i>data from clusters on the second lane. In yet further implementation, the neural network-based base caller <b>1514</b> is trained on <i>A. baumanni </i>data from clusters on the second lane and tested on the <i>E. coli </i>data from clusters on the second lane.</p><p id="p-0935" num="0943">In <figref idref="DRAWINGS">FIG. <b>122</b></figref>, the base calling accuracy (measured by the error percentage) is shown for each of these implementations for two sequencing runs (e.g., Read: 1 and Read: 2). Comparing <figref idref="DRAWINGS">FIG. <b>120</b></figref> with <figref idref="DRAWINGS">FIG. <b>122</b></figref>, it can be seen that the implementations covered by the later result in an error reduction between fifty to eighty percent.</p><p id="p-0936" num="0944"><figref idref="DRAWINGS">FIG. <b>123</b></figref> depicts an error profile for the lane-to-lane generalization discussed above with respect to <figref idref="DRAWINGS">FIGS. <b>119</b>, <b>120</b>, <b>121</b>, and <b>122</b></figref>. In one implementation, the error profile detects error in base calling A and T bases in the green channel.</p><p id="p-0937" num="0945"><figref idref="DRAWINGS">FIG. <b>124</b></figref> attributes the source of the error detected by the error profile of <figref idref="DRAWINGS">FIG. <b>123</b></figref> to low cluster intensity in the green channel.</p><p id="p-0938" num="0946"><figref idref="DRAWINGS">FIG. <b>125</b></figref> compares error profiles of the RTA base caller and the neural network-based base caller <b>1514</b> for two sequencing runs (Read <b>1</b> and Read <b>2</b>). The comparison confirms superior base calling accuracy of the neural network-based base caller <b>1514</b>.</p><p id="p-0939" num="0947"><figref idref="DRAWINGS">FIG. <b>126</b><i>a </i></figref>shows run-to-run generalization of the neural network-based base caller <b>1514</b> on four different instruments.</p><p id="p-0940" num="0948"><figref idref="DRAWINGS">FIG. <b>126</b><i>b </i></figref>shows run-to-run generalization of the neural network-based base caller <b>1514</b> on four different runs executed on a same instrument.</p><p id="p-0941" num="0949"><figref idref="DRAWINGS">FIG. <b>127</b></figref> shows the genome statistics of the training data used to train the neural network-based base caller <b>1514</b>.</p><p id="p-0942" num="0950"><figref idref="DRAWINGS">FIG. <b>128</b></figref> shows the genome context of the training data used to train the neural network-based base caller <b>1514</b>.</p><p id="p-0943" num="0951"><figref idref="DRAWINGS">FIG. <b>129</b></figref> shows the base calling accuracy of the neural network-based base caller <b>1514</b> in base calling long reads (e.g., 2&#xd7;250).</p><p id="p-0944" num="0952"><figref idref="DRAWINGS">FIG. <b>130</b></figref> illustrates one implementation of how the neural network-based base caller <b>1514</b> attends to the central cluster pixel(s) and its neighboring pixels across image patches.</p><p id="p-0945" num="0953"><figref idref="DRAWINGS">FIG. <b>131</b></figref> shows various hardware components and configurations used to train and run the neural network-based base caller <b>1514</b>, according to one implementation. In other implementations, different hardware components and configurations are used.</p><p id="p-0946" num="0954"><figref idref="DRAWINGS">FIG. <b>132</b></figref> shows various sequencing tasks that can be performed using the neural network-based base caller <b>1514</b>. Some examples include quality scoring (QScoring) and variant classification. <figref idref="DRAWINGS">FIG. <b>132</b></figref> also lists some example sequencing instruments for which the neural network-based base caller <b>1514</b> performs base calling.</p><p id="p-0947" num="0955"><figref idref="DRAWINGS">FIG. <b>133</b></figref> is a scatter plot <b>13300</b> visualized by t-Distributed Stochastic Neighbor Embedding (t-SNE) and portrays base calling results of the neural network-based base caller <b>1514</b>. Scatter plot <b>13300</b> shows that the base calling results are clustered into 64 (4<sup>3</sup>) groups, with each group mostly corresponding to a particular input 3-mer (trinucleotide repeating pattern). This is the case because the neural network-based base caller <b>1514</b> processes input data for at least three sequencing cycles and learns sequence-specific motifs to produce a current base call based on the previous and successive base calls.</p><p id="p-0948" num="0956">Quality Scoring</p><p id="p-0949" num="0957">Quality scoring refers to the process of assigning a quality score to each base call. Quality scores are defined according to the Phred framework, which transforms the values of predictive features of sequencing traces to a probability based on a quality table. The quality table is obtained by training on calibration data sets and is updated when characteristics of the sequencing platform change. The probabilistic interpretation of quality scores allows fair integration of different sequencing reads in the downstream analysis such as variant calling and sequence assembly. Thus, a valid model to define quality scores is indispensable for any base caller.</p><p id="p-0950" num="0958">We first describe what quality scores are. A quality score is a measure of the probability of a sequencing error in a base call. A high quality score implies that a base call is more reliable and less likely to be incorrect. For example, if the quality score of a base is Q30, the probability that this base is called incorrectly is 0.001. This also indicates that the base call accuracy is 99.9%.</p><p id="p-0951" num="0959">The following table shows the relationship between the base call quality scores and their corresponding error probability, base call accuracy rate, and base call error rate:</p><p id="p-0952" num="0000"><tables id="TABLE-US-00001" num="00001"><table frame="none" colsep="0" rowsep="0" pgwide="1"><tgroup align="left" colsep="0" rowsep="0" cols="4"><colspec colname="1" colwidth="42pt" align="center"/><colspec colname="2" colwidth="77pt" align="left"/><colspec colname="3" colwidth="63pt" align="center"/><colspec colname="4" colwidth="77pt" align="center"/><thead><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row><row><entry>Quality Score</entry><entry>Error Probability</entry><entry>Base Call Error Rate</entry><entry>Base Call Accuracy Rate</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></thead><tbody valign="top"><row><entry>Q10</entry><entry>&#x2003;&#x2003;&#x2002;0.1 (1 in 10)</entry><entry>&#x2003;&#x2002;&#x2009;10%</entry><entry>&#x2009;&#x2003;&#x2003;90%</entry></row><row><entry>Q20</entry><entry>&#x2003;&#x2003;0.01 (1 in 100)</entry><entry>&#x2003;&#x2003;&#x2009;1%</entry><entry>&#x2009;&#x2003;&#x2003;99%</entry></row><row><entry>Q30</entry><entry>&#x2003;&#x2002;0.001 (1 in 1,000)</entry><entry>&#x2002;&#x2002;&#x2002;0.1%</entry><entry>&#x2002;&#x2002;&#x2002;99.9%</entry></row><row><entry>Q40</entry><entry>&#x2003;0.0001 (1 in 10,000)</entry><entry>&#x2002;&#x2002;0.01%</entry><entry>&#x2002;&#x2002;99.99%</entry></row><row><entry>Q50</entry><entry>&#x2002;0.00001 (1 in 100,000)</entry><entry>&#x2002;0.001%</entry><entry>&#x2002;99.999%</entry></row><row><entry>Q60</entry><entry>0.000001 (1 in 1,000,000)</entry><entry>0.0001%</entry><entry>99.9999%</entry></row><row><entry namest="1" nameend="4" align="center" rowsep="1"/></row></tbody></tgroup></table></tables></p><p id="p-0953" num="0960">We now describe how quality scores are generated. During a sequencing run, a quality score is assigned to each base call for every cluster, on every tile, for every sequencing cycle. Illumina quality scores are calculated for each base call in a two-step process. For each base call, a number of quality predictor values are computed. Quality predictor values are observable properties of clusters from which base calls are extracted. These include properties such as intensity profiles and signal-to-noise ratios and measure various aspects of base call reliability. They have been empirically determined to correlate with the quality of the base call.</p><p id="p-0954" num="0961">A quality model, also known as a quality table or Q-table, lists combinations of quality predictor values and relates them to corresponding quality scores; this relationship is determined by a calibration process using empirical data. To estimate a new quality score, the quality predictor values are computed for a new base call and compared to values in the pre-calibrated quality table.</p><p id="p-0955" num="0962">We now describe how a quality table is calibrated. Calibration is a process in which a statistical quality table is derived from empirical data that includes various well-characterized human and non-human samples sequenced on a number of instruments. Using a modified version of the Phred algorithm, a quality table is developed and refined using characteristics of the raw signals and error rates determined by aligning reads to the appropriate references.</p><p id="p-0956" num="0963">We now describe why quality tables change from time to time. Quality tables provide quality scores for runs generated by specific instrument configurations and versions of chemistry. When significant characteristics of the sequencing platform change, such as new hardware, software, or chemistry versions, the quality model requires recalibration. For example, improvements in sequencing chemistry require quality table recalibration to accurately score the new data, which consumes a substantial amount of processing time and computational resources.</p><p id="p-0957" num="0964">Neural Network-Based Quality Scoring</p><p id="p-0958" num="0965">We disclose neural network-based techniques for quality scoring that do not use the quality predictor values or the quality tables and instead infer quality scores from confidence over predictions of well-calibrated neural networks. In the context of neural networks, &#x201c;calibration&#x201d; refers to the consistency or correlation between subjective forecasts and empirical long-run frequencies. This is a frequentist notion of certainty: if a neural network claims that 90% of the time a particular label is the correct label, then, during evaluation, 90% of all labels ascribed probability 90% of being correct, should be the correct label. Note that calibration is an orthogonal concern to accuracy: a neural network's predictions may be accurate and yet miscalibrated.</p><p id="p-0959" num="0966">The disclosed neural networks are well-calibrated because they are trained on large-scale training sets with diverse sequencing characteristics that adequately model the base calling domain of real-world sequencing runs. In particular, sequencing images obtained from a variety of sequencing platforms, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, cluster densities, and flow cells are used as training examples to train the neural networks. In other implementations, different base calling and quality scoring models are respectively used for different sequencing platforms, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, cluster densities, and/or flow cells.</p><p id="p-0960" num="0967">For each of the four base call classes (A, C, T, and G), large numbers of sequencing images are used as training examples that identify intensity patterns representative of the respective base call class under a wide range of sequencing conditions. This in turn obviates the need of extending classification capabilities of the neural networks to new classes not present in the training. Furthermore, each training example is accurately labelled with a corresponding ground truth based on aligning reads to the appropriate references. What results is well-calibrated neural networks whose confidence over predictions can be interpreted as a certainty measure for quality scoring, expressed mathematically below.</p><p id="p-0961" num="0968">Let Y={A, C, T, G} denote the set of class labels for the base call classes A, C, T, and G and X denote a space of inputs. Let N<sub>&#x3b8;</sub>(y|x) denote the probability distribution one of the disclosed neural networks predicts on an input x&#x3f5;X and &#x3b8; denote the neural network's parameters. For a training example X<sub>i </sub>with correct label y<sub>i</sub>, the neural network predicts label &#x177;<sub>i</sub>=argmax<sub>y&#x3f5;Y</sub>N<sub>&#x3b8;</sub>(y|x<sub>i</sub>). The prediction gets correctness score c<sub>i</sub>=1 if &#x177;<sub>i</sub>=y<sub>i </sub>and 0 otherwise and a confidence score r<sub>i</sub>=N<sub>&#x3b8;</sub>(&#x177;<sub>i</sub>|x<sub>i</sub>).</p><p id="p-0962" num="0969">The neural network N<sub>&#x3b8;</sub>(y|x) is well-calibrated over a data distribution D because over all (x<sub>i</sub>, y<sub>i</sub>)&#x3f5;D and r<sub>i</sub>=&#x3b1; the probability that c<sub>i</sub>=1 is &#x3b1;. For example, out of a sample from D, given 100 predictions, each with confidence 0.8, 80 are correctly classified by the neural network N<sub>&#x3b8;</sub>(y|x). More formally, P<sub>&#x3b8;,D</sub>(r,c) denotes the distribution over r and c values of the predictions of the neural network N<sub>&#x3b8;</sub>(y|x) on D and is expressed as P<sub>&#x3b8;,D</sub>(c=1|r=I<sub>&#x3b1;</sub>)=&#x3b1; &#x2200;<sub>&#x3b1;</sub>&#x3f5;[0,1], where I<sub>&#x3b1; </sub>denotes a small non-zero interval around &#x3b1;.</p><p id="p-0963" num="0970">Because the well-calibrated neural networks are trained on diverse training sets, unlike the quality predictor values or the quality tables, they are not specific to instrument configurations and chemistry versions. This has two advantages. First, for different types of sequencing instruments, the well-calibrated neural networks obviate the need of deriving different quality tables from separate calibration processes. Second, for a same sequencing instrument, they obviate the need of recalibration when characteristics of the sequencing instrument change. More details follow.</p><p id="p-0964" num="0971">Inferring Quality Scores from Softmax Confidence Probabilities</p><p id="p-0965" num="0972">The first well-calibrated neural network is the neural network-based base caller <b>1514</b> that processes input data derived from the sequencing images <b>108</b> and produces base call confidence probabilities for the base being A, C, T, and G. Base call confidence probabilities can also be considered likelihoods or classification scores. In one implementation, the neural network-based base caller <b>1514</b> uses a softmax function to generate the base call confidence probabilities as softmax scores.</p><p id="p-0966" num="0973">Quality scores are inferred from the base call confidence probabilities generated by the softmax function of the neural network-based base caller <b>1514</b> because the softmax scores are calibrated (i.e., they are representative of the ground truth correctness likelihood) and thus naturally correspond to the quality scores.</p><p id="p-0967" num="0974">We demonstrate correspondence between the base call confidence probabilities and the quality scores by selecting a set of the base call confidence probabilities produced by the neural network-based base caller <b>1514</b> during training and determining their base calling error rate (or base calling accuracy rate).</p><p id="p-0968" num="0975">So, for example, we select the base call confidence probability &#x201c;0.90&#x201d; produced by the neural network-based base caller <b>1514</b>. We take numerous (e.g., ranging from 10000 to 1000000) instances when the neural network-based base caller <b>1514</b> made the base call prediction with 0.90 softmax score. The numerous instances can be obtained either from the validation set or the test set. We then, based on comparison to corresponding ground truth base calls associated with respective ones of the numerous instances, determine in how many of the numerous instances the base call prediction was correct.</p><p id="p-0969" num="0976">We observe that the base call was correctly predicted in ninety percent of the numerous instances, with ten percent miscalls. This means that for the 0.90 softmax score, the base calling error rate is 10% and the base calling accuracy rate is 90%, which in turn corresponds to quality score Q10 (see table above). Similarly, for other softmax scores like 0.99, 0.999, 0.9999, 0.99999, and 0.999999 we observe correspondence with quality scores Q20, Q30, Q40, Q50, and Q60, respectively. This is illustrated in <figref idref="DRAWINGS">FIG. <b>136</b><i>a</i></figref>. In other implementations, we observe correspondence between the softmax scores and quality scores such as Q9, Q11, Q12, Q23, Q25, Q29, Q37, and Q39.</p><p id="p-0970" num="0977">We also observe correspondence with binned quality scores. For example, 0.80 softmax score corresponds to binned quality score Q06, 0.95 softmax score corresponds to binned quality score Q15, 0.993 softmax score corresponds to binned quality score Q22, 0.997 softmax score corresponds to binned quality score Q27, 0.9991 softmax score corresponds to binned quality score Q33, 0.9995 softmax score corresponds to binned quality score Q37, and 0.9999 softmax score corresponds to binned quality score Q40. This is illustrated in <figref idref="DRAWINGS">FIG. <b>136</b></figref><i>b. </i></p><p id="p-0971" num="0978">The sample size used herein are large to avoid small sample issues and can, for example, range from 10000 to 1000000. In some implementations, the sample size of instances used to determine the base calling error rates (or the base calling accuracy rates) is selected based on the softmax score being evaluated. For example, for 0.99 softmax score, the sample includes one hundred instances, for 0.999 softmax score, the sample includes one thousand instances, for 0.9999 softmax score, the sample includes ten thousand instances, for 0.99999 softmax score, the sample includes hundred thousand instances, and for 0.999999 softmax score, the sample includes one million instances.</p><p id="p-0972" num="0979">Regarding softmax, softmax is an output activation function for multiclass classification. Formally, training a so-called softmax classifier is regression to a class probability, rather than a true classifier as it does not return the class but rather a confidence prediction of each class's likelihood. The softmax function takes a class of values and converts them to probabilities that sum to one. The softmax function squashes a k-dimensional vector of arbitrary real values to k-dimensional vector of real values within the range zero to one. Thus, using the softmax function ensures that the output is a valid, exponentially normalized probability mass function (nonnegative and summing to one).</p><p id="p-0973" num="0980">Consider that {tilde over (y)}<sub>i </sub>is the ith element of the vector {tilde over (y)}=[{tilde over (y)}<sub>1</sub>, {tilde over (y)}<sub>2</sub>, . . . {tilde over (y)}<sub>n</sub>]:</p><p id="p-0974" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mrow>  <mrow>   <msub>    <mover>     <mi>y</mi>     <mo>~</mo>    </mover>    <mi>i</mi>   </msub>   <mo>=</mo>   <mrow>    <msub>     <mrow>      <mo>(</mo>      <mrow>       <mi>softmax</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mrow>        <mo>(</mo>        <mover>         <mi>z</mi>         <mo>~</mo>        </mover>        <mo>)</mo>       </mrow>      </mrow>      <mo>)</mo>     </mrow>     <mi>i</mi>    </msub>    <mo>=</mo>    <mfrac>     <mrow>      <mi>exp</mi>      <mo>&#x2062;</mo>      <mtext>  </mtext>      <mrow>       <mo>(</mo>       <mover>        <msub>         <mi>z</mi>         <mi>i</mi>        </msub>        <mo>~</mo>       </mover>       <mo>)</mo>      </mrow>     </mrow>     <mrow>      <munderover>       <mo>&#x2211;</mo>       <mrow>        <mi>j</mi>        <mo>=</mo>        <mn>1</mn>       </mrow>       <mrow>        <mi>j</mi>        <mo>=</mo>        <mi>N</mi>       </mrow>      </munderover>      <mtext> </mtext>      <mrow>       <mi>exp</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mrow>        <mo>(</mo>        <msub>         <mover>          <mi>z</mi>          <mo>~</mo>         </mover>         <mi>j</mi>        </msub>        <mo>)</mo>       </mrow>      </mrow>     </mrow>    </mfrac>   </mrow>  </mrow>  <mo>,</mo> </mrow></math></maths></p><p id="p-0975" num="0000">where</p><p id="p-0976" num="0981">{tilde over (y)} is a vector of length n, where n is the number of classes in the classification. These elements have values between zero and one, and sum to one so that they represent a valid probability distribution.</p><p id="p-0977" num="0982">An example softmax activation function <b>13406</b> is shown in <figref idref="DRAWINGS">FIG. <b>134</b></figref>. Softmax <b>13406</b> is applied to three classes as</p><p id="p-0978" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mrow>  <mi>z</mi>  <mo>&#x21a6;</mo>  <mrow>   <mi>softmax</mi>   <mo>&#x2062;</mo>   <mtext>  </mtext>   <mrow>    <mrow>     <mo>(</mo>     <mrow>      <mo>[</mo>      <mrow>       <mrow>        <mi>z</mi>        <mo>;</mo>        <mfrac>         <mi>z</mi>         <mrow>          <mn>1</mn>          <mo>&#x2062;</mo>          <mn>0</mn>         </mrow>        </mfrac>        <mo>;</mo>       </mrow>       <mo>&#x2062;</mo>       <mi>&#x2010;</mi>       <mo>&#x2062;</mo>       <mrow>        <mn>2</mn>        <mo>&#x2062;</mo>        <mi>z</mi>       </mrow>      </mrow>      <mo>]</mo>     </mrow>     <mo>)</mo>    </mrow>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0979" num="0000">Note that the three outputs always sum to one. They thus define a discrete probability mass function.</p><p id="p-0980" num="0983">When used for classification, {tilde over (y)}<sub>i </sub>gives the probability of being in class i.</p><p id="p-0981" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>P</i>(<i>Y=i|{tilde over (z)}</i>)=(softmax(<i>{tilde over (z)}</i>))<sub>i</sub><i>={tilde over (y)}</i><sub>i </sub><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0982" num="0984">The name &#x201c;softmax&#x201d; can be somewhat confusing. The function is more closely related to the argmax function than the max function. The term &#x201c;soft&#x201d; derives from the fact that the softmax function is continuous and differentiable. The argmax function, with its result represented as a one-hot vector, is not continuous or differentiable. The softmax function thus provides a &#x201c;softened&#x201d; version of the argmax. It would perhaps be better to call the softmax function &#x201c;softargmax,&#x201d; but the current name is an entrenched convention.</p><p id="p-0983" num="0985"><figref idref="DRAWINGS">FIG. <b>134</b></figref> illustrates one implementation of selecting <b>13400</b> the base call confidence probabilities <b>10704</b> of the neural network-based base caller <b>1514</b> for quality scoring. The base call confidence probabilities <b>10704</b> of the neural network-based base caller <b>1514</b> can be classification scores (e.g., softmax scores or sigmoid scores) or regression scores. In one implementation, the base call confidence probabilities <b>10704</b> are produced during the training <b>10700</b>.</p><p id="p-0984" num="0986">In some implementations, the selection <b>13400</b> is done based on quantization, which is performed by a quantizer <b>13402</b> that accesses the base call confidence probabilities <b>10704</b> and produces quantized classification scores <b>13404</b>. The quantized classification scores <b>13404</b> can be any real-number. In one implementation, the quantized classification scores <b>13404</b> are selected based on a selection formula defined as</p><p id="p-0985" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mrow>   <mn>0</mn>   <mo>.</mo>   <mn>9</mn>  </mrow>  <mo>&#x2062;</mo>  <mrow>   <munderover>    <mo>&#x2211;</mo>    <mrow>     <mi>i</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>    <mi>n</mi>   </munderover>   <mrow>    <mn>0</mn>    <mo>.</mo>    <msup>     <mn>1</mn>     <mrow>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </msup>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0986" num="0000">In another implementation, the quantized classification scores <b>13404</b> are selected based on a selection formula defined as</p><p id="p-0987" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mrow>  <mover>   <munder>    <mo>&#x2200;</mo>    <mrow>     <mi>i</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>   </munder>   <mrow>    <mi>n</mi>    <mo>=</mo>    <mn>10</mn>   </mrow>  </mover>  <mrow>   <mn>0.1</mn>   <mrow>    <mi>i</mi>    <mo>.</mo>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0988" num="0987"><figref idref="DRAWINGS">FIG. <b>135</b></figref> shows one implementation of the neural network-based quality scoring <b>13500</b>. For each of the quantized classification scores <b>13404</b>, a base calling error rate <b>13508</b> and/or a base calling accuracy rate <b>13510</b> is determined by comparing its base call predictions <b>10704</b> against corresponding ground truth base calls <b>10708</b> (e.g., over batches with varying sample size). The comparison is performed by a comparer <b>13502</b>, which in turn includes a base calling error rate determiner <b>13504</b> and a base calling accuracy rate determiner <b>13506</b>.</p><p id="p-0989" num="0988">Then, to establish the correspondence between the quantized classification scores <b>13404</b> and the quality scores, a fit is determined between the quantized classification scores <b>13404</b> and their base calling error rate <b>13508</b> (and/or their base calling accuracy rate <b>13510</b>) by a fit determiner <b>13512</b>. In one implementation, the fit determiner <b>13512</b> is a regression model.</p><p id="p-0990" num="0989">Based on the fit, the quality scores are correlated with the quantized classification scores <b>13404</b> by a correlator <b>13514</b>.</p><p id="p-0991" num="0990"><figref idref="DRAWINGS">FIGS. <b>136</b><i>a</i>-<b>136</b><i>b </i></figref>depict one implementation of correspondence <b>13600</b> between the quality scores and the base call confidence predictions made by the neural network-based base caller <b>1514</b>. The base call confidence probabilities of the neural network-based base caller <b>1514</b> can be classification scores (e.g., softmax scores or sigmoid scores) or regression scores. <figref idref="DRAWINGS">FIG. <b>136</b><i>a </i></figref>is a quality score correspondence scheme <b>13600</b><i>a </i>for quality scores. <figref idref="DRAWINGS">FIG. <b>136</b><i>b </i></figref>is a quality score correspondence scheme <b>13600</b><i>a </i>for binned quality scores.</p><p id="p-0992" num="0991">Inference</p><p id="p-0993" num="0992"><figref idref="DRAWINGS">FIG. <b>137</b></figref> shows one implementation of inferring quality scores from base call confidence predictions made by the neural network-based base caller <b>1514</b> during inference <b>13700</b>. The base call confidence probabilities of the neural network-based base caller <b>1514</b> can be classification scores (e.g., softmax scores or sigmoid scores) or regression scores.</p><p id="p-0994" num="0993">During the inference <b>13700</b>, the predicted base call <b>13706</b> is assigned the quality score <b>13708</b> to which its base call confidence probability (i.e., the highest softmax score (in red)) most corresponds to. In some implementations, the quality score correspondence <b>13600</b> is made by looking up the quality score correspondence schemes <b>13600</b><i>a</i>-<b>13600</b><i>b </i>and is operationalized by a quality score inferrer <b>13712</b>.</p><p id="p-0995" num="0994">In some implementations, a chastity filter <b>13710</b> terminates the base calling of a given cluster when the quality score <b>13708</b> assigned to its called base, or an average quality score over successive base calling cycles, falls below a preset threshold.</p><p id="p-0996" num="0995">The inference <b>13700</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>13714</b>, including parallelization techniques such as batching. The inference <b>13700</b> is performed on inference data <b>13702</b> that includes the input data (with the image channels derived from the sequencing images <b>108</b> and/or the supplemental channels (e.g., the distance channels, the scaling channel)). The inference <b>13700</b> is operationalized by a tester <b>13704</b>.</p><p id="p-0997" num="0996">Directly Predicting Base Call Quality</p><p id="p-0998" num="0997">The second well-calibrated neural network is the neural network-based quality scorer <b>13802</b> that processes input data derived from the sequencing images <b>108</b> and directly produces a quality indication.</p><p id="p-0999" num="0998">In one implementation, the neural network-based quality scorer <b>13802</b> is a multilayer perceptron (MLP). In another implementation, the neural network-based quality scorer <b>13802</b> is a feedforward neural network. In yet another implementation, the neural network-based quality scorer <b>13802</b> is a fully-connected neural network. In a further implementation, the neural network-based quality scorer <b>13802</b> is a fully convolutional neural network. In yet further implementation, the neural network-based quality scorer <b>13802</b> is a semantic segmentation neural network.</p><p id="p-1000" num="0999">In one implementation, the neural network-based quality scorer <b>13802</b> is a convolutional neural network (CNN) with a plurality of convolution layers. In another implementation, it is a recurrent neural network (RNN) such as a long short-term memory network (LSTM), bi-directional LSTM (Bi-LSTM), or a gated recurrent unit (GRU). In yet another implementation, it includes both a CNN and a RNN.</p><p id="p-1001" num="1000">In yet other implementations, the neural network-based quality scorer <b>13802</b> can use 1D convolutions, 2D convolutions, 3D convolutions, 4D convolutions, 5D convolutions, dilated or atrous convolutions, transpose convolutions, depthwise separable convolutions, pointwise convolutions, 1&#xd7;1 convolutions, group convolutions, flattened convolutions, spatial and cross-channel convolutions, shuffled grouped convolutions, spatial separable convolutions, and deconvolutions. It can use one or more loss functions such as logistic regression/log loss, multi-class cross-entropy/softmax loss, binary cross-entropy loss, mean-squared error loss, L1 loss, L2 loss, smooth L1 loss, and Huber loss. It can use any parallelism, efficiency, and compression schemes such TFRecords, compressed encoding (e.g., PNG), sharding, parallel calls for map transformation, batching, prefetching, model parallelism, data parallelism, and synchronous/asynchronous SGD. It can include upsampling layers, downsampling layers, recurrent connections, gates and gated memory units (like an LSTM or GRU), residual blocks, residual connections, highway connections, skip connections, peephole connections, activation functions (e.g., non-linear transformation functions like rectifying linear unit (ReLU), leaky ReLU, exponential liner unit (ELU), sigmoid and hyperbolic tangent (tan h)), batch normalization layers, regularization layers, dropout, pooling layers (e.g., max or average pooling), global average pooling layers, and attention mechanisms.</p><p id="p-1002" num="1001">In some implementations, the neural network-based quality scorer <b>13802</b> has the same architecture as the neural network-based base caller <b>1514</b>.</p><p id="p-1003" num="1002">The input data can include the image channels derived from the sequencing images <b>108</b> and/or the supplemental channels (e.g., the distance channels, the scaling channel). The neural network-based quality scorer <b>13802</b> processes the input data and produces an alternative representation of the input data. The alternative representation is a convolved representation in some implementations and a hidden representation in other implementations. The alternative representation is then processed by an output layer to produce an output. The output is used to produce the quality indication.</p><p id="p-1004" num="1003">In one implementation, the same input data is fed to the neural network-based base caller <b>1514</b> and the neural network-based quality scorer <b>13802</b> to produce (i) a base call from the neural network-based base caller <b>1514</b> and (ii) a corresponding quality indication from the neural network-based quality scorer <b>13802</b>. In some implementations, the neural network-based base caller <b>1514</b> and the neural network-based quality scorer <b>13802</b> are jointly trained with end-to-end backpropagation.</p><p id="p-1005" num="1004">In one implementation, the neural network-based quality scorer <b>13802</b> outputs a quality indication for a single target cluster for a particular sequencing cycle. In another implementation, it outputs a quality indication for each target cluster in a plurality of target clusters for the particular sequencing cycle. In yet another implementation, it outputs a quality indication for each target cluster in a plurality of target clusters for each sequencing cycle in a plurality of sequencing cycles, thereby producing a quality indication sequence for each target cluster.</p><p id="p-1006" num="1005">In one implementation, the neural network-based quality scorer <b>13802</b> is a convolutional neural network trained on training examples comprising data from the sequencing images <b>108</b> and labeled with base call quality ground truths. The neural network-based quality scorer <b>13802</b> is trained using a backpropagation-based gradient update technique that progressively matches base call quality predictions <b>13804</b> of the convolutional neural network <b>13802</b> with the base call quality ground truths <b>13808</b>. In some implementations, we label a base as 0 if it was a wrong base call and 1 if otherwise. As a result, the output corresponds to the probability of error. In one implementation, this obviates the need of using the sequence context as input features.</p><p id="p-1007" num="1006">An input module of the convolutional neural network <b>13802</b> feeds data from the sequencing images <b>108</b> captured at one or more sequencing cycles to the convolutional neural network <b>13802</b> for determining quality of one or more bases called for one or more clusters.</p><p id="p-1008" num="1007">An output module of the convolutional neural network <b>13802</b> translates analysis by the convolutional neural network <b>13802</b> into an output <b>13902</b> that identifies the quality of the one or more bases called for the one or more clusters.</p><p id="p-1009" num="1008">In one implementation, the output module further comprises a softmax classification layer that produces likelihoods for the quality status being high-quality, medium-quality (optional, as indicated by dotted lines), and low-quality. In another implementation, the output module further comprises a softmax classification layer that produces likelihoods for the quality status being high-quality and low-quality. A person skilled in the art will appreciate that other classes that bucket quality scores differently and discernably can be used. The softmax classification layer produces likelihoods for the quality being assigned a plurality of quality scores. Based on the likelihoods, the quality is assigned a quality score from one of the plurality of quality scores. The quality scores are logarithmically based on base calling error probabilities. The plurality of quality scores includes Q6, Q10, Q15, Q20, Q22, Q27, Q30, Q33, Q37, Q40, and Q50. In another implementation, the output module further comprises a regression layer that produces continuous values which identify the quality.</p><p id="p-1010" num="1009">In some implementations, the neural network-based quality scorer <b>13802</b> further comprises a supplemental input module that supplements the data from the sequencing images <b>108</b> with quality predictor values for the bases called and feeds the quality predictor values to the convolutional neural network <b>13802</b> along with the data from the sequencing images.</p><p id="p-1011" num="1010">In some implementations, the quality predictor values include online overlap, purity, phasing, start5, hexamer score, motif accumulation, endiness, approximate homopolymer, intensity decay, penultimate chastity, signal overlap with background (SOWB), and/or shifted purity G adjustment. In other implementations, the quality predictor values include peak height, peak width, peak location, relative peak locations, peak height ration, peak spacing ration, and/or peak correspondence. Additional details about the quality predictor values can be found in US Patent Publication Nos. 2018/0274023 and 2012/0020537, which are incorporated by reference as if fully set forth herein.</p><p id="p-1012" num="1011">Training</p><p id="p-1013" num="1012"><figref idref="DRAWINGS">FIG. <b>138</b></figref> shows one implementation of training <b>13800</b> the neural network-based quality scorer <b>13802</b> to process input data derived from the sequencing images <b>108</b> and directly produce quality indications. The neural network-based quality scorer <b>13802</b> is trained using a backpropagation-based gradient update technique that compares the predicted quality indications <b>13804</b> against the correct quality indications <b>13808</b> and computes an error <b>13806</b> based on the comparison. The error <b>13806</b> is then used to calculate gradients, which are applied to the weights and parameters of the neural network-based quality scorer <b>13802</b> during backward propagation <b>13810</b>. The training <b>13800</b> is operationalized by the trainer <b>1510</b> using a stochastic gradient update algorithm such as ADAM.</p><p id="p-1014" num="1013">The trainer <b>1510</b> uses training data <b>13812</b> (derived from the sequencing images <b>108</b>) to train the neural network-based quality scorer <b>13802</b> over thousands and millions of iterations of the forward propagation <b>13816</b> that produces the predicted quality indications and the backward propagation <b>13810</b> that updates the weights and parameters based on the error <b>13806</b>. In some implementations, the training data <b>13812</b> is supplemented with the quality predictor values <b>13814</b>. Additional details about the training <b>13800</b> can be found in Appendix entitled &#x201c;Deep Learning Tools&#x201d;.</p><p id="p-1015" num="1014">Inference</p><p id="p-1016" num="1015"><figref idref="DRAWINGS">FIG. <b>139</b></figref> shows one implementation of directly producing quality indications as outputs of the neural network-based quality scorer <b>13802</b> during inference <b>13900</b>. The inference <b>13900</b> includes hundreds, thousands, and/or millions of iterations of forward propagation <b>13908</b>, including parallelization techniques such as batching. The inference <b>13900</b> is performed on inference data <b>13904</b> that includes the input data (with the image channels derived from the sequencing images <b>108</b> and/or the supplemental channels (e.g., the distance channels, the scaling channel)). In some implementations, the inference data <b>13904</b> is supplemented with the quality predictor values <b>13906</b>. The inference <b>13900</b> is operationalized by a tester <b>13910</b>.</p><p id="p-1017" num="1016">Lossless Transformations</p><p id="p-1018" num="1017"><figref idref="DRAWINGS">FIG. <b>140</b></figref> depicts one implementation of using lossless transformation <b>1400</b> to generate transformed data <b>14004</b> that can be fed as input to the neural network-based template generator <b>1512</b>, the neural network-based base caller <b>1514</b>, and the neural network-based quality scorer <b>13802</b>. Some examples of the lossless transformation <b>1400</b> include convolutions, deconvolutions, and Fourier transforms.</p><p id="p-1019" num="1018">The lossless transformation <b>1400</b> can be applied by a lossless transformer <b>14002</b> that comprises a plurality of filters 1-n (e.g., convolution filters with convolution kernels). The lossless transformation <b>1400</b> can be applied on the input data <b>9500</b> and/or the input image data <b>1702</b> to produce the transformed data <b>14004</b>.</p><p id="p-1020" num="1019">The transformed data <b>14004</b> can be fed as input to the neural network-based template generator <b>1512</b> to produce the cluster metadata, to the neural network-based base caller <b>1514</b> to produce the base calls, and/or to the neural network-based quality scorer <b>13802</b> to produce the quality scores.</p><p id="p-1021" num="1020">In some implementations, the transformed data <b>14004</b> is deconvolved by a deconvolution layer <b>14006</b> to reconstruct essential features of the input data <b>9500</b> and/or the input image data <b>1702</b>. The deconvolution layer <b>14006</b> can be an initial layer of the neural network-based template generator <b>1512</b>, the neural network-based base caller <b>1514</b>, and/or the neural network-based quality scorer <b>13802</b>.</p><p id="p-1022" num="1021">End-to-End Integration with Intensity Modification</p><p id="p-1023" num="1022">The discussion now turns to how the neural network-based template generator <b>1512</b> is integrated with the neural network-based base caller <b>1514</b> using intensity modification techniques.</p><p id="p-1024" num="1023">In many of the base calling implementations discussed above, the input data <b>9500</b> that is fed as input to the neural network-based base caller <b>1514</b> comprises: (i) the image data <b>7902</b> (image channels) derived from the sequencing images <b>108</b>, (ii) the supplemental distance data (distance channels) derived from the output <b>1714</b> of the neural network-based template generator <b>1512</b> (e.g., the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>), and (iii) the supplemental scaling data (scaling channel). In these implementations, intensity values in the image data <b>7902</b> are not modified, but rather supplemented with distance values that communicate the cluster shape information by conveying which pixels in the image data <b>7902</b> contain the cluster centers and which pixels in the image data <b>7902</b> are farther away from the cluster centers.</p><p id="p-1025" num="1024">We now disclose base calling implementations that modify the image data <b>7902</b> to incorporate the cluster shape information, thus obviating the need of calculating and using the supplemental distance channels. The image data <b>7902</b> is modified based on the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b>, which are in turn the output <b>1714</b> of the neural network-based template generator <b>1512</b>. Thus, in this context, &#x201c;integration&#x201d; refers to modifying data processed by the neural network-based base caller <b>1514</b> based on information produced by the neural network-based template generator <b>1512</b> (e.g., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b>), as opposed to supplementing the former with the latter.</p><p id="p-1026" num="1025">Both the decay and ternary maps contain the cluster shape information that identifies the subpixels as: (1) background subpixels, (2) cluster center subpixels, and (3) cluster or cluster interior subpixels belonging to a same cluster. The cluster shape information is included in the template image in the upsampled, subpixel domain to distinguish the cluster boundaries at a fine-grained level. However, the image data <b>7902</b>, which contains the cluster and background intensities, is typically in the optical, pixel domain.</p><p id="p-1027" num="1026">Though the template image and the image data <b>7902</b> are in different domains, they represent the same imaged area. The template image is derived from processing of the input image data <b>1702</b> for a certain number of initial sequencing cycles of a sequencing run and post-processing of the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>. In contrast, modifications for cluster shape information incorporation are not limited to the image data <b>7902</b> for the initial sequencing cycles, but instead applied to the image data <b>7902</b> for each sequencing cycle that is to be base called.</p><p id="p-1028" num="1027">In some implementations though, when the cluster sizes are large enough, the output of the neural network-based base caller <b>1514</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the optical, pixel domain. Accordingly, in such implementations, the template image is also in the optical, pixel domain.</p><p id="p-1029" num="1028">So, consider, for example, that the sequencing run comprises 300 sequencing cycles that are to be base called. Then, the template image is derived from the processing the input image data <b>1702</b> for the first 2 to 7 sequencing cycles through the neural network-based template generator <b>1512</b> and post-processing of the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> produced by the neural network-based template generator <b>1512</b> as output Whereas the image data <b>7902</b> for each of the 300 sequencing cycles is modified for cluster shape information and then processed through the neural network-based base caller <b>1514</b> to produce a base call for each of the 300 sequencing cycles.</p><p id="p-1030" num="1029">We disclose intensity modification techniques that incorporate the cluster shape information in the image data <b>7902</b> for base calling by the neural network-based base caller <b>1514</b>. More details follow.</p><p id="p-1031" num="1030">Area Weighting Factoring (AWF)</p><p id="p-1032" num="1031">The first type of intensity modification techniques are area weighting factoring techniques in which the intensity modifications are applied to pixels in the image data <b>7902</b> in the optical, pixel domain.</p><p id="p-1033" num="1032">Since the template image and the image data <b>7902</b> represent the same imaged area, there is many-to-one correspondence between subpixel blocks in the template image and respective pixels in the image data <b>7902</b>. For example, the first block of 16 subpixels in the template image corresponds to the first pixel in the image data <b>7902</b>, the second block of 16 subpixels in the template image corresponds to the second pixel in the image data <b>7902</b>, and so on.</p><p id="p-1034" num="1033">For a given cluster that is to be base called, we access its cluster shape information from the template image and identify which pixels in the image data <b>7902</b> contain parts of the given cluster, i.e., which pixels in the image data <b>7902</b> cover the given cluster or depict intensity emissions from the given cluster.</p><p id="p-1035" num="1034">Then, for each identified pixel in the image data <b>7902</b>, we determine how many subpixels in the template image that correspond to the identified pixel in the image data <b>7902</b> contain parts of the given cluster, i.e., how many subpixels in the template image that correspond to the identified pixel in the image data <b>7902</b> cover the given cluster or depict intensity emissions from the given cluster. Then, based on the determined subpixel count, we calculate and assign an area weighting factor (AWF) to each identified pixel in the image data <b>7902</b>.</p><p id="p-1036" num="1035">Factoring a Single Cluster Per Pixel</p><p id="p-1037" num="1036">AWF for a single pixel i is calculated as follows:</p><p id="p-1038" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mrow>  <mrow>   <mi>AWF</mi>   <mo>&#x2062;</mo>   <mtext>  </mtext>   <mi>for</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>pixel</mi>   <mo>&#x2062;</mo>   <mrow>    <mtext> </mtext>    <mtext>  </mtext>   </mrow>   <mo>&#x2062;</mo>   <mi>i</mi>  </mrow>  <mo>=</mo>  <mfrac>   <mtable>    <mtr>     <mtd>      <mrow>       <mi>Number</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>of</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>subpixels</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>in</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>the</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>template</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>image</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>that</mi>       <mtext>   </mtext>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mi>correspond</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>to</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>pixel</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>i</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>and</mi>       <mo>&#x2062;</mo>       <mtext>    </mtext>       <mi>depict</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>the</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>given</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>cluster</mi>       <mo>&#x2062;</mo>       <mrow>        <mtext> </mtext>        <mtext>  </mtext>       </mrow>      </mrow>     </mtd>    </mtr>   </mtable>   <mrow>    <mi>Total</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>number</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>of</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>subpixels</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>that</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>correspond</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>to</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>pixel</mi>    <mo>&#x2062;</mo>    <mtext>  </mtext>    <mi>i</mi>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-1039" num="1037">The above AWF calculation excludes from the subpixel count: (i) background subpixels and (ii) subpixels containing parts of any other cluster (i.e., subpixels depicting clusters other than the given cluster). An example of this is illustrated in <figref idref="DRAWINGS">FIG. <b>143</b></figref>.</p><p id="p-1040" num="1038">We then modify each identified pixel's intensity value based on its AWF. This yields a modified version of the image data <b>7902</b>, which is processed by the neural network-based base caller <b>1514</b> to base call the given cluster.</p><p id="p-1041" num="1039">Modified intensity value (MIV) of pixel i is calculated as follows:</p><p id="p-1042" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>MIV of pixel <i>i</i>=AWF for pixel <i>i</i>&#xd7;optical intensity value of pixel <i>i </i>(in the image data <b>7902</b>)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-1043" num="1040"><figref idref="DRAWINGS">FIG. <b>143</b></figref> depicts one example of area weighting factoring <b>14300</b> for contribution from only a single cluster per pixel. In <figref idref="DRAWINGS">FIG. <b>143</b></figref>, intensities of pixels in sequencing image <b>14304</b> of the image data <b>7902</b> are modified. The sequencing image <b>14304</b> comprises four pixels with intensities <b>100</b>, <b>140</b>, <b>160</b>, and <b>320</b>, respectively.</p><p id="p-1044" num="1041">Template image <b>14302</b> contains the cluster shape information for the sequencing image <b>14304</b>. The template image <b>14302</b> includes four subpixel blocks respectively corresponding to the four pixels in the sequencing image <b>14304</b> (i.e., sixteen subpixels in the template image <b>14302</b> per pixel in the sequencing image <b>14304</b>). The template image <b>14302</b> also identifies background subpixels and cluster subpixels for three clusters A, B, and C.</p><p id="p-1045" num="1042">An AWF for each of the four pixels in the sequencing image <b>14304</b> is then calculated to account only for cluster A per pixel and stored as AWFs <b>14306</b> in the template image <b>14302</b>. Note that the AWFs for the second and third pixels are 7/16 and 8/16, respectively. Even though the second pixel receives contributions from two clusters A and C, its AWF takes into account only the seven subpixels that cover cluster A (in red) and ignores the four subpixels that cover cluster C (in orange). Similarly, even though the third pixel receives contributions from two clusters A and B, its AWF takes into account only the eight subpixels that cover cluster A (in red) and ignores the four subpixels that cover cluster B (in green). Background subpixels are not counted.</p><p id="p-1046" num="1043">The AWFs <b>14306</b> are further used to modify intensities of each of the four pixels and to produce a modified sequencing image <b>14308</b>. The modified sequencing image <b>14308</b> is processed by the neural network-based base caller <b>1514</b> for base calling.</p><p id="p-1047" num="1044">Factoring Multiple Clusters Per Pixel</p><p id="p-1048" num="1045">In some implementations, we account for contributions from multiple clusters to a single pixel in the image data <b>7902</b>. AWF for a single pixel i that receives contributions from multiple clusters is calculated as following:</p><p id="p-1049" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mrow>  <mrow>   <mi>AWF</mi>   <mo>&#x2062;</mo>   <mtext>  </mtext>   <mi>for</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>pixel</mi>   <mo>&#x2062;</mo>   <mrow>    <mtext> </mtext>    <mtext>  </mtext>   </mrow>   <mo>&#x2062;</mo>   <mi>i</mi>  </mrow>  <mo>=</mo>  <mfrac>   <mtable>    <mtr>     <mtd>      <mrow>       <mi>Number</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>of</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>subpixels</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>in</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>the</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>template</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>image</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>that</mi>       <mtext>   </mtext>      </mrow>     </mtd>    </mtr>    <mtr>     <mtd>      <mrow>       <mi>correspond</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>to</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>pixel</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>i</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>and</mi>       <mo>&#x2062;</mo>       <mtext>    </mtext>       <mi>depict</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>at</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>least</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>one</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>cluster</mi>       <mo>&#x2062;</mo>       <mrow>        <mtext> </mtext>        <mtext>  </mtext>       </mrow>      </mrow>     </mtd>    </mtr>   </mtable>   <mrow>    <mi>Total</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>number</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>of</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>subpixels</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>that</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>correspond</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>to</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>pixel</mi>    <mo>&#x2062;</mo>    <mtext>  </mtext>    <mi>i</mi>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-1050" num="1046">The above AWF calculation excludes background subpixels from the subpixel count, but includes in the subpixel count those subpixels that contain parts of other clusters (i.e., subpixels depicting clusters other than the given cluster). An example of this is illustrated in <figref idref="DRAWINGS">FIG. <b>144</b></figref>.</p><p id="p-1051" num="1047"><figref idref="DRAWINGS">FIG. <b>144</b></figref> depicts one example of area weighting factoring <b>14400</b> for contributions from multiple clusters per pixel. In <figref idref="DRAWINGS">FIG. <b>144</b></figref>, intensities of pixels in sequencing image <b>14404</b> of the image data <b>7902</b> are modified. The sequencing image <b>14404</b> comprises four pixels with intensities <b>100</b>, <b>140</b>, <b>160</b>, and <b>320</b>, respectively.</p><p id="p-1052" num="1048">Template image <b>14402</b> contains the cluster shape information for the sequencing image <b>14404</b>. The template image <b>14402</b> includes four subpixel blocks respectively corresponding to the four pixels in the sequencing image <b>14404</b> (i.e., sixteen subpixels in the template image <b>14302</b> per pixel in the sequencing image <b>14404</b>). The template image <b>14402</b> also identifies background subpixels and cluster subpixels for three clusters A, B, and C.</p><p id="p-1053" num="1049">An AWF for each of the four pixels in the sequencing image <b>14404</b> is then calculated to account for all the three clusters A, B, and C per pixel and stored as AWFs <b>14406</b> in the template image <b>14402</b>. Note that the AWFs for the second and third pixels are 11/16 and 12/16, respectively. Since the second pixel receives contributions from two clusters A and C, its AWF takes into account the seven subpixels that cover cluster A (in red) and also the four subpixels that cover cluster C (in orange). Similarly, since the third pixel receives contributions from two clusters A and B, its AWF takes into account the eight subpixels that cover cluster A (in red) and also the four subpixels that cover cluster B (in green). Background subpixels are not counted.</p><p id="p-1054" num="1050">The AWFs <b>14406</b> are further used to modify intensities of each of the four pixels and to produce a modified sequencing image <b>14408</b>. The modified sequencing image <b>14408</b> is processed by the neural network-based base caller <b>1514</b> for base calling.</p><p id="p-1055" num="1051">The area weighting factoring techniques described above can be used for base calling a single target cluster and also for simultaneously base calling multiple target clusters.</p><p id="p-1056" num="1052">Upsampling and Background Masking</p><p id="p-1057" num="1053">The second type of intensity modification techniques are upsampling and background masking techniques in which the image data <b>7902</b> is first upsampled to be in the same upsampled, subpixel domain as the template image and then the intensity modifications are applied to subpixels in the upsampled version of the image data <b>7902</b>.</p><p id="p-1058" num="1054">Since the template image and the image data <b>7902</b> represent the same imaged area, there is one-to-one correspondence between subpixels in the template image and respective subpixels in the upsampled version of the image data <b>7902</b>. For example, the first subpixel in the template image corresponds to the first subpixel in the upsampled version of the image data <b>7902</b>, the second subpixel in the template image corresponds to the second subpixel in the upsampled version of the image data <b>7902</b>, and so on.</p><p id="p-1059" num="1055">In some implementations though, when the cluster sizes are large enough, the output of the neural network-based base caller <b>1514</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the optical, pixel domain. Accordingly, in such implementations, the template image is also in the optical, pixel domain.</p><p id="p-1060" num="1056">Interpolation</p><p id="p-1061" num="1057">Using the cluster shape information in the template image, we first identify, among the subpixels in the template image that blockwise correspond to the pixels in the image data <b>7902</b>, which subpixels in the template image are background subpixels not contributing to/depicting intensity emissions from/covering any cluster and which are cluster subpixels contributing to/depicting intensity emissions from/covering at least one cluster.</p><p id="p-1062" num="1058">We then use interpolation to upsample the image data <b>7902</b> in the upsampled, subpixel domain and produce the upsampled version of the image data <b>7902</b> such that (1) those subpixels in the upsampled version of the image data <b>7902</b> that respectively correspond to the identified background subpixels in the template image are assigned a same background intensity (e.g., a zero value or a near zero value) and (2) those subpixels in the upsampled version of the image data <b>7902</b> that respectively correspond to the identified cluster subpixels in the template image are assigned cluster intensities that are interpolated from the pixel intensities in the optical, pixel domain. An example of this is illustrated in <figref idref="DRAWINGS">FIG. <b>145</b></figref>.</p><p id="p-1063" num="1059"><figref idref="DRAWINGS">FIG. <b>145</b></figref> depicts one example of using interpolation for upsampling and background masking <b>14500</b>. In <figref idref="DRAWINGS">FIG. <b>145</b></figref>, intensities of pixels in sequencing image <b>14504</b> of the image data <b>7902</b> are modified. The sequencing image <b>14504</b> comprises four pixels with intensities <b>160</b>, <b>80</b>, <b>320</b>, and <b>200</b>, respectively.</p><p id="p-1064" num="1060">Template image <b>14502</b> contains the cluster shape information for the sequencing image <b>14504</b>. The template image <b>14502</b> includes four subpixel blocks respectively corresponding to the four pixels in the sequencing image <b>14504</b> (i.e., sixteen subpixels in the template image <b>14502</b> per pixel in the sequencing image <b>14504</b>). The template image <b>14502</b> also identifies background subpixels and cluster subpixels for three clusters A, B, and C.</p><p id="p-1065" num="1061">Interpolation is used to upsample the sequencing image <b>14504</b> and to produce an upsampled sequencing image <b>14506</b> with subpixels. The interpolation assigns the background subpixels the background intensity and assigns the cluster subpixels the cluster intensities interpolated from the pixel intensities.</p><p id="p-1066" num="1062">Subpixel Count Weighting</p><p id="p-1067" num="1063">Here, the cluster intensities are calculated differently. That is, instead of interpolating the pixel intensities, each pixel's intensity in the optical, pixel domain is distributed equally among those cluster subpixels in the upsampled version of the image data <b>7902</b> that constitute the corresponding pixel. For each pixel, the count of its constituent cluster subpixels among which its intensity is equally distributed can be determined based on the area weighting factor (AWF) described above that takes into account contributions from multiple clusters. The background subpixels are assigned the background intensity, as discussed above. An example of this is illustrated in <figref idref="DRAWINGS">FIG. <b>146</b></figref>.</p><p id="p-1068" num="1064"><figref idref="DRAWINGS">FIG. <b>146</b></figref> depicts one example of using subpixel count weighting for upsampling and background masking <b>14600</b>. In <figref idref="DRAWINGS">FIG. <b>146</b></figref>, intensities of pixels in sequencing image <b>14604</b> of the image data <b>7902</b> are modified. The sequencing image <b>14604</b> comprises four pixels with intensities <b>160</b>, <b>80</b>, <b>320</b>, and <b>200</b>, respectively.</p><p id="p-1069" num="1065">Template image <b>14602</b> contains the cluster shape information for the sequencing image <b>14604</b>. The template image <b>14602</b> includes four subpixel blocks respectively corresponding to the four pixels in the sequencing image <b>14604</b> (i.e., sixteen subpixels in the template image <b>14602</b> per pixel in the sequencing image <b>14604</b>). The template image <b>14602</b> also identifies background subpixels and cluster subpixels for three clusters A, B, and C.</p><p id="p-1070" num="1066">Subpixel count weighting is used to upsample the sequencing image <b>14604</b> and produce an upsampled sequencing image <b>14606</b> with subpixels. The subpixel count weighting assigns the background subpixels the background intensity and distributes each pixel's entire intensity to its constituent cluster subpixels. That is, the intensity allocation from the pixel to its constituent cluster subpixels utilizes all of the pixel's intensity, without wasting some of the pixel's intensity on no or minimal allocation to the background subpixels constituting the pixel. In some implementations, the pixel's intensity is equally distributed among its constituent cluster subpixels.</p><p id="p-1071" num="1067">In other implementations, the upsampling is performed using at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage.</p><p id="p-1072" num="1068">In some implementations, prior to the upsampling, the image data <b>7902</b> is aligned with the template image using cycle-specific and imaging channel-specific transformations.</p><p id="p-1073" num="1069">The upsampled version of the image data <b>7902</b>, containing the cluster intensities and the background intensity, is processed by the neural network-based base caller <b>1514</b> for base calling.</p><p id="p-1074" num="1070">In other implementations, the values in the decay map, the binary map, and/or the ternary map are used to directly modulate the intensities of pixels in the image data <b>7902</b> or the intensities of subpixels in the upsampled version of the image data <b>7902</b>.</p><p id="p-1075" num="1071">Integration Workflow</p><p id="p-1076" num="1072">Area Weighting Factoring</p><p id="p-1077" num="1073"><figref idref="DRAWINGS">FIG. <b>141</b></figref> illustrates one implementation of integrating the neural network-based template generator <b>1512</b> with the neural network-based base caller <b>1514</b> using area weighting factoring.</p><p id="p-1078" num="1074">First, the neural network-based template generator <b>1512</b> processes the input image data <b>1702</b> for some initial sequencing cycles of a sequencing run and produces as output the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>. The input image data <b>1702</b> is in turn derived from the sequencing images <b>108</b>, as described above with reference to <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>to <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In one implementation, the input image data <b>1702</b> is in the upsampled, subpixel domain/resolution prior to being fed as input to the neural network-based template generator <b>1512</b>. In another implementation, an upsampling layer of the neural network-based template generator <b>1512</b> upsamples the input image data <b>1702</b> to be in the upsampled, subpixel domain/resolution. The upsampling can be achieved by interpolation techniques such as bicubic interpolation.</p><p id="p-1079" num="1075">From the output <b>1714</b> (the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>) of the neural network-based template generator <b>1512</b>, the template image <b>14102</b> is derived through post-processing as discussed above. The template image <b>14202</b> contains the cluster metadata in the upsampled, subpixel domain/resolution. The cluster metadata <b>1812</b> identifies cluster centers, cluster shapes, cluster boundaries, and/or cluster background. &#x201c;Template image&#x201d; or &#x201c;template&#x201d; can refer to a data structure that contains or identifies the cluster metadata <b>1812</b> derived from the decay map <b>1716</b>, the ternary map <b>1718</b>, and/or the binary map <b>1718</b>.</p><p id="p-1080" num="1076">In some implementations though, when the cluster sizes are large enough, the output of the neural network-based base caller <b>1514</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the optical, pixel domain. Accordingly, in such implementations, the template image <b>14202</b> is also in the optical, pixel domain.</p><p id="p-1081" num="1077">Then, an area weighting factor determiner <b>14104</b> uses the template image <b>14102</b> to determine the area weighting factors and store them in the template image <b>14102</b>, as discussed above.</p><p id="p-1082" num="1078">Then, for each of the sequencing cycles of the sequencing run, the image data <b>7902</b> is modified by an intensity modifier <b>14106</b> based on the area weighting factors stored in the template image <b>14102</b>. In other implementations, the area weighting factors can be stored elsewhere.</p><p id="p-1083" num="1079">What results is intensity modified images <b>14108</b>, which are processed by the neural network-based base caller <b>1514</b> to produce the base calls <b>14110</b>. Note that the intensity modified images <b>14108</b> do not include any supplemental distance channels, but can include the supplemental scaling channel.</p><p id="p-1084" num="1080">In other implementations, the intensity modification is performed only for a subset of the sequencing cycles of the sequencing run.</p><p id="p-1085" num="1081">Upsampling and Background Masking</p><p id="p-1086" num="1082"><figref idref="DRAWINGS">FIG. <b>142</b></figref> illustrates another implementation of integrating the neural network-based template generator <b>1512</b> with the neural network-based base caller <b>1514</b> using upsampling and background masking.</p><p id="p-1087" num="1083">First, the neural network-based template generator <b>1512</b> processes the input image data <b>1702</b> for some initial sequencing cycles of a sequencing run and produces as output the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>. The input image data <b>1702</b> is in turn derived from the sequencing images <b>108</b>, as described above with reference to <figref idref="DRAWINGS">FIG. <b>21</b><i>b </i></figref>to <figref idref="DRAWINGS">FIG. <b>24</b></figref>. In one implementation, the input image data <b>1702</b> is in the upsampled, subpixel domain/resolution prior to being fed as input to the neural network-based template generator <b>1512</b>. In another implementation, an upsampling layer of the neural network-based template generator <b>1512</b> upsamples the input image data <b>1702</b> to be in the upsampled, subpixel domain/resolution. The upsampling can be achieved by interpolation techniques such as bicubic interpolation.</p><p id="p-1088" num="1084">From the output (the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b>) of the neural network-based template generator <b>1512</b>, the template image <b>14202</b> is derived through post-processing as discussed above. The template image <b>14202</b> contains the cluster metadata in the upsampled, subpixel domain/resolution. The cluster metadata <b>1812</b> identifies cluster centers, cluster shapes, cluster boundaries, and/or cluster background. &#x201c;Template image&#x201d; or &#x201c;template&#x201d; can refer to a data structure that contains or identifies the cluster metadata <b>1812</b> derived from the decay map <b>1716</b>, the ternary map <b>1718</b>, and/or the binary map <b>1718</b>.</p><p id="p-1089" num="1085">In some implementations though, when the cluster sizes are large enough, the output of the neural network-based base caller <b>1514</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the optical, pixel domain. Accordingly, in such implementations, the template image <b>14202</b> is also in the optical, pixel domain.</p><p id="p-1090" num="1086">Then, an image integrator <b>14204</b> uses the template image <b>14202</b> to upsample the image data <b>7902</b> for each of the sequencing cycles of the sequencing run using interpolation or subpixel count weighting and to produce an upsampled version <b>14212</b> of the image data <b>7902</b> for each of the sequencing cycles of the sequencing run, as discussed above.</p><p id="p-1091" num="1087">The upsampling is operationalized by an image upsampler <b>14208</b>. In one implementation, the upsampled version <b>14212</b> of the image data <b>7902</b> is generated prior to being fed as input to the neural network-based base caller <b>1514</b>. In another implementation, an upsampling layer the neural network-based base caller <b>1514</b> upsamples the image data <b>7902</b> and generates the upsampled version <b>14212</b> of the image data <b>7902</b>. The upsampling can be achieved by interpolation techniques such as bicubic interpolation.</p><p id="p-1092" num="1088">The image integrator <b>14204</b> also applies a background mask to the background subpixels in the upsampled version <b>14212</b> of the image data <b>7902</b> for each of the sequencing cycles of the sequencing run, as discussed above. The background masking is applied by a background masker <b>14210</b>.</p><p id="p-1093" num="1089">In some implementations, prior to the upsampling, the image integrator <b>14204</b> also aligns the image data <b>7902</b> for each of the sequencing cycles of the sequencing run with the template image <b>14202</b>, as discussed above. The aligning is operationalized by an image aligner <b>14206</b>.</p><p id="p-1094" num="1090">Then, for each of the sequencing cycles of the sequencing run, the upsampled version <b>14212</b> of the image data <b>7902</b> is processed by the neural network-based base caller <b>1514</b> to produce the base calls <b>14214</b>. Note that the upsampled version <b>14212</b> of the image data <b>7902</b> does not include any supplemental distance channels, but can include the supplemental scaling channel.</p><p id="p-1095" num="1091">In other implementations, the upsampling and background masking is performed only for a subset of the sequencing cycles of the sequencing run.</p><p id="p-1096" num="0000">End-to-End Integration without Intensity Modification, instead using Non-Distance Supplemental Channels</p><p id="p-1097" num="1092">The discussion now turns to how the neural network-based template generator <b>1512</b> is integrated with the neural network-based base caller <b>1514</b> without modifying intensity data of the sequencing images. The implementations discussed below provide new supplemental channels that are different than the supplemental distance channel discussed above. These new supplemental channels also convey the cluster shape information.</p><p id="p-1098" num="1093">1. Decay Map, Ternary Map, Binary Map as Supplemental Channels</p><p id="p-1099" num="1094">We now disclose base calling implementations that supplement the image data <b>7902</b> with the output <b>1714</b> of the neural network-based template generator <b>1512</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b>. Thus, in this context, &#x201c;integration&#x201d; refers to supplementing data processed by the neural network-based base caller <b>1514</b> with information produced by the neural network-based template generator <b>1512</b> (e.g., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b>).</p><p id="p-1100" num="1095">The decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the upsampled, subpixel domain; however, the image data <b>7902</b>, which contains the cluster and background intensities, is typically in the optical, pixel domain.</p><p id="p-1101" num="1096">In some implementations though, when the cluster sizes are large enough, the output of the neural network-based base caller <b>1514</b>, i.e., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the optical, pixel domain. Accordingly, in such implementations, the template image is also in the optical, pixel domain.</p><p id="p-1102" num="1097">Upsampling the Input Image Data</p><p id="p-1103" num="1098">When the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are in the upsampled, subpixel domain, in some implementations, the input image data <b>1702</b> is upsampled to be in the upsampled, subpixel domain. In one implementation, the upsampler <b>2302</b> uses interpolation (e.g., bicubic interpolation) to upsample the sequencing images <b>108</b> in the series of image sets <b>2100</b> by an upsampling factor (e.g., 4&#xd7;) and the series of upsampled image sets <b>2300</b>.</p><p id="p-1104" num="1099">Then, the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> are subpixel-wise supplemented with the input image data <b>1702</b> (also in the upsampled, subpixel domain) and fed as a supplemental channel to the neural network-based base caller <b>1514</b>, along with the input image data <b>1702</b> (also in the upsampled, subpixel domain).</p><p id="p-1105" num="1100">Downsampling the Decay Map, Ternary Map, Binary Map</p><p id="p-1106" num="1101">In other implementations, when the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b> are produced in the upsampled, subpixel domain, they are downsampled to be in the optical, pixel domain. In one implementation, the downsampling can involve grouping subpixels based on a downsampling factor and taking an average of the output values of the grouped subpixels and assigning it to a corresponding pixel in the optical, pixel domain. The output values are weighted decay values in the case of the decay map <b>1716</b>. The output values are three-way classification scores in the case of the ternary map <b>1718</b>. The output values are two-way classification scores in the case of the binary map <b>1720</b>. In another implementation, the downsampling can involve grouping subpixels based on belonging to a same cluster and taking an average of the output values of the grouped subpixels and assigning it to corresponding pixel(s) in the optical, pixel domain.</p><p id="p-1107" num="1102">Then, the decay map <b>1716</b>, the ternary map <b>1718</b>, or the binary map <b>1720</b> in the optical, pixel domain are pixel-wise supplemented with the input image data <b>1702</b> (also in the optical, pixel domain) and fed as a supplemental channel to the neural network-based base caller <b>1514</b>, along with the input image data <b>1702</b> (also in the optical, pixel domain).</p><p id="p-1108" num="1103">2. Area Weighting Factors as Supplemental Channels</p><p id="p-1109" num="1104">In one implementation, area weighting factors contained in the template image (e.g., <b>14306</b> and <b>14406</b>) are calculated as described above, but instead of being used to modify the intensity values and generate modified sequencing images (e.g., <b>14308</b> and <b>14408</b>), they themselves are provided as supplemental channels that accompany the unmodified, original sequencing images (e.g., <b>14304</b> and <b>14404</b>). That is, since the area weighting factors contained in the template image (e.g., <b>14306</b> and <b>14406</b>) are in the optical, pixel domain, they are pixel-wise supplemented with the unmodified input image data <b>1702</b> (also in the optical, pixel domain) and fed as a supplemental channel to the neural network-based base caller <b>1514</b>, along with the unmodified input image data <b>1702</b> (also in the optical, pixel domain).</p><p id="p-1110" num="1105">Thus, in this context, &#x201c;integration&#x201d; refers to supplementing data processed by the neural network-based base caller <b>1514</b> with information (e.g., area weighting factors) derived from the output of the neural network-based template generator <b>1512</b> (e.g., the decay map <b>1716</b>, the ternary map <b>1718</b>, and the binary map <b>1720</b>).</p><heading id="h-0021" level="2">Data Pre-Processing</heading><p id="p-1111" num="1106">In some implementations, the technology disclosed uses pre-processing techniques that apply to pixels in the image data <b>202</b> and produce pre-processed image data <b>202</b><i>p</i>. In such implementations, instead of the image data <b>202</b>, the pre-processed image data <b>202</b><i>p </i>is provided as input to the neural network-based base caller <b>1514</b>. The data pre-processing is operationalized by a data pre-processor <b>15002</b>, which in turn can contain a data normalizer <b>15032</b> and a data augmenter <b>15034</b>.</p><p id="p-1112" num="1107"><figref idref="DRAWINGS">FIG. <b>150</b></figref> shows different implementations of data pre-processing, which can include data normalization and data augmentation.</p><p id="p-1113" num="1108">Data Normalization</p><p id="p-1114" num="1109">In one implementation, data normalization is applied on pixels in the image data <b>202</b> on an image patch-by-image patch basis. This includes normalizing intensity values of pixels in an image patch such that a pixel intensity histogram of the resulting normalized image patch has a fifth percentile of zero and a ninety-fifth percentile of one. That is, in the normalized image patch, (i) 5% of the pixels have intensity values less than zero and (ii) another 5% of the pixels have intensity values greater than one. Respective image patches of the image data <b>202</b> can be normalized separately, or the image data <b>202</b> can be normalized all at once. What results is normalized image patches <b>15016</b>, which are one example of the pre-processed image data <b>202</b><i>p</i>. The data normalization is operationalized by the data normalizer <b>15032</b>.</p><p id="p-1115" num="1110">Data Augmentation</p><p id="p-1116" num="1111">In one implementation, data augmentation is applied on the intensity values of the pixels in the image data <b>202</b>. This includes (i) multiplying the intensity values of all the pixels in the image data <b>202</b> with a same scaling factor and (ii) adding a same offset value to the scaled intensity values of all the pixels in the image data <b>202</b>. For a single pixel, this can be expressed by the following formulation:</p><p id="p-1117" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>augmented pixel intensity (API)=<i>aX+b </i><?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-1118" num="1112">where a is the scaling factor, X is the original pixel intensity, b is the offset value, aX is the scaled pixel intensity</p><p id="p-1119" num="1113">What results is augmented image patches <b>15026</b>, which are also one example of the pre-processed image data <b>202</b><i>p</i>. The data augmentation is operationalized by the data augmenter <b>15034</b>.</p><p id="p-1120" num="1114"><figref idref="DRAWINGS">FIG. <b>151</b></figref> shows that the data normalization technique (DeepRTA (norm)) and the data augmentation technique (DeepRTA (augment)) of <figref idref="DRAWINGS">FIG. <b>150</b></figref> reduce the base calling error percentage when the neural network-based base caller <b>1514</b> is trained on bacterial data and tested on human data, where the bacterial data and the human data share the same assay (e.g., both contain intronic data).</p><p id="p-1121" num="1115"><figref idref="DRAWINGS">FIG. <b>152</b></figref> shows that the data normalization technique (DeepRTA (norm)) and the data augmentation technique (DeepRTA (augment)) of <figref idref="DRAWINGS">FIG. <b>150</b></figref> reduce the base calling error percentage when the neural network-based base caller <b>1514</b> is trained on non-exonic data (e.g., intronic data) and tested on exonic data.</p><p id="p-1122" num="1116">In other words, the data normalization and the data augmentation techniques of <figref idref="DRAWINGS">FIG. <b>150</b></figref> allow the neural network-based base caller <b>1514</b> to generalize better on data not seen in training and thus reduce overfitting.</p><p id="p-1123" num="1117">In one implementation, the data augmentation is applied during both training and inference. In another implementation, the data augmentation is applied only during the training. In yet another implementation, the data augmentation is applied only during the inference.</p><p id="p-1124" num="1118"><figref idref="DRAWINGS">FIGS. <b>147</b>A and <b>147</b>B</figref> depict one implementation of a sequencing system. The sequencing system comprises a configurable processor.</p><p id="p-1125" num="1119"><figref idref="DRAWINGS">FIG. <b>147</b>C</figref> is a simplified block diagram of a system for analysis of sensor data from the sequencing system, such as base call sensor outputs.</p><p id="p-1126" num="1120"><figref idref="DRAWINGS">FIG. <b>148</b>A</figref> is a simplified diagram showing aspects of the base calling operation, including functions of a runtime program executed by a host processor.</p><p id="p-1127" num="1121"><figref idref="DRAWINGS">FIG. <b>148</b>B</figref> is a simplified diagram of a configuration of a configurable processor such as the one depicted in <figref idref="DRAWINGS">FIG. <b>147</b>C</figref>.</p><p id="p-1128" num="1122"><figref idref="DRAWINGS">FIG. <b>149</b></figref> is a computer system that can be used by the sequencing system of <figref idref="DRAWINGS">FIG. <b>147</b>A</figref> to implement the technology disclosed herein.</p><p id="p-1129" num="1123">Sequencing System</p><p id="p-1130" num="1124"><figref idref="DRAWINGS">FIGS. <b>147</b>A and <b>147</b>B</figref> depict one implementation of a sequencing system <b>14700</b>A. The sequencing system <b>14700</b>A comprises a configurable processor <b>14746</b>. The configurable processor <b>14746</b> implements the base calling techniques disclosed herein. The sequencing system is also referred to as a &#x201c;sequencer.&#x201d;</p><p id="p-1131" num="1125">The sequencing system <b>14700</b>A can operate to obtain any information or data that relates to at least one of a biological or chemical substance. In some implementations, the sequencing system <b>14700</b>A is a workstation that may be similar to a bench-top device or desktop computer. For example, a majority (or all) of the systems and components for conducting the desired reactions can be within a common housing <b>14702</b>.</p><p id="p-1132" num="1126">In particular implementations, the sequencing system <b>14700</b>A is a nucleic acid sequencing system configured for various applications, including but not limited to de novo sequencing, resequencing of whole genomes or target genomic regions, and metagenomics. The sequencer may also be used for DNA or RNA analysis. In some implementations, the sequencing system <b>14700</b>A may also be configured to generate reaction sites in a biosensor. For example, the sequencing system <b>14700</b>A may be configured to receive a sample and generate surface attached clusters of clonally amplified nucleic acids derived from the sample. Each cluster may constitute or be part of a reaction site in the biosensor.</p><p id="p-1133" num="1127">The exemplary sequencing system <b>14700</b>A may include a system receptacle or interface <b>14710</b> that is configured to interact with a biosensor <b>14712</b> to perform desired reactions within the biosensor <b>14712</b>. In the following description with respect to <figref idref="DRAWINGS">FIG. <b>147</b>A</figref>, the biosensor <b>14712</b> is loaded into the system receptacle <b>14710</b>. However, it is understood that a cartridge that includes the biosensor <b>14712</b> may be inserted into the system receptacle <b>14710</b> and in some states the cartridge can be removed temporarily or permanently. As described above, the cartridge may include, among other things, fluidic control and fluidic storage components.</p><p id="p-1134" num="1128">In particular implementations, the sequencing system <b>14700</b>A is configured to perform a large number of parallel reactions within the biosensor <b>14712</b>. The biosensor <b>14712</b> includes one or more reaction sites where desired reactions can occur. The reaction sites may be, for example, immobilized to a solid surface of the biosensor or immobilized to beads (or other movable substrates) that are located within corresponding reaction chambers of the biosensor. The reaction sites can include, for example, clusters of clonally amplified nucleic acids. The biosensor <b>14712</b> may include a solid-state imaging device (e.g., CCD or CMOS imager) and a flow cell mounted thereto. The flow cell may include one or more flow channels that receive a solution from the sequencing system <b>14700</b>A and direct the solution toward the reaction sites. Optionally, the biosensor <b>14712</b> can be configured to engage a thermal element for transferring thermal energy into or out of the flow channel.</p><p id="p-1135" num="1129">The sequencing system <b>14700</b>A may include various components, assemblies, and systems (or sub-systems) that interact with each other to perform a predetermined method or assay protocol for biological or chemical analysis. For example, the sequencing system <b>14700</b>A includes a system controller <b>14706</b> that may communicate with the various components, assemblies, and sub-systems of the sequencing system <b>14700</b>A and also the biosensor <b>14712</b>. For example, in addition to the system receptacle <b>14710</b>, the sequencing system <b>14700</b>A may also include a fluidic control system <b>14708</b> to control the flow of fluid throughout a fluid network of the sequencing system <b>14700</b>A and the biosensor <b>14712</b>; a fluid storage system <b>14714</b> that is configured to hold all fluids (e.g., gas or liquids) that may be used by the bioassay system; a temperature control system <b>14704</b> that may regulate the temperature of the fluid in the fluid network, the fluid storage system <b>14714</b>, and/or the biosensor <b>14712</b>; and an illumination system <b>14716</b> that is configured to illuminate the biosensor <b>14712</b>. As described above, if a cartridge having the biosensor <b>14712</b> is loaded into the system receptacle <b>14710</b>, the cartridge may also include fluidic control and fluidic storage components.</p><p id="p-1136" num="1130">Also shown, the sequencing system <b>14700</b>A may include a user interface <b>14718</b> that interacts with the user. For example, the user interface <b>14718</b> may include a display <b>14720</b> to display or request information from a user and a user input device <b>14722</b> to receive user inputs. In some implementations, the display <b>14720</b> and the user input device <b>14722</b> are the same device. For example, the user interface <b>14718</b> may include a touch-sensitive display configured to detect the presence of an individual's touch and also identify a location of the touch on the display. However, other user input devices <b>14722</b> may be used, such as a mouse, touchpad, keyboard, keypad, handheld scanner, voice-recognition system, motion-recognition system, and the like. As will be discussed in greater detail below, the sequencing system <b>14700</b>A may communicate with various components, including the biosensor <b>14712</b> (e.g., in the form of a cartridge), to perform the desired reactions. The sequencing system <b>14700</b>A may also be configured to analyze data obtained from the biosensor to provide a user with desired information.</p><p id="p-1137" num="1131">The system controller <b>14706</b> may include any processor-based or microprocessor-based system, including systems using microcontrollers, reduced instruction set computers (RISC), application specific integrated circuits (ASICs), field programmable gate array (FPGAs), coarse-grained reconfigurable architectures (CGRAs), logic circuits, and any other circuit or processor capable of executing functions described herein. The above examples are exemplary only, and are thus not intended to limit in any way the definition and/or meaning of the term system controller. In the exemplary implementation, the system controller <b>14706</b> executes a set of instructions that are stored in one or more storage elements, memories, or modules in order to at least one of obtain and analyze detection data. Detection data can include a plurality of sequences of pixel signals, such that a sequence of pixel signals from each of the millions of sensors (or pixels) can be detected over many base calling cycles. Storage elements may be in the form of information sources or physical memory elements within the sequencing system <b>14700</b>A.</p><p id="p-1138" num="1132">The set of instructions may include various commands that instruct the sequencing system <b>14700</b>A or biosensor <b>14712</b> to perform specific operations such as the methods and processes of the various implementations described herein. The set of instructions may be in the form of a software program, which may form part of a tangible, non-transitory computer readable medium or media. As used herein, the terms &#x201c;software&#x201d; and &#x201c;firmware&#x201d; are interchangeable, and include any computer program stored in memory for execution by a computer, including RAM memory, ROM memory, EPROM memory, EEPROM memory, and non-volatile RAM (NVRAM) memory. The above memory types are exemplary only, and are thus not limiting as to the types of memory usable for storage of a computer program.</p><p id="p-1139" num="1133">The software may be in various forms such as system software or application software. Further, the software may be in the form of a collection of separate programs, or a program module within a larger program or a portion of a program module. The software also may include modular programming in the form of object-oriented programming. After obtaining the detection data, the detection data may be automatically processed by the sequencing system <b>14700</b>A, processed in response to user inputs, or processed in response to a request made by another processing machine (e.g., a remote request through a communication link). In the illustrated implementation, the system controller <b>14706</b> includes an analysis module <b>14744</b>. In other implementations, system controller <b>14706</b> does not include the analysis module <b>14744</b> and instead has access to the analysis module <b>14744</b> (e.g., the analysis module <b>14744</b> may be separately hosted on cloud).</p><p id="p-1140" num="1134">The system controller <b>14706</b> may be connected to the biosensor <b>14712</b> and the other components of the sequencing system <b>14700</b>A via communication links. The system controller <b>14706</b> may also be communicatively connected to off-site systems or servers. The communication links may be hardwired, corded, or wireless. The system controller <b>14706</b> may receive user inputs or commands, from the user interface <b>14718</b> and the user input device <b>14722</b>.</p><p id="p-1141" num="1135">The fluidic control system <b>14708</b> includes a fluid network and is configured to direct and regulate the flow of one or more fluids through the fluid network. The fluid network may be in fluid communication with the biosensor <b>14712</b> and the fluid storage system <b>14714</b>. For example, select fluids may be drawn from the fluid storage system <b>14714</b> and directed to the biosensor <b>14712</b> in a controlled manner, or the fluids may be drawn from the biosensor <b>14712</b> and directed toward, for example, a waste reservoir in the fluid storage system <b>14714</b>. Although not shown, the fluidic control system <b>14708</b> may include flow sensors that detect a flow rate or pressure of the fluids within the fluid network. The sensors may communicate with the system controller <b>14706</b>.</p><p id="p-1142" num="1136">The temperature control system <b>14704</b> is configured to regulate the temperature of fluids at different regions of the fluid network, the fluid storage system <b>14714</b>, and/or the biosensor <b>14712</b>. For example, the temperature control system <b>14704</b> may include a thermocycler that interfaces with the biosensor <b>14712</b> and controls the temperature of the fluid that flows along the reaction sites in the biosensor <b>14712</b>. The temperature control system <b>14704</b> may also regulate the temperature of solid elements or components of the sequencing system <b>14700</b>A or the biosensor <b>14712</b>. Although not shown, the temperature control system <b>14704</b> may include sensors to detect the temperature of the fluid or other components. The sensors may communicate with the system controller <b>14706</b>.</p><p id="p-1143" num="1137">The fluid storage system <b>14714</b> is in fluid communication with the biosensor <b>14712</b> and may store various reaction components or reactants that are used to conduct the desired reactions therein. The fluid storage system <b>14714</b> may also store fluids for washing or cleaning the fluid network and biosensor <b>14712</b> and for diluting the reactants. For example, the fluid storage system <b>14714</b> may include various reservoirs to store samples, reagents, enzymes, other biomolecules, buffer solutions, aqueous, and non-polar solutions, and the like. Furthermore, the fluid storage system <b>14714</b> may also include waste reservoirs for receiving waste products from the biosensor <b>14712</b>. In implementations that include a cartridge, the cartridge may include one or more of a fluid storage system, fluidic control system or temperature control system. Accordingly, one or more of the components set forth herein as relating to those systems can be contained within a cartridge housing. For example, a cartridge can have various reservoirs to store samples, reagents, enzymes, other biomolecules, buffer solutions, aqueous, and non-polar solutions, waste, and the like. As such, one or more of a fluid storage system, fluidic control system or temperature control system can be removably engaged with a bioassay system via a cartridge or other biosensor.</p><p id="p-1144" num="1138">The illumination system <b>14716</b> may include a light source (e.g., one or more LEDs) and a plurality of optical components to illuminate the biosensor. Examples of light sources may include lasers, arc lamps, LEDs, or laser diodes. The optical components may be, for example, reflectors, dichroics, beam splitters, collimators, lenses, filters, wedges, prisms, mirrors, detectors, and the like. In implementations that use an illumination system, the illumination system <b>14716</b> may be configured to direct an excitation light to reaction sites. As one example, fluorophores may be excited by green wavelengths of light, as such the wavelength of the excitation light may be approximately 532 nm. In one implementation, the illumination system <b>14716</b> is configured to produce illumination that is parallel to a surface normal of a surface of the biosensor <b>14712</b>. In another implementation, the illumination system <b>14716</b> is configured to produce illumination that is off-angle relative to the surface normal of the surface of the biosensor <b>14712</b>. In yet another implementation, the illumination system <b>14716</b> is configured to produce illumination that has plural angles, including some parallel illumination and some off-angle illumination.</p><p id="p-1145" num="1139">The system receptacle or interface <b>14710</b> is configured to engage the biosensor <b>14712</b> in at least one of a mechanical, electrical, and fluidic manner. The system receptacle <b>14710</b> may hold the biosensor <b>14712</b> in a desired orientation to facilitate the flow of fluid through the biosensor <b>14712</b>. The system receptacle <b>14710</b> may also include electrical contacts that are configured to engage the biosensor <b>14712</b> so that the sequencing system <b>14700</b>A may communicate with the biosensor <b>14712</b> and/or provide power to the biosensor <b>14712</b>. Furthermore, the system receptacle <b>14710</b> may include fluidic ports (e.g., nozzles) that are configured to engage the biosensor <b>14712</b>. In some implementations, the biosensor <b>14712</b> is removably coupled to the system receptacle <b>14710</b> in a mechanical manner, in an electrical manner, and also in a fluidic manner.</p><p id="p-1146" num="1140">In addition, the sequencing system <b>14700</b>A may communicate remotely with other systems or networks or with other bioassay systems <b>14700</b>A. Detection data obtained by the bioassay system(s) <b>14700</b>A may be stored in a remote database.</p><p id="p-1147" num="1141"><figref idref="DRAWINGS">FIG. <b>147</b>B</figref> is a block diagram of a system controller <b>14706</b> that can be used in the system of <figref idref="DRAWINGS">FIG. <b>147</b>A</figref>. In one implementation, the system controller <b>14706</b> includes one or more processors or modules that can communicate with one another. Each of the processors or modules may include an algorithm (e.g., instructions stored on a tangible and/or non-transitory computer readable storage medium) or sub-algorithms to perform particular processes. The system controller <b>14706</b> is illustrated conceptually as a collection of modules, but may be implemented utilizing any combination of dedicated hardware boards, DSPs, processors, etc. Alternatively, the system controller <b>14706</b> may be implemented utilizing an off-the-shelf PC with a single processor or multiple processors, with the functional operations distributed between the processors. As a further option, the modules described below may be implemented utilizing a hybrid configuration in which certain modular functions are performed utilizing dedicated hardware, while the remaining modular functions are performed utilizing an off-the-shelf PC and the like. The modules also may be implemented as software modules within a processing unit.</p><p id="p-1148" num="1142">During operation, a communication port <b>14750</b> may transmit information (e.g., commands) to or receive information (e.g., data) from the biosensor <b>14712</b> (<figref idref="DRAWINGS">FIG. <b>147</b>A</figref>) and/or the sub-systems <b>14708</b>, <b>14714</b>, <b>14704</b> (<figref idref="DRAWINGS">FIG. <b>147</b>A</figref>). In implementations, the communication port <b>14750</b> may output a plurality of sequences of pixel signals. A communication link <b>14734</b> may receive user input from the user interface <b>14718</b> (<figref idref="DRAWINGS">FIG. <b>147</b>A</figref>) and transmit data or information to the user interface <b>14718</b>. Data from the biosensor <b>14712</b> or sub-systems <b>14708</b>, <b>14714</b>, <b>14704</b> may be processed by the system controller <b>14706</b> in real-time during a bioassay session. Additionally or alternatively, data may be stored temporarily in a system memory during a bioassay session and processed in slower than real-time or off-line operation.</p><p id="p-1149" num="1143">As shown in <figref idref="DRAWINGS">FIG. <b>147</b>B</figref>, the system controller <b>14706</b> may include a plurality of modules <b>14726</b>-<b>14748</b> that communicate with a main control module <b>14724</b>, along with a central processing unit (CPU) <b>14752</b>. The main control module <b>14724</b> may communicate with the user interface <b>14718</b> (<figref idref="DRAWINGS">FIG. <b>147</b>A</figref>). Although the modules <b>14726</b>-<b>14748</b> are shown as communicating directly with the main control module <b>14724</b>, the modules <b>14726</b>-<b>14748</b> may also communicate directly with each other, the user interface <b>14718</b>, and the biosensor <b>14712</b>. Also, the modules <b>14726</b>-<b>14748</b> may communicate with the main control module <b>14724</b> through the other modules.</p><p id="p-1150" num="1144">The plurality of modules <b>14726</b>-<b>14748</b> include system modules <b>14728</b>-<b>14732</b>, <b>14726</b> that communicate with the sub-systems <b>14708</b>, <b>14714</b>, <b>14704</b>, and <b>14716</b>, respectively. The fluidic control module <b>14728</b> may communicate with the fluidic control system <b>14708</b> to control the valves and flow sensors of the fluid network for controlling the flow of one or more fluids through the fluid network. The fluid storage module <b>14730</b> may notify the user when fluids are low or when the waste reservoir is at or near capacity. The fluid storage module <b>14730</b> may also communicate with the temperature control module <b>14732</b> so that the fluids may be stored at a desired temperature. The illumination module <b>14726</b> may communicate with the illumination system <b>14716</b> to illuminate the reaction sites at designated times during a protocol, such as after the desired reactions (e.g., binding events) have occurred. In some implementations, the illumination module <b>14726</b> may communicate with the illumination system <b>14716</b> to illuminate the reaction sites at designated angles.</p><p id="p-1151" num="1145">The plurality of modules <b>14726</b>-<b>14748</b> may also include a device module <b>14736</b> that communicates with the biosensor <b>14712</b> and an identification module <b>14738</b> that determines identification information relating to the biosensor <b>14712</b>. The device module <b>14736</b> may, for example, communicate with the system receptacle <b>14710</b> to confirm that the biosensor has established an electrical and fluidic connection with the sequencing system <b>14700</b>A. The identification module <b>14738</b> may receive signals that identify the biosensor <b>14712</b>. The identification module <b>14738</b> may use the identity of the biosensor <b>14712</b> to provide other information to the user. For example, the identification module <b>14738</b> may determine and then display a lot number, a date of manufacture, or a protocol that is recommended to be run with the biosensor <b>14712</b>.</p><p id="p-1152" num="1146">The plurality of modules <b>14726</b>-<b>14748</b> also includes an analysis module <b>14744</b> (also called signal processing module or signal processor) that receives and analyzes the signal data (e.g., image data) from the biosensor <b>14712</b>. Analysis module <b>14744</b> includes memory (e.g., RAM or Flash) to store detection/image data. Detection data can include a plurality of sequences of pixel signals, such that a sequence of pixel signals from each of the millions of sensors (or pixels) can be detected over many base calling cycles. The signal data may be stored for subsequent analysis or may be transmitted to the user interface <b>14718</b> to display desired information to the user. In some implementations, the signal data may be processed by the solid-state imager (e.g., CMOS image sensor) before the analysis module <b>14744</b> receives the signal data.</p><p id="p-1153" num="1147">The analysis module <b>14744</b> is configured to obtain image data from the light detectors at each of a plurality of sequencing cycles. The image data is derived from the emission signals detected by the light detectors and process the image data for each of the plurality of sequencing cycles through the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> and produce a base call for at least some of the analytes at each of the plurality of sequencing cycle. The light detectors can be part of one or more over-head cameras (e.g., Illumina's GAIIx's CCD camera taking images of the clusters on the biosensor <b>14712</b> from the top), or can be part of the biosensor <b>14712</b> itself (e.g., Illumina's iSeq's CMOS image sensors underlying the clusters on the biosensor <b>14712</b> and taking images of the clusters from the bottom).</p><p id="p-1154" num="1148">The output of the light detectors is the sequencing images, each depicting intensity emissions of the clusters and their surrounding background. The sequencing images depict intensity emissions generated as a result of nucleotide incorporation in the sequences during the sequencing. The intensity emissions are from associated analytes and their surrounding background. The sequencing images are stored in memory <b>14748</b>.</p><p id="p-1155" num="1149">Protocol modules <b>14740</b> and <b>14742</b> communicate with the main control module <b>14724</b> to control the operation of the sub-systems <b>14708</b>, <b>14714</b>, and <b>14704</b> when conducting predetermined assay protocols. The protocol modules <b>14740</b> and <b>14742</b> may include sets of instructions for instructing the sequencing system <b>14700</b>A to perform specific operations pursuant to predetermined protocols. As shown, the protocol module may be a sequencing-by-synthesis (SBS) module <b>14740</b> that is configured to issue various commands for performing sequencing-by-synthesis processes. In SBS, extension of a nucleic acid primer along a nucleic acid template is monitored to determine the sequence of nucleotides in the template. The underlying chemical process can be polymerization (e.g., as catalyzed by a polymerase enzyme) or ligation (e.g., catalyzed by a ligase enzyme). In a particular polymerase-based SBS implementation, fluorescently labeled nucleotides are added to a primer (thereby extending the primer) in a template dependent fashion such that detection of the order and type of nucleotides added to the primer can be used to determine the sequence of the template. For example, to initiate a first SBS cycle, commands can be given to deliver one or more labeled nucleotides, DNA polymerase, etc., into/through a flow cell that houses an array of nucleic acid templates. The nucleic acid templates may be located at corresponding reaction sites. Those reaction sites where primer extension causes a labeled nucleotide to be incorporated can be detected through an imaging event. During an imaging event, the illumination system <b>14716</b> may provide an excitation light to the reaction sites. Optionally, the nucleotides can further include a reversible termination property that terminates further primer extension once a nucleotide has been added to a primer. For example, a nucleotide analog having a reversible terminator moiety can be added to a primer such that subsequent extension cannot occur until a deblocking agent is delivered to remove the moiety. Thus, for implementations that use reversible termination a command can be given to deliver a deblocking reagent to the flow cell (before or after detection occurs). One or more commands can be given to effect wash(es) between the various delivery steps. The cycle can then be repeated n times to extend the primer by n nucleotides, thereby detecting a sequence of length n. Exemplary sequencing techniques are described, for example, in Bentley et al., Nature 456:53-59 (200147); WO 04/01147497; U.S. Pat. No. 7,057,026; WO 91/0667147; WO 07/123744; U.S. Pat. Nos. 7,329,492; 7,211,414; 7,315,019; 7,405,21471, and US 200147/0147014701472, each of which is incorporated herein by reference.</p><p id="p-1156" num="1150">For the nucleotide deliver), step of an SBS cycle, either a single type of nucleotide can be delivered at a time, or multiple different nucleotide types (e.g., A, C, T and G together) can be delivered. For a nucleotide deliver), configuration where only a single type of nucleotide is present at a time, the different nucleotides need not have distinct labels since they can be distinguished based on temporal separation inherent in the individualized delivery. Accordingly, a sequencing method or apparatus can use single color detection. For example, an excitation source need only provide excitation at a single wavelength or in a single range of wavelengths. For a nucleotide deliver), configuration where deliver), results in multiple different nucleotides being present in the flow cell at one time, sites that incorporate different nucleotide types can be distinguished based on different fluorescent labels that are attached to respective nucleotide types in the mixture. For example, four different nucleotides can be used, each having one of four different fluorophores. In one implementation, the four different fluorophores can be distinguished using excitation in four different regions of the spectrum. For example, four different excitation radiation sources can be used. Alternatively, fewer than four different excitation sources can be used, but optical filtration of the excitation radiation from a single source can be used to produce different ranges of excitation radiation at the flow cell.</p><p id="p-1157" num="1151">In some implementations, fewer than four different colors can be detected in a mixture having four different nucleotides. For example, pairs of nucleotides can be detected at the same wavelength, but distinguished based on a difference in intensity for one member of the pair compared to the other, or based on a change to one member of the pair (e.g., via chemical modification, photochemical modification or physical modification) that causes apparent signal to appear or disappear compared to the signal detected for the other member of the pair. Exemplary apparatus and methods for distinguishing four different nucleotides using detection of fewer than four colors are described for example in US Pat. App. Ser. Nos. 61/53147,294 and 61/619,1477147, which are incorporated herein by reference in their entireties. U.S. application Ser. No. 13/624,200, which was filed on Sep. 21, 2012, is also incorporated by reference in its entirety.</p><p id="p-1158" num="1152">The plurality of protocol modules may also include a sample-preparation (or generation) module <b>14742</b> that is configured to issue commands to the fluidic control system <b>14708</b> and the temperature control system <b>14704</b> for amplifying a product within the biosensor <b>14712</b>. For example, the biosensor <b>14712</b> may be engaged to the sequencing system <b>14700</b>A. The amplification module <b>14742</b> may issue instructions to the fluidic control system <b>14708</b> to deliver necessary amplification components to reaction chambers within the biosensor <b>14712</b>. In other implementations, the reaction sites may already contain some components for amplification, such as the template DNA and/or primers. After delivering the amplification components to the reaction chambers, the amplification module <b>14742</b> may instruct the temperature control system <b>14704</b> to cycle through different temperature stages according to known amplification protocols. In some implementations, the amplification and/or nucleotide incorporation is performed isothermally.</p><p id="p-1159" num="1153">The SBS module <b>14740</b> may issue commands to perform bridge PCR where clusters of clonal amplicons are formed on localized areas within a channel of a flow cell. After generating the amplicons through bridge PCR, the amplicons may be &#x201c;linearized&#x201d; to make single stranded template DNA, or sstDNA, and a sequencing primer may be hybridized to a universal sequence that flanks a region of interest. For example, a reversible terminator-based sequencing by synthesis method can be used as set forth above or as follows.</p><p id="p-1160" num="1154">Each base calling or sequencing cycle can extend an sstDNA by a single base which can be accomplished for example by using a modified DNA polymerase and a mixture of four types of nucleotides. The different types of nucleotides can have unique fluorescent labels, and each nucleotide can further have a reversible terminator that allows only a single-base incorporation to occur in each cycle. After a single base is added to the sstDNA, excitation light may be incident upon the reaction sites and fluorescent emissions may be detected. After detection, the fluorescent label and the terminator may be chemically cleaved from the sstDNA. Another similar base calling or sequencing cycle may follow. In such a sequencing protocol, the SBS module <b>14740</b> may instruct the fluidic control system <b>14708</b> to direct a flow of reagent and enzyme solutions through the biosensor <b>14712</b>. Exemplary reversible terminator-based SBS methods which can be utilized with the apparatus and methods set forth herein are described in US Patent Application Publication No. 2007/0166705 A1, US Patent Application Publication No. 2006/01147147901 A1, U.S. Pat. No. 7,057,026, US Patent Application Publication No. 2006/0240439 A1, US Patent Application Publication No. 2006/0214714714709 A1, PCT Publication No. WO 05/014914714, US Patent Application Publication No. 2005/014700900 A1, PCT Publication No. WO 06/0147B199 and PCT Publication No. WO 07/01470251, each of which is incorporated herein by reference in its entirety. Exemplary reagents for reversible terminator-based SBS are described in U.S. Pat. Nos. 7,541,444; 7,057,026; 7,414,14716; U.S. Pat. Nos. 7,427,673; 7,566,537; 7,592,435 and WO 07/141473536147, each of which is incorporated herein by reference in its entirety.</p><p id="p-1161" num="1155">In some implementations, the amplification and SBS modules may operate in a single assay protocol where, for example, template nucleic acid is amplified and subsequently sequenced within the same cartridge.</p><p id="p-1162" num="1156">The sequencing system <b>14700</b>A may also allow the user to reconfigure an assay protocol. For example, the sequencing system <b>14700</b>A may offer options to the user through the user interface <b>14718</b> for modifying the determined protocol. For example, if it is determined that the biosensor <b>14712</b> is to be used for amplification, the sequencing system <b>14700</b>A may request a temperature for the annealing cycle. Furthermore, the sequencing system <b>14700</b>A may issue warnings to a user if a user has provided user inputs that are generally not acceptable for the selected assay protocol.</p><p id="p-1163" num="1157">In implementations, the biosensor <b>14712</b> includes millions of sensors (or pixels), each of which generates a plurality of sequences of pixel signals over successive base calling cycles. The analysis module <b>14744</b> detects the plurality of sequences of pixel signals and attributes them to corresponding sensors (or pixels) in accordance to the row-wise and/or column-wise location of the sensors on an array of sensors.</p><p id="p-1164" num="1158"><figref idref="DRAWINGS">FIG. <b>147</b>C</figref> is a simplified block diagram of a system for analysis of sensor data from the sequencing system <b>14700</b>A, such as base call sensor outputs. In the example of <figref idref="DRAWINGS">FIG. <b>147</b>C</figref>, the system includes the configurable processor <b>14746</b>. The configurable processor <b>14746</b> can execute a base caller (e.g., the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>) in coordination with a runtime program executed by the central processing unit (CPU) <b>14752</b> (i.e., a host processor). The sequencing system <b>14700</b>A comprises the biosensor <b>14712</b> and flow cells. The flow cells can comprise one or more tiles in which clusters of genetic material are exposed to a sequence of analyte flows used to cause reactions in the clusters to identify the bases in the genetic material. The sensors sense the reactions for each cycle of the sequence in each tile of the flow cell to provide tile data. Genetic sequencing is a data intensive operation, which translates base call sensor data into sequences of base calls for each cluster of genetic material sensed in during a base call operation.</p><p id="p-1165" num="1159">The system in this example includes the CPU <b>14752</b>, which executes a runtime program to coordinate the base call operations, memory <b>14748</b>B to store sequences of arrays of tile data, base call reads produced by the base calling operation, and other information used in the base call operations. Also, in this illustration the system includes memory <b>14748</b>A to store a configuration file (or files), such as FPGA bit files, and model parameters for the neural networks used to configure and reconfigure the configurable processor <b>14746</b>, and execute the neural networks. The sequencing system <b>14700</b>A can include a program for configuring a configurable processor and in some embodiments a reconfigurable processor to execute the neural networks.</p><p id="p-1166" num="1160">The sequencing system <b>14700</b>A is coupled by a bus <b>14789</b> to the configurable processor <b>14746</b>. The bus <b>14789</b> can be implemented using a high throughput technology, such as in one example bus technology compatible with the PCIe standards (Peripheral Component Interconnect Express) currently maintained and developed by the PCI-SIG (PCI Special Interest Group). Also in this example, a memory <b>14748</b>A is coupled to the configurable processor <b>14746</b> by bus <b>14793</b>. The memory <b>14748</b>A can be on-board memory, disposed on a circuit board with the configurable processor <b>14746</b>. The memory <b>14748</b>A is used for high speed access by the configurable processor <b>14746</b> of working data used in the base call operation. The bus <b>14793</b> can also be implemented using a high throughput technology, such as bus technology compatible with the PCIe standards.</p><p id="p-1167" num="1161">Configurable processors, including field programmable gate arrays FPGAs, coarse grained reconfigurable arrays CGRAs, and other configurable and reconfigurable devices, can be configured to implement a variety of functions more efficiently or faster than might be achieved using a general purpose processor executing a computer program. Configuration of configurable processors involves compiling a functional description to produce a configuration file, referred to sometimes as a bitstream or bit file, and distributing the configuration file to the configurable elements on the processor. The configuration file defines the logic functions to be executed by the configurable processor, by configuring the circuit to set data flow patterns, use of distributed memory and other on-chip memory resources, lookup table contents, operations of configurable logic blocks and configurable execution units like multiply-and-accumulate units, configurable interconnects and other elements of the configurable array. A configurable processor is reconfigurable if the configuration file may be changed in the field, by changing the loaded configuration file. For example, the configuration file may be stored in volatile SRAM elements, in non-volatile read-write memory elements, and in combinations of the same, distributed among the array of configurable elements on the configurable or reconfigurable processor. A variety of commercially available configurable processors are suitable for use in a base calling operation as described herein. Examples include Google's Tensor Processing Unit (TPU)&#x2122;, rackmount solutions like GX4 Rackmount Series&#x2122;, GX9 Rackmount Series&#x2122;, NVIDIA DGX-1&#x2122;, Microsoft' Stratix V FPGA&#x2122;, Graphcore's Intelligent Processor Unit (IPU)&#x2122;, Qualcomm's Zeroth Platform&#x2122; with Snapdragon Processors&#x2122;, NVIDIA's Volta&#x2122;, NVIDIA's DRIVE PX&#x2122;, NVIDIA's JETSON TX1/TX2 MODULE&#x2122;, Intel's Nirvana&#x2122;, Movidius VPU&#x2122;, Fujitsu DPI&#x2122;, ARM's DynamicIQ&#x2122;, IBM TrueNorth&#x2122;, Lambda GPU Server with Testa V100s&#x2122;, Xilinx Alveo&#x2122; U200, Xilinx Alveo&#x2122; U250, Xilinx Alveo&#x2122; U280, Intel/Altera Stratix&#x2122; GX2800, Intel/Altera Stratix&#x2122; GX2800, and Intel Stratix&#x2122; GX10M. In some examples, a host CPU can be implemented on the same integrated circuit as the configurable processor.</p><p id="p-1168" num="1162">Embodiments described herein implement the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> using the configurable processor <b>14746</b>. The configuration file for the configurable processor <b>14746</b> can be implemented by specifying the logic functions to be executed using a high level description language HDL or a register transfer level RTL language specification. The specification can be compiled using the resources designed for the selected configurable processor to generate the configuration file. The same or similar specification can be compiled for the purposes of generating a design for an application-specific integrated circuit which may not be a configurable processor.</p><p id="p-1169" num="1163">Alternatives for the configurable processor configurable processor <b>14746</b>, in all embodiments described herein, therefore include a configured processor comprising an application specific ASIC or special purpose integrated circuit or set of integrated circuits, or a system-on-a-chip SOC device, or a graphics processing unit (GPU) processor or a coarse-grained reconfigurable architecture (CGRA) processor, configured to execute a neural network based base call operation as described herein.</p><p id="p-1170" num="1164">In general, configurable processors and configured processors described herein, as configured to execute runs of a neural network, are referred to herein as neural network processors.</p><p id="p-1171" num="1165">The configurable processor <b>14746</b> is configured in this example by a configuration file loaded using a program executed by the CPU <b>14752</b>, or by other sources, which configures the array of configurable elements <b>14791</b> (e.g., configuration logic blocks (CLB) such as look up tables (LUTs), flip-flops, compute processing units (PMUs), and compute memory units (CMUs), configurable I/O blocks, programmable interconnects), on the configurable processor to execute the base call function. In this example, the configuration includes data flow logic <b>14797</b> which is coupled to the buses <b>14789</b> and <b>14793</b> and executes functions for distributing data and control parameters among the elements used in the base call operation.</p><p id="p-1172" num="1166">Also, the configurable processor <b>14746</b> is configured with base call execution logic <b>14797</b> to execute the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>. The logic <b>14797</b> comprises multi-cycle execution clusters (e.g., <b>14779</b>) which, in this example, includes execution cluster 1 through execution cluster X. The number of multi-cycle execution clusters can be selected according to a trade-off involving the desired throughput of the operation, and the available resources on the configurable processor <b>14746</b>.</p><p id="p-1173" num="1167">The multi-cycle execution clusters are coupled to the data flow logic <b>14797</b> by data flow paths <b>14799</b> implemented using configurable interconnect and memory resources on the configurable processor <b>14746</b>. Also, the multi-cycle execution clusters are coupled to the data flow logic <b>14797</b> by control paths <b>14795</b> implemented using configurable interconnect and memory resources for example on the configurable processor <b>14746</b>, which provide control signals indicating available execution clusters, readiness to provide input units for execution of a run of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> to the available execution clusters, readiness to provide trained parameters for the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>, readiness to provide output patches of base call classification data, and other control data used for execution of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>.</p><p id="p-1174" num="1168">The configurable processor <b>14746</b> is configured to execute runs of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> using trained parameters to produce classification data for the sensing cycles of the base calling operation. A run of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> is executed to produce classification data for a subject sensing cycle of the base calling operation. A run of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> operates on a sequence including a number N of arrays of tile data from respective sensing cycles of N sensing cycles, where the N sensing cycles provide sensor data for different base call operations for one base position per operation in time sequence in the examples described herein. Optionally, some of the N sensing cycles can be out of sequence if needed according to a particular neural network model being executed. The number N can be any number greater than one. In some examples described herein, sensing cycles of the N sensing cycles represent a set of sensing cycles for at least one sensing cycle preceding the subject sensing cycle and at least one sensing cycle following the subject cycle in time sequence. Examples are described herein in which the number N is an integer equal to or greater than five.</p><p id="p-1175" num="1169">The data flow logic <b>14797</b> is configured to move tile data and at least some trained parameters of the model parameters from the memory <b>14748</b>A to the configurable processor <b>14746</b> for runs of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>, using input units for a given run including tile data for spatially aligned patches of the N arrays. The input units can be moved by direct memory access operations in one DMA operation, or in smaller units moved during available time slots in coordination with the execution of the neural network deployed.</p><p id="p-1176" num="1170">Tile data for a sensing cycle as described herein can comprise an array of sensor data having one or more features. For example, the sensor data can comprise two images which are analyzed to identify one of four bases at a base position in a genetic sequence of DNA, RNA, or other genetic material. The tile data can also include metadata about the images and the sensors. For example, in embodiments of the base calling operation, the tile data can comprise information about alignment of the images with the clusters such as distance from center information indicating the distance of each pixel in the array of sensor data from the center of a cluster of genetic material on the tile.</p><p id="p-1177" num="1171">During execution of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b> as described below, tile data can also include data produced during execution of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>, referred to as intermediate data, which can be reused rather than recomputed during a run of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>. For example, during execution of the neural network-based quality scorer <b>6102</b> and/or the neural network-based base caller <b>218</b>, the data flow logic <b>14797</b> can write intermediate data to the memory <b>14748</b>A in place of the sensor data for a given patch of an array of tile data. Embodiments like this are described in more detail below.</p><p id="p-1178" num="1172">As illustrated, a system is described for analysis of base call sensor output, comprising memory (e.g., <b>14748</b>A) accessible by the runtime program storing tile data including sensor data for a tile from sensing cycles of a base calling operation. Also, the system includes a neural network processor, such as configurable processor <b>14746</b> having access to the memory. The neural network processor is configured to execute runs of a neural network using trained parameters to produce classification data for sensing cycles. As described herein, a run of the neural network is operating on a sequence of N arrays of tile data from respective sensing cycles of N sensing cycles, including a subject cycle, to produce the classification data for the subject cycle. The data flow logic <b>14797</b> is provided to move tile data and the trained parameters from the memory to the neural network processor for runs of the neural network using input units including data for spatially aligned patches of the N arrays from respective sensing cycles of N sensing cycles.</p><p id="p-1179" num="1173">Also, a system is described in which the neural network processor has access to the memory, and includes a plurality of execution clusters, the execution clusters in the plurality of execution clusters configured to execute a neural network. The data flow logic <b>14797</b> has access to the memory and to execution clusters in the plurality of execution clusters, to provide input units of tile data to available execution clusters in the plurality of execution clusters, the input units including a number N of spatially aligned patches of arrays of tile data from respective sensing cycles, including a subject sensing cycle, and to cause the execution clusters to apply the N spatially aligned patches to the neural network to produce output patches of classification data for the spatially aligned patch of the subject sensing cycle, where N is greater than 1.</p><p id="p-1180" num="1174"><figref idref="DRAWINGS">FIG. <b>148</b>A</figref> is a simplified diagram showing aspects of the base calling operation, including functions of a runtime program executed by a host processor. In this diagram, the output of image sensors from a flow cell are provided on lines <b>14800</b> to image processing threads <b>14801</b>, which can perform processes on images such as alignment and arrangement in an array of sensor data for the individual tiles and resampling of images, and can be used by processes which calculate a tile cluster mask for each tile in the flow cell, which identifies pixels in the array of sensor data that correspond to clusters of genetic material on the corresponding tile of the flow cell. The outputs of the image processing threads <b>14801</b> are provided on lines <b>14806</b> to a dispatch logic <b>14810</b> in the CPU which routes the arrays of tile data to a data cache <b>14804</b> (e.g., SSD storage) on a high-speed bus <b>14807</b>, or on high-speed bus <b>14805</b> to the neural network processor hardware <b>14820</b>, such as the configurable processor <b>14746</b> of <figref idref="DRAWINGS">FIG. <b>147</b>C</figref>, according to the state of the base calling operation. The processed and transformed images can be stored on the data cache <b>14804</b> for sensing cycles that were previously used. The hardware <b>14820</b> returns classification data output by the neural network to the dispatch logic <b>148148</b>, which passes the information to the data cache <b>14804</b>, or on lines <b>14811</b> to threads <b>14802</b> that perform base call and quality score computations using the classification data, and can arrange the data in standard formats for base call reads. The outputs of the threads <b>14802</b> that perform base calling and quality score computations are provided on lines <b>14812</b> to threads <b>14803</b> that aggregate the base call reads, perform other operations such as data compression, and write the resulting base call outputs to specified destinations for utilization by the customers.</p><p id="p-1181" num="1175">In some embodiments, the host can include threads (not shown) that perform final processing of the output of the hardware <b>14820</b> in support of the neural network. For example, the hardware <b>14820</b> can provide outputs of classification data from a final layer of the multi-cluster neural network. The host processor can execute an output activation function, such as a softmax function, over the classification data to configure the data for use by the base call and quality score threads <b>14802</b>. Also, the host processor can execute input operations (not shown), such as batch normalization of the tile data prior to input to the hardware <b>14820</b>.</p><p id="p-1182" num="1176"><figref idref="DRAWINGS">FIG. <b>148</b>B</figref> is a simplified diagram of a configuration of a configurable processor <b>14746</b> such as that of <figref idref="DRAWINGS">FIG. <b>147</b>C</figref>. In <figref idref="DRAWINGS">FIG. <b>148</b>B</figref>, the configurable processor <b>14746</b> comprises an FPGA with a plurality of high speed PCIe interfaces. The FPGA is configured with a wrapper <b>14890</b> which comprises the data flow logic <b>14797</b> described with reference to <figref idref="DRAWINGS">FIG. <b>147</b>C</figref>. The wrapper <b>14890</b> manages the interface and coordination with a runtime program in the CPU across the CPU communication link <b>14877</b> and manages communication with the on-board DRAM <b>14899</b> (e.g., memory <b>14748</b>A) via DRAM communication link <b>14897</b>. The data flow logic <b>14797</b> in the wrapper <b>14890</b> provides patch data retrieved by traversing the arrays of tile data on the on-board DRAM <b>14899</b> for the number N cycles to a cluster <b>14885</b>, and retrieves process data <b>14887</b> from the cluster <b>14885</b> for delivery back to the on-board DRAM <b>14899</b>. The wrapper <b>14890</b> also manages transfer of data between the on-board DRAM <b>14899</b> and host memory, for both the input arrays of tile data, and for the output patches of classification data. The wrapper transfers patch data on line <b>14883</b> to the allocated cluster <b>14885</b>. The wrapper provides trained parameters, such as weights and biases on line <b>14881</b> to the cluster <b>14885</b> retrieved from the on-board DRAM <b>14899</b>. The wrapper provides configuration and control data on line <b>14879</b> to the cluster <b>14885</b> provided from, or generated in response to, the runtime program on the host via the CPU communication link <b>14877</b>. The cluster can also provide status signals on line <b>14889</b> to the wrapper <b>14890</b>, which are used in cooperation with control signals from the host to manage traversal of the arrays of tile data to provide spatially aligned patch data, and to execute the multi-cycle neural network over the patch data using the resources of the cluster <b>14885</b>.</p><p id="p-1183" num="1177">As mentioned above, there can be multiple clusters on a single configurable processor managed by the wrapper <b>14890</b> configured for executing on corresponding ones of multiple patches of the tile data. Each cluster can be configured to provide classification data for base calls in a subject sensing cycle using the tile data of multiple sensing cycles described herein.</p><p id="p-1184" num="1178">In examples of the system, model data, including kernel data like filter weights and biases can be sent from the host CPU to the configurable processor, so that the model can be updated as a function of cycle number. A base calling operation can comprise, for a representative example, on the order of hundreds of sensing cycles. Base calling operation can include paired end reads in some embodiments. For example, the model trained parameters may be updated once every 20 cycles (or other number of cycles), or according to update patterns implemented for particular systems and neural network models. In some embodiments including paired end reads in which a sequence for a given string in a genetic cluster on a tile includes a first part extending from a first end down (or up) the string, and a second part extending from a second end up (or down) the string, the trained parameters can be updated on the transition from the first part to the second part.</p><p id="p-1185" num="1179">In some examples, image data for multiple cycles of sensing data for a tile can be sent from the CPU to the wrapper <b>14890</b>. The wrapper <b>14890</b> can optionally do some pre-processing and transformation of the sensing data and write the information to the on-board DRAM <b>14899</b>. The input tile data for each sensing cycle can include arrays of sensor data including on the order of 4000&#xd7;3000 pixels per sensing cycle per tile or more, with two features representing colors of two images of the tile, and one or two bytes per feature per pixel. For an embodiment in which the number N is three sensing cycles to be used in each run of the multi-cycle neural network, the array of tile data for each run of the multi-cycle neural network can consume on the order of hundreds of megabytes per tile. In some embodiments of the system, the tile data also includes an array of DFC data, stored once per tile, or other type of metadata about the sensor data and the tiles.</p><p id="p-1186" num="1180">In operation, when a multi-cycle cluster is available, the wrapper allocates a patch to the cluster. The wrapper fetches a next patch of tile data in the traversal of the tile and sends it to the allocated cluster along with appropriate control and configuration information. The cluster can be configured with enough memory on the configurable processor to hold a patch of data including patches from multiple cycles in some systems, that is being worked on in place, and a patch of data that is to be worked on when the current patch of processing is finished using a ping-pong buffer technique or raster scanning technique in various embodiments.</p><p id="p-1187" num="1181">When an allocated cluster completes its run of the neural network for the current patch and produces an output patch, it will signal the wrapper. The wrapper will read the output patch from the allocated cluster, or alternatively the allocated cluster will push the data out to the wrapper. Then the wrapper will assemble output patches for the processed tile in the DRAM <b>14899</b>. When the processing of the entire tile has been completed, and the output patches of data transferred to the DRAM, the wrapper sends the processed output array for the tile back to the host/CPU in a specified format. In some embodiments, the on-board DRAM <b>14899</b> is managed by memory management logic in the wrapper <b>14890</b>. The runtime program can control the sequencing operations to complete analysis of all the arrays of tile data for all the cycles in the run in a continuous flow to provide real time analysis.</p><p id="p-1188" num="1182">Computer System</p><p id="p-1189" num="1183"><figref idref="DRAWINGS">FIG. <b>149</b></figref> is a computer system <b>14900</b> that can be used by the sequencing system <b>800</b>A to implement the technology disclosed herein. Computer system <b>14900</b> includes at least one central processing unit (CPU) <b>14972</b> that communicates with a number of peripheral devices via bus subsystem <b>14955</b>. These peripheral devices can include a storage subsystem <b>14910</b> including, for example, memory devices and a file storage subsystem <b>14936</b>, user interface input devices <b>14938</b>, user interface output devices <b>14976</b>, and a network interface subsystem <b>14974</b>. The input and output devices allow user interaction with computer system <b>14900</b>. Network interface subsystem <b>14974</b> provides an interface to outside networks, including an interface to corresponding interface devices in other computer systems.</p><p id="p-1190" num="1184">In one implementation, the system controller <b>7806</b> is communicably linked to the storage subsystem <b>14910</b> and the user interface input devices <b>14938</b>.</p><p id="p-1191" num="1185">User interface input devices <b>14938</b> can include a keyboard; pointing devices such as a mouse, trackball, touchpad, or graphics tablet; a scanner; a touch screen incorporated into the display; audio input devices such as voice recognition systems and microphones; and other types of input devices. In general, use of the term &#x201c;input device&#x201d; is intended to include all possible types of devices and ways to input information into computer system <b>14900</b>.</p><p id="p-1192" num="1186">User interface output devices <b>14976</b> can include a display subsystem, a printer, a fax machine, or non-visual displays such as audio output devices. The display subsystem can include an LED display, a cathode ray tube (CRT), a flat-panel device such as a liquid crystal display (LCD), a projection device, or some other mechanism for creating a visible image. The display subsystem can also provide a non-visual display such as audio output devices. In general, use of the term &#x201c;output device&#x201d; is intended to include all possible types of devices and ways to output information from computer system <b>14900</b> to the user or to another machine or computer system.</p><p id="p-1193" num="1187">Storage subsystem <b>14910</b> stores programming and data constructs that provide the functionality of some or all of the modules and methods described herein. These software modules are generally executed by deep learning processors <b>14978</b>.</p><p id="p-1194" num="1188">Deep learning processors <b>14978</b> can be graphics processing units (GPUs), field-programmable gate arrays (FPGAs), application-specific integrated circuits (ASICs), and/or coarse-grained reconfigurable architectures (CGRAs). Deep learning processors <b>14978</b> can be hosted by a deep learning cloud platform such as Google Cloud Platform&#x2122;, Xilinx&#x2122;, and Cirrascale&#x2122;. Examples of deep learning processors <b>14978</b> include Google's Tensor Processing Unit (TPU)&#x2122;, rackmount solutions like GX4 Rackmount Series&#x2122;, GX149 Rackmount Series&#x2122;, NVIDIA DGX-1&#x2122;, Microsoft' Stratix V FPGA&#x2122;, Graphcore's Intelligent Processor Unit (IPU)&#x2122;, Qualcomm's Zeroth Platform&#x2122; with Snapdragon Processors&#x2122;, s Volta&#x2122;, s DRIVE PX&#x2122;, NVIDIA's JETSON TX1/TX2 MODULE&#x2122;, Intel's Nirvana&#x2122;, Movidius VPU&#x2122; Fujitsu DPI&#x2122;, ARM's DynamicIQ&#x2122;, IBM TrueNorth&#x2122;, Lambda GPU Server with Testa V100s&#x2122;, and others.</p><p id="p-1195" num="1189">Memory subsystem <b>14922</b> used in the storage subsystem <b>14910</b> can include a number of memories including a main random access memory (RAM) <b>14932</b> for storage of instructions and data during program execution and a read only memory (ROM) <b>14934</b> in which fixed instructions are stored. A file storage subsystem <b>14936</b> can provide persistent storage for program and data files, and can include a hard disk drive, a floppy disk drive along with associated removable media, a CD-ROM drive, an optical drive, or removable media cartridges. The modules implementing the functionality of certain implementations can be stored by file storage subsystem <b>14936</b> in the storage subsystem <b>14910</b>, or in other machines accessible by the processor.</p><p id="p-1196" num="1190">Bus subsystem <b>14955</b> provides a mechanism for letting the various components and subsystems of computer system <b>14900</b> communicate with each other as intended. Although bus subsystem <b>14955</b> is shown schematically as a single bus, alternative implementations of the bus subsystem can use multiple busses.</p><p id="p-1197" num="1191">Computer system <b>14900</b> itself can be of varying types including a personal computer, a portable computer, a workstation, a computer terminal, a network computer, a television, a mainframe, a server farm, a widely-distributed set of loosely networked computers, or any other data processing system or user device. Due to the ever-changing nature of computers and networks, the description of computer system <b>14900</b> depicted in <figref idref="DRAWINGS">FIG. <b>149</b></figref> is intended only as a specific example for purposes of illustrating the preferred implementations of the present invention. Many other configurations of computer system <b>14900</b> are possible having more or less components than the computer system depicted in <figref idref="DRAWINGS">FIG. <b>149</b></figref>.</p><heading id="h-0022" level="2">Sequencing Process</heading><p id="p-1198" num="1192">Implementations set forth herein may be applicable to analyzing nucleic acid sequences to identify sequence variations. Implementations may be used to analyze potential variants/alleles of a genetic position/locus and determine a genotype of the genetic locus or, in other words, provide a genotype call for the locus. By way of example, nucleic acid sequences may be analyzed in accordance with the methods and systems described in US Patent Application Publication No. 2016/0085910 and US Patent Application Publication No. 2013/0296175, the complete subject matter of which are expressly incorporated by reference herein in their entirety.</p><p id="p-1199" num="1193">In one implementation, a sequencing process includes receiving a sample that includes or is suspected of including nucleic acids, such as DNA. The sample may be from a known or unknown source, such as an animal (e.g., human), plant, bacteria, or fungus. The sample may be taken directly from the source. For instance, blood or saliva may be taken directly from an individual. Alternatively, the sample may not be obtained directly from the source. Then, one or more processors direct the system to prepare the sample for sequencing. The preparation may include removing extraneous material and/or isolating certain material (e.g., DNA). The biological sample may be prepared to include features for a particular assay. For example, the biological sample may be prepared for sequencing-by-synthesis (SBS). In certain implementations, the preparing may include amplification of certain regions of a genome. For instance, the preparing may include amplifying predetermined genetic loci that are known to include STRs and/or SNPs. The genetic loci may be amplified using predetermined primer sequences.</p><p id="p-1200" num="1194">Next, the one or more processors direct the system to sequence the sample. The sequencing may be performed through a variety of known sequencing protocols. In particular implementations, the sequencing includes SBS. In SBS, a plurality of fluorescently-labeled nucleotides are used to sequence a plurality of clusters of amplified DNA (possibly millions of clusters) present on the surface of an optical substrate (e.g., a surface that at least partially defines a channel in a flow cell). The flow cells may contain nucleic acid samples for sequencing where the flow cells are placed within the appropriate flow cell holders.</p><p id="p-1201" num="1195">The nucleic acids can be prepared such that they comprise a known primer sequence that is adjacent to an unknown target sequence. To initiate the first SBS sequencing cycle, one or more differently labeled nucleotides, and DNA polymerase, etc., can be flowed into/through the flow cell by a fluid flow subsystem. Either a single type of nucleotide can be added at a time, or the nucleotides used in the sequencing procedure can be specially designed to possess a reversible termination property, thus allowing each cycle of the sequencing reaction to occur simultaneously in the presence of several types of labeled nucleotides (e.g., A, C, T, G). The nucleotides can include detectable label moieties such as fluorophores. Where the four nucleotides are mixed together, the polymerase is able to select the correct base to incorporate and each sequence is extended by a single base. Non-incorporated nucleotides can be washed away by flowing a wash solution through the flow cell. One or more lasers may excite the nucleic acids and induce fluorescence. The fluorescence emitted from the nucleic acids is based upon the fluorophores of the incorporated base, and different fluorophores may emit different wavelengths of emission light. A deblocking reagent can be added to the flow cell to remove reversible terminator groups from the DNA strands that were extended and detected. The deblocking reagent can then be washed away by flowing a wash solution through the flow cell. The flow cell is then ready for a further cycle of sequencing starting with introduction of a labeled nucleotide as set forth above. The fluidic and detection operations can be repeated several times to complete a sequencing run. Example sequencing methods are described, for example, in Bentley et al., Nature 456:53-59 (2008), International Publication No. WO 04/018497; U.S. Pat. No. 7,057,026; International Publication No. WO 91/06678; International Publication No. WO 07/123744; U.S. Pat. Nos. 7,329,492; 7,211,414; 7,315,019; 7,405,281, and U.S. Patent Application Publication No. 2008/0108082, each of which is incorporated herein by reference.</p><p id="p-1202" num="1196">In some implementations, nucleic acids can be attached to a surface and amplified prior to or during sequencing. For example, amplification can be carried out using bridge amplification to form nucleic acid clusters on a surface. Useful bridge amplification methods are described, for example, in U.S. Pat. No. 5,641,658; U.S. Patent Application Publication No. 2002/0055100; U.S. Pat. No. 7,115,400; U.S. Patent Application Publication No. 2004/0096853; U.S. Patent Application Publication No. 2004/0002090; U.S. Patent Application Publication No. 2007/0128624; and U.S. Patent Application Publication No. 2008/0009420, each of which is incorporated herein by reference in its entirety. Another useful method for amplifying nucleic acids on a surface is rolling circle amplification (RCA), for example, as described in Lizardi et al., Nat. Genet. 19:225-232 (1998) and U.S. Patent Application Publication No. 2007/0099208 A1, each of which is incorporated herein by reference.</p><p id="p-1203" num="1197">One example SBS protocol exploits modified nucleotides having removable 3&#x2032; blocks, for example, as described in International Publication No. WO 04/018497, U.S. Patent Application Publication No. 2007/0166705A1, and U.S. Pat. No. 7,057,026, each of which is incorporated herein by reference. For example, repeated cycles of SBS reagents can be delivered to a flow cell having target nucleic acids attached thereto, for example, as a result of the bridge amplification protocol. The nucleic acid clusters can be converted to single stranded form using a linearization solution. The linearization solution can contain, for example, a restriction endonuclease capable of cleaving one strand of each cluster. Other methods of cleavage can be used as an alternative to restriction enzymes or nicking enzymes, including inter alia chemical cleavage (e.g., cleavage of a diol linkage with periodate), cleavage of abasic sites by cleavage with endonuclease (for example &#x2018;USER&#x2019;, as supplied by NEB, Ipswich, Mass., USA, part number M5505S), by exposure to heat or alkali, cleavage of ribonucleotides incorporated into amplification products otherwise comprised of deoxyribonucleotides, photochemical cleavage or cleavage of a peptide linker. After the linearization operation a sequencing primer can be delivered to the flow cell under conditions for hybridization of the sequencing primer to the target nucleic acids that are to be sequenced.</p><p id="p-1204" num="1198">A flow cell can then be contacted with an SBS extension reagent having modified nucleotides with removable 3&#x2032; blocks and fluorescent labels under conditions to extend a primer hybridized to each target nucleic acid by a single nucleotide addition. Only a single nucleotide is added to each primer because once the modified nucleotide has been incorporated into the growing polynucleotide chain complementary to the region of the template being sequenced there is no free <b>3</b>&#x2032;&#x2014;OH group available to direct further sequence extension and therefore the polymerase cannot add further nucleotides. The SBS extension reagent can be removed and replaced with scan reagent containing components that protect the sample under excitation with radiation. Example components for scan reagent are described in U.S. Patent Application Publication No. 2008/0280773 A1 and U.S. patent application Ser. No. 13/018,255, each of which is incorporated herein by reference. The extended nucleic acids can then be fluorescently detected in the presence of scan reagent. Once the fluorescence has been detected, the 3&#x2032; block may be removed using a deblock reagent that is appropriate to the blocking group used. Example deblock reagents that are useful for respective blocking groups are described in WO004018497, US 2007/0166705A1 and U.S. Pat. No. 7,057,026, each of which is incorporated herein by reference. The deblock reagent can be washed away leaving target nucleic acids hybridized to extended primers having 3&#x2032;&#x2014;OH groups that are now competent for addition of a further nucleotide. Accordingly the cycles of adding extension reagent, scan reagent, and deblock reagent, with optional washes between one or more of the operations, can be repeated until a desired sequence is obtained. The above cycles can be carried out using a single extension reagent delivery operation per cycle when each of the modified nucleotides has a different label attached thereto, known to correspond to the particular base. The different labels facilitate discrimination between the nucleotides added during each incorporation operation. Alternatively, each cycle can include separate operations of extension reagent deliver), followed by separate operations of scan reagent delivery and detection, in which case two or more of the nucleotides can have the same label and can be distinguished based on the known order of delivery.</p><p id="p-1205" num="1199">Although the sequencing operation has been discussed above with respect to a particular SBS protocol, it will be understood that other protocols for sequencing any of a variety of other molecular analyses can be carried out as desired.</p><p id="p-1206" num="1200">Then, the one or more processors of the system receive the sequencing data for subsequent analysis. The sequencing data may be formatted in various manners, such as in a .BAM file. The sequencing data may include, for example, a number of sample reads. The sequencing data may include a plurality of sample reads that have corresponding sample sequences of the nucleotides. Although only one sample read is discussed, it should be understood that the sequencing data may include, for example, hundreds, thousands, hundreds of thousands, or millions of sample reads. Different sample reads may have different numbers of nucleotides. For example, a sample read may range between 10 nucleotides to about 500 nucleotides or more. The sample reads may span the entire genome of the source(s). As one example, the sample reads are directed toward predetermined genetic loci, such as those genetic loci having suspected STRs or suspected SNPs.</p><p id="p-1207" num="1201">Each sample read may include a sequence of nucleotides, which may be referred to as a sample sequence, sample fragment or a target sequence. The sample sequence may include, for example, primer sequences, flanking sequences, and a target sequence. The number of nucleotides within the sample sequence may include 30, 40, 50, 60, 70, 80, 90, 100 or more. In some implementations, one or more the sample reads (or sample sequences) includes at least 150 nucleotides, 200 nucleotides, 300 nucleotides, 400 nucleotides, 500 nucleotides, or more. In some implementations, the sample reads may include more than 1000 nucleotides, 2000 nucleotides, or more. The sample reads (or the sample sequences) may include primer sequences at one or both ends.</p><p id="p-1208" num="1202">Next, the one or more processors analyze the sequencing data to obtain potential variant call(s) and a sample variant frequency of the sample variant call(s). The operation may also be referred to as a variant call application or variant caller. Thus, the variant caller identifies or detects variants and the variant classifier classifies the detected variants as somatic or germline. Alternative variant callers may be utilized in accordance with implementations herein, wherein different variant callers may be used based on the type of sequencing operation being performed, based on features of the sample that are of interest and the like. One non-limiting example of a variant call application, such as the Pisces&#x2122; application by Illumina Inc. (San Diego, Calif.) hosted at https://github.com/Illumina/Pisces and described in the article Dunn, Tamsen &#x26; Berry, Gwenn &#x26; Emig-Agius, Dorothea &#x26; Jiang, Yu &#x26; Iyer, Anita &#x26; Udar, Nitin &#x26; Stromberg, Michael. (2017). Pisces: An Accurate and Versatile Single Sample Somatic and Germline Variant Caller. 595-595. 10.1145/3107411.3108203, the complete subject matter of which is expressly incorporated herein by reference in its entirety.</p><p id="p-1209" num="1203">Such a variant call application can comprise four sequentially executed modules:</p><p id="p-1210" num="1204">(1) Pisces Read Stitcher: Reduces noise by stitching paired reads in a BAM (read one and read two of the same molecule) into consensus reads. The output is a stitched BAM.</p><p id="p-1211" num="1205">(2) Pisces Variant Caller: Calls small SNVs, insertions and deletions. Pisces includes a variant-collapsing algorithm to coalesce variants broken up by read boundaries, basic filtering algorithms, and a simple Poisson-based variant confidence-scoring algorithm. The output is a VCF.</p><p id="p-1212" num="1206">(3) Pisces Variant Quality Recalibrator (VQR): In the event that the variant calls overwhelmingly follow a pattern associated with thermal damage or FFPE deamination, the VQR step will downgrade the variant Q score of the suspect variant calls. The output is an adjusted VCF.</p><p id="p-1213" num="1207">(4) Pisces Variant Phaser (Scylla): Uses a read-backed greedy clustering method to assemble small variants into complex alleles from clonal subpopulations. This allows for the more accurate determination of functional consequence by downstream tools. The output is an adjusted VCF.</p><p id="p-1214" num="1208">Additionally or alternatively, the operation may utilize the variant call application Strelka&#x2122; application by Illumina Inc. hosted at https://github.com/Illumina/strelka and described in the article T Saunders, Christopher &#x26; Wong, Wendy &#x26; Swamy, Sajani &#x26; Becq, Jennifer &#x26; J Murray, Lisa &#x26; Cheetham, Keira. (2012). Strelka: Accurate somatic small-variant calling from sequenced tumor-normal sample pairs. Bioinformatics (Oxford, England). 28. 1811-7. 10.1093/bioinformatics/bts271, the complete subject matter of which is expressly incorporated herein by reference in its entirety. Furthermore, additionally or alternatively, the operation may utilize the variant call application Strelka2&#x2122; application by Illumina Inc. hosted at https://github.com/Illumina/strelka and described in the article Kim, S., Scheffler, K., Halpern, A. L., Bekritsky, M. A., Noh, E., Kallberg, M., Chen, X., Beyter, D., Krusche, P., and Saunders, C. T. (2017). Strelka2: Fast and accurate variant calling for clinical sequencing applications, the complete subject matter of which is expressly incorporated herein by reference in its entirety. Moreover, additionally or alternatively, the operation may utilize a variant annotation/call tool, such as the Nirvana&#x2122; application by Illumina Inc. hosted at https://github.com/Illumina/Nirvana/wiki and described in the article Stromberg, Michael &#x26; Roy, Rajat &#x26; Lajugie, Julien &#x26; Jiang, Yu &#x26; Li, Haochen &#x26; Margulies, Elliott. (2017). Nirvana: Clinical Grade Variant Annotator. 596-596. 10.1145/3107411.3108204, the complete subject matter of which is expressly incorporated herein by reference in its entirety.</p><p id="p-1215" num="1209">Such a variant annotation/call tool can apply different algorithmic techniques such as those disclosed in Nirvana:</p><p id="p-1216" num="1210">a. Identifying all overlapping transcripts with Interval Array: For functional annotation, we can identify all transcripts overlapping a variant and an interval tree can be used. However, since a set of intervals can be static, we were able to further optimize it to an Interval Array. An interval tree returns all overlapping transcripts in O(min(n,k lg n)) time, where n is the number of intervals in the tree and k is the number of overlapping intervals. In practice, since k is really small compared to n for most variants, the effective runtime on interval tree would be O(k lg n). We improved to O(lg n+k) by creating an interval array where all intervals are stored in a sorted array so that we only need to find the first overlapping interval and then enumerate through the remaining (k&#x2212;1).</p><p id="p-1217" num="1211">b. CNVs/SVs (Yu): annotations for Copy Number Variation and Structural Variants can be provided. Similar to the annotation of small variants, transcripts overlapping with the SV and also previously reported structural variants can be annotated in online databases. Unlike the small variants, not all overlapping transcripts need be annotated, since too many transcripts will be overlapped with a large SVs. Instead, all overlapping transcripts can be annotated that belong to a partial overlapping gene. Specifically, for these transcripts, the impacted introns, exons and the consequences caused by the structural variants can be reported. An option to allow output all overlapping transcripts is available, but the basic information for these transcripts can be reported, such as gene symbol, flag whether it is canonical overlap or partial overlapped with the transcripts. For each SV/CNV, it is also of interest to know if these variants have been studied and their frequencies in different populations. Hence, we reported overlapping SVs in external databases, such as 1000 genomes, DGV and ClinGen. To avoid using an arbitrary cutoff to determine which SV is overlapped, instead all overlapping transcripts can be used and the reciprocal overlap can be calculated, i.e. the overlapping length divided by the minimum of the length of these two SVs.</p><p id="p-1218" num="1212">c. Reporting supplementary annotations: Supplementary annotations are of two types: small and structural variants (SVs). SVs can be modeled as intervals and use the interval array discussed above to identify overlapping SVs. Small variants are modeled as points and matched by position and (optionally) allele. As such, they are searched using a binary-search-like algorithm. Since the supplementary annotation database can be quite large, a much smaller index is created to map chromosome positions to file locations where the supplementary annotation resides. The index is a sorted array of objects (made up of chromosome position and file location) that can be binary searched using position. To keep the index size small, multiple positions (up to a certain max count) are compressed to one object that stores the values for the first position and only deltas for subsequent positions. Since we use Binary search, the runtime is O(lg n), where n is the number of items in the database.</p><p id="p-1219" num="1213">d. VEP cache files</p><p id="p-1220" num="1214">e. Transcript Database: The Transcript Cache (cache) and Supplementary database (SAdb) files are serialized dump of data objects such as transcripts and supplementary annotations. We use Ensembl VEP cache as our data source for cache. To create the cache, all transcripts are inserted in an interval array and the final state of the array is stored in the cache files. Thus, during annotation, we only need to load a pre-computed interval array and perform searches on it. Since the cache is loaded up in memory and searching is very fast (described above), finding overlapping transcripts is extremely quick in Nirvana (profiled to less than 1% of total runtime?).</p><p id="p-1221" num="1215">f. Supplementary Database: The data sources for SAdb are listed under supplementary material. The SAdb for small variants is produced by a k-way merge of all data sources such that each object in the database (identified by reference name and position) holds all relevant supplementary annotations. Issues encountered during parsing data source files have been documented in detail in Nirvana's home page. To limit memory usage, only the SA index is loaded up in memory. This index allows a quick lookup of the file location for a supplementary annotation. However, since the data has to be fetched from disk, adding supplementary annotation has been identified as Nirvana's largest bottleneck (profiled at &#x2dc;30% of total runtime.)</p><p id="p-1222" num="1216">g. Consequence and Sequence Ontology: Nirvana's functional annotation (when provided) follows the Sequence Ontology (SO) (http://www.sequenceontology.org/) guidelines. On occasions, we had the opportunity to identify issues in the current SO and collaborate with the SO team to improve the state of annotation.</p><p id="p-1223" num="1217">Such a variant annotation tool can include pre-processing. For example, Nirvana included a large number of annotations from External data sources, like ExAC, EVS, 1000 Genomes project, dbSNP, ClinVar, Cosmic, DGV and ClinGen. To make full use of these databases, we have to sanitize the information from them. We implemented different strategy to deal with different conflicts that exist from different data sources. For example, in case of multiple dbSNP entries for the same position and alternate allele, we join all ids into a comma separated list of ids; if there are multiple entries with different CAF values for the same allele, we use the first CAF value. For conflicting ExAC and EVS entries, we consider the number of sample counts and the entry with higher sample count is used. In 1000 Genome Projects, we removed the allele frequency of the conflicting allele. Another issue is inaccurate information. We mainly extracted the allele frequencies information from 1000 Genome Projects, however, we noticed that for GRCh38, the allele frequency reported in the info field did not exclude samples with genotype not available, leading to deflated frequencies for variants which are not available for all samples. To guarantee the accuracy of our annotation, we use all of the individual level genotype to compute the true allele frequencies. As we know, the same variants can have different representations based on different alignments. To make sure we can accurately report the information for already identified variants, we have to preprocess the variants from different resources to make them have consistent representation. For all external data sources, we trimmed alleles to remove duplicated nucleotides in both reference allele and alternative allele. For ClinVar, we directly parsed the xml file we performed a five-prime alignment for all variants, which is often used in vcf file. Different databases can contain the same set of information. To avoid unnecessary duplicates, we removed some duplicated information. For example, we removed variants in DGV which has data source as 1000 genome projects, since we already reported these variants in 1000 genomes with more detailed information.</p><p id="p-1224" num="1218">In accordance with at least some implementations, the variant call application provides calls for low frequency variants, germline calling and the like. As non-limiting example, the variant call application may run on tumor-only samples and/or tumor-normal paired samples. The variant call application may search for single nucleotide variations (SNV), multiple nucleotide variations (MNV), indels and the like. The variant call application identifies variants, while filtering for mismatches due to sequencing or sample preparation errors. For each variant, the variant caller identifies the reference sequence, a position of the variant, and the potential variant sequence(s) (e.g., A to C SNV, or AG to A deletion). The variant call application identifies the sample sequence (or sample fragment), a reference sequence/fragment, and a variant call as an indication that a variant is present. The variant call application may identify raw fragments, and output a designation of the raw fragments, a count of the number of raw fragments that verify the potential variant call, the position within the raw fragment at which a supporting variant occurred and other relevant information. Non-limiting examples of raw fragments include a duplex stitched fragment, a simplex stitched fragment, a duplex un-stitched fragment and a simplex un-stitched fragment.</p><p id="p-1225" num="1219">The variant call application may output the calls in various formats, such as in a .VCF or .GVCF file. By way of example only, the variant call application may be included in a MiSeqReporter pipeline (e.g., when implemented on the MiSeq&#xae; sequencer instrument). Optionally, the application may be implemented with various workflows. The analysis may include a single protocol or a combination of protocols that analyze the sample reads in a designated manner to obtain desired information.</p><p id="p-1226" num="1220">Then, the one or more processors perform a validation operation in connection with the potential variant call. The validation operation may be based on a quality score, and/or a hierarchy of tiered tests, as explained hereafter. When the validation operation authenticates or verifies that the potential variant call, the validation operation passes the variant call information (from the variant call application) to the sample report generator. Alternatively, when the validation operation invalidates or disqualifies the potential variant call, the validation operation passes a corresponding indication (e.g., a negative indicator, a no call indicator, an in-valid call indicator) to the sample report generator. The validation operation also may pass a confidence score related to a degree of confidence that the variant call is correct or the in-valid call designation is correct.</p><p id="p-1227" num="1221">Next, the one or more processors generate and store a sample report. The sample report may include, for example, information regarding a plurality of genetic loci with respect to the sample. For example, for each genetic locus of a predetermined set of genetic loci, the sample report may at least one of provide a genotype call; indicate that a genotype call cannot be made; provide a confidence score on a certainty of the genotype call; or indicate potential problems with an assay regarding one or more genetic loci. The sample report may also indicate a gender of an individual that provided a sample and/or indicate that the sample include multiple sources. As used herein, a &#x201c;sample report&#x201d; may include digital data (e.g., a data file) of a genetic locus or predetermined set of genetic locus and/or a printed report of the genetic locus or the set of genetic loci. Thus, generating or providing may include creating a data file and/or printing the sample report, or displaying the sample report.</p><p id="p-1228" num="1222">The sample report may indicate that a variant call was determined, but was not validated. When a variant call is determined invalid, the sample report may indicate additional information regarding the basis for the determination to not validate the variant call. For example, the additional information in the report may include a description of the raw fragments and an extent (e.g., a count) to which the raw fragments support or contradicted the variant call. Additionally or alternatively, the additional information in the report may include the quality score obtained in accordance with implementations described herein.</p><p id="p-1229" num="1223">Variant Call Application</p><p id="p-1230" num="1224">Implementations disclosed herein include analyzing sequencing data to identify potential variant calls. Variant calling may be performed upon stored data for a previously performed sequencing operation. Additionally or alternatively, it may be performed in real time while a sequencing operation is being performed. Each of the sample reads is assigned to corresponding genetic loci. The sample reads may be assigned to corresponding genetic loci based on the sequence of the nucleotides of the sample read or, in other words, the order of nucleotides within the sample read (e.g., A, C, G, T). Based on this analysis, the sample read may be designated as including a possible variant/allele of a particular genetic locus. The sample read may be collected (or aggregated or binned) with other sample reads that have been designated as including possible variants/alleles of the genetic locus. The assigning operation may also be referred to as a calling operation in which the sample read is identified as being possibly associated with a particular genetic position/locus. The sample reads may be analyzed to locate one or more identifying sequences (e.g., primer sequences) of nucleotides that differentiate the sample read from other sample reads. More specifically, the identifying sequence(s) may identify the sample read from other sample reads as being associated with a particular genetic locus.</p><p id="p-1231" num="1225">The assigning operation may include analyzing the series of n nucleotides of the identifying sequence to determine if the series of n nucleotides of the identifying sequence effectively matches with one or more of the select sequences. In particular implementations, the assigning operation may include analyzing the first n nucleotides of the sample sequence to determine if the first n nucleotides of the sample sequence effectively matches with one or more of the select sequences. The number n may have a variety of values, which may be programmed into the protocol or entered by a user. For example, the number n may be defined as the number of nucleotides of the shortest select sequence within the database. The number n may be a predetermined number. The predetermined number may be, for example, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, or 30 nucleotides. However, fewer or more nucleotides may be used in other implementations. The number n may also be selected by an individual, such as a user of the system. The number n may be based on one or more conditions. For instance, the number n may be defined as the number of nucleotides of the shortest primer sequence within the database or a designated number, whichever is the smaller number. In some implementations, a minimum value for n may be used, such as 15, such that any primer sequence that is less than 15 nucleotides may be designated as an exception.</p><p id="p-1232" num="1226">In some cases, the series of n nucleotides of an identifying sequence may not precisely match the nucleotides of the select sequence. Nonetheless, the identifying sequence may effectively match the select sequence if the identifying sequence is nearly identical to the select sequence. For example, the sample read may be called for a genetic locus if the series of n nucleotides (e.g., the first n nucleotides) of the identifying sequence match a select sequence with no more than a designated number of mismatches (e.g., 3) and/or a designated number of shifts (e.g., 2). Rules may be established such that each mismatch or shift may count as a difference between the sample read and the primer sequence. If the number of differences is less than a designated number, then the sample read may be called for the corresponding genetic locus (i.e., assigned to the corresponding genetic locus). In some implementations, a matching score may be determined that is based on the number of differences between the identifying sequence of the sample read and the select sequence associated with a genetic locus. If the matching score passes a designated matching threshold, then the genetic locus that corresponds to the select sequence may be designated as a potential locus for the sample read. In some implementations, subsequent analysis may be performed to determine whether the sample read is called for the genetic locus.</p><p id="p-1233" num="1227">If the sample read effectively matches one of the select sequences in the database (i.e., exactly matches or nearly matches as described above), then the sample read is assigned or designated to the genetic locus that correlates to the select sequence. This may be referred to as locus calling or provisional-locus calling, wherein the sample read is called for the genetic locus that correlates to the select sequence. However, as discussed above, a sample read may be called for more than one genetic locus. In such implementations, further analysis may be performed to call or assign the sample read for only one of the potential genetic loci. In some implementations, the sample read that is compared to the database of reference sequences is the first read from paired-end sequencing. When performing paired-end sequencing, a second read (representing a raw fragment) is obtained that correlates to the sample read. After assigning, the subsequent analysis that is performed with the assigned reads may be based on the type of genetic locus that has been called for the assigned read.</p><p id="p-1234" num="1228">Next, the sample reads are analyzed to identify potential variant calls. Among other things, the results of the analysis identify the potential variant call, a sample variant frequency, a reference sequence and a position within the genomic sequence of interest at which the variant occurred. For example, if a genetic locus is known for including SNPs, then the assigned reads that have been called for the genetic locus may undergo analysis to identify the SNPs of the assigned reads. If the genetic locus is known for including polymorphic repetitive DNA elements, then the assigned reads may be analyzed to identify or characterize the polymorphic repetitive DNA elements within the sample reads. In some implementations, if an assigned read effectively matches with an STR locus and an SNP locus, a warning or flag may be assigned to the sample read. The sample read may be designated as both an STR locus and an SNP locus. The analyzing may include aligning the assigned reads in accordance with an alignment protocol to determine sequences and/or lengths of the assigned reads. The alignment protocol may include the method described in International Patent Application No. PCT/US2013/030867 (Publication No. WO 2014/142831), filed on Mar. 15, 2013, which is herein incorporated by reference in its entirety.</p><p id="p-1235" num="1229">Then, the one or more processors analyze raw fragments to determine whether supporting variants exist at corresponding positions within the raw fragments. Various types of raw fragments may be identified. For example, the variant caller may identify a type of raw fragment that exhibits a variant that validates the original variant call. For example, the type of raw fragment may represent a duplex stitched fragment, a simplex stitched fragment, a duplex un-stitched fragment or a simplex un-stitched fragment Optionally other raw fragments may be identified instead of or in addition to the foregoing examples. In connection with identifying each type of raw fragment, the variant caller also identifies the position, within the raw fragment, at which the supporting variant occurred, as well as a count of the number of raw fragments that exhibited the supporting variant. For example, the variant caller may output an indication that 10 reads of raw fragments were identified to represent duplex stitched fragments having a supporting variant at a particular position X. The variant caller may also output indication that five reads of raw fragments were identified to represent simplex un-stitched fragments having a supporting variant at a particular position Y. The variant caller may also output a number of raw fragments that corresponded to reference sequences and thus did not include a supporting variant that would otherwise provide evidence validating the potential variant call at the genomic sequence of interest.</p><p id="p-1236" num="1230">Next, a count is maintained of the raw fragments that include supporting variants, as well as the position at which the supporting variant occurred. Additionally or alternatively, a count may be maintained of the raw fragments that did not include supporting variants at the position of interest (relative to the position of the potential variant call in the sample read or sample fragment). Additionally or alternatively, a count may be maintained of raw fragments that correspond to a reference sequence and do not authenticate or confirm the potential variant call. The information determined is output to the variant call validation application, including a count and type of the raw fragments that support the potential variant call, positions of the supporting variance in the raw fragments, a count of the raw fragments that do not support the potential variant call and the like.</p><p id="p-1237" num="1231">When a potential variant call is identified, the process outputs an indicating of the potential variant call, the variant sequence, the variant position and a reference sequence associated therewith. The variant call is designated to represent a &#x201c;potential&#x201d; variant as errors may cause the call process to identify a false variant In accordance with implementations herein, the potential variant call is analyzed to reduce and eliminate false variants or false positives. Additionally or alternatively, the process analyzes one or more raw fragments associated with a sample read and outputs a corresponding variant call associated with the raw fragments.</p><heading id="h-0023" level="2">Technical Improvements and Terminology</heading><p id="p-1238" num="1232">Base calling includes incorporation or attachment of a fluorescently-labeled tag with an analyte. The analyte can be a nucleotide or an oligonucleotide, and the tag can be for a particular nucleotide type (A, C, T, or G). Excitation light is directed toward the analyte having the tag, and the tag emits a detectable fluorescent signal or intensity emission. The intensity emission is indicative of photons emitted by the excited tag that is chemically attached to the analyte.</p><p id="p-1239" num="1233">Throughout this application, including the claims, when phrases such as or similar to &#x201c;images, image data, or image regions depicting intensity emissions of analytes and their surrounding background&#x201d; are used, they refer to the intensity emissions of the tags attached to the analytes. A person skilled in the art will appreciate that the intensity emissions of the attached tags are representative of or equivalent to the intensity emissions of the analytes to which the tags are attached, and are therefore used interchangeably. Similarly, properties of the analytes refer to properties of the tags attached to the analytes or of the intensity emissions from the attached tags. For example, a center of an analyte refers to the center of the intensity emissions emitted by a tag attached to the analyte. In another example, the surrounding background of an analyte refers to the surrounding background of the intensity emissions emitted by a tag attached to the analyte.</p><p id="p-1240" num="1234">All literature and similar material cited in this application, including, but not limited to, patents, patent applications, articles, books, treatises, and web pages, regardless of the format of such literature and similar materials, are expressly incorporated by reference in their entirety. In the event that one or more of the incorporated literature and similar materials differs from or contradicts this application, including but not limited to defined terms, term usage, described techniques, or the like, this application controls.</p><p id="p-1241" num="1235">The technology disclosed uses neural networks to improve the quality and quantity of nucleic acid sequence information that can be obtained from a nucleic acid sample such as a nucleic acid template or its complement, for instance, a DNA or RNA polynucleotide or other nucleic acid sample. Accordingly, certain implementations of the technology disclosed provide higher throughput polynucleotide sequencing, for instance, higher rates of collection of DNA or RNA sequence data, greater efficiency in sequence data collection, and/or lower costs of obtaining such sequence data, relative to previously available methodologies.</p><p id="p-1242" num="1236">The technology disclosed uses neural networks to identify the center of a solid-phase nucleic acid cluster and to analyze optical signals that are generated during sequencing of such clusters, to discriminate unambiguously between adjacent, abutting or overlapping clusters in order to assign a sequencing signal to a single, discrete source cluster. These and related implementations thus permit retrieval of meaningful information, such as sequence data, from regions of high-density cluster arrays where useful information could not previously be obtained from such regions due to confounding effects of overlapping or very closely spaced adjacent clusters, including the effects of overlapping signals (e.g., as used in nucleic acid sequencing) emanating therefrom.</p><p id="p-1243" num="1237">As described in greater detail below, in certain implementations there is provided a composition that comprises a solid support having immobilized thereto one or a plurality of nucleic acid clusters as provided herein. Each cluster comprises a plurality of immobilized nucleic acids of the same sequence and has an identifiable center having a detectable center label as provided herein, by which the identifiable center is distinguishable from immobilized nucleic acids in a surrounding region in the cluster. Also described herein are methods for making and using such clusters that have identifiable centers.</p><p id="p-1244" num="1238">The presently disclosed implementations will find uses in numerous situations where advantages are obtained from the ability to identify, determine, annotate, record or otherwise assign the position of a substantially central location within a cluster, such as high-throughput nucleic acid sequencing, development of image analysis algorithms for assigning optical or other signals to discrete source clusters, and other applications where recognition of the center of an immobilized nucleic acid cluster is desirable and beneficial.</p><p id="p-1245" num="1239">In certain implementations, the present invention contemplates methods that relate to high-throughput nucleic acid analysis such as nucleic acid sequence determination (e.g., &#x201c;sequencing&#x201d;). Exemplary high-throughput nucleic acid analyses include without limitation de novo sequencing, re-sequencing, whole genome sequencing, gene expression analysis, gene expression monitoring, epigenetic analysis, genome methylation analysis, allele specific primer extension (APSE), genetic diversity profiling, whole genome polymorphism discovery and analysis, single nucleotide polymorphism analysis, hybridization based sequence determination methods, and the like. One skilled in the art will appreciate that a variety of different nucleic acids can be analyzed using the methods and compositions of the present invention.</p><p id="p-1246" num="1240">Although the implementations of the present invention are described in relation to nucleic acid sequencing, they are applicable in any field where image data acquired at different time points, spatial locations or other temporal or physical perspectives is analyzed. For example, the methods and systems described herein are useful in the fields of molecular and cell biology where image data from microarrays, biological specimens, cells, organisms and the like is acquired and at different time points or perspectives and analyzed. Images can be obtained using any number of techniques known in the art including, but not limited to, fluorescence microscopy, light microscopy, confocal microscopy, optical imaging, magnetic resonance imaging, tomography scanning or the like. As another example, the methods and systems described herein can be applied where image data obtained by surveillance, aerial or satellite imaging technologies and the like is acquired at different time points or perspectives and analyzed. The methods and systems are particularly useful for analyzing images obtained for a field of view in which the analytes being viewed remain in the same locations relative to each other in the field of view. The analytes may however have characteristics that differ in separate images, for example, the analytes may appear different in separate images of the field of view. For example, the analytes may appear different with regard to the color of a given analyte detected in different images, a change in the intensity of signal detected for a given analyte in different images, or even the appearance of a signal for a given analyte in one image and disappearance of the signal for the analyte in another image.</p><p id="p-1247" num="1241">Examples described herein may be used in various biological or chemical processes and systems for academic or commercial analysis. More specifically, examples described herein may be used in various processes and systems where it is desired to detect an event, property, quality, or characteristic that is indicative of a designated reaction. For example, examples described herein include light detection devices, biosensors, and their components, as well as bioassay systems that operate with biosensors. In some examples, the devices, biosensors and systems may include a flow cell and one or more light sensors that are coupled together (removably or fixedly) in a substantially unitary structure.</p><p id="p-1248" num="1242">The devices, biosensors and bioassay systems may be configured to perform a plurality of designated reactions that may be detected individually or collectively. The devices, biosensors and bioassay systems may be configured to perform numerous cycles in which the plurality of designated reactions occurs in parallel. For example, the devices, biosensors and bioassay systems may be used to sequence a dense array of DNA features through iterative cycles of enzymatic manipulation and light or image detection/acquisition. As such, the devices, biosensors and bioassay systems (e.g., via one or more cartridges) may include one or more microfluidic channel that delivers reagents or other reaction components in a reaction solution to a reaction site of the devices, biosensors and bioassay systems. In some examples, the reaction solution may be substantially acidic, such as comprising a pH of less than or equal to about 5, or less than or equal to about 4, or less than or equal to about 3. In some other examples, the reaction solution may be substantially alkaline/basic, such as comprising a pH of greater than or equal to about 8, or greater than or equal to about 9, or greater than or equal to about 10. As used herein, the term &#x201c;acidity&#x201d; and grammatical variants thereof refer to a pH value of less than about 7, and the terms &#x201c;basicity,&#x201d; &#x201c;alkalinity&#x201d; and grammatical variants thereof refer to a pH value of greater than about 7.</p><p id="p-1249" num="1243">In some examples, the reaction sites are provided or spaced apart in a predetermined manner, such as in a uniform or repeating pattern. In some other examples, the reaction sites are randomly distributed. Each of the reaction sites may be associated with one or more light guides and one or more light sensors that detect light from the associated reaction site. In some examples, the reaction sites are located in reaction recesses or chambers, which may at least partially compartmentalize the designated reactions therein.</p><p id="p-1250" num="1244">As used herein, a &#x201c;designated reaction&#x201d; includes a change in at least one of a chemical, electrical, physical, or optical property (or quality) of a chemical or biological substance of interest, such as an analyte-of-interest. In particular examples, a designated reaction is a positive binding event, such as incorporation of a fluorescently labeled biomolecule with an analyte-of-interest, for example. More generally, a designated reaction may be a chemical transformation, chemical change, or chemical interaction. A designated reaction may also be a change in electrical properties. In particular examples, a designated reaction includes the incorporation of a fluorescently-labeled molecule with an analyte. The analyte may be an oligonucleotide and the fluorescently-labeled molecule may be a nucleotide. A designated reaction may be detected when an excitation light is directed toward the oligonucleotide having the labeled nucleotide, and the fluorophore emits a detectable fluorescent signal. In alternative examples, the detected fluorescence is a result of chemiluminescence or bioluminescence. A designated reaction may also increase fluorescence (or Forster) resonance energy transfer (FRET), for example, by bringing a donor fluorophore in proximity to an acceptor fluorophore, decrease FRET by separating donor and acceptor fluorophores, increase fluorescence by separating a quencher from a fluorophore, or decrease fluorescence by co-locating a quencher and fluorophore.</p><p id="p-1251" num="1245">As used herein, a &#x201c;reaction solution,&#x201d; &#x201c;reaction component&#x201d; or &#x201c;reactant&#x201d; includes any substance that may be used to obtain at least one designated reaction. For example, potential reaction components include reagents, enzymes, samples, other biomolecules, and buffer solutions, for example. The reaction components may be delivered to a reaction site in a solution and/or immobilized at a reaction site. The reaction components may interact directly or indirectly with another substance, such as an analyte-of-interest immobilized at a reaction site. As noted above, the reaction solution may be substantially acidic (i.e., include a relatively high acidity) (e.g., comprising a pH of less than or equal to about 5, a pH less than or equal to about 4, or a pH less than or equal to about 3) or substantially alkaline/basic (i.e., include a relatively high alkalinity/basicity) (e.g., comprising a pH of greater than or equal to about 8, a pH of greater than or equal to about 9, or a pH of greater than or equal to about 10).</p><p id="p-1252" num="1246">As used herein, the term &#x201c;reaction site&#x201d; is a localized region where at least one designated reaction may occur. A reaction site may include support surfaces of a reaction structure or substrate where a substance may be immobilized thereon. For example, a reaction site may include a surface of a reaction structure (which may be positioned in a channel of a flow cell) that has a reaction component thereon, such as a colony of nucleic acids thereon. In some such examples, the nucleic acids in the colony have the same sequence, being for example, clonal copies of a single stranded or double stranded template. However, in some examples a reaction site may contain only a single nucleic acid molecule, for example, in a single stranded or double stranded form.</p><p id="p-1253" num="1247">A plurality of reaction sites may be randomly distributed along the reaction structure or arranged in a predetermined manner (e.g., side-by-side in a matrix, such as in microarrays). A reaction site can also include a reaction chamber or recess that at least partially defines a spatial region or volume configured to compartmentalize the designated reaction. As used herein, the term &#x201c;reaction chamber&#x201d; or &#x201c;reaction recess&#x201d; includes a defined spatial region of the support structure (which is often in fluid communication with a flow channel). A reaction recess may be at least partially separated from the surrounding environment other or spatial regions. For example, a plurality of reaction recesses may be separated from each other by shared walls, such as a detection surface. As a more specific example, the reaction recesses may be nanowells comprising an indent, pit, well, groove, cavity or depression defined by interior surfaces of a detection surface and have an opening or aperture (i.e., be open-sided) so that the nanowells can be in fluid communication with a flow channel.</p><p id="p-1254" num="1248">In some examples, the reaction recesses of the reaction structure are sized and shaped relative to solids (including semi-solids) so that the solids may be inserted, fully or partially, therein. For example, the reaction recesses may be sized and shaped to accommodate a capture bead. The capture bead may have clonally amplified DNA or other substances thereon. Alternatively, the reaction recesses may be sized and shaped to receive an approximate number of beads or solid substrates. As another example, the reaction recesses may be filled with a porous gel or substance that is configured to control diffusion or filter fluids or solutions that may flow into the reaction recesses.</p><p id="p-1255" num="1249">In some examples, light sensors (e.g., photodiodes) are associated with corresponding reaction sites. A light sensor that is associated with a reaction site is configured to detect light emissions from the associated reaction site via at least one light guide when a designated reaction has occurred at the associated reaction site. In some cases, a plurality of light sensors (e.g. several pixels of a light detection or camera device) may be associated with a single reaction site. In other cases, a single light sensor (e.g. a single pixel) may be associated with a single reaction site or with a group of reaction sites. The light sensor, the reaction site, and other features of the biosensor may be configured so that at least some of the light is directly detected by the light sensor without being reflected.</p><p id="p-1256" num="1250">As used herein, a &#x201c;biological or chemical substance&#x201d; includes biomolecules, samples-of-interest, analytes-of-interest, and other chemical compound(s). A biological or chemical substance may be used to detect, identify, or analyze other chemical compound(s), or function as intermediaries to study or analyze other chemical compound(s). In particular examples, the biological or chemical substances include a biomolecule. As used herein, a &#x201c;biomolecule&#x201d; includes at least one of a biopolymer, nucleoside, nucleic acid, polynucleotide, oligonucleotide, protein, enzyme, polypeptide, antibody, antigen, ligand, receptor, polysaccharide, carbohydrate, polyphosphate, cell, tissue, organism, or fragment thereof or any other biologically active chemical compound(s) such as analogs or mimetics of the aforementioned species. In a further example, a biological or chemical substance or a biomolecule includes an enzyme or reagent used in a coupled reaction to detect the product of another reaction such as an enzyme or reagent, such as an enzyme or reagent used to detect pyrophosphate in a pyrosequencing reaction. Enzymes and reagents useful for pyrophosphate detection are described, for example, in U.S. Patent Publication No. 2005/0244870 A1, which is incorporated by reference in its entirety.</p><p id="p-1257" num="1251">Biomolecules, samples, and biological or chemical substances may be naturally occurring or synthetic and may be suspended in a solution or mixture within a reaction recess or region. Biomolecules, samples, and biological or chemical substances may also be bound to a solid phase or gel material. Biomolecules, samples, and biological or chemical substances may also include a pharmaceutical composition. In some cases, biomolecules, samples, and biological or chemical substances of interest may be referred to as targets, probes, or analytes.</p><p id="p-1258" num="1252">As used herein, a &#x201c;biosensor&#x201d; includes a device that includes a reaction structure with a plurality of reaction sites that is configured to detect designated reactions that occur at or proximate to the reaction sites. A biosensor may include a solid-state light detection or &#x201c;imaging&#x201d; device (e.g., CCD or CMOS light detection device) and, optionally, a flow cell mounted thereto. The flow cell may include at least one flow channel that is in fluid communication with the reaction sites. As one specific example, the biosensor is configured to fluidically and electrically couple to a bioassay system. The bioassay system may deliver a reaction solution to the reaction sites according to a predetermined protocol (e.g., sequencing-by-synthesis) and perform a plurality of imaging events. For example, the bioassay system may direct reaction solutions to flow along the reaction sites. At least one of the reaction solutions may include four types of nucleotides having the same or different fluorescent labels. The nucleotides may bind to the reaction sites, such as to corresponding oligonucleotides at the reaction sites. The bioassay system may then illuminate the reaction sites using an excitation light source (e.g., solid-state light sources, such as light-emitting diodes (LEDs)). The excitation light may have a predetermined wavelength or wavelengths, including a range of wavelengths. The fluorescent labels excited by the incident excitation light may provide emission signals (e.g., light of a wavelength or wavelengths that differ from the excitation light and, potentially, each other) that may be detected by the light sensors.</p><p id="p-1259" num="1253">As used herein, the term &#x201c;immobilized,&#x201d; when used with respect to a biomolecule or biological or chemical substance, includes substantially attaching the biomolecule or biological or chemical substance at a molecular level to a surface, such as to a detection surface of a light detection device or reaction structure. For example, a biomolecule or biological or chemical substance may be immobilized to a surface of the reaction structure using adsorption techniques including non-covalent interactions (e.g., electrostatic forces, van der Waals, and dehydration of hydrophobic interfaces) and covalent binding techniques where functional groups or linkers facilitate attaching the biomolecules to the surface. Immobilizing biomolecules or biological or chemical substances to the surface may be based upon the properties of the surface, the liquid medium carrying the biomolecule or biological or chemical substance, and the properties of the biomolecules or biological or chemical substances themselves. In some cases, the surface may be functionalized (e.g., chemically or physically modified) to facilitate immobilizing the biomolecules (or biological or chemical substances) to the surface.</p><p id="p-1260" num="1254">In some examples, nucleic acids can be immobilized to the reaction structure, such as to surfaces of reaction recesses thereof. In particular examples, the devices, biosensors, bioassay systems and methods described herein may include the use of natural nucleotides and also enzymes that are configured to interact with the natural nucleotides. Natural nucleotides include, for example, ribonucleotides or deoxyribonucleotides. Natural nucleotides can be in the mono-, di-, or tri-phosphate form and can have a base selected from adenine (A), Thymine (T), uracil (U), guanine (G) or cytosine (C). It will be understood, however, that non-natural nucleotides, modified nucleotides or analogs of the aforementioned nucleotides can be used.</p><p id="p-1261" num="1255">As noted above, a biomolecule or biological or chemical substance may be immobilized at a reaction site in a reaction recess of a reaction structure. Such a biomolecule or biological substance may be physically held or immobilized within the reaction recesses through an interference fit, adhesion, covalent bond, or entrapment. Examples of items or solids that may be disposed within the reaction recesses include polymer beads, pellets, agarose gel, powders, quantum dots, or other solids that may be compressed and/or held within the reaction chamber. In certain implementations, the reaction recesses may be coated or filled with a hydrogel layer capable of covalently binding DNA oligonucleotides. In particular examples, a nucleic acid superstructure, such as a DNA ball, can be disposed in or at a reaction recess, for example, by attachment to an interior surface of the reaction recess or by residence in a liquid within the reaction recess. A DNA ball or other nucleic acid superstructure can be performed and then disposed in or at a reaction recess. Alternatively, a DNA ball can be synthesized in situ at a reaction recess. A substance that is immobilized in a reaction recess can be in a solid, liquid, or gaseous state.</p><p id="p-1262" num="1256">As used herein, the term &#x201c;analyte&#x201d; is intended to mean a point or area in a pattern that can be distinguished from other points or areas according to relative location. An individual analyte can include one or more molecules of a particular type. For example, an analyte can include a single target nucleic acid molecule having a particular sequence or an analyte can include several nucleic acid molecules having the same sequence (and/or complementary sequence, thereof). Different molecules that are at different analytes of a pattern can be differentiated from each other according to the locations of the analytes in the pattern. Example analytes include without limitation, wells in a substrate, beads (or other particles) in or on a substrate, projections from a substrate, ridges on a substrate, pads of gel material on a substrate, or channels in a substrate.</p><p id="p-1263" num="1257">Any of a variety of target analytes that are to be detected, characterized, or identified can be used in an apparatus, system or method set forth herein. Exemplary analytes include, but are not limited to, nucleic acids (e.g., DNA, RNA or analogs thereof), proteins, polysaccharides, cells, antibodies, epitopes, receptors, ligands, enzymes (e.g. kinases, phosphatases or polymerases), small molecule drug candidates, cells, viruses, organisms, or the like.</p><p id="p-1264" num="1258">The terms &#x201c;analyte&#x201d;, &#x201c;nucleic acid&#x201d;, &#x201c;nucleic acid molecule&#x201d;, and &#x201c;polynucleotide&#x201d; are used interchangeably herein. In various implementations, nucleic acids may be used as templates as provided herein (e.g., a nucleic acid template, or a nucleic acid complement that is complementary to a nucleic acid nucleic acid template) for particular types of nucleic acid analysis, including but not limited to nucleic acid amplification, nucleic acid expression analysis, and/or nucleic acid sequence determination or suitable combinations thereof. Nucleic acids in certain implementations include, for instance, linear polymers of deoxyribonucleotides in 3&#x2032;-5&#x2032; phosphodiester or other linkages, such as deoxyribonucleic acids (DNA), for example, single- and double-stranded DNA, genomic DNA, copy DNA or complementary DNA (cDNA), recombinant DNA, or any form of synthetic or modified DNA. In other implementations, nucleic acids include for instance, linear polymers of ribonucleotides in 3&#x2032;-5&#x2032; phosphodiester or other linkages such as ribonucleic acids (RNA), for example, single- and double-stranded RNA, messenger (mRNA), copy RNA or complementary RNA (cRNA), alternatively spliced mRNA, ribosomal RNA, small nucleolar RNA (snoRNA), microRNAs (miRNA), small interfering RNAs (sRNA), piwi RNAs (piRNA), or any form of synthetic or modified RNA. Nucleic acids used in the compositions and methods of the present invention may vary in length and may be intact or full-length molecules or fragments or smaller parts of larger nucleic acid molecules. In particular implementations, a nucleic acid may have one or more detectable labels, as described elsewhere herein.</p><p id="p-1265" num="1259">The terms &#x201c;analyte&#x201d;, &#x201c;cluster&#x201d;, &#x201c;nucleic acid cluster&#x201d;, &#x201c;nucleic acid colony&#x201d;, and &#x201c;DNA cluster&#x201d; are used interchangeably and refer to a plurality of copies of a nucleic acid template and/or complements thereof attached to a solid support. Typically and in certain preferred implementations, the nucleic acid cluster comprises a plurality of copies of template nucleic acid and/or complements thereof, attached via their 5&#x2032; termini to the solid support. The copies of nucleic acid strands making up the nucleic acid clusters may be in a single or double stranded form. Copies of a nucleic acid template that are present in a cluster can have nucleotides at corresponding positions that differ from each other, for example, due to presence of a label moiety. The corresponding positions can also contain analog structures having different chemical structure but similar Watson-Crick base-pairing properties, such as is the case for uracil and thymine.</p><p id="p-1266" num="1260">Colonies of nucleic acids can also be referred to as &#x201c;nucleic acid clusters&#x201d;. Nucleic acid colonies can optionally be created by cluster amplification or bridge amplification techniques as set forth in further detail elsewhere herein. Multiple repeats of a target sequence can be present in a single nucleic acid molecule, such as a concatamer created using a rolling circle amplification procedure.</p><p id="p-1267" num="1261">The nucleic acid clusters of the invention can have different shapes, sizes and densities depending on the conditions used. For example, clusters can have a shape that is substantially round, multi-sided, donut-shaped or ring-shaped. The diameter of a nucleic acid cluster can be designed to be from about 0.2 &#x3bc;m to about 6 &#x3bc;m, about 0.3 &#x3bc;m to about 4 &#x3bc;m, about 0.4 &#x3bc;m to about 3 &#x3bc;m, about 0.5 &#x3bc;m to about 2 &#x3bc;m, about 0.75 &#x3bc;m to about 1.5 &#x3bc;m, or any intervening diameter. In a particular implementation, the diameter of a nucleic acid cluster is about 0.5 &#x3bc;m, about 1 &#x3bc;m, about 1.5 &#x3bc;m, about 2 &#x3bc;m, about 2.5 &#x3bc;m, about 3 &#x3bc;m, about 4 &#x3bc;m, about 5 &#x3bc;m, or about 6 &#x3bc;m. The diameter of a nucleic acid cluster may be influenced by a number of parameters, including, but not limited to the number of amplification cycles performed in producing the cluster, the length of the nucleic acid template or the density of primers attached to the surface upon which clusters are formed. The density of nucleic acid clusters can be designed to typically be in the range of 0.1/mm<sup>2</sup>, 1/mm<sup>2</sup>, 10/mm<sup>2</sup>, 100/mm<sup>2</sup>, 1,000/mm<sup>2</sup>, 10,000/mm<sup>2 </sup>to 100,000/mm<sup>2</sup>. The present invention further contemplates, in part, higher density nucleic acid clusters, for example, 100,000/mm<sup>2 </sup>to 1,000,000/mm<sup>2 </sup>and 1,000,000/mm<sup>2 </sup>to 10,000,000/mm<sup>2</sup>.</p><p id="p-1268" num="1262">As used herein, an &#x201c;analyte&#x201d; is an area of interest within a specimen or field of view. When used in connection with microarray devices or other molecular analytical devices, an analyte refers to the area occupied by similar or identical molecules. For example, an analyte can be an amplified oligonucleotide or any other group of a polynucleotide or polypeptide with a same or similar sequence. In other implementations, an analyte can be any element or group of elements that occupy a physical area on a specimen. For example, an analyte could be a parcel of land, a body of water or the like. When an analyte is imaged, each analyte will have some area. Thus, in many implementations, an analyte is not merely one pixel.</p><p id="p-1269" num="1263">The distances between analytes can be described in any number of ways. In some implementations, the distances between analytes can be described from the center of one analyte to the center of another analyte. In other implementations, the distances can be described from the edge of one analyte to the edge of another analyte, or between the outer-most identifiable points of each analyte. The edge of an analyte can be described as the theoretical or actual physical boundary on a chip, or some point inside the boundary of the analyte. In other implementations, the distances can be described in relation to a fixed point on the specimen or in the image of the specimen.</p><p id="p-1270" num="1264">Generally several implementations will be described herein with respect to a method of analysis. It will be understood that systems are also provided for carrying out the methods in an automated or semi-automated way. Accordingly, this disclosure provides neural network-based template generation and base calling systems, wherein the systems can include a processor; a storage device; and a program for image analysis, the program including instructions for carrying out one or more of the methods set forth herein. Accordingly, the methods set forth herein can be carried out on a computer, for example, having components set forth herein or otherwise known in the art.</p><p id="p-1271" num="1265">The methods and systems set forth herein are useful for analyzing any of a variety of objects. Particularly useful objects are solid supports or solid-phase surfaces with attached analytes. The methods and systems set forth herein provide advantages when used with objects having a repeating pattern of analytes in an xy plane. An example is a microarray having an attached collection of cells, viruses, nucleic acids, proteins, antibodies, carbohydrates, small molecules (such as drug candidates), biologically active molecules or other analytes of interest.</p><p id="p-1272" num="1266">An increasing number of applications have been developed for arrays with analytes having biological molecules such as nucleic acids and polypeptides. Such microarrays typically include deoxyribonucleic acid (DNA) or ribonucleic acid (RNA) probes. These are specific for nucleotide sequences present in humans and other organisms. In certain applications, for example, individual DNA or RNA probes can be attached at individual analytes of an array. A test sample, such as from a known person or organism, can be exposed to the array, such that target nucleic acids (e.g., gene fragments, mRNA, or amplicons thereof) hybridize to complementary probes at respective analytes in the array. The probes can be labeled in a target specific process (e.g., due to labels present on the target nucleic acids or due to enzymatic labeling of the probes or targets that are present in hybridized form at the analytes). The array can then be examined by scanning specific frequencies of light over the analytes to identify which target nucleic acids are present in the sample.</p><p id="p-1273" num="1267">Biological microarrays may be used for genetic sequencing and similar applications. In general, genetic sequencing comprises determining the order of nucleotides in a length of target nucleic acid, such as a fragment of DNA or RNA. Relatively short sequences are typically sequenced at each analyte, and the resulting sequence information may be used in various bioinformatics methods to logically fit the sequence fragments together so as to reliably determine the sequence of much more extensive lengths of genetic material from which the fragments were derived. Automated, computer-based algorithms for characteristic fragments have been developed, and have been used more recently in genome mapping, identification of genes and their function, and so forth. Microarrays are particularly useful for characterizing genomic content because a large number of variants are present and this supplants the alternative of performing many experiments on individual probes and targets. The microarray is an ideal format for performing such investigations in a practical manner.</p><p id="p-1274" num="1268">Any of a variety of analyte arrays (also referred to as &#x201c;microarrays&#x201d;) known in the art can be used in a method or system set forth herein. A typical array contains analytes, each having an individual probe or a population of probes. In the latter case, the population of probes at each analyte is typically homogenous having a single species of probe. For example, in the case of a nucleic acid array, each analyte can have multiple nucleic acid molecules each having a common sequence. However, in some implementations the populations at each analyte of an array can be heterogeneous. Similarly, protein arrays can have analytes with a single protein or a population of proteins typically, but not always, having the same amino acid sequence. The probes can be attached to the surface of an array for example, via covalent linkage of the probes to the surface or via non-covalent interaction(s) of the probes with the surface. In some implementations, probes, such as nucleic acid molecules, can be attached to a surface via a gel layer as described, for example, in U.S. patent application Ser. No. 13/784,368 and US Pat. App. Pub. No. 2011/0059865 A1, each of which is incorporated herein by reference.</p><p id="p-1275" num="1269">Example arrays include, without limitation, a BeadChip Array available from Illumina, Inc. (San Diego, Calif.) or others such as those where probes are attached to beads that are present on a surface (e.g. beads in wells on a surface) such as those described in U.S. Pat. Nos. 6,266,459; 6,355,431; 6,770,441; 6,859,570; or 7,622,294; or PCT Publication No. WO 00/63437, each of which is incorporated herein by reference. Further examples of commercially available microarrays that can be used include, for example, an Affymetrix&#xae; GeneChip&#xae; microarray or other microarray synthesized in accordance with techniques sometimes referred to as VLSIPS&#x2122; (Very Large Scale Immobilized Polymer Synthesis) technologies. A spotted microarray can also be used in a method or system according to some implementations of the present disclosure. An example spotted microarray is a CodeLink&#x2122; Array available from Amersham Biosciences. Another microarray that is useful is one that is manufactured using inkjet printing methods such as SurePrint&#x2122; Technology available from Agilent Technologies.</p><p id="p-1276" num="1270">Other useful arrays include those that are used in nucleic acid sequencing applications. For example, arrays having amplicons of genomic fragments (often referred to as clusters) are particularly useful such as those described in Bentley et al., Nature 456:53-59 (2008), WO 04/018497; WO 91/06678; WO 07/123744; U.S. Pat. Nos. 7,329,492; 7,211,414; 7,315,019; 7,405,281, or 7,057,026; or US Pat. App. Pub. No. 2008/0108082 A1, each of which is incorporated herein by reference. Another type of array that is useful for nucleic acid sequencing is an array of particles produced from an emulsion PCR technique. Examples are described in Dressman et al., Proc. Natl. Acad. Sci. USA 100:8817-8822 (2003), WO 05/010145, US Pat App. Pub. No. 2005/0130173 or US Pat. App. Pub. No. 2005/0064460, each of which is incorporated herein by reference in its entirety.</p><p id="p-1277" num="1271">Arrays used for nucleic acid sequencing often have random spatial patterns of nucleic acid analytes. For example, HiSeq or MiSeq sequencing platforms available from Illumina Inc. (San Diego, Calif.) utilize flow cells upon which nucleic acid arrays are formed by random seeding followed by bridge amplification. However, patterned arrays can also be used for nucleic acid sequencing or other analytical applications. Example patterned arrays, methods for their manufacture and methods for their use are set forth in U.S. Ser. No. 13/787,396; U.S. Ser. No. 13/783,043; U.S. Ser. No. 13/784,368; US Pat. App. Pub. No. 2013/0116153 A1; and US Pat. App. Pub. No. 2012/0316086 A1, each of which is incorporated herein by reference. The analytes of such patterned arrays can be used to capture a single nucleic acid template molecule to seed subsequent formation of a homogenous colony, for example, via bridge amplification. Such patterned arrays are particularly useful for nucleic acid sequencing applications.</p><p id="p-1278" num="1272">The size of an analyte on an array (or other object used in a method or system herein) can be selected to suit a particular application. For example, in some implementations, an analyte of an array can have a size that accommodates only a single nucleic acid molecule. A surface having a plurality of analytes in this size range is useful for constructing an array of molecules for detection at single molecule resolution. Analytes in this size range are also useful for use in arrays having analytes that each contain a colony of nucleic acid molecules. Thus, the analytes of an array can each have an area that is no larger than about 1 mm<sup>2</sup>, no larger than about 500 &#x3bc;m<sup>2</sup>, no larger than about 100 &#x3bc;m<sup>2</sup>, no larger than about 10 &#x3bc;m<sup>2</sup>, no larger than about 1 &#x3bc;m<sup>2</sup>, no larger than about 500 nm<sup>2</sup>, or no larger than about 100 nm<sup>2</sup>, no larger than about 10 nm<sup>2</sup>, no larger than about 5 nm<sup>2</sup>, or no larger than about 1 nm<sup>2</sup>. Alternatively or additionally, the analytes of an array will be no smaller than about 1 mm<sup>2</sup>, no smaller than about 500 &#x3bc;m<sup>2</sup>, no smaller than about 100 &#x3bc;m<sup>2</sup>, no smaller than about 10 &#x3bc;m<sup>2</sup>, no smaller than about 1 &#x3bc;m<sup>2</sup>, no smaller than about 500 nm<sup>2</sup>, no smaller than about 100 nm<sup>2</sup>, no smaller than about 10 nm<sup>2</sup>, no smaller than about 5 nm<sup>2</sup>, or no smaller than about 1 nm<sup>2</sup>. Indeed, an analyte can have a size that is in a range between an upper and lower limit selected from those exemplified above. Although several size ranges for analytes of a surface have been exemplified with respect to nucleic acids and on the scale of nucleic acids, it will be understood that analytes in these size ranges can be used for applications that do not include nucleic acids. It will be further understood that the size of the analytes need not necessarily be confined to a scale used for nucleic acid applications.</p><p id="p-1279" num="1273">For implementations that include an object having a plurality of analytes, such as an array of analytes, the analytes can be discrete, being separated with spaces between each other. An array useful in the invention can have analytes that are separated by edge to edge distance of at most 100 &#x3bc;m, 50 &#x3bc;m, 10 &#x3bc;m, 5 &#x3bc;m, 1 &#x3bc;m, 0.5 &#x3bc;m, or less. Alternatively or additionally, an array can have analytes that are separated by an edge to edge distance of at least 0.5 &#x3bc;m, 1 &#x3bc;m, 5 &#x3bc;m, 10 &#x3bc;m, 50 &#x3bc;m, 100 &#x3bc;m, or more. These ranges can apply to the average edge to edge spacing for analytes as well as to the minimum or maximum spacing.</p><p id="p-1280" num="1274">In some implementations the analytes of an array need not be discrete and instead neighboring analytes can abut each other. Whether or not the analytes are discrete, the size of the analytes and/or pitch of the analytes can vary such that arrays can have a desired density. For example, the average analyte pitch in a regular pattern can be at most 100 &#x3bc;m, 50 &#x3bc;m, 10 &#x3bc;m, 5 &#x3bc;m, 1 &#x3bc;m, 0.5 &#x3bc;m, or less. Alternatively or additionally, the average analyte pitch in a regular pattern can be at least 0.5 &#x3bc;m, 1 &#x3bc;m, 5 &#x3bc;m, 10 &#x3bc;m, 50 &#x3bc;m, 100 &#x3bc;m, or more. These ranges can apply to the maximum or minimum pitch for a regular pattern as well. For example, the maximum analyte pitch for a regular pattern can be at most 100 &#x3bc;m, 50 &#x3bc;m, 10 &#x3bc;m, 5 &#x3bc;m, 1 &#x3bc;m, 0.5 &#x3bc;m, or less; and/or the minimum analyte pitch in a regular pattern can be at least 0.5 &#x3bc;m, 1 &#x3bc;m, 5 &#x3bc;m, 10 &#x3bc;m, 50 &#x3bc;m, 100 &#x3bc;m, or more.</p><p id="p-1281" num="1275">The density of analytes in an array can also be understood in terms of the number of analytes present per unit area. For example, the average density of analytes for an array can be at least about 1&#xd7;10<sup>3 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>4 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>5 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>6 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>7 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>8 </sup>analytes/mm<sup>2</sup>, or 1&#xd7;10<sup>9 </sup>analytes/mm<sup>2</sup>, or higher. Alternatively or additionally the average density of analytes for an array can be at most about 1&#xd7;10<sup>9 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>8 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>7 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>6 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>5 </sup>analytes/mm<sup>2</sup>, 1&#xd7;10<sup>4 </sup>analytes/mm<sup>2</sup>, or 1&#xd7;10<sup>3 </sup>analytes/mm<sup>2</sup>, or less.</p><p id="p-1282" num="1276">The above ranges can apply to all or part of a regular pattern including, for example, all or part of an array of analytes.</p><p id="p-1283" num="1277">The analytes in a pattern can have any of a variety of shapes. For example, when observed in a two dimensional plane, such as on the surface of an array, the analytes can appear rounded, circular, oval, rectangular, square, symmetric, asymmetric, triangular, polygonal, or the like. The analytes can be arranged in a regular repeating pattern including, for example, a hexagonal or rectilinear pattern. A pattern can be selected to achieve a desired level of packing. For example, round analytes are optimally packed in a hexagonal arrangement. Of course other packing arrangements can also be used for round analytes and vice versa.</p><p id="p-1284" num="1278">A pattern can be characterized in terms of the number of analytes that are present in a subset that forms the smallest geometric unit of the pattern. The subset can include, for example, at least about 2, 3, 4, 5, 6, 10 or more analytes. Depending upon the size and density of the analytes the geometric unit can occupy an area of less than 1 mm<sup>2</sup>, 500 &#x3bc;m<sup>2</sup>, 100 &#x3bc;m<sup>2</sup>, 50 &#x3bc;m<sup>2</sup>, 10 &#x3bc;m<sup>2</sup>, 1 &#x3bc;m<sup>2</sup>, 500 nm<sup>2</sup>, 100 nm<sup>2</sup>, 50 nm<sup>2</sup>, 10 nm<sup>2</sup>, or less. Alternatively or additionally, the geometric unit can occupy an area of greater than 10 nm<sup>2</sup>, 50 nm<sup>2</sup>, 100 nm<sup>2</sup>, 500 nm<sup>2</sup>, 1 &#x3bc;m<sup>2</sup>, 10 &#x3bc;m<sup>2</sup>, 50 &#x3bc;m<sup>2</sup>, 100 &#x3bc;m<sup>2</sup>, 500 &#x3bc;m<sup>2</sup>, 1 mm<sup>2</sup>, or more. Characteristics of the analytes in a geometric unit, such as shape, size, pitch and the like, can be selected from those set forth herein more generally with regard to analytes in an array or pattern.</p><p id="p-1285" num="1279">An array having a regular pattern of analytes can be ordered with respect to the relative locations of the analytes but random with respect to one or more other characteristic of each analyte. For example, in the case of a nucleic acid array, the nuclei acid analytes can be ordered with respect to their relative locations but random with respect to one's knowledge of the sequence for the nucleic acid species present at any particular analyte. As a more specific example, nucleic acid arrays formed by seeding a repeating pattern of analytes with template nucleic acids and amplifying the template at each analyte to form copies of the template at the analyte (e.g., via cluster amplification or bridge amplification) will have a regular pattern of nucleic acid analytes but will be random with regard to the distribution of sequences of the nucleic acids across the array. Thus, detection of the presence of nucleic acid material generally on the array can yield a repeating pattern of analytes, whereas sequence specific detection can yield non-repeating distribution of signals across the array.</p><p id="p-1286" num="1280">It will be understood that the description herein of patterns, order, randomness and the like pertain not only to analytes on objects, such as analytes on arrays, but also to analytes in images. As such, patterns, order, randomness and the like can be present in any of a variety of formats that are used to store, manipulate or communicate image data including, but not limited to, a computer readable medium or computer component such as a graphical user interface or other output device.</p><p id="p-1287" num="1281">As used herein, the term &#x201c;image&#x201d; is intended to mean a representation of all or part of an object. The representation can be an optically detected reproduction. For example, an image can be obtained from fluorescent, luminescent, scatter, or absorption signals. The part of the object that is present in an image can be the surface or other xy plane of the object Typically, an image is a 2 dimensional representation, but in some cases information in the image can be derived from 3 or more dimensions. An image need not include optically detected signals. Non-optical signals can be present instead. An image can be provided in a computer readable format or medium such as one or more of those set forth elsewhere herein.</p><p id="p-1288" num="1282">As used herein, &#x201c;image&#x201d; refers to a reproduction or representation of at least a portion of a specimen or other object. In some implementations, the reproduction is an optical reproduction, for example, produced by a camera or other optical detector. The reproduction can be a non-optical reproduction, for example, a representation of electrical signals obtained from an array of nanopore analytes or a representation of electrical signals obtained from an ion-sensitive CMOS detector. In particular implementations non-optical reproductions can be excluded from a method or apparatus set forth herein. An image can have a resolution capable of distinguishing analytes of a specimen that are present at any of a variety of spacings including, for example, those that are separated by less than 100 &#x3bc;m, 50 &#x3bc;m, 10 &#x3bc;m, 5 &#x3bc;m, 1 &#x3bc;m or 0.5 &#x3bc;m.</p><p id="p-1289" num="1283">As used herein, &#x201c;acquiring&#x201d;, &#x201c;acquisition&#x201d; and like terms refer to any part of the process of obtaining an image file. In some implementations, data acquisition can include generating an image of a specimen, looking for a signal in a specimen, instructing a detection device to look for or generate an image of a signal, giving instructions for further analysis or transformation of an image file, and any number of transformations or manipulations of an image file.</p><p id="p-1290" num="1284">As used herein, the term &#x201c;template&#x201d; refers to a representation of the location or relation between signals or analytes. Thus, in some implementations, a template is a physical grid with a representation of signals corresponding to analytes in a specimen. In some implementations, a template can be a chart, table, text file or other computer file indicative of locations corresponding to analytes. In implementations presented herein, a template is generated in order to track the location of analytes of a specimen across a set of images of the specimen captured at different reference points. For example, a template could be a set of x,y coordinates or a set of values that describe the direction and/or distance of one analyte with respect to another analyte.</p><p id="p-1291" num="1285">As used herein, the term &#x201c;specimen&#x201d; can refer to an object or area of an object of which an image is captured. For example, in implementations where images are taken of the surface of the earth, a parcel of land can be a specimen. In other implementations where the analysis of biological molecules is performed in a flow cell, the flow cell may be divided into any number of subdivisions, each of which may be a specimen. For example, a flow cell may be divided into various flow channels or lanes, and each lane can be further divided into 2, 3, 4, 5, 6, 7, 8, 9, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 120, 140, 160, 180, 200, 400, 600, 800, 1000 or more separate regions that are imaged. One example of a flow cell has 8 lanes, with each lane divided into 120 specimens or tiles. In another implementation, a specimen may be made up of a plurality of tiles or even an entire flow cell. Thus, the image of each specimen can represent a region of a larger surface that is imaged.</p><p id="p-1292" num="1286">It will be appreciated that references to ranges and sequential number lists described herein include not only the enumerated number but all real numbers between the enumerated numbers.</p><p id="p-1293" num="1287">As used herein, a &#x201c;reference point&#x201d; refers to any temporal or physical distinction between images. In a preferred implementation, a reference point is a time point. In a more preferred implementation, a reference point is a time point or cycle during a sequencing reaction. However, the term &#x201c;reference point&#x201d; can include other aspects that distinguish or separate images, such as angle, rotational, temporal, or other aspects that can distinguish or separate images.</p><p id="p-1294" num="1288">As used herein, a &#x201c;subset of images&#x201d; refers to a group of images within a set. For example, a subset may contain 1, 2, 3, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30, 40, 50, 60 or any number of images selected from a set of images. In particular implementations, a subset may contain no more than 1, 2, 3, 4, 6, 8, 10, 12, 14, 16, 18, 20, 30, 40, 50, 60 or any number of images selected from a set of images. In a preferred implementation, images are obtained from one or more sequencing cycles with four images correlated to each cycle. Thus, for example, a subset could be a group of 16 images obtained through four cycles.</p><p id="p-1295" num="1289">A base refers to a nucleotide base or nucleotide, A (adenine), C (cytosine), T (thymine), or G (guanine). This application uses &#x201c;base(s)&#x201d; and &#x201c;nucleotide(s)&#x201d; interchangeably.</p><p id="p-1296" num="1290">The term &#x201c;chromosome&#x201d; refers to the heredity-bearing gene carrier of a living cell, which is derived from chromatin strands comprising DNA and protein components (especially histones). The conventional internationally recognized individual human genome chromosome numbering system is employed herein.</p><p id="p-1297" num="1291">The term &#x201c;site&#x201d; refers to a unique position (e.g., chromosome ID, chromosome position and orientation) on a reference genome. In some implementations, a site may be a residue, a sequence tag, or a segment's position on a sequence. The term &#x201c;locus&#x201d; may be used to refer to the specific location of a nucleic acid sequence or polymorphism on a reference chromosome.</p><p id="p-1298" num="1292">The term &#x201c;sample&#x201d; herein refers to a sample, typically derived from a biological fluid, cell, tissue, organ, or organism containing a nucleic acid or a mixture of nucleic acids containing at least one nucleic acid sequence that is to be sequenced and/or phased. Such samples include, but are not limited to sputum/oral fluid, amniotic fluid, blood, a blood fraction, fine needle biopsy samples (e.g., surgical biopsy, fine needle biopsy, etc.), urine, peritoneal fluid, pleural fluid, tissue explant, organ culture and any other tissue or cell preparation, or fraction or derivative thereof or isolated therefrom. Although the sample is often taken from a human subject (e.g., patient), samples can be taken from any organism having chromosomes, including, but not limited to dogs, cats, horses, goats, sheep, cattle, pigs, etc. The sample may be used directly as obtained from the biological source or following a pretreatment to modify the character of the sample. For example, such pretreatment may include preparing plasma from blood, diluting viscous fluids and so forth. Methods of pretreatment may also involve, but are not limited to, filtration, precipitation, dilution, distillation, mixing, centrifugation, freezing, lyophilization, concentration, amplification, nucleic acid fragmentation, inactivation of interfering components, the addition of reagents, lysing, etc.</p><p id="p-1299" num="1293">The term &#x201c;sequence&#x201d; includes or represents a strand of nucleotides coupled to each other. The nucleotides may be based on DNA or RNA. It should be understood that one sequence may include multiple sub-sequences. For example, a single sequence (e.g., of a PCR amplicon) may have 350 nucleotides. The sample read may include multiple sub-sequences within these 350 nucleotides. For instance, the sample read may include first and second flanking subsequences having, for example, 20-50 nucleotides. The first and second flanking sub-sequences may be located on either side of a repetitive segment having a corresponding sub-sequence (e.g., 40-100 nucleotides). Each of the flanking sub-sequences may include (or include portions of) a primer sub-sequence (e.g., 10-30 nucleotides). For ease of reading, the term &#x201c;sub-sequence&#x201d; will be referred to as &#x201c;sequence,&#x201d; but it is understood that two sequences are not necessarily separate from each other on a common strand. To differentiate the various sequences described herein, the sequences may be given different labels (e.g., target sequence, primer sequence, flanking sequence, reference sequence, and the like). Other terms, such as &#x201c;allele,&#x201d; may be given different labels to differentiate between like objects. The application uses &#x201c;read(s)&#x201d; and &#x201c;sequence read(s)&#x201d; interchangeably.</p><p id="p-1300" num="1294">The term &#x201c;paired-end sequencing&#x201d; refers to sequencing methods that sequence both ends of a target fragment. Paired-end sequencing may facilitate detection of genomic rearrangements and repetitive segments, as well as gene fusions and novel transcripts. Methodology for paired-end sequencing are described in PCT publication WO07010252, PCT application Serial No. PCTGB2007/003798 and US patent application publication US 2009/0088327, each of which is incorporated by reference herein. In one example, a series of operations may be performed as follows; (a) generate clusters of nucleic acids; (b) linearize the nucleic acids; (c) hybridize a first sequencing primer and carry out repeated cycles of extension, scanning and deblocking, as set forth above; (d) &#x201c;invert&#x201d; the target nucleic acids on the flow cell surface by synthesizing a complimentary copy; (e) linearize the resynthesized strand; and (f) hybridize a second sequencing primer and carry out repeated cycles of extension, scanning and deblocking, as set forth above. The inversion operation can be carried out be delivering reagents as set forth above for a single cycle of bridge amplification.</p><p id="p-1301" num="1295">The term &#x201c;reference genome&#x201d; or &#x201c;reference sequence&#x201d; refers to any particular known genome sequence, whether partial or complete, of any organism which may be used to reference identified sequences from a subject. For example, a reference genome used for human subjects as well as many other organisms is found at the National Center for Biotechnology Information at ncbi.nlm.nih.gov. A &#x201c;genome&#x201d; refers to the complete genetic information of an organism or virus, expressed in nucleic acid sequences. A genome includes both the genes and the noncoding sequences of the DNA. The reference sequence may be larger than the reads that are aligned to it. For example, it may be at least about 100 times larger, or at least about 1000 times larger, or at least about 10,000 times larger, or at least about 105 times larger, or at least about 106 times larger, or at least about 107 times larger. In one example, the reference genome sequence is that of a full length human genome. In another example, the reference genome sequence is limited to a specific human chromosome such as chromosome 13. In some implementations, a reference chromosome is a chromosome sequence from human genome version hg19. Such sequences may be referred to as chromosome reference sequences, although the term reference genome is intended to cover such sequences. Other examples of reference sequences include genomes of other species, as well as chromosomes, sub-chromosomal regions (such as strands), etc., of any species. In various implementations, the reference genome is a consensus sequence or other combination derived from multiple individuals. However, in certain applications, the reference sequence may be taken from a particular individual. In other implementations, the &#x201c;genome&#x201d; also covers so-called &#x201c;graph genomes&#x201d;, which use a particular storage format and representation of the genome sequence. In one implementation, graph genomes store data in a linear file. In another implementation, the graph genomes refer to a representation where alternative sequences (e.g., different copies of a chromosome with small differences) are stored as different paths in a graph. Additional information regarding graph genome implementations can be found in https://www.biorxiv.org/content/biorxiv/early/2018/03/20/194530.full.pdf, the content of which is hereby incorporated herein by reference in its entirety.</p><p id="p-1302" num="1296">The term &#x201c;read&#x201d; refer to a collection of sequence data that describes a fragment of a nucleotide sample or reference. The term &#x201c;read&#x201d; may refer to a sample read and/or a reference read. Typically, though not necessarily, a read represents a short sequence of contiguous base pairs in the sample or reference. The read may be represented symbolically by the base pair sequence (in ATCG) of the sample or reference fragment. It may be stored in a memory device and processed as appropriate to determine whether the read matches a reference sequence or meets other criteria. A read may be obtained directly from a sequencing apparatus or indirectly from stored sequence information concerning the sample. In some cases, a read is a DNA sequence of sufficient length (e.g., at least about 25 bp) that can be used to identify a larger sequence or region, e.g., that can be aligned and specifically assigned to a chromosome or genomic region or gene.</p><p id="p-1303" num="1297">Next-generation sequencing methods include, for example, sequencing by synthesis technology (Illumina), pyrosequencing (454), ion semiconductor technology (Ion Torrent sequencing), single-molecule real-time sequencing (Pacific Biosciences) and sequencing by ligation (SOLiD sequencing). Depending on the sequencing methods, the length of each read may vary from about 30 bp to more than 10,000 bp. For example, the DNA sequencing method using SOLiD sequencer generates nucleic acid reads of about 50 bp. For another example, Ion Torrent Sequencing generates nucleic acid reads of up to 400 bp and 454 pyrosequencing generates nucleic acid reads of about 700 bp. For yet another example, single-molecule real-time sequencing methods may generate reads of 10,000 bp to 15,000 bp. Therefore, in certain implementations, the nucleic acid sequence reads have a length of 30-100 bp, 50-200 bp, or 50-400 bp.</p><p id="p-1304" num="1298">The terms &#x201c;sample read&#x201d;, &#x201c;sample sequence&#x201d; or &#x201c;sample fragment&#x201d; refer to sequence data for a genomic sequence of interest from a sample. For example, the sample read comprises sequence data from a PCR amplicon having a forward and reverse primer sequence. The sequence data can be obtained from any select sequence methodology. The sample read can be, for example, from a sequencing-by-synthesis (SBS) reaction, a sequencing-by-ligation reaction, or any other suitable sequencing methodology for which it is desired to determine the length and/or identity of a repetitive element. The sample read can be a consensus (e.g., averaged or weighted) sequence derived from multiple sample reads. In certain implementations, providing a reference sequence comprises identifying a locus-of-interest based upon the primer sequence of the PCR amplicon.</p><p id="p-1305" num="1299">The term &#x201c;raw fragment&#x201d; refers to sequence data for a portion of a genomic sequence of interest that at least partially overlaps a designated position or secondary position of interest within a sample read or sample fragment. Non-limiting examples of raw fragments include a duplex stitched fragment, a simplex stitched fragment, a duplex un-stitched fragment and a simplex un-stitched fragment. The term &#x201c;raw&#x201d; is used to indicate that the raw fragment includes sequence data having some relation to the sequence data in a sample read, regardless of whether the raw fragment exhibits a supporting variant that corresponds to and authenticates or confirms a potential variant in a sample read. The term &#x201c;raw fragment&#x201d; does not indicate that the fragment necessarily includes a supporting variant that validates a variant call in a sample read. For example, when a sample read is determined by a variant call application to exhibit a first variant, the variant call application may determine that one or more raw fragments lack a corresponding type of &#x201c;supporting&#x201d; variant that may otherwise be expected to occur given the variant in the sample read.</p><p id="p-1306" num="1300">The terms &#x201c;mapping&#x201d;, &#x201c;aligned,&#x201d; &#x201c;alignment,&#x201d; or &#x201c;aligning&#x201d; refer to the process of comparing a read or tag to a reference sequence and thereby determining whether the reference sequence contains the read sequence. If the reference sequence contains the read, the read may be mapped to the reference sequence or, in certain implementations, to a particular location in the reference sequence. In some cases, alignment simply tells whether or not a read is a member of a particular reference sequence (i.e., whether the read is present or absent in the reference sequence). For example, the alignment of a read to the reference sequence for human chromosome 13 will tell whether the read is present in the reference sequence for chromosome 13. A tool that provides this information may be called a set membership tester. In some cases, an alignment additionally indicates a location in the reference sequence where the read or tag maps to. For example, if the reference sequence is the whole human genome sequence, an alignment may indicate that a read is present on chromosome 13, and may further indicate that the read is on a particular strand and/or site of chromosome 13.</p><p id="p-1307" num="1301">The term &#x201c;indel&#x201d; refers to the insertion and/or the deletion of bases in the DNA of an organism. A micro-indel represents an indel that results in a net change of 1 to 50 nucleotides. In coding regions of the genome, unless the length of an indel is a multiple of 3, it will produce a frameshift mutation. Indels can be contrasted with point mutations. An indel inserts and deletes nucleotides from a sequence, while a point mutation is a form of substitution that replaces one of the nucleotides without changing the overall number in the DNA. Indels can also be contrasted with a Tandem Base Mutation (TBM), which may be defined as substitution at adjacent nucleotides (primarily substitutions at two adjacent nucleotides, but substitutions at three adjacent nucleotides have been observed.</p><p id="p-1308" num="1302">The term &#x201c;variant&#x201d; refers to a nucleic acid sequence that is different from a nucleic acid reference. Typical nucleic acid sequence variant includes without limitation single nucleotide polymorphism (SNP), short deletion and insertion polymorphisms (Indel), copy number variation (CNV), microsatellite markers or short tandem repeats and structural variation. Somatic variant calling is the effort to identify variants present at low frequency in the DNA sample. Somatic variant calling is of interest in the context of cancer treatment. Cancer is caused by an accumulation of mutations in DNA. A DNA sample from a tumor is generally heterogeneous, including some normal cells, some cells at an early stage of cancer progression (with fewer mutations), and some late-stage cells (with more mutations). Because of this heterogeneity, when sequencing a tumor (e.g., from an FFPE sample), somatic mutations will often appear at a low frequency. For example, a SNV might be seen in only 10% of the reads covering a given base. A variant that is to be classified as somatic or germline by the variant classifier is also referred to herein as the &#x201c;variant under test&#x201d;.</p><p id="p-1309" num="1303">The term &#x201c;noise&#x201d; refers to a mistaken variant call resulting from one or more errors in the sequencing process and/or in the variant call application.</p><p id="p-1310" num="1304">The term &#x201c;variant frequency&#x201d; represents the relative frequency of an allele (variant of a gene) at a particular locus in a population, expressed as a fraction or percentage. For example, the fraction or percentage may be the fraction of all chromosomes in the population that carry that allele. By way of example, sample variant frequency represents the relative frequency of an allele/variant at a particular locus/position along a genomic sequence of interest over a &#x201c;population&#x201d; corresponding to the number of reads and/or samples obtained for the genomic sequence of interest from an individual. As another example, a baseline variant frequency represents the relative frequency of an allele/variant at a particular locus/position along one or more baseline genomic sequences where the &#x201c;population&#x201d; corresponding to the number of reads and/or samples obtained for the one or more baseline genomic sequences from a population of normal individuals.</p><p id="p-1311" num="1305">The term &#x201c;variant allele frequency (VAF)&#x201d; refers to the percentage of sequenced reads observed matching the variant divided by the overall coverage at the target position. VAF is a measure of the proportion of sequenced reads carrying the variant.</p><p id="p-1312" num="1306">The terms &#x201c;position&#x201d;, &#x201c;designated position&#x201d;, and &#x201c;locus&#x201d; refer to a location or coordinate of one or more nucleotides within a sequence of nucleotides. The terms &#x201c;position&#x201d;, &#x201c;designated position&#x201d;, and &#x201c;locus&#x201d; also refer to a location or coordinate of one or more base pairs in a sequence of nucleotides.</p><p id="p-1313" num="1307">The term &#x201c;haplotype&#x201d; refers to a combination of alleles at adjacent sites on a chromosome that are inherited together. A haplotype may be one locus, several loci, or an entire chromosome depending on the number of recombination events that have occurred between a given set of loci, if any occurred.</p><p id="p-1314" num="1308">The term &#x201c;threshold&#x201d; herein refers to a numeric or non-numeric value that is used as a cutoff to characterize a sample, a nucleic acid, or portion thereof (e.g., a read). A threshold may be varied based upon empirical analysis. The threshold may be compared to a measured or calculated value to determine whether the source giving rise to such value suggests should be classified in a particular manner. Threshold values can be identified empirically or analytically. The choice of a threshold is dependent on the level of confidence that the user wishes to have to make the classification. The threshold may be chosen for a particular purpose (e.g., to balance sensitivity and selectivity). As used herein, the term &#x201c;threshold&#x201d; indicates a point at which a course of analysis may be changed and/or a point at which an action may be triggered. A threshold is not required to be a predetermined number. Instead, the threshold may be, for instance, a function that is based on a plurality of factors. The threshold may be adaptive to the circumstances. Moreover, a threshold may indicate an upper limit, a lower limit, or a range between limits.</p><p id="p-1315" num="1309">In some implementations, a metric or score that is based on sequencing data may be compared to the threshold. As used herein, the terms &#x201c;metric&#x201d; or &#x201c;score&#x201d; may include values or results that were determined from the sequencing data or may include functions that are based on the values or results that were determined from the sequencing data. Like a threshold, the metric or score may be adaptive to the circumstances. For instance, the metric or score may be a normalized value. As an example of a score or metric, one or more implementations may use count scores when analyzing the data. A count score may be based on number of sample reads. The sample reads may have undergone one or more filtering stages such that the sample reads have at least one common characteristic or quality. For example, each of the sample reads that are used to determine a count score may have been aligned with a reference sequence or may be assigned as a potential allele. The number of sample reads having a common characteristic may be counted to determine a read count. Count scores may be based on the read count. In some implementations, the count score may be a value that is equal to the read count. In other implementations, the count score may be based on the read count and other information. For example, a count score may be based on the read count for a particular allele of a genetic locus and a total number of reads for the genetic locus. In some implementations, the count score may be based on the read count and previously-obtained data for the genetic locus. In some implementations, the count scores may be normalized scores between predetermined values. The count score may also be a function of read counts from other loci of a sample or a function of read counts from other samples that were concurrently run with the sample-of-interest. For instance, the count score may be a function of the read count of a particular allele and the read counts of other loci in the sample and/or the read counts from other samples. As one example, the read counts from other loci and/or the read counts from other samples may be used to normalize the count score for the particular allele.</p><p id="p-1316" num="1310">The terms &#x201c;coverage&#x201d; or &#x201c;fragment coverage&#x201d; refer to a count or other measure of a number of sample reads for the same fragment of a sequence. A read count may represent a count of the number of reads that cover a corresponding fragment. Alternatively, the coverage may be determined by multiplying the read count by a designated factor that is based on historical knowledge, knowledge of the sample, knowledge of the locus, etc.</p><p id="p-1317" num="1311">The term &#x201c;read depth&#x201d; (conventionally a number followed by &#x201c;x&#x201d;) refers to the number of sequenced reads with overlapping alignment at the target position. This is often expressed as an average or percentage exceeding a cutoff over a set of intervals (such as exons, genes, or panels). For example, a clinical report might say that a panel average coverage is 1,105&#xd7; with 98% of targeted bases covered &#x3e;100&#xd7;.</p><p id="p-1318" num="1312">The terms &#x201c;base call quality score&#x201d; or &#x201c;Q score&#x201d; refer to a PHRED-scaled probability ranging from 0-50 inversely proportional to the probability that a single sequenced base is correct. For example, a T base call with Q of 20 is considered likely correct with a probability of 99.99%. Any base call with Q&#x3c;20 should be considered low quality, and any variant identified where a substantial proportion of sequenced reads supporting the variant are of low quality should be considered potentially false positive.</p><p id="p-1319" num="1313">The terms &#x201c;variant reads&#x201d; or &#x201c;variant read number&#x201d; refer to the number of sequenced reads supporting the presence of the variant.</p><p id="p-1320" num="1314">Regarding &#x201c;strandedness&#x201d; (or DNA strandedness), the genetic message in DNA can be represented as a string of the letters A, G, C, and T. For example, 5&#x2032;-AGGACA-3&#x2032;. Often, the sequence is written in the direction shown here, i.e., with the 5&#x2032; end to the left and the 3&#x2032; end to the right. DNA may sometimes occur as single-stranded molecule (as in certain viruses), but normally we find DNA as a double-stranded unit. It has a double helical structure with two antiparallel strands. In this case, the word &#x201c;antiparallel&#x201d; means that the two strands run in parallel, but have opposite polarity. The double-stranded DNA is held together by pairing between bases and the pairing is always such that adenine (A) pairs with thymine (T) and cytosine (C) pairs with guanine (G). This pairing is referred to as complementarity, and one strand of DNA is said to be the complement of the other. The double-stranded DNA may thus be represented as two strings, like this: 5&#x2032;-AGGACA-3&#x2032; and 3&#x2032;-TCCTGT-5&#x2032;. Note that the two strands have opposite polarity. Accordingly, the strandedness of the two DNA strands can be referred to as the reference strand and its complement, forward and reverse strands, top and bottom strands, sense and antisense strands, or Watson and Crick strands.</p><p id="p-1321" num="1315">The reads alignment (also called reads mapping) is the process of figuring out where in the genome a sequence is from. Once the alignment is performed, the &#x201c;mapping quality&#x201d; or the &#x201c;mapping quality score (MAPQ)&#x201d; of a given read quantifies the probability that its position on the genome is correct. The mapping quality is encoded in the phred scale where P is the probability that the alignment is not correct. The probability is calculated as: P=10<sup>(&#x2212;MAQ/10)</sup>, where MAPQ is the mapping quality. For example, a mapping quality of 40=10 to the power of &#x2212;4, meaning that there is a 0.01% chance that the read was aligned incorrectly. The mapping quality is therefore associated with several alignment factors, such as the base quality of the read, the complexity of the reference genome, and the paired-end information. Regarding the first, if the base quality of the read is low, it means that the observed sequence might be wrong and thus its alignment is wrong. Regarding the second, the mappability refers to the complexity of the genome. Repeated regions are more difficult to map and reads falling in these regions usually get low mapping quality. In this context, the MAPQ reflects the fact that the reads are not uniquely aligned and that their real origin cannot be determined. Regarding the third, in case of paired-end sequencing data, concordant pairs are more likely to be well aligned. The higher is the mapping quality, the better is the alignment. A read aligned with a good mapping quality usually means that the read sequence was good and was aligned with few mismatches in a high mappability region. The MAPQ value can be used as a quality control of the alignment results. The proportion of reads aligned with an MAPQ higher than 20 is usually for downstream analysis.</p><p id="p-1322" num="1316">As used herein, a &#x201c;signal&#x201d; refers to a detectable event such as an emission, preferably light emission, for example, in an image. Thus, in preferred implementations, a signal can represent any detectable light emission that is captured in an image (i.e., a &#x201c;spot&#x201d;). Thus, as used herein, &#x201c;signal&#x201d; can refer to both an actual emission from an analyte of the specimen, and can refer to a spurious emission that does not correlate to an actual analyte. Thus, a signal could arise from noise and could be later discarded as not representative of an actual analyte of a specimen.</p><p id="p-1323" num="1317">As used herein, the term &#x201c;clump&#x201d; refers to a group of signals. In particular implementations, the signals are derived from different analytes. In a preferred implementation, a signal clump is a group of signals that cluster together. In a more preferred implementation, a signal clump represents a physical region covered by one amplified oligonucleotide. Each signal clump should be ideally observed as several signals (one per template cycle, and possibly more due to cross-talk). Accordingly, duplicate signals are detected where two (or more) signals are included in a template from the same clump of signals.</p><p id="p-1324" num="1318">As used herein, terms such as &#x201c;minimum,&#x201d; &#x201c;maximum,&#x201d; &#x201c;minimize,&#x201d; &#x201c;maximize&#x201d; and grammatical variants thereof can include values that are not the absolute maxima or minima. In some implementations, the values include near maximum and near minimum values. In other implementations, the values can include local maximum and/or local minimum values. In some implementations, the values include only absolute maximum or minimum values.</p><p id="p-1325" num="1319">As used herein, &#x201c;cross-talk&#x201d; refers to the detection of signals in one image that are also detected in a separate image. In a preferred implementation, cross-talk can occur when an emitted signal is detected in two separate detection channels. For example, where an emitted signal occurs in one color, the emission spectrum of that signal may overlap with another emitted signal in another color. In a preferred implementation, fluorescent molecules used to indicate the presence of nucleotide bases A, C, G and T are detected in separate channels. However, because the emission spectra of A and C overlap, some of the C color signal may be detected during detection using the A color channel. Accordingly, cross-talk between the A and C signals allows signals from one color image to appear in the other color image. In some implementations, G and T cross-talk. In some implementations, the amount of cross-talk between channels is asymmetric. It will be appreciated that the amount of cross-talk between channels can be controlled by, among other things, the selection of signal molecules having an appropriate emission spectrum as well as selection of the size and wavelength range of the detection channel.</p><p id="p-1326" num="1320">As used herein, &#x201c;register&#x201d;, &#x201c;registering&#x201d;, &#x201c;registration&#x201d; and like terms refer to any process to correlate signals in an image or data set from a first time point or perspective with signals in an image or data set from another time point or perspective. For example, registration can be used to align signals from a set of images to form a template. In another example, registration can be used to align signals from other images to a template. One signal may be directly or indirectly registered to another signal. For example, a signal from image &#x201c;S&#x201d; may be registered to image &#x201c;G&#x201d; directly. As another example, a signal from image &#x201c;N&#x201d; may be directly registered to image &#x201c;G&#x201d;, or alternatively, the signal from image &#x201c;N&#x201d; may be registered to image &#x201c;S&#x201d;, which has previously been registered to image &#x201c;G&#x201d;. Thus, the signal from image &#x201c;N&#x201d; is indirectly registered to image &#x201c;G&#x201d;.</p><p id="p-1327" num="1321">As used herein, the term &#x201c;fiducial&#x201d; is intended to mean a distinguishable point of reference in or on an object. The point of reference can be, for example, a mark, second object, shape, edge, area, irregularity, channel, pit, post or the like. The point of reference can be present in an image of the object or in another data set derived from detecting the object. The point of reference can be specified by an x and/or y coordinate in a plane of the object. Alternatively or additionally, the point of reference can be specified by a z coordinate that is orthogonal to the xy plane, for example, being defined by the relative locations of the object and a detector. One or more coordinates for a point of reference can be specified relative to one or more other analytes of an object or of an image or other data set derived from the object.</p><p id="p-1328" num="1322">As used herein, the term &#x201c;optical signal&#x201d; is intended to include, for example, fluorescent, luminescent, scatter, or absorption signals. Optical signals can be detected in the ultraviolet (UV) range (about 200 to 390 nm), visible (VIS) range (about 391 to 770 nm), infrared (IR) range (about 0.771 to 25 microns), or other range of the electromagnetic spectrum. Optical signals can be detected in a way that excludes all or part of one or more of these ranges.</p><p id="p-1329" num="1323">As used herein, the term &#x201c;signal level&#x201d; is intended to mean an amount or quantity of detected energy or coded information that has a desired or predefined characteristic. For example, an optical signal can be quantified by one or more of intensity, wavelength, energy, frequency, power, luminance or the like. Other signals can be quantified according to characteristics such as voltage, current, electric field strength, magnetic field strength, frequency, power, temperature, etc. Absence of signal is understood to be a signal level of zero or a signal level that is not meaningfully distinguished from noise.</p><p id="p-1330" num="1324">As used herein, the term &#x201c;simulate&#x201d; is intended to mean creating a representation or model of a physical thing or action that predicts characteristics of the thing or action. The representation or model can in many cases be distinguishable from the thing or action. For example, the representation or model can be distinguishable from a thing with respect to one or more characteristic such as color, intensity of signals detected from all or part of the thing, size, or shape. In particular implementations, the representation or model can be idealized, exaggerated, muted, or incomplete when compared to the thing or action. Thus, in some implementations, a representation of model can be distinguishable from the thing or action that it represents, for example, with respect to at least one of the characteristics set forth above. The representation or model can be provided in a computer readable format or medium such as one or more of those set forth elsewhere herein.</p><p id="p-1331" num="1325">As used herein, the term &#x201c;specific signal&#x201d; is intended to mean detected energy or coded information that is selectively observed over other energy or information such as background energy or information. For example, a specific signal can be an optical signal detected at a particular intensity, wavelength or color; an electrical signal detected at a particular frequency, power or field strength; or other signals known in the art pertaining to spectroscopy and analytical detection.</p><p id="p-1332" num="1326">As used herein, the term &#x201c;swath&#x201d; is intended to mean a rectangular portion of an object. The swath can be an elongated strip that is scanned by relative movement between the object and a detector in a direction that is parallel to the longest dimension of the strip. Generally, the width of the rectangular portion or strip will be constant along its full length. Multiple swaths of an object can be parallel to each other. Multiple swaths of an object can be adjacent to each other, overlapping with each other, abutting each other, or separated from each other by an interstitial area.</p><p id="p-1333" num="1327">As used herein, the term &#x201c;variance&#x201d; is intended to mean a difference between that which is expected and that which is observed or a difference between two or more observations. For example, variance can be the discrepancy between an expected value and a measured value. Variance can be represented using statistical functions such as standard deviation, the square of standard deviation, coefficient of variation or the like.</p><p id="p-1334" num="1328">As used herein, the term &#x201c;xy coordinates&#x201d; is intended to mean information that specifies location, size, shape, and/or orientation in an xy plane. The information can be, for example, numerical coordinates in a Cartesian system. The coordinates can be provided relative to one or both of the x and y axes or can be provided relative to another location in the xy plane. For example, coordinates of a analyte of an object can specify the location of the analyte relative to location of a fiducial or other analyte of the object.</p><p id="p-1335" num="1329">As used herein, the term &#x201c;xy plane&#x201d; is intended to mean a 2 dimensional area defined by straight line axes x and y. When used in reference to a detector and an object observed by the detector, the area can be further specified as being orthogonal to the direction of observation between the detector and object being detected.</p><p id="p-1336" num="1330">As used herein, the term &#x201c;z coordinate&#x201d; is intended to mean information that specifies the location of a point, line or area along an axes that is orthogonal to an xy plane. In particular implementations, the z axis is orthogonal to an area of an object that is observed by a detector. For example, the direction of focus for an optical system may be specified along the z axis.</p><p id="p-1337" num="1331">In some implementations, acquired signal data is transformed using an affine transformation. In some such implementations, template generation makes use of the fact that the affine transforms between color channels are consistent between runs. Because of this consistency, a set of default offsets can be used when determining the coordinates of the analytes in a specimen. For example, a default offsets file can contain the relative transformation (shift, scale, skew) for the different channels relative to one channel, such as the A channel. In other implementations, however, the offsets between color channels drift during a run and/or between runs, making offset-driven template generation difficult. In such implementations, the methods and systems provided herein can utilize offset-less template generation, which is described further below.</p><p id="p-1338" num="1332">In some aspects of the above implementations, the system can comprise a flow cell. In some aspects, the flow cell comprises lanes, or other configurations, of tiles, wherein at least some of the tiles comprise one or more arrays of analytes. In some aspects, the analytes comprise a plurality of molecules such as nucleic acids. In certain aspects, the flow cell is configured to deliver a labeled nucleotide base to an array of nucleic acids, thereby extending a primer hybridized to a nucleic acid within a analyte so as to produce a signal corresponding to a analyte comprising the nucleic acid. In preferred implementations, the nucleic acids within a analyte are identical or substantially identical to each other.</p><p id="p-1339" num="1333">In some of the systems for image analysis described herein, each image in the set of images includes color signals, wherein a different color corresponds to a different nucleotide base. In some aspects, each image of the set of images comprises signals having a single color selected from at least four different colors. In some aspects, each image in the set of images comprises signals having a single color selected from four different colors. In some of the systems described herein, nucleic acids can be sequenced by providing four different labeled nucleotide bases to the array of molecules so as to produce four different images, each image comprising signals having a single color, wherein the signal color is different for each of the four different images, thereby producing a cycle of four color images that corresponds to the four possible nucleotides present at a particular position in the nucleic acid. In certain aspects, the system comprises a flow cell that is configured to deliver additional labeled nucleotide bases to the array of molecules, thereby producing a plurality of cycles of color images.</p><p id="p-1340" num="1334">In preferred implementations, the methods provided herein can include determining whether a processor is actively acquiring data or whether the processor is in a low activity state. Acquiring and storing large numbers of high-quality images typically requires massive amounts of storage capacity. Additionally, once acquired and stored, the analysis of image data can become resource intensive and can interfere with processing capacity of other functions, such as ongoing acquisition and storage of additional image data. Accordingly, as used herein, the term low activity state refers to the processing capacity of a processor at a given time. In some implementations, a low activity state occurs when a processor is not acquiring and/or storing data. In some implementations, a low activity state occurs when some data acquisition and/or storage is taking place, but additional processing capacity remains such that image analysis can occur at the same time without interfering with other functions.</p><p id="p-1341" num="1335">As used herein, &#x201c;identifying a conflict&#x201d; refers to identifying a situation where multiple processes compete for resources. In some such implementations, one process is given priority over another process. In some implementations, a conflict may relate to the need to give priority for allocation of time, processing capacity, storage capacity or any other resource for which priority is given. Thus, in some implementations, where processing time or capacity is to be distributed between two processes such as either analyzing a data set and acquiring and/or storing the data set, a conflict between the two processes exists and can be resolved by giving priority to one of the processes.</p><p id="p-1342" num="1336">Also provided herein are systems for performing image analysis. The systems can include a processor; a storage capacity; and a program for image analysis, the program comprising instructions for processing a first data set for storage and the second data set for analysis, wherein the processing comprises acquiring and/or storing the first data set on the storage device and analyzing the second data set when the processor is not acquiring the first data set. In certain aspects, the program includes instructions for identifying at least one instance of a conflict between acquiring and/or storing the first data set and analyzing the second data set; and resolving the conflict in favor of acquiring and/or storing image data such that acquiring and/or storing the first data set is given priority. In certain aspects, the first data set comprises image files obtained from an optical imaging device. In certain aspects, the system further comprises an optical imaging device. In some aspects, the optical imaging device comprises a light source and a detection device.</p><p id="p-1343" num="1337">As used herein, the term &#x201c;program&#x201d; refers to instructions or commands to perform a task or process. The term &#x201c;program&#x201d; can be used interchangeably with the term module. In certain implementations, a program can be a compilation of various instructions executed under the same set of commands. In other implementations, a program can refer to a discrete batch or file.</p><p id="p-1344" num="1338">Set forth below are some of the surprising effects of utilizing the methods and systems for performing image analysis set forth herein. In some sequencing implementations, an important measure of a sequencing system's utility is its overall efficiency. For example, the amount of mappable data produced per day and the total cost of installing and running the instrument are important aspects of an economical sequencing solution. To reduce the time to generate mappable data and to increase the efficiency of the system, real-time base calling can be enabled on an instrument computer and can run in parallel with sequencing chemistry and imaging. This allows much of the data processing and analysis to be completed before the sequencing chemistry finishes. Additionally, it can reduce the storage required for intermediate data and limit the amount of data that needs to travel across the network.</p><p id="p-1345" num="1339">While sequence output has increased, the data per run transferred from the systems provided herein to the network and to secondary analysis processing hardware has substantially decreased. By transforming data on the instrument computer (acquiring computer), network loads are dramatically reduced. Without these on-instrument, off-network data reduction techniques, the image output of a fleet of DNA sequencing instruments would cripple most networks.</p><p id="p-1346" num="1340">The widespread adoption of the high-throughput DNA sequencing instruments has been driven in part by ease of use, support for a range of applications, and suitability for virtually any lab environment. The highly efficient algorithms presented herein allow significant analysis functionality to be added to a simple workstation that can control sequencing instruments. This reduction in the requirements for computational hardware has several practical benefits that will become even more important as sequencing output levels continue to increase. For example, by performing image analysis and base calling on a simple tower, heat production, laboratory footprint, and power consumption are kept to a minimum. In contrast, other commercial sequencing technologies have recently ramped up their computing infrastructure for primary analysis, with up to five times more processing power, leading to commensurate increases in heat output and power consumption. Thus, in some implementations, the computational efficiency of the methods and systems provided herein enables customers to increase their sequencing throughput while keeping server hardware expenses to a minimum.</p><p id="p-1347" num="1341">Accordingly, in some implementations, the methods and/or systems presented herein act as a state machine, keeping track of the individual state of each specimen, and when it detects that a specimen is ready to advance to the next state, it does the appropriate processing and advances the specimen to that state. A more detailed example of how the state machine monitors a file system to determine when a specimen is ready to advance to the next state according to a preferred implementation is set forth in Example 1 below.</p><p id="p-1348" num="1342">In preferred implementations, the methods and systems provided herein are multi-threaded and can work with a configurable number of threads. Thus, for example in the context of nucleic acid sequencing, the methods and systems provided herein are capable of working in the background during a live sequencing run for real-time analysis, or it can be run using a pre-existing set of image data for off-line analysis. In certain preferred implementations, the methods and systems handle multi-threading by giving each thread its own subset of specimen for which it is responsible. This minimizes the possibility of thread contention.</p><p id="p-1349" num="1343">A method of the present disclosure can include a step of obtaining a target image of an object using a detection apparatus, wherein the image includes a repeating pattern of analytes on the object. Detection apparatus that are capable of high resolution imaging of surfaces are particularly useful. In particular implementations, the detection apparatus will have sufficient resolution to distinguish analytes at the densities, pitches, and/or analyte sizes set forth herein. Particularly useful are detection apparatus capable of obtaining images or image data from surfaces. Example detectors are those that are configured to maintain an object and detector in a static relationship while obtaining an area image. Scanning apparatus can also be used. For example, an apparatus that obtains sequential area images (e.g., so called &#x2018;step and shoot&#x2019; detectors) can be used. Also useful are devices that continually scan a point or line over the surface of an object to accumulate data to construct an image of the surface. Point scanning detectors can be configured to scan a point (i.e., a small detection area) over the surface of an object via a raster motion in the x-y plane of the surface. Line scanning detectors can be configured to scan a line along they dimension of the surface of an object, the longest dimension of the line occurring along the x dimension. It will be understood that the detection device, object or both can be moved to achieve scanning detection. Detection apparatus that are particularly useful, for example in nucleic acid sequencing applications, are described in US Pat App. Pub. Nos. 2012/0270305 A1; 2013/0023422 A1; and 2013/0260372 A1; and U.S. Pat. Nos. 5,528,050; 5,719,391; 8,158,926 and 8,241,573, each of which is incorporated herein by reference.</p><p id="p-1350" num="1344">The implementations disclosed herein may be implemented as a method, apparatus, system or article of manufacture using programming or engineering techniques to produce software, firmware, hardware, or any combination thereof. The term &#x201c;article of manufacture&#x201d; as used herein refers to code or logic implemented in hardware or computer readable media such as optical storage devices, and volatile or non-volatile memory devices. Such hardware may include, but is not limited to, field programmable gate arrays (FPGAs), coarse grained reconfigurable architectures (CGRAs), application-specific integrated circuits (ASICs), complex programmable logic devices (CPLDs), programmable logic arrays (PLAs), microprocessors, or other similar processing devices. In particular implementations, information or algorithms set forth herein are present in non-transient storage media.</p><p id="p-1351" num="1345">In particular implementations, a computer implemented method set forth herein can occur in real time while multiple images of an object are being obtained. Such real time analysis is particularly useful for nucleic acid sequencing applications wherein an array of nucleic acids is subjected to repeated cycles of fluidic and detection steps. Analysis of the sequencing data can often be computationally intensive such that it can be beneficial to perform the methods set forth herein in real time or in the background while other data acquisition or analysis algorithms are in process. Example real time analysis methods that can be used with the present methods are those used for the MiSeq and HiSeq sequencing devices commercially available from Illumina, Inc. (San Diego, Calif.) and/or described in US Pat. App. Pub. No. 2012/0020537 A1, which is incorporated herein by reference.</p><p id="p-1352" num="1346">An example data analysis system, formed by one or more programmed computers, with programming being stored on one or more machine readable media with code executed to carry out one or more steps of methods described herein. In one implementation, for example, the system includes an interface designed to permit networking of the system to one or more detection systems (e.g., optical imaging systems) that are configured to acquire data from target objects. The interface may receive and condition data, where appropriate. In particular implementations the detection system will output digital image data, for example, image data that is representative of individual picture elements or pixels that, together, form an image of an array or other object. A processor processes the received detection data in accordance with a one or more routines defined by processing code. The processing code may be stored in various types of memory circuitry.</p><p id="p-1353" num="1347">In accordance with the presently contemplated implementations, the processing code executed on the detection data includes a data analysis routine designed to analyze the detection data to determine the locations and metadata of individual analytes visible or encoded in the data, as well as locations at which no analyte is detected (i.e., where there is no analyte, or where no meaningful signal was detected from an existing analyte). In particular implementations, analyte locations in an array will typically appear brighter than non-analyte locations due to the presence of fluorescing dyes attached to the imaged analytes. It will be understood that the analytes need not appear brighter than their surrounding area, for example, when a target for the probe at the analyte is not present in an array being detected. The color at which individual analytes appear may be a function of the dye employed as well as of the wavelength of the light used by the imaging system for imaging purposes. Analytes to which targets are not bound or that are otherwise devoid of a particular label can be identified according to other characteristics, such as their expected location in the microarray.</p><p id="p-1354" num="1348">Once the data analysis routine has located individual analytes in the data, a value assignment may be carried out In general, the value assignment will assign a digital value to each analyte based upon characteristics of the data represented by detector components (e.g., pixels) at the corresponding location. That is, for example when imaging data is processed, the value assignment routine may be designed to recognize that a specific color or wavelength of light was detected at a specific location, as indicated by a group or cluster of pixels at the location. In a typical DNA imaging application, for example, the four common nucleotides will be represented by four separate and distinguishable colors. Each color, then, may be assigned a value corresponding to that nucleotide.</p><p id="p-1355" num="1349">As used herein, the terms &#x201c;module&#x201d;, &#x201c;system,&#x201d; or &#x201c;system controller&#x201d; may include a hardware and/or software system and circuitry that operates to perform one or more functions. For example, a module, system, or system controller may include a computer processor, controller, or other logic-based device that performs operations based on instructions stored on a tangible and non-transitory computer readable storage medium, such as a computer memory. Alternatively, a module, system, or system controller may include a hard-wired device that performs operations based on hard-wired logic and circuitry. The module, system, or system controller shown in the attached figures may represent the hardware and circuitry that operates based on software or hardwired instructions, the software that directs hardware to perform the operations, or a combination thereof. The module, system, or system controller can include or represent hardware circuits or circuitry that include and/or are connected with one or more processors, such as one or computer microprocessors.</p><p id="p-1356" num="1350">As used herein, the terms &#x201c;software&#x201d; and &#x201c;firmware&#x201d; are interchangeable, and include any computer program stored in memory for execution by a computer, including RAM memory, ROM memory, EPROM memory, EEPROM memory, and non-volatile RAM (NVRAM) memory. The above memory types are examples only, and are thus not limiting as to the types of memory usable for storage of a computer program.</p><p id="p-1357" num="1351">In the molecular biology field, one of the processes for nucleic acid sequencing in use is sequencing-by-synthesis. The technique can be applied to massively parallel sequencing projects. For example, by using an automated platform, it is possible to carry out hundreds of thousands of sequencing reactions simultaneously. Thus, one of the implementations of the present invention relates to instruments and methods for acquiring, storing, and analyzing image data generated during nucleic acid sequencing.</p><p id="p-1358" num="1352">Enormous gains in the amount of data that can be acquired and stored make streamlined image analysis methods even more beneficial. For example, the image analysis methods described herein permit both designers and end users to make efficient use of existing computer hardware. Accordingly, presented herein are methods and systems which reduce the computational burden of processing data in the face of rapidly increasing data output. For example, in the field of DNA sequencing, yields have scaled 15-fold over the course of a recent year, and can now reach hundreds of gigabases in a single run of a DNA sequencing device. If computational infrastructure requirements grew proportionately, large genome-scale experiments would remain out of reach to most researchers. Thus, the generation of more raw sequence data will increase the need for secondary analysis and data storage, making optimization of data transport and storage extremely valuable. Some implementations of the methods and systems presented herein can reduce the time, hardware, networking, and laboratory infrastructure requirements needed to produce usable sequence data.</p><p id="p-1359" num="1353">The present disclosure describes various methods and systems for carrying out the methods. Examples of some of the methods are described as a series of steps. However, it should be understood that implementations are not limited to the particular steps and/or order of steps described herein. Steps may be omitted, steps may be modified, and/or other steps may be added. Moreover, steps described herein may be combined, steps may be performed simultaneously, steps may be performed concurrently, steps may be split into multiple sub-steps, steps may be performed in a different order, or steps (or a series of steps) may be re-performed in an iterative fashion. In addition, although different methods are set forth herein, it should be understood that the different methods (or steps of the different methods) may be combined in other implementations.</p><p id="p-1360" num="1354">In some implementations, a processing unit, processor, module, or computing system that is &#x201c;configured to&#x201d; perform a task or operation may be understood as being particularly structured to perform the task or operation (e.g., having one or more programs or instructions stored thereon or used in conjunction therewith tailored or intended to perform the task or operation, and/or having an arrangement of processing circuitry tailored or intended to perform the task or operation). For the purposes of clarity and the avoidance of doubt, a general purpose computer (which may become &#x201c;configured to&#x201d; perform the task or operation if appropriately programmed) is not &#x201c;configured to&#x201d; perform a task or operation unless or until specifically programmed or structurally modified to perform the task or operation.</p><p id="p-1361" num="1355">Moreover, the operations of the methods described herein can be sufficiently complex such that the operations cannot be mentally performed by an average human being or a person of ordinary skill in the art within a commercially reasonable time period. For example, the methods may rely on relatively complex computations such that such a person cannot complete the methods within a commercially reasonable time.</p><p id="p-1362" num="1356">Throughout this application various publications, patents or patent applications have been referenced. The disclosures of these publications in their entireties are hereby incorporated by reference in this application in order to more fully describe the state of the art to which this invention pertains.</p><p id="p-1363" num="1357">The term &#x201c;comprising&#x201d; is intended herein to be open-ended, including not only the recited elements, but further encompassing any additional elements.</p><p id="p-1364" num="1358">As used herein, the term &#x201c;each&#x201d;, when used in reference to a collection of items, is intended to identify an individual item in the collection but does not necessarily refer to every item in the collection. Exceptions can occur if explicit disclosure or context clearly dictates otherwise.</p><p id="p-1365" num="1359">Although the invention has been described with reference to the examples provided above, it should be understood that various modifications can be made without departing from the invention.</p><p id="p-1366" num="1360">The modules in this application can be implemented in hardware or software, and need not be divided up in precisely the same blocks as shown in the figures. Some can also be implemented on different processors or computers, or spread among a number of different processors or computers. In addition, it will be appreciated that some of the modules can be combined, operated in parallel or in a different sequence than that shown in the figures without affecting the functions achieved. Also as used herein, the term &#x201c;module&#x201d; can include &#x201c;sub-modules&#x201d;, which themselves can be considered herein to constitute modules. The blocks in the figures designated as modules can also be thought of as flowchart steps in a method.</p><p id="p-1367" num="1361">As used herein, the &#x201c;identification&#x201d; of an item of information does not necessarily require the direct specification of that item of information. Information can be &#x201c;identified&#x201d; in a field by simply referring to the actual information through one or more layers of indirection, or by identifying one or more items of different information which are together sufficient to determine the actual item of information. In addition, the term &#x201c;specify&#x201d; is used herein to mean the same as &#x201c;identify&#x201d;.</p><p id="p-1368" num="1362">As used herein, a given signal, event or value is &#x201c;in dependence upon&#x201d; a predecessor signal, event or value of the predecessor signal, event or value influenced by the given signal, event or value. If there is an intervening processing element, step or time period, the given signal, event or value can still be &#x201c;in dependence upon&#x201d; the predecessor signal, event or value. If the intervening processing element or step combines more than one signal, event or value, the signal output of the processing element or step is considered &#x201c;in dependence upon&#x201d; each of the signal, event or value inputs. If the given signal, event or value is the same as the predecessor signal, event or value, this is merely a degenerate case in which the given signal, event or value is still considered to be &#x201c;in dependence upon&#x201d; or &#x201c;dependent on&#x201d; or &#x201c;based on&#x201d; the predecessor signal, event or value. &#x201c;Responsiveness&#x201d; of a given signal, event or value upon another signal, event or value is defined similarly.</p><p id="p-1369" num="1363">As used herein, &#x201c;concurrently&#x201d; or &#x201c;in parallel&#x201d; does not require exact simultaneity. It is sufficient if the evaluation of one of the individuals begins before the evaluation of another of the individuals completes.</p><heading id="h-0024" level="2">Particular Implementations</heading><p id="p-1370" num="1364">We describe various implementations of neural network-based template generation and neural network-based base calling. One or more features of an implementation can be combined with the base implementation. Implementations that are not mutually exclusive are taught to be combinable. One or more features of an implementation can be combined with other implementations. This disclosure periodically reminds the user of these options. Omission from some implementations of recitations that repeat these options should not be taken as limiting the combinations taught in the preceding sections&#x2014;these recitations are hereby incorporated forward by reference into each of the following implementations.</p><p id="p-1371" num="1365">Subpixel Base Calling</p><p id="p-1372" num="1366">We disclose a computer-implemented method of determining metadata about analytes on a tile of a flow cell. The method includes accessing a series of image sets generated during a sequencing run, each image set in the series generated during a respective sequencing cycle of the sequencing run, each image in the series depicting the analytes and their surrounding background, and each image in the series having a plurality of subpixels. The method includes obtaining, from a base caller, a base call classifying each of the subpixels as one of four bases (A, C, T, and G), thereby producing a base call sequence for each of the subpixels across a plurality of sequencing cycles of the sequencing run. The method includes generating an analyte map that identifies the analytes as disjointed regions of contiguous subpixels which share a substantially matching base call sequence. The method includes determining spatial distribution of analytes, including their shapes and sizes based on the disjointed regions and storing the analyte map in memory for use as ground truth for training a classifier.</p><p id="p-1373" num="1367">The method described in this section and other sections of the technology disclosed can include one or more of the following features and/or features described in connection with additional methods disclosed. In the interest of conciseness, the combinations of features disclosed in this application are not individually enumerated and are not repeated with each base set of features. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1374" num="1368">In one implementation, the method includes identifying as background those subpixels in the analyte map that do not belong to any of the disjointed regions. In one implementation, the method includes obtaining, from the base caller, the base call classifying each of the subpixels as one of five bases (A, C, T, G, and N). In one implementation, the analyte map identifies analyte boundary portions between two contiguous subpixels whose base call sequences do not substantially match.</p><p id="p-1375" num="1369">In one implementation, the method includes identifying origin subpixels at preliminary center coordinates of the analytes determined by the base caller, and breadth-first searching for substantially matching base call sequences by beginning with the origin subpixels and continuing with successively contiguous non-origin subpixels. In one implementation, the method includes, on an analyte-by-analyte basis, determining hyperlocated center coordinates of the analytes by calculating centers of mass of the disjointed regions of the analyte map as an average of coordinates of respective contiguous subpixels forming the disjointed regions, and storing the hyperlocated center coordinates of the analytes in the memory on the analyte-by-analyte basis for use as ground truth for training the classifier.</p><p id="p-1376" num="1370">In one implementation, the method includes, on the analyte-by-analyte basis, identifying centers of mass subpixels in the disjointed regions of the analyte map at the hyperlocated center coordinates of the analytes, upsampling the analyte map using interpolation and storing the upsampled analyte map in the memory for use as ground truth for training the classifier, and, in the upsampled analyte map, on the analyte-by-analyte basis, assigning a value to each contiguous subpixel in the disjointed regions based on a decay factor that is proportional to distance of a contiguous subpixel from a center of mass subpixel in a disjointed region to which the contiguous subpixel belongs. In one implementation, the value is a intensity value normalized between zero and one. In one implementation, the method includes, in the upsampled analyte map, assigning a same predetermined value to all the subpixels identified as the background. In one implementation, the predetermined value is a zero intensity value.</p><p id="p-1377" num="1371">In one implementation, the method includes generating a decay map from the upsampled analyte map that expresses the contiguous subpixels in the disjointed regions and the subpixels identified as the background based on their assigned values, and storing the decay map in the memory for use as ground truth for training the classifier. In one implementation, each subpixel in the decay map has a value normalized between zero and one. In one implementation, the method includes, in the upsampled analyte map, categorizing, on the analyte-by-analyte basis, the contiguous subpixels in the disjointed regions as analyte interior subpixels belonging to a same analyte, the centers of mass subpixels as analyte center subpixels, subpixels containing the analyte boundary portions as boundary subpixels, and the subpixels identified as the background as background subpixels, and storing the categorizations in the memory for use as ground truth for training the classifier.</p><p id="p-1378" num="1372">In one implementation, the method includes, storing, on the analyte-by-analyte basis, coordinates of the analyte interior subpixels, the analyte center subpixels, the boundary subpixels, and the background subpixels in the memory for use as ground truth for training the classifier, downscaling the coordinates by a factor used to upsample the analyte map, and, storing, on the analyte-by-analyte basis, the downscaled coordinates in the memory for use as ground truth for training the classifier.</p><p id="p-1379" num="1373">In one implementation, the method includes, in a binary ground truth data generated from the upsampled analyte map, using color coding to label the analyte center subpixels as belonging to an analyte center class and all other subpixels are belonging to a non-center class, and storing the binary ground truth data in the memory for use as ground truth for training the classifier. In one implementation, the method includes, in a ternary ground truth data generated from the upsampled analyte map, using color coding to label the background subpixels as belonging to a background class, the analyte center subpixels as belonging to an analyte center class, and the analyte interior subpixels as belonging to an analyte interior class, and storing the ternary ground truth data in the memory for use as ground truth for training the classifier.</p><p id="p-1380" num="1374">In one implementation, the method includes generating analyte maps for a plurality of tiles of the flow cell, storing the analyte maps in memory and determining spatial distribution of analytes in the tiles based on the analyte maps, including their shapes and sizes, in the upsampled analyte maps of the analytes in the tiles, categorizing, on an analyte-by-analyte basis, subpixels as analyte interior subpixels belonging to a same analyte, analyte center subpixels, boundary subpixels, and background subpixels, storing the categorizations in the memory for use as ground truth for training the classifier, storing, on the analyte-by-analyte basis across the tiles, coordinates of the analyte interior subpixels, the analyte center subpixels, the boundary subpixels, and the background subpixels in the memory for use as ground truth for training the classifier, downscaling the coordinates by the factor used to upsample the analyte map, and, storing, on the analyte-by-analyte basis across the tiles, the downscaled coordinates in the memory for use as ground truth for training the classifier.</p><p id="p-1381" num="1375">In one implementation, the base call sequences are substantially matching when a predetermined portion of base calls match on an ordinal position-wise basis. In one implementation, the base caller produces the base call sequences by interpolating intensity of the subpixels, including at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage. In one implementation, the subpixels are identified to the base caller based on their integer or non-integer coordinates.</p><p id="p-1382" num="1376">In one implementation, the method includes requiring that at least some of the disjointed regions have a predetermined minimum number of subpixels. In one implementation, the flow cell has at least one patterned surface with an array of wells that occupy the analytes. In such an implementation, the method includes, based on the determined shapes and sizes of the analytes, determining which ones of the wells are substantially occupied by at least one analyte, which ones of the wells are minimally occupied, and which ones of the wells are co-occupied by multiple analytes.</p><p id="p-1383" num="1377">In one implementation, the flow cell has at least one nonpatterned surface and the analytes are unevenly scattered over the nonpatterned surface. In one implementation, the density of the analytes ranges from about 100,000 analytes/mm<sup>2 </sup>to about 1,000,000 analytes/mm<sup>2</sup>. In one implementation, the density of the analytes ranges from about 1,000,000 analytes/mm<sup>2 </sup>to about 10,000,000 analytes/mm<sup>2</sup>. In one implementation, the subpixels are quarter subpixels. In another implementation, the subpixels are half subpixels. In one implementation, the preliminary center coordinates of the analytes determined by the base caller are defined in a template image of the tile, and a pixel resolution, an image coordinate system, and measurement scales of the image coordinate system are same for the template image and the images. In one implementation, each image set has four images. In another implementation, each image set has two images. In yet another implementation, each image set has one image. In one implementation, the sequencing run utilizes four-channel chemistry. In another implementation, the sequencing run utilizes two-channel chemistry. In yet another implementation, the sequencing run utilizes one-channel chemistry.</p><p id="p-1384" num="1378">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1385" num="1379">We disclose a computer-implemented method of determining metadata about analytes on a tile of a flow cell. The method includes accessing a set of images of the tile captured during a sequencing run and preliminary center coordinates of the analytes determined by a base caller. The method includes, for each image set, obtaining, from a base caller, a base call classifying, as one of four bases origin subpixels that contain the preliminary center coordinates and a predetermined neighborhood of contiguous subpixels that are successively contiguous to respective ones of the origin subpixels, thereby producing a base call sequence for each of the origin subpixels and for each of the predetermined neighborhood of contiguous subpixels. The method includes generating an analyte map that identifies the analytes as disjointed regions of contiguous subpixels that are successively contiguous to at least some of the respective ones of the origin subpixels and share a substantially matching base call sequence of the one of four bases with the at least some of the respective ones of the origin subpixels. The method includes storing the analyte map in memory and determining the shapes and the sizes of the analytes based on the disjointed regions in the analyte map.</p><p id="p-1386" num="1380">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1387" num="1381">In one implementation, the predetermined neighborhood of contiguous subpixels is a m&#xd7;n subpixel patch centered at pixels containing the origin subpixels and the subpixel patch is 3&#xd7;3 pixels. In one implementation, the predetermined neighborhood of contiguous subpixels is a n-connected subpixel neighborhood centered at pixels containing the origin subpixels. In one implementation, the method includes, identifying as background those subpixels in the analyte map that do not belong to any of the disjointed regions.</p><p id="p-1388" num="1382">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1389" num="1383">Training Data Generation</p><p id="p-1390" num="1384">We disclose a computer-implemented method of generating training data for neural network-based template generation and base calling. The method includes accessing a multitude of images of a flow cell captured over a plurality of cycles of a sequencing run, the flow cell having a plurality of tiles and, in the multitude of images, each of the tiles having a sequence of image sets generated over the plurality of cycles, and each image in the sequence of image sets depicting intensity emissions of analytes and their surrounding background on a particular one of the tiles at a particular one the cycles. The method includes constructing a training set having a plurality of training examples, each training example corresponding to a particular one of the tiles and including image data from at least some image sets in the sequence of image sets of the particular one of the tiles. The method includes generating at least one ground truth data representation for each of the training examples, the ground truth data representation identifying at least one of spatial distribution of analytes and their surrounding background on the particular one of the tiles whose intensity emissions are depicted by the image data, including at least one of analyte shapes, analyte sizes, and/or analyte boundaries, and/or centers of the analytes.</p><p id="p-1391" num="1385">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1392" num="1386">In one implementation, the image data includes images in each of the at least some image sets in the sequence of image sets of the particular one of the tiles, and the images have a resolution of 1800&#xd7;1800. In one implementation, the image data includes at least one image patch from each of the images, and the image patch covers a portion of the particular one of the tiles and has a resolution of 20&#xd7;20. In one implementation, the image data includes an upsampled representation of the image patch, and the upsampled representation has a resolution of 80&#xd7;80. In one implementation, the ground truth data representation has an upsampled resolution of 80&#xd7;80. 10012581 In one implementation, multiple training examples correspond to a same particular one of the tiles and respectively include as image data different image patches from each image in each of at least some image sets in a sequence of image sets of the same particular one of the tiles, and at least some of the different image patches overlap with each other. In one implementation, the ground truth data representation identifies the analytes as disjoint regions of adjoining subpixels, the centers of the analytes as centers of mass subpixels within respective ones of the disjoint regions, and their surrounding background as subpixels that do not belong to any of the disjoint regions. In one implementation, the ground truth data representation uses color coding to identify each subpixel as either being a analyte center or a non-center. In one implementation, the ground truth data representation uses color coding to identify each subpixel as either being analyte interior, analyte center, or surrounding background.</p><p id="p-1393" num="1387">In one implementation, the method includes, storing, in memory, the training examples in the training set and associated ground truth data representations as the training data for the neural network-based template generation and base calling. In one implementation, the method includes generating the training data for a variety of flow cells, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, and analyte densities.</p><p id="p-1394" num="1388">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1395" num="1389">Metadata &#x26; Base Calls Generation</p><p id="p-1396" num="1390">In one implementation, a method includes accessing sequencing images of analytes produced by a sequencer, generating training data from the sequencing images, and using the training data for training a neural network to generate metadata about the analytes. Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1397" num="1391">In one implementation, a method includes accessing sequencing images of analytes produced by a sequencer, generating training data from the sequencing images, and using the training data for training a neural network to base call the analytes. Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1398" num="1392">Regression Model</p><p id="p-1399" num="1393">We disclose a computer-implemented method of identifying analytes on a tile of a flow cell and related analyte metadata. The method includes processing input image data from a sequence of image sets through a neural network and generating an alternative representation of the input image data. Each image in the sequence of image sets covers the tile, and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. The method includes processing the alternative representation through an output layer and generating an output that identifies analytes, whose intensity emissions are depicted by the input image data, as disjoint regions of adjoining subpixels, centers of the analytes as center subpixels at centers of mass of the respective ones of the disjoint regions, and their surrounding background as background subpixels not belonging to any of the disjoint regions.</p><p id="p-1400" num="1394">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1401" num="1395">In one implementation, the adjoining subpixels in the respective ones of the disjoint regions have intensity values weighted according to distance of an adjoining subpixel from a center subpixel in a disjoint region to which the adjoining subpixel belongs. In one implementation, the center subpixels have highest intensity values within the respective ones of the disjoint regions. In one implementation, the background subpixels all have a same lowest intensity value in the output. In one implementation, the output layer normalizes the intensity values between zero and one.</p><p id="p-1402" num="1396">In one implementation, the method includes applying a peak locator to the output to find peak intensities in the output, determining location coordinates of the centers of the analytes based on the peak intensities, downscaling the location coordinates by an upsampling factor used to prepare the input image data, and storing the downscaled location coordinates in memory for use in base calling the analytes. In one implementation, the method includes categorizing the adjoining subpixels in the respective ones of the disjoint regions as analyte interior subpixels belonging to a same analyte, and storing the categorization and downscaled location coordinates of the analyte interior subpixels in the memory on an analyte-by-analyte basis for use in base calling the analytes. In one implementation, the method includes, on the analyte-by-analyte basis, determining distances of the analyte interior subpixels from respective ones of the centers of the analytes, and storing the distances in the memory on the analyte-by-analyte basis for use in base calling the analytes.</p><p id="p-1403" num="1397">In one implementation, the method includes extracting intensities from the analyte interior subpixels in the respective ones of the disjoint regions, including using at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage, and storing the intensities in the memory on the analyte-by-analyte basis for use in base calling the analytes.</p><p id="p-1404" num="1398">In one implementation, the method includes based on the disjoint regions, determining, as part of the related analyte metadata, spatial distribution of the analytes, including at least one of analyte shapes, analyte sizes, and/or analyte boundaries, and storing the related analyte metadata in the memory on the analyte-by-analyte basis for use in base calling the analytes.</p><p id="p-1405" num="1399">In one implementation, the input image data includes images in the sequence of image sets, and the images have a resolution of 3000&#xd7;3000. In one implementation, the input image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the input image data includes an upsampled representation of the image patch from each of the images in the sequence of image sets, and the upsampled representation has a resolution of 80&#xd7;80. In one implementation, the output has an upsampled resolution of 80&#xd7;80.</p><p id="p-1406" num="1400">In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps. In one implementation, the density of the analytes ranges from about 100,000 analytes/mm<sup>2 </sup>to about 1,000,000 analytes/mm<sup>2</sup>. In another implementation, the density of the analytes ranges from about 1,000,000 analytes/mm<sup>2 </sup>to about 10,000,000 analytes/mm<sup>2</sup>.</p><p id="p-1407" num="1401">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1408" num="1402">Training Regression Model</p><p id="p-1409" num="1403">We disclose a computer-implemented method of training a neural network to identify analytes and related analyte metadata. The method includes obtaining training data for training the neural network. The training data includes a plurality of training examples and corresponding ground truth data that should be generated by the neural network by processing the training examples. Each training example includes image data from a sequence of image sets. Each image in the sequence of image sets covers a tile of a flow cell and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. Each ground truth data identifies analytes, whose intensity emissions are depicted by the image data of a corresponding training example, as disjoint regions of adjoining subpixels, centers of the analytes as center subpixels at centers of mass of the respective ones of the disjoint regions, and their surrounding background as background subpixels not belonging to any of the disjoint regions. The method includes using a gradient descent training technique to train the neural network and generating outputs for the training examples that progressively match the ground truth data, including iteratively optimizing a loss function that minimizes error between the outputs and the ground truth data, and updating parameters of the neural network based on the error.</p><p id="p-1410" num="1404">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1411" num="1405">In one implementation, the method includes, upon error convergence after a final iteration, storing the updated parameters of the neural network in memory to be applied to further neural network-based template generation and base calling. In one implementation, in the ground truth data, the adjoining subpixels in the respective ones of the disjoint regions have intensity values weighted according to distance of an adjoining subpixel from a center subpixel in a disjoint region to which the adjoining subpixel belongs. In one implementation, in the ground truth data, the center subpixels have highest intensity values within the respective ones of the disjoint regions. In one implementation, in the ground truth data, the background subpixels all have a same lowest intensity value in the output. In one implementation, in the ground truth data, the intensity values are normalized between zero and one.</p><p id="p-1412" num="1406">In one implementation, the loss function is mean squared error and the error is minimized on a subpixel-basis between the normalized intensity values of corresponding subpixels in the outputs and the ground truth data. In one implementation, the ground truth data identify, as part of the related analyte metadata, spatial distribution of the analytes, including at least one of analyte shapes, analyte sizes, and/or analyte boundaries. In one implementation, the image data includes images in the sequence of image sets, and the images have a resolution of 1800&#xd7;1800. In one implementation, the image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the image data includes an upsampled representation of the image patch from each of the images in the sequence of image sets, and the upsampled representation of the image patch has a resolution of 80&#xd7;80.</p><p id="p-1413" num="1407">In one implementation, in the training data, multiple training examples respectively include as image data different image patches from each image in a sequence of image sets of a same tile, and at least some of the different image patches overlap with each other. In one implementation, the ground truth data has an upsampled resolution of 80&#xd7;80. In one implementation, the training data includes training examples for a plurality of tiles of the flow cell. In one implementation, the training data includes training examples for a variety of flow cells, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, and analyte densities. In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps for subpixel-wise classification by a final classification layer.</p><p id="p-1414" num="1408">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1415" num="1409">Neural Network-Based Template Generator</p><p id="p-1416" num="1410">We disclose a computer-implemented method of determining metadata about analytes on a flow cell. The method includes accessing image data that depicts intensity emissions of the analytes, processing the image data through one or more layers of a neural network and generating an alternative representation of the image data, and processing the alternative representation through an output layer and generating an output that identifies at least one of shapes and sizes of the analytes and/or centers of the analytes.</p><p id="p-1417" num="1411">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1418" num="1412">In one implementation, the image data further depicts intensity emissions of surrounding background of the analytes. In such an implementation, the method includes the output identifying spatial distribution of the analytes on the flow cell, including the surrounding background and boundaries between the analytes. In one implementation, the method includes determining center location coordinates of the analytes on the flow cell based on the output. In one implementation, the neural network is a convolutional neural network. In one implementation, the neural network is a recurrent neural network. In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, followed by the output layer, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps.</p><p id="p-1419" num="1413">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1420" num="1414">Binary Classification Model</p><p id="p-1421" num="1415">We disclose a computer-implemented method of identifying analytes on a tile of a flow cell and related analyte metadata. The method includes processing input image data from a sequence of image sets through a neural network and generating an alternative representation of the image data. In one implementation, each image in the sequence of image sets covers the tile, and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. The method includes processing the alternative representation through a classification layer and generating an output that identifies centers of analytes whose intensity emissions are depicted by the input image data. The output has a plurality of subpixels, and each subpixel in the plurality of subpixels is classified as either an analyte center or a non-center.</p><p id="p-1422" num="1416">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1423" num="1417">In one implementation, the classification layer assigns each subpixel in the output a first likelihood score of being the analyte center, and a second likelihood score of being the non-center. In one implementation, the first and second likelihood scores are determined based on a softmax function and exponentially normalized between zero and one. In one implementation, the first and second likelihood scores are determined based on a sigmoid function and normalized between zero and one. In one implementation, each subpixel in the output is classified as either the analyte center or the non-center based on which one of the first and second likelihood scores is higher than the other. In one implementation, each subpixel in the output is classified as either the analyte center or the non-center based on whether the first and second likelihood scores are above a predetermined threshold likelihood score. In one implementation, the output identifies the centers at centers of mass of respective ones of the analytes. In one implementation, in the output, subpixels classified as analyte centers are assigned a same first predetermined value, and subpixels classified as non-centers are all assigned a same second predetermined value. In one implementation, the first and second predetermined values are intensity values. In one implementation, the first and second predetermined values are continuous values.</p><p id="p-1424" num="1418">In one implementation, the method includes determining location coordinates of subpixels classified as analyte centers, downscaling the location coordinates by an upsampling factor used to prepare the input image data, and storing the downscaled location coordinates in memory for use in base calling the analytes. In one implementation, the input image data includes images in the sequence of image sets, and the images have a resolution of 3000&#xd7;3000. In one implementation, the input image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the input image data includes an upsampled representation of the image patch from each of the images in the sequence of image sets, and the upsampled representation has a resolution of 80&#xd7;80. In one implementation, the output has an upsampled resolution of 80&#xd7;80.</p><p id="p-1425" num="1419">In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, followed by the classification layer, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps for subpixel-wise classification by the classification layer. In one implementation, the density of the analytes ranges from about 100,000 analytes/mm<sup>2 </sup>to about 1,000,000 analytes/mm<sup>2</sup>. In another implementation, the density of the analytes ranges from about 1,000,000 analytes/mm<sup>2 </sup>to about 10,000,000 analytes/mm<sup>2</sup>.</p><p id="p-1426" num="1420">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1427" num="1421">Training Binary Classification Model</p><p id="p-1428" num="1422">We disclose a computer-implemented method of training a neural network to identify analytes and related analyte metadata. The method includes obtaining training data for training the neural network. The training data includes a plurality of training examples and corresponding ground truth data that should be generated by the neural network by processing the training examples. Each training example includes image data from a sequence of image sets. Each image in the sequence of image sets covers a tile of a flow cell and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. Each ground truth data identifies centers of analytes, whose intensity emissions are depicted by the image data of a corresponding training example. The ground truth data has a plurality of subpixels, and each subpixel in the plurality of subpixels is classified as either an analyte center or a non-center. The method includes using a gradient descent training technique to train the neural network and generating outputs for the training examples that progressively match the ground truth data, including iteratively optimizing a loss function that minimizes error between the outputs and the ground truth data, and updating parameters of the neural network based on the error.</p><p id="p-1429" num="1423">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1430" num="1424">In one implementation, the method includes, upon error convergence after a final iteration, storing the updated parameters of the neural network in memory to be applied to further neural network-based template generation and base calling. In one implementation, in the ground truth data, subpixels classified as analyte centers are all assigned a same first predetermined class score, and subpixels classified as non-centers are all assigned a same second predetermined class score. In one implementation, in each output, each subpixel has a first prediction score of being the analyte center, and a second prediction score of being the non-center. In one implementation, the loss function is custom weighted binary cross entropy loss and the error is minimized on a subpixel-basis between the prediction scores and the class scores of corresponding subpixels in the outputs and the ground truth data. In one implementation, the ground truth data identifies the centers at centers of mass of respective ones of the analytes. In one implementation, in the ground truth data, subpixels classified as analyte centers are all assigned a same first predetermined value, and subpixels classified as non-centers are all assigned a same second predetermined value. In one implementation, the first and second predetermined values are intensity values. In another implementation, the first and second predetermined values are continuous values.</p><p id="p-1431" num="1425">In one implementation, the ground truth data identify, as part of the related analyte metadata, spatial distribution of the analytes, including at least one of analyte shapes, analyte sizes, and/or analyte boundaries. In one implementation, the image data includes images in the sequence of image sets, and the images have a resolution of 1800&#xd7;1800. In one implementation, the image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the image data includes an upsampled representation of the image patch from each of the images in the sequence of image sets, and the upsampled representation of the image patch has a resolution of 80&#xd7;80. In one implementation, in the training data, multiple training examples respectively include as image data different image patches from each image in a sequence of image sets of a same tile, and at least some of the different image patches overlap with each other. In one implementation, the ground truth data has an upsampled resolution of 80&#xd7;80. In one implementation, the training data includes training examples for a plurality of tiles of the flow cell. In one implementation, the training data includes training examples for a variety of flow cells, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, and analyte densities. In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, followed by a classification layer, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps for subpixel-wise classification by the classification layer.</p><p id="p-1432" num="1426">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1433" num="1427">Ternary Classification Model</p><p id="p-1434" num="1428">We disclose a computer-implemented method of identifying analytes on a tile of a flow cell and related analyte metadata. The method includes processing input image data from a sequence of image sets through a neural network and generating an alternative representation of the image data. Each image in the sequence of image sets covers the tile, and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. The method includes processing the alternative representation through a classification layer and generating an output that identifies spatial distribution of analytes and their surrounding background whose intensity emissions are depicted by the input image data, including at least one of analyte centers, analyte shapes, analyte sizes, and/or analyte boundaries. The output has a plurality of subpixels, and each subpixel in the plurality of subpixels is classified as either background, analyte center, or analyte interior.</p><p id="p-1435" num="1429">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1436" num="1430">In one implementation, the classification layer assigns each subpixel in the output a first likelihood score of being the background, a second likelihood score of being the analyte center, and a third likelihood score of being the analyte interior. In one implementation, the first, second, and third likelihood scores are determined based on a softmax function and exponentially normalized between zero and one. In one implementation, each subpixel in the output is classified as either the background, the analyte center, or the analyte interior based on which one among the first, second, and third likelihood scores is highest. In one implementation, each subpixel in the output is classified as either the background, the analyte center, or the analyte interior based on whether the first, second, and third likelihood scores are above a predetermined threshold likelihood score. In one implementation, the output identifies the analyte centers at centers of mass of respective ones of the analytes. In one implementation, in the output, subpixels classified as background are all assigned a same first predetermined value, subpixels classified as analyte centers are all assigned a same second predetermined value, and subpixels classified as analyte interior are all assigned a same third predetermined value. In one implementation, the first, second, and third predetermined values are intensity values. In one implementation, the first, second, and third predetermined values are continuous values.</p><p id="p-1437" num="1431">In one implementation, the method includes determining location coordinates of subpixels classified as analyte centers on an analyte-by-analyte basis, downscaling the location coordinates by an upsampling factor used to prepare the input image data, and storing the downscaled location coordinates in memory on the analyte-by-analyte basis for use in base calling the analytes. In one implementation, the method includes determining location coordinates of subpixels classified as analyte interior on the analyte-by-analyte basis, downscaling the location coordinates by an upsampling factor used to prepare the input image data, and storing the downscaled location coordinates in memory on the analyte-by-analyte basis for use in base calling the analytes. In one implementation, the method includes, on the analyte-by-analyte basis, determining distances of the subpixels classified as analyte interior from respective ones of the subpixels classified as analyte centers, and storing the distances in the memory on the analyte-by-analyte basis for use in base calling the analytes. In one implementation, the method includes, on the analyte-by-analyte basis, extracting intensities from the subpixels classified as analyte interior, including using at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage, and storing the intensities in the memory on the analyte-by-analyte basis for use in base calling the analytes.</p><p id="p-1438" num="1432">In one implementation, the input image data includes images in the sequence of image sets, and the images have a resolution of 3000&#xd7;3000. In one implementation, the input image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the input image data includes an upsampled representation of the image patch from each of the images in the sequence of image sets, and the upsampled representation has a resolution of 80&#xd7;80. In one implementation, the output has an upsampled resolution of 80&#xd7;80. In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, followed by the classification layer, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps for subpixel-wise classification by the classification layer. In one implementation, the density of the analytes ranges from about 100,000 analytes/mm<sup>2 </sup>to about 1,000,000 analytes/mm<sup>2</sup>. In another implementation, the density of the analytes ranges from about 1,000,000 analytes/mm<sup>2 </sup>to about 10,000,000 analytes/mm<sup>2</sup>.</p><p id="p-1439" num="1433">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1440" num="1434">Training Ternary Classification Model</p><p id="p-1441" num="1435">We disclose a computer-implemented method of training a neural network to identify analytes and related analyte metadata. The method includes obtaining training data for training the neural network. The training data includes a plurality of training examples and corresponding ground truth data that should be generated by the neural network by processing the training examples. Each training example includes image data from a sequence of image sets. Each image in the sequence of image sets covers a tile of a flow cell and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. Each ground truth data identifies spatial distribution of analytes and their surrounding background whose intensity emissions are depicted by the input image data, including analyte centers, analyte shapes, analyte sizes, and analyte boundaries. The ground truth data has a plurality of subpixels, and each subpixel in the plurality of subpixels is classified as either background, analyte center, or analyte interior. The method includes using a gradient descent training technique to train the neural network and generating outputs for the training examples that progressively match the ground truth data, including iteratively optimizing a loss function that minimizes error between the outputs and the ground truth data, and updating parameters of the neural network based on the error.</p><p id="p-1442" num="1436">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1443" num="1437">In one implementation, the method includes, upon error convergence after a final iteration, storing the updated parameters of the neural network in memory to be applied to further neural network-based template generation and base calling. In one implementation, in the ground truth data, subpixels classified as background are all assigned a same first predetermined class score, subpixels classified as analyte centers are all assigned a same second predetermined class score, and subpixels classified as analyte interior are all assigned a same third predetermined class score.</p><p id="p-1444" num="1438">In one implementation, in each output, each subpixel has a first prediction score of being the background, a second prediction score of being the analyte center, and a third prediction score of being the analyte interior. In one implementation, the loss function is custom weighted ternary cross entropy loss and the error is minimized on a subpixel-basis between the prediction scores and the class scores of corresponding subpixels in the outputs and the ground truth data. In one implementation, the ground truth data identifies the analyte centers at centers of mass of respective ones of the analytes. In one implementation, in the ground truth data, subpixels classified as background are all assigned a same first predetermined value, subpixels classified as analyte centers are all assigned a same second predetermined value, and subpixels classified as analyte interior are all assigned a same third predetermined value. In one implementation, the first, second, and third predetermined values are intensity values. In one implementation, the first, second, and third predetermined values are continuous values. In one implementation, the image data includes images in the sequence of image sets, and the images have a resolution of 1800&#xd7;1800. In one implementation, the image data includes images in the sequence of image sets, and the images have a resolution of 1800&#xd7;1800.</p><p id="p-1445" num="1439">In one implementation, the image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the image data includes an upsampled representation of the image patch from each of the images in the sequence of image sets, and the upsampled representation of the image patch has a resolution of 80&#xd7;80. In one implementation, in the training data, multiple training examples respectively include as image data different image patches from each image in a sequence of image sets of a same tile, and at least some of the different image patches overlap with each other. In one implementation, the ground truth data has an upsampled resolution of 80&#xd7;80. In one implementation, the training data includes training examples for a plurality of tiles of the flow cell. In one implementation, the training data includes training examples for a variety of flow cells, sequencing instruments, sequencing protocols, sequencing chemistries, sequencing reagents, and analyte densities. In one implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, followed by a classification layer, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps for subpixel-wise classification by the classification layer.</p><p id="p-1446" num="1440">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1447" num="1441">Segmentation</p><p id="p-1448" num="1442">We disclose a computer-implemented method of determining analyte metadata. The method includes processing input image data derived from a sequence of image sets through a neural network and generating an alternative representation of the input image data. The input image data has an array of units that depicts analytes and their surrounding background. The method includes processing the alternative representation through an output layer and generating an output value for each unit in the array. The method includes thresholding output values of the units and classifying a first subset of the units as background units depicting the surrounding background. The method includes locating peaks in the output values of the units and classifying a second subset of the units as center units containing centers of the analytes. The method includes applying a segmenter to the output values of the units and determining shapes of the analytes as non-overlapping regions of contiguous units separated by the background units and centered at the center units. The segmenter begins with the center units and determines, for each center unit, a group of successively contiguous units that depict a same analyte whose center is contained in the center unit.</p><p id="p-1449" num="1443">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1450" num="1444">In one implementation, the units are pixels. In another implementation, the units are subpixels. In yet another implementation, the units are superpixels. In one implementation, the output values are continuous values. In another implementation, the output values are softmax scores. In one implementation, the contiguous units in the respective ones of the non-overlapping regions have output values weighted according to distance of a contiguous unit from a center unit in a non-overlapping region to which the contiguous unit belongs. In one implementation, the center units have highest output values within the respective ones of the non-overlapping regions.</p><p id="p-1451" num="1445">In one implementation, the non-overlapping regions have irregular contours and the units are subpixels. In such an implementation, the method includes determining analyte intensity of a given analyte by identifying subpixels that contribute to the analyte intensity of the given analyte based on a corresponding non-overlapping region of contiguous subpixels that identifies a shape of the given analyte, locating the identified subpixels in one or more optical, pixel-resolution images generated for one or more image channels at a current sequencing cycle, in each of the images, interpolating intensities of the identified subpixels, combining the interpolated intensities, and normalizing the combined interpolated intensities to produce a per-image analyte intensity for the given analyte in each of the images, and combining the per-image analyte intensity for each of the images to determine the analyte intensity of the given analyte at the current sequencing cycle. In one implementation, the normalizing is based on a normalization factor, and the normalization factor is a number of the identified subpixels. In one implementation, the method includes base calling the given analyte based on the analyte intensity at the current sequencing cycle.</p><p id="p-1452" num="1446">In one implementation, the non-overlapping regions have irregular contours and the units are subpixels. In such an implementation, the method includes determining analyte intensity of a given analyte by identifying subpixels that contribute to the analyte intensity of the given analyte based on a corresponding non-overlapping region of contiguous subpixels that identifies a shape of the given analyte, locating the identified subpixels in one or more subpixel resolution images upsampled from corresponding optical, pixel-resolution images generated for one or more image channels at a current sequencing cycle, in each of the upsampled images, combining intensities of the identified subpixels and normalizing the combined intensities to produce a per-image analyte intensity for the given analyte in each of the upsampled images, and combining the per-image analyte intensity for each of the upsampled images to determine the analyte intensity of the given analyte at the current sequencing cycle. In one implementation, the normalizing is based on a normalization factor, and the normalization factor is a number of the identified subpixels. In one implementation, the method includes base calling the given analyte based on the analyte intensity at the current sequencing cycle.</p><p id="p-1453" num="1447">In one implementation, each image in the sequence of image sets covers a tile, and depicts intensity emissions of analytes on a tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on a flow cell. In one implementation, the input image data includes at least one image patch from each of the images in the sequence of image sets, and the image patch covers a portion of the tile and has a resolution of 20&#xd7;20. In one implementation, the input image data includes an upsampled, subpixel resolution representation of the image patch from each of the images in the sequence of image sets, and the upsampled, subpixel representation has a resolution of 80&#xd7;80.</p><p id="p-1454" num="1448">In one implementation, the neural network is a convolutional neural network. In another implementation, the neural network is a recurrent neural network. In yet another implementation, the neural network is a residual neural network with residual bocks and residual connections. In yet further implementation, the neural network is a deep fully convolutional segmentation neural network with an encoder subnetwork and a corresponding decoder network, the encoder subnetwork includes a hierarchy of encoders, and the decoder subnetwork includes a hierarchy of decoders that map low resolution encoder feature maps to full input resolution feature maps.</p><p id="p-1455" num="1449">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1456" num="1450">Peak Detection</p><p id="p-1457" num="1451">We disclose a computer-implemented method of determining analyte metadata. The method includes processing input image data derived from a sequence of image sets through a neural network and generating an alternative representation of the input image data. The input image data has an array of units that depicts analytes and their surrounding background. The method includes processing the alternative representation through an output layer and generating an output value for each unit in the array. The method includes thresholding output values of the units and classifying a first subset of the units as background units depicting the surrounding background. The method includes locating peaks in the output values of the units and classifying a second subset of the units as center units containing centers of the analytes.</p><p id="p-1458" num="1452">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1459" num="1453">In one implementation, the method includes applying a segmenter to the output values of the units and determining shapes of the analytes as non-overlapping regions of contiguous units separated by the background units and centered at the center units. The segmenter begins with the center units and determines, for each center unit, a group of successively contiguous units that depict a same analyte whose center is contained in the center unit.</p><p id="p-1460" num="1454">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1461" num="1455">Neural Network-Based Analyte Metadata Generator</p><p id="p-1462" num="1456">In one implementation, a method includes processing image data through a neural network and generating an alternative representation of the image data. The image data depicts intensity emissions of analytes. The method includes processing the alternative representation through an output layer and generating an output that identifies metadata about the analytes, including at least one of spatial distribution of the analytes, shapes of the analytes, centers of the analytes, and/or boundaries between the analytes, i.e. analyte boundary/boundaries. Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1463" num="1457">Units-Based Regression Model</p><p id="p-1464" num="1458">We disclose a computer-implemented method of identifying analytes on a tile of a flow cell and related analyte metadata. The method includes processing input image data from a sequence of image sets through a neural network and generating an alternative representation of the input image data. Each image in the sequence of image sets covers the tile, and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. The method includes processing the alternative representation through an output layer and generating an output that identifies analytes, whose intensity emissions are depicted by the input image data, as disjoint regions of adjoining units, centers of the analytes as center units at centers of mass of the respective ones of the disjoint regions, and their surrounding background as background units not belonging to any of the disjoint regions.</p><p id="p-1465" num="1459">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1466" num="1460">In one implementation, the units are pixels. In another implementation, the units are subpixels. In yet another implementation, the units are superpixels. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1467" num="1461">Units-Based Binary Classification Model</p><p id="p-1468" num="1462">We disclose a computer-implemented method of identifying analytes on a tile of a flow cell and related analyte metadata. The method includes processing input image data from a sequence of image sets through a neural network and generating an alternative representation of the image data. Each image in the sequence of image sets covers the tile, and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. The method includes processing the alternative representation through a classification layer and generating an output that identifies centers of analytes whose intensity emissions are depicted by the input image data. The output has a plurality of units, and each unit in the plurality of units is classified as either an analyte center or a non-center.</p><p id="p-1469" num="1463">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1470" num="1464">In one implementation, the units are pixels. In another implementation, the units are subpixels. In yet another implementation, the units are superpixels. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1471" num="1465">Units-Based Ternary Classification Model</p><p id="p-1472" num="1466">We disclose a computer-implemented method of identifying analytes on a tile of a flow cell and related analyte metadata. The method includes processing input image data from a sequence of image sets through a neural network and generating an alternative representation of the image data. Each image in the sequence of image sets covers the tile, and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell. The method includes processing the alternative representation through a classification layer and generating an output that identifies spatial distribution of analytes and their surrounding background whose intensity emissions are depicted by the input image data, including at least one of analyte centers, analyte shapes, analyte sizes, and/or analyte boundaries. The output has a plurality of units, and each unit in the plurality of units is classified as either background, analyte center, or analyte interior.</p><p id="p-1473" num="1467">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1474" num="1468">In one implementation, the units are pixels. In another implementation, the units are subpixels. In yet another implementation, the units are superpixels. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1475" num="1469">Base Calling&#x2014;Single Analyte Distance Channel</p><p id="p-1476" num="1470">We disclose a neural network-implemented method of base calling analytes synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more image channels. The method includes processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a template generator to identify reference centers of the analytes in a template image. The method includes accessing one or more images in each of a current image set generated at a current one of the plurality of sequencing cycles, of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles. The method includes registering each of the images in the current, preceding, and succeeding image sets with the template image to determine cycle-specific and image channel-specific transformations. The method includes applying the transformations to the reference centers of the analytes to identify transformed centers of the analytes in each of the images. The method includes for a particular one of the analytes being base called, extracting an image patch from each of the images in the current, preceding, succeeding image sets such that each image patch contains in its center pixel a transformed center of the particular one of the analytes identified in a respective one of the images, and depicts intensity emissions of the particular one of the analytes, of some adjacent ones of the analytes, and of their surrounding background in a corresponding one of the image channels. The method includes, for each image patch, generating distance information that identifies distances of its pixels' centers from the transformed center of the particular one of the analytes contained its center pixel. The method includes constructing input data by pixel-wise encoding the distance information into each image patch. The method includes convolving the input data through a convolutional neural network to generate a convolved representation of the input data. The method includes processing the convolved representation through an output layer to produce likelihoods of a base incorporated in the particular one of the analytes at the current one of the plurality of sequencing cycles being A, C, T, and G. The method includes classifying the base as A, C, T, or G based on the likelihoods.</p><p id="p-1477" num="1471">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1478" num="1472">In one implementation, the method includes, for each image patch, generating analyte-attribution information that identifies which of its pixels cover the particular one of the analytes and which of its pixels do not, and constructing the input data by pixel-wise encoding the analyte-attribution information into each image patch. In one implementation, the pixels that cover the particular one of the analytes are assigned a non-zero value in the analyte-attribution information. In one implementation, the pixels that do not cover the particular one of the analytes are assigned a zero value in the analyte-attribution information. In one implementation, the method includes providing as input to the convolutional neural network position coordinates of the transformed centers of the analytes. In one such implementation, the input is fed to a first layer of the convolutional neural network. In another such implementation, the input is fed to one or more intermediate layers of the convolutional neural network. In yet another such implementation, the input is fed to a final layer of the convolutional neural network. In one implementation, the method includes providing as input to the convolutional neural network an intensity scaling channel that has scaling values corresponding to pixels of the image patch. In such an implementation, the scaling values are based on a mean intensity of the center pixel of the image patch containing the center of the particular one of the analytes. In one implementation, the intensity scaling channel pixel-wise includes a same scaling value for all the pixels of the image patch. In one implementation, the mean intensity of the center pixel is determined for each of the corresponding one of the image channels.</p><p id="p-1479" num="1473">In one implementation, the mean intensity of the center pixel is determined for a first image channel by averaging intensity values of the center pixel observed during two or more preceding sequencing cycles that produced an A and a T base call for the particular one of the analytes. In one implementation, the mean intensity of the center pixel is determined for a second image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced an A and a C base call for the particular one of the analytes. In one implementation, the mean intensity of the center pixel is determined for a first image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced an A base call for the particular one of the analytes. In one implementation, the mean intensity of the center pixel is determined for a second image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced a G base call for the particular one of the analytes. In one implementation, the mean intensity of the center pixel is determined for a third image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced a T base call for the particular one of the analytes. In one implementation, the mean intensity of the center pixel is determined for a third image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced a C base call for the particular one of the analytes.</p><p id="p-1480" num="1474">In one implementation, the sequencing run implements paired-end sequencing that sequences both ends of fragments in the analytes in a forward direction and a reverse direction using a first read primer and a second read primer, thereby producing a read pair for each fragment, the read pair having a forward read and a reverse read. In one implementation, the both ends of the fragments are sequenced serially to produce the forward and reverse reads one after the other. In one implementation, the both ends of the fragments are sequenced simultaneously to produce the forward and reverse reads concurrently. In one implementation, the forward and reverse reads each contain one or more of the fragments. In one implementation, the one or more of the fragments are sequenced serially. In one implementation, the one or more of the fragments are sequenced simultaneously. In one implementation, the sequencing run implements single-read sequencing that sequences the fragments in one direction using a single read primer. In one implementation, the sequencing run implements circular sequencing that sequences double stranded copies of the fragments in a loop, and the loop iterates over a double stranded copy of a given fragment multiple times. In one implementation, the sequencing run implements stacked sequencing that sequences stacked copies of the fragments, and the stacked copies of a given fragment are stacked vertically or horizontally. In one implementation, the size of the image patch ranges from 3&#xd7;3 pixels to 10000&#xd7;10000 pixels.</p><p id="p-1481" num="1475">In one implementation, the transformed center is a floating point coordinate value. In such an implementation, the method includes rounding the floating point coordinate value using a rounding operation to produce an integer coordinate value for the transformed center, and identifying the center pixel based on an overlap between its integer coordinates and the integer coordinate value produced for the transformed center. In one implementation, the rounding operation is at least one of floor function, ceil function, and/or round function. In one implementation, the rounding operation is at least one of integer function and/or integer plus sign function. In one implementation, the template generator is a neural network-based template generator. In one implementation, the output layer is a softmax layer, and the likelihoods are exponentially normalized score distribution of the base incorporated in the particular one of the analytes at the current one of the plurality of sequencing cycles being A, C, T, and G.</p><p id="p-1482" num="1476">In one implementation, each one of the image channels is one of a plurality of filter wavelength bands. In another implementation, each one of the image channels is one of a plurality of image events. In one implementation, the flow cell has at least one patterned surface with an array of wells that occupy the analytes. In another implementation, the flow cell has at least one nonpatterned surface and the analytes are unevenly scattered over the nonpatterned surface. In one implementation, the image set has four images. In another implementation, the image set has two images. In yet another implementation, the image set has one image. In one implementation, the sequencing run utilizes four-channel chemistry. In another implementation, the sequencing run utilizes two-channel chemistry. In yet another implementation, the sequencing run utilizes one-channel chemistry.</p><p id="p-1483" num="1477">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1484" num="1478">We disclose a neural network-implemented method of base calling analytes synthesized during a sequencing run. The method includes convolving input data through a convolutional neural network to generate a convolved representation of the input data. The input data includes image patches extracted from one or more images in each of a current image set generated at a current sequencing cycle of the sequencing run, of one or more preceding image sets respectively generated at one or more sequencing cycles of the sequencing run preceding the current sequencing cycle, and of one or more succeeding image sets respectively generated at one or more sequencing cycles of the sequencing run succeeding the current sequencing cycle. Each of the image patches depicts intensity emissions of a target analyte being base called, of some adjacent analytes, and of their surrounding background in a corresponding image channel. The input data further includes distance information which is pixel-wise encoded in each of the image patches to identify distances of an image patch's pixels' centers from a center of the target analyte located in a center pixel of the image patch. The method includes processing the convolved representation through an output layer to produce an output. The method includes base calling the target analyte at the current sequencing cycle based on the output.</p><p id="p-1485" num="1479">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1486" num="1480">In one implementation, the method includes processing the convolved representation through the output layer to produce likelihoods of a base incorporated in the target analyte at the current sequencing cycle being A, C, T, and G, and classifying the base as A, C, T, or G based on the likelihoods. In one implementation, the likelihoods are exponentially normalized scores produced by a softmax layer.</p><p id="p-1487" num="1481">In one implementation, the method includes deriving, from the output, an output pair for the target analyte that identifies a class label of a base incorporated in the target analyte at the current sequencing cycle being A, C, T, or G, and base calling the target analyte based on the class label. In one implementation, a class label of 1, 0 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 1, 1 identifies a T base, and a class label of 0, 0 identifies a G base. In another implementation, a class label of 1, 1 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 0.5, 0.5 identifies a T base, and a class label of 0, 0 identifies a G base. In yet another implementation, a class label of 1, 0 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 0.5, 0.5 identifies a T base, and a class label of 0, 0 identifies a G base. In yet further implementation, a class label of 1, 2 identifies an A base, a class label of 0, 1 identifies a C base, a class label of 1, 1 identifies a T base, and a class label of 0, 0 identifies a G base. In one implementation, the method includes deriving, from the output, a class label for the target analyte that identifies a base incorporated in the target analyte at the current sequencing cycle being A, C, T, or G, and base calling the target analyte based on the class label. In one implementation, a class label of 0.33 identifies an A base, a class label of 0.66 identifies a C base, a class label of 1 identifies a T base, and a class label of 0 identifies a G base. In another implementation, a class label of 0.50 identifies an A base, a class label of 0.75 identifies a C base, a class label of 1 identifies a T base, and a class label of 0.25 identifies a G base. In one implementation, the method includes deriving, from the output, a single output value, comparing the single output value against class value ranges corresponding to bases A, C, T, and G, based on the comparing, assigning the single output value to a particular class value range, and base calling the target analyte based on the assigning. In one implementation, the single output value is derived using a sigmoid function, and the single output value ranges from 0 to 1. In another implementation, a class value range of 0-0.25 represents an A base, a class value range of 0.25-0.50 represents a C base, a class value range of 0.50-0.75 represents a T base, and a class value range of 0.75-1 represents a G base.</p><p id="p-1488" num="1482">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1489" num="1483">We disclose a neural network-implemented method of base calling analytes synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more image channels. The method includes processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a template generator to identify reference centers of the analytes in a template image. The method includes accessing one or more images in each of a current image set generated at a current one of the plurality of sequencing cycles, of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles. The method includes registering each of the images in the current, preceding, and succeeding image sets with the template image to determine cycle-specific and image channel-specific transformations. The method includes applying the transformations to the reference centers of the analytes to identify transformed centers of the analytes in each of the images. The method includes, for a particular one of the analytes being base called, extracting an image patch from each of the images in the current, preceding, succeeding image sets such that each image patch contains in its center pixel a transformed center of the particular one of the analytes identified in a respective one of the images, and depicts intensity emissions of the particular one of the analytes, of some adjacent ones of the analytes, and of their surrounding background in a corresponding one of the image channels. The method includes, for each image patch, generating distance information that identifies distances of its pixels' centers from the transformed center of the particular one of the analytes contained its center pixel. The method includes constructing input data by pixel-wise encoding the distance information into each image patch. The method includes convolving the input data through a convolutional neural network to generate a convolved representation of the input data. The method includes processing the convolved representation through an output layer to produce an output. The method includes base calling the particular one of the analytes at the current one of the plurality of sequencing cycles based on the output.</p><p id="p-1490" num="1484">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1491" num="1485">In one implementation, the method includes processing the convolved representation through the output layer to produce likelihoods of a base incorporated in the particular one of the analytes at the current one of the plurality of sequencing cycles being A, C, T, and G, and classifying the base as A, C, T, or G based on the likelihoods.</p><p id="p-1492" num="1486">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1493" num="1487">In one implementation, computer-implemented method includes processing input data through a neural network and producing an alternative representation of the input data. The input data includes per-cycle image data for each of one or more sequencing cycles of a sequencing run. The per-cycle image data depicts intensity emissions of one or more analytes and their surrounding background captured at a respective sequencing cycle. The method includes processing the alternative representation through an output layer and producing an output. The method includes base calling one or more of the analytes at one or more of the sequencing cycles based on the output.</p><p id="p-1494" num="1488">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1495" num="1489">In one implementation, the method includes accompanying the per-cycle image data with supplemental distance information that identifies distances between pixels of the per-cycle image data and one or more of the analytes. In such an implementation, the distances incorporate context about centers, shapes, and/or boundaries of one or more of the analytes in the processing by the neural network and the output layer. In one implementation, the method includes accompanying the per-cycle image data with supplemental scaling information that assigns scaling values to the pixels of the per-cycle image data. In such an implementation, the scaling values account for variance in intensities of one or more of the analytes.</p><p id="p-1496" num="1490">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1497" num="1491">Base Calling&#x2014;Multi-Analyte Distance Channel</p><p id="p-1498" num="1492">We disclose a neural network-implemented method of base calling analytes synthesized during a sequencing run. The method includes accessing input data that includes a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. Each per-cycle image patch set in the sequence has an image patch for a respective one of one or more image channels. Each image patch has pixel intensity data for pixels that cover a plurality of analytes and their surrounding background, and pixel distance data that identifies each pixel's center-to-center distance from a nearest one of the analytes selected based on center-to-center distances between the pixel and each of the analytes. The method includes convolving the input data through a convolutional neural network to generate a convolved representation of the input data. The method includes processing the convolved representation through an output layer to produce a score distribution for each of the analytes that identifies likelihoods of a base incorporated in a respective one of the analytes at a current sequencing cycle being A, C, T, and G. The method includes base calling each of the analytes based on the likelihoods.</p><p id="p-1499" num="1493">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1500" num="1494">In one implementation, the pixel distance data is pixel-wise encoded into each image patch. In one implementation, the center-to-center distance is derived from a distance formula that uses position coordinates of transformed centers of the analytes and position coordinates of pixel centers. In one implementation, the method includes providing as input to the convolutional neural network intensity scaling channels that have scaling values corresponding to pixels of each image patch, and the scaling values are based on a combination of mean intensities of center pixels in each image patch that contain the transformed centers of the analytes. In one implementation, the intensity scaling channels pixel-wise apply same scaling values to the pixel intensity data of all the pixels of an image patch. In one implementation, the intensity scaling channels pixel-wise apply different scaling values to the pixel intensity data of the pixels of the image patch on a pixel neighborhood basis such that a first scaling value derived from a mean intensity of a first center pixel is applied to a first pixel neighborhood of adjoining pixels that are successively contiguous to the first center pixel, and another scaling value derived from a mean intensity of another center pixel is applied to another pixel neighborhood of adjoining pixels that are successively contiguous to the another center pixel. In one implementation, the pixel neighborhood is a m&#xd7;n pixel patch centered at the center pixels, and the pixel patch is 3&#xd7;3 pixels. In one implementation, the pixel neighborhood is a n-connected pixel neighborhood centered at the center pixels. In one implementation, the mean intensities of the center pixels are determined for each of the corresponding one of the image channels. In one implementation, the mean intensities of the center pixels are determined for a first image channel by averaging intensity values of the center pixels observed during two or more preceding sequencing cycles that produced an A and a T base call for respective ones of the analytes. In one implementation, the mean intensities of the center pixels are determined for a second image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced an A and a C base call for respective ones of the analytes. In one implementation, the mean intensities of the center pixels are determined for a first image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced an A base call for respective ones of the analytes. In one implementation, the mean intensities of the center pixels are determined for a second image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced a G base call for respective ones of the analytes. In one implementation, the mean intensities of the center pixels are determined for a third image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced a T base call for respective ones of the analytes. In one implementation, the mean intensities of the center pixels are determined for a third image channel by averaging intensity values of the center pixel observed during the two or more preceding sequencing cycles that produced a C base call for respective ones of the analytes. In one implementation, the method includes, for each image patch, generating analyte-attribution information that identifies which of its pixels cover the analytes and which of its pixels do not, and constructing the input data by pixel-wise encoding the analyte-attribution information into each image patch. In one implementation, the pixels that cover the analytes are assigned a non-zero value in the analyte-attribution information. In one implementation, the pixels that do not cover the analytes are assigned a zero value in the analyte-attribution information. In one implementation, the size of each image patch ranges from 3&#xd7;3 pixels to 10000&#xd7;10000 pixels. In one implementation, the output layer is a softmax layer, and the score distribution is an exponentially normalized score distribution.</p><p id="p-1501" num="1495">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1502" num="1496">We disclose a neural network-implemented method of base calling analytes synthesized during a sequencing run. The method includes accessing input data that includes a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. Each per-cycle image patch set in the sequence has an image patch for a respective one of one or more image channels. Each image patch has pixel intensity data for pixels that cover a plurality of analytes and their surrounding background, and pixel distance data that identifies each pixel's center-to-center distance from a nearest one of the analytes selected based on center-to-center distances between the pixel and each of the analytes. The method includes convolving the input data through a convolutional neural network to generate a convolved representation of the input data. The method includes processing the convolved representation through an output layer to produce an output. The method includes base calling each of the analytes at a current sequencing cycle based on the output.</p><p id="p-1503" num="1497">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1504" num="1498">In one implementation, the method includes deriving, from the output, a score distribution for each of the analytes that identifies likelihoods of a base incorporated in a respective one of the analytes at the current sequencing cycle being A, C, T, and G, and base calling each of the analytes based on the likelihoods. In one implementation, the output layer is a softmax layer, and the score distribution is an exponentially normalized score distribution. In one implementation, the method includes deriving, from the output, an output pair for each of the analytes that identifies a class label of a base incorporated in a respective one of the analytes at the current sequencing cycle being A, C, T, and G, and base calling each of the analytes based on the class label. In one implementation, the method includes deriving, from the output, a single output value, comparing the single output value against class value ranges corresponding to bases A, C, T, and G, based on the comparing, assigning the single output value to a particular class value range, and base calling each of the analytes based on the assigning. In one implementation, the single output value is derived using a sigmoid function, and the single output value ranges from 0 to 1.</p><p id="p-1505" num="1499">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1506" num="1500">Base Calling&#x2014;Multi-Analyte Shape-Based Distance Channel</p><p id="p-1507" num="1501">We disclose a neural network-implemented method of base calling analytes synthesized during a sequencing run. The method includes accessing input data that includes a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. Each per-cycle image patch set in the sequence has an image patch for a respective one of one or more image channels. Each image patch depicts intensity emissions of a plurality of analytes and their surrounding background using analyte pixels that depict analyte intensities and background pixels that depict background intensities. Each image patch is encoded with analyte distance data that identifies each analyte pixel's center-to-center distance from an assigned one of the analytes selected based on classifying each analyte pixel to only one of the analytes. The method includes convolving the input data through a convolutional neural network to generate a convolved representation of the input data. The method includes processing the convolved representation through an output layer to produce a score distribution for each of the analytes that identifies likelihoods of a base incorporated in a respective one of the analytes at a current sequencing cycle being A, C, T, and G. The method includes base calling each of the analytes based on the likelihoods.</p><p id="p-1508" num="1502">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1509" num="1503">In one implementation, the analytes have irregular shapes that span multiple analyte pixels and pixel-to-analyte classification is based on the irregular shapes. In one implementation, all background pixels are assigned a same minimum center-to-center distance in the analyte distance data. In one implementation, all background pixels are assigned a same minimum intensity. In one implementation, each analyte pixel is classified to only one of the analytes based on a decay map produced by a neural network-based template generator. In such an implementation, the decay map identifies the analytes as disjointed regions of adjoining pixels, centers of the analytes as center pixels at centers of mass of the respective ones of the disjointed regions, and their surrounding background as background pixels not belonging to any of the disjointed regions. In one implementation, the adjoining pixels in the respective ones of the disjointed regions have intensity values weighted according to distance of an adjoining pixel from a center pixel in a disjointed region to which the adjoining pixel belongs. In one implementation, the adjoining pixels in the respective ones of the disjointed regions are categorized as analyte interior pixels belonging to and co-depicting a same analyte and stored in memory on an analyte-by-analyte basis. In one implementation, the center pixels have highest intensity values within the respective ones of the disjointed regions. In one implementation, the background pixels all have a same lowest intensity value in the decay map. In one implementation, the analyte distance data is pixel-wise encoding into each image patch. In one implementation, the center-to-center distance is derived from a distance formula that uses position coordinates of transformed centers of the analytes and position coordinates of pixel centers. In one implementation, the transformed centers of the analytes are derived by applying cycle-specific and image channel-specific transformations to the centers of the analytes identified by the decay map.</p><p id="p-1510" num="1504">In one implementation, the method includes providing as input to the convolutional neural network intensity scaling channels that have scaling values corresponding to pixels of each image patch. In such an implementation, the scaling values are based on a combination of mean intensities of center pixels in each image patch that contain the transformed centers of the analytes. In one implementation, the intensity scaling channels pixel-wise apply different scaling values to the pixel intensity data of the pixels of an image patch on a pixel group basis such that a first scaling value derived from a mean intensity of a first center pixel containing a center of a first analyte is applied to a first pixel group of adjoining pixels that belong to and co-depict the first analyte, and another scaling value derived from a mean intensity of another center pixel containing a center of another analyte is applied to another pixel group of adjoining pixels that belong to and co-depict the another analyte. In one implementation, the mean intensities of the center pixels are determined for each of the corresponding one of the image channels. In one implementation, the method includes, for each image patch, generating analyte-attribution information that identifies which of its pixels cover the analytes and which of its pixels do not, and constructing the input data by pixel-wise encoding the analyte-attribution information into each image patch. In one implementation, the pixels that cover the analytes are assigned a non-zero value in the analyte-attribution information. In another implementation, the pixels that do not cover the analytes are assigned a zero value in the analyte-attribution information.</p><p id="p-1511" num="1505">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1512" num="1506">We disclose a neural network-implemented method of base calling analytes synthesized during a sequencing run. The method includes accessing input data that includes a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. Each per-cycle image patch set in the sequence has an image patch for a respective one of one or more image channels. Each image patch depicts intensity emissions of a plurality of analytes and their surrounding background using analyte pixels that depict analyte intensities and background pixels that depict background intensities. Each image patch is encoded with analyte distance data that identifies each analyte pixel's center-to-center distance from an assigned one of the analytes selected based on classifying each analyte pixel to only one of the analytes. The method includes convolving the input data through a convolutional neural network to generate a convolved representation of the input data. The method includes processing the convolved representation through an output layer to produce an output. The method includes base calling each of the analytes at a current sequencing cycle based on the output.</p><p id="p-1513" num="1507">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1514" num="1508">Specialized Architecture</p><p id="p-1515" num="1509">We disclose a network-implemented method of base calling analytes using sequencing images that have registration error with respect to each other. The method includes accessing a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. The sequence has registration error between image patches across the per-cycle image patch sets and within the per-cycle image patch sets. Each image patch in the sequence depicts intensity information of a target analyte being base called, of some adjacent analytes, and of their surrounding background in a corresponding image channel at a corresponding sequencing cycle in the series. Each image patch in the sequence is pixel-wise encoded with distance information that identifies distances of its pixels' centers from a center of the target analyte located in its center pixel. The method includes separately processing each per-cycle image patch set through a first convolutional subnetwork to produce an intermediate convolved representation for each sequencing cycle, including applying convolutions that combine the intensity and distance information and combine resulting convolved representations only within a sequencing cycle and not between sequencing cycles. The method includes groupwise processing intermediate convolved representations for successive sequencing cycles in the series through a second convolutional subnetwork to produce a final convolved representation for the series, including applying convolutions that combine the intermediate convolved representations and combine resulting convolved representations between the sequencing cycles. The method includes processing the final convolved representation through an output layer to produce an output. The method includes base calling the target analyte at a current sequencing cycle based on the output.</p><p id="p-1516" num="1510">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1517" num="1511">In one implementation, each image patch in the sequence has pixel intensity data for pixels that cover a plurality of analytes and their surrounding background, and pixel distance data that identifies each pixel's center-to-center distance from a nearest one of the analytes selected based on center-to-center distances between the pixel and each of the analytes. In such an implementation, the method includes base calling each of the analytes at the current sequencing cycle based on the output. In one implementation, each image patch in the sequence depicts intensity emissions of a plurality of analytes and their surrounding background using analyte pixels that depict analyte intensities and background pixels that depict background intensities, and is encoded with analyte distance data that identifies each analyte pixel's center-to-center distance from an assigned one of the analytes selected based on classifying each analyte pixel to only one of the analytes. In such an implementation, the method includes base calling each of the analytes at the current sequencing cycle based on the output. In one implementation, the method includes providing as input to the first convolutional subnetwork position coordinates of the target analyte and/or the adjacent analytes. In one implementation, the method includes providing as input to the second convolutional subnetwork position coordinates of the target analyte and/or the adjacent analytes. In one implementation, the method includes providing as input to the output layer position coordinates of the target analyte and/or the adjacent analytes.</p><p id="p-1518" num="1512">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1519" num="1513">We disclose a network-implemented method of base calling analytes using image data with registration error. The method includes accessing input data for a series of sequencing cycles of a sequencing run. The input data has an image tensor for each sequencing cycle. Each image tensor has data for one or more image channels, including, for each image channel, pixel intensity data for pixels covering a target analyte being base called, some adjacent analytes, and surrounding background, and pixel distance data for distances from a center of the target analyte to centers of the pixels. The input data has cross-cycle registration error between pixels across the image tensors and cross-image channel registration error between pixels within the image tensors. The method includes separately processing each input tensor through a spatial convolutional network with a sequence of spatial convolution layers to produce a spatially convolved representation for each sequencing cycle, including beginning with a first spatial convolution layer that combines the pixel intensities and distances only within a sequencing cycle and not between sequencing cycles, and continuing with successive spatial convolution layers that combine outputs of preceding spatial convolution layers only within each sequencing cycle in the series of sequencing cycles and not between the sequencing cycles. The method includes groupwise processing spatially convolved representations for successive sequencing cycles through a temporal convolutional network with a sequence of temporal convolution layers to produce a temporally convolved representation for the series, including beginning with a first temporal convolution layer that combines the spatially convolved representations between the sequencing cycles in the series of sequencing cycles, and continuing with successive temporal convolution layers that combine successive outputs of preceding temporal convolution layers. The method includes processing the temporally convolved representation through an output layer to produce an output. The method includes base calling the target analyte at a current sequencing cycle based on the output.</p><p id="p-1520" num="1514">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1521" num="1515">In one implementation, the groupwise processing further includes convolving over successive intermediate convolved representations within overlapping sliding windows. In one implementation, the successive temporal convolution layers combine the successive outputs within overlapping sliding windows. In one implementation, the pixel distance data is pixel-wise encoding into each image tensor. In one implementation, each image tensor in the sequence has pixel intensity data for pixels that cover a plurality of analytes and their surrounding background, and pixel distance data that identifies each pixel's center-to-center distance from a nearest one of the analytes selected based on center-to-center distances between the pixel and each of the analytes. In one implementation, the method includes base calling each of the analytes at the current sequencing cycle based on the output. In one implementation, each image tensor in the sequence depicts intensity emissions of a plurality of analytes and their surrounding background using analyte pixels that depict analyte intensities and background pixels that depict background intensities, and is encoded with analyte distance data that identifies each analyte pixel's center-to-center distance from an assigned one of the analytes selected based on classifying each analyte pixel to only one of the analytes. In one implementation, the method includes base calling each of the analytes at the current sequencing cycle based on the output. In one implementation, the method includes providing as input to the first convolutional subnetwork position coordinates of the target analyte and/or the adjacent analytes. In one implementation, the method includes providing as input to the second convolutional subnetwork position coordinates of the target analyte and/or the adjacent analytes. In one implementation, the method includes providing as input to the output layer position coordinates of the target analyte and/or the adjacent analytes.</p><p id="p-1522" num="1516">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1523" num="1517">Reframing</p><p id="p-1524" num="1518">We disclose a neural network-implemented method of base calling analytes synthesized during a sequencing run. The method includes accessing a sequence of per-cycle image patch sets generated for a series of sequencing cycles of a sequencing run. Each per-cycle image patch set in the sequence has an image patch for a respective one of one or more image channels. Each image patch has pixel intensity data for pixels covering a target analyte being base called, some adjacent analytes, and surrounding background. The method includes reframing the pixels of each image patch to center a center of the target analyte in a center pixel. The method includes convolving reframed image patches through a convolutional neural network to generate a convolved representation of the reframed image patches. The method includes processing the convolved representation through an output layer to produce an output. The method includes base calling the target analyte at a current sequencing cycle based on the output.</p><p id="p-1525" num="1519">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1526" num="1520">In one implementation, the reframing further includes intensity interpolation of the pixels of each image patch to compensate for the refraining. In one implementation, the intensity interpolation further includes at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage. In one implementation, prior to the reframing, the center of the target analyte is located in the center pixel of each image patch at an offset from a center of the center pixel. In one implementation, the reframing further includes requiring that non-center pixels of each image patch are equidistant from respective centers of the target analyte. In one implementation, each image patch in the sequence has pixel intensity data for pixels that depict a plurality of analytes and their surrounding background, and pixel distance data that identifies each pixel's center-to-center distance from a nearest one of the analytes selected based on center-to-center distances between the pixel and each of the analytes. In one implementation, the method includes base calling each of the analytes at the current sequencing cycle based on the output. In one implementation, each image patch in the sequence depicts intensity emissions of a plurality of analytes and their surrounding background using analyte pixels that depict analyte intensities and background pixels that depict background intensities, and is encoded with analyte distance data that identifies each analyte pixel's center-to-center distance from an assigned one of the analytes selected based on classifying each analyte pixel to only one of the analytes. In one implementation, the method includes base calling each of the analytes at the current sequencing cycle based on the output. In one implementation, the method includes providing as input to the first convolutional subnetwork position coordinates of the target analyte and/or the adjacent analytes. In one implementation, the method includes providing as input to the second convolutional subnetwork position coordinates of the target analyte and/or the adjacent analytes. In one implementation, the method includes providing as input to the output layer position coordinates of the target analyte and/or the adjacent analytes.</p><p id="p-1527" num="1521">We disclose a neural network-implemented method of base calling analytes on a flow cell. The method includes accessing a sequence of image sets generated over a plurality of sequencing cycles of a sequencing run that synthesizes the analytes on the flow cell. Each image in the sequence of image sets covers a non-overlapping region of the flow cell and depicts intensity emissions of a subset of the analytes on the non-overlapping region and their surrounding background captured in a corresponding image channel at a respective one of the plurality of sequencing cycles. The method includes determining a nucleotide base (A, C, T, or G) incorporated at a particular one of the plurality of sequencing cycles in a particular one of the subset of the analytes by selecting, from the sequence of image sets, a current image set generated at the particular one of the plurality of sequencing cycles, one or more preceding image sets respectively generated at one or more of the plurality of sequence cycles preceding the particular one of the plurality of sequencing cycles, and one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the particular one of the plurality of sequencing cycles. The method includes extracting images patches from images in each of the selected image sets. The images patches are centered at the particular one of the subset of the analytes and include additional adjacent analytes from the subset of the analytes. The method includes convolving the image patches through one or more layers of a convolutional neural network to generate a convolved representation of the image patches. The method includes processing the convolved representation through an output layer to produce likelihoods for the nucleotide base being A, C, T, and G. The method includes classifying the nucleotide base as A, C, T, or G based on the likelihoods.</p><p id="p-1528" num="1522">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1529" num="1523">In one implementation, the method includes producing a sequence of base calls for the particular one of the subset of the analytes over the plurality of sequencing cycles by iterating the selecting, the extracting, the convolving, the processing, and the classifying for each of the plurality of sequencing cycles. In one implementation, the method includes producing a sequence of base calls for a plurality of analytes in the subset over the plurality of sequencing cycles by iterating the selecting, the extracting, the convolving, the processing, and the classifying for each of the plurality of sequencing cycles for each of the plurality of analytes in the subset. In one implementation, the non-overlapping region of the flow cell is a tile. In one implementation, the corresponding image channel is one of a plurality of filter wavelength bands. In one implementation, the corresponding image channel is one of a plurality of image events.</p><p id="p-1530" num="1524">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1531" num="1525">Simultaneously Base Calling Multiple Clusters at Multiple Cycles</p><p id="p-1532" num="1526">We disclose a neural network-implemented method of base calling analytes on a flow cell. The method includes obtaining input image data from a sequence of image sets. The sequence of image sets is generated over a plurality of sequencing cycles of a sequencing run that synthesizes the analytes on the flow cell. Each image in the sequence of image sets covers a non-overlapping region of the flow cell and depicts intensity emissions of a subset of the analytes on the non-overlapping region and their surrounding background captured in a corresponding image channel at a respective one of the plurality of sequencing cycles. The method includes processing the input image data through one or more layers of a neural network to generate an alternative representation of the input image data. The method includes processing the alternative representation through an output layer to generate an output that identifies a nucleotide base (A, C, T, or G) incorporated in at least some of the analytes in the subset at each of the each of the plurality of sequencing cycles, thereby producing a sequence of base calls for each of the at least some of the analytes in the subset over the plurality of sequencing cycles.</p><p id="p-1533" num="1527">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1534" num="1528">In one implementation, the output layer is a softmax layer, and the output is an exponentially normalized score distribution of the nucleotide base incorporated at each of the plurality of sequencing cycles in each of the at least some of the analytes in subset being A, C, T, and G. In one implementation, the input image data includes images in the sequence of image sets. In one implementation, the input image data includes at least one image patch from each of the images in the sequence of image sets. In one implementation, the neural network is a convolutional neural network. In another implementation, the neural network is a residual neural network. In yet another implementation, the neural network is a recurrent neural network.</p><p id="p-1535" num="1529">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1536" num="1530">Recurrent Convolution-Based Base Calling</p><p id="p-1537" num="1531">We disclose a neural network-based system for base calling. The system comprises a hybrid neural network with a recurrent module and a convolution module. The recurrent module uses inputs from the convolution module. The convolution module processes image data for a series of sequencing cycles of a sequencing run through one or more convolution layers and produces one or more convolved representations of the image data. The image data depicts intensity emissions of one or more analytes and their surrounding background. The recurrent module produces current hidden state representations based on convolving the convolved representations and previous hidden state representations. The output module produces a base call for at least one of the analytes and for at least one of the sequencing cycles based on the current hidden state representations.</p><p id="p-1538" num="1532">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1539" num="1533">We disclose a neural network-implemented method of base calling. The method includes separately processing each per-cycle input data in a sequence of per-cycle input data through a cascade of convolution layers of a convolutional neural network. The sequence of per-cycle input data is generated for a series of sequencing cycles of a sequencing run, and each per-cycle input data includes image channels that depict intensity emissions of one or more analytes and their surrounding background captured at a respective sequencing cycle. The method includes, for each sequencing cycle, based on the separate processing, producing a convolved representation at each of the convolution layers, thereby producing a sequence of convolved representations, mixing its per-cycle input data with its corresponding sequence of convolved representations and producing a mixed representation, and flattening its mixed representation and producing a flattened mixed representation. The method includes arranging flattened mixed representations of successive sequencing cycles as a stack. The method includes processing the stack in forward and backward directions through a recurrent neural network that convolves over a subset of the flattened mixed representations in the stack on a sliding window basis, with each sliding window corresponding to a respective sequencing cycle, and successively produces a current hidden state representation at each time step for each sequencing cycle based on (i) the subset of the flattened mixed representations in a current sliding window over the stack and (ii) a previous hidden state representation. The method includes base calling each of the analytes at each of the sequencing cycles based on results of processing the stack in forward and backward directions. The recurrent neural network can be a gated recurrent neural network, such as an LSTM and a GRU.</p><p id="p-1540" num="1534">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1541" num="1535">The method includes base calling each of the analytes at a given sequencing cycle by combining forward and backward current hidden state representations of the given sequencing cycle on a time step-basis and producing a combined hidden state representation, processing the combined hidden state representation through one or more fully-connected networks and producing a dense representation, processing the dense representation through a softmax layer to produce likelihoods of bases incorporated in each of the analytes at the given sequencing cycle being A, C, T, and G, and classifying the bases as A, C, T, or G based on the likelihoods. In one implementation, the combining includes concatenation. In another implementation, the combining includes summation. In yet another implementation, the combining includes averaging.</p><p id="p-1542" num="1536">In one implementation, each per-cycle input data includes distance channels that supplement the image channels and contain center-to-center distances between pixels in the corresponding image channels and one or more analyte centers. In one implementation, each per-cycle input data includes a scaling channel that supplements the image channels and contains scaling values based on mean intensities of one or more pixels in the image channels. In one implementation, the mixing further includes concatenating the convolved representations and the per-cycle input data. In one implementation, the mixing further includes summing the convolved representations and the per-cycle input data. In one implementation, the flattened mixed representation is a two-dimensional array. In one implementation, the subset of the flattened mixed representations is a three-dimensional volume. In one implementation, the recurrent neural network applies three-dimensional convolutions to the three-dimensional volume. In one implementation, the three-dimensional convolutions use SAME padding. In one implementation, the convolution layers use SAME padding. In one implementation, the recurrent neural network is a long short-term memory (LSTM) network that comprises an input gate, an activation gate, a forget gate, and an output gate. In such an implementation, the method includes processing (i) the subset of the flattened mixed representations in the current sliding window over the stack and (ii) the previous hidden state representation through the input gate, the activation gate, the forget gate, and the output gate and producing the current hidden state representation at each time step for each sequencing cycle. The input gate, the activation gate, the forget gate, and the output gate apply convolutions on (i) the subset of the flattened mixed representations in the current sliding window over the stack and (ii) the previous hidden state representation.</p><p id="p-1543" num="1537">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1544" num="1538">In one implementation, a neural network-implemented method of base calling includes convolving image data for a series of sequencing cycles of a sequencing run through one or more convolution layers of a convolution module and producing one or more convolved representations of the image data. The image data depicts intensity emissions of one or more analytes and their surrounding background. The method includes convolving the convolved representations and previous hidden state representations through a recurrent module and producing current hidden state representations. The method includes processing the current hidden state representations through an output module and producing a base call for at least one of the analytes and for at least one of the sequencing cycles.</p><p id="p-1545" num="1539">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1546" num="1540">Inferring Quality Scores</p><p id="p-1547" num="1541">We disclose a computer-implemented method of assigning quality scores to bases called by a neural network-based base caller. The method includes quantizing classification scores of predicted base calls produced by the neural network-based base caller in response to processing training data during training. The method includes determining a fit between the quantized classification scores and their base calling error rates. That is, for each quantized classification score, a set of training examples in the training data that are assigned the quantized classification score is determined. For each training example in the determined set of training examples, the predicted base call for the training example is compared to the ground truth base call for the training example and an error rate is determined from the comparison across the determined set of training examples to provide the error rate for the particular quantized classification score. The method includes correlating the quality scores to the quantized classification scores based on the fit.</p><p id="p-1548" num="1542">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1549" num="1543">In one implementation, the set of quantized classification scores includes a subset of the classification scores of predicted base calls produced by the neural network-based base caller in response to processing the training data during the training, and the classification scores are real numbers. In one implementation, the set of quantized classification scores includes all the classification scores of predicted base calls produced by the neural network-based base caller in response to processing the training data during the training, and the classification scores are real numbers. In one implementation, the classification scores are exponentially normalized softmax scores that sum to unity and are produced by a softmax output layer of the neural network-based base caller. In one implementation, the set of quantized classification scores is selected based on a selection formula defined as</p><p id="p-1550" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mrow>  <mrow>   <mn>0</mn>   <mo>.</mo>   <mn>9</mn>  </mrow>  <mo>&#x2062;</mo>  <mrow>   <munderover>    <mo>&#x2211;</mo>    <mrow>     <mi>i</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>    <mi>n</mi>   </munderover>   <mrow>    <mn>0</mn>    <mo>.</mo>    <msup>     <mn>1</mn>     <mrow>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </msup>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-1551" num="0000">and applied to the softmax scores. In one implementation, the set of quantized classification scores is selected based on a selection formula defined as</p><p id="p-1552" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mover>   <munder>    <mo>&#x2200;</mo>    <mrow>     <mi>i</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>   </munder>   <mrow>    <mi>n</mi>    <mo>=</mo>    <mn>10</mn>   </mrow>  </mover>  <mrow>   <mn>0.1</mn>   <mi>i</mi>  </mrow> </mrow></math></maths></p><p id="p-1553" num="0000">and applied to the softmax scores. In one implementation, the method includes, based on the correlation, assigning the quality scores to bases called by the neural network-based base caller during inference. In one implementation, the method includes assigning the quality scores based on applying a quality score correspondence scheme to the bases called by the neural network-based base caller during the inference. In such an implementation, the scheme maps ranges of classification scores, produced by the neural network-based base caller in response to processing inference data, during the inference, to corresponding quantized classification scores in the set. In one implementation, the method includes, during the inference, stopping base calling an analyte whose quality score is below a set threshold for a current base calling cycle. In one implementation, the method includes, during the inference, stopping base calling an analyte whose average quality score is below a set threshold after successive base calling cycles. In one implementation, a sample size used for comparing the predicted base calls to the corresponding ground truth base calls is specific to each quantized classification score. In one implementation, a sample size used for comparing the predicted base calls to the corresponding ground truth base calls is specific to each quantized classification score. In one implementation, the fit is determined using a regression model. In one implementation, the method includes for each quantized classification score, determining a base calling accuracy rate by comparing its predicted base calls to corresponding ground truth base calls, and determining the fit between the quantized classification scores and their base calling accuracy rates. In one implementation, the corresponding ground truth base calls are derived from well-characterized human and non-human samples sequenced on a number of sequencing instruments, sequencing chemistries, and sequencing protocols.</p><p id="p-1554" num="1544">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1555" num="1545">Predicting Quality Scores</p><p id="p-1556" num="1546">We disclose a neural network-based quality scorer that runs on numerous processors operating in parallel and is coupled to memory. The system comprises a convolutional neural network running on the numerous processors. The convolutional neural network is trained on training examples comprising data from sequencing images and labeled with base call quality ground truths using a backpropagation-based gradient update technique that progressively matches base call quality predictions of the convolutional neural network with the base call quality ground truths. The system comprises an input module of the convolutional neural network which runs on at least one of the numerous processors and feeds data from sequencing images captured at one or more sequencing cycles to the convolutional neural network for determining quality status of one or more bases called for one or more analytes. The system comprises an output module of the convolutional neural network which runs on at least one of the numerous processors and translates analysis by the convolutional neural network into an output that identifies the quality status of the one or more bases called for the one or more analytes.</p><p id="p-1557" num="1547">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1558" num="1548">In one implementation, the output module further comprises a softmax classification layer that produces likelihoods for the quality status being high-quality, medium-quality, and low-quality. In such an implementation, based on the likelihoods, the quality status is classified as high-quality, medium-quality, or low-quality. In one implementation, the softmax classification layer produces likelihoods for the quality status being assigned a plurality of quality scores. In such an implementation, based on the likelihoods, the quality status is assigned a quality score from one of the plurality of quality scores. In one implementation, the quality scores are logarithmically based on base calling error probabilities, and the plurality of quality scores includes Q6, Q10, Q43, Q20, Q22, Q27, Q30, Q33, Q37, Q40, and Q50. In one implementation, the output module further comprises a regression layer that produces continuous values which identify the quality status. In one implementation, the system comprises a supplemental input module that supplements the data from the sequencing images with quality predictor values for the bases called, and feeds the quality predictor values to the convolutional neural network along with the data from the sequencing images. In one implementation, the quality predictor values include online overlap, purity, phasing, start5, hexamer score, motif accumulation, endiness, approximate homopolymer, intensity decay, penultimate chastity, signal overlap with background (SOWB), and/or shifted purity G adjustment. In one implementation, the quality predictor values include peak height, peak width, peak location, relative peak locations, peak height ratio, peak spacing ratio, and/or peak correspondence.</p><p id="p-1559" num="1549">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1560" num="1550">We also disclose a neural network-implemented method of quality scoring. The method includes feeding data from sequencing images captured at one or more sequencing cycles to a convolutional neural network for determining quality status of one or more bases called for one or more analytes. The convolutional neural network is trained on training examples comprising data from sequencing images and labeled with base call quality ground truths. The training comprises using a backpropagation-based gradient update technique that progressively matches base call quality predictions of the convolutional neural network with the base call quality ground truths. The method includes translating analysis by the convolutional neural network into an output that identifies the quality status of the one or more bases called for the one or more analytes.</p><p id="p-1561" num="1551">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1562" num="1552">In one implementation, a computer-implemented method includes processing input data for one or more analytes through a neural network and producing an alternative representation of the input data, processing the alternative representation through an output layer to produce an output, the output identifies likelihoods of a base incorporated in a particular one of the analytes being A, C, T, and G, calling bases for one or more of the analytes based on the output, and determining quality of the called bases based on the likelihoods identified by the output.</p><p id="p-1563" num="1553">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1564" num="1554">We disclose a neural network-based quality scorer, which runs on numerous processors operating in parallel and is coupled to memory. The system comprises a neural network running on the numerous processors, trained on training examples comprising data from sequencing images and labeled with base call quality ground truths using a backpropagation-based gradient update technique that progressively matches base call quality predictions of the neural network with the base call quality ground truths. The system comprises an input module of the neural network which runs on at least one of the numerous processors and feeds data from sequencing images captured at one or more sequencing cycles to the neural network for determining quality status of one or more bases called for one or more analytes. The system comprises an output module of the neural network which runs on at least one of the numerous processors and translates analysis by the neural network into an output that identifies the quality status of the one or more bases called for the one or more analytes.</p><p id="p-1565" num="1555">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1566" num="1556">End-to-End Integration</p><p id="p-1567" num="1557">There is provided a computer implemented method, the method comprising processing first image data comprising images of analytes and their surrounding background captured by a sequencing system for one or more sequencing cycles of a sequencing run through a neural network and producing a base call for one or more of the analytes of the one or more sequencing cycles of the sequencing run. The method may comprise performing one or more sequencing cycles to capture the images of analytes and their surrounding background. In some embodiments the method comprises performing a plurality of sequencing cycles, wherein each of the plurality of sequencing cycles generates image data. The computer-implemented method may include processing a first input through a first neural network and producing a first output. The first input comprises first image data derived from images of analytes and their surrounding background captured by a sequencing system for a sequencing run. The method may include processing the first output through a post-processor and producing template data indicating one or more properties of respective portions of the first image data, i.e. about the analytes and their surrounding background. The method may include processing a second input through a second neural network and producing a second output. The second input may comprise the first image data modified using the template data, second image data modified using the template data and/or first and/or second image data and supplemental data. The supplemental data may comprise the template data. The second image data is derived from images of the analytes and their surrounding background. The second output identifies base calls for one or more of the analytes at one or more sequencing cycles of the sequencing run.</p><p id="p-1568" num="1558">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1569" num="1559">In one implementation, the metadata comprises a template image at an upsampled, subpixel resolution, and, based on the metadata, each subpixel in the template image is identified as either background subpixel, analyte center subpixel, or analyte interior subpixel. In one implementation, the images of the analytes and their surrounding background are captured at an optical, pixel-resolution. In one implementation, the method includes determining area weighting factors for pixels in the images based on how many subpixels in the template image, corresponding to a pixel in the images, contain parts of one or more of the analytes, and modifying intensities of the pixels based on the area weighting factors and including the pixels with the modified intensities in the second input as the third image data for base calling by the second neural network. In one implementation, the method includes upsampling the images to the upsampled, subpixel resolution and producing upsampled images. The upsampling includes assigning a background intensity to those subpixels in the upsampled images that correspond to background subpixels in the template image and assigning analyte intensities to those subpixels in the upsampled images that correspond to analyte center subpixels and analyte interior subpixels in the template image, and including the upsampled images in the second input as the third image data for base calling by the second neural network. In one implementation, the background intensity has a zero or minimal value. In one implementation, the analyte intensities are determined by interpolating intensities of the pixels in the optical, pixel-resolution. In one implementation, the method includes upsampling the images to the upsampled, subpixel resolution and producing upsampled images. The upsampling includes distributing entire intensity of a pixel in the optical, pixel domain among only those constituent subpixels of the pixel in the upsampled images that correspond to the analyte center subpixels and the analyte interior subpixels in the template image, and including the upsampled images in the second input as the third image data for base calling by the second neural network. In one implementation, the metadata identifies centers of the analytes. In another implementation, the metadata identifies shapes of the analytes. In yet another implementation, the metadata identifies boundaries between the analytes. In one implementation, the method includes determining quality of the base calls based on the second output.</p><p id="p-1570" num="1560">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1571" num="1561">We disclose a computer-implemented method that includes using a first neural network to determine metadata about analytes, the metadata identifies centers of the analytes, shapes of the analytes, and/or boundaries between the analytes, and using a second neural network to base call the analytes based on the determined metadata.</p><p id="p-1572" num="1562">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1573" num="1563">In one implementation, the method includes, based on the determined metadata, constructing an input for processing by the second neural network. The input includes modified intensity values that incorporate context about the centers, shapes, and/or boundaries of the analytes in the processing by the second neural network. In one implementation, the method includes processing the modified intensity values through the second neural network to base call the analytes. In one implementation, the method includes accompanying an input that is fed to the second neural network for processing with supplemental data derived based on the determined metadata. The supplemental data incorporates context about the centers, shapes, and/or boundaries of the analytes in the processing by the second neural network. In one implementation, the method includes processing the input and the supplemental data through the second neural network to base call the analytes.</p><p id="p-1574" num="1564">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1575" num="1565">We disclose a computer-implemented method that includes performing a sequencing procedure on analytes. The sequencing procedure comprises a plurality of sequencing cycles and each of the plurality of sequencing cycles generates image data. In one implementation, the method includes processing the image data for each of the plurality of sequencing cycles through a neural network and producing a base call for at least some of the analytes at each of the plurality of sequencing cycles.</p><p id="p-1576" num="1566">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1577" num="1567">In one implementation, the method includes, prior to processing the image data for each of the plurality of sequencing cycles through the neural network, processing the image data for some of the plurality of sequencing cycles through another neural network and determining metadata about the analytes. The metadata identifies centers and/or shapes of the analytes. In one implementation, the method includes, based on the determined metadata, base calling at least some of the analytes at each of the plurality of sequencing cycles using the neural network.</p><p id="p-1578" num="1568">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1579" num="1569">We disclose a sequencing system that comprises a receptacle coupled to a biosensor system, an illumination system, and a system controller coupled to the receptacle and having an analysis module. The biosensor system is configured to have an array of light detectors, the biosensor system has a biosensor, and the biosensor has reaction sites configured to contain analytes. The illumination system is configured to direct excitation light toward the biosensor and illuminate the analytes in the reaction sites. At least some of the analytes provide emission signals when illuminated. The system controller is coupled to the receptacle and has an analysis module. The analysis module is configured to obtain image data from the light detectors at each of a plurality of sequencing cycles. The image data is derived from the emission signals detected by the light detectors and process the image data for each of the plurality of sequencing cycles through a neural network and produce a base call for at least some of the analytes at each of the plurality of sequencing cycles.</p><p id="p-1580" num="1570">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1581" num="1571">We disclose a computer-implemented method of using analyte centers, shapes, and boundaries identified in a template image at an upsampled, subpixel resolution to interpret images captured at an optical, pixel-resolution to base call analytes synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more image channels captured at the optical, pixel-resolution. The method includes generating the template image with area weighting factors, including processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a neural network-based template generator to identify the analyte centers, shapes, and boundaries of the analytes at the upsampled, subpixel resolution, evaluating analyte shape and boundaries of a particular analyte to identify at least one pixel that contains part of the particular analyte, to set an area weighting factor based on how many subpixels in the identified pixel contain parts of the particular analyte, and to store the area weighting factor in the template image, and performing the evaluating to identify, to set, and to store for pixels that also contain part of the particular analyte, for pixels in each of the images captured at the optical, pixel-resolution, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel, generating a modified version of each of the images with pixels having modified pixel intensity values, processing modified versions of the images through a neural network-based base caller to generate an alternative representation of the modified versions, and base calling the particular analyte using the alternative representation.</p><p id="p-1582" num="1572">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1583" num="1573">In one implementation, the base calling further includes accessing one or more images at the optical, pixel-resolution in each of a current image set generated at a current one of the plurality of sequencing cycles, of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles, for pixels in each of the images, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel, generating a modified version of each of the images with pixels having modified pixel intensity values, for the particular analyte, extracting an image patch from each modified version such that each image patch has an array of pixels, and contains in its center pixel a center of the particular analyte identified in the template image, convolving image patches extracted from modified versions of the images through a convolutional neural network to generate a convolved representation of the image patches, processing the convolved representation through an output layer to produce, for the center pixel, likelihoods of a base incorporated in the particular analyte at the current one of the plurality of sequencing cycles being A, C, T, and G, and classifying the base as A, C, T, or G based on the likelihoods. In one implementation, the method includes, prior to modifying the pixel intensity values, aligning each of the images captured at the optical, pixel-resolution with the template image using cycle-specific and image channel-specific transformations.</p><p id="p-1584" num="1574">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1585" num="1575">We disclose a computer-implemented method of using analyte centers, shapes, and boundaries identified in a template image at an upsampled, subpixel resolution to interpret images captured at an optical, pixel-resolution to base call analytes synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more image channels captured at the optical, pixel-resolution. The method includes generating the template image with area weighting factors, including processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a neural network-based template generator to determine at least one primary analyte for which a pixel contains part of the primary analyte and to set an area weighting factor based on how many subpixels in the pixel contain parts of the primary analyte, and performing the evaluating to determine and to set for numerous analytes and numerous pixels, for pixels in each of the images captured at the optical, pixel-resolution, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel, generating a modified version of each of the images with pixels having modified pixel intensity values, as input to a forward pass of a neural network-based base caller, processing modified versions of the images through the neural network-based base caller to generate an alternative representation of the modified versions, and as output of the forward pass, simultaneously base calling each one of the numerous analytes using the alternative representation.</p><p id="p-1586" num="1576">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1587" num="1577">In one implementation, the base calling further includes accessing one or more images at the optical, pixel-resolution in each of a current image set generated at a current one of the plurality of sequencing cycles, of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles, for pixels in each of the images, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel, generating a modified version of each of the images with pixels having modified pixel intensity values, extracting an image patch from each modified version such that each image patch has an array of pixels, convolving image patches extracted from modified versions of the images through a convolutional neural network to generate a convolved representation of the image patches, processing the convolved representation through an output layer to produce, for each pixel in the array, likelihoods of a base incorporated at the current one of the plurality of sequencing cycles being A, C, T, and G, classifying the base as A, C, T, or G based on the likelihoods, and base calling each one of the numerous analytes based on a base classification assigned to a respective pixel containing a center of a corresponding analyte. In one implementation, the method includes, prior to modifying the pixel intensity values, aligning each of the images captured at the optical, pixel-resolution with the template image using cycle-specific and image channel-specific transformations.</p><p id="p-1588" num="1578">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1589" num="1579">We disclose a computer-implemented method of using analyte centers, shapes, and boundaries identified in a template image at an upsampled, subpixel resolution to interpret images captured at an optical, pixel-resolution to base call analytes synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more image channels captured at the optical, pixel-resolution. The method includes processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a neural network-based template generator to generate the template image at the upsampled, subpixel resolution. By &#x201c;initial ones of the plurality of sequencing cycles&#x201d; this will be understood to refer to one or more initial sequencing cycles, for example one or more of sequencing cycle 1 to 10, 2 to 10, 2 to 8 or 2 to 7. The template image classifies subpixels into classes including analyte center, background, and belonging to an analyte upsampling each of the images captured at the optical, pixel-resolution into subpixel domain, the upsampling includes assigning a background intensity to subpixels identified in the template image as not contributing to any analyte, as input to a forward pass of a neural network-based base caller, processing upsampled images through the neural network-based base caller to generate an alternative representation of the upsampled images, and as output of the forward pass, simultaneously base calling a plurality of the analytes using the alternative representation.</p><p id="p-1590" num="1580">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1591" num="1581">In one implementation, the base calling further includes accessing one or more images at the optical, pixel-resolution in each of a current image set generated at a current one of the plurality of sequencing cycles, of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles, upsampling each of the images captured at the optical, pixel-resolution into subpixel domain, the upsampling includes assigning a background intensity to subpixels identified in the template image as not contributing to any analyte, extracting an image patch from each upsampled image such that each image patch has an array of subpixels, convolving image patches extracted from the upsampled images through a convolutional neural network to generate a convolved representation of the image patches, processing the convolved representation through an output layer to produce, for each subpixel in the array, likelihoods of a base incorporated at the current one of the plurality of sequencing cycles being A, C, T, and G, classifying the base as A, C, T, or G based on the likelihoods, and base calling each one of the plurality of the analytes based on a base classification assigned to a respective subpixel containing a center of a corresponding analyte.</p><p id="p-1592" num="1582">In one implementation, the method includes, prior to the upsampling, aligning each of the images captured at the optical, pixel-resolution with the template image using cycle-specific and image channel-specific transformations. In one implementation, the upsampling is performed using at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage. In one implementation, the background intensity has a zero value. In one implementation, the background intensity has a near zero value.</p><p id="p-1593" num="1583">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1594" num="1584">We disclose a computer-implemented method of using analyte centers, shapes, and boundaries identified in a template image at an upsampled, subpixel resolution to interpret images captured at an optical, pixel-resolution to base call analytes synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more image channels captured at the optical, pixel-resolution. The method includes generating the template image with per-subpixel area weighting factors, including processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a neural network-based template generator to identify the analyte centers, shapes, and boundaries of the analytes at the upsampled, subpixel resolution, and evaluating analyte shape and boundaries of the analytes to determine how many subpixels in a respective pixel contain parts of any analyte, to set a per-subpixel area weighting factor for the subpixels in the respective pixel, and to store the per-subpixel area weighting factor in the template image, upsampling each of the images captured at the optical, pixel-resolution into subpixel domain, the upsampling includes distributing intensity of a respective pixel among first subpixels of the respective pixel identified in the template image as contributing to any analyte by applying the per-subpixel area weighting factor, and assigning a background intensity to second subpixels in the respective pixel identified in the template image as not contributing to any analyte, as input to a forward pass of a neural network-based base caller, processing upsampled images through the neural network-based base caller to generate an alternative representation of the upsampled images, and as output of the forward pass, simultaneously base calling a plurality of the analytes using the alternative representation.</p><p id="p-1595" num="1585">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations.</p><p id="p-1596" num="1586">In one implementation, the base calling further includes accessing one or more images at the optical, pixel-resolution in each of a current image set generated at a current one of the plurality of sequencing cycles, of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles, upsampling each of the images captured at the optical, pixel-resolution into subpixel domain, the upsampling includes distributing intensity of a respective pixel among first subpixels of the respective pixel identified in the template image as contributing to any analyte by applying the per-subpixel area weighting factor, and assigning a background intensity to second subpixels in the respective pixel identified in the template image as not contributing to any analyte, extracting an image patch from each upsampled image such that each image patch has an array of subpixels, convolving image patches extracted from the upsampled images through a convolutional neural network to generate a convolved representation of the image patches, processing the convolved representation through an output layer to produce, for each subpixel in the array, likelihoods of a base incorporated at the current one of the plurality of sequencing cycles being A, C, T, and G, classifying the base as A, C, T, or G based on the likelihoods, and base calling each one of the plurality of the analytes based on a base classification assigned to a respective subpixel containing a center of a corresponding analyte.</p><p id="p-1597" num="1587">In one implementation, the method includes, prior to the upsampling, aligning each of the images captured at the optical, pixel-resolution with the template image using cycle-specific and image channel-specific transformations. In one implementation, the background intensity has a zero value. In another implementation, the background intensity has a near zero value.</p><p id="p-1598" num="1588">Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1599" num="1589">In one implementation, a computer-implemented method includes evaluating a template image in an upsampled subpixel domain for a particular analyte to identify at least one pixel that contains part of the particular analyte and to set an area weighting factor based on how many subpixels in the identified pixel contain parts of the particular analyte, performing the evaluating to determine and to set for adjoining pixels to the identified pixel that also contain part of the particular analyte, and modifying a pixel intensity value of the identified pixel and the adjoining pixels for processing based on the area weighting factor for a respective pixel.</p><p id="p-1600" num="1590">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1601" num="1591">In one implementation, a computer-implemented method includes evaluating a pixel in a template image in an upsampled subpixel domain to determine at least a primary analyte for which the pixel contains part of the primary analyte and to set an area weighting factor based on how many subpixels in the identified pixel contain parts of the primary analyte, performing the evaluating to determine and to set for numerous pixels in a field of an optical image, and modifying a pixel intensity value of the identified pixel and adjoining pixels for processing based on the area weighting factor for a respective pixel.</p><p id="p-1602" num="1592">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1603" num="1593">In one implementation, a computer-implemented method includes accessing a template image in an upsampled subpixel domain, the template image identifies subpixels that contain parts of any analyte and, during upsampling of a field of optical images into the subpixel domain, assigning a background intensity to subpixels identified in the template image as not contributing to any analyte.</p><p id="p-1604" num="1594">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1605" num="1595">In one implementation, a computer-implemented method includes evaluating an identified pixel in a template image in an upsampled subpixel domain to determine how many subpixels in the identified pixel contain parts of any analyte and to set a per-subpixel area weighting factor for the subpixels in the identified pixel, performing the evaluating to determine and to set for numerous pixels in a field of an optical image and storing the per-subpixel area weighting factors for the numerous pixels in the template image, and, during upsampling of the field of the optical image into the subpixel domain, distributing intensity of a particular pixel among first subpixels of a particular pixel identified in the template as contributing to any analyte by applying the per-subpixel area weighting factor and assigning a background intensity to second subpixels a particular pixel identified in the template as not contributing to any analyte.</p><p id="p-1606" num="1596">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><p id="p-1607" num="1597">We disclose a computer-implemented method of using cluster centers, shapes, and boundaries identified in a template image at an upsampled, subpixel resolution to interpret images captured at an optical, pixel-resolution to base call deoxyribonucleic acid (DNA) clusters synthesized on a tile of a flow cell during a sequencing run, the sequencing run having a plurality of sequencing cycles, each of the plurality of sequencing cycles generating an image set with one or more images, and each of the images depicting intensity emissions of the DNA clusters and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel-resolution. The method includes generating the template image with area weighting factors, including processing initial image sets respectively generated at initial ones of the plurality of sequencing cycles through a neural network-based template generator to determine at least one primary DNA cluster for which a pixel contains part of the primary DNA cluster and to set an area weighting factor based on how many subpixels in the pixel contain parts of the primary DNA cluster, and performing the evaluating to determine and to set for numerous DNA clusters and numerous pixels, supplementing each of the images captured at the optical, pixel-resolution with the template image with the area weighting factors by pixel-wise encoding the area weighting factors with pixels in the images, as input to a forward pass of a neural network-based base caller, processing the images and the supplemental template image through the neural network-based base caller to generate an alternative representation of the input, and as output of the forward pass, simultaneously base calling each one of the numerous DNA clusters using the alternative representation.</p><p id="p-1608" num="1598">Each of the features discussed in the particular implementation section for other implementations apply equally to this implementation. As indicated above, all the other features are not repeated here and should be considered repeated by reference. The reader will understand how features identified in these implementations can readily be combined with sets of base features identified in other implementations. Other implementations of the method described in this section can include a non-transitory computer readable storage medium storing instructions executable by a processor to perform any of the methods described above. Yet another implementation of the method described in this section can include a system including memory and one or more processors operable to execute instructions, stored in the memory, to perform any of the methods described above.</p><heading id="h-0025" level="1">CLAUSES</heading><p id="p-1609" num="1599">The disclosure also includes the following clauses:</p><p id="p-1610" num="1600">Clauses Set 1<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="1601">1. A computer-implemented method, comprising:    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="1602">processing first image data comprising images of analytes and their surrounding background captured by a sequencing system for one or more sequencing cycles of a sequencing run through a neural network and producing a base call for one or more of the analytes of the one or more sequencing cycles of the sequencing run.</li>    </ul>    </li>    <li id="ul0005-0002" num="1603">2. The computer-implemented method of clause 1, wherein processing the first image data comprises:    <ul id="ul0007" list-style="none">        <li id="ul0007-0001" num="1604">processing a first input through a first neural network to produce a first output, wherein the first input comprises the first image data;</li>        <li id="ul0007-0002" num="1605">processing the first output through a post-processor to produce template data indicating one or more properties of respective portions of the first image data; and</li>        <li id="ul0007-0003" num="1606">processing a second input through a second neural network to produce a second output, wherein the second input comprises the first image data and supplemental data; wherein the supplemental data comprises the template data, and wherein the second output identifies base calls for one or more of the analytes at one or more sequencing cycles of the sequencing run.</li>    </ul>    </li>    <li id="ul0005-0003" num="1607">3. The computer-implemented method of clause 1, wherein processing the first image data comprises:    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="1608">processing a first input through a first neural network to produce a first output, wherein the first input comprises the first image data;</li>        <li id="ul0008-0002" num="1609">processing the first output through a post-processor to produce template data indicating one or more properties of respective portions of the first image data; and</li>        <li id="ul0008-0003" num="1610">processing a second input through a second neural network to produce a second output, wherein the second input comprises the first image data modified using the template data, and wherein the second output identifies base calls for one or more of the analytes at one or more sequencing cycles of the sequencing run.</li>    </ul>    </li>    <li id="ul0005-0004" num="1611">4. The computer-implemented method of clause 3, wherein the second input further comprises second image data modified using the template data, the second image data comprising images of analytes and their surrounding background captured by the sequencing system for one or more additional sequencing cycles of the sequencing run.</li>    <li id="ul0005-0005" num="1612">5. The computer-implemented method of any of clauses 2 to 5, wherein the template data comprises a template image, wherein the template image is at an upsampled, subpixel resolution.</li>    <li id="ul0005-0006" num="1613">6. The computer-implemented method of clause 5, wherein each subpixel in the template image is identified as either background subpixel, analyte center subpixel, or analyte interior subpixel.</li>    <li id="ul0005-0007" num="1614">7. The computer-implemented method of any of clauses 1 to 6, wherein the images of the analytes and their surrounding background are captured at an optical, pixel resolution.</li>    <li id="ul0005-0008" num="1615">8. The computer-implemented method of any of clauses 3 to 7, wherein modification using the template data comprises:    <ul id="ul0009" list-style="none">        <li id="ul0009-0001" num="1616">calculating an area weighting factor for one or more pixels in the first and/or second image data based on how many subpixels in the template data that correspond to a pixel in the images of the first and/or second image data contain parts of one or more of the analytes; and</li>        <li id="ul0009-0002" num="1617">modifying intensities of the pixels based on the area weighting factor.</li>    </ul>    </li>    <li id="ul0005-0009" num="1618">9. The computer-implemented method of clause 6 or any of clauses 7 or 8 when dependent on clause 6, wherein modification using the template data comprises:    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="1619">upsampling the images of analytes and their surrounding background to the upsampled, subpixel resolution to produce upsampled images, and assigning a background intensity to those subpixels in the upsampled images that correspond to background subpixels in the template image and assigning analyte intensities to those subpixels in the upsampled images that correspond to analyte center subpixels and analyte interior subpixels in the template image.</li>    </ul>    </li>    <li id="ul0005-0010" num="1620">10. The computer-implemented method of clause 9, wherein the background intensity has a zero value.</li>    <li id="ul0005-0011" num="1621">11. The computer-implemented method of clause 9 or clause 10, wherein the analyte intensities are determined by interpolating intensities of the pixels in the optical, pixel resolution.</li>    <li id="ul0005-0012" num="1622">12. The computer-implemented method of clause 6 or any of clauses 7-11 when dependent on clause 6, wherein modification using the template image comprises:    <ul id="ul0011" list-style="none">        <li id="ul0011-0001" num="1623">upsampling the images of analytes and their surrounding background to the upsampled, subpixel resolution to produce upsampled images, and distributing an entire intensity of a pixel in the optical, pixel domain among only those constituent subpixels of the pixel in the upsampled images that correspond to the analyte center subpixels and the analyte interior subpixels in the template image.</li>    </ul>    </li>    <li id="ul0005-0013" num="1624">13. The computer-implemented method of any of clauses 2-12, wherein the template data identifies at least one of the properties selected from the group consisting of: spatial distribution of the analytes, analyte shape, centers of the analytes and analyte boundary.</li>    <li id="ul0005-0014" num="1625">14. The computer-implemented method of any of clauses 2-13, further comprising calculating a quality of the base calls based on the second output.</li>    <li id="ul0005-0015" num="1626">15. The computer-implemented method according to any one of clauses 1-14, further comprising performing one or more sequencing cycles to capture the images of analytes and their surrounding background.</li>    <li id="ul0005-0016" num="1627">16. The computer-implemented method of any one of clauses 1-15, further comprising performing a plurality of sequencing cycles, wherein each of the plurality of sequencing cycles generates image data.</li>    <li id="ul0005-0017" num="1628">17. A computer-implemented method, comprising:    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="1629">using a first neural network to determine template data about analytes, wherein the template data identifies at least one of the properties selected from the group consisting of: spatial distribution of the analytes, analyte shape, centers of the analytes and analyte boundary; and</li>        <li id="ul0012-0002" num="1630">using a second neural network to base call the analytes based on the template data.</li>    </ul>    </li>    <li id="ul0005-0018" num="1631">18. The computer-implemented method of clause 17, wherein the template data comprises modified intensity values to identify at least one of the properties selected from the group consisting of: spatial distribution of the analytes, analyte shape, centers of the analytes and analyte boundary; and processing the modified intensity values through the second neural network to base call the analytes.</li>    <li id="ul0005-0019" num="1632">19. The computer-implemented method of clause 17 or 18, wherein the template data comprises a template image.</li>    <li id="ul0005-0020" num="1633">20. The computer-implemented method of clause 19, further comprising:    <ul id="ul0013" list-style="none">        <li id="ul0013-0001" num="1634">evaluating the template image in an upsampled subpixel domain for at least one particular analyte to identify a pixel that contains part of the at least one particular analyte and adjoining pixels to the pixel that also contain part of the at least one particular analyte;</li>        <li id="ul0013-0002" num="1635">calculating an area weighting factor for each pixel based on how many subpixels in each of the identified pixels contain parts of the at least one particular analyte; and</li>        <li id="ul0013-0003" num="1636">modifying a pixel intensity value of the identified pixel and the adjoining pixels for processing based on the area weighting factor for a respective pixel.</li>    </ul>    </li>    <li id="ul0005-0021" num="1637">21. The computer-implemented method of clause 20, wherein evaluating the template image further comprises:    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="1638">processing one or more initial image sets respectively generated at one or more initial sequencing cycles of a plurality of sequencing cycles through the first neural network to produce the template image to identify the centers, shapes, and boundaries of the analytes at the upsampled, subpixel resolution; wherein each image set comprises one or more images, each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel resolution.</li>    </ul>    </li>    <li id="ul0005-0022" num="1639">22. The computer-implemented method of clause 20 or 21, wherein evaluating the template image further comprises:    <ul id="ul0015" list-style="none">        <li id="ul0015-0001" num="1640">evaluating the analyte shape and boundaries of the at least one particular analyte to identify at least one pixel that contains part of the at least one particular analyte and adjoining pixels to the pixel that also contain part of the at least one particular analyte; and wherein the method further comprises</li>        <li id="ul0015-0002" num="1641">storing the area weighting factor in the template image; and</li>        <li id="ul0015-0003" num="1642">generating a modified version of each of the images with pixels having modified pixel intensity values;</li>        <li id="ul0015-0004" num="1643">processing modified versions of the images through the second neural network to generate an alternative representation of the modified versions; and</li>        <li id="ul0015-0005" num="1644">base calling the at least one particular analyte using the alternative representation.</li>    </ul>    </li>    <li id="ul0005-0023" num="1645">23. The computer-implemented method of clause 22, wherein the base calling further comprises:    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="1646">accessing one or more images at the optical, pixel resolution in each        <ul id="ul0017" list-style="none">            <li id="ul0017-0001" num="1647">of a current image set generated at a current one of the plurality of sequencing cycles,</li>            <li id="ul0017-0002" num="1648">of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one</li>            <li id="ul0017-0003" num="1649">of the plurality of sequencing cycles, and of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles;</li>        </ul>        </li>        <li id="ul0016-0002" num="1650">for pixels in each of the images, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel; generating a modified version of each of the images with pixels having modified pixel intensity values;</li>        <li id="ul0016-0003" num="1651">for the at least one particular analyte, extracting an image patch from each modified version such that each image patch has an array of pixels, and        <ul id="ul0018" list-style="none">            <li id="ul0018-0001" num="1652">contains in its center pixel a center of the particular analyte identified in the template image;</li>        </ul>        </li>        <li id="ul0016-0004" num="1653">convolving image patches extracted from modified versions of the images through a convolutional neural network of the second neural network to generate a convolved representation of the image patches;</li>        <li id="ul0016-0005" num="1654">processing the convolved representation through an output layer to produce, for the center pixel, likelihoods of a base incorporated in the at least one particular analyte at the current one of the plurality of sequencing cycles being A, C, T, and G; and</li>        <li id="ul0016-0006" num="1655">classifying the base as A, C, T, or G based on the likelihoods.</li>    </ul>    </li>    <li id="ul0005-0024" num="1656">24. The computer-implemented method according to clause 22 or 23, further comprising:    <ul id="ul0019" list-style="none">        <li id="ul0019-0001" num="1657">prior to modifying the pixel intensity values, aligning each of the images captured at the optical, pixel resolution with the template image using cycle-specific and imaging channel-specific transformations.</li>    </ul>    </li>    <li id="ul0005-0025" num="1658">25. The computer-implemented method of clause 19, further comprising:    <ul id="ul0020" list-style="none">        <li id="ul0020-0001" num="1659">evaluating the template image in an upsampled subpixel domain to identify subpixels that contain parts of any analyte; and</li>        <li id="ul0020-0002" num="1660">assigning a background intensity to subpixels identified in the template image as not contributing to any analyte.</li>    </ul>    </li>    <li id="ul0005-0026" num="1661">26. The computer-implemented method of clause 25, wherein evaluating the template image in an upsampled subpixel domain further comprises:    <ul id="ul0021" list-style="none">        <li id="ul0021-0001" num="1662">calculating how many subpixels in at least one pixel contain parts of any analyte and calculating a per-subpixel area weighting factor for the subpixels in the at least one pixel.</li>    </ul>    </li>    <li id="ul0005-0027" num="1663">27. The computer-implemented method of clause 25 or 26, wherein the method comprises:    <ul id="ul0022" list-style="none">        <li id="ul0022-0001" num="1664">processing one or more initial image sets respectively generated at one or more initial sequencing cycles of a plurality of sequencing cycles through the first neural network to produce the template image at the upsampled, subpixel resolution, wherein each image set comprises one or more images, each of the images depicting intensity emissions of the analytes and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel resolution and wherein the template image classifies subpixels into classes including analyte center, background, and analyte interior;</li>        <li id="ul0022-0002" num="1665">upsampling each of the images captured at the optical, pixel resolution into a subpixel domain and assigning a background intensity to subpixels of each of the images identified in the template image as not contributing to any analyte;</li>        <li id="ul0022-0003" num="1666">processing the upsampled images through the second neural network to generate an alternative representation of the upsampled images; and base calling a plurality of the analytes using the alternative representation.</li>    </ul>    </li>    <li id="ul0005-0028" num="1667">28. The computer-implemented method of clause 27, wherein upsampling each of the images further comprises:    <ul id="ul0023" list-style="none">        <li id="ul0023-0001" num="1668">distributing intensity of a particular pixel among first subpixels of the particular pixel identified in the template image as contributing to any analyte by applying the per-subpixel area weighting factor and assigning a background intensity to second subpixels of the particular pixel identified in the template as not contributing to any analyte.</li>    </ul>    </li>    <li id="ul0005-0029" num="1669">29. The computer-implemented method of clause 28, wherein the prior to upsampling the method comprises:    <ul id="ul0024" list-style="none">        <li id="ul0024-0001" num="1670">accessing one or more images at the optical, pixel resolution in each        <ul id="ul0025" list-style="none">            <li id="ul0025-0001" num="1671">of a current image set generated at a current one of the plurality of sequencing cycles,</li>            <li id="ul0025-0002" num="1672">of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and</li>            <li id="ul0025-0003" num="1673">of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles; and after upsampling the method comprises:</li>        </ul>        </li>        <li id="ul0024-0002" num="1674">extracting an image patch from each upsampled image such that each image patch has an array of subpixels;</li>        <li id="ul0024-0003" num="1675">convolving image patches extracted from the upsampled images through the convolutional neural network of the second neural network to generate a convolved representation of the image patches;</li>        <li id="ul0024-0004" num="1676">processing the convolved representation through an output layer to produce, for each subpixel in the array, likelihoods of a base incorporated at the current one of the plurality of sequencing cycles being A, C, T, and G;</li>        <li id="ul0024-0005" num="1677">classifying the base as A, C, T, or G based on the likelihoods; and</li>        <li id="ul0024-0006" num="1678">base calling each one of the plurality of the analytes based on a base classification assigned to a respective subpixel containing a center of a corresponding analyte.</li>    </ul>    </li>    <li id="ul0005-0030" num="1679">30. The computer-implemented method of clause 28 or 29, further comprising:    <ul id="ul0026" list-style="none">        <li id="ul0026-0001" num="1680">prior to the upsampling, aligning each of the images captured at the optical, pixel resolution with the template image using cycle-specific and imaging channel-specific transformations.</li>    </ul>    </li>    <li id="ul0005-0031" num="1681">31. The computer-implemented method of any one of clauses 29 to 30, wherein the upsampling is performed using at least one of nearest neighbor intensity extraction, Gaussian based intensity extraction, intensity extraction based on average of 2&#xd7;2 subpixel area, intensity extraction based on brightest of 2&#xd7;2 subpixel area, intensity extraction based on average of 3&#xd7;3 subpixel area, bilinear intensity extraction, bicubic intensity extraction, and/or intensity extraction based on weighted area coverage.</li>    <li id="ul0005-0032" num="1682">32. A sequencing system, comprising:    <ul id="ul0027" list-style="none">        <li id="ul0027-0001" num="1683">a receptacle coupled to a biosensor system, the biosensor system configured to comprise an array of light detectors, the biosensor system comprising a biosensor, and the biosensor comprising reaction sites configured to contain analytes;</li>        <li id="ul0027-0002" num="1684">an illumination system configured to direct excitation light toward the biosensor and illuminate the analytes in the reaction sites, wherein at least some of the analytes provide emission signals when illuminated; and</li>        <li id="ul0027-0003" num="1685">a system controller coupled to the receptacle and comprising an analysis module, the analysis module configured to:        <ul id="ul0028" list-style="none">            <li id="ul0028-0001" num="1686">obtain image data from the light detectors at each of a plurality of sequencing cycles, wherein the image data is derived from the emission signals detected by the light detectors; and</li>            <li id="ul0028-0002" num="1687">process the image data for each of the plurality of sequencing cycles through a neural network and produce a base call for at least some of the analytes at each of the plurality of sequencing cycles.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-1611" num="1688">Clauses Set 2<ul id="ul0029" list-style="none">    <li id="ul0029-0001" num="1689">1. A computer-implemented method of end-to-end sequencing, including template generation and base calling, comprising:    <ul id="ul0030" list-style="none">        <li id="ul0030-0001" num="1690">accessing first image data and second image data that contain pixels in an optical, pixel resolution,        <ul id="ul0031" list-style="none">            <li id="ul0031-0001" num="1691">wherein the first image data comprises images of clusters and their surrounding background captured by a sequencing system for initial ones of sequencing cycles of a sequencing run, and</li>            <li id="ul0031-0002" num="1692">wherein the second image data comprises images of the clusters and their surrounding background captured by the sequencing system for the sequencing cycles of the sequencing run;</li>        </ul>        </li>        <li id="ul0030-0002" num="1693">processing the first image data through a neural network-based template generator, and producing a cluster map that identifies cluster metadata,        <ul id="ul0032" list-style="none">            <li id="ul0032-0001" num="1694">wherein the cluster metadata includes cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries, and</li>            <li id="ul0032-0002" num="1695">wherein the neural network-based template generator is trained on a task of mapping the images of the clusters to the cluster metadata;</li>        </ul>        </li>        <li id="ul0030-0003" num="1696">encoding the cluster metadata in a template image in an upsampled, subpixel resolution,        <ul id="ul0033" list-style="none">            <li id="ul0033-0001" num="1697">wherein subpixels of the template and the pixels of the images of the clusters represent a same image area;</li>        </ul>        </li>        <li id="ul0030-0004" num="1698">modifying intensity values of the pixels of the second image data based on the template image, and producing an intensity modified version of the second image data with an intensity distribution that accounts for the cluster metadata; and</li>        <li id="ul0030-0005" num="1699">processing the intensity modified version of the second image data through a neural network-based base caller, and producing base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run, wherein the neural network-based base caller is trained on a task of mapping the images of the clusters to the base calls.</li>    </ul>    </li>    <li id="ul0029-0002" num="1700">2. The computer-implemented method of clause 1, further including:    <ul id="ul0034" list-style="none">        <li id="ul0034-0001" num="1701">supplementing the second image data with the template image; and</li>        <li id="ul0034-0002" num="1702">processing the second image data, supplemented with the template image, through the neural network-based base caller, and producing base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run.</li>    </ul>    </li>    <li id="ul0029-0003" num="1703">3. The computer-implemented method of clause 1, wherein each subpixel in the template image is identified as either background subpixel, cluster center subpixel, or cluster interior subpixel.</li>    <li id="ul0029-0004" num="1704">4. The computer-implemented method of any of clauses 1 to 3, wherein modifying intensity values of the pixels of the second image data comprises:    <ul id="ul0035" list-style="none">        <li id="ul0035-0001" num="1705">calculating an area weighting factor for one or more pixels in the second image data based on how many subpixels in the template image that correspond to a pixel in the images of the second image data contain parts of one or more of the clusters; and</li>        <li id="ul0035-0002" num="1706">modifying intensities of the pixels based on the area weighting factor.</li>    </ul>    </li>    <li id="ul0029-0005" num="1707">5. The computer-implemented method of any of clauses 1 to 4, wherein modifying intensity values of the pixels of the second image data comprises:    <ul id="ul0036" list-style="none">        <li id="ul0036-0001" num="1708">upsampling the images of clusters and their surrounding background to the upsampled, subpixel resolution to produce upsampled images, and assigning a background intensity to those subpixels in the upsampled images that correspond to background subpixels in the template image and assigning cluster intensities to those subpixels in the upsampled images that correspond to cluster center subpixels and cluster interior subpixels in the template image.</li>    </ul>    </li>    <li id="ul0029-0006" num="1709">6. The computer-implemented method of clause 5, wherein the background intensity has a zero value.</li>    <li id="ul0029-0007" num="1710">7. The computer-implemented method of any of clauses 1 to 6, wherein the cluster intensities are determined by interpolating intensities of the pixels in the optical, pixel resolution.</li>    <li id="ul0029-0008" num="1711">8. The computer-implemented method of any of clauses 1 to 7, wherein modifying intensity values of the pixels of the second image data comprises:    <ul id="ul0037" list-style="none">        <li id="ul0037-0001" num="1712">upsampling the images of clusters and their surrounding background to the upsampled, subpixel resolution to produce upsampled images, and distributing an entire intensity of a pixel in the optical, pixel domain among only those constituent subpixels of the pixel in the upsampled images that correspond to the cluster center subpixels and the cluster interior subpixels in the template image.</li>    </ul>    </li>    <li id="ul0029-0009" num="1713">9. A computer-implemented method, comprising:    <ul id="ul0038" list-style="none">        <li id="ul0038-0001" num="1714">using a first neural network to determine template image about clusters, wherein the template image identifies at least one of the properties selected from the group consisting of: spatial distribution of the clusters, cluster shape, centers of the clusters and cluster boundary; and</li>        <li id="ul0038-0002" num="1715">using a second neural network to base call the clusters based on the template image.</li>    </ul>    </li>    <li id="ul0029-0010" num="1716">10. The computer-implemented method of clause 10, wherein the template image comprises modified intensity values to identify at least one of the properties selected from the group consisting of: spatial distribution of the clusters, cluster shape, centers of the clusters and cluster boundary; and    <ul id="ul0039" list-style="none">        <li id="ul0039-0001" num="1717">processing the modified intensity values through the second neural network to base call the clusters.</li>    </ul>    </li>    <li id="ul0029-0011" num="1718">11. The computer-implemented method of clause 9 or 10, wherein the template image comprises a template image.</li>    <li id="ul0029-0012" num="1719">12. The computer-implemented method of clause 11, further comprising:    <ul id="ul0040" list-style="none">        <li id="ul0040-0001" num="1720">evaluating the template image in an upsampled subpixel domain for at least one particular cluster to identify a pixel that contains part of the at least one particular cluster and adjoining pixels to the pixel that also contain part of the at least one particular cluster;</li>        <li id="ul0040-0002" num="1721">calculating an area weighting factor for each pixel based on how many subpixels in each of the identified pixels contain parts of the at least one particular cluster; and</li>        <li id="ul0040-0003" num="1722">modifying a pixel intensity value of the identified pixel and the adjoining pixels for processing based on the area weighting factor for a respective pixel.</li>    </ul>    </li>    <li id="ul0029-0013" num="1723">13. The computer-implemented method of clause 12, wherein evaluating the template image further comprises:    <ul id="ul0041" list-style="none">        <li id="ul0041-0001" num="1724">processing one or more initial image sets respectively generated at one or more initial sequencing cycles of a plurality of sequencing cycles through the first neural network to produce the template image to identify the centers, shapes, and boundaries of the clusters at the upsampled, subpixel resolution; wherein each image set comprises one or more images, each of the images depicting intensity emissions of the clusters and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel resolution.</li>    </ul>    </li>    <li id="ul0029-0014" num="1725">14. The computer-implemented method of clause 12 or 13, wherein evaluating the template image further comprises:    <ul id="ul0042" list-style="none">        <li id="ul0042-0001" num="1726">evaluating the cluster shape and boundaries of the at least one particular cluster to identify at least one pixel that contains part of the at least one particular cluster and adjoining pixels to the pixel that also contain part of the at least one particular cluster; and wherein the method further comprises</li>        <li id="ul0042-0002" num="1727">storing the area weighting factor in the template image; and</li>        <li id="ul0042-0003" num="1728">generating a modified version of each of the images with pixels having modified pixel intensity values;</li>        <li id="ul0042-0004" num="1729">processing modified versions of the images through the second neural network to generate an alternative representation of the modified versions; and</li>        <li id="ul0042-0005" num="1730">base calling the at least one particular cluster using the alternative representation.</li>    </ul>    </li>    <li id="ul0029-0015" num="1731">15. The computer-implemented method of clause 14, wherein the base calling further comprises:    <ul id="ul0043" list-style="none">        <li id="ul0043-0001" num="1732">accessing one or more images at the optical, pixel resolution in each        <ul id="ul0044" list-style="none">            <li id="ul0044-0001" num="1733">of a current image set generated at a current one of the plurality of sequencing cycles,</li>            <li id="ul0044-0002" num="1734">of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and</li>            <li id="ul0044-0003" num="1735">of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles;</li>        </ul>        </li>        <li id="ul0043-0002" num="1736">for pixels in each of the images, modifying a pixel intensity value based on the area weighting factor in the template image for a respective pixel;</li>        <li id="ul0043-0003" num="1737">generating a modified version of each of the images with pixels having modified pixel intensity values;</li>        <li id="ul0043-0004" num="1738">for the at least one particular cluster, extracting an image patch from each modified version such that each image patch has an array of pixels, and contains in its center pixel a center of the particular cluster identified in the template image;</li>        <li id="ul0043-0005" num="1739">convolving image patches extracted from modified versions of the images through a convolutional neural network of the second neural network to generate a convolved representation of the image patches;</li>        <li id="ul0043-0006" num="1740">processing the convolved representation through an output layer to produce, for the center pixel, likelihoods of a base incorporated in the at least one particular cluster at the current one of the plurality of sequencing cycles being A, C, T, and G; and</li>        <li id="ul0043-0007" num="1741">classifying the base as A, C, T, or G based on the likelihoods.</li>    </ul>    </li>    <li id="ul0029-0016" num="1742">16. The computer-implemented method any of clauses 14 or 15, further comprising:    <ul id="ul0045" list-style="none">        <li id="ul0045-0001" num="1743">prior to modifying the pixel intensity values, aligning each of the images captured at the optical, pixel resolution with the template image using cycle-specific and imaging channel-specific transformations.</li>    </ul>    </li>    <li id="ul0029-0017" num="1744">17. The computer-implemented method of clause 9, further comprising:    <ul id="ul0046" list-style="none">        <li id="ul0046-0001" num="1745">evaluating the template image in an upsampled subpixel domain to identify subpixels that contain parts of any cluster; and</li>        <li id="ul0046-0002" num="1746">assigning a background intensity to subpixels identified in the template image as not contributing to any cluster.</li>    </ul>    </li>    <li id="ul0029-0018" num="1747">18. The computer-implemented method of clause 17, wherein evaluating the template image in an upsampled subpixel domain further comprises:    <ul id="ul0047" list-style="none">        <li id="ul0047-0001" num="1748">calculating how many subpixels in at least one pixel contain parts of any cluster and calculating a per-subpixel area weighting factor for the subpixels in the at least one pixel.</li>    </ul>    </li>    <li id="ul0029-0019" num="1749">19. The computer-implemented method of clause 17 or 18, wherein the method comprises:    <ul id="ul0048" list-style="none">        <li id="ul0048-0001" num="1750">processing one or more initial image sets respectively generated at one or more initial sequencing cycles of a plurality of sequencing cycles through the first neural network to produce the template image at the upsampled, subpixel resolution, wherein each image set comprises one or more images, each of the images depicting intensity emissions of the clusters and their surrounding background in a respective one of one or more imaging channels captured at the optical, pixel resolution and wherein the template image classifies subpixels into classes including cluster center, background, and cluster interior;</li>        <li id="ul0048-0002" num="1751">upsampling each of the images captured at the optical, pixel resolution into a subpixel domain and assigning a background intensity to subpixels of each of the images identified in the template image as not contributing to any cluster;</li>        <li id="ul0048-0003" num="1752">processing the upsampled images through the second neural network to generate an alternative representation of the upsampled images; and</li>        <li id="ul0048-0004" num="1753">base calling a plurality of the clusters using the alternative representation.</li>    </ul>    </li>    <li id="ul0029-0020" num="1754">20. The computer-implemented method of clause 19, wherein upsampling each of the images further comprises:    <ul id="ul0049" list-style="none">        <li id="ul0049-0001" num="1755">distributing intensity of a particular pixel among first subpixels of the particular pixel identified in the template image as contributing to any cluster by applying the per-subpixel area weighting factor and assigning a background intensity to second subpixels of the particular pixel identified in the template as not contributing to any cluster.</li>    </ul>    </li>    <li id="ul0029-0021" num="1756">21. The computer-implemented method of clause 20, wherein the prior to upsampling the method comprises:    <ul id="ul0050" list-style="none">        <li id="ul0050-0001" num="1757">accessing one or more images at the optical, pixel resolution in each        <ul id="ul0051" list-style="none">            <li id="ul0051-0001" num="1758">of a current image set generated at a current one of the plurality of sequencing cycles,</li>            <li id="ul0051-0002" num="1759">of a one or more preceding image sets respectively generated at one or more of the plurality of sequencing cycles preceding the current one of the plurality of sequencing cycles, and</li>            <li id="ul0051-0003" num="1760">of a one or more succeeding image sets respectively generated at one or more of the plurality of sequencing cycles succeeding the current one of the plurality of sequencing cycles; and after upsampling the method comprises:</li>        </ul>        </li>        <li id="ul0050-0002" num="1761">extracting an image patch from each upsampled image such that each image patch has an array of subpixels;</li>        <li id="ul0050-0003" num="1762">convolving image patches extracted from the upsampled images through the convolutional neural network of the second neural network to generate a convolved representation of the image patches;</li>        <li id="ul0050-0004" num="1763">processing the convolved representation through an output layer to produce, for each subpixel in the array, likelihoods of a base incorporated at the current one of the plurality of sequencing cycles being A, C, T, and G;</li>        <li id="ul0050-0005" num="1764">classifying the base as A, C, T, or G based on the likelihoods; and</li>        <li id="ul0050-0006" num="1765">base calling each one of the plurality of the clusters based on a base classification assigned to a respective subpixel containing a center of a corresponding cluster.</li>    </ul>    </li>    <li id="ul0029-0022" num="1766">22. The computer-implemented method of clause 20 or 21, further comprising:    <ul id="ul0052" list-style="none">        <li id="ul0052-0001" num="1767">prior to the upsampling, aligning each of the images captured at the optical, pixel resolution with the template image using cycle-specific and imaging channel-specific transformations.</li>    </ul>    </li>    <li id="ul0029-0023" num="1768">23. A sequencing system, comprising:    <ul id="ul0053" list-style="none">        <li id="ul0053-0001" num="1769">a receptacle coupled to a biosensor system, the biosensor system configured to comprise an array of light detectors, the biosensor system comprising a biosensor, and the biosensor comprising reaction sites configured to contain clusters;</li>        <li id="ul0053-0002" num="1770">an illumination system configured to direct excitation light toward the biosensor and illuminate the clusters in the reaction sites, wherein at least some of the clusters provide emission signals when illuminated; and</li>        <li id="ul0053-0003" num="1771">a system controller coupled to the receptacle and comprising an analysis module, the analysis module configured to:        <ul id="ul0054" list-style="none">            <li id="ul0054-0001" num="1772">obtain image data from the light detectors at each of a plurality of sequencing cycles, wherein the image data is derived from the emission signals detected by the light detectors; and</li>            <li id="ul0054-0002" num="1773">process the image data for each of the plurality of sequencing cycles through a neural network and produce a base call for at least some of the clusters at each of the plurality of sequencing cycles.</li>        </ul>        </li>    </ul>    </li></ul></p><p id="p-1612" num="1774">Clauses Set 3<ul id="ul0055" list-style="none">    <li id="ul0055-0001" num="1775">1. A computer-implemented method, including:    <ul id="ul0056" list-style="none">        <li id="ul0056-0001" num="1776">processing input data through a neural network and producing an alternative representation of the input data, wherein the input data includes per-cycle data for each of one or more sequencing cycles of a sequencing run, and wherein the per-cycle data is indicative of one or more analytes at a respective sequencing cycle;</li>        <li id="ul0056-0002" num="1777">processing the alternative representation through an output layer and producing an output; and</li>        <li id="ul0056-0003" num="1778">base calling one or more of the analytes at one or more of the sequencing cycles based on the output</li>    </ul>    </li>    <li id="ul0055-0002" num="1779">2. The neural network-implemented method of clause 1, wherein the per-cycle data is indicative of a surrounding background at the respective sequencing cycle.</li>    <li id="ul0055-0003" num="1780">3. The neural network-implemented method of any of clauses 1-2, wherein the input data is image data and the per-cycle data comprises intensity emissions indicative of the one or more analytes and of the surrounding background captured at the respective sequencing cycle.</li>    <li id="ul0055-0004" num="1781">4. The computer-implemented method of clause 3, further including accompanying the per-cycle data with supplemental distance information that identifies distances between pixels of the per-cycle data and those pixels that depict the intensity emissions indicative of the one or more of the analytes.</li>    <li id="ul0055-0005" num="1782">5. The computer-implemented method of clause 3, further including accompanying the per-cycle data with supplemental scaling information that assigns scaling values to the pixels of the per-cycle data.</li>    <li id="ul0055-0006" num="1783">6. The neural network-implemented method of clause 1, wherein the per-cycle data is indicative of a voltage change detected at the respective sequencing cycle.</li>    <li id="ul0055-0007" num="1784">7. The neural network-implemented method of clause 1, wherein the per-cycle data is indicative of an electric current signal measured at the respective sequencing cycle.</li>    <li id="ul0055-0008" num="1785">8. A neural network-implemented method of base calling analytes synthesized during a sequencing run comprising a plurality of sequencing cycles, the method including:    <ul id="ul0057" list-style="none">        <li id="ul0057-0001" num="1786">convolving input data through a convolutional neural network to generate a convolved representation of the input data,        <ul id="ul0058" list-style="none">            <li id="ul0058-0001" num="1787">wherein the input data includes image patches extracted from one or more images in each of a current image set generated at a current sequencing cycle of the sequencing run, of one or more preceding image sets respectively generated at one or more sequencing cycles of the sequencing run preceding the current sequencing cycle, and of one or more succeeding image sets respectively generated at one or more sequencing cycles of the sequencing run succeeding the current sequencing cycle,</li>            <li id="ul0058-0002" num="1788">wherein each of the image patches depicts intensity emissions of a target analyte being base called, and</li>            <li id="ul0058-0003" num="1789">wherein the input data further includes distance information indicating respective distances of pixels of the image patch from a center pixel of the image patch;</li>        </ul>        </li>        <li id="ul0057-0002" num="1790">processing the convolved representation through an output layer to produce an output; and</li>        <li id="ul0057-0003" num="1791">base calling the target analyte at the current sequencing cycle based on the output.</li>    </ul>    </li>    <li id="ul0055-0009" num="1792">9. The neural network-implemented method of clause 8, further including:    <ul id="ul0059" list-style="none">        <li id="ul0059-0001" num="1793">providing as input to the convolutional neural network position coordinates of centers of image regions representing respective analytes,</li>        <li id="ul0059-0002" num="1794">wherein the input is provided to a first layer of the convolutional neural network,</li>        <li id="ul0059-0003" num="1795">wherein the input is provided to one or more intermediate layers of the convolutional neural network, and</li>        <li id="ul0059-0004" num="1796">wherein the input is provided to a final layer of the convolutional neural network.</li>    </ul>    </li>    <li id="ul0055-0010" num="1797">10. The neural network-implemented method of any of clauses 8-9, further including:    <ul id="ul0060" list-style="none">        <li id="ul0060-0001" num="1798">providing as input to the convolutional neural network an intensity scaling channel that has scaling values corresponding to pixels of the image patches, and</li>        <li id="ul0060-0002" num="1799">wherein the scaling values are based on a mean intensity of center pixels of the image patches that each contain a particular target analyte.</li>    </ul>    </li>    <li id="ul0055-0011" num="1800">11. The neural network-implemented method of any of clauses 8-10, wherein the intensity scaling channel pixel-wise includes a same scaling value for all the pixels of the image patches.</li>    <li id="ul0055-0012" num="1801">12. The neural network-implemented method of clause 8, wherein each image patch further comprises pixel distance data indicating a distance between respective pixels and a nearest one of the plurality of analytes, the nearest one of the plurality of analytes selected based on center-to-center distances between the pixel and each of the analytes.</li>    <li id="ul0055-0013" num="1802">13. The neural network-implemented method of clause 8, wherein each image patch further comprises analyte distance data that identifies a distance of each analyte pixel from an assigned one of the plurality of analytes selected based on classifying each analyte pixel to only one of the analytes.</li>    <li id="ul0055-0014" num="1803">14. The neural network-implemented method of any of clauses 8-13, wherein convolving the input data through the convolutional neural network to generate the convolved representation of the input data comprises:    <ul id="ul0061" list-style="none">        <li id="ul0061-0001" num="1804">separately processing each per-cycle image patch set through a first convolutional subnetwork of the convolutional neural network to produce an intermediate convolved representation for each sequencing cycle, including applying convolutions that combine the intensity and distance information and combine resulting convolved representations only within a sequencing cycle and not between sequencing cycles;</li>        <li id="ul0061-0002" num="1805">groupwise processing intermediate convolved representations for successive sequencing cycles in the series through a second convolutional subnetwork of the convolutional neural network to produce a final convolved representation for the series, including applying convolutions that combine the intermediate convolved representations and combine resulting convolved representations between the sequencing cycles;</li>        <li id="ul0061-0003" num="1806">and wherein processing the convolved representation through the output layer to produce the output comprises processing the final convolved representation through the output layer.</li>    </ul>    </li>    <li id="ul0055-0015" num="1807">15. The neural network-implemented method of any of clauses 8-14, further including: refraining the pixels of each image patch to center a center of the target analyte in a center pixel to generate reframed image patches; and    <ul id="ul0062" list-style="none">        <li id="ul0062-0001" num="1808">wherein convolving the input data through the convolutional neural network to generate the convolved representation of the input data comprises convolving the reframed image patches through the convolutional neural network to generate the convolved representation.</li>    </ul>    </li>    <li id="ul0055-0016" num="1809">16. The neural network-implemented method of clause 15, wherein the reframing further includes intensity interpolation of the pixels of each image patch to compensate for the reframing.</li>    <li id="ul0055-0017" num="1810">17. A neural network-implemented method of base calling, the method including:    <ul id="ul0063" list-style="none">        <li id="ul0063-0001" num="1811">separately processing each per-cycle input data in a sequence of per-cycle input data through a cascade of convolution layers of the convolutional neural network, wherein        <ul id="ul0064" list-style="none">            <li id="ul0064-0001" num="1812">the sequence of per-cycle input data is generated for a series of sequencing cycles of a sequencing run, and</li>            <li id="ul0064-0002" num="1813">each per-cycle input data includes image channels that depict intensity emissions of one or more analytes and their surrounding background captured at a respective sequencing cycle;</li>        </ul>        </li>        <li id="ul0063-0002" num="1814">for each sequencing cycle,        <ul id="ul0065" list-style="none">            <li id="ul0065-0001" num="1815">based on the separate processing, producing a convolved representation at each of the convolution layers, thereby producing a sequence of convolved representations,</li>            <li id="ul0065-0002" num="1816">mixing its per-cycle input data with its corresponding sequence of convolved representations and producing a mixed representation, and</li>            <li id="ul0065-0003" num="1817">flattening its mixed representation and producing a flattened mixed representation;</li>        </ul>        </li>        <li id="ul0063-0003" num="1818">arranging flattened mixed representations of successive sequencing cycles as a stack;</li>        <li id="ul0063-0004" num="1819">processing the stack in forward and backward directions through a recurrent neural network that        <ul id="ul0066" list-style="none">            <li id="ul0066-0001" num="1820">convolves over a subset of the flattened mixed representations in the stack on a sliding window basis, with each sliding window corresponding to a respective sequencing cycle, and</li>            <li id="ul0066-0002" num="1821">successively produces a current hidden state representation at each time step for each sequencing cycle based on (i) the subset of the flattened mixed representations in a current sliding window over the stack and (ii) a previous hidden state representation; and</li>        </ul>        </li>        <li id="ul0063-0005" num="1822">base calling each of the analytes at each of the sequencing cycles based on results of processing the stack in forward and backward directions.</li>    </ul>    </li>    <li id="ul0055-0018" num="1823">18. The neural network-implemented method of clause 17, further including:    <ul id="ul0067" list-style="none">        <li id="ul0067-0001" num="1824">base calling each of the analytes at a given sequencing cycle by:        <ul id="ul0068" list-style="none">            <li id="ul0068-0001" num="1825">combining forward and backward current hidden state representations of the given sequencing cycle on a time step-basis and producing a combined hidden state representation, wherein the combining includes concatenation or summation or averaging;</li>            <li id="ul0068-0002" num="1826">processing the combined hidden state representation through one or more fully-connected networks and producing a dense representation;</li>            <li id="ul0068-0003" num="1827">processing the dense representation through a softmax layer to produce likelihoods of bases incorporated in each of the analytes at the given sequencing cycle being A, C, T, and G; and</li>            <li id="ul0068-0004" num="1828">classifying the bases as A, C, T, or G based on the likelihoods.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0055-0019" num="1829">19. A neural network-based system for base calling, the system comprising:    <ul id="ul0069" list-style="none">        <li id="ul0069-0001" num="1830">a hybrid neural network with a recurrent module and a convolution module, wherein the recurrent module uses inputs from the convolution module;</li>        <li id="ul0069-0002" num="1831">the convolution module processing image data for a series of sequencing cycles of a sequencing run through one or more convolution layers and producing one or more convolved representations of the image data, wherein the image data depicts intensity emissions of one or more analytes and their surrounding background;</li>        <li id="ul0069-0003" num="1832">the recurrent module producing current hidden state representations based on convolving the convolved representations and previous hidden state representations; and</li>        <li id="ul0069-0004" num="1833">an output module producing a base call for at least one of the analytes and for at least one of the sequencing cycles based on the current hidden state representations.</li>    </ul>    </li>    <li id="ul0055-0020" num="1834">20. A computer-implemented method of base calling clusters, including:    <ul id="ul0070" list-style="none">        <li id="ul0070-0001" num="1835">processing input data through a neural network and producing an alternative representation of the input data,        <ul id="ul0071" list-style="none">            <li id="ul0071-0001" num="1836">wherein the input data includes (i) per-cycle data for each of one or more sequencing cycles of a sequencing run and (ii) supplemental distance information,</li>            <li id="ul0071-0002" num="1837">wherein the per-cycle data comprises pixels that depict intensity emissions indicative of the one or more clusters and of the surrounding background captured at a respective one of the sequencing cycles,</li>            <li id="ul0071-0003" num="1838">wherein the per-cycle data is accompanied with the supplemental distance information that identifies distances between the pixels of the per-cycle data;</li>            <li id="ul0071-0004" num="1839">wherein, during the processing of the pixels of the per-cycle data by the neural network, the supplemental distance information supplies additive bias that conveys to the neural network which of the pixels of the per-cycle data contain centers of the clusters and which of the pixels of the per-cycle data are farther away from the centers of the clusters;</li>        </ul>        </li>        <li id="ul0070-0002" num="1840">processing the alternative representation through an output layer and producing an output; and</li>        <li id="ul0070-0003" num="1841">base calling one or more of the clusters at one or more of the sequencing cycles based on the output.</li>    </ul>    </li>    <li id="ul0055-0021" num="1842">21. The computer-implemented method of clause 20, wherein the additive bias improves accuracy of the base calling.</li>    <li id="ul0055-0022" num="1843">22. The computer-implemented method of clause 21, wherein the neural network uses the supplemental distance information to assign a sequencing signal to its proper source cluster by attending to central cluster pixels, their neighboring pixels, and alternative representations derived from them more than perimeter cluster pixels, background pixels, and alternative representations derived from them.</li></ul></p><p id="p-1613" num="1844">Clauses Set 4<ul id="ul0072" list-style="none">    <li id="ul0072-0001" num="1845">1. A computer-implemented method, including:    <ul id="ul0073" list-style="none">        <li id="ul0073-0001" num="1846">processing input data for one or more analytes through a neural network-based base caller and producing an alternative representation of the input data;</li>        <li id="ul0073-0002" num="1847">processing the alternative representation through an output layer to produce an output, wherein the output identifies likelihoods of a base incorporated in a particular one of the analytes being A, C, T, and G;</li>        <li id="ul0073-0003" num="1848">calling bases for one or more of the analytes based on the output; and</li>        <li id="ul0073-0004" num="1849">determining quality scores of the called bases based on the likelihoods identified by the output</li>    </ul>    </li>    <li id="ul0072-0002" num="1850">2. The computer-implemented method of clause 1, wherein determining the quality scores of the called bases based on the likelihoods comprises:    <ul id="ul0074" list-style="none">        <li id="ul0074-0001" num="1851">quantizing classification scores of base calls produced by the neural network-based base caller in response to processing training data during training;</li>        <li id="ul0074-0002" num="1852">selecting a set of quantized classification scores;</li>        <li id="ul0074-0003" num="1853">for each quantized classification score in the set, determining a base calling error rate by comparing its predicted base calls to corresponding ground truth base calls;</li>        <li id="ul0074-0004" num="1854">determining a fit between the quantized classification scores and their base calling error rates; and</li>        <li id="ul0074-0005" num="1855">correlating the quality scores to the quantized classification scores based on the fit.</li>    </ul>    </li>    <li id="ul0072-0003" num="1856">3. The computer-implemented method of any of clauses 1-2, wherein the set of quantized classification scores includes a subset of the classification scores of predicted base calls produced by the neural network-based base caller in response to processing the training data during the training, and wherein the classification scores are real numbers.</li>    <li id="ul0072-0004" num="1857">4. The computer-implemented method of any of clauses 1-3, wherein the set of quantized classification scores includes all the classification scores of predicted base calls produced by the neural network-based base caller in response to processing the training data during the training, and wherein the classification scores are real numbers.</li>    <li id="ul0072-0005" num="1858">5. The computer-implemented method of any of clauses 1-4, wherein the classification scores are exponentially normalized softmax scores that sum to unity and are produced by a softmax output layer of the neural network-based base caller.</li>    <li id="ul0072-0006" num="1859">6. The computer-implemented method of any of clauses 1-5, wherein the set of quantized classification scores is selected based on a selection formula defined as</li></ul></p><p id="p-1614" num="0000"><maths id="MATH-US-00010" num="00010"><math overflow="scroll"> <mrow>  <mrow>   <mn>0</mn>   <mo>.</mo>   <mn>9</mn>  </mrow>  <mo>&#x2062;</mo>  <mrow>   <munderover>    <mo>&#x2211;</mo>    <mrow>     <mi>i</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>    <mi>n</mi>   </munderover>   <mrow>    <mn>0</mn>    <mo>.</mo>    <msup>     <mn>1</mn>     <mrow>      <mo>(</mo>      <mrow>       <mi>i</mi>       <mo>-</mo>       <mn>1</mn>      </mrow>      <mo>)</mo>     </mrow>    </msup>   </mrow>  </mrow> </mrow></math></maths><ul id="ul0075" list-style="none">    <li id="ul0075-0001" num="0000">&#x2003;and applied to the softmax scores.</li>    <li id="ul0075-0002" num="1860">7. The computer-implemented method of any of clauses 1-6, wherein the set of quantized classification scores is selected based on a selection formula defined as</li></ul></p><p id="p-1615" num="0000"><maths id="MATH-US-00011" num="00011"><math overflow="scroll"> <mrow>  <mover>   <munder>    <mo>&#x2200;</mo>    <mrow>     <mi>i</mi>     <mo>=</mo>     <mn>1</mn>    </mrow>   </munder>   <mrow>    <mi>n</mi>    <mo>=</mo>    <mn>10</mn>   </mrow>  </mover>  <mrow>   <mn>0.1</mn>   <mi>i</mi>  </mrow> </mrow></math></maths><ul id="ul0076" list-style="none">    <li id="ul0076-0001" num="0000">&#x2003;and applied to the softmax scores.</li>    <li id="ul0076-0002" num="1861">8. The computer-implemented method of any of clauses 1-7, further including:    <ul id="ul0077" list-style="none">        <li id="ul0077-0001" num="1862">based on the correlation, assigning the quality scores to bases called by the neural network-based base caller during inference.</li>    </ul>    </li>    <li id="ul0076-0003" num="1863">9. The computer-implemented method of clause 8, further including:    <ul id="ul0078" list-style="none">        <li id="ul0078-0001" num="1864">assigning the quality scores based on applying a quality score correspondence scheme to the bases called by the neural network-based base caller during the inference; and</li>        <li id="ul0078-0002" num="1865">wherein the scheme maps ranges of classification scores, produced by the neural network-based base caller in response to processing inference data during the inference, to corresponding quantized classification scores in the set.</li>    </ul>    </li>    <li id="ul0076-0004" num="1866">10. The computer-implemented method of any of clauses 8-9, further including:    <ul id="ul0079" list-style="none">        <li id="ul0079-0001" num="1867">during the inference, stopping base calling an analyte whose quality score is below a set threshold for a current base calling cycle.</li>    </ul>    </li>    <li id="ul0076-0005" num="1868">11. The computer-implemented method of any of clauses 8-10, further including:    <ul id="ul0080" list-style="none">        <li id="ul0080-0001" num="1869">during the inference, stopping base calling an analyte whose average quality score is below a set threshold after successive base calling cycles.</li>    </ul>    </li>    <li id="ul0076-0006" num="1870">12. The computer-implemented method of any of clauses 8-11, wherein a sample size used for comparing the predicted base calls to the corresponding ground truth base calls is specific to each quantized classification score.</li>    <li id="ul0076-0007" num="1871">13. The computer-implemented method of any of clauses 8-12, wherein the fit is determined using a regression model.</li>    <li id="ul0076-0008" num="1872">14. The computer-implemented method of any of clauses 8-13, further including:    <ul id="ul0081" list-style="none">        <li id="ul0081-0001" num="1873">for each quantized classification score, determining a base calling accuracy rate by comparing its predicted base calls to corresponding ground truth base calls; and</li>        <li id="ul0081-0002" num="1874">determining the fit between the quantized classification scores and their base calling accuracy rates.</li>    </ul>    </li>    <li id="ul0076-0009" num="1875">15. The computer-implemented method of any of clauses 8-14, wherein the corresponding ground truth base calls are derived from well-characterized human and non-human samples sequenced on a number of sequencing instruments, sequencing chemistries, and sequencing protocols.</li>    <li id="ul0076-0010" num="1876">16. A neural network-based quality scorer, comprising:    <ul id="ul0082" list-style="none">        <li id="ul0082-0001" num="1877">numerous processors operating in parallel and coupled to memory;</li>        <li id="ul0082-0002" num="1878">a neural network running on the numerous processors, trained on training examples comprising data from sequencing images and labeled with base call quality ground truths using a backpropagation-based gradient update technique that progressively matches base call quality predictions of the neural network with the base call quality ground truths that identify known correct base calls;</li>        <li id="ul0082-0003" num="1879">an input module of the neural network which runs on at least one of the numerous processors and feeds data from sequencing images captured at one or more sequencing cycles to the neural network for determining quality of one or more bases called for one or more analytes; and</li>        <li id="ul0082-0004" num="1880">an output module of the neural network which runs on at least one of the numerous processors and translates analysis by the neural network into an output that identifies the quality of the one or more bases called for the one or more analytes.</li>    </ul>    </li>    <li id="ul0076-0011" num="1881">17. The neural network-based quality scorer of clause 16, wherein the neural network is a convolutional neural network.</li>    <li id="ul0076-0012" num="1882">18. The neural network-based quality scorer of clause 16, wherein the output module further comprises a softmax classification layer that produces likelihoods for the quality being high-quality, medium-quality, and low-quality, further comprising:    <ul id="ul0083" list-style="none">        <li id="ul0083-0001" num="1883">based on the likelihoods, classifying the quality as high-quality, medium-quality, or low-quality.</li>    </ul>    </li>    <li id="ul0076-0013" num="1884">19. The neural network-based quality scorer of clause 16, wherein the softmax classification layer produces likelihoods for the quality being assigned a plurality of quality scores, further comprising:    <ul id="ul0084" list-style="none">        <li id="ul0084-0001" num="1885">based on the likelihoods, assigning the quality a quality score from one of the plurality of quality scores.</li>    </ul>    </li>    <li id="ul0076-0014" num="1886">20. The neural network-based quality scorer of any of clauses 16-19, wherein the quality scores are logarithmically based on base calling error probabilities, and    <ul id="ul0085" list-style="none">        <li id="ul0085-0001" num="1887">wherein the plurality of quality scores includes Q6, Q10, Q15, Q20, Q22, Q27, Q30, Q33, Q37, Q40, and Q50.</li>    </ul>    </li>    <li id="ul0076-0015" num="1888">21. The neural network-based quality scorer of any of clauses 16-20, wherein the output module further comprises a regression layer that produces continuous values which identify the quality.</li>    <li id="ul0076-0016" num="1889">22. The neural network-based quality scorer of any of clauses 16-21, further comprising:    <ul id="ul0086" list-style="none">        <li id="ul0086-0001" num="1890">a supplemental input module that supplements the data from the sequencing images with quality predictor values for the bases called, and feeds the quality predictor values to the convolutional neural network along with the data from the sequencing images.</li>    </ul>    </li>    <li id="ul0076-0017" num="1891">23. The neural network-based quality scorer of clause 22, wherein the quality predictor values include online overlap, purity, phasing, start5, hexamer score, motif accumulation, endiness, approximate homopolymer, intensity decay, penultimate chastity, signal overlap with background (SOWB), and/or shifted purity G adjustment.</li>    <li id="ul0076-0018" num="1892">24. The neural network-based quality scorer of clause 22, wherein the quality predictor values include peak height, peak width, peak location, relative peak locations, peak height ration, peak spacing ration, and/or peak correspondence.</li></ul></p><p id="p-1616" num="1893">Clauses Set 5<ul id="ul0087" list-style="none">    <li id="ul0087-0001" num="1894">1. A computer-implemented method of determining image regions indicative of analytes on a tile of a flow cell, the method comprising:    <ul id="ul0088" list-style="none">        <li id="ul0088-0001" num="1895">accessing a series of image sets generated during a sequencing run, each image set in the series generated during a respective sequencing cycle of the sequencing run, each image in the series depicting the analytes and their surrounding background, and each image in the series having a plurality of subpixels;</li>        <li id="ul0088-0002" num="1896">obtaining, from a base caller, a base call classifying each of the subpixels, thereby producing a base call sequence for each of the subpixels across a plurality of sequencing cycles of the sequencing run;</li>        <li id="ul0088-0003" num="1897">determining a plurality of disjointed regions of contiguous subpixels which share a substantially matching base call sequence; and</li>        <li id="ul0088-0004" num="1898">generating an analyte map identifying the determined disjointed regions.</li>    </ul>    </li>    <li id="ul0087-0002" num="1899">2. The computer-implemented method of clause 1, further including:    <ul id="ul0089" list-style="none">        <li id="ul0089-0001" num="1900">training a classifier based upon the determined plurality of disjointed regions of contiguous subpixels, the classifier being a neural network-based template generator for processing input image data to generate a decay map, a ternary map, or a binary map, representing one or more properties of each of a plurality of analytes represented in the input image data for base calling by a neural network-based base caller,</li>        <li id="ul0089-0002" num="1901">preferably in order to increase the level of throughput in high-throughput nucleic acid sequencing technologies.</li>    </ul>    </li>    <li id="ul0087-0003" num="1902">3. The computer-implemented method of any of clauses 1-2, further including:    <ul id="ul0090" list-style="none">        <li id="ul0090-0001" num="1903">generating the analyte map by identifying as background those subpixels that do not belong to any of the disjointed regions.</li>    </ul>    </li>    <li id="ul0087-0004" num="1904">4. The computer-implemented method of any of clauses 1-3, wherein the analyte map identifies analyte boundary portions between two contiguous subpixels whose base call sequences do not substantially match.</li>    <li id="ul0087-0005" num="1905">5. The computer-implemented method of any of clauses 1-4, wherein the determining the plurality of disjointed regions of contiguous subpixels further includes:    <ul id="ul0091" list-style="none">        <li id="ul0091-0001" num="1906">identifying origin subpixels at preliminary center coordinates of the analytes determined by the base caller; and</li>        <li id="ul0091-0002" num="1907">breadth-first searching for substantially matching base call sequences by beginning with the origin subpixels and continuing with successively contiguous non-origin subpixels.</li>    </ul>    </li>    <li id="ul0087-0006" num="1908">6. The computer-implemented method of any of clauses 1-5, further including:    <ul id="ul0092" list-style="none">        <li id="ul0092-0001" num="1909">determining hyperlocated center coordinates of the analytes by calculating centers of mass of the disjointed regions of the analyte map as an average of coordinates of respective contiguous subpixels forming the disjointed regions; and</li>        <li id="ul0092-0002" num="1910">storing the hyperlocated center coordinates of the analytes in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0007" num="1911">7. The computer-implemented method of clause 6, further including:    <ul id="ul0093" list-style="none">        <li id="ul0093-0001" num="1912">identifying centers of mass subpixels in the disjointed regions of the analyte map at the hyperlocated center coordinates of the analytes;</li>        <li id="ul0093-0002" num="1913">upsampling the analyte map using interpolation and storing the upsampled analyte map in the memory for use as ground truth for training the classifier; and</li>        <li id="ul0093-0003" num="1914">in the upsampled analyte map, assigning a value to each contiguous subpixel in the disjointed regions based on a decay factor that is proportional to distance of a contiguous subpixel from a center of mass subpixel in a disjointed region to which the contiguous subpixel belongs.</li>    </ul>    </li>    <li id="ul0087-0008" num="1915">8. The computer-implemented method of clause 7, the method more preferably further including:    <ul id="ul0094" list-style="none">        <li id="ul0094-0001" num="1916">generating the decay map from the upsampled analyte map that expresses the contiguous subpixels in the disjointed regions and the subpixels identified as the background based on their assigned values; and</li>        <li id="ul0094-0002" num="1917">storing the decay map in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0009" num="1918">9. The computer-implemented method of clause 8, the method even more preferably further including:    <ul id="ul0095" list-style="none">        <li id="ul0095-0001" num="1919">in the upsampled analyte map, categorizing, on the analyte-by-analyte basis, the contiguous subpixels in the disjointed regions as analyte interior subpixels belonging to a same analyte, the centers of mass subpixels as analyte center subpixels, subpixels containing the analyte boundary portions as boundary subpixels, and the subpixels identified as the background as background subpixels; and</li>        <li id="ul0095-0002" num="1920">storing the categorizations in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0010" num="1921">10. The computer-implemented method of any of clauses 1-9, further including:    <ul id="ul0096" list-style="none">        <li id="ul0096-0001" num="1922">storing, on the analyte-by-analyte basis, coordinates of the analyte interior subpixels, the analyte center subpixels, the boundary subpixels, and the background subpixels in the memory for use as ground truth for training the classifier;</li>        <li id="ul0096-0002" num="1923">downscaling the coordinates by a factor used to upsample the analyte map; and</li>        <li id="ul0096-0003" num="1924">storing, on the analyte-by-analyte basis, the downscaled coordinates in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0011" num="1925">11. The computer-implemented method of any of clauses 1-10, further including:    <ul id="ul0097" list-style="none">        <li id="ul0097-0001" num="1926">in a binary ground truth data generated from the upsampled analyte map, using color coding to label the analyte center subpixels as belonging to an analyte center class and all other subpixels are belonging to a non-center class; and</li>        <li id="ul0097-0002" num="1927">storing the binary ground truth data in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0012" num="1928">12. The computer-implemented method of any of clauses 1-11, further including:    <ul id="ul0098" list-style="none">        <li id="ul0098-0001" num="1929">in a ternary ground truth data generated from the upsampled analyte map, using color coding to label the background subpixels as belonging to a background class, the analyte center subpixels as belonging to an analyte center class, and the analyte interior subpixels as belonging to an analyte interior class; and</li>        <li id="ul0098-0002" num="1930">storing the ternary ground truth data in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0013" num="1931">13. The computer-implemented method of any of clauses 1-12, further including:    <ul id="ul0099" list-style="none">        <li id="ul0099-0001" num="1932">generating analyte maps for a plurality of tiles of the flow cell;</li>        <li id="ul0099-0002" num="1933">storing the analyte maps in memory and determining spatial distribution of analytes in the tiles based on the analyte maps, including their shapes and sizes;</li>        <li id="ul0099-0003" num="1934">in the upsampled analyte maps of the analytes in the tiles, categorizing, on an analyte-by-analyte basis, subpixels as analyte interior subpixels belonging to a same analyte, analyte center subpixels, boundary subpixels, and background subpixels;</li>        <li id="ul0099-0004" num="1935">storing the categorizations in the memory for use as ground truth for training the classifier;</li>        <li id="ul0099-0005" num="1936">storing, on the analyte-by-analyte basis across the tiles, coordinates of the analyte interior subpixels, the analyte center subpixels, the boundary subpixels, and the background subpixels in the memory for use as ground truth for training the classifier;</li>        <li id="ul0099-0006" num="1937">downscaling the coordinates by the factor used to upsample the analyte map; and</li>        <li id="ul0099-0007" num="1938">storing, on the analyte-by-analyte basis across the tiles, the downscaled coordinates in the memory for use as ground truth for training the classifier.</li>    </ul>    </li>    <li id="ul0087-0014" num="1939">14. The computer-implemented method of any of clauses 1-13, wherein the base call sequences are substantially matching when a predetermined portion of base calls match on an ordinal position-wise basis.</li>    <li id="ul0087-0015" num="1940">15. The computer-implemented method of any of clauses 1-14, wherein the determining the plurality of disjointed regions of contiguous subpixels which share a substantially matching base call sequence is based upon a predetermined minimum number of subpixels for a disjointed region.</li>    <li id="ul0087-0016" num="1941">16. The computer-implemented method of any of clauses 1-15, wherein the flow cell has at least one patterned surface with an array of wells that occupy the analytes, further including:    <ul id="ul0100" list-style="none">        <li id="ul0100-0001" num="1942">based on the determined shapes and sizes of the analytes, determining        <ul id="ul0101" list-style="none">            <li id="ul0101-0001" num="1943">which ones of the wells are substantially occupied by at least one analyte,</li>            <li id="ul0101-0002" num="1944">which ones of the wells are minimally occupied, and</li>            <li id="ul0101-0003" num="1945">which ones of the wells are co-occupied by multiple analytes.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0087-0017" num="1946">17. A computer-implemented method of determining metadata about analytes on a tile of a flow cell, the method comprising:    <ul id="ul0102" list-style="none">        <li id="ul0102-0001" num="1947">accessing a set of images of the tile captured during a sequencing run and preliminary center coordinates of the analytes determined by a base caller;</li>        <li id="ul0102-0002" num="1948">for each image set, obtaining, from a base caller, a base call classifying, as one of four bases,        <ul id="ul0103" list-style="none">            <li id="ul0103-0001" num="1949">origin subpixels that contain the preliminary center coordinates and</li>            <li id="ul0103-0002" num="1950">a predetermined neighborhood of contiguous subpixels that are successively contiguous to respective ones of the origin subpixels,</li>            <li id="ul0103-0003" num="1951">thereby producing a base call sequence for each of the origin subpixels and for each of the predetermined neighborhood of contiguous subpixels;</li>        </ul>        </li>        <li id="ul0102-0003" num="1952">generating an analyte map that identifies the analytes as disjointed regions of contiguous subpixels that        <ul id="ul0104" list-style="none">            <li id="ul0104-0001" num="1953">are successively contiguous to at least some of the respective ones of the origin subpixels and</li>            <li id="ul0104-0002" num="1954">share a substantially matching base call sequence of the one of four bases with the at least some of the respective ones of the origin subpixels; and</li>        </ul>        </li>        <li id="ul0102-0004" num="1955">storing the analyte map in memory and determining the shapes and the sizes of the analytes based on the disjointed regions in the analyte map.</li>    </ul>    </li>    <li id="ul0087-0018" num="1956">18. A computer-implemented method of generating training data for neural network-based template generation and base calling, the method comprising:    <ul id="ul0105" list-style="none">        <li id="ul0105-0001" num="1957">accessing a multitude of images of a flow cell captured over a plurality of cycles of a sequencing run, the flow cell having a plurality of tiles and, in the multitude of images, each of the tiles having a sequence of image sets generated over the plurality of cycles, and each image in the sequence of image sets depicting intensity emissions of analytes and their surrounding background on a particular one of the tiles at a particular one the cycles;</li>        <li id="ul0105-0002" num="1958">constructing a training set having a plurality of training examples, each training example corresponding to a particular one of the tiles and including image data from at least some image sets in the sequence of image sets of the particular one of the tiles; and</li>        <li id="ul0105-0003" num="1959">generating at least one ground truth data representation for each of the training examples, the ground truth data representation identifying at least one property of analytes on the particular one of the tiles whose intensity emissions are depicted by the image data and being determined at least in part using the method of any of clauses 1-17.</li>    </ul>    </li>    <li id="ul0087-0019" num="1960">19. The computer-implemented method of clause 18, wherein the at least one property of analytes is selected from the group consisting of: spatial distribution of analytes on the tile; analyte shape; analyte size; analyte boundary; and center of contiguous regions including a single analyte.</li>    <li id="ul0087-0020" num="1961">20. The computer-implemented method of any of clauses 18-19, wherein the image data includes images in each of the at least some image sets in the sequence of image sets of the particular one of the tiles.</li>    <li id="ul0087-0021" num="1962">21. The computer-implemented method of any of clauses 18-20, wherein the image data includes at least one image patch from each of the images.</li>    <li id="ul0087-0022" num="1963">22. The computer-implemented method of any of clauses 18-21, wherein the image data includes an upsampled representation of the image patch.</li>    <li id="ul0087-0023" num="1964">23. The computer-implemented method of any of clauses 18-22, wherein multiple training examples correspond to a same particular one of the tiles and respectively include as image data different image patches from each image in each of at least some image sets in a sequence of image sets of the same particular one of the tiles, and wherein at least some of the different image patches overlap with each other.</li>    <li id="ul0087-0024" num="1965">24. The computer-implemented method of any of clauses 18-23, wherein the ground truth data representation identifies the analytes as disjoint regions of adjoining subpixels, the centers of the analytes as centers of mass subpixels within respective ones of the disjoint regions, and their surrounding background as subpixels that do not belong to any of the disjoint regions.</li>    <li id="ul0087-0025" num="1966">25. The computer-implemented method of any of clauses 18-24, further including:    <ul id="ul0106" list-style="none">        <li id="ul0106-0001" num="1967">storing, in memory, the training examples in the training set and associated ground truth data representations as the training data for the neural network-based template generation and base calling.</li>    </ul>    </li>    <li id="ul0087-0026" num="1968">26. A computer-implemented method, including:    <ul id="ul0107" list-style="none">        <li id="ul0107-0001" num="1969">accessing sequencing images of analytes produced by a sequencer;</li>        <li id="ul0107-0002" num="1970">generating training data from the sequencing images; and</li>        <li id="ul0107-0003" num="1971">using the training data for training a neural network to generate metadata about the analytes.</li>    </ul>    </li>    <li id="ul0087-0027" num="1972">27. A computer-implemented method, including:    <ul id="ul0108" list-style="none">        <li id="ul0108-0001" num="1973">accessing sequencing images of analytes produced by a sequencer;</li>        <li id="ul0108-0002" num="1974">generating training data from the sequencing images; and</li>        <li id="ul0108-0003" num="1975">using the training data for training a neural network to base call the analytes.</li>    </ul>    </li>    <li id="ul0087-0028" num="1976">28. A computer-implemented method of determining image regions indicative of analytes on a tile of a flow cell, the method comprising:    <ul id="ul0109" list-style="none">        <li id="ul0109-0001" num="1977">accessing a series of image sets generated during a sequencing run, each image set in the series generated during a respective sequencing cycle of the sequencing run, each image in the series depicting the analytes and their surrounding background, and each image in the series having a plurality of subpixels;</li>        <li id="ul0109-0002" num="1978">obtaining, from a base caller, a base call classifying each of the subpixels, thereby producing a base call sequence for each of the subpixels across a plurality of sequencing cycles of the sequencing run; and</li>        <li id="ul0109-0003" num="1979">determining a plurality of disjointed regions of contiguous subpixels which share a substantially matching base call sequence.</li>    </ul>    </li></ul></p><p id="p-1617" num="1980">Clauses Set 6<ul id="ul0110" list-style="none">    <li id="ul0110-0001" num="1981">1. A computer-implemented method of generating ground truth training data to train a neural network-based template generator for cluster metadata determination task, the method comprising:    <ul id="ul0111" list-style="none">        <li id="ul0111-0001" num="1982">accessing a series of image sets generated during a sequencing run, each image set in the series generated during a respective sequencing cycle of the sequencing run, each image in the series depicting clusters and their surrounding background, each image in the series having pixels in a pixel domain, and each of the pixels is divided into a plurality of subpixels in a subpixel domain;</li>        <li id="ul0111-0002" num="1983">obtaining, from a base caller, a base call classifying each of the subpixels as one of four bases (A, C, T, and G), thereby producing a base call sequence for each of the subpixels across a plurality of sequencing cycles of the sequencing run;</li>        <li id="ul0111-0003" num="1984">generating a cluster map that identifies the clusters as disjointed regions of contiguous subpixels which share a substantially matching base call sequence;</li>        <li id="ul0111-0004" num="1985">determining cluster metadata based on the disjointed regions in the cluster map,        <ul id="ul0112" list-style="none">            <li id="ul0112-0001" num="1986">wherein the cluster metadata includes cluster centers, cluster shapes, cluster sizes, cluster background, and/or cluster boundaries; and using the cluster metadata to generate ground truth training data for training a neural network-based template generator for cluster metadata determination task,</li>            <li id="ul0112-0002" num="1987">wherein the ground truth training data comprises a decay map, a ternary map, or a binary map,</li>            <li id="ul0112-0003" num="1988">wherein the neural network-based template generator is trained to produce the decay map, the ternary map, or the binary map as output based on the ground truth training data, and</li>            <li id="ul0112-0004" num="1989">wherein, upon execution of the cluster metadata determination task during inference, the cluster metadata is in turn determined from the decay map, the ternary map, or the binary map that are produced as the output by the trained neural network-based template generator.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0110-0002" num="1990">2. The computer-implemented method of clause 1, further including:    <ul id="ul0113" list-style="none">        <li id="ul0113-0001" num="1991">using the cluster metadata derived from the decay map, the ternary map, or the binary map produced as the output by the neural network-based template generator for base calling by a neural network-based base caller, in order to increase throughput in high-throughput nucleic acid sequencing technologies.</li>    </ul>    </li>    <li id="ul0110-0003" num="1992">3. The computer-implemented method of clause 1, further including:    <ul id="ul0114" list-style="none">        <li id="ul0114-0001" num="1993">generating the cluster map by identifying as background those subpixels that do not belong to any of the disjointed regions.</li>    </ul>    </li>    <li id="ul0110-0004" num="1994">4. The computer-implemented method of clause 1, wherein the cluster map identifies cluster boundary portions between two contiguous subpixels whose base call sequences do not substantially match.</li>    <li id="ul0110-0005" num="1995">5. The computer-implemented method of clause 1, wherein the cluster map is generated based on:    <ul id="ul0115" list-style="none">        <li id="ul0115-0001" num="1996">identifying origin subpixels at preliminary center coordinates of the clusters determined by the base caller; and</li>        <li id="ul0115-0002" num="1997">breadth-first searching for substantially matching base call sequences by beginning with the origin subpixels and continuing with successively contiguous non-origin subpixels.</li>    </ul>    </li>    <li id="ul0110-0006" num="1998">6. The computer-implemented method of clause 1, further including:    <ul id="ul0116" list-style="none">        <li id="ul0116-0001" num="1999">determining hyperlocated center coordinates of the clusters by calculating centers of mass of the disjointed regions of the cluster map as an average of coordinates of respective contiguous subpixels forming the disjointed regions; and</li>        <li id="ul0116-0002" num="2000">storing the hyperlocated center coordinates of the clusters in the memory for use as the ground truth training data for training the neural network-based template generator.</li>    </ul>    </li>    <li id="ul0110-0007" num="2001">7. The computer-implemented method of clause 6, further including:    <ul id="ul0117" list-style="none">        <li id="ul0117-0001" num="2002">identifying centers of mass subpixels in the disjointed regions of the cluster map at the hyperlocated center coordinates of the clusters;</li>        <li id="ul0117-0002" num="2003">upsampling the cluster map using interpolation and storing the upsampled cluster map in the memory for use as the ground truth training data for training the neural network-based template generator; and</li>        <li id="ul0117-0003" num="2004">in the upsampled cluster map, assigning a value to each contiguous subpixel in the disjointed regions based on a decay factor that is proportional to distance of a contiguous subpixel from a center of mass subpixel in a disjointed region to which the contiguous subpixel belongs.</li>    </ul>    </li>    <li id="ul0110-0008" num="2005">8. The computer-implemented method of clause 7, further including:    <ul id="ul0118" list-style="none">        <li id="ul0118-0001" num="2006">generating the decay map from the upsampled cluster map that expresses the contiguous subpixels in the disjointed regions and the subpixels identified as the background based on their assigned values; and</li>        <li id="ul0118-0002" num="2007">storing the decay map in the memory for use as the ground truth training data for training the neural network-based template generator.</li>    </ul>    </li>    <li id="ul0110-0009" num="2008">9. The computer-implemented method of clause 8, further including:    <ul id="ul0119" list-style="none">        <li id="ul0119-0001" num="2009">in the upsampled cluster map, categorizing, on the cluster-by-cluster basis, the contiguous subpixels in the disjointed regions as cluster interior subpixels belonging to a same cluster, the centers of mass subpixels as cluster center subpixels, subpixels containing the cluster boundary portions as boundary subpixels, and the subpixels identified as the background as background subpixels; and</li>        <li id="ul0119-0002" num="2010">storing the categorizations in the memory for use as the ground truth training data for training the neural network-based template generator.</li>    </ul>    </li>    <li id="ul0110-0010" num="2011">10. The computer-implemented method of clause 9, further including:    <ul id="ul0120" list-style="none">        <li id="ul0120-0001" num="2012">storing, on the cluster-by-cluster basis, coordinates of the cluster interior subpixels, the cluster center subpixels, the boundary subpixels, and the background subpixels in the memory for use as the ground truth training data for training the neural network-based template generator;</li>        <li id="ul0120-0002" num="2013">downscaling the coordinates by a factor used to upsample the cluster map; and</li>        <li id="ul0120-0003" num="2014">storing, on the cluster-by-cluster basis, the downscaled coordinates in the memory for use as the ground truth training data for training the neural network-based template generator.</li>    </ul>    </li>    <li id="ul0110-0011" num="2015">11. The computer-implemented method of clause 10, further including:    <ul id="ul0121" list-style="none">        <li id="ul0121-0001" num="2016">generating cluster maps for a plurality of tiles of the flow cell;</li>        <li id="ul0121-0002" num="2017">storing the cluster maps in memory and determining the cluster metadata of clusters in the tiles based on the cluster maps, including the cluster centers, the cluster shapes, the cluster sizes, the cluster background, and/or the cluster boundaries;</li>        <li id="ul0121-0003" num="2018">in the upsampled cluster maps of the clusters in the tiles, categorizing, on a cluster-by-cluster basis, subpixels as cluster interior subpixels belonging to a same cluster, cluster center subpixels, boundary subpixels, and background subpixels;</li>        <li id="ul0121-0004" num="2019">storing the categorizations in the memory for use as the ground truth training data for training the neural network-based template generator;</li>        <li id="ul0121-0005" num="2020">storing, on the cluster-by-cluster basis across the tiles, coordinates of the cluster interior subpixels, the cluster center subpixels, the boundary subpixels, and the background subpixels in the memory for use as the ground truth training data for training the neural network-based template generator;</li>        <li id="ul0121-0006" num="2021">downscaling the coordinates by the factor used to upsample the cluster map; and</li>        <li id="ul0121-0007" num="2022">storing, on the cluster-by-cluster basis across the tiles, the downscaled coordinates in the memory for use as the ground truth training data for training the neural network-based template generator.</li>    </ul>    </li>    <li id="ul0110-0012" num="2023">12. The computer-implemented method of clause 11, wherein the base call sequences are substantially matching when a predetermined portion of base calls match on an ordinal position-wise basis.</li>    <li id="ul0110-0013" num="2024">13. The computer-implemented method of clause 1, wherein the cluster map is generated based upon a predetermined minimum number of subpixels for a disjointed region.</li>    <li id="ul0110-0014" num="2025">14. The computer-implemented method of clause 1, wherein the flow cell has at least one patterned surface with an array of wells that occupy the clusters, further including:    <ul id="ul0122" list-style="none">        <li id="ul0122-0001" num="2026">based on the determined shapes and sizes of the clusters, determining        <ul id="ul0123" list-style="none">            <li id="ul0123-0001" num="2027">which ones of the wells are substantially occupied by at least one cluster,</li>            <li id="ul0123-0002" num="2028">which ones of the wells are minimally occupied, and</li>            <li id="ul0123-0003" num="2029">which ones of the wells are co-occupied by multiple clusters.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0110-0015" num="2030">15. A computer-implemented method of determining metadata about clusters on a tile of a flow cell, the method comprising:    <ul id="ul0124" list-style="none">        <li id="ul0124-0001" num="2031">accessing a set of images of the tile captured during a sequencing run and preliminary center coordinates of the clusters determined by a base caller;</li>        <li id="ul0124-0002" num="2032">for each image set, obtaining, from a base caller, a base call classifying, as one of four bases,        <ul id="ul0125" list-style="none">            <li id="ul0125-0001" num="2033">origin subpixels that contain the preliminary center coordinates and</li>            <li id="ul0125-0002" num="2034">a predetermined neighborhood of contiguous subpixels that are successively contiguous to respective ones of the origin subpixels,</li>            <li id="ul0125-0003" num="2035">thereby producing a base call sequence for each of the origin subpixels and for each of the predetermined neighborhood of contiguous subpixels;</li>        </ul>        </li>        <li id="ul0124-0003" num="2036">generating a cluster map that identifies the clusters as disjointed regions of contiguous subpixels that        <ul id="ul0126" list-style="none">            <li id="ul0126-0001" num="2037">are successively contiguous to at least some of the respective ones of the origin subpixels and</li>            <li id="ul0126-0002" num="2038">share a substantially matching base call sequence of the one of four bases with the at least some of the respective ones of the origin subpixels; and</li>        </ul>        </li>        <li id="ul0124-0004" num="2039">storing the cluster map in memory and determining the shapes and the sizes of the clusters based on the disjointed regions in the cluster map.</li>    </ul>    </li></ul></p><p id="p-1618" num="2040">16. A computer-implemented method of generating training data for neural network-based template generation and base calling, the method comprising:<ul id="ul0127" list-style="none">    <li id="ul0127-0001" num="0000">    <ul id="ul0128" list-style="none">        <li id="ul0128-0001" num="2041">accessing a multitude of images of a flow cell captured over a plurality of cycles of a sequencing run, the flow cell having a plurality of tiles and, in the multitude of images, each of the tiles having a sequence of image sets generated over the plurality of cycles, and each image in the sequence of image sets depicting intensity emissions of clusters and their surrounding background on a particular one of the tiles at a particular one the cycles;</li>        <li id="ul0128-0002" num="2042">constructing a training set having a plurality of training examples, each training example corresponding to a particular one of the tiles and including image data from at least some image sets in the sequence of image sets of the particular one of the tiles; and</li>        <li id="ul0128-0003" num="2043">generating at least one ground truth data representation for each of the training examples, the ground truth data representation identifying at least one property of analytes on the particular one of the tiles whose intensity emissions are depicted by the image data.</li>    </ul>    </li>    <li id="ul0127-0002" num="2044">17. The computer-implemented method of clause 16, wherein the at least one property of clusters is selected from the group consisting of: spatial distribution of clusters on the tile; cluster shape; cluster size; cluster boundary; and center of contiguous regions including a single cluster.</li>    <li id="ul0127-0003" num="2045">18. The computer-implemented method of clause 16, wherein the image data includes images in each of the at least some image sets in the sequence of image sets of the particular one of the tiles.</li>    <li id="ul0127-0004" num="2046">19. The computer-implemented method of clause 18, wherein the image data includes at least one image patch from each of the images.</li>    <li id="ul0127-0005" num="2047">20. The computer-implemented method of clause 19, wherein the image data includes an upsampled representation of the image patch.</li>    <li id="ul0127-0006" num="2048">21. The computer-implemented method of clause 16, wherein multiple training examples correspond to a same particular one of the tiles and respectively include as image data different image patches from each image in each of at least some image sets in a sequence of image sets of the same particular one of the tiles, and    <ul id="ul0129" list-style="none">        <li id="ul0129-0001" num="2049">wherein at least some of the different image patches overlap with each other.</li>    </ul>    </li>    <li id="ul0127-0007" num="2050">22. The computer-implemented method of clause 16, wherein the ground truth data representation identifies the clusters as disjoint regions of adjoining subpixels, the centers of the clusters as centers of mass subpixels within respective ones of the disjoint regions, and their surrounding background as subpixels that do not belong to any of the disjoint regions.</li>    <li id="ul0127-0008" num="2051">23. The computer-implemented method of clause 16, further including:    <ul id="ul0130" list-style="none">        <li id="ul0130-0001" num="2052">storing, in memory, the training examples in the training set and associated ground truth data representations as the training data for the neural network-based template generation and base calling.</li>    </ul>    </li>    <li id="ul0127-0009" num="2053">24. A computer-implemented method, including:    <ul id="ul0131" list-style="none">        <li id="ul0131-0001" num="2054">accessing sequencing images of clusters produced by a sequencer;</li>        <li id="ul0131-0002" num="2055">generating training data from the sequencing images; and</li>        <li id="ul0131-0003" num="2056">using the training data for training a neural network to generate metadata about the clusters.</li>    </ul>    </li>    <li id="ul0127-0010" num="2057">25. A computer-implemented method, including:    <ul id="ul0132" list-style="none">        <li id="ul0132-0001" num="2058">accessing sequencing images of clusters produced by a sequencer;</li>        <li id="ul0132-0002" num="2059">generating training data from the sequencing images; and</li>        <li id="ul0132-0003" num="2060">using the training data for training a neural network to base call the clusters.</li>    </ul>    </li>    <li id="ul0127-0011" num="2061">26. A computer-implemented method of determining image regions indicative of analytes on a tile of a flow cell, the method comprising:    <ul id="ul0133" list-style="none">        <li id="ul0133-0001" num="2062">accessing a series of image sets generated during a sequencing run, each image set in the series generated during a respective sequencing cycle of the sequencing run, each image in the series depicting the analytes and their surrounding background, and each image in the series having a plurality of subpixels;</li>        <li id="ul0133-0002" num="2063">obtaining, from a base caller, a base call classifying each of the subpixels, thereby producing a base call sequence for each of the subpixels across a plurality of sequencing cycles of the sequencing run;</li>        <li id="ul0133-0003" num="2064">determining a plurality of disjointed regions of contiguous subpixels which share a substantially matching base call sequence; and</li>        <li id="ul0133-0004" num="2065">generating a cluster map identifying the determined disjointed regions.</li>    </ul>    </li></ul></p><p id="p-1619" num="2066">Clauses Set 7<ul id="ul0134" list-style="none">    <li id="ul0134-0001" num="2067">1. A neural network-implemented method of determining analyte data from image data generated based upon one or more analytes, the method including:    <ul id="ul0135" list-style="none">        <li id="ul0135-0001" num="2068">receiving input image data, the input image data derived from a sequence of images,        <ul id="ul0136" list-style="none">            <li id="ul0136-0001" num="2069">wherein each image in the sequence of images represents an imaged region and depicts intensity emissions indicative of the one or more analytes and a surrounding background of the intensity emissions at a respective one of a plurality of sequencing cycles of a sequencing run, and</li>            <li id="ul0136-0002" num="2070">wherein the input image data comprises image patches extracted from each image in the sequence of images;</li>        </ul>        </li>        <li id="ul0135-0002" num="2071">processing the input image data through a neural network to generate an alternative representation of the input image data; and</li>        <li id="ul0135-0003" num="2072">processing the alternative representation through an output layer to generate an output indicating properties of respective portions of the imaged region.</li>    </ul>    </li>    <li id="ul0134-0002" num="2073">2. The neural network-implemented method of clause 1, wherein the properties include    <ul id="ul0137" list-style="none">        <li id="ul0137-0001" num="2074">whether a portion represents background or analyte, and</li>        <li id="ul0137-0002" num="2075">whether a portion represents a center of a plurality of contiguous image portions each representing a same analyte.</li>    </ul>    </li>    <li id="ul0134-0003" num="2076">3. The neural network-implemented method of clause 1, wherein the output identifies the one or more analytes, whose intensity emissions are depicted by the input image data, as disjoint regions of adjoining units, centers of the one or more analytes as center units at centers of mass of the respective ones of the disjoint regions, and the surrounding background of the intensity emissions as background units not belonging to any of the disjoint regions.</li>    <li id="ul0134-0004" num="2077">4. The neural network-implemented method of clause 3, wherein the adjoining units in the respective ones of the disjoint regions have intensity values weighted according to distance of an adjoining unit from a center unit in a disjoint region to which the adjoining unit belongs.</li>    <li id="ul0134-0005" num="2078">5. The neural network-implemented method of any of clauses 1-4, wherein the output is a binary map which classifies each portion as analyte or background.</li>    <li id="ul0134-0006" num="2079">6. The neural network-implemented method of any of clauses 1-5, wherein the output is a ternary map which classifies each portion as analyte, background, or center.</li>    <li id="ul0134-0007" num="2080">7. The neural network-implemented method of any of clauses 1-6, further including:    <ul id="ul0138" list-style="none">        <li id="ul0138-0001" num="2081">applying a peak locator to the output to find peak intensities in the output;</li>        <li id="ul0138-0002" num="2082">determining location coordinates of the centers of the analytes based on the peak intensities;</li>        <li id="ul0138-0003" num="2083">downscaling the location coordinates by an upsampling factor used to prepare the input image data; and</li>        <li id="ul0138-0004" num="2084">storing the downscaled location coordinates in memory for use in base calling the analytes.</li>    </ul>    </li>    <li id="ul0134-0008" num="2085">8. The neural network-implemented method of any of clauses 1-7, further including:    <ul id="ul0139" list-style="none">        <li id="ul0139-0001" num="2086">categorizing the adjoining units in the respective ones of the disjoint regions as analyte interior units belonging to a same analyte; and</li>        <li id="ul0139-0002" num="2087">storing the categorization and downscaled location coordinates of the analyte interior units in the memory on an analyte-by-analyte basis for use in base calling the analytes.</li>    </ul>    </li>    <li id="ul0134-0009" num="2088">9. The neural network-implemented method of any of clauses 1-8, further including:    <ul id="ul0140" list-style="none">        <li id="ul0140-0001" num="2089">obtaining training data for training the neural network,        <ul id="ul0141" list-style="none">            <li id="ul0141-0001" num="2090">wherein the training data includes a plurality of training examples and corresponding ground truth data,</li>            <li id="ul0141-0002" num="2091">wherein each training example includes image data from a sequence of image sets,            <ul id="ul0142" list-style="none">                <li id="ul0142-0001" num="2092">wherein each image in the sequence of image sets represents a tile of a flow cell and depicts intensity emissions of analytes on the tile and their surrounding background captured for a particular image channel at a particular one of a plurality of sequencing cycles of a sequencing run performed on the flow cell, and</li>                <li id="ul0142-0002" num="2093">wherein each ground truth data identifies properties of respective portions of the training examples; and</li>            </ul>            </li>        </ul>        </li>        <li id="ul0140-0002" num="2094">using a gradient descent training technique to train the neural network and generating outputs for the training examples that progressively match the ground truth data, including iteratively        <ul id="ul0143" list-style="none">            <li id="ul0143-0001" num="2095">optimizing a loss function that minimizes error between the outputs and the ground truth data, and</li>            <li id="ul0143-0002" num="2096">updating parameters of the neural network based on the error.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0134-0010" num="2097">10. The neural network-implemented method of any of clauses 1-9, wherein the properties comprise identifying whether a unit is a center or a non-center.</li>    <li id="ul0134-0011" num="2098">11. The neural network-implemented method of clause 9, further including:    <ul id="ul0144" list-style="none">        <li id="ul0144-0001" num="2099">upon error convergence after a final iteration, storing the updated parameters of the neural network in memory to be applied to further neural network-based template generation and base calling.</li>    </ul>    </li>    <li id="ul0134-0012" num="2100">12. The neural network-implemented method of any of clauses 9-11, wherein, in the ground truth data, the adjoining units in the respective ones of the disjoint regions have intensity values weighted according to distance of an adjoining unit from a center unit in a disjoint region to which the adjoining unit belongs.</li>    <li id="ul0134-0013" num="2101">13. The neural network-implemented method of any of clauses 9-11, wherein, in the ground truth data, the center units have highest intensity values within the respective ones of the disjoint regions.</li>    <li id="ul0134-0014" num="2102">14. The neural network-implemented method of any of clauses 9-13, wherein the loss function is mean squared error and the error is minimized on a unit-basis between the normalized intensity values of corresponding units in the outputs and the ground truth data.</li>    <li id="ul0134-0015" num="2103">15. The neural network-implemented method of any of clauses 9-14, wherein, in the training data, multiple training examples respectively include as image data different image patches from each image in a sequence of image sets of a same tile, and wherein at least some of the different image patches overlap with each other.</li>    <li id="ul0134-0016" num="2104">16. The neural network-implemented method of any of clauses 9-15, wherein, in the ground truth data, units classified as analyte centers are all assigned a same first predetermined class score, and units classified as non-centers are all assigned a same second predetermined class score.</li>    <li id="ul0134-0017" num="2105">17. The neural network-implemented method of any of clauses 9-16, wherein the loss function is custom-weighted binary cross-entropy loss and the error is minimized on a unit-basis between the prediction scores and the class scores of corresponding units in the outputs and the ground truth data.</li>    <li id="ul0134-0018" num="2106">18. The neural network-implemented method of any of clauses 9-17, wherein, in the ground truth data,    <ul id="ul0145" list-style="none">        <li id="ul0145-0001" num="2107">units classified as background are all assigned a same first predetermined class score,</li>        <li id="ul0145-0002" num="2108">units classified as analyte centers are all assigned a same second predetermined class score, and</li>        <li id="ul0145-0003" num="2109">units classified as analyte interior are all assigned a same third predetermined class score.</li>    </ul>    </li>    <li id="ul0134-0019" num="2110">19. The neural network-implemented method of any of clauses 1-18, further including:    <ul id="ul0146" list-style="none">        <li id="ul0146-0001" num="2111">thresholding output values of the units and classifying a first subset of the units as background units depicting the surrounding background;</li>        <li id="ul0146-0002" num="2112">locating peaks in the output values of the units and classifying a second subset of the units as center units containing centers of the analytes; and</li>        <li id="ul0146-0003" num="2113">applying a segmenter to the output values of the units and determining shapes of the analytes as non-overlapping regions of contiguous units separated by the background units and centered at the center units, wherein the segmenter begins with the center units and determines, for each center unit, a group of successively contiguous units that depict a same analyte whose center is contained in the center unit.</li>    </ul>    </li>    <li id="ul0134-0020" num="2114">20. The neural network-implemented method of any of clauses 1-19, wherein the non-overlapping regions have irregular contours and the units are units, further including:    <ul id="ul0147" list-style="none">        <li id="ul0147-0001" num="2115">determining analyte intensity of a given analyte by:        <ul id="ul0148" list-style="none">            <li id="ul0148-0001" num="2116">identifying units that contribute to the analyte intensity of the given analyte based on a corresponding non-overlapping region of contiguous units that identifies a shape of the given analyte;</li>            <li id="ul0148-0002" num="2117">locating the identified units in one or more optical, pixel resolution images generated for one or more image channels at a current sequencing cycle;</li>            <li id="ul0148-0003" num="2118">in each of the images, interpolating intensities of the identified units, combining the interpolated intensities, and normalizing the combined interpolated intensities to produce a per-image analyte intensity for the given analyte in each of the images; and</li>            <li id="ul0148-0004" num="2119">combining the per-image analyte intensity for each of the images to determine the analyte intensity of the given analyte at the current sequencing cycle.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0134-0021" num="2120">21. The neural network-implemented method of any of clauses 1-20, wherein the non-overlapping regions have irregular contours and the units are units, further including:    <ul id="ul0149" list-style="none">        <li id="ul0149-0001" num="2121">determining analyte intensity of a given analyte by:        <ul id="ul0150" list-style="none">            <li id="ul0150-0001" num="2122">identifying units that contribute to the analyte intensity of the given analyte based on a corresponding non-overlapping region of contiguous units that identifies a shape of the given analyte;</li>            <li id="ul0150-0002" num="2123">locating the identified units in one or more unit resolution images upsampled from corresponding optical, pixel resolution images generated for one or more image channels at a current sequencing cycle;</li>            <li id="ul0150-0003" num="2124">in each of the upsampled images, combining intensities of the identified units and normalizing the combined intensities to produce a per-image analyte intensity for the given analyte in each of the upsampled images; and</li>            <li id="ul0150-0004" num="2125">combining the per-image analyte intensity for each of the upsampled images to determine the analyte intensity of the given analyte at the current sequencing cycle.</li>        </ul>        </li>    </ul>    </li>    <li id="ul0134-0022" num="2126">22. The neural network-implemented method of any of clauses 1-21, wherein the normalizing is based on a normalization factor, and wherein the normalization factor is a number of the identified units.</li>    <li id="ul0134-0023" num="2127">23. The neural network-implemented method of any of clauses 1-22, further including:    <ul id="ul0151" list-style="none">        <li id="ul0151-0001" num="2128">base calling the given analyte based on the analyte intensity at the current sequencing cycle.</li>    </ul>    </li>    <li id="ul0134-0024" num="2129">24. A neural network-implemented method of determining metadata about analytes on a flow cell, the method including:    <ul id="ul0152" list-style="none">        <li id="ul0152-0001" num="2130">accessing image data that depicts intensity emissions of the analytes;</li>        <li id="ul0152-0002" num="2131">processing the image data through one or more layers of a neural network and generating an alternative representation of the image data; and</li>        <li id="ul0152-0003" num="2132">processing the alternative representation through an output layer and generating an output that identifies at least one of shapes and sizes of the analytes and/or centers of the analytes.</li>    </ul>    </li>    <li id="ul0134-0025" num="2133">25. The neural network-implemented method of clause 24, wherein the image data further depicts intensity emissions of surrounding background of the analytes, further including:    <ul id="ul0153" list-style="none">        <li id="ul0153-0001" num="2134">the output identifying spatial distribution of the analytes on the flow cell, including the surrounding background and boundaries between the analytes.</li>    </ul>    </li>    <li id="ul0134-0026" num="2135">26. A computer-implemented method, including:    <ul id="ul0154" list-style="none">        <li id="ul0154-0001" num="2136">processing image data through a neural network and generating an alternative representation of the image data, wherein the image data depicts intensity emissions of analytes; and</li>        <li id="ul0154-0002" num="2137">processing the alternative representation through an output layer and generating an output that identifies metadata about the analytes, including at least one of spatial distribution of the analytes, shapes of the analytes, centers of the analytes, and/or boundaries between the analytes.</li>    </ul>    </li>    <li id="ul0134-0027" num="2138">27. A neural network-implemented method of determining cluster metadata from image data generated based upon one or more clusters, the method including:    <ul id="ul0155" list-style="none">        <li id="ul0155-0001" num="2139">receiving input image data, the input image data derived from a sequence of images,        <ul id="ul0156" list-style="none">            <li id="ul0156-0001" num="2140">wherein each image in the sequence of images represents an imaged region and depicts intensity emissions of the one or more clusters and their surrounding background at a respective one of a plurality of sequencing cycles of a sequencing run, and</li>            <li id="ul0156-0002" num="2141">wherein the input image data comprises image patches extracted from each image in the sequence of images;</li>        </ul>        </li>        <li id="ul0155-0002" num="2142">processing the input image data through a neural network to generate an alternative representation of the input image data, wherein the neural network is trained for cluster metadata determination task, including determining cluster background, cluster centers, and cluster shapes;</li>        <li id="ul0155-0003" num="2143">processing the alternative representation through an output layer to generate an output indicating properties of respective portions of the imaged region;</li>        <li id="ul0155-0004" num="2144">thresholding output values of the output and classifying a first subset of the respective portions of the imaged region as background portions depicting the surrounding background;</li>        <li id="ul0155-0005" num="2145">locating peaks in the output values of the output and classifying a second subset of the respective portions of the imaged region as center portions containing centers of the clusters; and</li>        <li id="ul0155-0006" num="2146">applying a segmenter to the output values of the output and determining shapes of the clusters as non-overlapping regions of contiguous portions of the imaged region separated by the background portions and centered at the center portions.</li>    </ul>    </li></ul></p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230004749A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.47mm" wi="76.20mm" file="US20230004749A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230004749A1-20230105-M00002.NB"><img id="EMI-M00002" he="12.02mm" wi="76.20mm" file="US20230004749A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003" nb-file="US20230004749A1-20230105-M00003.NB"><img id="EMI-M00003" he="5.25mm" wi="76.20mm" file="US20230004749A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230004749A1-20230105-M00004.NB"><img id="EMI-M00004" he="7.79mm" wi="76.20mm" file="US20230004749A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230004749A1-20230105-M00005.NB"><img id="EMI-M00005" he="6.01mm" wi="76.20mm" file="US20230004749A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230004749A1-20230105-M00006.NB"><img id="EMI-M00006" he="9.14mm" wi="76.20mm" file="US20230004749A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230004749A1-20230105-M00007.NB"><img id="EMI-M00007" he="9.14mm" wi="76.20mm" file="US20230004749A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230004749A1-20230105-M00008.NB"><img id="EMI-M00008" he="7.79mm" wi="76.20mm" file="US20230004749A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230004749A1-20230105-M00009.NB"><img id="EMI-M00009" he="6.01mm" wi="76.20mm" file="US20230004749A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00010" nb-file="US20230004749A1-20230105-M00010.NB"><img id="EMI-M00010" he="7.79mm" wi="76.20mm" file="US20230004749A1-20230105-M00010.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00011" nb-file="US20230004749A1-20230105-M00011.NB"><img id="EMI-M00011" he="6.01mm" wi="76.20mm" file="US20230004749A1-20230105-M00011.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computer-implemented base calling method, comprising:<claim-text>processing through a neural network first image data comprising images of clusters and their surrounding background captured by a sequencing system for one or more sequencing cycles of a sequencing run; and</claim-text><claim-text>producing a base call for one or more of the clusters of the one or more sequencing cycles of the sequencing run.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the first image data comprises:<claim-text>processing a first input through a first neural network to produce a first output, wherein the first input comprises the first image data;</claim-text><claim-text>processing the first output through a post-processor to produce template data indicating one or more properties of respective portions of the first image data; and</claim-text><claim-text>processing a second input through a second neural network to produce a second output, wherein the second input comprises the first image data and supplemental data, wherein the supplemental data comprises the template data, and wherein the second output identifies base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein processing the first image data comprises:<claim-text>processing a first input through a first neural network to produce a first output, wherein the first input comprises the first image data;</claim-text><claim-text>processing the first output through a post-processor to produce template data indicating one or more properties of respective portions of the first image data; and</claim-text><claim-text>processing a second input through a second neural network to produce a second output, wherein the second input comprises the first image data modified using the template data, and wherein the second output identifies base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the second input further comprises second image data modified using the template data, the second image data comprising images of clusters and their surrounding background captured by the sequencing system for one or more additional sequencing cycles of the sequencing run.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the template data comprises a template image, wherein the template image is at an upsampled, subpixel resolution.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computer-implemented method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein each subpixel in the template image is identified as either background subpixel, cluster center subpixel, or cluster interior subpixel.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the images of the clusters and their surrounding background are captured at an optical, pixel resolution.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computer-implemented method of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein modification using the template data comprises:<claim-text>calculating an area weighting factor for one or more pixels in the first and/or second image data based on how many subpixels in the template data that correspond to a pixel in the images of the first and/or second image data contain parts of one or more of the clusters; and</claim-text><claim-text>modifying intensities of the pixels based on the area weighting factor.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein modification using the template data comprises:<claim-text>upsampling the images of the clusters and their surrounding background to an upsampled, subpixel resolution to produce upsampled images, and assigning a background intensity to those subpixels in the upsampled images that correspond to background subpixels in a template image and assigning cluster intensities to those subpixels in the upsampled images that correspond to cluster center subpixels and cluster interior subpixels in the template image.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the background intensity has a zero value.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computer-implemented method of <claim-ref idref="CLM-00009">claim 9</claim-ref>, wherein the cluster intensities are determined by interpolating intensities of pixels in an optical, pixel resolution.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computer-implemented method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein modification using the template data comprises:<claim-text>upsampling the images of the clusters and their surrounding background to an upsampled, subpixel resolution to produce upsampled images, and distributing an entire intensity of ate pixel in an optical, pixel resolution among only those constituent subpixels of the pixel in the upsampled images that correspond to cluster center subpixels and cluster interior subpixels in a template image.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the template data identifies at least one of the properties selected from the group consisting of: spatial distribution of the clusters, cluster shape, centers of the clusters and cluster boundary.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computer-implemented method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, further comprising calculating a quality of the base calls based on the second output.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising performing one or more sequencing cycles to capture the images of the cluster and their surrounding background.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computer-implemented method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising performing a plurality of sequencing cycles, wherein each of the plurality of sequencing cycles generates image data.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. A sequencing system, comprising:<claim-text>a receptacle coupled to a biosensor system, the biosensor system configured to comprise an array of light detectors, the biosensor system comprising a biosensor, and the biosensor comprising reaction sites configured to contain analytes;</claim-text><claim-text>an illumination system configured to direct excitation light toward the biosensor and illuminate the analytes in the reaction sites, wherein at least some of the analytes provide emission signals when illuminated; and</claim-text><claim-text>a system controller coupled to the receptacle and comprising an analysis module, the analysis module configured to:<claim-text>obtain image data from the light detectors at each of a plurality of sequencing cycles of a sequencing run, wherein the image data is derived from the emission signals detected by the light detectors and comprises images of clusters and their surrounding background,</claim-text><claim-text>process the image data for each of the plurality of sequencing cycles through a neural network, and</claim-text><claim-text>produce a base call for one or more of the clusters at each of the plurality of sequencing cycles.</claim-text></claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The sequencing system of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the analysis module is further configured to:<claim-text>process a first input through a first neural network to produce a first output, wherein the first input comprises the image data;</claim-text><claim-text>process the first output to produce template data indicating one or more properties of respective portions of the image data; and</claim-text><claim-text>process a second input through a second neural network to produce a second output, wherein the second input comprises the image data and supplemental data, wherein the supplemental data comprises the template data, and wherein the second output identifies base calls for one or more of the clusters at each of the plurality of sequencing cycles.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. A non-transitory computer readable storage medium impressed with computer program instructions to base call clusters, the instructions, when executed on a processor, implement a method comprising:<claim-text>processing through a neural network first image data comprising images of the clusters and their surrounding background captured by a sequencing system for one or more sequencing cycles of a sequencing run; and</claim-text><claim-text>producing a base call for one or more of the clusters of the one or more sequencing cycles of the sequencing run.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer readable storage medium of <claim-ref idref="CLM-00019">claim 19</claim-ref>, wherein processing the first image data comprises:<claim-text>processing a first input through a first neural network to produce a first output, wherein the first input comprises the first image data;</claim-text><claim-text>processing the first output through a post-processor to produce template data indicating one or more properties of respective portions of the first image data; and</claim-text><claim-text>processing a second input through a second neural network to produce a second output, wherein the second input comprises the first image data and supplemental data, wherein the supplemental data comprises the template data, and wherein the second output identifies base calls for one or more of the clusters at one or more sequencing cycles of the sequencing run.</claim-text></claim-text></claim></claims></us-patent-application>