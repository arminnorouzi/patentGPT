<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001294A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001294</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17903941</doc-number><date>20220906</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>26</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>01</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>211</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>216</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>213</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>69</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>26</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>012</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>19</main-group><subgroup>006</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>211</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>216</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>213</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>69</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20140902</date></cpc-version-indicator><section>A</section><class>63</class><subclass>F</subclass><main-group>13</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>3</main-group><subgroup>011</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">AUGMENTED REALITY SYSTEM AND METHOD OF OPERATION THEREOF</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17353676</doc-number><date>20210621</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11433297</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17903941</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>16459358</doc-number><date>20190701</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11040276</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17353676</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>14965006</doc-number><date>20151210</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10335677</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>16459358</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62096109</doc-number><date>20141223</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only"><addressbook><last-name>Fuchs</last-name><first-name>Matthew Daniel</first-name><address><city>Los Gatos</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Fuchs</last-name><first-name>Matthew Daniel</first-name><address><city>Los Gatos</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A system and method of operation of an augmented reality system includes: a position sensor (<b>140</b>) for calculating a current location (<b>144</b>); an orientation sensor (<b>142</b>), coupled to the position sensor (<b>140</b>), for calculating a current orientation (<b>146</b>); and a control mechanism (<b>118</b>), coupled to the position sensor (<b>140</b>), for presenting a system object (<b>126</b>) based on the current location (<b>144</b>), the current orientation (<b>146</b>), an object location (<b>128</b>), an object orientation (<b>130</b>), an access right (<b>120</b>), a visibility (<b>134</b>), and a persistence (<b>136</b>).</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="104.99mm" wi="158.75mm" file="US20230001294A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="233.85mm" wi="156.21mm" orientation="landscape" file="US20230001294A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="232.58mm" wi="133.60mm" file="US20230001294A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="210.48mm" wi="124.04mm" file="US20230001294A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="229.95mm" wi="144.61mm" file="US20230001294A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="226.06mm" wi="138.01mm" orientation="landscape" file="US20230001294A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="233.85mm" wi="163.83mm" file="US20230001294A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="233.51mm" wi="143.85mm" file="US20230001294A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="218.78mm" wi="148.25mm" orientation="landscape" file="US20230001294A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="230.55mm" wi="102.02mm" file="US20230001294A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="217.25mm" wi="160.87mm" file="US20230001294A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="230.21mm" wi="137.84mm" file="US20230001294A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="222.42mm" wi="137.08mm" file="US20230001294A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="232.66mm" wi="152.82mm" file="US20230001294A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="233.93mm" wi="107.61mm" file="US20230001294A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION(S)</heading><p id="p-0002" num="0001">This application claims priority as a Continuation of U.S. application Ser. No. 17/353,676, filed Jun. 21, 2021, which claims priority as Continuation of U.S. application Ser. No. 16/459,358, filed Jul. 1, 2019, now U.S. Pat. No. 11,040,276 issued on Jun. 22, 2021, which claims priority as a Continuation of U.S. application Ser. No. 14/965,006, filed Dec. 10, 2015, now U.S. Pat. No. 10,335,677 issued on Jul. 2, 2019, which claims priority to U.S. Provisional Patent Application Ser. No. 62/096,109 filed Dec. 23, 2014, the entire contents of which are incorporated by reference as if fully set forth herein, under 35 U.S.C. &#xa7; 120. The applicants hereby rescind any disclaimer of claim scope in the parent applications or the prosecution history thereof and advise the USPTO that the claims in this application may be broader than any claim in the parent application(s).</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">TECHNICAL FIELD</heading><p id="p-0003" num="0002">The present invention relates generally to an augmented reality system, and more particularly to an augmented reality system with visibility and persistence control.</p><heading id="h-0003" level="1">BACKGROUND ART</heading><p id="p-0004" num="0003">The development of modern Internet systems allows display of information related to specific content. Web pages can be configured to include hyperlinks to contextual content anywhere in the world. Clicking on contextual links can present the user with related information including graphics, video, audio, image, 3-dimensional (3D) information, or a combination thereof.</p><p id="p-0005" num="0004">Such contextual information can be displayed on a wide variety of devices including computer monitors, projectors, smart glasses, tablet computers, smart phones, and 3-dimensional displays. In the case of virtual reality systems, information can be linked to locations and displayed in a representation of a 3D virtual synthetic world.</p><p id="p-0006" num="0005">Presenting information in a contextual way can increase the effectiveness of the information by associating the information with locations, times, people, and things. Displaying information in the appropriate context can increase the value and usability of the contextual information.</p><p id="p-0007" num="0006">In view of the need for the effective use of information due to the increase in Internet traffic, it is increasingly critical that answers be found to these problems. In view of the ever-increasing commercial competitive pressures, along with growing expectations of the populace, it is critical that answers be found for these problems. Additionally, the need to reduce costs, improve efficiencies and performance, and meet critical time pressures adds an even greater urgency to the critical necessity for finding answers to these problems.</p><p id="p-0008" num="0007">Solutions to these problems have been long sought but prior developments have not taught or suggested any solutions and, thus, solutions to these problems have long eluded those skilled in the art.</p><heading id="h-0004" level="1">DISCLOSURE OF THE INVENTION</heading><p id="p-0009" num="0008">The present invention provides a method of operation of an augmented reality system that includes: detecting a current location; detecting a current orientation; detecting a system object having an object location within a detection threshold of the current location; retrieving a content associated with the system object; calculating a persistence of the system object based on the current time and a persistence extent; calculating a visibility of the system object based on an access right and the object location; and presenting the content of the system object to a control mechanism based on the persistence and the visibility.</p><p id="p-0010" num="0009">The present invention provides an augmented reality system that includes: a position sensor for calculating a current location; an orientation sensor, coupled to the position sensor, for calculating a current orientation; and a control mechanism, coupled to the position sensor, for presenting a system object based on the current location, the current orientation, an object location, an object orientation, an access right, a visibility, and a persistence.</p><p id="p-0011" num="0010">Certain embodiments of the invention have other steps or elements in addition to or in place of those mentioned above. The steps or element will become apparent to those skilled in the art from a reading of the following detailed description when taken with reference to the accompanying drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0005" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is an exemplary diagram of an augmented reality system in a first embodiment of the present invention,</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is an example of a global coordinate system,</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is an example of a building coordinate system,</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is an example of one of the frames of reference using the building coordinate system,</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a first example of the viewing unit,</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a second example of the viewing unit,</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a first example of object visibility,</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a second example of the object visibility,</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is an exemplary diagram of the relationship between the frames of reference and the points of interest,</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is an exemplary diagram of the relationship between basic types,</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is an exemplary diagram of the relationship between the points of interest and the roles,</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is an example of the visibility properties,</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is an example of the roles and the token sets,</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is an example of the persistence,</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is an example of the multi-level maps,</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is an example of the global coordinate system for the frames of reference,</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is an example of a visibility tree,</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is an example of the building coordinate system for the frames of reference,</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is an exemplary diagram of the agents cooperating with one another,</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is an example of an agent data structure,</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is an example of a query data structure,</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>22</b></figref> is an example of an ownership data structure,</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>23</b></figref> is an example of the agent actions,</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is an example of related systems, and</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>25</b></figref> is an example of a process flow of operation of the augmented reality system.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>26</b></figref> is a flow chart of a method of operation of an augmented reality system in a further embodiment of the present invention.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0006" level="1">BEST MODE FOR CARRYING OUT THE INVENTION</heading><p id="p-0038" num="0037">The following embodiments are described in sufficient detail to enable those skilled in the art to make and use the invention. It is to be understood that other embodiments would be evident based on the present disclosure, and that system, process, or mechanical changes may be made without departing from the scope of the present invention.</p><p id="p-0039" num="0038">In the following description, numerous specific details are given to provide a thorough understanding of the invention. However, it will be apparent that the invention may be practiced without these specific details. In order to avoid obscuring the present invention, some well-known circuits, system configurations, and process steps are not disclosed in detail.</p><p id="p-0040" num="0039">The drawings showing embodiments of the system are semi-diagrammatic and not to scale and, particularly, some of the dimensions are for the clarity of presentation and are shown exaggerated in the drawing FIGS. Similarly, although the views in the drawings for ease of description generally show similar orientations, this depiction in the FIGS. is arbitrary for the most part. Generally, the invention can be operated in any orientation.</p><p id="p-0041" num="0040">Where multiple embodiments are disclosed and described having some features in common, for clarity and ease of illustration, description, and comprehension thereof, similar and like features will be described with the same or similar reference numerals.</p><p id="p-0042" num="0041">For expository purposes, the term &#x201c;horizontal&#x201d; as used herein is defined as a plane parallel to the plane or surface of the surface of the earth, regardless of its orientation. The term &#x201c;vertical&#x201d; refers to a direction perpendicular to the horizontal as just defined. Terms, such as &#x201c;above&#x201d;, &#x201c;below&#x201d;, &#x201c;bottom&#x201d;, &#x201c;top&#x201d;, &#x201c;side&#x201d;, &#x201c;higher&#x201d;, &#x201c;lower&#x201d;, &#x201c;upper&#x201d;, &#x201c;over&#x201d;, and &#x201c;under&#x201d;, are defined with respect to the horizontal plane, as shown in the figures.</p><p id="p-0043" num="0042">Referring now to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, therein is shown an exemplary diagram of an augmented reality system <b>100</b> in a first embodiment of the present invention. The augmented reality system <b>100</b> can manage and present information associated with a current location <b>144</b> in the real world.</p><p id="p-0044" num="0043">The augmented reality system <b>100</b> can access and show information related to system objects <b>126</b> that can represent elements in the real world. For example, the augmented reality system <b>100</b> can overlay information about the system objects <b>126</b> over a real world image <b>156</b>. The real world image <b>156</b> is a picture of the current scene from the perspective of the user. Information associated with the current location <b>144</b> can be displayed by the augmented reality system <b>100</b> when the user looks at the current location <b>144</b> in the real world.</p><p id="p-0045" num="0044">The current location <b>144</b> is a physical position in the real world. The current location <b>144</b> can be determined in a variety of ways. For example, the current location <b>144</b> can be the center of the user's head, the geometric center of the user, a focal plane based users perception, a coordinate, or a combination thereof. The current location <b>144</b> can be refined based on the information associated with points or objects around the current location <b>144</b>. The current location <b>144</b> can describe the position of the user, an arbitrary location, a target location, a destination location, a known location, or a combination thereof. The current location can be calculated by a device, entered manually, received from an external source, or a combination thereof.</p><p id="p-0046" num="0045">The system objects <b>126</b> can include frames of reference <b>102</b>, points of interest <b>106</b>, items <b>110</b>, roles <b>122</b>, and other similar entities. The augmented reality system <b>100</b> can be described using one or more of the frames of reference <b>102</b>, each specifying a frame geometry <b>104</b> for positioning the points of interest <b>106</b> (POI) which are attached to the items <b>110</b> that contain or are linked to a content <b>108</b>.</p><p id="p-0047" num="0046">The frames of reference <b>102</b> are systems for defining locations. For example, the frames of reference <b>102</b> can include coordinate-based systems, object based systems, relative location systems, or a combination thereof.</p><p id="p-0048" num="0047">The points of interest <b>106</b> are identifiers for locations. Each of the points of interest <b>106</b> can be associated with a physical location <b>148</b>. The points of interest <b>106</b> can be a logical or physical location. The physical location <b>148</b> can be a geographical coordinate, a relative location, a location based on a beacon, a room in a building, a portion of an object, or a combination thereof. The logical location may be a phrase in the text of a book. For example, the logical location can be a bookmark with notes associated with a certain page.</p><p id="p-0049" num="0048">The items <b>110</b> are logical representations of objects in one of the frames of reference <b>102</b>. For example, the items <b>110</b> are data that can represent content, messages, structures, or other entities in the augmented reality system <b>100</b>. The items <b>110</b> can be representations or data that can be operated upon by the augmented reality system <b>100</b>. The items <b>110</b> can be data that the hardware of the augmented reality system <b>100</b> can represent or use to receive other content. In general, the frame geometry <b>104</b> describes the physical space, the points of interest <b>106</b> designate particular parts of that space, and the items <b>110</b> can attach content to those particular locations. The frame geometry <b>104</b> can be used to represent and recognize chairs, tables, and other physical objects. However, one of the points of interest <b>106</b> can designate a particular chair, table, or portion of a chair. The items <b>110</b> could then be assigned to that one of the points of interest <b>106</b>. The points of interest <b>106</b> are the links between the physical and the virtual worlds.</p><p id="p-0050" num="0049">The roles <b>122</b> are sets of properties of the system objects <b>126</b>. For example, the roles <b>122</b> can be used to describe security categories, operational groups, identifiers, or a combination thereof. The roles <b>122</b> can be used to control access to the system object <b>126</b>.</p><p id="p-0051" num="0050">The augmented reality system <b>100</b> can support the management and linking of the system objects <b>126</b> with the real world based on two important properties. Each of the system objects <b>126</b> can have the properties of visibility <b>134</b> and persistence <b>136</b>.</p><p id="p-0052" num="0051">The visibility <b>134</b> indicates whether one of the system objects <b>126</b> can be perceived. The system objects <b>126</b> can be perceived by one of the agents <b>116</b> and shown to one of the users. The visibility <b>134</b> can be determined on an object by object basis and can depend on the location, rights, security, and other properties of the system objects <b>126</b>.</p><p id="p-0053" num="0052">The visibility <b>134</b> can control who can interact with the system objects <b>126</b>. Each of the agents <b>116</b> interacting with virtual content can perceive some of the same objects at the same place but in different ways. For example, a general text message, such as a menu, may appear to different users in different languages at the same location. Alternatively, a private message for another person should only be visible to the intended person. The message can be invisible to everyone else. In another example, only players of a treasure hunt games should be able to perceive the game content and then perhaps only if they've solved the appropriate prior clues.</p><p id="p-0054" num="0053">The augmented reality system <b>100</b> can manage the lifecycle of the system objects <b>126</b> over time. Each of the system objects <b>126</b> can have the property of persistence <b>136</b>. The persistence <b>136</b> is an existence descriptor. The persistence <b>136</b> describes where the system objects <b>126</b> are and how long they will be in a particular location. The persistence <b>136</b> can be calculated to determine if the system objects <b>126</b> exist at a current time <b>154</b>. The current time <b>154</b> is the time and date either in the local area or a universal time format. Although the persistence <b>136</b> is described as a property, it is understood that the persistence <b>136</b> can be represented by a data structure having multiple values including hierarchical values.</p><p id="p-0055" num="0054">The persistence <b>136</b> can describe the location of the system objects <b>126</b> and their duration at a particular location. For example, if a virtual object is placed on a wall, then it should still be there when the user returns later. The virtual object should be there in the same spot when another of the agents <b>116</b> with appropriate permissions arrives to view it. The virtual object should persist in the augmented world even if any local application or equipment even changes.</p><p id="p-0056" num="0055">The persistence <b>136</b> of the system objects <b>126</b> can have different characteristics. For example, the system objects <b>126</b> can be configured to only be available on Wednesdays. Another of the system objects <b>126</b> can be configured to only be available for a defined period of time. The persistence <b>136</b> can be continuous, intermittent, data dependent, condition dependent, location dependent, or a combination thereof.</p><p id="p-0057" num="0056">The persistence <b>136</b> of the system objects <b>126</b> can control how the system objects <b>126</b> are shared among users. The description of the virtual object and its position must be available over the network to any user. The properties of the virtual object can be stored in the network so it is no longer dependent on a local system or server. Because of the need to manage a large number of the agents <b>116</b> interacting in real time with multiple services responding to agent changes, such as movement, and object changes should occur within a few hundred milliseconds.</p><p id="p-0058" num="0057">The frames of reference <b>102</b> are the coordinate systems for representing locations. The frames of reference <b>102</b> can be implemented in a variety of ways. For example, the augmented reality system <b>100</b> can use the frames of reference <b>102</b> based on geographical coordinates, relative coordinates, structural contexts, or a combination thereof. In another example, the frames of reference <b>102</b> can include the surface of the earth, the rooms of a building, the pages of a book, the surface of a picture, or a relative location based on a signal from a beacon. The frames of reference <b>102</b> can be temporally based including points in time, such as those related to a blog post on a particular day.</p><p id="p-0059" num="0058">The frame geometry <b>104</b> can specify how to identify a point in one of the frames of reference <b>102</b>. For example, the frame geometry <b>104</b> can be global positioning system (GPS) coordinates, a connectivity graph of a building, sensory data, location data relative to a beacon, orientation information, Simultaneous Location and Mapping (SLAM) point clouds, or a combination thereof.</p><p id="p-0060" num="0059">The points of interest <b>106</b> specify a particular region according to the frame geometry <b>104</b> in one of the frames of reference <b>102</b>. The points of interest <b>106</b> can represent not just a single point, but can also describe a three dimensional volume, a two-dimensional surface, or a time-varying entity such as an object in motion. The points of interest <b>106</b> can have a precise location that may require real-time computation to calculate.</p><p id="p-0061" num="0060">The points of interest <b>106</b> can include additional location information, such as computer vision data. For example, one of the points of interest <b>106</b> can be the &#x201c;Starry Night&#x201d; painting on a particular wall of the Museum of Modern Art in New York City. The visual information can help locate the exact position and orientation of the painting in a particular room because other location technologies may be deprecated inside the building. The points of interest <b>106</b> can include additional location information to support a sensor fusion approach to determine the exact location in difficult conditions by combining multiple inputs such as GPS, magnetometer, cameras, or other similar inputs.</p><p id="p-0062" num="0061">The items <b>110</b> are objects that can have the content <b>108</b> associated with them. The items <b>110</b> can represent information attached to the points of interest <b>106</b>. For example, the one of the items <b>110</b> can be a message, information, a note, or similar data.</p><p id="p-0063" num="0062">The points of interest <b>106</b> can cover the physical word, while the items <b>110</b> can be logical objects associated with one of the points of interest <b>106</b>. There can be a plurality of the items <b>110</b> associated with a single one of the points of interest <b>106</b>. In an illustrative example, the upper lip of the Mona Lisa painting can be one of the points of interest <b>106</b> and a user can associated one of the items <b>110</b>, such as a moustache, with the upper lip.</p><p id="p-0064" num="0063">The content <b>108</b> is information that can be associated with the items <b>110</b>. The content <b>108</b> can be text, audio, graphics, images, video, 3D content, relationships, behavior, actions, properties, or a combination thereof. The content <b>108</b> can be retrieved based on the items <b>110</b>.</p><p id="p-0065" num="0064">The augmented reality system <b>100</b> can include the agents <b>116</b>. The agents <b>116</b> are devices for interacting with the augmented reality system <b>100</b>. For example, the agents <b>116</b> can be a computational device with a set of sensors and displays that can change position in the physical world and one or more of the frames of reference <b>102</b>. The agents <b>116</b> can also include a communication unit to link to the network and interact with the augmented reality system <b>100</b> and the physical world.</p><p id="p-0066" num="0065">The agents <b>116</b> can display information directly to human users. However, the agents <b>116</b> can also provide information for other automated system such as other ones of the agents <b>116</b> or other computer systems. The agents <b>116</b> can also change their own behavior based on the information perceived.</p><p id="p-0067" num="0066">The agents <b>116</b> can also present information directly to other agents <b>116</b> or external systems. The agents <b>116</b> can exchange information with the external systems and initiate actions in the physical world. For example, based on information from the augmented reality system <b>100</b>, the agents <b>116</b> can activate an alarm system, close valves, change the lighting levels, control the temperature of an air conditioning system, interface with an ecommerce system to change prices on products, control turnstiles, or similar activities.</p><p id="p-0068" num="0067">Although the augmented reality system <b>100</b> is described as displaying information, it is understood that displaying information includes transferring data to other system to perform actions. The act of displaying the information involves using the information for a variety of purposes including controlling display devices, controlling other systems, performing actions, or a combination thereof.</p><p id="p-0069" num="0068">The agents <b>116</b> can be associated to the points of interest <b>106</b>. Groups of the agents <b>116</b> can be co-located with one another based on the co-location of the points of interest <b>106</b>.</p><p id="p-0070" num="0069">The augmented reality system <b>100</b> can support a system of access rights <b>120</b>. The access rights <b>120</b> can define the permissions and allowable actions for the system objects <b>126</b>. The system objects <b>126</b> can include the frames of reference <b>102</b>, the points of interest <b>106</b>, the items <b>110</b>, and other entities in the system. The access rights <b>120</b> can include access to the visibility <b>134</b> property, but there can be an arbitrarily large number of the access rights <b>120</b>. For example, the access rights <b>120</b> can include the right to create one of the points of interest <b>106</b> in one of the frames of reference <b>102</b> or to attach one of the items <b>110</b> to one of the points of interest <b>106</b>.</p><p id="p-0071" num="0070">The access rights <b>120</b> can be configured in a variety of ways. For example, the access rights <b>120</b> can be organized into sets known as the roles <b>122</b>. Different ones of the access rights <b>120</b> can appear in different ones of the roles <b>122</b>.</p><p id="p-0072" num="0071">In an illustrative example, one of the frames of reference <b>102</b> can designate a room in a building. The frame geometry <b>104</b> can be represented by a three-dimensional coordinate reference. The augmented reality system <b>100</b> can represent the physical layout of the room, including the coordinates of the items <b>110</b> such as tables and chairs.</p><p id="p-0073" num="0072">The user of the augmented reality system <b>100</b> can leave a message <b>114</b> for a friend by associating the content <b>108</b> with one of the points of interest <b>106</b> on a table. The message <b>114</b> can be assigned the access rights <b>120</b> to be visible only to another person who has the access rights <b>120</b> to perceive the message <b>114</b>. One of the items <b>110</b> containing the content <b>108</b> expressing the message and can be assigned the access rights <b>120</b> to be visible.</p><p id="p-0074" num="0073">The augmented reality system <b>100</b> can also display an avatar <b>112</b> with the message <b>114</b>. The avatar <b>112</b> is a display element that can indicate the owner or originator of the message <b>114</b>. The avatar <b>112</b> can be represented by a human figure, a stick figure, a sign, an animated figure, a graphic, or other similar element.</p><p id="p-0075" num="0074">The message <b>114</b> can be displayed using a viewing unit <b>118</b>. The viewing unit <b>118</b> is a device for viewing the system objects <b>126</b> and the content <b>108</b> in the augmented reality system <b>100</b>. The viewing unit <b>118</b>, such as smart glasses, a tablet computer, a smart phone, an e-reader, or other similar viewing device, can show the message as part of a scene <b>124</b> or can display the message <b>114</b> by itself.</p><p id="p-0076" num="0075">Although the viewing unit <b>118</b> unit is described as a device for displaying information, it is understood that the viewing device <b>118</b> can perform other actions. The augmented reality system <b>100</b> can present the information about the system objects <b>126</b> to the viewing unit <b>118</b> to perform an action. The terms viewing and displaying are abstractions of the general terminology for performing the action. The action can include displaying the information, performing an activity based on the information, controlling another system based on the information being presented, or a combination thereof. The viewing unit <b>118</b> can be a display unit, an actuator, a control mechanism, or a combination thereof. The control mechanism is a device for controlling another system, device, mechanism, computer, controller, or other external system. For example, the control mechanism can be a controller for an electronic system.</p><p id="p-0077" num="0076">The agents <b>116</b> can include the viewing unit <b>118</b> to display the message <b>114</b>. For example, the agents <b>116</b> can be implemented as software (not shown) running on the viewing unit <b>118</b>. The viewing unit <b>118</b> can have the current location <b>144</b>. The current location <b>144</b> is the physical location of the viewing unit <b>118</b>.</p><p id="p-0078" num="0077">Each of the agents <b>116</b> can be associated with a set of the roles <b>122</b>. Via the roles <b>122</b>, each of the agents <b>116</b> can be associated with a particular set of the access rights <b>120</b>. The access rights <b>120</b> associated with each of the agents <b>116</b> control which of the content <b>108</b> can be displayed by the viewing unit <b>118</b> of the agents <b>116</b>.</p><p id="p-0079" num="0078">Each of the agents <b>116</b> can be associated with one or more of the frames of reference <b>102</b>. Thus, the combination of the points of interest <b>106</b>, one of the frames of reference <b>102</b>, and the roles <b>122</b> associated with one of the agents <b>116</b> can determine which of the content <b>108</b> can be displayed for one of the agents <b>116</b>.</p><p id="p-0080" num="0079">For example, one of the agents <b>116</b> can enter or leave one of the frames of reference <b>102</b> representing a building by physically entering or leaving the associated building in the real world. Each of the agents <b>116</b> can include an object location <b>128</b> and an object orientation <b>130</b> for the current one of the frames of reference <b>102</b>.</p><p id="p-0081" num="0080">The object location <b>128</b> and the object orientation <b>130</b> can be based on the frame geometry <b>104</b> of the current one of the frames of reference <b>102</b>. The object location <b>128</b> and the object orientation <b>130</b> can determine which of the points of interest <b>106</b> are in a field of view <b>132</b> for an agent. The field of view <b>132</b> for the agents <b>116</b> can be used to determine the visibility <b>134</b> of the system objects <b>126</b> relative to one of the agents <b>116</b> near the current one of the points of interest <b>106</b>. Further the object location <b>128</b> and the object orientation <b>130</b> can determine which of the system objects <b>126</b> can interact with one of the agents <b>116</b>.</p><p id="p-0082" num="0081">The augmented reality system <b>100</b> can convey the object location <b>128</b> and an object state <b>152</b> to the agents <b>116</b>. When the agents <b>116</b> receive the information about the object location <b>128</b>, the object orientation <b>130</b>, and an object state <b>152</b>, the agents <b>116</b> can determine what actions can take place. The object state <b>152</b> are the properties of one of the system objects <b>126</b>. The object state <b>152</b> can vary over time.</p><p id="p-0083" num="0082">For example, the agents <b>116</b> can receive enough information to identify where one of the items <b>110</b> is located in the current one of the frame of reference <b>102</b>. The information can include the location of the item, a thumbnail image of the item, other state information about the item, a universal resource locator (URL) linking the item to a source of data, or a combination thereof.</p><p id="p-0084" num="0083">The augmented reality system <b>100</b> can be configured to dynamically load information about the items <b>110</b> by loading the information about the points of interest <b>106</b> near the object location <b>128</b>. The information can include the content <b>108</b>, the message <b>114</b>, or other information tagged with the object location <b>128</b> of the points of interest <b>106</b>.</p><p id="p-0085" num="0084">Once the agents <b>116</b> are aware of the items <b>110</b>, the augmented reality system <b>100</b> can allow the agents <b>116</b> to communication directly with an external data system having information about the items <b>110</b>. The URL of the external data system can be associated with the items <b>110</b>. Alternatively, the information associated the items <b>110</b> can include code, such as JavaScript, or other routines to perform information transfers. The external data system can also provide additional information to render and display the system objects <b>126</b>.</p><p id="p-0086" num="0085">The augmented reality system <b>100</b> can display the system objects <b>126</b> via the viewing unit <b>118</b> based on the current location <b>144</b> and a current orientation <b>146</b> provided by sensors <b>138</b> associated with the viewing unit <b>118</b>. The current location <b>144</b> and the current orientation <b>146</b> can determine the visibility <b>134</b> of the system objects <b>126</b> in the augmented reality system <b>100</b>. The visibility <b>134</b> can also indicate the right to be aware of one of the system objects <b>126</b> that may not be in the direct line of sight.</p><p id="p-0087" num="0086">The viewing unit <b>118</b> can include a variety of sensors. For example, the sensors <b>138</b> can include a position sensor <b>140</b>, an orientation sensor <b>142</b>, GPS unit, a camera, an imaging system, accelerometers, cell tower triangulation systems, or a combination thereof.</p><p id="p-0088" num="0087">The position sensor <b>140</b> is a device to determine the current location <b>144</b> of the user using the viewing unit <b>118</b>. For example, the position sensor <b>140</b> can be a GPS unit, a cell tower triangulation system, WiFi-based position sensors, an optical location system, or a similar location system. The current location <b>144</b> can be determined using a sensor fusion approach by combining several sensor inputs to refine the location. The position sensor <b>140</b> can also be the combination of several sensory subsystems to determine the current location <b>144</b> to the accuracy required by the augmented reality system <b>100</b>. The accuracy required can be a system or user level parameters that can be adjusted as necessary. Although the position sensor <b>140</b> is described as a sensory device, it is understood that the position sensor <b>140</b> can support the direct entry of a location by users or other external systems. Thus, the current location <b>144</b> can be arbitrarily selected by the user by entering a location value into the position sensor <b>140</b>, directly calculated by the position sensor <b>140</b>, received from an external source, or a combination thereof. Detecting the current location <b>144</b> can include calculating, entering, receiving, or other actions for acquiring the current location <b>144</b>.</p><p id="p-0089" num="0088">The orientation sensor <b>142</b> is a sensor to determine the current orientation <b>146</b> of the user while viewing the scene. For example, the orientation sensor <b>142</b> can be an accelerometer, a mechanical orientation system, or a similar system. Although the orientation sensor <b>142</b> is described as a sensory device, it is understood that the orientation sensor <b>142</b> can support the direct entry of the orientation information. Thus, the current orientation <b>146</b> can be directly selected by the user by entering an orientation value into the orientation sensor <b>142</b>, calculated by the orientation sensor <b>142</b>, received from an external source, or a combination thereof. Detecting the current orientation <b>146</b> can include calculating, entering, receiving, or other actions for acquiring the current orientation <b>146</b>.</p><p id="p-0090" num="0089">The viewing unit <b>118</b> can continuously update the current location <b>144</b> and the current orientation <b>146</b> of the user. The current location <b>144</b> and the current orientation <b>146</b> of the viewing unit <b>118</b> can determine which of the points of interest <b>106</b> are nearby. Nearby is when the points of interest <b>106</b> are within a detection threshold <b>150</b> of the current location <b>144</b>. The detection threshold <b>150</b> is the distance where one of the points of interest <b>106</b> can be used. The detection threshold <b>150</b> can be different for each user. The detection threshold <b>150</b> can be based on the characteristics of one of the frames of reference <b>102</b>.</p><p id="p-0091" num="0090">The augmented reality system <b>100</b> can download information to the agents <b>116</b> to optimize bandwidth using a variety of techniques. For example, each of the system objects <b>126</b> can have an awareness radius in which it is active. Information about the system objects <b>126</b> that are beyond the radius can be downloaded, but they are only displayed when they are within the awareness radius.</p><p id="p-0092" num="0091">The ubiquity of wireless internet devices brings the ability to fuse the virtual world with the real world. Augmented reality can imbue any object or point in space with virtual content to provide a richer informational experience. On the Internet, content and links are available in in a web browser environment. This allows a user to read a page and follow links to related content. However, in normal circumstances, the user is limited to the computer screen and the web browser.</p><p id="p-0093" num="0092">In the augmented reality system <b>100</b>, content and links are tied to the physical world as expressed by the representations of people, buildings, cars, monsters, books, restaurant reviews, etc. Interaction is controlled by the physical proximity of real world objects and their augmented reality representations and enhancements. While normal web content can be accessed by entering or clicking on a defined link presented on a web page, the augmented world content can be enabled by moving and interacting with the physical world. The augmented reality world provides an analogous system to let you find and use the content physically surrounding you. In the augmented reality system <b>100</b> any object or location in the real world can be configured with a hyperlink to the content <b>108</b> or other actions.</p><p id="p-0094" num="0093">It has been discovered that the augmented reality system <b>100</b> improves the seamlessness of the display of information by limiting the visibility <b>134</b> and the persistence <b>136</b> of the system objects <b>126</b>. Reducing the amount of information presented to the user makes the user more efficient and makes the experience more natural.</p><p id="p-0095" num="0094">The augmented reality system <b>100</b> can allow any point in space or any object to be made into a link at any time by any user of the system. Users can be walking through a fog of links and services without knowing. The augmented reality system <b>100</b> can provide a shared experience where users can have their own view into this huge shared augmented reality.</p><p id="p-0096" num="0095">The augmented reality system <b>100</b> can perform computations, display media, show messages, support interactions between users, perform transactions, and other operations on a mobile computational device, such as the viewing unit <b>118</b>, that is triggered by the interaction of the device with the physical world around it based on sensor data, either alone or in conjunction with other networked devices. Although augmented reality can be thought of as the display of virtual content on a smartphone, tablet computer, e-reader, or smart glasses keyed off some real world object, such as a picture or Quick Response (QR) code, the augmented reality system <b>100</b> shows that it is not limited to showing pictures to people. The augmented reality system <b>100</b> supports the collaborative actions between devices moving about the real world, the creation and lifecycle management of virtual objects, and interacting between the real world and the augmented world, with content display as one important type of interaction, but not the only one.</p><p id="p-0097" num="0096">The implementation of the augmented reality system <b>100</b> requires the interoperation of a variety of technologies. The augmented reality system <b>100</b> is an integration of systems such as the computational cloud, servers, computer vision systems, three-dimensional graphics systems, Global Positioning Systems (GPS), wireless networking, etc. to provide a global space that can be shared by the users of the augmented reality system <b>100</b>.</p><p id="p-0098" num="0097">It has been discovered that displaying the system objects <b>126</b> based on the persistence <b>136</b> can improve performance by reducing system overhead. Tagging each of the system objects <b>126</b> with the persistence <b>136</b> if the system objects <b>126</b> are active at the current time <b>154</b> simplifies the detection of the system objects <b>126</b>.</p><p id="p-0099" num="0098">Referring now to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, therein is shown an example of a global coordinate system <b>202</b>. One of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can use the global coordinate system <b>202</b> to represent locations. For example, the global coordinate system <b>202</b> can be a GPS system providing latitude and longitude coordinates to designate locations on the globe.</p><p id="p-0100" num="0099">The GPS system can include GPS satellites <b>204</b> that can facilitate the reception of the GPS signals around a globe <b>206</b> to determine the location of structures in the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. A set of the coordinates can define the location of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the frames of reference <b>102</b>, or other objects in the augmented reality system <b>100</b>.</p><p id="p-0101" num="0100">Referring now to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, therein is shown an example of a building coordinate system <b>302</b>. The building coordinate system <b>302</b> can represent the locations based on the structure of the building. The building coordinate system <b>302</b> can identify a location based on a floor <b>304</b> and a room <b>306</b>. For example, the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of one of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> could be defined as room <b>2</b> on floor <b>3</b> of the building. Thus, one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can use the building coordinate system <b>302</b> to indicate the locations within the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. An entry point <b>308</b> can be used to enter the building coordinate system <b>302</b> from the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0102" num="0101">Referring now to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, therein is shown an example of one of the frames of reference using the building coordinate system <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref>. One of the points of interest <b>106</b> can be in one of the rooms in the building. One of the frames of reference <b>102</b> can use the building coordinate system <b>302</b> in a hierarchical configuration to represent locations.</p><p id="p-0103" num="0102">The building coordinate system <b>302</b> for one of the frames of reference <b>102</b> can include the entry point <b>308</b>, the room <b>306</b>, and a hallway <b>310</b>. For example, one of the frames of reference <b>102</b> can indicate that one of the points of interest <b>106</b> is in room <b>2</b> on the third floor of the building.</p><p id="p-0104" num="0103">The building coordinate system <b>302</b> can be configured in a variety of ways. For example, the building coordinate system <b>302</b> can represent locations using the floor number and a detailed description of the rooms and corridors on each of the floors. Each of the rooms and corridors can be uniquely identified in the building coordinate system <b>302</b>.</p><p id="p-0105" num="0104">The entry point <b>308</b> can represent the interface between two of the frames of reference <b>102</b>. For example, one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can navigate to the building in the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and then enter the building at the entry point <b>308</b>. After entering the building, the building coordinate system <b>302</b> can be used.</p><p id="p-0106" num="0105">Referring now to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, therein is shown a first example of the viewing unit <b>118</b>. The viewing unit <b>118</b> can include smart glasses that can be worn over the user's eyes, smart contact lenses, or a similar system. The viewing unit <b>118</b> can display a representation of the physical world and the content <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0107" num="0106">The viewing unit <b>118</b> can include displays, computing units, position sensors, orientation sensors, imaging units, and other similar components. Using the various sensors, the viewing unit <b>118</b> can present a display of the scene <b>124</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to the user with the content <b>108</b> oriented properly within the scene <b>124</b> based on the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the message <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0108" num="0107">The viewing unit <b>118</b> can display the real world imagery and provide an information overlay to display the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the proper location and orientation based on the real world. The sensors <b>138</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, such as the position sensor <b>140</b> and the orientation sensor <b>142</b>, can determine the current location <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the user in real time and in the real world and align the display of information with the real world based on the current orientation <b>146</b>. Thus, the viewing unit <b>118</b> can display the content <b>108</b> in the proper relative location when viewed through the viewing unit <b>118</b>.</p><p id="p-0109" num="0108">Referring now to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, therein is shown a second example of the viewing unit <b>118</b>. The viewing unit <b>118</b> can include a tablet computer or smart phone that can be viewed by the user. The viewing unit <b>118</b> can display the physical world and the content <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the augmented reality system <b>100</b>.</p><p id="p-0110" num="0109">The viewing unit <b>118</b> can also be configured to only display the content <b>108</b> and only when the viewing unit <b>118</b> is properly oriented and aimed at the location of the content <b>108</b>. For example, the user can hold up the tablet computer and point the camera at the scene. The tablet computer, acting as one of the agents <b>116</b>, can display the image from the camera and the augmented reality content overlay. Alternatively, the tablet computer can display specific content, such as a message.</p><p id="p-0111" num="0110">The viewing unit <b>118</b>, such as the tablet computer or smart phone, can include displays, computing units, position sensors, orientation sensors, imaging units, and other similar components. Using the various sensors, the viewing unit <b>118</b> can present a display of the scene <b>124</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to the user with the content <b>108</b> oriented properly within the scene <b>124</b> based on the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the message <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0112" num="0111">The viewing unit <b>118</b> can display the real world imagery and provide an information overlay to display the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the proper location and orientation based on the real world. The sensors <b>138</b>, such as the position sensor <b>140</b> and the orientation sensor <b>142</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, can determine the current location <b>144</b> of the user in the real world and align the display of information with the real world based on the current orientation <b>146</b>. Thus, the viewing unit <b>118</b> can display the content <b>108</b> in the proper relative location when viewed through the viewing unit <b>118</b>.</p><p id="p-0113" num="0112">Referring now to <figref idref="DRAWINGS">FIG. <b>7</b></figref>, therein is shown a first example of object visibility. The augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can include the content <b>108</b> intended for multiple users. Each piece of the content <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can have the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> configured to only be visible to a particular user or group of users.</p><p id="p-0114" num="0113">In an illustrative example, if two users seated at a shared table leave messages for two different users, then the messages can have the access rights <b>120</b> to only be visible to the intended user. Each of the users have created the message <b>114</b> for their intended viewer and associated the message <b>114</b> with one of the points of interest <b>106</b> on the item <b>110</b>, such as the table. The message <b>114</b> can include information such as a personal note <b>702</b> to Bill, an email <b>704</b> for Janet, a menu <b>706</b>, a daily special <b>708</b>, and a game notice <b>710</b> for users playing a game.</p><p id="p-0115" num="0114">Although the table includes both of the message <b>114</b>, each of the targeted users can only perceive the message <b>114</b> that was intended for them. Here, Bill will only perceive the message for Bill and only Janet will perceive the message for Janet. Each user can also perceive the avatar associated with the message <b>114</b>.</p><p id="p-0116" num="0115">In addition, all users will be able to perceive the message <b>114</b> for the daily special <b>708</b> that says &#x201c;Star Coffee Special $2&#x201d; assigned to one of the points of interest <b>106</b> on the item <b>110</b>, such as the table. The access rights <b>120</b> for the message <b>114</b> for &#x201c;Star Coffee&#x201d; are configured to be visible to all users.</p><p id="p-0117" num="0116">Each of the users of the augmented reality system <b>100</b> can be provided with their own individualized virtual content based on their individual context including the current location <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the current orientation <b>146</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the object orientation <b>130</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the field of view <b>132</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the access rights <b>120</b>. The unique view can be facilitated by the use of the viewing unit <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> which can include software running on devices such as a tablet computer, smart phones, e-readers, smart glasses, or a combination thereof.</p><p id="p-0118" num="0117">There are many reasons to control the visibility within one of the points of interest <b>106</b>. Because there can be any number of the points of interest <b>106</b> associated with any one of the object location <b>128</b>, it is essential to reduce the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of non-essential items to reduce the clutter of the scene <b>124</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In addition, some of the items <b>110</b> in the scene <b>124</b> may not be visible because the visibility may be dependent on time or other factors.</p><p id="p-0119" num="0118">The visibility <b>134</b> may be modified because one of the points of interest <b>106</b> may depend on an interaction with another object, item, or agent. For example, if one of the items <b>110</b> was enclosed in a virtual box, then it will only become visible when it is removed from the box. In another example, the items in a virtual treasure hunt may only be visible after other items have been encountered.</p><p id="p-0120" num="0119">The visibility <b>134</b> can be dependent on the privacy of the content <b>108</b>. For example, the privacy settings for the communications or the messages between the users can affect the visibility <b>134</b>. Private messages from one user to one or more other users should only be visible to the intended recipients. In another example, the message <b>114</b> from a teacher to a group of students should only be visible to the students and the teacher. In yet another example, a teacher taking a group of students on a trip to a museum may provide pre-seeded content that is both time and privacy protected.</p><p id="p-0121" num="0120">The visibility <b>134</b> can be controlled for commercial reasons. For example, entertainment content in the augmented reality system <b>100</b> can be made visible only to paying customers. Access to some of the content <b>108</b> can be limited based on membership, such as a club membership.</p><p id="p-0122" num="0121">The augmented reality system <b>100</b> can allow users to selectively control what they view. Because some users may not be interested in some content, the user profile can be used to control the visibility of some items they do and do not want to see. The user can identify categories of items or individual items to make visible and ignore other items.</p><p id="p-0123" num="0122">The visibility <b>134</b> can also be based on the distance between the user and the items <b>110</b>. The distance can be measured in different ways based on the coordinate system of the current one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, the items <b>110</b> that are far away can have reduced visibility. The items <b>110</b> that are in different rooms in the frame of reference <b>102</b> using the building coordinate system can be set to invisible.</p><p id="p-0124" num="0123">The visibility <b>134</b> can also be controlled based on the result of a query operation. For example, if the user queries the system and one of the items <b>110</b> is returned, the returned item can be made visible only after being returned in the search result.</p><p id="p-0125" num="0124">The visibility <b>134</b> is an access right and part of the general area of access control. Because the augmented reality system <b>100</b> sits between the agents <b>116</b> and the owner of each of the items <b>110</b>, there needs to be an easy, low cost way to address and determine the visibility of the items <b>110</b>. In some cases, the computational effort to determine the visibility of the items <b>110</b> can be offloaded to external data systems, such as systems owned or controlled by the owner of the items <b>110</b>.</p><p id="p-0126" num="0125">Referring now to <figref idref="DRAWINGS">FIG. <b>8</b></figref>, therein is shown a second example of the object visibility. The viewing unit <b>118</b> can be configured to only display the content <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> corresponding to the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the user of the viewing unit <b>118</b>.</p><p id="p-0127" num="0126">In an illustrative example, one of the points of interest <b>106</b> can be associated with one or more of the content <b>108</b> each having a different set of the access rights <b>120</b>. The viewing unit <b>118</b> can be configured to only display the content <b>108</b> and the message <b>114</b> having the marching set of the access rights <b>120</b> that are associated with the user of the viewing unit <b>118</b>. The content <b>108</b> can be associated with one of the points of interest <b>106</b>.</p><p id="p-0128" num="0127">In this example, the viewing unit <b>118</b> of user Bill can perceive the personal note <b>702</b> addressed to Bill located at one of the points of interest <b>106</b>. The user Bill can also view the general information such as the menu <b>706</b> and the daily special <b>708</b> that are visible to everybody. The viewing unit <b>118</b> cannot view the email <b>704</b> addressed to Janet because Bill does not have the access rights <b>120</b> to view that information. Further, if Bill is a user playing the game then a game notice <b>710</b> can been perceived saying &#x201c;You have found the Sword of Power.&#x201d;</p><p id="p-0129" num="0128">Referring now to <figref idref="DRAWINGS">FIG. <b>9</b></figref>, therein is shown an exemplary diagram of the relationship between the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The frames of reference <b>102</b> can include others of the frames of reference <b>102</b> and multiples of the points of interest <b>106</b>.</p><p id="p-0130" num="0129">One of the frames of reference <b>102</b>, such as the first frame <b>902</b>, can encompass one or more of the frames of reference <b>102</b> and the points of interest <b>106</b>. Within the first frame <b>902</b> there can be several of the frames of reference <b>102</b> and several of the points of interest <b>106</b>. a second frame <b>904</b>, a third frame <b>906</b>, a first point <b>908</b>, a second point <b>910</b>, a third point <b>912</b>, and a fourth point <b>914</b>. The frames of reference <b>102</b> can be in a hierarchical relationship. In some cases, the frames of reference <b>102</b> can be orthogonal and non-intersecting. In other cases, some of the frames of reference <b>102</b> can be overlapping.</p><p id="p-0131" num="0130">The points of interest <b>106</b> can have different configurations. For example, a first point <b>908</b> can be one of the points of interest <b>106</b> that is defined by a single point in the geometry of the first frame <b>902</b>. In another example, the first point <b>908</b> could be defined by a single room in one of the frames of reference <b>102</b> using a building geometry.</p><p id="p-0132" num="0131">In yet another example, the second point <b>910</b> can be one of the points of interest <b>106</b> that is defined by a circular area around the coordinates of the second point <b>910</b>. The second point <b>910</b> can be defined by a coordinate point and a description of the area around the coordinate point in the geometry of the first frame <b>902</b>. The description of the area can include a simple radius, a description of a geometrical shape, a mathematical equation, or a combination thereof.</p><p id="p-0133" num="0132">In still another example, the third point <b>912</b> can be one of the points of interest <b>106</b> defined by the coordinate of an arbitrary shape, such as a half circle or half sphere. The description of the area for the third point <b>912</b> can be defined by a set of coordinate points for the arbitrary shape, an image mask, relative coordinates based on an existing shape, or other descriptions of the arbitrary shape. If the third point <b>912</b> represents a sphere of a given radius centered on a point on a window of a building, then the description of the particular one of the points of interest <b>106</b> can include the portion of a sphere surrounding the third point <b>912</b> and within the building, resulting in a hemispherical volume for the third point <b>912</b> within the building. The portion of the sphere outside the building is not part of the third point <b>912</b>.</p><p id="p-0134" num="0133">Each of the points of interest <b>106</b> are only visible within one of the frames of reference <b>102</b>. This is shown by the third point <b>912</b> which is only visible from within the first frame <b>902</b>.</p><p id="p-0135" num="0134">The first frame <b>902</b> can include a second frame <b>904</b> and a third frame <b>906</b>. The second frame <b>904</b> can reside within the first frame <b>902</b>. The third frame <b>906</b> can reside entirely within the second frame <b>904</b> and the first frame <b>902</b>. This can be illustrated where the first frame <b>902</b> can be defined by physical geographic coordinates, such as GPS coordinates. The second frame <b>904</b> can utilize a building coordinate system. The third frame <b>906</b> can utilize a hierarchical coordinate system within the building coordinate system, such as a room-centric coordinate system, a book-centric coordinate system, a furniture-based coordinate system, or a similar descriptive coordinate system suitable for existing within the building coordinate system.</p><p id="p-0136" num="0135">Users and the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can move between the frames of reference <b>102</b>. For example, the agents <b>116</b> can move from the first frame <b>902</b> into the second frame <b>904</b> by interacting with one of the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> attached to the one of the points of interest <b>106</b> in the second frame <b>904</b>.</p><p id="p-0137" num="0136">The agents <b>116</b> can be in multiple frames at the same time. For example, when in a museum tour or when playing an augmented reality game, the agents <b>116</b> can be in one of the frames of reference <b>102</b> with the building coordinate system <b>302</b> and in another of the frames of reference <b>102</b> with the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0138" num="0137">In another example, the agents <b>116</b> can move into the third frame <b>906</b> by entering into proximity to one of the points of interest <b>106</b> within the third frame <b>906</b>. The third frame <b>906</b> can be one of the items <b>110</b> of one of the points of interest <b>106</b> of the second frame <b>904</b>.</p><p id="p-0139" num="0138">In an illustrative example, consider an augmented reality tour of a museum. The museum can be represented by the second frame <b>904</b> which can be one of the frames of reference <b>102</b> at a location within the first frame <b>902</b>, which can be a GPS coordinate system representing the real world. The second frame <b>904</b> could be the frame of reference <b>102</b> for the museum as a whole using a building-centric coordinate system. The user or the agents <b>116</b> can enter the second frame <b>904</b> based on the physical proximity to the museum in the real world and interacting with one of the items <b>110</b>, such as a virtual ticket window within the museum, to purchase the augmented reality museum tour.</p><p id="p-0140" num="0139">Similarly, the third frame <b>906</b> can be a painting in the museum. When wandering through the museum, the painting can stand out as being one of the points of interest <b>106</b>. But, when the user approaches the painting in the real world, the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can display the points of interest <b>106</b> that may be associated with the picture. The points of interest can be associated with the content <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with the picture such as an audio track describing the picture, the message <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with the painting style of the painting, or similar information.</p><p id="p-0141" num="0140">Referring now to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, therein is shown an exemplary diagram of the relationship between basic types. The basic types describe the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The base types can include the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and geometries.]</p><p id="p-0142" num="0141">The frames of reference <b>102</b> can include a first frame <b>1002</b>, a second frame <b>1004</b>, a third frame <b>1006</b>, and a fourth frame <b>1018</b>. The points of interest <b>106</b> can include a first point <b>1008</b>, a second point <b>1010</b>, a third point <b>1012</b>, a fourth point <b>1014</b>, and a fifth point <b>1036</b>. The geometries can include a first geometry <b>1020</b>, a second geometry <b>1022</b>, and a third geometry <b>1024</b>. The geometries can describe the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with the system objects <b>126</b>.</p><p id="p-0143" num="0142">Each of the frames of reference <b>102</b> can be associated with one of the geometries. For example, the first frame <b>1002</b> can be associated with the first geometry <b>1020</b>. Similarly the second frame <b>1004</b>, the third frame <b>1006</b>, and the fourth frame <b>1018</b>, can be associated with the second geometry <b>1022</b>, the third geometry <b>1024</b>, respectively.</p><p id="p-0144" num="0143">The points of interest <b>106</b> can be associated with the system objects <b>126</b>, such as the items <b>110</b> or other frames. The items <b>110</b> can include a first item <b>1038</b> or a second item <b>1040</b>.</p><p id="p-0145" num="0144">The first frame <b>1002</b> and the second frame <b>1004</b> can share the same geometry. Others of the frames of reference <b>102</b> can have their own independent geometries.</p><p id="p-0146" num="0145">The points of interest <b>106</b> can share items with one of the items <b>110</b> residing in both of the points of interest <b>106</b>. For example, the first item <b>1038</b> can be in both the second point <b>1010</b> and the fourth point <b>1014</b>. The first item <b>1038</b> can be visible in both of the frames of reference <b>102</b>. The first item <b>1038</b> can be associated with the content <b>108</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The content <b>108</b> can be a URL linked to an external data system.</p><p id="p-0147" num="0146">The first point <b>1008</b> and the third point <b>1012</b> can both be associated with one or more of the frames of reference <b>102</b>. Each of the points of interest <b>106</b> can be encountered by being near one of the points of interest <b>106</b>.</p><p id="p-0148" num="0147">The second item <b>1040</b> can be linked to the fourth frame <b>1018</b> to show that the second item <b>1040</b> can interact with the fourth frame <b>1018</b>. This can occur when one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can interact with the second item <b>1040</b> before being able to enter the fourth frame <b>1018</b>. For example, the second item <b>1040</b> can be a virtual door that needs to be opened before one of the agents <b>116</b> can enter the fourth frame <b>1018</b>.</p><p id="p-0149" num="0148">The frames of reference <b>102</b>, the points of interest <b>106</b>, and the items <b>110</b> can be the system objects <b>126</b> that have functions and properties. This is similar to the generalized paradigm of object oriented systems.</p><p id="p-0150" num="0149">One of the important properties of each of the system objects <b>126</b> in the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Not all of the agents <b>116</b> in one of the frames of reference <b>102</b> can be aware of all of the system objects <b>126</b>.</p><p id="p-0151" num="0150">The augmented reality system <b>100</b> can include a set of access tokens <b>1034</b> (AT). Each method of the system objects <b>126</b> can be associated with some set of the access tokens <b>1034</b>. Any one of the agents <b>116</b> associated with one of the access tokens <b>1034</b> has access to the system objects <b>126</b> controlled by the access tokens <b>1034</b>. The access tokens <b>1034</b> are a mechanism to implement the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the system objects <b>126</b>.</p><p id="p-0152" num="0151">The visibility <b>134</b> is an important property of the system objects <b>126</b> and only the agents <b>116</b> possessing the appropriate ones of the access tokens <b>1034</b> can view the related one of the system objects <b>126</b>. The same one of the access tokens <b>1034</b> can be used by more than one of the system objects <b>126</b>. For example, all of the system objects <b>126</b> associated with one game can use one of the access tokens <b>1034</b>, such as &#x201c;Token A&#x201d;, to enable the visibility <b>134</b> of the system objects <b>126</b>. Any one of the agents <b>116</b> having the &#x201c;Token A&#x201d; can perceive the system objects <b>126</b> in their game, but not necessarily the system objects <b>126</b> in other games.</p><p id="p-0153" num="0152">The access tokens <b>1034</b> can be organized into the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Each of the roles can provide the agents <b>116</b> with a common set of abilities and attributes.</p><p id="p-0154" num="0153">Referring now to <figref idref="DRAWINGS">FIG. <b>11</b></figref>, therein is shown an exemplary diagram of the relationship between the points of interest <b>106</b> and the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Each of the roles <b>122</b> can have a property to represent the visibility <b>134</b> and an insert function <b>1112</b>. The visibility property can determine the visibility <b>134</b> of the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with the points of interest <b>106</b>. The insert function <b>1112</b> can insert one of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> into one of the points of interest <b>106</b>.</p><p id="p-0155" num="0154">Each of the roles <b>122</b> can include the access tokens <b>1034</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> for controlling access to some number of the properties and methods. For example, a first token <b>1102</b> can allow access to the visibility <b>134</b> for the first point <b>908</b> and the second point <b>910</b>. The roles <b>122</b> can vary over time.</p><p id="p-0156" num="0155">The roles <b>122</b> can control access to the insertion methods. For example, a second token <b>1104</b> can allow access to the insertion method of the first point <b>908</b>. The third token <b>1106</b> can allow access to the insertion method of the second point <b>910</b>. The tokens can be grouped to form the roles <b>122</b>. The first role <b>1108</b> can include the first token <b>1102</b> and the second token <b>1104</b> for controlling the visibility <b>134</b> and the insert function <b>1112</b> of the first point <b>908</b>. The second role <b>1110</b> can include the first token <b>1102</b> and the second token <b>1104</b> for controlling the visibility <b>134</b> and the insertion function <b>1112</b> of the second point <b>910</b>.</p><p id="p-0157" num="0156">At any point in time, one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be associated with some of the roles <b>122</b> and some of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, one of the agents <b>116</b> can be associated with one of the frames of reference <b>102</b> by entering or leaving the physical location associated with the building of the frames of reference <b>102</b>. Each of the agents <b>116</b> can be associated with the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and optionally the object orientation <b>130</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> as specified by the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the current one of the frames of reference <b>102</b>. The frames of reference <b>102</b> can determine which of the points of interest <b>106</b> and the items <b>110</b> can be visible within the field of view <b>132</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the agents <b>116</b> based on the visibility <b>134</b> rights for the points of interest <b>106</b> and the items <b>110</b>.</p><p id="p-0158" num="0157">The augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is intended to convey the object location <b>128</b> and some of the state and state changes of the system objects <b>126</b> for the agents <b>116</b>. The augmented reality system <b>100</b> can store most of the information associated with the items <b>110</b>, but it can also retrieve some of the information from external systems.</p><p id="p-0159" num="0158">Referring now to <figref idref="DRAWINGS">FIG. <b>12</b></figref>, therein is shown an example of the visibility properties. One of the system objects <b>126</b> can be associated with a set of properties and the access rights <b>120</b>. The system objects <b>126</b> can be associated with the visibility <b>134</b> for a list of the points of interest <b>106</b>, the object location <b>128</b>, the object orientation <b>130</b>, the roles <b>122</b>, and token sets <b>1202</b>.</p><p id="p-0160" num="0159">The token sets <b>1202</b> can be an implementation of an authorization system. Each of the system objects <b>126</b> can have a set of the access rights <b>120</b>. The access rights <b>120</b> can control functions such as create, delete, update, and other object-specific methods for the object and higher-level interactions. The access rights <b>120</b> can be grouped into the roles <b>122</b> or be individually assigned. The roles <b>122</b> can include both the individual ones of the access rights <b>120</b> and the token sets <b>1202</b>.</p><p id="p-0161" num="0160">For example, the augmented reality system <b>100</b> can use a variant of the role Based Access Control (RBAC) technique. Each of the system objects <b>126</b> can have a set of operations, such as being able to have access to some property of the system objects <b>126</b>. Each of the operations can be associated with some set of the access tokens <b>1034</b>. The access tokens <b>1034</b> can be known as permissions. Each of the roles <b>122</b> can be sets of the access tokens <b>1034</b>. The token sets <b>1202</b> can be a set of the access tokens <b>1034</b> including sets that include the roles <b>122</b>.</p><p id="p-0162" num="0161">In an illustrative example, suppose there is a set of the system objects <b>126</b> each with a read and a write function. Further, one of the system objects <b>126</b> can be different from the others and need a different write function. The augmented reality system <b>100</b> can implement three of the access tokens <b>1034</b>: NormalRead, NormalWrite, and SpecialWrite. Most of the system objects <b>126</b> can use one of the access tokens <b>1034</b>, NormalRead, for the read operation and one of the access tokens <b>1034</b>, NormalWrite, for the write operation. Some of the system objects <b>126</b> can be assigned the access tokens <b>1034</b> for SpecialWrite instead of NormalWrite.</p><p id="p-0163" num="0162">The augmented reality system <b>100</b> can also include the access tokens <b>1034</b> to support three types of the roles <b>122</b>: Reader, Writer, SuperUser. The Reader role can be assigned the Normal Read token. The Writer role can be assigned the NormalRead and the NormalWrite tokens. The SuperUser role can be assigned the SpecialWrite token.</p><p id="p-0164" num="0163">The users associated with the Reader role can read. The users associated with the Writer role can write. And the users associated with the SuperUser role can have access to the SuperWrite token.</p><p id="p-0165" num="0164">The agents <b>116</b> can be assigned a set of the roles <b>122</b> to determine the possible interactions with the system objects <b>126</b> in the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The roles <b>122</b> associated with the agents <b>116</b> can vary over time.</p><p id="p-0166" num="0165">The roles <b>122</b> can be implemented in a variety of ways. Because the number of the roles in the augmented reality system <b>100</b> can be arbitrarily large, the system must be able to represent and index a large number of the roles <b>122</b>. Each of the system objects <b>126</b> can have their own set of the roles <b>122</b>. The system objects <b>126</b> can be associated with the roles <b>122</b> from different sources. The roles <b>122</b> are time and location sensitive. The implementation of the roles <b>122</b> for the system objects <b>126</b> provides the fine grained semantics of the augmented reality system <b>100</b>.</p><p id="p-0167" num="0166">The agents <b>116</b> may have many of the roles <b>122</b> associated with them at one time, but the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be configured to only track a subset at a particular time and location. For example, when the agents <b>116</b> are interacting with the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the user can acquire certain of the roles <b>122</b> for a limited duration in a limited locale. The roles <b>122</b> associated with one of the items <b>110</b> can be removed when no longer needed.</p><p id="p-0168" num="0167">For example, a game player could use one of the agents <b>116</b> associated with a general one of the roles <b>122</b> for making content and content updates from the game visible when the player is near the physical location of the content. But the player could additionally be assigned another one of the roles <b>122</b> associated with the game that provides the ability to interact with some of the items <b>110</b> in the game.</p><p id="p-0169" num="0168">In another example, one of the agents <b>116</b> can interact with some of the items <b>110</b> to establish a contract that provides a set of temporary rights, such as giving consent in a shopping mall to receive advertisements from nearby stores. The access rights <b>120</b> can allow the exchange of shopper information for discount codes.</p><p id="p-0170" num="0169">The visibility of the system objects <b>126</b> can be modified when the system objects <b>126</b> are added or updated in a particular location. However, an efficient mechanism must be used to map the visibility between the systems objects <b>126</b> and the agents <b>116</b> or the roles <b>122</b>. One alternative mechanism to associate the list of the system objects <b>126</b> that are visible to each of the agents <b>116</b> or the roles <b>122</b> is to use the access tokens <b>1034</b>. The augmented reality system <b>100</b> can be configured such that the number of the access tokens <b>1034</b> will not exceed the number of operations. One of the agents <b>116</b> can either perform an operation or not, so it is sufficient to have one of the access tokens <b>1034</b> per operation. Each of the roles <b>122</b> can either have the access tokens <b>1034</b> or not. In practice, the total number of the access tokens <b>1034</b> will be much less. Similarly, the number of the roles <b>122</b> should no significantly exceed the number of the agents <b>116</b>. Each of the agents <b>116</b> may have some unique abilities, such as self-modification, but the number of the shared capabilities will be far smaller than the total number of users.</p><p id="p-0171" num="0170">Alternatively, in cases where many of the system objects <b>126</b> in one of the frames of reference <b>102</b> are similar, then groups of the system objects <b>126</b> can be operated on as a group. The group of the system objects <b>126</b> can be associated with the token sets <b>1202</b> that include a variety of properties and methods that apply to all of the system objects <b>126</b> in the group.</p><p id="p-0172" num="0171">The visibility <b>134</b> can be implemented in a variety of ways. For example, the first level of the visibility <b>134</b> is associated with the roots of the coordinate system that the agents <b>116</b> have perceived or have access to. The agents <b>116</b> can have access to a basic set of the roles <b>122</b> associated with the global space of one of the frames of reference <b>102</b>, such as one using a GPS coordinate system. The roles <b>122</b> can expose some of the items <b>110</b> near the current location <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of one of the agents <b>116</b>. Interaction with the items <b>110</b> can expose additional ones of the roles <b>122</b> related to the items <b>110</b>.</p><p id="p-0173" num="0172">The roles <b>122</b> associated with the visibility <b>134</b> can be linked to one or more of the frames of reference <b>102</b> and the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with them. The concept of a geometry is a technique to better segment the points of interest <b>106</b> in a more organized and coherent fashion.</p><p id="p-0174" num="0173">For example, if the user is in a museum, then the point of interest <b>106</b> can be located within the frame geometry <b>104</b> of the museum, such as using a building coordinate system rather than GPS. Using a building coordinate system can be much more accurate, as GPS is limited in accuracy and may not even be available indoors, while a model of the interior of the museum can be used for the building coordinate system and enhanced with other location information such as beacons or WiFi access point information.</p><p id="p-0175" num="0174">Referring now to <figref idref="DRAWINGS">FIG. <b>13</b></figref>, therein is shown an example of the roles <b>122</b> and the token sets <b>1202</b>. The agents <b>116</b> can be associated with the roles <b>122</b> which can be further associated with one of the sets of tokens <b>1202</b>.</p><p id="p-0176" num="0175">Each of the agents <b>116</b> can have a variety of the roles <b>122</b>. However, because the number of the roles <b>122</b> can proliferate quickly, there is a need to control the total number of the roles <b>122</b> associated with each of the agents <b>116</b> to better manage system resources. Further, when one of the agents <b>116</b> moves, the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> must update the set of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> nearby including detecting and managing any of the points of interest <b>106</b> that have been added or updated. This process must be managed efficiently in terms of system resources.</p><p id="p-0177" num="0176">To do this efficiently, the amount of data manipulated by the augmented reality system must be reduced using a variety of techniques. For example, working memory should be used to determine what real-time actions must be taken. The code and data structures should be cached in physical memory or otherwise organized to reduce the overhead of disk accesses. Using locally cached information can improve the performances when informing the users about the creation, updating, and deletion of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Further, system maintenance tasks, such as updates and other notification, can be distributed around the augmented reality system <b>100</b> to increase the degree of parallelism.</p><p id="p-0178" num="0177">Each of the system objects <b>126</b> has a set of the roles <b>122</b> associated with the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, as well as a visibility range <b>1302</b> in which it is visible, expressed in terms of the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> it belongs to. The visibility range <b>1302</b> is the distance where one of the system objects <b>126</b> can been visible. If one of the system objects <b>126</b> is updated or added to the augmented reality system <b>100</b>, then only the agents <b>116</b> with the appropriate one of the roles <b>122</b> and in the current location <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> properly configured with respect to the geometry need to be updated.</p><p id="p-0179" num="0178">Referring now to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, therein is shown an example of the persistence <b>136</b>. The persistence <b>136</b> is determined by a set of properties, often hierarchical, that can define lifecycle of the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of one of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0180" num="0179">In an illustrative example, a user can create one of the system objects <b>126</b>, such as the personal note <b>702</b>, and associate it with the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to be visible to the user Bill. One of the items <b>110</b> associated with one of the points of interest <b>106</b>, such as a table, can be associated with a message for the daily special <b>708</b> and the personal note <b>702</b>. Because the message is time-sensitive, the personal note <b>702</b> can be configured to have the persistence <b>136</b> set to 5 minutes. After the personal note <b>702</b> is created, it will automatically be deleted after 5 minutes. However, the persistence <b>136</b> of the daily special <b>708</b> can last for the entire day.</p><p id="p-0181" num="0180">The persistence <b>136</b> is inherently tied to the interface between the real world and the augmented reality world. Preserving the persistence <b>136</b> of the system objects <b>126</b> can be negatively impacted because of the unreliability of the sensor needed to locate the system objects <b>126</b>, such as the items <b>110</b>, in the real-world. The processing required to properly manage the current location <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the system objects <b>126</b> must be robust.</p><p id="p-0182" num="0181">To efficiently manage the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the points of interest <b>106</b> can be segmented based on the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Each of the points of interest <b>106</b> can be associated with one of the frames of reference <b>102</b> to accurately determine the current location <b>144</b> of the points of interest <b>106</b> based on the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the selected one of the frames of reference <b>102</b>.</p><p id="p-0183" num="0182">Two of the frames of reference <b>102</b> are generally constant. First, the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> using GPS location and time. Second, a personal coordinate system of one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. This can be a coordinate system that physically tracks one of the agents <b>116</b> in the real world. Other coordinate systems are entered and exited based on the interactions with the points of interest <b>106</b> by the agents <b>116</b>.</p><p id="p-0184" num="0183">Each of the agents <b>116</b> can enter and interact with one of the frames of reference <b>102</b> in a variety of ways. For example, the agents <b>116</b> can enter the frames of reference <b>102</b> by fiat. Each of the agents <b>116</b> is automatically in two of the frames of reference <b>102</b>, the global coordinate system <b>202</b> and a personal coordinate system <b>1406</b> of one of the agents <b>116</b>. The agents <b>116</b> can also be associated with additional ones of the frames of reference <b>102</b>, either through external interactions such as by buying rights or joining a game.</p><p id="p-0185" num="0184">In another example, one of the frames of reference <b>102</b> can be linked to an existing one of the frames of reference <b>102</b>. For example, the global coordinate system <b>202</b> provides a persistent hierarchy that the agents <b>116</b> are always aware of. One of the agents <b>116</b> can become aware of certain ones of the points of interest <b>106</b> as it maneuvers around the space defined in the global coordinate system <b>202</b> as indicated by the GPS coordinates. One of the agents <b>116</b> can simultaneously enter another one of the frames of reference <b>102</b>, such as the building coordinate system, when it has access to the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the system objects <b>126</b> related to that one of the frames of reference <b>102</b>.</p><p id="p-0186" num="0185">One of the frames of reference <b>102</b> can announce itself, such as by using a beacon signaling the proximity to one of the agents <b>116</b> in the real world. For example, where one of the frames of reference <b>102</b> is an airplane, the entire local system can move together and the items <b>110</b> in the frame of reference <b>102</b> of the airplane can be controlled locally as part of the local frame. The system objects <b>126</b> and the points of interest <b>106</b> associated with the airplane can be discovered when the airplane is approached in the real world. The points of interest <b>106</b>, such as the seats in the airplane, may only be visible when the agents <b>116</b> are interacting with the frames of reference <b>102</b> associated with the airplane, such as when using a flight reservation system.</p><p id="p-0187" num="0186">The frames of reference <b>102</b> can also be discovered through a query <b>1408</b>. The augmented reality system <b>100</b> can support querying to search the system objects <b>126</b> within the system. The query <b>1408</b> is a search operation to find the system objects <b>126</b> that match a query term <b>1410</b>. The query term <b>1410</b> is a list of search terms or objects to be found. A query result <b>1412</b> is returned after executing the query <b>1408</b>. The query result <b>1412</b> is a list or set of the system objects <b>126</b> that match or are associated with the query term <b>1410</b>.</p><p id="p-0188" num="0187">Using a book example, one of the agents <b>116</b> could make the query <b>1408</b> in one of the frames of reference <b>102</b>, such as a book store, and retrieve a set of books. Each of the books can be represented in one of the frames of reference <b>102</b> using a book coordinate system. Thus, one of the agents <b>116</b> can interact with the points of interest <b>106</b> within a book, such as leaving an annotation attached to a paragraph on one of the pages.</p><p id="p-0189" num="0188">It has been discovered that implementing a hierarchical approach with multiple types of the frames of reference <b>102</b> increases functionality for the augmented reality system <b>100</b>. By adding annotations to the points of interest <b>106</b> of the book, the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be controlled to limit access to the annotations to only some of the agents <b>116</b> when they are in one of the frames of reference <b>102</b>. Providing a means to localize the scope can reduce the data traffic, simplifies the user interface, and improves the overall user experience.</p><p id="p-0190" num="0189">When any one of the system objects <b>126</b> is created, it must be associated with at least one of the frames of reference <b>102</b>. Each of the system objects <b>126</b>, such as one of the points of interest <b>106</b> or one of the items <b>110</b>, can have a persistence extent <b>1404</b>, such as a time to live parameter. The persistence extent <b>1404</b> of one of the system objects <b>126</b> can control the length of time it will exist within the augmented reality system <b>100</b>.</p><p id="p-0191" num="0190">In static circumstances, the items <b>110</b> can reside at one of the points of interest <b>106</b> in a particular one of the frames of reference <b>102</b>. However, if one of the items <b>110</b> is in motion, such one associated with one of the points of interest <b>106</b> like a sword being swung in an augmented reality game, then the augmented reality system <b>100</b> can define one of the points of interest <b>106</b> that covers a three-dimensional volume. The items <b>110</b> can be displayed anywhere within the volume of the particular one of the points of interest <b>106</b>. Exact information about the whereabouts of one of them items <b>110</b> can be provided by the game server or another application specific system. This can allow the application specific systems to process complex, local interactions to take the processing burden off of the augmented reality system <b>100</b>. When the particular one of the items <b>110</b> becomes stationary, the augmented reality system <b>100</b> can associate it with a stationary one of the point of interest <b>106</b>. Alternatively, one of the agents <b>116</b> can be designated as one of the frames of reference <b>102</b> and the items <b>110</b> associated with this one of the frames of reference <b>102</b> can be tracked by other ones of the agents <b>116</b>.</p><p id="p-0192" num="0191">In an illustrative example, the augmented reality system <b>100</b> can provide one of the frames of reference <b>102</b> to represent the entire universe. Since most of the agents <b>116</b> would not need to access the information outside of the global coordinate system <b>202</b> used to represent the earth, the agents <b>116</b> not interested in astronomical phenomenon would never enter it.</p><p id="p-0193" num="0192">The augmented reality system <b>100</b> can include a number of the items <b>110</b>. Each of the items <b>110</b> can have associated content and can be linked to one or more of the points of interest <b>106</b>. Each of the points of interest <b>106</b> can be located in one of the frames of reference <b>102</b> and has some associated information. Each of the frames of reference <b>102</b> includes the frame geometry <b>104</b> that specifies to the agents <b>116</b> how to use and access the associated ones of the points of interest <b>106</b>. The frame geometry <b>104</b> can also help determine the visibility <b>134</b> of the points of interest <b>106</b>. If one of the points of interest <b>106</b> is visible to one of the agents <b>116</b> then that agent can interact with the items <b>110</b> associated with it.</p><p id="p-0194" num="0193">The augmented reality system <b>100</b> can track the agents <b>116</b> including knowing the current location <b>144</b>, the current one of the frames of reference <b>102</b>, the token sets <b>1202</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, the access rights <b>120</b>, and other properties, parameters, and methods. The augmented reality system <b>100</b> can provide the information needed to locate the points of interest <b>106</b> and to manipulate the items <b>110</b> associated with each of the points of interest <b>106</b>.</p><p id="p-0195" num="0194">It has been discovered that displaying the system objects <b>126</b> based on the persistence <b>136</b> and the persistence extent <b>1404</b> can improve performance by reducing system overhead. Tagging each of the system objects <b>126</b> with the persistence <b>136</b> if the system objects <b>126</b> are active at the current time <b>154</b> simplifies the detection of the system objects <b>126</b>.</p><p id="p-0196" num="0195">Referring now to <figref idref="DRAWINGS">FIG. <b>15</b></figref>, therein is shown an example of multi-level maps <b>1502</b>. The multi-level maps <b>1502</b> show the hierarchical structure of one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0197" num="0196">At a high level of abstraction, the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can represent an arbitrarily large number of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> scattered around a surface, each visible within some range. This can be represented using the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> and using the multi-level maps <b>1502</b> that partition different levels of detail. This is similar to the way that Google&#x2122;, Amazon&#x2122;, and other mapping systems represent large areas. The multi-level maps <b>1502</b> show the same terrain at various levels of scaling. Each level provides some amount of detail based on what is visible at that level, or what was considered relevant by the map creator, with more detail presented at higher levels of scaling.</p><p id="p-0198" num="0197">The augmented reality system <b>100</b> can use a similar technique to represent the frames of reference <b>102</b> at different levels of granularity. Each level contains a number of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, but each one of the points of interest <b>106</b> has the visibility range <b>1302</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref> associated with it. The level of the map can be proportional to the visibility range <b>1302</b> of the points of interest <b>106</b> that it contains. This can define the set of the system objects <b>126</b> visible at some level of resolution.</p><p id="p-0199" num="0198">For example, in a scene zoomed out to view the entire United States of America, the Pentagon might be visible as a landmark but would disappear as the user zooms in to the western United States. For a military client using the augmented reality system <b>100</b>, however, it can be important to always know where the Pentagon is. Appearance at a level indicates the points of interest <b>106</b> that are visible to users within the area displayed, so the set of the points of interest <b>106</b> to display can be additive going down.</p><p id="p-0200" num="0199">When navigating to a lower levels of the multi-level maps <b>1502</b>, the degree of detail can get down to the point where one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is in physical proximity to the one of the points of interest <b>106</b> and further detail cannot be resolved because of the limits of measuring the physical location of the user. At some point, the successive location refinement breaks down and exceeds the granularity of the GPS sensors and the degradation of GPS signals and the exact mapping of the system objects <b>126</b> in the augmented reality system <b>100</b> may no longer be correct.</p><p id="p-0201" num="0200">For example, a virtual object can be associated with one of the points of interest <b>106</b> located on the corner of a table. If the table is moved and the resolution of the system exceeds the GPS resolution, then the virtual object may now appear to be floating in space. Therefore the system needs to be able to combine results from multiple algorithms. The augmented reality system <b>100</b> can use a variety of scene analysis techniques and incorporate the generic functionality as needed to identify and manage scene information.</p><p id="p-0202" num="0201">At this level of detail, the augmented reality system <b>100</b> can utilize another of the frames of reference <b>102</b>, such as a room coordinate system, and use the physical geometry of the location, orientation, and additional semantics of placement to identify one of the points of interest <b>106</b> as being attached to the representation of the table, as opposed to a particular GPS coordinates. The additional information can include computer vision information, SLAM, beacon data, physical sensor data, tagged data points, physical mapping data, or similar information. This additional information can be extremely important. For example, given two of the points of interest <b>106</b> that are in two stores in a mall adjacent to each other, placed against the wall. Person A is in store <b>1</b> and person B is in store <b>2</b>, each looking at the wall. Each person should just perceive one of the points of interest <b>106</b> and not the other in the other store.</p><p id="p-0203" num="0202">In terms of coordinates, the points of interest <b>106</b> could be just a couple of inches from each other and thus difficult to physically differentiate. However, given diagram of the mall architecture, the augmented reality system <b>100</b> using a building coordinate system for one of the frames of reference <b>102</b> would allow the points of interest <b>106</b> to be differentiated. Additionally, &#x201c;knowing&#x201d; that the user is in a store with a wall between the points of interest <b>106</b>, the augmented reality system <b>100</b> can determine the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of each of the points of interest <b>106</b> and display the points of interest <b>106</b> appropriately.</p><p id="p-0204" num="0203">The different ones of the frames of reference <b>102</b> can be used where the GPS information is unavailable or unreliable. For example, using the building coordinate system <b>302</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> can allow the visual information about the environment of one of the agents <b>116</b> to be used to determine the finer grained location of one of the system objects <b>126</b>. If one of the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is located underneath a virtual table in the corner of a room, then the position of the item can be determined simply by knowing the GPS location of the table and knowing that wherever the table is located, the message is underneath. The exact location of the message can be determined relative to the physical structure of the table because the item is associated with the table.</p><p id="p-0205" num="0204">In terms of the persistence <b>136</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the items <b>110</b>, one of the system objects <b>126</b> can exist if it can be located in the appropriate data structure representing the system objects <b>126</b> in one of the frames of reference <b>102</b>. However, because the system objects <b>126</b> can enter and leave one of the frames of reference <b>102</b> at any time, the augmented reality system <b>100</b> must keep track of where the agents <b>116</b> are located so they can receive updates for relevant changes to the state and location of the system objects <b>126</b>.</p><p id="p-0206" num="0205">The augmented reality system <b>100</b> can be implemented in a variety of ways. In addition, certain operations within the augmented reality system <b>100</b> can be optimized to reduce the overall system computational load. For example, the one of the operations can include finding all of the system objects <b>126</b> for some set of the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with one of the points of interest <b>106</b>.</p><p id="p-0207" num="0206">In addition, another of the optimized operations can package the necessary data for the system objects <b>126</b> that are transferred to a different location or a different one of the frames of reference <b>102</b>. Because the amount of data may be arbitrarily large, it can be impractical to maintain all data in active memory. The optimization can minimize the number of disk accesses by reducing the amount of data. Reducing the data can also minimize the latency of sending the system objects <b>126</b> data to one of the agents <b>116</b>. For example, packaging the system objects <b>126</b> can include data compression to reduce the overall data size.</p><p id="p-0208" num="0207">Other optimizations can include adding and moving the system objects <b>126</b>, and finding all of the agents <b>116</b> in a defined area, and broadcasting updates to a set of the agents <b>116</b>. The broadcast updates can include changes to the system objects <b>126</b> such as when the system objects are new, moved, and removed. The optimizations can be implemented based on the coordinate system used by each of the frames of reference <b>102</b>.</p><p id="p-0209" num="0208">Although the augmented reality system <b>100</b> may appear to provide a single, seamless continuum of the objects, the levels, and the frames of reference <b>102</b>, it can be implemented in a decentralized manner. Some of the more complex of the frames of reference <b>102</b> can be managed by local systems or application specific systems to reduce the processing load on the augmented reality system <b>100</b>. This can include the frames of reference <b>102</b> such as airplanes, stores, malls, building, gaming establishments, libraries, or other complex systems that are conceptually embedded in a larger one of the frames of reference <b>102</b>, such as one using the global coordinate system <b>202</b>. These systems may require large amounts of processing power to interact with. For example, when one of the agents <b>116</b> enters a store, the particular one of the frames of reference <b>102</b> for the store can be managed by a local server. This can improve the performance of the system when dealing with rapid, real-time changes, such as during action-intensive portions of a game.</p><p id="p-0210" num="0209">Referring now to <figref idref="DRAWINGS">FIG. <b>16</b></figref>, therein is shown an example of the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref> for the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The global coordinate system <b>202</b> can be used for navigation within one of the frames of reference <b>102</b>.</p><p id="p-0211" num="0210">In the global coordinate system <b>202</b>, the world can be divided into segments <b>1602</b>. The segments <b>1602</b> are sized to be manageable pieces. For example, the world can be divided into the segments <b>1602</b> by longitude and then latitude. In another example, the world can be divided into the segments <b>1602</b> using a binary tree structure.</p><p id="p-0212" num="0211">There is a need for a rapid way to check the segments <b>1602</b> for the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that may apply to them. A rapid way to do that is to associate each region with a Bloom filter <b>1604</b>. The Bloom filter <b>1604</b> is a test to determine whether an element is a member of a set.</p><p id="p-0213" num="0212">For example, each one of the roles <b>122</b> can be assigned a unique integer and is then hashed to some set of values, representing the bits to be checked. This can be done when each of the roles <b>122</b> is created, so the hashes only needs to be calculated once. Similarly, each of the users can have a signature represented by OR-ing of the each of the hashes of the roles <b>122</b>. This gives a list of bits that need to be set in the Bloom filter <b>1604</b>, each with a list of the roles <b>122</b> dependent on that bit. The Bloom filter <b>1604</b> can determine if there is any relevant content in one of the segments <b>1602</b> by checking the Bloom filter <b>1604</b> for each of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, removing any of the roles <b>122</b> where the associated bit is not set, and then seeing which of the roles <b>122</b> are left. Based on the way the Bloom filter <b>1604</b> works, there is a very high probability that each of the segments <b>1602</b> have some of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> with the roles <b>122</b> that remain.</p><p id="p-0214" num="0213">In another alternative example, each of the access token <b>1034</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref> can be assigned a unique integer. Each of the roles <b>122</b> can be represented by the union (OR operation) of the bits of its access tokens <b>1034</b> and each of the agents <b>116</b> similarly having the union of the bits of the roles <b>122</b> associated to it.</p><p id="p-0215" num="0214">Each operation can have another of the access tokens <b>1034</b>, which can be represented by a unique integer and then hashed to some values, and calculated once. Each of the roles <b>122</b> can include a set of the access tokens <b>1034</b>, so it has a set of bits from the union of the bits for all its access tokens <b>1034</b>. Finally, each of the agents <b>116</b> can have a set of the roles <b>122</b> which is also representable as the union of the bits from the roles <b>122</b> associated with it.</p><p id="p-0216" num="0215">At status checking time, some set of these bits can be checked. Since each bit is associated with at least one of the tokens, any bit not set means at least one of the access tokens <b>1034</b> is eliminated. If there is no relevant content in the segment, then processing is reduced because it is likely that only one bit per token needs to be checked.</p><p id="p-0217" num="0216">The bits representing the access tokens <b>1034</b> can be put in an array using a first in, first out (FIFO) queue for efficiency. The first bits from each array can be entered in the queue. The augmented reality system <b>100</b> can repeatedly de-queue a bit and check it. If the bit is present, then place the next bit from that one of the access tokens <b>1034</b> in the queue. If the bit is not present, then that one of the access tokens <b>1034</b> is eliminated from further consideration. If all the bits from at least one of the arrays in the filter is found, then the system can check for content.</p><p id="p-0218" num="0217">In an illustrative example, a complete Earth-based system can have approximately 510 trillion square meters of surface area. It would be reasonable to use the Bloom filter <b>1604</b> operation on each of the frames of reference <b>102</b>, especially as most locations will have few of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. But even breaking the surface into only one billion of the segments <b>1602</b> means that one gets from the surface of the earth to a square meter in 49 levels or binary orders of magnitude. If we only consider areas of 10 meter<sup>2</sup>, then only 48 levels are required to uniquely identify 100 trillion of the segments <b>1602</b>, if the segments <b>1602</b> are structured into a segment tree <b>1606</b>.</p><p id="p-0219" num="0218">Each level can correspond to the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the system objects <b>126</b>. At the top level, one of the system objects <b>126</b> is only visible within 10 meters is at the 10 meter<sup>2 </sup>level. Most the system objects <b>126</b> should be concentrated at the lower levels with visibility of just a few meters or kilometers and only a few should have visibility beyond that. In addition, the points of interest <b>106</b> will generally be highly concentrated in inhabited areas. It is estimated that 95% of the earth's population is concentrated in 10% of the surface area and urban areas cover just 3%. Even these contain large areas which would have few of the points of interest <b>106</b>, which would probably be clustered around a small number of areas.</p><p id="p-0220" num="0219">One way of organizing the segments <b>1602</b> that only requires creating occupied segments is to identify them with a technique such as geo-hashing, where the bits of the hash are from successively adding longitude and latitude bits, so longer values are more accurate than shorter ones. In effect, the first bit specifies east or west hemisphere, the second specifies north or south of the equator, etc. Each bit adds to the precision. In general, therefore, locations that are near each other will hash to strings with a common prefix.</p><p id="p-0221" num="0220">To accomplish this, the segments <b>1602</b> of varying sizes can be organized in the segment tree <b>1606</b>, where nodes <b>1608</b> of the segment tree <b>1606</b> have some substring of an address. The path through the segment tree <b>1606</b> from a root <b>1610</b> to leaves <b>1612</b> contains the whole key, but each interior one of the nodes <b>1608</b> can contain a substring from its parent down to the area actually containing something.</p><p id="p-0222" num="0221">If everything being tracked is within a few meters of each other, there is only one of the nodes <b>1608</b> with a whole key. For example, if everyone were in either NYC or Moscow, there would be a root node, left and right hemisphere the nodes <b>1608</b> with long substrings, and then a set of the nodes <b>1608</b> near the leaves <b>1612</b> the frames of reference <b>102</b> the local dispersion. However, if the tracked system objects <b>126</b> are evenly spread around the world, then each one of the nodes <b>1608</b> would only have one bit of the hash and the full tree would be there. Each segment would need a &#x201c;dirty word&#x201d; giving the time of the last update. One could have this refer to whether there's been a change in the subtree, or just one of the segments <b>1602</b>. We prefer the latter, as writes are expensive, especially writes that imply synchronization over large parts of the data structure.</p><p id="p-0223" num="0222">This way the size of the segment tree <b>1606</b> is proportional to the dispersion of tracked the system objects <b>126</b> around the earth. In particular, the leaves will closely match this dispersion, and they consist of at least half of the nodes <b>1608</b> in the segment tree <b>1606</b>. Where there are few of the points of interest <b>106</b>, there is no need to generate the segment tree <b>1606</b> all the way to the leaves. If there is a fixed cost to looking at another level of magnification, then there is some number of the points of interest <b>106</b> at which it's less costly to store them at a level of higher magnification and do some filtering than to break a level up so each of the points of interest <b>106</b> is at the right range.</p><p id="p-0224" num="0223">The segment tree <b>1606</b> can be used for both inserting the system objects <b>126</b> and identifying affected ones of the agents <b>116</b>. The size of the segment tree <b>1606</b> depends on the number of the nodes <b>1608</b>, and the number of the nodes <b>1608</b> is a reflection of the time it takes to process the contents of one of the nodes <b>1608</b>. For example, if we have the segment tree <b>1606</b> in which we've inserted one of the points of interest <b>106</b>, then the agents <b>116</b> entering the system will traverse down the segment tree <b>1606</b> to locate the one of the points of interest <b>106</b> it needs. At any one of the nodes <b>1608</b> it may need to do some processing. This will take a certain amount of time. One of the nodes <b>1608</b> can be split when the time to process its contents is above some threshold, such as ensuring a service level agreement (SLA) of under 200 milliseconds.</p><p id="p-0225" num="0224">Each of the system objects <b>126</b> that belong in one of the segments <b>1602</b> can be arranged so their data on disk to can be read in a small number of disk accesses. For example, a system such as HBase can store all the data for a segment so that it is grouped together. Once all the data is read, it's easy to filter through to retain only those necessary for the agents <b>116</b>. Traversing down the segment tree <b>1606</b> can be done in parallel, with the forking tasks to look at all the sub-segments of any of the segments <b>1602</b>.</p><p id="p-0226" num="0225">Nevertheless it's necessary to consider the possibility of congestion at any level of the segment tree <b>1606</b>, particularly near the root <b>1610</b>. There is a high likelihood that there would be certain levels of granularity (global, countrywide, regional, metropolitan) at which there may be many of the system objects <b>126</b>. This would require significantly more bits for the Bloom filter and storing data for a level in multiple chunks to avoid reading too much data over and over.</p><p id="p-0227" num="0226">In an illustrative example, examine a system with one trillion of the system objects <b>126</b>, one billion of the roles <b>122</b>, and hardware supporting a processing threshold on the order of one thousand of the system objects <b>126</b> visible to any one of the agents <b>116</b> at a time. This would require approximately one billion of the segments <b>1602</b>. To have a miss rate of 1%, each of these would require 10 K bits, or 1250 bytes, or 10 terabytes. If there are about 1 kilobytes of storage for each one of the points of interest <b>106</b> for recognition and any initial data (or approximately 1 MB/segment), then the system will need approximately 1 petabyte of storage to store the entire system. If the object data is stored in an optimal fashion with only a couple of disk accesses for each one of the segments <b>1602</b>, then an estimated response time of under 200 milliseconds can be achieved for each megabyte of data that is streamed to the client.</p><p id="p-0228" num="0227">There are two kinds of the trees needed for one of the frames of reference <b>102</b> using the global coordinate system <b>202</b>. The first is the segment tree <b>1606</b> of the points of interest <b>106</b>, where each of the points of interest <b>106</b> is roughly at a level corresponding to its visibility range. Each of the nodes <b>1608</b> is associated with the Bloom filter <b>1604</b> to identify the visibility the roles <b>122</b> of the points of interest <b>106</b>. The agents <b>116</b> can enter the system at some GPS coordinate associated with one of the points of interest <b>106</b>.</p><p id="p-0229" num="0228">The segment tree <b>1606</b> can be traversed to determine all of the points of interest <b>106</b> to the agents <b>116</b>. This can include both those having an appropriate visibility one of the roles <b>122</b> and those within range. If one of the agents <b>116</b> moves and is no longer in the same segment, then it needs to traverse the segment tree <b>1606</b> again from the common parent of the old and new ones of the segments <b>1602</b> down to its new segment for the points of interest <b>106</b>. The previous set of the points of interest <b>106</b> can be discarded. This will generally be a small number of the segments <b>1602</b>, but there can be pathological cases in any given segmentation scheme.</p><p id="p-0230" num="0229">Also, depending on where one of the agents <b>116</b> is in one of the segments <b>1602</b>, it may be necessary to traverse multiple paths to ensure everything within a given distance is covered. Alternatively, the points of interest <b>106</b> can be placed in all of the segments <b>1602</b> that intersect its range, but that may require managing multiple inserts.</p><p id="p-0231" num="0230">Given the likely uneven dispersion of the system objects <b>126</b>, it can be possible to create a non-uniform segmentation at larger levels, such as ensuring that populated regions all have a common root far down the system, and putting large uninhabited areas together. This can minimize the number of paths that need to be traversed, but that would have its own overhead. Traversing the segment tree <b>1606</b> and reading data from disk can be performed in parallel, with the results packaged together the frames of reference <b>102</b> transmission to the agents <b>116</b>.</p><p id="p-0232" num="0231">Referring now to <figref idref="DRAWINGS">FIG. <b>17</b></figref>, therein is shown an example of a visibility tree <b>1702</b>. The visibility tree <b>1702</b> can map the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the segments <b>1602</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref>. The visibility tree <b>1702</b> is the second type of tree needed for the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> using the global coordinate system <b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>.</p><p id="p-0233" num="0232">The visibility tree <b>1702</b> for each of the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can control the visibility <b>134</b>. The nodes <b>1608</b> of the visibility tree <b>1702</b> just have lists of the visibility <b>134</b> of the agents <b>116</b> within that geographic region and their last known positions. When one of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is updated or created, then it must inform all users within range. It must traverse the visibility tree <b>1702</b> to find the segments <b>1602</b> potentially containing users and then check for the actual distance from the agents <b>116</b>. The constraint on the size of the one of the nodes <b>1608</b> is the time it takes to make the necessary comparisons. Even if the agents <b>116</b> with the roles <b>122</b> controlling the visibility <b>134</b> are widely dispersed, there is no point in dividing the root one of the nodes <b>1608</b> if there are only a few of them. Again, the visibility tree <b>1702</b> for the visibility <b>134</b> may be traversed in parallel.</p><p id="p-0234" num="0233">To ensure that updates are not lost, one of the system objects <b>126</b> can first insert itself in visibility tree <b>1702</b> describing the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> before traversing the segment tree <b>1702</b> describing the roles <b>122</b> for the visibility <b>134</b>. And the agents <b>116</b> always update the visibility tree <b>1702</b> with the roles <b>122</b> before traversing the segment tree <b>1702</b> having the points of interest <b>106</b>. This way it is possible that one of the agents <b>116</b> will be told to update one of the system objects <b>126</b> even after its read the most recent version, but there won't be a chance for it to pass through the segment tree <b>1702</b> having the points of interest <b>106</b> before an update and then update the visibility tree <b>1702</b> having the roles <b>122</b> after the agents <b>116</b> have been informed and miss the update.</p><p id="p-0235" num="0234">In an alternative example, suppose one of the system objects <b>126</b> is updated and the agents <b>116</b> need to be informed. Instead of trying to inform all of the agents <b>116</b> in the augmented reality system <b>100</b>, only the agents <b>116</b> having the visibility <b>134</b> for that one of the system objects <b>126</b> should be informed. The visibility <b>134</b> is one of the access rights <b>120</b>, so the system can restrict the notification only to the agents <b>116</b> with that one of the access rights <b>120</b> and to the agents <b>116</b> who are close enough to view the object.</p><p id="p-0236" num="0235">In the case where only a small number of the agents <b>116</b> have the visibility <b>134</b> of one of the system objects <b>126</b>, the system can quickly check the distance from the agents <b>116</b> to the system objects <b>126</b> and only notify the agents <b>116</b> that are close enough to view the system objects <b>126</b>. In the case where there are thousands or millions of the system objects <b>126</b>, then the system can use a tree structure that grows and shrinks as the agents <b>116</b> with the access rights <b>120</b> enter and leave the system. Using a tree structure can reduce the computational power required for doing the search for the access rights <b>120</b> for the visibility <b>134</b>.</p><p id="p-0237" num="0236">When there are few users, the agents <b>116</b> can be checked quickly. As the number increases, then the root of the tree can be broken down into children and then successively break the children up as they become more crowded, down to some level where breaking them up would be more expensive then checking them all.</p><p id="p-0238" num="0237">All the agents <b>116</b> can be at the leaves of the tree, but one of the system objects <b>126</b> looking for the agents <b>116</b> enters at the top of the tree, or the local equivalent, and searches the adjacent nodes in parallel to discover the agents <b>116</b> that are nearby.</p><p id="p-0239" num="0238">Yet another approach would be to use one tree <b>1702</b> for both purposes. The visibility tree <b>1702</b> can represent the roles <b>122</b> and the points of interest <b>106</b>. However this is more complex, as the two of the visibility tree <b>1702</b> are organized around different principles. It can also increase the number of updates to a single shared structure. The segments <b>1602</b> would have an associated set of the roles <b>122</b>. Now, the agents <b>116</b> only exist at the roots of the system, so the presence of the agents <b>116</b> could require splits to the visibility tree <b>1702</b>.</p><p id="p-0240" num="0239">Also, since when an update occurs, it's necessary to find all the users, it's necessary to propagate up the visibility tree <b>1702</b> having the roles <b>122</b> present in the each subtree, otherwise many of the segments <b>1602</b> could be looked at unnecessarily. This strongly increases the number of updates necessary when users move around.</p><p id="p-0241" num="0240">Having the visibility tree <b>1702</b> for the roles <b>122</b> implies a push architecture, where users need to be updated immediately when there are changes. This makes sense when there is real time interaction. A more &#x201c;weblike&#x201d; approach is a pull architecture, where each the agents <b>116</b> periodically requests updates from the system. The current state of each of the agents <b>116</b> is correct to within the period of the poll. This is adequate in many circumstances.</p><p id="p-0242" num="0241">When the agents <b>116</b> poll, the segment tree <b>1606</b> is re-traversed and the segments <b>1602</b> updated since the last poll are checked. The system can combine both architectures with most the roles <b>122</b> being &#x201c;pull&#x201d; and the high value ones of the roles <b>122</b> being &#x201c;push&#x201d;. For example, notifications to the agents <b>116</b> could be &#x201c;push&#x201d;.</p><p id="p-0243" num="0242">An alternative to the Bloom filter <b>1604</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref> approach would be using hash tables. However, this would require at least 64 bit identifiers, since 1T&#x3e;2<sup>32 </sup>as well as extra space for hashing, so storage requirements would be in the range of 200 K or so for each of the segments <b>1602</b>.</p><p id="p-0244" num="0243">The alternative to all this is to run several independent applications, each taking care of its own set of the system objects <b>126</b>. This ends up being far more resource intensive and arguably slower for the user. Rather than sending one message to an aggregated service, the agents <b>116</b> would need to send messages continuously for each running application, each responding with its own data.</p><p id="p-0245" num="0244">Upon entering a location, several messages would go out and some return. There would be no ability to reuse geometric data about a location to minimize the recognition overhead on the client. Different applications might use different recognition libraries, adding to client overhead. In addition, there is no easy way for the system objects <b>126</b> not in the small set of running apps to start communicating with the agents <b>116</b> except by using an expensive global communication subsystem with the related overhead for facilitating communicating between all of the applications and all of the agents <b>116</b>. This can provide connectivity and communication between all elements, but at an appreciable cost.</p><p id="p-0246" num="0245">Referring now to <figref idref="DRAWINGS">FIG. <b>18</b></figref>, therein is shown an example of the building coordinate system <b>302</b> for the frames of reference <b>102</b>. The building coordinate system <b>302</b> can be used to navigation within one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0247" num="0246">The building can be one of the frames of reference <b>102</b> represented by a graph of subspaces representing elements such as floors, rooms, doors, windows, and hallways, the building coordinate system <b>302</b> can uniquely identify each of the elements in the building.</p><p id="p-0248" num="0247">For example, the building coordinate system <b>302</b> can have a first floor with a first room, a second room, and a third room. The first room can have a first window, a second window, and a first door. The second room can have a first door. The third room can have a first window and a first door. The second floor can have a first room and a second room. The first room can have a first window, a first door, and a second door. The second room can have a first window and a first door.</p><p id="p-0249" num="0248">The building can be entered from some number of points in the global coordinate system or GPS space. For example, the first floor can have a first door that can be the entry point <b>308</b> of <figref idref="DRAWINGS">FIG. <b>3</b></figref> for the building.</p><p id="p-0250" num="0249">At any point in time one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is in one of the frames of reference <b>102</b> for the building. The underlying geometry of one of the frames of reference <b>102</b> for the building can understand the graph and has additional information. For example, the additional information can include GPS coordinate information, the vicinity of beacons or Wi-Fi access points, and visual information, such processed images, or SLAM point clouds. In addition, sensors in the real world can track the agents <b>116</b> movement through the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. These data together can establish where the agents <b>116</b> are in the frames of reference <b>102</b> of the building and determine the orientation of the agents <b>116</b>.</p><p id="p-0251" num="0250">Similarly, any of the points of interest <b>106</b> in the space can be placed in the frames of reference <b>102</b> by putting it in a room and then locking it in place either using coordinates, sensor data, or a combination including SLAM, markers, or other processed visual data. Any of the frames of reference <b>102</b> using the building coordinate system <b>302</b> can have a multi-level structure, so some of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be visible in multiple rooms, or on a given floor.</p><p id="p-0252" num="0251">Each of the frames of reference <b>102</b> can include a structure containing the full data for one of the frames of reference <b>102</b>. The structure can include the data for all of the points of interest <b>106</b> associated with the various nodes.</p><p id="p-0253" num="0252">In another illustrative example, once one of the agents <b>116</b> enters one of the frames of reference <b>102</b>, it can retrieve the frame geometry <b>104</b> information and receives other information to be able to move around within the current one of the frames of reference <b>102</b>. The type and amount of information depends on the particular implementation.</p><p id="p-0254" num="0253">The agents <b>116</b> can be sent a complete description of the geometry at once, or it can be sent data in pieces. At first it would get information for the entry one of the nodes <b>1608</b> and its immediate connectivity, and then be sent further information as it moves from one of the nodes <b>1608</b> of <figref idref="DRAWINGS">FIG. <b>16</b></figref> to another one of the nodes <b>1608</b> in the graph. This would include enough information to understand its position in the space, as well as the points of interest <b>106</b> that are immediately visible.</p><p id="p-0255" num="0254">As the agents <b>116</b> move around the current one of the frames of reference <b>102</b>, additional information would be passed to it. If there are updates to one of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, then all of the agents <b>116</b> that are affected can receive a notification. Depending how many of the agents <b>116</b> find themselves in the same part of the current one of the frames of reference <b>102</b>, the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can simply look for all of the agents <b>116</b> at the affected node, or use a one of the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to one of the nodes <b>1608</b> mapping and then send the notification to each of the agents <b>116</b>.</p><p id="p-0256" num="0255">As described, each one of the nodes <b>1608</b>, such as a representation of a room in a building, functions independently, so only the system objects <b>126</b> in the current room are visible at any time. Alternatively, another one of the frames of reference <b>102</b> can implement a more advanced geometry would have a more complete understanding of the physical geometry, so that more of the points of interest <b>106</b> can be visible if they were actually in the line of sight of the agents <b>116</b>, even if in another room and only visible through a window.</p><p id="p-0257" num="0256">Referring now to <figref idref="DRAWINGS">FIG. <b>19</b></figref>, therein is shown an exemplary diagram of the agents <b>116</b> cooperating with one another. The agents <b>116</b> can cooperate with one another to perform actions within the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0258" num="0257">The agents <b>116</b> can interact with one another because the agents <b>116</b> can also be one of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The augmented reality system <b>100</b> can coordinate behavior among groups of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that are moving, such as the agents <b>116</b>. For example, consider a rideshare application such as Uber&#x2122; or Lyft&#x2122;. Each of the taxis can be the points of interest <b>106</b> and can be associated with the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in one of the frames of reference <b>102</b> using the global coordinate system <b>202</b>. When someone wants a taxi, the one of the agents <b>116</b> can be assigned one of the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that lets it perceive the taxis and also becomes one of the items <b>110</b> that the taxis can see. The agents <b>116</b> are now aware of all the taxis in their vicinity and taxis are aware of all the agents <b>116</b> looking for rides.</p><p id="p-0259" num="0258">In another example, the agents <b>116</b> can negotiate a price for a transaction using the items <b>110</b>. One of the agents <b>116</b> for the passenger can update itself with the desired destination and an offered price. The points of interest <b>106</b> representing the taxis can each create another one of the items <b>110</b>, such as a ride bid, to propose their own price for the trip. The passenger can then accept the desired ride bid and lock in the transaction with the taxi with the best price. In a related example, the agents <b>116</b> of multiple passengers could make their taxi requests available to other passenger and negotiate shared rides.</p><p id="p-0260" num="0259">Referring now to <figref idref="DRAWINGS">FIG. <b>20</b></figref>, therein is shown an example of an agent data structure <b>2002</b>. The agent data structure <b>2002</b> of <figref idref="DRAWINGS">FIG. <b>20</b></figref> can represent the agents <b>116</b> in the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0261" num="0260">To represent the agents <b>116</b> within the augmented reality system <b>100</b>, the data structure representing the agents <b>116</b> can indicate a relationship with the points of interest <b>106</b> where the agents are near and the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> that may be associated with the agents <b>116</b>. Although the data structure is shown for a single one of the agents <b>116</b>, it is understood that multiples of the agents <b>116</b> can be represented using a larger data structure, such as an array, a list, a data store, or other similar data structure.</p><p id="p-0262" num="0261">The agents <b>116</b> can be associated with one or more of the points of interest <b>106</b>. Being associated with one of the points of interest <b>106</b> allows the agents <b>116</b> to interact with each of the points of interest <b>106</b> and the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with the points of interest <b>106</b>.</p><p id="p-0263" num="0262">The agents <b>116</b> can also be associated with one or more of the roles <b>122</b>. Being associated with one of the roles <b>122</b> allows the agents <b>116</b> to have the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the token sets <b>1202</b> associated with the roles <b>122</b>. The access rights <b>120</b> and the token sets <b>1202</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref> can control the way the agents <b>116</b> can interact with the system objects <b>126</b> associated with the points of interest <b>106</b>.</p><p id="p-0264" num="0263">The agents <b>116</b> can also be associated with one or more of the items <b>110</b>. Being associated with one of the items <b>110</b> allows the agents <b>116</b> to have the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the token sets <b>1202</b> associated with the items <b>110</b>. The access rights <b>120</b> and the token sets <b>1202</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref> can control the way the agents <b>116</b> can interact with the system objects <b>126</b> associated with the points of interest <b>106</b>. The items <b>110</b> can be associated with scripts that execute locally. Local behavior and activity allows the distribution of intelligence across the augmented reality system <b>100</b> to make more efficient use of the available computing resources.</p><p id="p-0265" num="0264">In an illustrative example, one of the points of interest <b>106</b> can be positioned in a location with one of the items <b>110</b> that can track the number of customers that pass that location. Because the items <b>110</b> can include script or other code objects that can execute locally, the items <b>110</b> can provide functionality such as incrementing a counter when another one of the system objects <b>126</b> come within a detection radius. The items <b>110</b> can function in a manner similar to one of the agents <b>116</b> in that is can be aware of the system objects <b>126</b> moving within the augmented reality system <b>100</b>. The scripts or other code object executing this functionality for the items <b>110</b> or the agents <b>116</b> can reside in the augmented reality system <b>100</b> or on external systems as necessary.</p><p id="p-0266" num="0265">In another illustrative example, the agents <b>116</b> can be associated with one of the roles <b>122</b> that controls the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the message <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with one of the points of interest <b>106</b>. When one of the agents <b>116</b> is in one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and near the particular one of the points of interest <b>106</b>, then the agents <b>116</b> can have access to the particular one of the message <b>114</b> where the access rights <b>120</b> grant the visibility <b>134</b> to the particular one of the roles <b>122</b>.</p><p id="p-0267" num="0266">Referring now to <figref idref="DRAWINGS">FIG. <b>21</b></figref>, therein is shown an example of a query data structure <b>2102</b>. The query data structure <b>2102</b> can implement the execution of the query <b>1408</b>.</p><p id="p-0268" num="0267">The augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can support querying to search the system objects <b>126</b> within the system. Querying is an integral part of interaction on the current Web and can also be an important part of the augmented world. Much more than the online world, querying in the augmented one is affected by where one queries and who is querying.</p><p id="p-0269" num="0268">For example, the augmented reality system <b>100</b> can execute the query <b>1408</b> using the query data structure <b>2102</b>. The query <b>1408</b> search operation that can return a list of the system objects <b>126</b> related to the query term <b>1410</b>. The query term <b>1410</b> can include words, text, objects, and images associated with the system objects <b>126</b> to make them searchable. The query result <b>1412</b> is the list of the system objects <b>126</b> that match the query term <b>1410</b>.</p><p id="p-0270" num="0269">The augmented reality system <b>100</b> can execute the query <b>1408</b> based on a particular set of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and based on the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, such as the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. For example, one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can perform the query <b>1408</b> using the query term <b>1410</b> having a bookstore frame of reference, the visibility set to all books in the fiction section of the store, and a text search string of &#x201c;Title: Sherlock Holmes and Author: Doyle&#x201d;. The augmented reality system <b>100</b> can return the set of the system objects <b>126</b> associated with books that match the search criteria.</p><p id="p-0271" num="0270">The returned set of the system objects <b>126</b> may also be associated with a location in one of the frames of reference <b>102</b> and some visibility rights based on the location of the agents <b>116</b>, but using one of the frames of reference <b>102</b> and visibility rights to boost corresponding results ensures that nearby, accessible ones of the system objects <b>126</b> will sort near the top of the returned items. Where visibility rights are associated with a found object, they must be filtered by the rights of the requester. In addition, location information can be used to filter out results by distance, or to present them to the agents <b>116</b> as part of the search results.</p><p id="p-0272" num="0271">Search points out another type of the frames of reference <b>102</b>, one which is only accessible within another, but which is not part of the outer one of the frames of reference <b>102</b>. For example, if one of the agents <b>116</b> is in a bookstore and queries on a book cover, then the one of the agents <b>116</b> in the current one of the frames of reference <b>102</b> can help narrow the search to book covers. One of the agents <b>116</b> for this search might return one of frames of reference <b>102</b> with bookstore employee comments that can only be perceived from inside the bookstore.</p><p id="p-0273" num="0272">Referring now to <figref idref="DRAWINGS">FIG. <b>22</b></figref>, therein is shown an example of an ownership data structure <b>2204</b>. The ownership data structure <b>2204</b> can describe ownership <b>2202</b> regarding an owner identifier <b>2206</b> for one or more of the system objects <b>126</b> within the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0274" num="0273">The augmented reality system <b>100</b> can specify the ownership <b>2202</b> of assets, such as the system objects <b>126</b> by associating the owner identifier <b>2206</b> to the system objects <b>126</b>. The owner identifier <b>2206</b> is a system level identifier to show which entity controls fundamental access to one of the system objects <b>126</b>. For example, the owner identifier <b>2206</b> can include the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, a user identification, an external system identifier, or a combination thereof.</p><p id="p-0275" num="0274">The ownership <b>2202</b> is necessary to control access and to support billing whoever pays for the operation of the augmented reality system <b>100</b>. Billing and ownership are intimately related to the allocation of the access rights <b>120</b>. For example, the ownership <b>2202</b> can be used to determine who can grant access to one of the system objects <b>126</b> in one of the token sets <b>1202</b> of <figref idref="DRAWINGS">FIG. <b>12</b></figref>. If another one of the agents <b>116</b> accesses one of the system objects <b>126</b>, then the augmented reality system <b>100</b> can record the usage and send a billing message to one of the agents <b>116</b>.</p><p id="p-0276" num="0275">The ownership <b>2202</b> can be implemented in a variety of ways. For example, the ownership can be associated with the access rights <b>120</b> and the roles <b>122</b>. The ownership <b>2202</b> can address the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> right, but also the access rights <b>120</b> such as creating a new one of the frames of reference <b>102</b> within an existing one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or adding the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> or the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> into the system. As with other aspects of access control, these can be supported by the system itself or offloaded to the owner.</p><p id="p-0277" num="0276">In an illustrative example, one of the agents <b>116</b> playing a game can be billed for access to one of the frames of reference <b>102</b> representing the building where the game is played. Granting access to the items <b>110</b> in the game can also be billed. This allows for billing when the user finds and takes &#x201c;the sword of power&#x201d; within the game.</p><p id="p-0278" num="0277">Referring now to <figref idref="DRAWINGS">FIG. <b>23</b></figref>, therein is shown an example of the agent actions. The agents <b>116</b> can operate on the frames of reference <b>102</b>, the points of interest <b>106</b>, the items <b>110</b>, and others of the system objects <b>126</b>.</p><p id="p-0279" num="0278">The agents <b>116</b> can perform a variety of actions in the system. One of the agents can announce its location with regards to one or more of the coordinate systems. One of the agents <b>116</b> can enter or leave one of the frames of reference <b>102</b>. One of the agents <b>116</b> can create one of the system objects <b>126</b>, such as one of the points of interest <b>106</b>, one of the frames of reference <b>102</b>, one of the items <b>110</b>, or a combination thereof.</p><p id="p-0280" num="0279">One of the agents <b>116</b> can execute the query <b>1408</b> having the query term <b>1410</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref>. One of the agents <b>116</b> can duplicate one or more of the system objects <b>126</b> in the system. This allows for the personalization of the system objects <b>126</b> in some location. For example, everyone sees the same initial version, but each person interacts with their own particular version with no necessary overlap. The duplicates may have persistence, or may not. If not, they may reside entirely locally and not interact with the server.</p><p id="p-0281" num="0280">One of the agents <b>116</b> can update the system objects <b>126</b>. This can include operations such as update, delete, edit, move, re-root, add to an overlapping coordinate system, or a combination thereof.</p><p id="p-0282" num="0281">One of the agents <b>116</b> can take temporary possession of one of the items <b>110</b>. While the augmented reality system <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can serve as an augmented reality server, it is not expected to continuous service real-time interaction with the system objects <b>126</b>. These operations should be mostly handled by the owner and the agents <b>116</b>. The agents <b>116</b> can take temporary ownership of some of the points of interest <b>106</b> and manage them within the system for a period of time. This can occur when the server stops conveying information about one of the points of interest <b>106</b>.</p><p id="p-0283" num="0282">This can also occur when the server encounters a problem taking possession of one of the points of interest <b>106</b>. The owner of the agents <b>116</b> or points of interest <b>106</b> can provides a URL or other information for where to get further information about the points of interest <b>106</b>, as well, as a volume in the appropriate coordinate systems where the points of interest <b>106</b> might be found.</p><p id="p-0284" num="0283">For example, in an augmented reality game, the points of interest <b>106</b> could be a magic talisman on a table. Once someone picks it up, they take possession. Anyone else who might perceive the talisman would need to communicate with the augmented reality game system for further information. The server would only convey that it could be in the area. When the agents <b>116</b> returns the object to its original place, then the server may regain possession and indicate a consistent location. Alternatively, the agents <b>116</b> might transfer the object to its local frame and put it in its inventory.</p><p id="p-0285" num="0284">One of the agents <b>116</b> can add or remove one of the roles <b>122</b>. This can require having the appropriate identity to create the roles <b>122</b> and attach them to one of the system objects <b>126</b>.</p><p id="p-0286" num="0285">In an illustrative example, the augmented reality system <b>100</b> can provide an underlying &#x201c;Location as a Service&#x201d; feature as part of normal operation. The display portion of the augmented reality system <b>100</b> is an example of one kind of application that needs location as a service. The location as a service can provide the location of the system objects <b>126</b> as needed.</p><p id="p-0287" num="0286">The agents <b>116</b> can interoperate with a service <b>2302</b> providing access to the system objects <b>126</b>. The service <b>2302</b> can be the underlying hardware systems used to implement a portion of the augmented reality system <b>100</b>. The service <b>2302</b> can act as a repository of the system objects <b>126</b> that is both global in scope and universal in application. The service <b>2302</b> can be application agnostic and interoperate with any of the internal and external systems that need to interact with the system objects <b>126</b>. For example, the system objects <b>126</b> in any application can be thought of as having a location that can be provided by the service <b>2302</b>. The system objects <b>126</b> can be indexed by location in one of the frames of reference <b>102</b> according to the frame geometry <b>104</b> and associated by a set of the access rights <b>120</b>. The system objects <b>126</b> can be stored in the same repository for multiple kinds of the frame geometry <b>104</b>.</p><p id="p-0288" num="0287">Searching the service <b>2302</b> can be optimized over the different types of the system objects <b>126</b>. The agents <b>116</b> can send requests to the service <b>2302</b> for objects. This may be on-demand or periodically. Further, the service <b>2302</b> can be configured to periodically send results for a particular request automatically. The requests can specify some set of locations in the frames of reference with parameters such as keywords and radii, etc. The requests can indicate the current physical location of one of the agents <b>116</b> in the frames of reference <b>102</b>, but not always. A person in New York City can request about the system objects <b>126</b> in Los Angeles to display results in a map.</p><p id="p-0289" num="0288">The services <b>2302</b> can return a list of the system objects <b>126</b> with some associated content. The list can only include data that one of the agents <b>116</b> has permission to receive. The content can include additional information providing some indication of how the agents <b>116</b> can use the content. For example, some of the content may include information on how to precisely locate one of the points of interest <b>106</b> and also provide other information to enhance the display when it is in the visual field of the viewing unit <b>118</b>. Other content can include a Waze&#xae; notice for cars to slow down as near a known speed trap. The content can include repair records for refrigerator, access to a web services API for the soda machine, the name of an application to activate, or other information to help use the content.</p><p id="p-0290" num="0289">The agents <b>116</b> are system objects <b>126</b>, so this can be a symmetric relationship. The system objects <b>126</b> can respond to the movements of the agents <b>116</b>.</p><p id="p-0291" num="0290">The service <b>2302</b> can support an open-ended set of use cases and provide information about the system objects <b>126</b> as necessary. The access control can enable the growth of the service <b>2302</b>, because not all of the system objects <b>126</b> can reside on a single instance of the service <b>2302</b>. Different instances of the service <b>2302</b> can interoperate and share the system objects <b>126</b>. Having explicit access control means that the system objects <b>126</b> can support the creation of new services that can be built on top as mashups by sharing objects.</p><p id="p-0292" num="0291">Referring now to <figref idref="DRAWINGS">FIG. <b>24</b></figref>, therein is shown an example of related systems <b>2402</b>. The augmented reality system <b>100</b> can interact with the related systems <b>2402</b> to provide information on the behavior and status of some of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0293" num="0292">The augmented reality system <b>100</b> can be implemented in a centralized or distributed configuration. For example, the augmented reality system <b>100</b> can be implemented in a cloud computing environment, a centralized data center, an array of portable computing devices, in a virtual computing environment, or a combination thereof.</p><p id="p-0294" num="0293">The augmented reality system <b>100</b> can be configured with a variety of internal and external systems. The augmented reality system <b>100</b> can include storage systems <b>2412</b>, processor systems <b>2414</b>, communication systems <b>2416</b>, scene capture systems <b>2418</b>, location systems <b>2420</b>, and sensor systems <b>2422</b>. The systems can be coupled to one another with logical data connections including network links, optical links, wireless links, direct connections, bus connections, remote connections, or a combination thereof.</p><p id="p-0295" num="0294">The storage systems <b>2412</b> can store data and executable code for the augmented reality system <b>100</b>. The storage systems <b>2412</b> can include memory units for active computing and large scale persistent storage. The storage systems <b>2412</b> can be distributed or localized. The storage systems <b>2412</b> can support the migration of data among units of the storage systems <b>2412</b> for load balancing, speed of local access, geographical priority, or a combination thereof. For example, the storage systems <b>2412</b> can include cloud storage, hard disk storage, solid state memory units, optical memory units, redundant storage units, tape storage units, or a combination thereof.</p><p id="p-0296" num="0295">The processor systems <b>2414</b> can implement the augmented reality system <b>100</b> by executing code (not shown) on one or more processing units. The processor system <b>2414</b> can be distributed or local. The processor systems <b>2414</b> can include cloud computing servers, dedicated servers, multiprocessors, arrays of computing elements, smart phones, smart glasses, tablet computers, notebook computers, desktop computers, other viewing devices, or a combination thereof.</p><p id="p-0297" num="0296">The communication systems <b>2416</b> can link the systems of the augmented reality system <b>100</b> together internally and externally. The communication systems <b>2416</b> can include wired network links, wireless network links, bus links, direct connections, optical communication links, matrix communication links, or a combination thereof.</p><p id="p-0298" num="0297">The scene capture systems <b>2418</b> are devices for receiving information about the physical world and creating data-based representations for use in the augmented reality system <b>100</b>. The scene capture systems <b>2418</b> can be local or distributed. The scene capture systems <b>2418</b> can include image sensors, magnetic sensors, optical sensors, infrared sensors, ultraviolet sensors, proximity sensors, contact sensors, or a combination thereof. The information from the scene capture systems <b>2418</b> can be digitized and stored in the storage systems <b>2412</b> for distribution via the communication systems <b>2416</b> to the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for display on the viewing units <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The information from the scene capture systems <b>2418</b> can be used to determine and store the physical location <b>148</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the system objects <b>126</b>. The scene capture systems <b>2418</b> can include the integration of scene capture information from internal and external scene capture mechanisms.</p><p id="p-0299" num="0298">The location systems <b>2420</b> are devices for establishing and enhancing location information associated with the scene capture system <b>2418</b> and the system objects <b>126</b>. The location systems <b>2420</b> can include radiolocation systems, direct location feeds, scene matching systems, pattern matching systems, three-dimensional geometry systems, correlation systems, physical modeling systems, matching systems, or a combination thereof. For example, the location systems <b>2420</b> can include systems for resolving fine grained locations of the system objects <b>126</b> based on the surface features of the systems objects <b>126</b> as detected within a scene. The location systems <b>2420</b> can include systems for creating 3D models of scenes to extract edge and corner locations, global positioning systems, beacon-based location systems, registration systems, or a combination thereof.</p><p id="p-0300" num="0299">The sensor systems <b>2422</b> are devices for measuring physical information for the augmented reality system <b>100</b>. The sensor systems <b>2422</b> can be distributed or local. The sensor systems <b>2422</b> can include pressure sensors, temperature sensors, magnetic field sensors, chemical sensors, touch sensors, audio sensors, olfactory sensors, taste sensors, radiation sensors, mechanical sensors, optical sensors, event counters, or a combination thereof. The information from the sensor system <b>2422</b> can be conveyed to the agents <b>116</b> as enhanced display information to be presented to the user to enhance the experience of the augmented reality system <b>100</b>. For example, the chemical sensor information from a warehouse fumes detector can be conveyed to one of the agents <b>116</b> and automatically displayed as a message <b>114</b> warning of a chlorine smell associated with one of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0301" num="0300">In an illustrative example, the augmented reality system <b>100</b> can performed continuous monitoring of the sensor system <b>2422</b> and have one of the agents <b>116</b> react appropriately based on detection of certain conditions. For example, the sensor system <b>2422</b> can be an event counter detecting the number of people who enter an event facility room. If the number of people exceeds a predefined threshold, such as a maximum occupancy, then the agent <b>116</b> can respond and present a warning or by directly preventing further entry by controlling an entry turnstile. The agents <b>116</b> can interact directly with other external control systems and control mechanisms to perform actions in the real world, such as controlling the turnstiles, sounding an alarm, activating a valve, changing the lighting, or other actions.</p><p id="p-0302" num="0301">The augmented reality system <b>100</b> can be coupled to the related systems <b>2402</b> to access information on the related systems <b>2402</b>. For example, the related systems <b>2402</b> can include an external data system <b>2404</b>, a gaming system <b>2406</b>, an ecommerce system <b>2408</b>, a control system <b>2409</b>, or a similar external system.</p><p id="p-0303" num="0302">For example, the augmented reality system <b>100</b> can be coupled to the ecommerce system <b>2408</b> when one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> enters a store in a mall. The ecommerce system <b>2408</b> can provide real-time pricing and inventory information to help support a sale.</p><p id="p-0304" num="0303">In another example, the augmented reality system <b>100</b> can be coupled to the gaming system <b>2406</b> when one of the agents <b>116</b> accesses a game. The gaming system <b>2406</b> can provide detailed access to the information about the game including artwork, gaming rules, game interaction, communication with gaming characters, or similar gaming information.</p><p id="p-0305" num="0304">In yet another example, the augmented reality system <b>100</b> can be coupled to the external data system <b>2404</b> to receive enhanced display information <b>2410</b>. The display information <b>2410</b> can be supplemental data to render the system objects <b>126</b> with better clarity, resolution, and detail. The external data system <b>2404</b> can include an imaging unit to provide more detailed graphics for the system objects <b>126</b> in the augmented reality system <b>100</b>. The external data system <b>2404</b> could provide detailed images of the artwork in a museum during an augmented reality tour.</p><p id="p-0306" num="0305">In a further example, the augmented reality system <b>100</b> can be coupled to the control system <b>2409</b> to perform actions on external devices, such as a control mechanism. For example, the control system <b>2409</b> can be a turnstile control system for a concert venue, a building air conditioning system, a chemical warning system in a warehouse, a water distribution system for a farm, or a similar external system for controlling devices in the physical world. The control mechanism can be a system controller, a value controller, internal settings, input devices, or a combination thereof. The augmented reality system <b>100</b> can receive and send information to the control system <b>2409</b>.</p><p id="p-0307" num="0306">The augmented reality system <b>100</b> can be distinguished from the gaming context because of the open nature of reality. Virtual reality systems and games take place inside closed worlds where everything is known to software executing somewhere. In virtual reality systems all the geometry is known and maintained by the system. All the system objects <b>126</b>, and their properties and physics are known by the system.</p><p id="p-0308" num="0307">Virtual reality system elements such as surfaces are clearly defined and the system is aware when something is attached to the surface. The system knows what the relevant physics are, and nothing changes without the explicit knowledge of the virtual reality system. Visibility in the virtual reality system is easily calculated because of the perfect knowledge of the system.</p><p id="p-0309" num="0308">However, in the augmented reality system <b>100</b>, knowledge of the frame geometry <b>104</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> can be limited to the sensors and processing power of the devices. It is not a priori evident what is a wall, how far it is from the agents <b>116</b>, and, therefore, whether one of the points of interest <b>106</b> is necessarily visible.</p><p id="p-0310" num="0309">It has been discovered that the performance of the augmented reality system <b>100</b> improved by being coupled with the sensors <b>138</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> in the real world to define the system objects <b>126</b> in one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Motion in the augmented reality system <b>100</b> is directly linked to motion and the frame geometry <b>104</b> of the real world.</p><p id="p-0311" num="0310">Another issue is determining which of the points of interest <b>106</b> are close enough to one the agents <b>116</b> for the agents <b>116</b> to be aware of the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. Processing limitations show that none of the agents <b>116</b> can handle being given a list of all of the points of interest <b>106</b> in the augmented reality system <b>100</b>. Because the augmented reality system <b>100</b> can have billions of the system objects <b>126</b>, even sending the necessary data to locate them a burdensome task. The performance problem is also demonstrated when there are millions of the points of interest <b>106</b> that could individually change over time. Therefore the agents <b>116</b> needs to be sent a limited set of the points of interest <b>106</b> that it might encounter before it actually sees them to prevent an unacceptable delay or lag.</p><p id="p-0312" num="0311">The augmented reality system <b>100</b> is different from gaming systems. In an illustrative gaming example, the points of interest <b>106</b> that are local are fixed to the immediate geometry of the game, which is known in advance and maintained by the game system. In addition, all the users in the game are in known locations and move to known locations within the system. Further, in a virtual reality game, motion in the real world does not correlate with motion in the game.</p><p id="p-0313" num="0312">In the augmented reality system <b>100</b> example, the agents <b>116</b> may not have the frame geometry <b>104</b> of the whole world. A particular space or region only becomes interesting when one of the points of interest <b>106</b> is created. The information about the new area can be received on demand and does not have to be pre-defined or pre-cached.</p><p id="p-0314" num="0313">Existing augmented reality toolkits are starting to look beyond simple marker-based points of interest <b>106</b> to consider at least local geometry and are implementing 3D techniques such as SLAM. However these efforts remain localized to a single application. The system described here can work with such toolkits by supporting a global repository for the system objects <b>126</b> orthogonal to the browser implementation.</p><p id="p-0315" num="0314">Referring now to <figref idref="DRAWINGS">FIG. <b>25</b></figref>, therein is shown an example of a process flow of operation of the augmented reality system <b>2500</b>. The augmented reality system <b>2500</b> can operate by displaying images of the real world overlaid with the representations of the system objects <b>126</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0316" num="0315">The augmented reality system <b>100</b> can be configured in a variety of ways. For example, the augmented reality system <b>100</b> can include a position module <b>2502</b>, a detection module <b>2504</b>, a visibility module <b>2506</b>, an action module <b>2508</b>, and a display module <b>2510</b>.</p><p id="p-0317" num="0316">The position module <b>2502</b> can determine the real world position and orientation of the viewing unit <b>118</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with one of the agents <b>116</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The position module <b>2502</b> can calculate the current location <b>144</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the current orientation <b>146</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the viewing unit <b>118</b> using the position sensor <b>140</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> and the orientation sensor <b>142</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0318" num="0317">One of the agents <b>116</b> can be implemented as software (not shown) executing on the viewing unit <b>118</b> or associated with the viewing unit <b>118</b>. The current location <b>144</b> and the current orientation <b>146</b> can be calculated at one time on a continuous basis and updated as needed by the agents <b>116</b>.</p><p id="p-0319" num="0318">The detection module <b>2504</b> can determine which of the system objects <b>126</b> are near the agents <b>116</b>. The detection module <b>2504</b> can correlate the current location <b>144</b> and the current orientation <b>146</b> in relation to the system objects <b>126</b>. For example, the detection module <b>2504</b> can determine the current one of the frames of reference <b>102</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> for one of the agents <b>116</b>. Then the detection module <b>2504</b> can determine the presence of the system objects <b>126</b>, such as the points of interest <b>106</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the items <b>110</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the message <b>114</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, or a combination thereof, that are within the detection threshold <b>150</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> of the current location <b>144</b> and have the persistence <b>136</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to exist at the current time <b>154</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0320" num="0319">In an illustrative example, the detection module <b>2504</b> can determine the location and orientation of the agents <b>116</b> and then operate on the agents <b>116</b>. First, the detection module <b>2504</b> can periodically send the location and orientation information to the server implementing the augmented reality system <b>100</b>, which can respond with a possibly empty list of interesting objects and updates, which can be locally stored in an information repository. The server can check the persistence <b>136</b> and the visibility <b>134</b> of the system objects <b>126</b> to determine a set of the system objects <b>126</b> that the agents <b>116</b> are likely to encounter. For example, it might send all the system objects <b>126</b> in a room. Caching the system objects <b>126</b> that are nearby can improve performance, especially when the user is moving.</p><p id="p-0321" num="0320">Second, the detection module <b>2504</b> can check the local repository to determine what to display at a particular moment, based on the location, orientation, the visibility <b>134</b>, and the persistence <b>136</b> associated with the system objects <b>126</b>.</p><p id="p-0322" num="0321">The visibility module <b>2506</b> can determine which of the system object <b>126</b> have the visibility <b>134</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to be perceived by one of the agents <b>116</b> based on the current location <b>144</b> and the current orientation <b>146</b> of the system objects <b>126</b> for viewing by the viewing unit <b>118</b>. The visibility <b>134</b> can be based on the roles <b>122</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the token sets <b>1202</b>, and the access rights <b>120</b> associated with the agents <b>116</b> and the system objects <b>126</b>. For example, one of the agents <b>116</b> can read the message <b>114</b> if they are associated with the access rights <b>120</b> to allow them to read the message <b>114</b>.</p><p id="p-0323" num="0322">The action module <b>2508</b> can perform operations on the system objects <b>126</b> in the augmented reality system <b>100</b>. The action module <b>2508</b> can determine if the agents <b>116</b> can perform an operation or modify the object state <b>152</b> of the system objects <b>126</b> based on the roles <b>122</b>, the token sets <b>1202</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the access rights <b>120</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> associated with the agents <b>116</b> and the system objects <b>126</b>. For example, one of the agents <b>116</b> can create the message <b>114</b> if they have the access rights <b>120</b> to allow creation of the message <b>114</b>. In another example, one of the agents <b>116</b> can retrieve one of the system objects <b>126</b> representing the &#x201c;sword of power&#x201d; if they have the access rights <b>120</b> for playing the game.</p><p id="p-0324" num="0323">The display module <b>2510</b> can display the information from the augmented reality system <b>100</b> as an overlay to the imagery of the real world. The display module <b>2510</b> can control the viewing unit <b>118</b> to display the system objects <b>126</b> in the proper location and orientation relative to the real world based on the current location <b>144</b>, the current orientation <b>146</b>, the object location <b>128</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>, and the object orientation <b>130</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The display module <b>2510</b> can update the viewing unit <b>118</b> on a one time or continuous basis. After completion of the display module <b>2510</b>, the control flow can pass back to the position module <b>2502</b>.</p><p id="p-0325" num="0324">It has been discovered that displaying the system objects <b>126</b> in the viewing unit <b>118</b> based on the visibility <b>134</b> and the persistence <b>136</b> of the system objects <b>126</b> increases the functionality of the augmented reality system <b>100</b>. Displaying the information in the viewing unit <b>118</b> enables the user to interact with information and manipulate the system objects <b>126</b>.</p><p id="p-0326" num="0325">Referring now to <figref idref="DRAWINGS">FIG. <b>26</b></figref>, therein is shown a flow chart of a method <b>2600</b> of operation of an augmented reality system in a further embodiment of the present invention. The method <b>2600</b> includes: detecting a current location in a block <b>2602</b>; detecting a current orientation in a block <b>2604</b>; detecting a system object having an object location within a detection threshold of the current location in a block <b>2606</b>; retrieving a content associated with the system object in a block <b>2608</b>; calculating a persistence of the system object based on the current time and a persistence extent in a block <b>2610</b>; calculating a visibility of the system object based on an access right and the object location in a block <b>2612</b>; and presenting the content of the system object to a control mechanism based on the persistence and the visibility in a block <b>2614</b>.</p><p id="p-0327" num="0326">Thus, it has been discovered that the augmented reality system of the present invention furnishes important and heretofore unknown and unavailable solutions, capabilities, and functional aspects for an augmented reality system. The resulting method, process, apparatus, device, product, and/or system is straightforward, cost-effective, uncomplicated, highly versatile and effective, can be surprisingly and unobviously implemented by adapting known technologies, and are thus readily suited for efficiently and economically manufacturing the augmented reality systems fully compatible with conventional manufacturing methods or processes and technologies.</p><p id="p-0328" num="0327">Another important aspect of the present invention is that it valuably supports and services the historical trend of reducing costs, simplifying manufacturing, and increasing performance. These and other valuable aspects of the present invention consequently further the state of the technology to at least the next level.</p><p id="p-0329" num="0328">While the invention has been described in conjunction with a specific best mode, it is to be understood that many alternatives, modifications, and variations will be apparent to those skilled in the art in light of the aforegoing description. Accordingly, it is intended to embrace all such alternatives, modifications, and variations that fall within the scope of the included claims. All matters hithertofore set forth herein or shown in the accompanying drawings are to be interpreted in an illustrative and non-limiting sense.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of operation for an augmented reality system comprising:<claim-text>entering a second frame of reference at an entry point at a first current location in a first frame of reference in a field of view of an agent;</claim-text><claim-text>detecting a system object in the second frame of reference, the system object having an access right permission matching another access right permission of the agent;</claim-text><claim-text>sending a message to an external system coupled to the system object;</claim-text><claim-text>receiving a content from the external system in response to the message;</claim-text><claim-text>calculating a persistence of the system object based on a current time and a persistence extent;</claim-text><claim-text>extracting a visibility of the system object from a visibility tree, the visibility tree having the visibility of the system object pre-defined and linked to anode associated to a second current location in the second frame of reference containing the system object; and</claim-text><claim-text>displaying the content over a real-world image on the agent based on the persistence of the system object and the visibility.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein extracting the visibility of the system object includes updating the visibility of the system object in the visibility tree based on the content received from the external system.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein detecting the system object includes:<claim-text>returning a query result based on a query term in the second frame of reference, the query result having at least one system object related to the query term; and</claim-text><claim-text>selecting the system object in the query result, the system object having an access right permission matching another access right permission of the agent.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein sending the message includes sending the message to a control mechanism of a control system for performing an action in the control system.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein sending the message includes sending the message to an external data system having an imaging unit to provide images with improved clarity, resolution, and detail and based on the first current location.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein sending the message includes sending the message to an ecommerce system and receiving real-time pricing and inventory information based on the system object.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref> wherein sending the message includes sending the message to a gaming system and receiving the content having a game communication with one or more gaming characters.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An augmented reality system comprising:<claim-text>a position sensor for calculating a second frame of reference at an entry point at a first current location in a first frame of reference in a field of view of an agent;</claim-text><claim-text>a communication unit, coupled to the position sensor, for detecting a system object in the second frame of reference, the system object having an access right permission matching another access right permission of the agent, for sending a message to an external system coupled to the system object, and for receiving a content from the external system in response to the message;</claim-text><claim-text>a control mechanism, coupled to the communication unit, for calculating a persistence of the system object based on a current time and a persistence extent, for extracting the visibility of the system object from a visibility tree linked to a node associated to a second current location in the second frame of reference containing the system object; and</claim-text><claim-text>a display unit, coupled to the control mechanism, for displaying the content over a real-world image on the agent based on the persistence of the system object and the visibility.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the communication unit is for updating the visibility of the system object in the visibility tree based on the content received from the external system.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the communication unit is for returning a query result based on a query term in the second frame of reference, the query result having at least one system object related to the query term and selecting the system object in the query result, the system object having an access right permission matching another access right permission of the agent.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the communication unit is for sending the message to a control mechanism of a control system for performing an action in the control system.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the communication unit is for sending the message to an external data system having an imaging unit to provide images with improved clarity, resolution, and detail and based on the first current location.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The system as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the communication unit is for sending the message to an ecommerce system and receiving real-time pricing and inventory information based on the system object.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The system as claimed in <claim-ref idref="CLM-00008">claim 8</claim-ref> wherein the communication unit is for sending the message to a gaming system and receiving the content having a game communication with one or more gaming characters.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. One or more non-transitory computer-readable media storing instructions that, when executed by one or more computing devices, cause:<claim-text>entering a second frame of reference at an entry point at a first current location in a first frame of reference in a field of view of an agent;</claim-text><claim-text>detecting a system object in the second frame of reference, the system object having an access right permission matching another access right permission of the agent;</claim-text><claim-text>sending a message to an external system coupled to the system object;</claim-text><claim-text>receiving a content from the external system in response to the message;</claim-text><claim-text>calculating a persistence of the system object based on a current time and a persistence extent;</claim-text><claim-text>extracting a visibility of the system object from a visibility tree, the visibility tree having the visibility of the system object pre-defined and linked to a node linked to a second current location in the second frame of reference containing the system object; and</claim-text><claim-text>displaying the content over a real-world image on the agent based on the persistence of the system object and the visibility.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The non-transitory computer-readable media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instruction when executed by the one or more computing devices, further cause receiving the content includes updating the visibility of the system object in the visibility tree based on the content received from the external system.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The non-transitory computer-readable media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions when executed by the one or more computing devices, further cause returning a query result based on a query term in the second frame of reference, the query result having at least one system object related to the query term and selecting the system object in the query result, the system object having an access right permission matching another access right permission of the agent.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The non-transitory computer-readable media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions when executed by the one or more computing devices, further cause sending the message to a control mechanism of a control system for performing an action in the control system.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The non-transitory computer-readable media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions when executed by the one or more computing devices, further cause sending the message to an external data system having an imaging unit to provide images with improved clarity, resolution, and detail and based on the first current location.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The non-transitory computer-readable media of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the instructions when executed by the one or more computing devices, further cause sending the message to an ecommerce system and receiving real-time pricing and inventory information based on the system object.</claim-text></claim></claims></us-patent-application>