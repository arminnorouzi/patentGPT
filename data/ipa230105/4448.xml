<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004449A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004449</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941252</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>52</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>50</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>245</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>524</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>4881</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>505</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20190101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>16</main-group><subgroup>245</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Optimizing Distributed and Parallelized Batch Data Processing</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17218587</doc-number><date>20210331</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11474881</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941252</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Bank of America Corporation</orgname><address><city>Charlotte</city><state>NC</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Matos</last-name><first-name>Marcus</first-name><address><city>Richardson</city><state>TX</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Aspects of the disclosure relate to providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing. A computing platform may initialize at least two processing workers. Subsequently, the computing platform may cause a first processing worker to perform a first query on a work queue database and initiate parallel processing of a first set of work items. Thereafter, the computing platform may cause the second processing worker to perform a second query on the work queue database and initiate parallel processing of a second set of work items. In some instances, performing the second query on the work queue database comprises reading at least one work item that was read and locked by the first processing worker.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="161.71mm" wi="141.39mm" file="US20230004449A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="184.32mm" wi="143.43mm" file="US20230004449A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="177.80mm" wi="135.64mm" file="US20230004449A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="239.35mm" wi="170.69mm" orientation="landscape" file="US20230004449A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="239.35mm" wi="170.69mm" orientation="landscape" file="US20230004449A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="243.59mm" wi="173.31mm" orientation="landscape" file="US20230004449A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="213.70mm" wi="118.03mm" file="US20230004449A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="171.37mm" wi="132.59mm" file="US20230004449A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="143.43mm" wi="132.84mm" file="US20230004449A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of and claims priority to co-pending U.S. application Ser. No. 17/218,587, filed Mar. 31, 2021, and entitled &#x201c;Optimizing Distributed and Parallelized Batch Data Processing,&#x201d; which is incorporated herein by reference in its entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Aspects of the disclosure relate to data processing systems, database systems, and parallel processing. In particular, one or more aspects of the disclosure relate to providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing.</p><p id="p-0004" num="0003">Enterprise organizations may utilize various computing infrastructure to maintain confidential information and/or other sensitive data that is created and/or used for various purposes. Ensuring that this data is secure and processed efficiently may be critically important both to protecting the integrity and confidentiality of the underlying information and associated resources and to ensuring the stability and utility of enterprise computer systems. In many instances, however, it may be difficult to provide and maintain efficient and effective processing of sets of work items in enterprise computing environments, particularly when relatively large or variably-sized sets of work items are queued for processed.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0005" num="0004">Aspects of the disclosure provide effective, efficient, scalable, and convenient technical solutions that address and overcome the technical problems associated with providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing.</p><p id="p-0006" num="0005">In accordance with one or more embodiments, a computing platform having at least one processor, a communication interface, and memory may initialize at least two processing workers, wherein initializing the at least two processing workers comprises initializing a first processing worker and a second processing worker. Subsequently, the computing platform may cause the first processing worker to perform a first query on a work queue database, wherein performing the first query on the work queue database comprises reading and locking a first set of work items identified in the work queue database. In addition, the computing platform may cause the first processing worker to initiate parallel processing of the first set of work items, wherein initiating parallel processing of the first set of work items comprises: processing a first subset of the first set of work items in parallel; and upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database. Thereafter, the computing platform may cause the second processing worker to perform a second query on the work queue database, wherein performing the second query on the work queue database comprises reading and locking a second set of work items identified in the work queue database. In addition, the computing platform may cause the second processing worker to initiate parallel processing of the second set of work items, wherein initiating parallel processing of the second set of work items comprises: processing a first subset of the second set of work items in parallel; and upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database.</p><p id="p-0007" num="0006">In some embodiments, performing the first query on the work queue database comprises causing a database engine associated with the work queue database to lock the first set of work items upon read. In some embodiments, performing the first query on the work queue database comprises selecting the first set of work items based on one or more priority parameters.</p><p id="p-0008" num="0007">In some embodiments, performing the first query on the work queue database comprises writing, in the work queue database, a first timestamp indicating when the first set of work items were read from the work queue database. In some embodiments, the first timestamp is tracked by a database engine associated with the work queue database to provide expired items to one or more other processing workers.</p><p id="p-0009" num="0008">In some embodiments, performing the second query on the work queue database comprises reading at least one work item that was read and locked by the first processing worker.</p><p id="p-0010" num="0009">In some embodiments, the computing platform may monitor a pending workload in the work queue database. Based on monitoring the pending workload in the work queue database, the computing platform may dynamically scale an availability level of processing workers.</p><p id="p-0011" num="0010">In some embodiments, monitoring the pending workload in the work queue database comprises requesting one or more size values from the work queue database. In some embodiments, dynamically scaling the availability level of processing workers comprises initializing at least one additional processing worker. In some embodiments, dynamically scaling the availability level of processing workers comprises destroying at least one processing worker.</p><p id="p-0012" num="0011">In some embodiments, the computing platform may cause a monitoring process to create one or more historical records. Subsequently, the computing platform may dynamically optimize one or more processing parameters based on the one or more historical records.</p><p id="p-0013" num="0012">In accordance with one or more additional or alternative embodiments, a computing platform having at least one processor, a communication interface, and memory may initialize a monitoring process configured to monitor a pending workload in a work queue database. Subsequently, the computing platform may cause the monitoring process to query the work queue database and create one or more historical records indicative of a workload processing status associated with one or more processing workers, wherein the one or more processing workers are configured to process work items identified in the work queue database based on one or more processing parameters. Thereafter, the computing platform may identify one or more new parameter values for the one or more processing parameters associated with the one or more processing workers based on the one or more historical records. Then, the computing platform may configure the one or more processing workers based on the one or more new parameter values identified for the one or more processing parameters associated with the one or more processing workers.</p><p id="p-0014" num="0013">In some embodiments, each processing worker of the one or more processing workers is configured to read a set of work items identified in the work queue database and process the set of work items in parallel. In some embodiments, each processing worker of the one or more processing workers is configured to write a timestamp into the work queue database upon reading the set of work items, the timestamp indicating when the set of work items were read from the work queue database. In some embodiments, each processing worker of the one or more processing workers is configured to lock the set of work items upon reading the set of work items.</p><p id="p-0015" num="0014">In some embodiments, identifying the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers may include: inputting, into at least one machine learning model, data selected from the one or more historical records; and receiving, from the machine learning model, the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers.</p><p id="p-0016" num="0015">In some embodiments, configuring the one or more processing workers comprises adjusting a number of work items requested by each processing worker of the one or more processing workers. In some embodiments, configuring the one or more processing workers comprises adjusting one or more priority settings used by each processing worker of the one or more processing workers. In some embodiments, configuring the one or more processing workers comprises adjusting one or more expiration time values of a database engine associated with the work queue database.</p><p id="p-0017" num="0016">In some embodiments, the computing platform may initialize at least two processing workers, wherein initializing the at least two processing workers comprises initializing a first processing worker and a second processing worker. Subsequently, the computing platform may cause the first processing worker to perform a first query on the work queue database, wherein performing the first query on the work queue database comprises reading and locking a first set of work items identified in the work queue database. In addition, the computing platform may initiate parallel processing of the first set of work items, wherein initiating parallel processing of the first set of work items comprises: processing a first subset of the first set of work items in parallel; and upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database. Thereafter, the computing platform may cause the second processing worker to perform a second query on the work queue database, wherein performing the second query on the work queue database comprises reading and locking a second set of work items identified in the work queue database. In addition, the computing platform may initiate parallel processing of the second set of work items, wherein initiating parallel processing of the second set of work items comprises: processing a first subset of the second set of work items in parallel; and upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database.</p><p id="p-0018" num="0017">These features, along with many others, are discussed in greater detail below.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0019" num="0018">The present disclosure is illustrated by way of example and not limited in the accompanying figures in which like reference numerals indicate similar elements and in which:</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> depict an illustrative computing environment for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments;</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref> depict an illustrative event sequence for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments;</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIGS. <b>3</b> and <b>4</b></figref> depict example graphical user interfaces for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments;</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an illustrative method for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments; and</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts another illustrative method for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0025" num="0024">In the following description of various illustrative embodiments, reference is made to the accompanying drawings, which form a part hereof, and in which is shown, by way of illustration, various embodiments in which aspects of the disclosure may be practiced. It is to be understood that other embodiments may be utilized, and structural and functional modifications may be made, without departing from the scope of the present disclosure.</p><p id="p-0026" num="0025">It is noted that various connections between elements are discussed in the following description. It is noted that these connections are general and, unless specified otherwise, may be direct or indirect, wired or wireless, and that the specification is not intended to be limiting in this respect.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIGS. <b>1</b>A and <b>1</b>B</figref> depict an illustrative computing environment for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments. Referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, computing environment <b>100</b> may include one or more computer systems. For example, computing environment <b>100</b> may include a distributed data processing computing platform <b>110</b>, enterprise computing infrastructure <b>120</b>, a work queue database <b>130</b>, an administrator computing device <b>140</b>, an enterprise user computing device <b>150</b>, and a remote user computing device <b>160</b>.</p><p id="p-0028" num="0027">As illustrated in greater detail below, distributed data processing computing platform <b>110</b> may include one or more computing devices configured to perform one or more of the functions described herein. For example, distributed data processing computing platform <b>110</b> may include one or more computers (e.g., laptop computers, desktop computers, servers, server blades, or the like).</p><p id="p-0029" num="0028">Enterprise computing infrastructure <b>120</b> may include one or more computing devices and/or other computer components (e.g., processors, memories, communication interfaces). In addition, and as illustrated in greater detail below, enterprise computing infrastructure <b>120</b> may be configured to provide various enterprise and/or back-office computing functions for an organization, such as a financial institution. For example, enterprise computing infrastructure <b>120</b> may include various servers and/or databases that host, execute, and/or store various enterprise resources (e.g., enterprise applications, enterprise databases, enterprise information). For instance, enterprise computing infrastructure <b>120</b> may include various servers and/or databases that store and/or otherwise maintain account information, such as financial account information including account balances, transaction history, account owner information, and/or other information. In addition, enterprise computing infrastructure <b>120</b> may host, execute, and/or store one or more enterprise applications that process and/or otherwise execute transactions on specific accounts based on commands and/or other information received from other computer systems included in computing environment <b>100</b>. Additionally or alternatively, enterprise computing infrastructure <b>120</b> may load data from distributed data processing computing platform <b>110</b>, manipulate and/or otherwise process such data, and return modified data and/or other data to distributed data processing computing platform <b>110</b> and/or to other computer systems included in computing environment <b>100</b>.</p><p id="p-0030" num="0029">Work queue database <b>130</b> may include one or more computing devices and/or other computer components (e.g., processors, memories, communication interfaces). In addition, and as illustrated in greater detail below, work queue database <b>130</b> may be configured to store information associated with various work items and/or other information. In some instances, such information may be organized into various data tables and/or sub-databases maintained by work queue database <b>130</b>. Additionally, work queue database <b>130</b> may store and/or execute one or more database engines which may manage the information stored by work queue database <b>130</b>, enable access to such information, and/or perform other functions.</p><p id="p-0031" num="0030">Administrator computing device <b>140</b> may be a personal computing device (e.g., desktop computer, laptop computer) or mobile computing device (e.g., smartphone, tablet). In addition, administrator computing device <b>140</b> may be linked to and/or used by an administrative user (who may, e.g., be a network administrator of an organization operating distributed data processing computing platform <b>110</b>).</p><p id="p-0032" num="0031">Enterprise user computing device <b>150</b> may be a personal computing device (e.g., desktop computer, laptop computer) or mobile computing device (e.g., smartphone, tablet). In addition, enterprise user computing device <b>150</b> may be linked to and/or used by an enterprise user (who may, e.g., be an employee or other affiliate of an organization operating distributed data processing computing platform <b>110</b>).</p><p id="p-0033" num="0032">Remote user computing device <b>160</b> may be a personal computing device (e.g., desktop computer, laptop computer) or mobile computing device (e.g., smartphone, tablet). In addition, remote user computing device <b>160</b> may be linked to and/or used by a remote user (who may, e.g., be an employee or other affiliate of an organization operating distributed data processing computing platform <b>110</b> and who may be located outside of an enterprise network associated with the organization).</p><p id="p-0034" num="0033">Computing environment <b>100</b> also may include one or more networks, which may interconnect one or more of distributed data processing computing platform <b>110</b>, enterprise computing infrastructure <b>120</b>, work queue database <b>130</b>, administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, and remote user computing device <b>160</b>. For example, computing environment <b>100</b> may include a private network <b>170</b> (which may, e.g., interconnect distributed data processing computing platform <b>110</b>, enterprise computing infrastructure <b>120</b>, work queue database <b>130</b>, administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, and/or one or more other systems which may be associated with an organization, such as a financial institution) and public network <b>180</b> (which may, e.g., interconnect remote user computing device <b>160</b> with private network <b>170</b> and/or one or more other systems, public networks, sub-networks, and/or the like).</p><p id="p-0035" num="0034">In one or more arrangements, work queue database <b>130</b>, administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>, and/or the other systems included in computing environment <b>100</b> may be any type of computing device capable of receiving a user interface, receiving input via the user interface, and communicating the received input to one or more other computing devices. For example, work queue database <b>130</b>, administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>, and/or the other systems included in computing environment <b>100</b> may, in some instances, be and/or include server computers, desktop computers, laptop computers, tablet computers, smart phones, or the like that may include one or more processors, memories, communication interfaces, storage devices, and/or other components. As noted above, and as illustrated in greater detail below, any and/or all of distributed data processing computing platform <b>110</b>, enterprise computing infrastructure <b>120</b>, work queue database <b>130</b>, administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, and remote user computing device <b>160</b> may, in some instances, be special-purpose computing devices configured to perform specific functions.</p><p id="p-0036" num="0035">Referring to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, distributed data processing computing platform <b>110</b> may include one or more processor(s) <b>111</b>, memory(s) <b>112</b>, and communication interface(s) <b>113</b>. A data bus may interconnect processor <b>111</b>, memory <b>112</b>, and communication interface <b>113</b>. Communication interface <b>113</b> may be a network interface configured to support communication between distributed data processing computing platform <b>110</b> and one or more networks (e.g., network <b>170</b>, network <b>180</b>, or the like). Memory <b>112</b> may include one or more program modules and/or processing engines having instructions that when executed by processor <b>111</b> cause distributed data processing computing platform <b>110</b> to perform one or more functions described herein and/or one or more databases that may store and/or otherwise maintain information which may be used by such program modules, processing engines, and/or processor <b>111</b>. In some instances, the one or more program modules, processing engines, and/or databases may be stored by and/or maintained in different memory units of distributed data processing computing platform <b>110</b> and/or by different computing devices that may form and/or otherwise make up distributed data processing computing platform <b>110</b>. For example, memory <b>112</b> may have, store, and/or include a distributed data processing module <b>112</b><i>a</i>, a distributed data processing database <b>112</b><i>b</i>, a machine learning engine <b>112</b><i>c</i>, and a plurality of workers <b>112</b><i>d</i>-<b>1</b>, <b>112</b><i>d</i>-<b>2</b>, <b>112</b><i>d</i>-<i>n. </i></p><p id="p-0037" num="0036">Distributed data processing module <b>112</b><i>a </i>may have instructions that direct and/or cause distributed data processing computing platform <b>110</b> to optimize distributed and parallelized batch data processing, as discussed in greater detail below. Distributed data processing database <b>112</b><i>b </i>may store information used by distributed data processing module <b>112</b><i>a </i>and/or distributed data processing computing platform <b>110</b> in optimizing distributed and parallelized batch data processing. Machine learning engine <b>112</b><i>c </i>may perform and/or provide one or more machine learning and/or artificial intelligence functions and/or services, as illustrated in greater detail below. Each worker of the plurality of workers <b>112</b><i>d</i>-<b>1</b>, <b>112</b><i>d</i>-<b>2</b>, <b>112</b><i>d</i>-<i>n </i>may retrieve and process work items and/or perform other functions, as illustrated in greater detail below.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIGS. <b>2</b>A-<b>2</b>C</figref> depict an illustrative event sequence for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments. Referring to <figref idref="DRAWINGS">FIG. <b>2</b>A</figref>, at step <b>201</b>, work queue database <b>130</b> may load an initial work queue. For instance, work queue database <b>130</b> may receive initial groups of work items from various other computing devices and/or systems (e.g., enterprise computing infrastructure <b>120</b>, administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, and/or remote user computing device <b>160</b>), tag and/or otherwise process the work items, and/or load the work items into one or more data tables that may form the initial work queue. Each work item may, for instance, be a record in the database that instructs distributed data processing computing platform <b>110</b> or another computer system to do a specific piece or sequence of processing work. In some instances, a work item may include one or more commands to be executed (e.g., by distributed data processing computing platform <b>110</b> and/or one or more workers) and/or embedded data to be processed. In this way, work queue database <b>130</b> may maintain a current work queue that can be filled from other systems and/or devices and may operate as a task list that includes functions and/or processing work to be completed and/or assigned to different workers.</p><p id="p-0039" num="0038">At step <b>202</b>, distributed data processing computing platform <b>110</b> may receive initial configuration settings from administrator computing device <b>140</b>. Such initial configuration settings may, for instance, define various parameters that can be used by different workers that may operate on distributed data processing computing platform <b>110</b>. For instance, one or more of these initial configuration settings may define a configurable number of work items that each worker should request from work queue database <b>130</b> in a given instance.</p><p id="p-0040" num="0039">At step <b>203</b>, distributed data processing computing platform <b>110</b> may initialize one or more workers. For example, at step <b>203</b>, distributed data processing computing platform <b>110</b> may initialize at least two processing workers, and initializing the at least two processing workers may include initializing a first processing worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) and a second processing worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>).</p><p id="p-0041" num="0040">At step <b>204</b>, distributed data processing computing platform <b>110</b> may cause worker <b>112</b><i>d</i>-<b>1</b> to query work queue database <b>130</b> and process work items. For example, at step <b>204</b>, distributed data processing computing platform <b>110</b> may cause the first processing worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) to perform a first query on a work queue database (e.g., work queue database <b>130</b>), where performing the first query on the work queue database includes reading and locking a first set of work items identified in the work queue database. For instance, worker <b>112</b><i>d</i>-<b>1</b> may read and lock a first set of work items identified in the work queue database (e.g., work queue database <b>130</b>) based on instructions and/or commands received from distributed data processing computing platform <b>110</b>. In addition, distributed data processing computing platform <b>110</b> may cause the first processing worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) to initiate parallel processing of the first set of work items, where initiating parallel processing of the first set of work items includes: processing a first subset of the first set of work items in parallel; and upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database (e.g., work queue database <b>130</b>). For instance, worker <b>112</b><i>d</i>-<b>1</b> may process a first subset of the first set of work items in parallel, and upon completion of processing each work item of the first subset of the first set of work items, mark the corresponding work item as completed in the work queue database (e.g., work queue database <b>130</b>) based on instructions and/or commands received from distributed data processing computing platform <b>110</b>.</p><p id="p-0042" num="0041">In marking the corresponding work item as completed in the work queue database (e.g., work queue database <b>130</b>), worker <b>112</b><i>d</i>-<b>1</b> may read data records that form or otherwise make up the work item, mark such data records as reserved, and/or write a timestamp to the database (or, e.g., cause such a timestamp to be recorded in the database or with a database engine associated with the database). Additionally or alternatively, in retrieving work items from the work queue database (e.g., work queue database <b>130</b>), worker <b>112</b><i>d</i>-<b>1</b> may take a configurable number of work items from a processing queue maintained in and/or associated with the work queue database (e.g., work queue database <b>130</b>). For example, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may be configured to read the queue associated with work queue database <b>130</b>, grab and/or reserve twenty pieces of work (e.g., work items), and begin processing such work items. For instance, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may be configured to process four items simultaneously in parallel (e.g., if the worker and/or the platform have sufficient bandwidth, at the time, to support parallel processing). After distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> splits the retrieved work items and begins processing them in parallel, and at the conclusion of processing a given work item, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may mark the corresponding data record as completed in the database (e.g., work queue database <b>130</b>). For example, in processing the set of work items retrieved from the database, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may write data to work queue database <b>130</b> and/or enterprise computing infrastructure <b>120</b>. When distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> completes processing that set of work items, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may go back to the queue and request additional work items from work queue database <b>130</b>. If there are no additional work items to process at that time, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may periodically ping the queue and wait for more work items to be added to the queue.</p><p id="p-0043" num="0042">In some embodiments, performing the first query on the work queue database may include causing a database engine associated with the work queue database to lock the first set of work items upon read. For example, in performing the first query on the work queue database (e.g., work queue database <b>130</b>), distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may cause a database engine associated with the work queue database (e.g., work queue database <b>130</b>) to lock the first set of work items upon read. By locking the first set of work items upon read, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may ensure that other workers (e.g., worker <b>112</b><i>d</i>-<b>2</b>) do not grab the same work items as the first worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>).</p><p id="p-0044" num="0043">In some embodiments, performing the first query on the work queue database may include selecting the first set of work items based on one or more priority parameters. For example, in performing the first query on the work queue database (e.g., work queue database <b>130</b>), distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may select the first set of work items based on one or more priority parameters. Such priority parameters may, for instance, specify that certain types of work items should be selected and/or processed before other types of work items. For instance, the priority parameters may specify that &#x2018;critical&#x2019; items should be processed before &#x2018;high priority&#x2019; items, which should be processed before &#x2018;medium priority&#x2019; items, which should be processed before &#x2018;low priority&#x2019; items.</p><p id="p-0045" num="0044">In some embodiments, performing the first query on the work queue database may include writing, in the work queue database, a first timestamp indicating when the first set of work items were read from the work queue database. For example, in performing the first query on the work queue database (e.g., work queue database <b>130</b>), distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> may write, in the work queue database (e.g., work queue database <b>130</b>), a first timestamp indicating when the first set of work items were read from the work queue database (e.g., <b>130</b>). This timestamp may, for instance, enable tracking of processing times for various work items, which in turn may enable various metrics to be computed and/or captured as well as for expired work items to be passed to alternative workers for processing.</p><p id="p-0046" num="0045">In some embodiments, the first timestamp may be tracked by a database engine associated with the work queue database to provide expired items to one or more other processing workers. For example, the first timestamp written by distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>1</b> in the work queue database (e.g., work queue database <b>130</b>) may be tracked by a database engine associated with the work queue database (e.g., work queue database <b>130</b>) to provide expired items to one or more other processing workers. For instance, the database engine may determine that a given item is expired if a predetermined amount of time (e.g., five minutes) has elapsed since the timestamp was written (and, e.g., the work item was read and locked by a given worker) and the item has not been marked completed. In these instances, the database engine may operate on the assumption that if the item has not been marked completed by the worker that read and locked the item within the predetermined amount of time (e.g., five minutes), that the worker has failed and that the item will not be processed. Thus, in these instances, the database engine may unlock the item so that the item becomes available again for processing by one or more other workers (e.g., worker <b>112</b><i>d</i>-<b>2</b>). In particular, once the item is unlocked and available again, another processing worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) may read and lock the item (and subsequently processed the item) even though the item was previously read and locked by the first processing worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>).</p><p id="p-0047" num="0046">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>B</figref>, at step <b>205</b>, distributed data processing computing platform <b>110</b> may cause worker <b>112</b><i>d</i>-<b>2</b> to query work queue database <b>130</b> and process work items. For example, at step <b>205</b>, distributed data processing computing platform <b>110</b> may cause the second processing worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) to perform a second query on the work queue database (e.g., work queue database <b>130</b>), where performing the second query on the work queue database includes reading and locking a second set of work items identified in the work queue database. For instance, worker <b>112</b><i>d</i>-<b>2</b> may read and lock a second set of work items identified in the work queue database (e.g., work queue database <b>130</b>) based on instructions and/or commands received from distributed data processing computing platform <b>110</b>. In addition, distributed data processing computing platform <b>110</b> may cause the second processing worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) to initiate parallel processing of the second set of work items, where initiating parallel processing of the second set of work items includes: processing a first subset of the second set of work items in parallel; and upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database (e.g., work queue database <b>130</b>). For instance, worker <b>112</b><i>d</i>-<b>2</b> may process a first subset of the second set of work items in parallel, and upon completion of processing each work item of the first subset of the second set of work items, mark the corresponding work item as completed in the work queue database (e.g., work queue database <b>130</b>) based on instructions and/or commands received from distributed data processing computing platform <b>110</b>.</p><p id="p-0048" num="0047">In processing the work items and marking work items complete, worker <b>112</b><i>d</i>-<b>2</b> may execute functions and/or perform steps similar to those described above with respect to worker <b>112</b><i>d</i>-<b>1</b>. For instance, worker <b>112</b><i>d</i>-<b>2</b> may read data records that form or otherwise make up the work item, mark such data records as reserved, and/or write a timestamp to the database (or, e.g., cause such a timestamp to be recorded in the database or with a database engine associated with the database). Additionally or alternatively, in retrieving work items from the work queue database (e.g., work queue database <b>130</b>), worker <b>112</b><i>d</i>-<b>2</b> may take a configurable number of work items from the processing queue maintained in and/or associated with the work queue database (e.g., work queue database <b>130</b>). For example, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> may be configured to read the queue associated with work queue database <b>130</b>, grab and/or reserve twenty pieces of work (e.g., work items), and begin processing such work items. Worker <b>112</b><i>d</i>-<b>2</b> may retrieve such work items by executing a command to &#x2018;select&#x2019; the top N database records (e.g., the top 20 database records) &#x2018;where&#x2019; such records are not locked. Because the database engine associated with work queue database <b>130</b> processes requests sequentially, this retrieval query will ensure that the second worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) does not get any of the non-expired records taken by the first worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) since each worker may command the database engine associated with work queue database <b>130</b> to lock records upon read. As a result, the second worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) might not even see the records that the first worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) is processing, because the first worker's records were locked and the database engine essentially makes those records invisible to the second worker.</p><p id="p-0049" num="0048">Like the first worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>), the second worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) may be configured to process four items simultaneously in parallel (e.g., if the worker and/or the platform have sufficient bandwidth, at the time, to support parallel processing). After distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> splits the retrieved work items and begins processing them in parallel, and at the conclusion of processing a given work item, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> may mark the corresponding data record as completed in the database (e.g., work queue database <b>130</b>). For example, in processing the set of work items retrieved from the database, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> may write data to work queue database <b>130</b> and/or enterprise computing infrastructure <b>120</b>. When distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> completes processing that set of work items, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> may go back to the queue and request additional work items from work queue database <b>130</b>. If there are no additional work items to process at that time, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> may periodically ping the queue and wait for more work items to be added to the queue.</p><p id="p-0050" num="0049">In some embodiments, performing the second query on the work queue database may include reading at least one work item that was read and locked by the first processing worker. For example, in performing the second query on the work queue database (e.g., work queue database <b>130</b>) at step <b>205</b>, distributed data processing computing platform <b>110</b> and/or worker <b>112</b><i>d</i>-<b>2</b> may read at least one work item that was read and locked by the first processing worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>). This may, for instance, occur because the timestamp(s) of the at least one work item has expired and the database engine associated with work queue database <b>130</b> is once again returning the at least one work item for processing. As introduced in examples discussed above, if work items are initially read and locked but not processed within a predetermined amount of time after being read and locked, these work items may then become available for processing by another worker. The &#x2018;locked&#x2019; status of these work items is maintained by and expires with the database engine associated with work queue database <b>130</b>, and so if a lock on some work items has expired when the second worker (e.g., worker <b>112</b><i>d</i>-<b>2</b>) queries the database, the database engine may allow those expired and now-unlocked work items to be read and locked by the second worker. In these instances, it may be assumed that the first worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) failed to process those work items is not going to be able to process them (e.g., because the worker failed or an error occurred from which the worker cannot recover). Additionally or alternatively, the first worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>) may be configured such that if it comes back online and starts looking for work again, the first worker will go back to work queue database <b>130</b> and request new work items for processing. The risk of duplicative efforts thus may be greatly reduced, if not eliminated, since the second worker might only end up with work items initially taken by the first worker in instances where the first worker has most likely failed.</p><p id="p-0051" num="0050">At step <b>206</b>, distributed data processing computing platform <b>110</b> may monitor the workload of the various processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>) and/or the work queue database (e.g., work queue database <b>130</b>). For example, at step <b>206</b>, distributed data processing computing platform <b>110</b> may monitor a pending workload in the work queue database (e.g., work queue database <b>130</b>). This may, for instance, include querying work queue database <b>130</b>, interrogating the database engine associated with work queue database <b>130</b>, and/or interrogating one or more hardware host systems associated with work queue database <b>130</b>.</p><p id="p-0052" num="0051">In some embodiments, monitoring the pending workload in the work queue database may include requesting one or more size values from the work queue database. For example, in monitoring the pending workload in the work queue database (e.g., work queue database <b>130</b>), distributed data processing computing platform <b>110</b> may request one or more size values from the work queue database (e.g., work queue database <b>130</b>). For instance, distributed data processing computing platform <b>110</b> may query and/or retrieve size values indicating the size(s) of one or more tables, columns, rows, or the like associated with records that are waiting to be processed, without locking such records.</p><p id="p-0053" num="0052">At step <b>207</b>, distributed data processing computing platform <b>110</b> may dynamically scale one or more worker processors. For example, at step <b>207</b>, based on monitoring the pending workload in the work queue database (e.g., work queue database <b>130</b>), distributed data processing computing platform <b>110</b> may dynamically scale an availability level of processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>). In some embodiments, dynamically scaling the availability level of processing workers may include initializing at least one additional processing worker. For example, in dynamically scaling the availability level of processing workers, distributed data processing computing platform <b>110</b> may initialize at least one additional processing worker. This may, for instance, include commanding and/or controlling a hypervisor to boot up a new virtual machine or other container that may host such a worker. In some embodiments, dynamically scaling the availability level of processing workers may include destroying at least one processing worker. For example, in dynamically scaling the availability level of processing workers, distributed data processing computing platform <b>110</b> may destroy at least one processing worker (e.g., by killing and/or otherwise deactivating one or more of worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>).</p><p id="p-0054" num="0053">At step <b>208</b>, distributed data processing computing platform <b>110</b> may generate one or more reports. Such reports may, for instance, include statistical information indicating how many worker items were processed in a given period of time by distributed data processing computing platform <b>110</b> and/or by individual workers, how many errors were encountered, how many items remain in work queue database <b>130</b>, and/or other information. Distributed data processing computing platform <b>110</b> may send and/or otherwise provide such reports to one or more other systems and/or devices (e.g., administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>), which may cause these other systems and/or devices to display such reports and/or interact with the information included in them.</p><p id="p-0055" num="0054">Referring to <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, at step <b>209</b>, distributed data processing computing platform <b>110</b> may send the reports to one or more other systems and/or devices (e.g., administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>). In some instances, by sending the reports to one or more other systems and/or devices (e.g., administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>), distributed data processing computing platform <b>110</b> may cause such systems and/or devices (e.g., administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>) to generate, display, and/or otherwise present one or more graphical user interfaces that include information associated with work item processing. For instance, distributed data processing computing platform <b>110</b> may cause such systems and/or devices (e.g., administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>) to generate, display, and/or otherwise present a graphical user interface similar to graphical user interface <b>300</b>, which is illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref>. As seen in <figref idref="DRAWINGS">FIG. <b>3</b></figref>, graphical user interface <b>300</b> may include text and/or other information indicating a high workload in the work queue database (e.g., &#x201c;Current processing times are below optimal levels. <b>1</b> additional worker processor is being initialized to handle the current workload.&#x201d;). Additionally or alternatively, distributed data processing computing platform <b>110</b> may cause such systems and/or devices (e.g., administrator computing device <b>140</b>, enterprise user computing device <b>150</b>, remote user computing device <b>160</b>) to generate, display, and/or otherwise present a graphical user interface similar to graphical user interface <b>400</b>, which is illustrated in <figref idref="DRAWINGS">FIG. <b>4</b></figref>. As seen in <figref idref="DRAWINGS">FIG. <b>4</b></figref>, graphical user interface <b>400</b> may include text and/or other information indicating a processor failure (e.g., &#x201c;Processing Worker A has failed in completing processing of Work Item Set X. Processing Worker B is picking up the dropped work items for completion.&#x201d;).</p><p id="p-0056" num="0055">At step <b>210</b>, distributed data processing computing platform <b>110</b> may initialize a monitoring process. For example, at step <b>210</b>, distributed data processing computing platform <b>110</b> may initialize a monitoring process configured to monitor a pending workload in a work queue database (e.g., work queue database <b>130</b>).</p><p id="p-0057" num="0056">At step <b>211</b>, distributed data processing computing platform <b>110</b> may cause the monitoring process to create records associated with the processing of work items in work queue database <b>130</b>. For example, at step <b>211</b>, distributed data processing computing platform <b>110</b> may cause a monitoring process to create one or more historical records. Such historical records may for instance, include information denoting current and/or prior data processing status of various work items. For example, distributed data processing computing platform <b>110</b> may cause the monitoring process to query the work queue database (e.g., work queue database <b>130</b>) and create one or more historical records indicative of a workload processing status associated with one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>), where the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>) are configured to process work items identified in the work queue database (e.g., work queue database <b>130</b>) based on one or more processing parameters.</p><p id="p-0058" num="0057">At step <b>212</b>, distributed data processing computing platform <b>110</b> may dynamically optimize itself and/or one or more other systems. For example, at step <b>212</b>, distributed data processing computing platform <b>110</b> may dynamically optimize one or more processing parameters based on the one or more historical records. For instance, distributed data processing computing platform <b>110</b> may adjust and/or otherwise optimize one or more processing parameters used by distributed data processing computing platform <b>110</b> and/or its associated workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) based on the one or more historical records created at step <b>211</b>. In some instances, in dynamically optimizing the one or more processing parameters based on the one or more historical records, distributed data processing computing platform <b>110</b> may tune one or more processing parameters based on one or more machine learning models and/or algorithms. For example, distributed data processing computing platform <b>110</b> may use learning engine <b>112</b><i>c </i>to tune the configurable number of work items retrieved by each processing worker (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) based on the one or more historical records.</p><p id="p-0059" num="0058">In some embodiments, after initializing the monitoring process and causing the monitoring process to query the work queue database and create one or more historical records and/or in dynamically optimizing the one or more processing parameters, distributed data processing computing platform <b>110</b> may identify one or more new parameter values for the one or more processing parameters associated with the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) based on the one or more historical records. For instance, distributed data processing computing platform <b>110</b> may identify such new parameter values (such as, e.g., the configurable number of work items, priority parameters, and/or other configuration parameters similar to those discussed in examples above) using learning engine <b>112</b><i>c </i>to process the one or more historical records.</p><p id="p-0060" num="0059">Subsequently, distributed data processing computing platform <b>110</b> may configure the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) based on the one or more new parameter values identified for the one or more processing parameters associated with the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>). For example, in configuring the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>), distributed data processing computing platform <b>110</b> may launch new instances of processing workers based on the new parameter values, modify settings and/or otherwise reconfigure existing processing workers based on the new parameter values, and/or otherwise change operational features based on the new parameter values.</p><p id="p-0061" num="0060">In some embodiments, each processing worker of the one or more processing workers may be configured to read a set of work items identified in the work queue database and process the set of work items in parallel. For example, each processing worker of the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) may be configured to read a set of work items identified in the work queue database (e.g., work queue database <b>130</b>) and process the set of work items in parallel, as illustrated in the examples discussed above.</p><p id="p-0062" num="0061">In some embodiments, each processing worker of the one or more processing workers may be configured to write a timestamp into the work queue database upon reading the set of work items, and the timestamp may indicate when the set of work items were read from the work queue database. For example, each processing worker of the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) may be configured to write a timestamp into the work queue database (e.g., work queue database <b>130</b>) upon reading the set of work items, and the timestamp may indicate when the set of work items were read from the work queue database (e.g., work queue database <b>130</b>), as illustrated in the examples discussed above.</p><p id="p-0063" num="0062">In some embodiments, each processing worker of the one or more processing workers may be configured to lock the set of work items upon reading the set of work items. For example, each processing worker of the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) may be configured to lock the set of work items upon reading the set of work items, as illustrated in the examples discussed above.</p><p id="p-0064" num="0063">In some embodiments, identifying the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers may include: inputting, into at least one machine learning model, data selected from the one or more historical records; and receiving, from the machine learning model, the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers. For example, in identifying the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>), distributed data processing computing platform <b>110</b> may input, into at least one machine learning model (which may, e.g., be stored by and/or executed on machine learning engine <b>112</b><i>c</i>), data selected from the one or more historical records. Subsequently, distributed data processing computing platform <b>110</b> may receive, from the machine learning model, the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>). For instance, in receiving the one or more new parameter values for the one or more processing parameters associated with the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>) from the machine learning model, distributed data processing computing platform <b>110</b> may receive new values for the configurable number of work items reserved by each worker when requesting new work items from work queue database <b>130</b>, the expiration time used by the database engine associated with work queue database <b>130</b> (which may, e.g., control how long the database engine may wait to release locked&#x2014;but incomplete&#x2014;work items to another worker), priority settings, and/or other settings.</p><p id="p-0065" num="0064">In some embodiments, configuring the one or more processing workers may include adjusting a number of work items requested by each processing worker of the one or more processing workers. For example, in configuring the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>), distributed data processing computing platform <b>110</b> may adjust a number of work items requested by each processing worker of the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>). This may, for instance, modify the number of work items reserved by each worker when requesting new work items from work queue database <b>130</b>.</p><p id="p-0066" num="0065">In some embodiments, configuring the one or more processing workers may include adjusting one or more priority settings used by each processing worker of the one or more processing workers. For example, in configuring the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>), distributed data processing computing platform <b>110</b> may adjust one or more priority settings used by each processing worker of the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>). This may, for instance, modify settings that define higher priority work items for relatively earlier processing and lower priority items for relatively later processing.</p><p id="p-0067" num="0066">In some embodiments, configuring the one or more processing workers may include adjusting one or more expiration time values of a database engine associated with the work queue database. For in configuring the one or more processing workers (e.g., worker <b>112</b><i>d</i>-<b>1</b>, worker <b>112</b><i>d</i>-<b>2</b>, worker <b>112</b><i>d</i>-<i>n</i>), distributed data processing computing platform <b>110</b> may adjust one or more expiration time values of a database engine associated with the work queue database (e.g., work queue database <b>130</b>). For instance, distributed data processing computing platform <b>110</b> may modify expiration time value(s) of the database engine that responds to queries from the processing workers. This may, for instance, modify settings that control how long the database engine of work queue database <b>130</b> waits to release locked, incomplete work items to another worker for processing.</p><p id="p-0068" num="0067"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts an illustrative method for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments. Referring to <figref idref="DRAWINGS">FIG. <b>5</b></figref>, at step <b>505</b>, a computing platform comprising at least one hardware processor, a communication interface, and memory may initialize at least two processing workers, and initializing the at least two processing workers may include initializing a first processing worker and a second processing worker. At step <b>510</b>, the computing platform may cause the first processing worker to: perform a first query on a work queue database, where performing the first query on the work queue database includes reading and locking a first set of work items identified in the work queue database; and initiate parallel processing of the first set of work items, where initiating parallel processing of the first set of work items includes: processing a first subset of the first set of work items in parallel; and upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database.</p><p id="p-0069" num="0068">At step <b>515</b>, the computing platform may cause the second processing worker to: perform a second query on the work queue database, where performing the second query on the work queue database includes reading and locking a second set of work items identified in the work queue database; and initiate parallel processing of the second set of work items, where initiating parallel processing of the second set of work items includes: processing a first subset of the second set of work items in parallel; and upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database. At step <b>520</b>, the computing platform may monitor a pending workload in the work queue database. At step <b>525</b>, based on monitoring the pending workload in the work queue database, the computing platform may dynamically scale an availability level of processing workers.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts another illustrative method for providing and maintaining efficient and effective processing of sets of work items in enterprise computing environments by optimizing distributed and parallelized batch data processing in accordance with one or more example embodiments. Referring to <figref idref="DRAWINGS">FIG. <b>6</b></figref>, at step <b>605</b>, a computing platform comprising at least one hardware processor, a communication interface, and memory may initialize a monitoring process configured to monitor a pending workload in a work queue database. At step <b>610</b>, the computing platform may cause the monitoring process to query the work queue database and create one or more historical records indicative of a workload processing status associated with one or more processing workers, where the one or more processing workers may be configured to process work items identified in the work queue database based on one or more processing parameters. At step <b>615</b>, the computing platform may identify one or more new parameter values for the one or more processing parameters associated with the one or more processing workers based on the one or more historical records. At step <b>620</b>, the computing platform may configure the one or more processing workers based on the one or more new parameter values identified for the one or more processing parameters associated with the one or more processing workers.</p><p id="p-0071" num="0070">One or more aspects of the disclosure may be embodied in computer-usable data or computer-executable instructions, such as in one or more program modules, executed by one or more computers or other devices to perform the operations described herein. Generally, program modules include routines, programs, objects, components, data structures, and the like that perform particular tasks or implement particular abstract data types when executed by one or more processors in a computer or other data processing device. The computer-executable instructions may be stored as computer-readable instructions on a computer-readable medium such as a hard disk, optical disk, removable storage media, solid-state memory, RAM, and the like. The functionality of the program modules may be combined or distributed as desired in various embodiments. In addition, the functionality may be embodied in whole or in part in firmware or hardware equivalents, such as integrated circuits, application-specific integrated circuits (ASICs), field programmable gate arrays (FPGA), and the like. Particular data structures may be used to more effectively implement one or more aspects of the disclosure, and such data structures are contemplated to be within the scope of computer executable instructions and computer-usable data described herein.</p><p id="p-0072" num="0071">Various aspects described herein may be embodied as a method, an apparatus, or as one or more computer-readable media storing computer-executable instructions. Accordingly, those aspects may take the form of an entirely hardware embodiment, an entirely software embodiment, an entirely firmware embodiment, or an embodiment combining software, hardware, and firmware aspects in any combination. In addition, various signals representing data or events as described herein may be transferred between a source and a destination in the form of light or electromagnetic waves traveling through signal-conducting media such as metal wires, optical fibers, or wireless transmission media (e.g., air or space). In general, the one or more computer-readable media may be and/or include one or more non-transitory computer-readable media.</p><p id="p-0073" num="0072">As described herein, the various methods and acts may be operative across one or more computing servers and one or more networks. The functionality may be distributed in any manner, or may be located in a single computing device (e.g., a server, a client computer, and the like). For example, in alternative embodiments, one or more of the computing platforms discussed above may be combined into a single computing platform, and the various functions of each computing platform may be performed by the single computing platform. In such arrangements, any and/or all of the above-discussed communications between computing platforms may correspond to data being accessed, moved, modified, updated, and/or otherwise used by the single computing platform. Additionally or alternatively, one or more of the computing platforms discussed above may be implemented in one or more virtual machines that are provided by one or more physical computing devices. In such arrangements, the various functions of each computing platform may be performed by the one or more virtual machines, and any and/or all of the above-discussed communications between computing platforms may correspond to data being accessed, moved, modified, updated, and/or otherwise used by the one or more virtual machines.</p><p id="p-0074" num="0073">Aspects of the disclosure have been described in terms of illustrative embodiments thereof. Numerous other embodiments, modifications, and variations within the scope and spirit of the appended claims will occur to persons of ordinary skill in the art from a review of this disclosure. For example, one or more of the steps depicted in the illustrative figures may be performed in other than the recited order, and one or more depicted steps may be optional in accordance with aspects of the disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A computing platform, comprising:<claim-text>at least one hardware processor;</claim-text><claim-text>a communication interface communicatively coupled to the at least one hardware processor; and</claim-text><claim-text>memory storing computer-readable instructions that, when executed by the at least one hardware processor, cause the computing platform to:<claim-text>initialize at least two processing workers, wherein initializing the at least two processing workers comprises initializing a first processing worker and a second processing worker;</claim-text><claim-text>cause the first processing worker to:<claim-text>perform a first query on a work queue database; and</claim-text><claim-text>initiate parallel processing of a first set of work items identified in the work queue database, wherein initiating parallel processing of the first set of work items comprises:<claim-text>processing a first subset of the first set of work items in parallel; and</claim-text><claim-text>upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database; and</claim-text></claim-text></claim-text><claim-text>cause the second processing worker to:<claim-text>perform a second query on the work queue database; and</claim-text><claim-text>initiate parallel processing of a second set of work items identified in the work queue database, wherein initiating parallel processing of the second set of work items comprises:<claim-text>processing a first subset of the second set of work items in parallel; and</claim-text><claim-text>upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database.</claim-text></claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The computing platform of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the first query on the work queue database comprises causing a database engine associated with the work queue database to lock the first set of work items upon read.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The computing platform of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the first query on the work queue database comprises selecting the first set of work items based on one or more priority parameters.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The computing platform of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the first query on the work queue database comprises writing, in the work queue database, a first timestamp indicating when the first set of work items were read from the work queue database.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The computing platform of <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the first timestamp is tracked by a database engine associated with the work queue database to provide expired items to one or more other processing workers.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The computing platform of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein performing the second query on the work queue database comprises reading at least one work item that was read and locked by the first processing worker.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The computing platform of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the memory stores additional computer-readable instructions that, when executed by the at least one hardware processor, cause the computing platform to:<claim-text>monitor a pending workload in the work queue database; and</claim-text><claim-text>based on monitoring the pending workload in the work queue database, dynamically scale an availability level of processing workers.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The computing platform of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein monitoring the pending workload in the work queue database comprises requesting one or more size values from the work queue database.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computing platform of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein dynamically scaling the availability level of processing workers comprises initializing at least one additional processing worker.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computing platform of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein dynamically scaling the availability level of processing workers comprises destroying at least one processing worker.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing platform of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the memory stores additional computer-readable instructions that, when executed by the at least one hardware processor, cause the computing platform to:<claim-text>cause a monitoring process to create one or more historical records; and</claim-text><claim-text>dynamically optimize one or more processing parameters based on the one or more historical records.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A method, comprising:<claim-text>at a computing platform comprising at least one hardware processor, a communication interface, and memory:<claim-text>initializing, by the at least one hardware processor, at least two processing workers, wherein initializing the at least two processing workers comprises initializing a first processing worker and a second processing worker;</claim-text><claim-text>causing, by the at least one hardware processor, the first processing worker to:<claim-text>perform a first query on a work queue database; and</claim-text><claim-text>initiate parallel processing of a first set of work items identified in the work queue database, wherein initiating parallel processing of the first set of work items comprises:</claim-text><claim-text>processing a first subset of the first set of work items in parallel; and</claim-text><claim-text>upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database; and</claim-text></claim-text><claim-text>causing, by the at least one hardware processor, the second processing worker to:<claim-text>perform a second query on the work queue database; and</claim-text><claim-text>initiate parallel processing of a second set of work items identified in the work queue database, wherein initiating parallel processing of the second set of work items comprises:<claim-text>processing a first subset of the second set of work items in parallel; and</claim-text><claim-text>upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database.</claim-text></claim-text></claim-text></claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein performing the first query on the work queue database comprises causing a database engine associated with the work queue database to lock the first set of work items upon read.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein performing the first query on the work queue database comprises selecting the first set of work items based on one or more priority parameters.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein performing the first query on the work queue database comprises writing, in the work queue database, a first timestamp indicating when the first set of work items were read from the work queue database.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the first timestamp is tracked by a database engine associated with the work queue database to provide expired items to one or more other processing workers.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein performing the second query on the work queue database comprises reading at least one work item that was read and locked by the first processing worker.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>monitoring, by the at least one hardware processor, a pending workload in the work queue database; and</claim-text><claim-text>based on monitoring the pending workload in the work queue database, dynamically scaling, by the at least one hardware processor, an availability level of processing workers.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein monitoring the pending workload in the work queue database comprises requesting one or more size values from the work queue database.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. One or more non-transitory computer-readable media storing instructions that, when executed by a computing platform comprising at least one hardware processor, a communication interface, and memory, cause the computing platform to:<claim-text>initialize at least two processing workers, wherein initializing the at least two processing workers comprises initializing a first processing worker and a second processing worker;</claim-text><claim-text>cause the first processing worker to:<claim-text>perform a first query on a work queue database; and</claim-text><claim-text>initiate parallel processing of a first set of work items identified in the work queue database, wherein initiating parallel processing of the first set of work items comprises:<claim-text>processing a first subset of the first set of work items in parallel; and</claim-text><claim-text>upon completion of processing each work item of the first subset of the first set of work items, marking the corresponding work item as completed in the work queue database; and</claim-text></claim-text></claim-text><claim-text>cause the second processing worker to:<claim-text>perform a second query on the work queue database; and</claim-text><claim-text>initiate parallel processing of a second set of work items identified in the work queue database, wherein initiating parallel processing of the second set of work items comprises:<claim-text>processing a first subset of the second set of work items in parallel; and</claim-text><claim-text>upon completion of processing each work item of the first subset of the second set of work items, marking the corresponding work item as completed in the work queue database.</claim-text></claim-text></claim-text></claim-text></claim></claims></us-patent-application>