<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230003894A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230003894</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17781727</doc-number><date>20201215</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="regional"><country>EP</country><doc-number>19216454.9</doc-number><date>20191216</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>894</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>246</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20200101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>894</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>17</main-group><subgroup>48</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>01</class><subclass>S</subclass><main-group>7</main-group><subgroup>4808</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>248</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>579</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>521</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>20221</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">TIME-OF-FLIGHT IMAGING CIRCUITRY, TIME-OF-FLIGHT IMAGING SYSTEM, TIME-OF-FLIGHT IMAGING METHOD</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Sony Semiconductor Solutions Corporation</orgname><address><city>Atsugi-shi, Kanagawa</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>ZLOKOLICA</last-name><first-name>Vladimir</first-name><address><city>Stuttgart</city><country>DE</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>KAMOVITCH</last-name><first-name>Alex</first-name><address><city>Stuttgart</city><country>DE</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>Sony Semiconductor Solutions Corporation</orgname><role>03</role><address><city>Atsugi-shi, Kanagawa</city><country>JP</country></address></addressbook></assignee></assignees><pct-or-regional-filing-data><document-id><country>WO</country><doc-number>PCT/EP2020/086280</doc-number><date>20201215</date></document-id><us-371c12-date><date>20220602</date></us-371c12-date></pct-or-regional-filing-data></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">The present disclosure generally pertains to a time-of-flight imaging circuitry configured to: obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determine a first image feature in the first image data; obtain second image data from the image sensor, the second image data being indicative of the scene; determine second image feature in the second image data; estimate a motion of the second image feature with respect to the first image feature; and merge the first and the second image data based on the estimated motion.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="79.59mm" wi="67.65mm" file="US20230003894A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="217.42mm" wi="111.76mm" file="US20230003894A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="243.08mm" wi="110.49mm" file="US20230003894A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="250.11mm" wi="154.86mm" orientation="landscape" file="US20230003894A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="189.31mm" wi="143.51mm" file="US20230003894A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="87.97mm" wi="130.98mm" file="US20230003894A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">TECHNICAL FIELD</heading><p id="p-0002" num="0001">The present disclosure generally pertains to a time-of-flight imaging circuitry, a time-of-flight imaging system, and a time-of-flight imaging method.</p><heading id="h-0002" level="1">TECHNICAL BACKGROUND</heading><p id="p-0003" num="0002">Generally, time-of-flight imaging systems are known. Such systems typically measure a roundtrip delay of emitted light, a phase-shift of light (which may indicate a roundtrip delay), a distortion of emitted light, or the like, for determining a depth map or a three-dimensional model of an object.</p><p id="p-0004" num="0003">Generally, in order to measure or to image a three-dimensional object, it is desirable to have a relatively exact measurement output. However, in known systems, so called multipath artifacts, aliasing effects, and the like, may deteriorate such a measurement.</p><p id="p-0005" num="0004">Although there exist techniques for processing time-of-flight image data, it is generally desirable to provide a time-of-flight imaging circuitry, a time-of-flight imaging system, and a time-of-flight imaging method.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">According to a first aspect the disclosure provides a time-of-flight imaging circuitry configured to: obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determine a first image feature in the first image data; obtain second image data from the image sensor, the second image data being indicative of the scene; determine a second image feature in the second image data; estimate a motion of the second image feature with respect to the first image feature; and merge the first and the second image data based on the estimated motion.</p><p id="p-0007" num="0006">According to a second aspect the disclosure provides a time-of-flight imaging system, comprising: a spotted light source configured to illuminate a scene with spotted light; an image sensor; and time-of-flight imaging circuitry configured to: obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determine a first image feature in the first image data; obtain second image data from the image sensor, the second image data being indicative of the scene; determine a second image feature in the second image data; estimate a motion of the second image feature with respect to the first image feature; and merge the first and the second image data based on the estimated motion.</p><p id="p-0008" num="0007">According to a third aspect the disclosure provides a time-of-flight imaging method, comprising: obtaining first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determining a first image feature in the first image data; obtaining second image data from the image sensor, the second image data being indicative of the scene; determining a second image feature in the second image data; estimating a motion of the second image feature with respect to the first image feature; and merging the first and the second image data based on the estimated motion.</p><p id="p-0009" num="0008">Further aspects are set forth in the dependent claims, the following description and the drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0010" num="0009">Embodiments are explained by way of example with respect to the accompanying drawings, in which:</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>1</b></figref> depicts a block diagram of a time-of-flight imaging system according to the present disclosure;</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a block diagram of a time-of-flight imaging method according to the present disclosure;</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts a block diagram of a further embodiment of a time-of-flight imaging method according to the present disclosure;</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts a further embodiment of a time-of-flight imaging method according to the present disclosure;</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a block diagram of a mobile phone according to the present disclosure;</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a block diagram of a further embodiment of a time-of-flight imaging method according to the present disclosure; and</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>7</b></figref> depicts a method for using a reference frame.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION OF EMBODIMENTS</heading><p id="p-0018" num="0017">Before a detailed description of the embodiments under reference of <figref idref="DRAWINGS">FIG. <b>1</b></figref> is given, general explanations are made.</p><p id="p-0019" num="0018">As mentioned in the outset, a measurement with a known time-of-flight system may lead to a multipath artifact, which may deteriorate the measurement, whereas it is generally desirable to decrease imaging artifacts in general.</p><p id="p-0020" num="0019">Moreover, it is generally desirable to increase a resolution of a time-of-flight measurement, of a depth-map and/or of a three-dimensional model of an object in some instances.</p><p id="p-0021" num="0020">It has been recognized that, in the case of spot time-of-flight, a scene may be illuminated with a limited number of light spots. Therefore, the resolution of an acquired time-of-flight image may be confined to this limited number of light spots.</p><p id="p-0022" num="0021">It has also been recognized that the resolution may be increased by increasing the number of light spots, wherein, it is desirable to maintain a size of a light source and not increase it for increasing the number of light spots.</p><p id="p-0023" num="0022">Thus, it has been recognized that this may be achieved by a movement or a motion of the light source between consecutive frames of the time-of-flight image acquisition, such that a wider area of the scene (and/or the object) may be illuminated.</p><p id="p-0024" num="0023">Moreover, it has been recognized that an image quality may be improved by a movement or a motion of an image sensor. In a single time-of-flight measurement, it may, in some instances, not be possible to distinguish between the light signal which is supposed to be measured and a reflected light signal, which may have travelled a longer way. Generally, such an effect is known as a multipath-effect.</p><p id="p-0025" num="0024">Hence, it has been recognized that an influence of a multipath effect, may be decreased by taking into account two (or more) (consecutive) measurements from different positions, which may be based on a motion of the image sensor, as indicated above.</p><p id="p-0026" num="0025">For example, by using a set of light spots with a predetermined number, a multipath artifact may be recognized or estimated, and thereby is may be possible to remove the artifact from an obtained depth and/or from an obtained confidence value.</p><p id="p-0027" num="0026">The multipath artifact may be recognized by taking into account at least one neighboring light spot with respect to a light spot (of the set of light spots). This may be possible since a non-continuous distribution of light (spots) may be used, in some embodiments.</p><p id="p-0028" num="0027">In other embodiments, which may use a continuous distribution of (modulated) light (e.g. indirect time-of-flight), a motion of an imaging element (e.g. pixel) with respect to an object or a fixed position of the object may be utilized for recognizing a multipath artifact.</p><p id="p-0029" num="0028">A continuous distribution of light may be achieved by increasing a spot density of a spotted light source, such light spots (partly) overlap. However, the present disclosure is not limited to generating a continuous distribution of light with a spotted light source, such that any light source may be utilized.</p><p id="p-0030" num="0029">In some embodiments, a recognition of a multipath artifact results in a reduced resolution (of a single frame). However, the reduced resolution is compensated for by acquiring multiple frames and merging them (as discussed herein).</p><p id="p-0031" num="0030">Therefore, some embodiments pertain to a time-of-flight imaging circuitry configured to: obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determine a first image feature in the first image data; obtain second image data from the image sensor, the second image data being indicative of the scene; determine a second image feature in the second image data; estimate a motion of the second image feature with respect to the first image feature; and merge the first and the second image data based on the estimated motion.</p><p id="p-0032" num="0031">Generally, a time-of-flight imaging circuitry may include any circuitry configured to carry out, process, evaluate, perform, and the like, a time-of-flight measurement, such as a processor, e.g. a CPU (central processing unit), a GPU (graphic processing unit), an FPGA (field programmable gate array), and the like, wherein also multiple of such components, also in combination, may be envisaged. Moreover, the time-of-flight imaging circuitry may include or may be included in a computer, a server, a camera, and the like, and/or combinations thereof.</p><p id="p-0033" num="0032">The time-of-flight imaging circuitry may be configured to obtain first image data from an image sensor.</p><p id="p-0034" num="0033">The image sensor may generally be any known image sensor, which may include one or multiple imaging elements (e.g. pixels) and be based on a known semiconductor or diode technology, such as CMOS (complementary metal oxide semiconductor), CCD (charge coupled device), CAPD (current assisted photonic demodulator), SPAD (single photon avalanche diode), and the like.</p><p id="p-0035" num="0034">The image sensor may be configured to generate an electric signal in response to light being incident on an image plane (e.g. on one or a plurality of pixels), as it is commonly known, wherein the electric signal may be processed and thereby, the first image data may be indicated, generated, and the like.</p><p id="p-0036" num="0035">Moreover, the image plane may have a larger area than the image sensor. For example, the image plane may be established by a total area in which the image sensor is moved, such that the image plane may at least cover this area (or even more).</p><p id="p-0037" num="0036">Obtaining of the first and/or the second image data may include a sending of a request to the image sensor (or to any circuitry coupled to the image sensor, such as a memory, and the like) in order to (actively) acquire the first image data, whereas, in some embodiments, the first (and/or second) image data are (passively) received by the time-of-flight imaging circuitry at a predetermined point of time. That means that obtaining may further include a reception of the first and/or the second image data in response to a request sent to the image sensor, received via a bus, from a data storage, or the like. In general, it should be noted that an active acquisition of the first image data may also include a passive reception, i.e. an (active) request may establish a passive reception of the first and/or the second image data at a predetermined point of time, and the like.</p><p id="p-0038" num="0037">The first (and/or the second) image data may be indicative of a scene. The scene may include an object of which a depth measurement shall be (or is) performed. Moreover, the scene may include a surrounding of the object (e.g. a background), which may have a larger projected area on the image plane than the object, such that the scene may still be captured in cases in which the object is not being (fully) captured by the image sensor after a movement of the image sensor, and the like.</p><p id="p-0039" num="0038">Generally, the scene may be illuminated with spotted light, which may originate from a spotted light source, such as a diode laser (or multiple diode lasers), VCSEL(s) (vertical-cavity surface-emitting laser), and the like, which may be configured to illuminate the scene with a plurality of light spots generated by the spotted light source. The spotted light may be based on a predefined pattern, wherein the shape and/or the arrangement of the plurality of light spots may be predefined, such that a distortion, smearing, deformation, and the like of at least one of the plurality of light spots and/or of the pattern (e.g. change of distances/arrangement of the different light spots) may be indicative of a distance or a depth between the image sensor and/or the light source to the scene.</p><p id="p-0040" num="0039">In some embodiments, the distance between (at least) two light spots may be indicative of the object (and/or the scene) and/or a respective (relative or absolute) depth of the two light spots or may be indicative of the image feature.</p><p id="p-0041" num="0040">Hence, a time-of-flight measurement according to the present disclosure may be performed at different lighting scenarios, e.g. in a dark lighting condition (e.g. with roughly no background light), in a bright lighting condition (e.g. in sunlight), in a room, in daylight, or the like.</p><p id="p-0042" num="0041">Moreover, different wavelength bands (or channels) may be used in the spotted light source.</p><p id="p-0043" num="0042">For example, the light source may emit different light colors (light having different wavelength ranges, e.g. infrared (wavelength range) and green (wavelength range)) for having a more precise feature reconstruction for features, which are more sensitive for the respective color (wavelength range). For example, human skin may have a known reflectivity for infrared light (e.g. eighty percent), whereas a flower or a plant may have a known reflectivity for green light (e.g. ninety percent), such that, in such embodiments, the light source may be configured to emit infrared light and green light, without limiting the present disclosure in that regard since any wavelength ranges may be emitted. Moreover, the present disclosure is not limited to the case of two different colors as three, four, five, or more colors may be emitted, as well.</p><p id="p-0044" num="0043">In the first image data, a first image feature may be determined.</p><p id="p-0045" num="0044">The first image data may be indicative of a confidence and/or a depth, as it is generally known.</p><p id="p-0046" num="0045">For example, at least one light spot of the scene may be analyzed in the (first and/or second) image data with respect to an image property.</p><p id="p-0047" num="0046">The image property may include a shape (of at least a part of the object and/or the scene), a pattern (of at least a part of the object and/or the scene), and the like.</p><p id="p-0048" num="0047">Moreover, at least one spot (another spot or the same spot) may be analyzed with respect to an image condition.</p><p id="p-0049" num="0048">The image condition may include a light intensity, a reflectance, a scattering property, and the like, of the object/and or the scene.</p><p id="p-0050" num="0049">In some embodiments, only an image condition (or more than one image condition) may be analyzed, whereas, in some embodiments, only an image property (or more than one image property) may be analyzed. In some embodiments, however, at least one image condition and at least one image property may be analyzed.</p><p id="p-0051" num="0050">At least one of such image conditions and/or image properties may correspond to the first image feature or may be represented or included by the first image feature.</p><p id="p-0052" num="0051">Hence, the first image feature may include (or be based on) at least one image condition and/or at least one image property.</p><p id="p-0053" num="0052">Moreover, the first image feature may be recognized, based on at least one image condition and/or at least one image property, for example by an artificial intelligence, an artificial neural network, which may utilize one or more machine learning algorithms for determining the first image feature, and the like.</p><p id="p-0054" num="0053">Furthermore, the time-of-flight imaging circuitry may obtain second image data, which may be generated in a similar way as the first image data, without limiting the present disclosure in that regard.</p><p id="p-0055" num="0054">In the second image data, a second image feature may be determined. The determination of the second image feature may be carried out in a similar way as the determination of the first image feature, without limiting the present disclosure in that regard. The second image feature may correspond to the first image feature, such that it may include the same image feature, wherein, in some embodiments, the second image feature may include the first image feature being imaged or measured from a different perspective, e.g. after a movement of the image sensor.</p><p id="p-0056" num="0055">However, in some embodiments, the second image feature may differ from the first image feature. For example, in response to or during an (intentional (e.g. controlled) or unintentional (e.g. a shaking of a hand)) movement of the image sensor, the second image feature may be determined to be projected to or recognized in the same (group of) pixel(s) as the first image feature before the movement.</p><p id="p-0057" num="0056">However, the second image feature may, in some embodiments, differ from the first image feature in that it may be recognized based on a different set of image conditions or image properties or in that particular values or magnitudes of the set of image conditions may differ from the values or magnitudes of the first image feature.</p><p id="p-0058" num="0057">For example, if the second image feature is a different pattern of the object than the first image feature, it may be indicative that a different part of the object is being imaged. This part may have not been imaged in the first image data at all or may have been imaged with a different group of pixels.</p><p id="p-0059" num="0058">In the first case (not imaged), a resolution of a resulting image may be increased. In the second case (different group of pixels), a multipath effect may be filtered.</p><p id="p-0060" num="0059">However, in some embodiments, a multipath effect may be ignored, since, e.g. it is small, or may have already been filtered (as discussed above).</p><p id="p-0061" num="0060">Moreover, a correspondence between the first and the second image feature (or a set of points which may be indicated by the first and/or the second image feature) may be recognized. For example, the first image feature may be indicative of a reference set of points and the second image feature may be indicative for second a set of points which is compared to the reference set of points.</p><p id="p-0062" num="0061">Furthermore, the recognized correspondence may be used to merge at least two frames with each other based on the recognized correspondence.</p><p id="p-0063" num="0062">Thereby, an image quality (e.g. resolution) may be increased.</p><p id="p-0064" num="0063">Such a recognizing of a correspondence and/or a merging may be repeated iteratively, in some embodiments, for maximizing an image quality.</p><p id="p-0065" num="0064">Based on the second image feature, a motion of the second image feature with respect to the first feature may be estimated.</p><p id="p-0066" num="0065">For example, a position of the first and the second image feature may be determined. The respective positions may include positions on the image plane, on the image sensor, within the scene, and the like. The different positions may then be compared, such that a motion of the second image feature with respect to the first image feature may be estimated or determined.</p><p id="p-0067" num="0066">Based on the estimated motion, the first and the second image data may be merged. Thereby, merged image data may be generated which may include the first and the second image feature and/or the respective positions of the first and the second image feature.</p><p id="p-0068" num="0067">The merging may include a combination of the first and the second image data taking into account the estimated motion and the first and/or the second image feature. Thus, the merged image data (or an image based on the merged image data) may have an increased resolution compared to the first and the second image data and may have a reduced number of imaging artifacts (e.g. multipath artifact).</p><p id="p-0069" num="0068">In some embodiments, the motion is based on a vibration of at least one of the image sensor and a light source generating the spotted light.</p><p id="p-0070" num="0069">A vibration may be a movement caused by a vibration device, such as an eccentric motor, a linear resonant actuator, and the like, as it is generally known, such that a motion, a displacement, and the like, of the image sensor and/or the light source is caused, as it is discussed herein, for determining the first and the second image feature.</p><p id="p-0071" num="0070">It should be noted that a motion or a movement is not limited to be caused by or based on a vibration, since a time-of-flight imaging circuitry may process first image data and second image data based on any kind of movement (or even no movement).</p><p id="p-0072" num="0071">For example, the movement may be caused by an (unintentional) shaking of a hand, a motion of a vehicle, which may cause a (random) movement, a vibration caused by a motor of the vehicle, and the like (in embodiments in which a time-of-flight imaging circuitry is provided in the vehicle).</p><p id="p-0073" num="0072">Moreover, a motion or movement may include a controlled (slow) movement onto a predetermined position, wherein an amplitude of such a controlled movement may typically be larger than of a vibration.</p><p id="p-0074" num="0073">Moreover, a (spotted) light source may be adapted to illuminate the object (or the scene) in a manner that with each illumination cycle, a different part of the object (or the scene) may be illuminated. Thereby, a movement may be simulated.</p><p id="p-0075" num="0074">In some embodiments, the time-of-flight imaging circuitry is further configured to carry out a triangulation including the first image feature and the second image feature for estimating the motion and/or a depth.</p><p id="p-0076" num="0075">The triangulation may include a known distance, e.g. a reference point for further specifying the position of the second image feature, and the like.</p><p id="p-0077" num="0076">Moreover, a triangulation may be utilized to determine a further image feature or a position of a further image feature taking into account the respective positions of the first and the second image feature.</p><p id="p-0078" num="0077">In some embodiments, as discussed above, a reference set of points and a second set of points may be acquired, and a correspondence may be recognized. Based on the recognized correspondence, and essential or fundamental matrix may be determined, which may be indicative for a rotation and a translation between the reference set of points of the second set of points, and, thereby between the first and the second image feature.</p><p id="p-0079" num="0078">Based on the rotation and translation, a triangulation may be performed and a depth of the second image feature may be determined, which may be compared to a depth of the first image. In a case in which the comparison of the respective depths is below a predetermined threshold, and the determined rotation and translation may be assumed to have a predetermined accuracy.</p><p id="p-0080" num="0079">If the predetermined accuracy is reached, a first and a second frame (including the first and the second image feature) may be merged (e.g. blended) taking into account the translation and rotation (e.g. for correcting a movement distortion), whereby a resolution may be increased.</p><p id="p-0081" num="0080">In some embodiments, the triangulation may be based on a disparity of a determined depth of the second image feature compared to the first image feature, as discussed above. The disparity may be defined as one over the depth. Thus, the difference (disparity) of the positions of the second image feature and the first image feature may be expressed as: x<sub>2</sub>-x<sub>1</sub>=(1/depth), x<sub>1 </sub>including a position of the first image feature, and x<sub>2 </sub>including a position of the second image feature. Assuming that the first and the second image feature correspond (to each other), and the depth is assumed constant, the position of, for example, the first image feature may be determined.</p><p id="p-0082" num="0081">In some embodiments, the time-of-flight imaging circuitry is further configured to match the first image feature and the second image feature.</p><p id="p-0083" num="0082">The matching, as indicated above, may be performed, if the first and the second image feature are the same, but shifted by a displacement caused by a motion.</p><p id="p-0084" num="0083">The matching may be performed based on the second image feature having the same (or similar, e.g. below a predetermined threshold) image condition or image property as the first image feature.</p><p id="p-0085" num="0084">For example, the second image feature may be matched with the first image feature when the light intensity of the second image feature is within a predetermined threshold to the light intensity of the first image feature.</p><p id="p-0086" num="0085">In some embodiments, the time-of-flight imaging circuitry is further configured to determine a first depth based on the match.</p><p id="p-0087" num="0086">Since the first and the second image feature may correspond to each other, but their position may be based on a displacement, as described herein, the respective features may be symbolically expressed in different (local) coordinate systems.</p><p id="p-0088" num="0087">Based on such coordinate systems, a more precise depth or distance determination may be possible (more precise than with only one time-of-flight measurement as known in the art), such that, based on the match, the distance may be determined, for example by taking a (weighted) mean of the distance of the first image feature and a distance of the second image feature.</p><p id="p-0089" num="0088">Moreover, the first distance may be determined in a global coordinate system or an image sensor coordinate system taking into account the two (or at least two) local coordinate systems and the respective positions of the first and the second image feature.</p><p id="p-0090" num="0089">In some embodiments, the time-of-flight imaging circuitry is further configured to determine at least one third image feature based on third image data indicating a second depth.</p><p id="p-0091" num="0090">The third image feature may be a further image feature being different from the first and/or the second image feature, which may be for example a different feature, pattern, and the like, (and thus may be indicated by a different imaging condition and/or imaging property) of the object. The third image feature may have a second depth.</p><p id="p-0092" num="0091">It should be noted that the second depth may generally be (roughly) the same depth as the first depth, but as indicated, may be found at a different position of the object and/or of the scene.</p><p id="p-0093" num="0092">In some embodiments, the time-of-flight circuitry is further configured to determine a third depth based on the first and the second depth.</p><p id="p-0094" num="0093">The third depth may be determined based on a processing of the first and the second depth, and therefore, it may not be necessary to perform a further time-of-flight measurement, such that the third depth may indicate a further or fourth (virtual) image feature, which may be determined, in some embodiments, by an interpolation including the first and the second depth.</p><p id="p-0095" num="0094">In some embodiments, the time-of-flight imaging circuitry is further configured to temporally align the first and the second image data based on the estimated motion.</p><p id="p-0096" num="0095">Since the estimated motion may be expressible in a velocity, a celerity, a speed, and the like, but may (symbolically) be depicted or interpreted as a vector between the first and the second image feature, wherein a time between the determination (or acquisition) of the first and the second image feature may be known, the first and the second image data may, thus, be temporally aligned, thereby simplifying the resulting image (or depth measurement) and increasing a precision of the measurement.</p><p id="p-0097" num="0096">Some embodiments pertain to a time-of-flight imaging system including: a spotted light source configured to illuminate a scene with spotted light; an image sensor; and time-of-flight imaging circuitry configured to: obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determine a first image feature in the first image data; obtain second image data from the image sensor, the second image data being indicative of the scene; determine a second image feature in the second image data; estimate a motion of the second image feature with respect to the first image feature; and merge the first and the second image data based on the estimated motion, as described herein.</p><p id="p-0098" num="0097">Generally, the time-of-flight imaging system may include further elements, such as a lens (stack), and the like, as they are generally known, and therefore, a description of such known components is omitted.</p><p id="p-0099" num="0098">The elements of the time-of-flight imaging system (spotted light source, image sensor, time-of-flight imaging circuitry, and the like) may be distributed in several sub-systems, or may be provided in an integrated system, such as a time-of-flight camera, a mobile phone, a car, and the like.</p><p id="p-0100" num="0099">For example, if the time-of-flight imaging system is a mobile phone, such that an application of the time-of-flight imaging system may be considered as a three-dimensional scanning, registration and/or recognition of an object, a time-of-flight acquisition may be performed within a predetermined distance between the mobile phone and the object.</p><p id="p-0101" num="0100">The mobile phone may provide a trigger (e.g. a virtual or real button) for starting a three-dimensional acquisition. In response to the trigger, a vibration of the mobile phone may be initiated. The vibration may last until a frame of the time-of-flight measurement is acquired (or extracted) (e.g. for ten seconds). During such an acquisition period, a time-of-flight imaging method (as described below) may be performed.</p><p id="p-0102" num="0101">Moreover, in some embodiments, a further acquisition may be initiated from a different angle (or from a different perspective) with respect to the object.</p><p id="p-0103" num="0102">Thereby, a three-dimensional model of the object including a mesh, a shading and/or a texture, and the like may be generated.</p><p id="p-0104" num="0103">However, the present disclosure is not limited to multiple acquisitions, such that the time-of-flight imaging method (discussed below) may be based on one acquisition (one shot), as well, for example for a recognition, a face authentication, and the like.</p><p id="p-0105" num="0104">In some embodiments, the time-of-flight imaging system further includes a vibration device configured to generate a vibration of the time-of-flight imaging system, wherein the vibration is indicative of the motion of the second image feature with respect to the first image feature, as discussed herein.</p><p id="p-0106" num="0105">The vibration device, as discussed above, may include an eccentric motor, a linear resonant actuator, and the like.</p><p id="p-0107" num="0106">Some embodiments pertain to a time-of-flight imaging method, including: obtaining first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light; determining a first image feature in the first image data; obtaining second image data from the image sensor, the second image data being indicative of the scene; determining a second image feature in the second image data; estimating a motion of the second image feature with respect to the first image feature; and merging the first and the second image data based on the estimated motion, as discussed herein.</p><p id="p-0108" num="0107">The time-of-flight imaging method according to the present disclosure may be executed by a time-of-flight imaging circuitry according to the present disclosure, a time-of-flight system according to the present disclosure, and the like.</p><p id="p-0109" num="0108">In some embodiments, the motion is based on a vibration of at least one of the image sensor and a light source generating the spotted light, as discussed herein. In some embodiments, the time-of-flight method further includes carrying out a triangulation including the first image feature and the second image feature for estimating the motion, as discussed herein. In some embodiments, the time-of-flight method further includes matching the first image feature and the second image feature, as discussed herein. In some embodiments, the time-of-flight method further includes determining a first depth based on the match, as discussed herein. In some embodiments, the time-of-flight method further includes determining at least one third image feature based on third image data indicating a second depth, as discussed herein. In some embodiments, the time-of-flight method further includes determining a third depth based on the first and the second depth, as discussed herein. In some embodiments, the third depth is based on an interpolation including the first and the second depth, as discussed herein. In some embodiments, the time-of-flight method further includes temporally aligning the first and the second image data based on the estimated motion, as discussed herein.</p><p id="p-0110" num="0109">The method(s) as described herein are also implemented in some embodiments as a computer program causing a computer and/or a processor to perform the method, when being carried out on the computer and/or processor. In some embodiments, also a non-transitory computer-readable recording medium is provided which stores therein a computer program product, which, when executed by a processor, such as the processor described above, causes the methods described herein to be performed.</p><p id="p-0111" num="0110">Returning to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, there is depicted block diagram of a time-of-flight imaging system <b>1</b> according to the present disclosure.</p><p id="p-0112" num="0111">The time-of-flight imaging system <b>1</b> has a lens stack <b>2</b> which is configured to focus light onto an image sensor <b>3</b>, as it is discussed herein.</p><p id="p-0113" num="0112">Moreover, a time-of-flight imaging circuitry <b>4</b>, as it is discussed herein, may obtain (first and second) image data from the image sensor <b>3</b>, determine a first and a second image feature, estimate a motion of the second image feature with respect to the first image feature, and merge the first and the second image data based on the estimated motion, as discussed herein.</p><p id="p-0114" num="0113">The time-of-flight imaging system <b>1</b> further includes a spotted light source <b>5</b> and a vibration device <b>6</b>, as discussed herein.</p><p id="p-0115" num="0114"><figref idref="DRAWINGS">FIG. <b>2</b></figref> depicts a block diagram of a time-of-flight imaging method <b>10</b> according to the present disclosure.</p><p id="p-0116" num="0115">In <b>11</b>, first image data are obtained from an image sensor, wherein the first image data are indicative of a scene, which is illuminated with spotted light, as discussed herein. In this embodiment, a time-of-flight imaging circuitry, which is configured to carry out the time-of-flight imaging method <b>10</b>, is connected to the image sensor via a bus, such that the image sensor transmits the first image data to the time-of-flight imaging circuitry.</p><p id="p-0117" num="0116">In <b>12</b>, a first image feature is determined in the first image data by a pattern recognition algorithm implemented in the time-of-flight imaging circuitry.</p><p id="p-0118" num="0117">In <b>13</b>, second image data are obtained from the image sensor via the bus.</p><p id="p-0119" num="0118">In <b>14</b>, a second image feature is determined in the second image data by the pattern recognition algorithm, as discussed herein.</p><p id="p-0120" num="0119">In <b>15</b>, a motion of the second image feature is estimated with respect to the first image feature by comparing a position of the second image feature with respect to the first image feature.</p><p id="p-0121" num="0120">In <b>16</b>, the first and the second image data are merged based on the estimated motion, as discussed herein.</p><p id="p-0122" num="0121"><figref idref="DRAWINGS">FIG. <b>3</b></figref> depicts, in a block diagram, a further embodiment of a time-of-flight imaging method <b>20</b> according to the present disclosure.</p><p id="p-0123" num="0122">The time-of-flight imaging method <b>20</b> differs from the time-of-flight imaging method <b>10</b>, which is described with respect to <figref idref="DRAWINGS">FIG. <b>2</b></figref> in that a motion is detected based on a triangulation, that the first and the second image data are temporally aligned and that a third image feature is determined based the first and the second depth.</p><p id="p-0124" num="0123">In this embodiment, the motion is determined based on confidence data, which is generally known in the field of time-of-flight. Based on the confidence data, the triangulation is performed for determining a depth.</p><p id="p-0125" num="0124">In <b>21</b>, first image data are obtained from an image sensor, wherein the first image data are indicative of a scene, which is illuminated with spotted light, as discussed herein. In this embodiment, the image sensor and a time-of-flight imaging circuitry carrying out the time-of-flight imaging method <b>20</b> are connected via a bus through which the image sensor transmits the first image data.</p><p id="p-0126" num="0125">In <b>22</b>, a first image feature is determined in the first image data by a pattern recognition algorithm implemented in the time-of-flight imaging circuitry.</p><p id="p-0127" num="0126">In <b>23</b>, second image data are obtained from the image sensor via the bus.</p><p id="p-0128" num="0127">In <b>24</b>, a second image feature is determined in the second image data by the pattern recognition algorithm.</p><p id="p-0129" num="0128"> In <b>25</b>, a triangulation including the first image feature and the second image feature is carried out. That is, based on a reference point and a position of the first image feature, a position of the second image feature is determined.</p><p id="p-0130" num="0129">In <b>26</b>, a motion of the second image feature is estimated with respect to the first image feature, based on the triangulation.</p><p id="p-0131" num="0130">In <b>27</b>, the first image feature and the second image feature are matched. That is, based on the motion, the respective positions of the first and the second image feature are transformed into a global coordinate system.</p><p id="p-0132" num="0131">In <b>28</b>, a first depth is determined based on the match, since a multipath effect is excluded in <b>27</b> by transforming the first and the second image feature into the global coordinate system.</p><p id="p-0133" num="0132">In <b>29</b>, the first and the second image data are temporally aligned based on the motion, as discussed herein.</p><p id="p-0134" num="0133">In <b>30</b>, the first and the second image data are merged based on the estimated motion, such that resulting merged image data have the determined depth based on one point of the global coordinate system.</p><p id="p-0135" num="0134">In <b>31</b>, at least one third image feature is determined based on third image data indicating a second depth. The third image feature is determined as the first and/or the second image feature. However, the third image feature is at a different position of the object and is, therefore, distinct from the first and the second image feature.</p><p id="p-0136" num="0135">In <b>32</b>, a third depth is determined based on the first and the second depth with an interpolation between the first and the second depth.</p><p id="p-0137" num="0136"><figref idref="DRAWINGS">FIG. <b>4</b></figref> depicts a time-of-flight imaging method <b>40</b> according to the present disclosure. The time-of-flight imaging method <b>40</b> differs from the previous embodiments of the time-of-flight imaging methods <b>20</b> in that it is performed by a mobile phone including a vibration device.</p><p id="p-0138" num="0137">A mobile phone <b>41</b> includes a time-of-flight imaging system <b>42</b>. It should be noted that, in this embodiment, a vibration device is not included in the time-of-flight imaging system <b>42</b>, but in the mobile phone <b>41</b>, such that the time-of-flight imaging system <b>42</b> undergoes a motion when the mobile phone <b>41</b> vibrates.</p><p id="p-0139" num="0138">From the mobile phone <b>41</b> (and the time-of-flight imaging system <b>42</b>), initial motion and position information, confidence data, and depth data, as it is generally known and described herein, are determined in <b>43</b>.</p><p id="p-0140" num="0139">Based on the initial motion and position information, which serve as a reference point in a global coordinate system (as discussed above), a further position of a time-of-flight image sensor of the time-of-flight imaging system <b>42</b> is determined in <b>44</b>.</p><p id="p-0141" num="0140">Moreover, for a plurality of confidence sub-frames <b>45</b>, which are acquired consecutively at points of time t, t+1, and t+2, confidence values of the time-of-flight measurement are determined. For a plurality of depth sub-frames <b>46</b>, which are acquired consecutively at roughly the points of time t, t+1, and t+2, depth values are determined.</p><p id="p-0142" num="0141">Based on the sensor position and movement, as well as the confidence values and a reference frame <b>47</b> of a previous measurement at the point of time t-<b>1</b>, a motion estimation is performed in <b>48</b>.</p><p id="p-0143" num="0142">Furthermore, the confidence values, which are determined based on the confidence sub-frames <b>45</b> at the points of time t, t+1, and t+2, and confidence values of the reference frame are matched and triangulated in <b>49</b>.</p><p id="p-0144" num="0143">In <b>50</b>, the matched confidence values are compared with a confidence value of the reference frame. Moreover, the depth values of the depth sub-frames <b>46</b> are compared with a depth value from the reference frame. On the basis of these comparisons, a further refinement of the measurement is performed.</p><p id="p-0145" num="0144">If the comparison in <b>50</b> is such that the confidence values and the depth values of the confidence sub-frames <b>45</b> and the depth sub-frames <b>46</b> converge with the confidence value and the depth value of the reference frame, in <b>51</b>, each confidence value and depth value of the sub-frames <b>45</b> and <b>46</b> is associated with the estimated motion, and based on this association, a temporal alignment and spatio-temporal (after the temporal alignment and after a determination of a second depth, as discussed above) interpolation between determined confidence values and depth values is performed, as discussed herein.</p><p id="p-0146" num="0145">The sub-frames <b>45</b> and <b>46</b> are processed to become a confidence frame and a depth frame, which then serve (together) as a reference frame T for a subsequent measurement, which is shown in <b>52</b>.</p><p id="p-0147" num="0146"><figref idref="DRAWINGS">FIG. <b>5</b></figref> depicts a block diagram of a mobile phone <b>60</b> (as a time-of-flight imaging system) including a vibration device <b>61</b>, an inertial measurement unit (IMU) <b>62</b>, a spot light source <b>63</b>, a time-of-flight image sensor <b>64</b>, and control circuitry <b>65</b> for controlling a vibration, for controlling a timing of the spot light source <b>63</b> and/or for controlling the time-of-flight image sensor <b>64</b>. Moreover, the control circuitry <b>65</b> is adapted as a time-of-flight imaging circuitry according to the present disclosure.</p><p id="p-0148" num="0147">The spot light source <b>63</b> is a dot projector which is configured to project a grid of small infrared dots (or spots) onto an object (or a scene, as discussed above) and is, in this embodiment including a plurality of VCSELs (vertical-cavity surface-emitting laser) for projecting the grid.</p><p id="p-0149" num="0148">With such a configuration, a three-dimensional scanning and registration of the object may be achieved, wherein a multipath effect is minimized, and wherein a geometrical photometrical (luminance) resolution is achieved.</p><p id="p-0150" num="0149">Thereby, an object recognition (e.g. face recognition) may be carried out efficiently.</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>6</b></figref> depicts a block diagram of a further embodiment of a time-of-flight imaging method <b>70</b>.</p><p id="p-0152" num="0151">In <b>71</b>, there is shown an image plane (e.g. of an image sensor) including a plurality of first image features <b>72</b> based on first image data and a plurality of second image features <b>73</b> based on second image data. The first and second image features <b>72</b> and <b>73</b> correspond to light spots, which are projected from a light source onto an object, wherein the light spots are captured by the image sensor and analyzed by a time-of-flight imaging circuitry, whereby the first and the second image features <b>72</b> and <b>73</b> are recognized.</p><p id="p-0153" num="0152">Thus, the first and second image features <b>72</b> and <b>73</b> of the light spots due to a motion of the light source based on a vibration, as discussed herein.</p><p id="p-0154" num="0153">In <b>74</b>, based on the first and second image features <b>72</b> and <b>73</b>, a global motion estimation of the second image features <b>73</b> with respect to the first image features <b>72</b> is performed and the second image features are aligned on the image plane <b>71</b> based on the estimated motion.</p><p id="p-0155" num="0154">In <b>75</b>, inpainting, interpolation, and filtering is performed for increasing a resolution and for filtering artifacts.</p><p id="p-0156" num="0155">In <b>76</b>, an image frame is output.</p><p id="p-0157" num="0156">The output image frame is then used as a reference image frame for a consecutive measurement, as discussed above.</p><p id="p-0158" num="0157"><figref idref="DRAWINGS">FIG. <b>7</b></figref> shows a method <b>80</b> for using a reference frame.</p><p id="p-0159" num="0158">There is shown a plurality of sub-frames <b>81</b> acquired at points of time t to t+5.</p><p id="p-0160" num="0159">For a first image frame, the first three sub-frames (t, t+1, and t+2) are taken into account, such that at a first output time T the first image frame (frame one) is output. For a second image frame, the third to sixth sub-frames (t+2, t+3, t+4, t+5) are taken into account, such that at a second output time T+1 the second image frame (frame two) is output taking frame one as a reference frame.</p><p id="p-0161" num="0160">It should be recognized that the embodiments describe methods with an exemplary ordering of method steps. The specific ordering of method steps is however given for illustrative purposes only and should not be construed as binding. For example, the ordering of <b>11</b> and <b>13</b> in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be exchanged. Also, the ordering of <b>12</b> and <b>14</b> in the embodiment of <figref idref="DRAWINGS">FIG. <b>2</b></figref> may be exchanged. Further, also the ordering of <b>29</b> and <b>31</b> in the embodiment of <figref idref="DRAWINGS">FIG. <b>3</b></figref> may be exchanged. Other changes of the ordering of method steps may be apparent to the skilled person.</p><p id="p-0162" num="0161">Please note that the division of the time-of-flight imaging system <b>60</b> into units <b>62</b> and <b>65</b> is only made for illustration purposes and that the present disclosure is not limited to any specific division of functions in specific units. For instance, the control circuitry <b>65</b> and the IMU <b>62</b> could be implemented by a respective programmed processor, field programmable gate array (FPGA) and the like.</p><p id="p-0163" num="0162">The methods can also be implemented as a computer program causing a computer and/or a processor, such as a time-of-flight imaging circuitry <b>4</b> discussed above, to perform the methods, when being carried out on the computer and/or processor. In some embodiments, also a non-transitory computer-readable recording medium is provided that stores therein a computer program product, which, when executed by a processor, such as the processor described above, causes the method described to be performed.</p><p id="p-0164" num="0163">All units and entities described in this specification and claimed in the appended claims can, if not stated otherwise, be implemented as integrated circuit logic, for example on a chip, and functionality provided by such units and entities can, if not stated otherwise, be implemented by software.</p><p id="p-0165" num="0164">In so far as the embodiments of the disclosure described above are implemented, at least in part, using software-controlled data processing apparatus, it will be appreciated that a computer program providing such software control and a transmission, storage or other medium by which such a computer program is provided are envisaged as aspects of the present disclosure.</p><p id="p-0166" num="0165">Note that the present technology can also be configured as described below.</p><p id="p-0167" num="0166">(1) A time-of-flight imaging circuitry configured to:<ul id="ul0001" list-style="none">    <li id="ul0001-0001" num="0000">    <ul id="ul0002" list-style="none">        <li id="ul0002-0001" num="0167">obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light;</li>        <li id="ul0002-0002" num="0168">determine a first image feature in the first image data;</li>        <li id="ul0002-0003" num="0169">obtain second image data from the image sensor, the second image data being indicative of the scene;</li>        <li id="ul0002-0004" num="0170">determine a second image feature in the second image data;</li>        <li id="ul0002-0005" num="0171">estimate a motion of the second image feature with respect to the first image feature; and</li>        <li id="ul0002-0006" num="0172">merge the first and the second image data based on the estimated motion.</li>    </ul>    </li></ul></p><p id="p-0168" num="0173">(2) The time-of-flight imaging circuitry of (1), wherein the motion is based on a vibration of at least one of the image sensor and a light source generating the spotted light.</p><p id="p-0169" num="0174">(3) The time-of-flight imaging circuitry of anyone of (1) and (2), further configured to carry out a triangulation including the first image feature and the second image feature for estimating the motion.</p><p id="p-0170" num="0175">(4) The time-of-flight imaging circuitry of anyone of (1) to (3), further configured to match the first image feature and the second image feature.</p><p id="p-0171" num="0176">(5) The time-of-flight imaging circuitry of anyone of (1) to (4), further configured to determine a first depth based on the match.</p><p id="p-0172" num="0177">(6) The time-of-flight imaging circuitry of (5), further configured to determine at least one third image feature based on third image data indicating a second depth.</p><p id="p-0173" num="0178">(7) The time-of-flight imaging circuitry of (6), further configured to determine a third depth based on the first and the second depth.</p><p id="p-0174" num="0179">(8) The time-of-flight imaging circuitry of (7), wherein the determination of the third depth is based on an interpolation including the first and the second depth.</p><p id="p-0175" num="0180">(9) The time-of-flight imaging circuitry of anyone of (1) to (8), further configured to temporally align the first and the second image data based on the estimated motion.</p><p id="p-0176" num="0181">(10) A time-of-flight imaging system, comprising:<ul id="ul0003" list-style="none">    <li id="ul0003-0001" num="0000">    <ul id="ul0004" list-style="none">        <li id="ul0004-0001" num="0182">a spotted light source configured to illuminate a scene with spotted light;</li>        <li id="ul0004-0002" num="0183">an image sensor; and</li>        <li id="ul0004-0003" num="0184">time-of-flight imaging circuitry configured to:</li>        <li id="ul0004-0004" num="0185">obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light;</li>        <li id="ul0004-0005" num="0186">determine a first image feature in the first image data;</li>        <li id="ul0004-0006" num="0187">obtain second image data from the image sensor, the second image data being indicative of the scene;</li>        <li id="ul0004-0007" num="0188">determine a second image feature in the second image data;</li>        <li id="ul0004-0008" num="0189">estimate a motion of the second image feature with respect to the first image feature; and</li>        <li id="ul0004-0009" num="0190">merge the first and the second image data based on the estimated motion.</li>    </ul>    </li></ul></p><p id="p-0177" num="0191">(11) The time-of-flight imaging system of (10), further comprising a vibration device configured to generate a vibration of the time-of-flight imaging system, wherein the vibration is indicative of the motion of the second image feature with respect to the first image feature.</p><p id="p-0178" num="0192">(12) A time-of-flight imaging method, comprising:<ul id="ul0005" list-style="none">    <li id="ul0005-0001" num="0000">    <ul id="ul0006" list-style="none">        <li id="ul0006-0001" num="0193">obtaining first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light;</li>        <li id="ul0006-0002" num="0194">determining a first image feature in the first image data;</li>        <li id="ul0006-0003" num="0195">obtaining second image data from the image sensor, the second image data being indicative of the scene;</li>        <li id="ul0006-0004" num="0196">determining a second image feature in the second image data;</li>        <li id="ul0006-0005" num="0197">estimating a motion of the second image feature with respect to the first image feature; and</li>        <li id="ul0006-0006" num="0198">merging the first and the second image data based on the estimated motion.</li>    </ul>    </li></ul></p><p id="p-0179" num="0199">(13) The time-of-flight imaging method of (12), wherein the motion is based on a vibration of at least one of the image sensor and a light source generating the spotted light.</p><p id="p-0180" num="0200">(14) The time-of-flight imaging method of anyone of (12) and (13), further comprising:<ul id="ul0007" list-style="none">    <li id="ul0007-0001" num="0000">    <ul id="ul0008" list-style="none">        <li id="ul0008-0001" num="0201">carrying out a triangulation including the first image feature and the second image feature for estimating the motion.</li>    </ul>    </li></ul></p><p id="p-0181" num="0202">(15) The time-of-flight imaging method of anyone of (12) to (14), further comprising:<ul id="ul0009" list-style="none">    <li id="ul0009-0001" num="0000">    <ul id="ul0010" list-style="none">        <li id="ul0010-0001" num="0203">matching the first image feature and the second image feature.</li>    </ul>    </li></ul></p><p id="p-0182" num="0204">(16) The time-of-flight imaging method of (15), further comprising:<ul id="ul0011" list-style="none">    <li id="ul0011-0001" num="0000">    <ul id="ul0012" list-style="none">        <li id="ul0012-0001" num="0205">determining a first depth based on the match.</li>    </ul>    </li></ul></p><p id="p-0183" num="0206">(17) The time-of-flight imaging method of (16), further comprising:<ul id="ul0013" list-style="none">    <li id="ul0013-0001" num="0000">    <ul id="ul0014" list-style="none">        <li id="ul0014-0001" num="0207">determining at least one third image feature based on third image data indicating a second depth.</li>    </ul>    </li></ul></p><p id="p-0184" num="0208">(18) The time-of-flight imaging method of (17), further comprising:<ul id="ul0015" list-style="none">    <li id="ul0015-0001" num="0000">    <ul id="ul0016" list-style="none">        <li id="ul0016-0001" num="0209">determining a third depth based on the first and the second depth.</li>    </ul>    </li></ul></p><p id="p-0185" num="0210">(19) The time-of-flight imaging method of (18), wherein the third depth is based on an interpolation including the first and the second depth.</p><p id="p-0186" num="0211">(20) The time-of-flight imaging method of anyone of (12) to (19), further comprising:<ul id="ul0017" list-style="none">    <li id="ul0017-0001" num="0000">    <ul id="ul0018" list-style="none">        <li id="ul0018-0001" num="0212">temporally aligning the first and the second image data based on the estimated motion. (21) A computer program comprising program code causing a computer to perform the method according to anyone of (11) to (20), when being carried out on a computer.</li>    </ul>    </li></ul></p><p id="p-0187" num="0213">(22) A non-transitory computer-readable recording medium that stores therein a computer program product, which, when executed by a processor, causes the method according to anyone of (11) to (20) to be performed.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A time-of-flight imaging circuitry configured to:<claim-text>obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light;</claim-text><claim-text>determine a first image feature in the first image data;</claim-text><claim-text>obtain second image data from the image sensor, the second image data being indicative of the scene;</claim-text><claim-text>determine a second image feature in the second image data;</claim-text><claim-text>estimate a motion of the second image feature with respect to the first image feature; and</claim-text><claim-text>merge the first and the second image data based on the estimated motion.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the motion is based on a vibration of at least one of the image sensor and a light source generating the spotted light.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further configured to carry out a triangulation including the first image feature and the second image feature for estimating the motion.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further configured to match the first image feature and the second image feature.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00004">claim 4</claim-ref>, further configured to determine a first depth based on the match.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00005">claim 5</claim-ref>, further configured to determine at least one third image feature based on third image data indicating a second depth.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00006">claim 6</claim-ref>, further configured to determine a third depth based on the first and the second depth.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the determination of the third depth is based on an interpolation including the first and the second depth.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The time-of-flight imaging circuitry of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further configured to temporally align the first and the second image data based on the estimated motion.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A time-of-flight imaging system, comprising:<claim-text>a spotted light source configured to illuminate a scene with spotted light;</claim-text><claim-text>an image sensor; and</claim-text><claim-text>time-of-flight imaging circuitry configured to:</claim-text><claim-text>obtain first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light;</claim-text><claim-text>determine a first image feature in the first image data;</claim-text><claim-text>obtain second image data from the image sensor, the second image data being indicative of the scene;</claim-text><claim-text>determine a second image feature in the second image data;</claim-text><claim-text>estimate a motion of the second image feature with respect to the first image feature; and</claim-text><claim-text>merge the first and the second image data based on the estimated motion.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The time-of-flight imaging system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, further comprising a vibration device configured to generate a vibration of the time-of-flight imaging system, wherein the vibration is indicative of the motion of the second image feature with respect to the first image feature.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. A time-of-flight imaging method, comprising:<claim-text>obtaining first image data from an image sensor, the first image data being indicative of a scene, which is illuminated with spotted light;</claim-text><claim-text>determining a first image feature in the first image data;</claim-text><claim-text>obtaining second image data from the image sensor, the second image data being indicative of the scene;</claim-text><claim-text>determining a second image feature in the second image data;</claim-text><claim-text>estimating a motion of the second image feature with respect to the first image feature; and</claim-text><claim-text>merging the first and the second image data based on the estimated motion.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the motion is based on a vibration of at least one of the image sensor and a light source generating the spotted light.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>carrying out a triangulation including the first image feature and the second image feature for estimating the motion.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>matching the first image feature and the second image feature.</claim-text></claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00015">claim 15</claim-ref>, further comprising:<claim-text>determining a first depth based on the match.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00016">claim 16</claim-ref>, further comprising:<claim-text>determining at least one third image feature based on third image data indicating a second depth.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00017">claim 17</claim-ref>, further comprising:<claim-text>determining a third depth based on the first and the second depth.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00018">claim 18</claim-ref>, wherein the third depth is based on an interpolation including the first and the second depth.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The time-of-flight imaging method of <claim-ref idref="CLM-00012">claim 12</claim-ref>, further comprising:<claim-text>temporally aligning the first and the second image data based on the estimated motion.</claim-text></claim-text></claim></claims></us-patent-application>