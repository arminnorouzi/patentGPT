<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230005505A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230005505</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17853384</doc-number><date>20220629</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>IN</country><doc-number>202141029538</doc-number><date>20210630</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>11</class><subclass>B</subclass><main-group>27</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>11</class><subclass>B</subclass><main-group>27</main-group><subgroup>005</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>49</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20220101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>V</subclass><main-group>20</main-group><subgroup>41</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">METHOD AND ELECTRONIC DEVICE FOR A SLOW MOTION VIDEO</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/KR2022/005741</doc-number><date>20220421</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17853384</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><address><city>Suwon-si</city><country>KR</country></address></addressbook><residence><country>KR</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SHUKLA</last-name><first-name>Vishwajeet</first-name><address><city>Noida</city><country>IN</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Meena</last-name><first-name>Manisha</first-name><address><city>Ghaziabad</city><country>IN</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Singour</last-name><first-name>Mayank</first-name><address><city>Mandla</city><country>IN</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Pandita</last-name><first-name>Ishan</first-name><address><city>Ghaziabad</city><country>IN</country></address></addressbook></inventor></inventors></us-parties><assignees><assignee><addressbook><orgname>SAMSUNG ELECTRONICS CO., LTD.</orgname><role>03</role><address><city>Suwon-si</city><country>KR</country></address></addressbook></assignee></assignees></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A method for generating a slow motion video. The method includes segmenting, by an electronic device, objects in the video. Further, the method includes determining, by the electronic device, an interaction between the segmented objects. Further, the method includes clustering, by the electronic device, the segmented objects in the video to generate object clusters based on the interaction. Further, the method includes determining, by the electronic device, a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters. Further, the method includes generating, by the electronic device, the slow motion video by applying the degree of slow motion effect to that has been determined to corresponding the object clusters.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="107.44mm" wi="158.75mm" file="US20230005505A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="69.85mm" wi="139.87mm" file="US20230005505A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="142.16mm" wi="120.73mm" file="US20230005505A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="74.34mm" wi="106.34mm" file="US20230005505A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="74.34mm" wi="106.34mm" file="US20230005505A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="172.72mm" wi="106.34mm" file="US20230005505A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="126.24mm" wi="105.41mm" file="US20230005505A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="196.68mm" wi="139.78mm" orientation="landscape" file="US20230005505A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="187.11mm" wi="139.70mm" orientation="landscape" file="US20230005505A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="102.62mm" wi="124.21mm" file="US20230005505A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="196.34mm" wi="79.93mm" orientation="landscape" file="US20230005505A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="57.23mm" wi="118.62mm" file="US20230005505A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="137.58mm" wi="132.59mm" file="US20230005505A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="57.23mm" wi="92.37mm" file="US20230005505A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="186.27mm" wi="125.05mm" file="US20230005505A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="197.19mm" wi="137.16mm" orientation="landscape" file="US20230005505A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="88.14mm" wi="133.18mm" file="US20230005505A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="66.21mm" wi="82.97mm" file="US20230005505A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="109.47mm" wi="135.97mm" file="US20230005505A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="154.86mm" wi="148.25mm" file="US20230005505A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="104.06mm" wi="142.07mm" file="US20230005505A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="196.51mm" wi="122.94mm" orientation="landscape" file="US20230005505A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="196.43mm" wi="122.94mm" orientation="landscape" file="US20230005505A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="196.43mm" wi="129.71mm" orientation="landscape" file="US20230005505A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00024" num="00024"><img id="EMI-D00024" he="196.43mm" wi="129.71mm" orientation="landscape" file="US20230005505A1-20230105-D00024.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00025" num="00025"><img id="EMI-D00025" he="206.59mm" wi="128.27mm" file="US20230005505A1-20230105-D00025.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00026" num="00026"><img id="EMI-D00026" he="197.19mm" wi="94.32mm" orientation="landscape" file="US20230005505A1-20230105-D00026.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00027" num="00027"><img id="EMI-D00027" he="67.99mm" wi="132.76mm" file="US20230005505A1-20230105-D00027.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00028" num="00028"><img id="EMI-D00028" he="167.05mm" wi="125.65mm" file="US20230005505A1-20230105-D00028.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00029" num="00029"><img id="EMI-D00029" he="196.43mm" wi="81.11mm" orientation="landscape" file="US20230005505A1-20230105-D00029.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00030" num="00030"><img id="EMI-D00030" he="182.80mm" wi="131.91mm" file="US20230005505A1-20230105-D00030.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00031" num="00031"><img id="EMI-D00031" he="171.03mm" wi="134.87mm" file="US20230005505A1-20230105-D00031.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00032" num="00032"><img id="EMI-D00032" he="195.92mm" wi="135.21mm" file="US20230005505A1-20230105-D00032.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00033" num="00033"><img id="EMI-D00033" he="181.36mm" wi="134.62mm" file="US20230005505A1-20230105-D00033.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a continuation of International Application No. PCT/KR2022/005741 designating the United States, filed on Apr. 21, 2022, in the Korean Intellectual Property Receiving Office and claiming priority to Indian Complete Patent Application No. 202141029538, filed on Jun. 30, 2021 in the Indian Patent Office, the disclosures of all of which are incorporated by reference herein in their entireties.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><heading id="h-0003" level="1">1. Field</heading><p id="p-0003" num="0002">The present disclosure relates to image processing techniques, and more specifically to a method and an electronic device for generating multiple sectional slow motion effects in a video.</p><heading id="h-0004" level="1">2. Description of Related Art</heading><p id="p-0004" num="0003">A slow motion (SloMo) effect is becoming more and more essential in smartphones, with a recent increase in demand for improvised video clips. A slow motion effect is an image processing technique which is used to make a moving object look virtually moving slower for analyzing minute changes in a motion of the object. Related art slow motion effect techniques record a slow motion video at a uniform frame rate. Due to recording the slow motion video at the uniform frame rate, a slow moving object in the slow motion video will become more slower, whereas the slow motion effect is useful to analyze fast moving objects. Therefore, the uniform frame rate of the slow motion video causes a disadvantage in that a flow of a section of a video frame containing the fast moving object visually pleasing, but the same might not happen for other sections containing the slow moving object. In other words, the slowed fast moving object becomes more visually pleasing while the slowed slow moving object becomes less visually pleasing.</p><p id="p-0005" num="0004"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates a video frame including a car and a human as objects in the video, where the human is walking slowly relative to the car and the car is moving quickly relative to the human. The video of the human and the car is recorded at the uniform frame rate of 480 frames per second (fps) using related art slow motion techniques. Therefore, the motion of the human appears to be relatively stagnant/unmoving in the slow motion video which hinders a cinematic experience of a user and is not desirable. Thus, it would be advantageous to provide a method for generating the slow motion video by resolving the aforementioned disadvantages in related art image processing methods.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0006" num="0005">An object of the embodiments herein is to provide a method and an electronic device for generating multiple sectional slow motion effects in a video. The provided method may adaptively apply variable frame rate interpolation in different portions of the video automatically and may introduce a multiple sectional slow motion effect to the video and a cinematographic experience in the video, which improves a visual experience of a user.</p><p id="p-0007" num="0006">Another object of the embodiments herein is to dynamically determine whether a sectional slow motion video recording is to be used for a scene based on a significance score of objects present in the scene.</p><p id="p-0008" num="0007">Another object of the embodiments herein is to replace a moving or still background in video frames with a more cinematically pleasing background at different sectional frame rates for an improved visual experience of the user.</p><p id="p-0009" num="0008">According to an aspect of one or more embodiments, there is provided a method for generating slow motion video. The method includes segmenting, by an electronic device, objects in the video. The method further includes determining, by the electronic device, an interaction between the segmented objects. The method further includes clustering, by the electronic device, the segmented objects in the video to generate object clusters based on the interactions. The method further includes determining, by the electronic device, a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters. The method further includes generating, by the electronic device, the slow motion video by applying the degree of slow motion effect to that has been determined to corresponding the object clusters.</p><p id="p-0010" num="0009">According to another aspect of one or more embodiments, there is provided an electronic device for generating a slow motion video. The electronic device includes a memory, a processor, and a sectional slow motion controller, coupled to the memory and the processor. The sectional slow motion controller is configured to perform operations comprising segmenting objects in the video, determining an interaction between the segmented objects, clustering the segmented objects in the video to generate object clusters based on the interaction, determining a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters, generating the slow motion video by applying the degree of slow motion effect that has been determined to corresponding the object clusters.</p><p id="p-0011" num="0010">In an embodiment, wherein the segmenting of the objects comprises determining a class confidence score of each video frame of the video using a Machine Learning (ML) model, the class confidence score being a quantitative measurement of a quality of content in the video frame, filtering the video frames based on the class confidence scores, identifying by the electronic device (<b>100</b>) the objects in the filtered video frames, estimating a depth score of each of the identified objects in the filtered video frames based on depth information in the filtered video frames, determining an optical flow rate of each of the identified objects in the filtered video frames, the optical flow rate being a quantitative measurement of a velocity of movement of pixels corresponding to the identified objects in the filtered video frames and generating the segmented objects from the filtered video frames by applying the optical flow rate and the depth score of each identified object to a semantic segmentation network.</p><p id="p-0012" num="0011">In an embodiment, wherein the determining of the interaction comprises determining the interaction between the segmented objects for a time duration by applying the optical flow rate and the depth score of each of the segmented objects to a memory based neural network.</p><p id="p-0013" num="0012">In an embodiment, wherein the determining of the degree of slow motion effect comprises determining the significance score of each of the object clusters, the significance score being a measure of a relative importance of each of the object clusters in the video and the determining the degree of slow motion effect for each of the objects based on the corresponding the significance score for the object clusters.</p><p id="p-0014" num="0013">In an embodiment, wherein the determining of the significance score comprises determining a cluster flow rate (&#x3b1;) of each of the object clusters based on an optical flow rate of each segmented object in the corresponding object clusters, generating a hash table of a proximity change coefficient (&#x3b2;) based on the change in clustering of the segmented objects at the different instants of time in the video, the proximity change coefficient (&#x3b2;) being a number of times that the segmented object changes into and out of the object clusters within a time duration, and determining the significance score of each of the object clusters based on based on at least one of the cluster flow rate (&#x3b1;) and the proximity change coefficient (&#x3b2;).</p><p id="p-0015" num="0014">In an embodiment, wherein the determining of the significance score comprises estimating, by the electronic device (<b>100</b>), a rigidness score of each of the segmented objects based on at least one of a relative frequency of the segmented objects occurring in the video, a perceptual aesthetic quality of the segmented objects and a degree of movement of the segmented objects based on the object classes of the segmented objects, the rigidness score being a measure of static aspects of an affinity for slow motion of the segmented object, and determining the significance score of each of the object clusters based on the rigidness score.</p><p id="p-0016" num="0015">In an embodiment, wherein the generating of the slow motion video comprises determining an interpolation rate for each of a plurality of sections in the video frames that correspond to the object clusters, based on the significance score of the object cluster corresponding to the section and interpolating the object clusters in each video frame based on the interpolation rate of the corresponding sections.</p><p id="p-0017" num="0016">In an embodiment, wherein the determining of the interaction comprises categorizing the segmented objects to one of an interacting class in response to determining the interaction between the segmented objects, and a non-interacting class in response to not determining the interaction between the segmented objects.</p><p id="p-0018" num="0017">In an embodiment, the memory based neural network outputs a probability score indicating whether the segmented objects are interacting.</p><p id="p-0019" num="0018">In an embodiment, the rigidness score is estimated using a weighted combination of (a) the relative frequency of the segmented objects occurring in the video, (b) the perceptual aesthetic quality of the segmented objects, and (c) the degree of movement of the segmented objects based on the object classes of the segmented objects, weights thereof being determined using regression analysis.</p><p id="p-0020" num="0019">In an embodiment, (b) the perceptual aesthetic quality of the segmented objects is determined as a probability using a neural network that is trained on a dataset of images labeled with a score of aesthetic values between 0 and 1.</p><p id="p-0021" num="0020">According to another aspect of one or more embodiments, there is provided a computer-readable storage medium, having a computer program stored thereon that performs, when executed by a processor, the above method.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0022" num="0021">The embodiments herein will be better understood from the following description with reference to the drawings, in which:</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> illustrates a video frame of a video that includes a car and a human as objects in the video, according to the related art;</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates a comparison of a slow motion video created using an related art method and a method according to an embodiment;</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a block diagram of an electronic device for generating multiple sectional slow motion effects in a video, according to an embodiment;</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a block diagram of an electronic device for generating multiple sectional slow motion effects in a video, according to an embodiment;</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is a block diagram of a sectional slow motion controller of the electronic device for generating a slow motion video by applying different degree of slow motion effect to different objects in the video, according to an embodiment;</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram illustrating a method for generating the multiple sectional slow motion effects in the video, according to an embodiment;</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example scenario of generating the multiple sectional slow motion effects in the video, according to an embodiment;</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a flow diagram illustrating a method of segmenting objects in the video, according to an embodiment;</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates an example scenario of determining a depth score of objects in a video frame, according to an embodiment;</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram illustrating a method for determining an interaction between the segmented objects, according to an embodiment;</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a flow diagram illustrating a method for generating a hash table of a proximity change coefficient, according to an embodiment;</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates an example scenario of generating the hash table of the proximity change coefficient, according to an embodiment;</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a flow diagram illustrating a method for estimating a rigidness score of an object, according to an embodiment;</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates an example scenario of different type of objects in different video frames, according to an embodiment;</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> illustrates an example scenario of determining linear coefficients &#x3bb;1, &#x3bb;2 according to an embodiment;</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow diagram illustrating a method for determining a significance score of an object cluster, according to an embodiment;</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a flow diagram illustrating a method for generating the multi sectional slow motion video, according to an embodiment;</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates an example scenario of generating the multi sectional slow motion video, according to an embodiment;</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>12</b>D</figref> illustrate a comparison of a related art method of interpolation and a masked interpolation in the method according to an embodiment;</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>13</b>A</figref> illustrates an example scenario of generating interpolation segments corresponding to an object, according to an embodiment;</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrates an example scenario of generating sectional slow motion clips, according to an embodiment;</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIGS. <b>14</b>-<b>15</b>B</figref> illustrate example scenarios for explaining inconsistency in interpolation, according to an embodiment;</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example scenario of generating the multiple sectional slow motion effects in the video, according to an embodiment;</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example scenario of generating the multiple sectional slow motion effects for a specific duration in the video, according to an embodiment;</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an example scenario of selecting an animated clip based on a maximum significance score for generating the multiple sectional slow motion effects for in the video, according to an embodiment; and</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates an example scenario of early triggering for recording a sectional slow motion video using a multi-camera module, according to an embodiment.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0049" num="0048">The above and other objects of the embodiments herein will be better appreciated and understood when considered in conjunction with the description and the accompanying drawings. It should be understood, however, that the following descriptions, while indicating various embodiments and numerous specific details thereof, are given by way of illustration and not of limitation. Many changes and modifications may be made within the scope of the embodiments herein without departing from the spirit thereof, and the embodiments herein include all such modifications.</p><p id="p-0050" num="0049">The embodiments herein and the various features and advantageous details thereof are explained more fully with reference to the non-limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. Descriptions of well-known components and processing techniques are omitted so as to not unnecessarily obscure the embodiments herein. Also, the various embodiments described herein are not necessarily mutually exclusive, as some embodiments may be combined with one or more other embodiments to form new embodiments. The term &#x201c;or&#x201d; as used herein, refers to a non-exclusive or, unless otherwise indicated. The examples used herein are intended merely to facilitate an understanding of ways in which the embodiments herein may be practiced and to further enable those skilled in the art to practice the embodiments herein. Accordingly, the examples should not be construed as limiting the scope of the embodiments herein.</p><p id="p-0051" num="0050">As is traditional in the field, embodiments may be described and illustrated in terms of blocks which carry out a described function or functions. These blocks, which may be referred to herein as managers, units, modules, hardware components or the like, are physically implemented by analog and/or digital circuits such as logic gates, integrated circuits, microprocessors, microcontrollers, memory circuits, passive electronic components, active electronic components, optical components, hardwired circuits and the like, and may, in some cases, be driven by firmware. The circuits may, for example, be embodied in one or more semiconductor chips, or on substrate supports such as printed circuit boards and the like. The circuits constituting a block may be implemented by dedicated hardware, or by a processor (e.g., one or more programmed microprocessors and associated circuitry), or by a combination of dedicated hardware to perform some functions of the block and a processor to perform other functions of the block. Each block of the embodiments may be physically separated into two or more interacting and discrete blocks without departing from the scope of the disclosure. Likewise, the blocks of the embodiments may be physically combined into more complex blocks without departing from the scope of the disclosure.</p><p id="p-0052" num="0051">The accompanying drawings are used to help easily understand various technical features and it should be understood that the embodiments presented herein are not limited by the accompanying drawings. As such, the present disclosure should be construed to extend to any alterations, equivalents and substitutes in addition to those which are particularly set out in the accompanying drawings. Although the terms &#x201c;first&#x201d;, &#x201c;second&#x201d;, etc. may be used herein to describe various elements, these elements should not be limited by these terms. These terms are generally only used to distinguish one element from another. In the accompanying drawings, like reference letters indicate corresponding parts in the various figures.</p><p id="p-0053" num="0052">Various embodiments disclosed herein provide a method for generating multiple sectional slow motion effects in a video. In other words, different sections of a same video may have different slow motion effects occurring at the same time. The method includes segmenting, by an electronic device, objects in the video. Further, the method includes determining, by the electronic device, an interaction between the segmented objects. Further, the method includes determining, by the electronic device, a degree of slow motion effect used for each segmented object in the video based on the interaction between the segmented objects and at least one parameter associated with an importance of a scene in the video. Further, the method includes generating, by the electronic device, a slow motion video by applying the determined degree of slow motion effect to corresponding segmented object. Further, the method includes storing, by the electronic device, the slow motion video.</p><p id="p-0054" num="0053"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> illustrates a comparison of a slow motion video created using a related art method and a method according to an embodiment. As shown in (<b>201</b>), consider a scene of two boys, one on a skateboard and one jumping rope, where, in the scene, a motion of the first boy on the skateboard (<b>202</b>) is slower than a motion of a second boy jumping rope (<b>203</b>). If a slow motion video of the scene is recorded using the related art method, every object (i.e., the boys, the skateboard, the jump rope) in the scene (<b>201</b>) are recorded in slow motion at a same video frame interpolation rate, and hence a cinematic effect will be missing in the slow motion recording of the scene. By contrast, as shown in (<b>204</b>), unlike related art methods and systems, the method according to an embodiment adaptively applies a variable frame rate interpolation (e.g. 2&#xd7; and 4&#xd7;) in different portions (i.e. a portion <b>202</b> of the boy on the skateboard, a portion <b>203</b> of the boy jumping rope) of the video automatically and introduces a sectional slow motion effect and a cinematographic effect in the video, which improves a visual experience of a user in watching the slow motion video.</p><p id="p-0055" num="0054">Unlike related art methods and systems, the method according to an embodiment may be used to dynamically determine whether a sectional slow motion video recording is to be used for a scene based on a significance score of objects present in the scene. Therefore, no user intervention is required for triggering recording of the sectional slow motion video which improves the visual experience of the videos.</p><p id="p-0056" num="0055">Unlike related art methods and systems, the method according to an embodiment may be used to replace a moving or still background in video frames with a more cinematically pleasing background at different sectional frame rates for an improved visual experience of the user in watching the slow motion videos.</p><p id="p-0057" num="0056">Referring now to the drawings, and more particularly to <figref idref="DRAWINGS">FIGS. <b>2</b>A through <b>19</b></figref>, there are shown various embodiments.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>2</b>A</figref> is a block diagram of an electronic device (<b>100</b>) for generating multiple sectional slow motion effects in a video, according to an embodiment. Examples of the electronic device (<b>100</b>) include, but are not limited to a smart phone, a tablet computer, a personal digital assistance (PDA), a desktop computer, an camera device, a wearable device, a multimedia device, etc. In an embodiment, the electronic device (<b>100</b>) may include a sectional slow motion controller (<b>110</b>), a memory (<b>120</b>), and a processor (<b>130</b>).</p><p id="p-0059" num="0058"><figref idref="DRAWINGS">FIG. <b>2</b>B</figref> is a block diagram of an electronic device (<b>100</b>) for generating multiple sectional slow motion effects in a video, according to an embodiment. In an embodiment, the electronic device (<b>100</b>) may include the sectional slow motion controller (<b>110</b>), the memory (<b>120</b>), the processor (<b>130</b>), a communicator (<b>140</b>).</p><p id="p-0060" num="0059">In some embodiments, the sectional slow motion controller (<b>110</b>), the memory (<b>120</b>), and the processor (<b>130</b>), and the communicator (<b>140</b>) may be communicatively connected by a bus. In another embodiment, the electronic device (<b>100</b>) may include the sectional slow motion controller (<b>110</b>), the memory (<b>120</b>), the processor (<b>130</b>), the communicator (<b>140</b>), and a multi-camera module (<b>150</b>), where the multi-camera module (<b>150</b>) may include a normal angle camera and a wide angle camera. In some embodiments, the sectional slow motion controller (<b>110</b>), the memory (<b>120</b>), the processor (<b>130</b>), the communicator (<b>140</b>) and the multi-camera module (<b>150</b>) may be communicatively connected by the bus.</p><p id="p-0061" num="0060">The sectional slow motion controller (<b>110</b>) may be implemented by processing circuitry such as logic gates, integrated circuits, microprocessors, microcontrollers, memory circuits, passive electronic components, active electronic components, optical components, hardwired circuits, and/or the like, and may in some cases be driven by firmware. The circuits may, for example, be embodied in one or more semiconductor chips, or on substrate supports such as printed circuit boards and the like.</p><p id="p-0062" num="0061">The sectional slow motion controller (<b>110</b>) is configured to receive a video. In an embodiment, the video may be a live video obtaining from a camera sensor or the multi-camera module or from an external server. In another embodiment, the video may be a recorded video obtained from the memory (<b>120</b>) or the external server. The sectional slow motion controller (<b>110</b>) is configured to segment objects in the video. In an embodiment, the sectional slow motion controller (<b>110</b>) is configured to decode the video and obtain video frames from the video. Further, the sectional slow motion controller (<b>110</b>) is configured to determine a class confidence score of each video frame using a Machine Learning (ML) model (e.g., one or more pre-trained convolution neural networks (CNNs)) by taking the video frames as an input, and outputting a probability with which the electronic device (<b>100</b>) is able to detect and discriminate the objects in the video frames. The probability corresponds to the class confidence score. The class confidence score may be a quantitative measurement of a quality of content in the video frame.</p><p id="p-0063" num="0062">The sectional slow motion controller (<b>110</b>) is configured filter the video frames with clear visibility (i.e., video frames that have discernable objects) from the video based on the class confidence score. The sectional slow motion controller (<b>110</b>) is configured to identify the objects in the filtered video frames. The sectional slow motion controller (<b>110</b>) is configured calculate depth information of each object in the filtered video frames, based on the video data of the video frames, and to estimate a depth score of each object from the filtered video frames based on the depth information of each object in the filtered video frames. The sectional slow motion controller (<b>110</b>) is configured to determine an optical flow rate of each object from the filtered video frames, where the optical flow rate of each object is a quantitative measurement of a velocity of movement of pixels corresponding to the object in the video. The sectional slow motion controller (<b>110</b>) is configured to generate the segmented object from the filtered video frames by applying the optical flow rate and the depth score of each object to a semantic segmentation network.</p><p id="p-0064" num="0063">The sectional slow motion controller (<b>110</b>) is configured to determine an interaction between the segmented objects. In an embodiment, the sectional slow motion controller (<b>110</b>) is configured to determine the interaction between the segmented objects for a time duration by applying the optical flow rate of each segmented object from the filtered video frames and the depth score of each segmented object to a memory based neural network (e.g., Long Short-Term Memory cells (LSTM)).</p><p id="p-0065" num="0064">The sectional slow motion controller (<b>110</b>) is configured to determine a degree (i.e., an amount) of slow motion effect to be used for each segmented object in the video based on the interaction between the segmented objects and at least one parameter associated with an importance of a scene in the video. In an embodiment, the sectional slow motion controller (<b>110</b>) is configured to cluster the segmented objects in each video frame of the video to form object clusters based on the interaction between the segmented objects. The sectional slow motion controller (<b>110</b>) is configured to determine a significance score of each object cluster based on a change in clustering of the segmented objects at different instants of time in the video and based on the at least one parameter, where the significance score of each object cluster is a measure of relative importance assigned to each object cluster in the video. The sectional slow motion controller (<b>110</b>) is configured to interpolate each object cluster based on the significance score of each object cluster to apply the determined degree of slow motion effect to the corresponding segmented object.</p><p id="p-0066" num="0065">In an embodiment, the sectional slow motion controller (<b>110</b>) is configured to determine a cluster flow rate (&#x3b1;), where the cluster flow rate is an average optical flow rate of each segmented object in the object cluster. The at least one parameter may include the cluster flow rate (&#x3b1;). The sectional slow motion controller (<b>110</b>) is configured to generate a hash table of a proximity change coefficient (&#x3b2;) using the change in clustering of the segmented objects at different instants of time in the video, where the proximity change coefficient is the number of times a segmented object changes from the object cluster within a time duration. The at least one parameter may include the proximity change coefficient (&#x3b2;). The sectional slow motion controller (<b>110</b>) is configured to estimate a rigidness score of the segmented object based on one or more of (a) a relative frequency of the segmented object occurring in the video and (b) a perceptual aesthetic quality of the video frame, where the rigidness score of the segmented object is a measure of static aspects of segmented object's affinity for slow motion. The sectional slow motion controller (<b>110</b>) is configured to determine the significance score of each object cluster based on the rigidness score, and based on the cluster flow rate (&#x3b1;) and/or the proximity change coefficient (&#x3b2;) of the segmented objects included in the object cluster.</p><p id="p-0067" num="0066">The sectional slow motion controller (<b>110</b>) is configured to determine a degree of slow motion effect corresponding to each object cluster based on the significance score of the object cluster, and to generate a slow motion video by applying the determined degree of slow motion effect to corresponding segmented objects of the object cluster. In an embodiment, the sectional slow motion controller (<b>110</b>) is configured to determine an interpolation rate of each section in the video frames based on the significance score of the object cluster in the section. The interpolation rate of each section in the video frames is called a sectional interpolation rate. The sectional slow motion controller (<b>110</b>) is configured to interpolate the object cluster in each video frame based on the interpolation rate. Further, the sectional slow motion controller (<b>110</b>) is configured to store the slow motion video.</p><p id="p-0068" num="0067">Clustering helps to group the objects together which are very frequently interacting in the video. Clustering helps to reduce inconsistency in the video. Clustering ensures a region in the video that has groups of objects which are frequently interacting will be slow motioned at a same rate. Clustering may be used to set a boundary on video where a slow motion effect is to be used.</p><p id="p-0069" num="0068">The memory (<b>120</b>) may store the slow motion video with multiple sectional slow motion effects as described above, a video with uniform frame rate, the hash table, a class confidence score threshold, and a wide angle video. The memory (<b>120</b>) stores instructions to be executed by the processor (<b>130</b>). The memory (<b>120</b>) may include non-volatile storage elements. Examples of such non-volatile storage elements may include magnetic hard discs, optical discs, floppy discs, flash memories, or forms of electrically programmable memories (EPROM) or electrically erasable and programmable (EEPROM) memories. In addition, the memory (<b>120</b>) may, in some examples, be considered a non-transitory storage medium. The term &#x201c;non-transitory&#x201d; may indicate that the storage medium is not embodied in a carrier wave or a propagated signal. However, the term &#x201c;non-transitory&#x201d; should not be interpreted that the memory (<b>120</b>) is non-movable. In some examples, the memory (<b>120</b>) may be configured to store larger amounts of information than a storage space provided in the memory (<b>120</b>). In certain examples, a non-transitory storage medium may store data that may, over time, change (e.g., in Random Access Memory (RAM) or cache). The memory (<b>120</b>) may be an internal storage or the memory (<b>120</b>) may be an external storage of the electronic device (<b>100</b>), a cloud storage, or any other type of external storage.</p><p id="p-0070" num="0069">The processor (<b>130</b>) is configured to access and execute instructions stored in the memory (<b>120</b>). The processor (<b>130</b>) may be a general-purpose processor, such as a Central Processing Unit (CPU), an Application Processor (AP), or the like, a graphics-only processing unit such as a Graphics Processing Unit (GPU), a Visual Processing Unit (VPU) and the like. The processor (<b>130</b>) may include multiple cores to execute the instructions.</p><p id="p-0071" num="0070">The communicator (<b>140</b>) is configured to communicate internally between hardware components in the electronic device (<b>100</b>). Further, the communicator (<b>140</b>) is configured to facilitate the communication between the electronic device (<b>100</b>) and other devices via one or more networks (e.g. Radio technology). The communicator (<b>140</b>) includes an electronic circuit specific to a standard that enables wired or wireless communication.</p><p id="p-0072" num="0071">At least one of the various functions of the sectional slow motion controller (<b>110</b>) described above may be implemented through a machine learning (ML) model. A function associated with the ML model may be performed through the memory (<b>120</b>) and the processor (<b>130</b>), for example by the non-volatile memory, the volatile memory, and the processor (<b>130</b>).</p><p id="p-0073" num="0072">The processor (<b>130</b>) may include one or a plurality of processors. The one or more processors may be a general purpose processor, such as a Central Processing Unit (CPU), an Application Processor (AP), or the like, a graphics-only processing unit such as a Graphics Processing Unit (GPU), a Visual Processing Unit (VPU), and/or an AI-dedicated processor such as a Neural Processing Unit (NPU).</p><p id="p-0074" num="0073">The one or a plurality of processors control the processing of the input data in accordance with a predefined operating rule or ML model stored in the memory (<b>120</b>), e.g., the non-volatile memory and the volatile memory. The predefined operating rule or artificial intelligence model is provided through training or learning.</p><p id="p-0075" num="0074">Here, being provided through learning denotes that, by applying a learning technique to a plurality of learning data, a predefined operating rule or ML model of a desired characteristic is made. The learning may be performed in a device itself in which the ML model according to an embodiment is performed, and/or may be implemented through a separate server/system.</p><p id="p-0076" num="0075">The ML model may consist of a plurality of neural network layers. Each layer has a plurality of weight values, and performs a layer operation through calculation of a previous layer and an operation of a plurality of weights. Examples of neural networks include, but are not limited to, convolutional neural network (CNN), attention based network, deep neural network (DNN), recurrent neural network (RNN), restricted Boltzmann Machine (RBM), deep belief network (DBN), bidirectional recurrent deep neural network (BRDNN), generative adversarial networks (GAN), and deep Q-networks.</p><p id="p-0077" num="0076">The learning technique is a method for training a predetermined target device (for example, a robot) using a plurality of learning data to cause, allow, or control the target device to make a determination or prediction. Examples of learning techniques include, but are not limited to, supervised learning, unsupervised learning, semi-supervised learning, or reinforcement learning.</p><p id="p-0078" num="0077">Although the <figref idref="DRAWINGS">FIG. <b>2</b>A</figref> shows the hardware components of the electronic device (<b>100</b>), it is to be understood that other embodiments are not limited thereon. In other embodiments, the electronic device (<b>100</b>) may include fewer or more number of components. Further, the labels or names of the components are used only for illustrative purpose and do not limit the scope of the disclosure. One or more components may be combined together to perform a same or substantially similar function for generating the multiple sectional slow motion effects in the video.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>2</b>C</figref> is a block diagram of the sectional slow motion controller (<b>110</b>) of the electronic device (<b>100</b>) for generating the slow motion video by applying different degree of slow motion effect to different objects in the video, according to an embodiment. In an embodiment, the sectional slow motion controller (<b>110</b>) may include a frame filtering engine (<b>111</b>), a segmentation engine (<b>112</b>), a rigidness score estimator (<b>113</b>), an interaction detector (<b>114</b>), a cluster generator (<b>115</b>), a temporal proximity change detector (<b>116</b>), a significance score determiner (<b>117</b>), and a masked interpolation engine (<b>118</b>).</p><p id="p-0080" num="0079">The frame filtering engine (<b>111</b>), the segmentation engine (<b>112</b>), the rigidness score estimator (<b>113</b>), the interaction detector (<b>114</b>), the cluster generator (<b>115</b>), the temporal proximity change detector (<b>116</b>), the significance score determiner (<b>117</b>), and the masked interpolation engine (<b>118</b>) are implemented by processing circuitry such as logic gates, integrated circuits, microprocessors, microcontrollers, memory circuits, passive electronic components, active electronic components, optical components, hardwired circuits, or the like, and may, in some cases, be driven by firmware. The circuits may, for example, be embodied in one or more semiconductor chips, or on substrate supports such as printed circuit boards and the like.</p><p id="p-0081" num="0080">The frame filtering engine (<b>111</b>) receives the video and obtains the video frames by decoding the video. The segmentation engine (<b>112</b>) segments the objects in the video. In an embodiment, the frame filtering engine (<b>111</b>) determines the class confidence score of each video frame using a class confidence ML model. The frame filtering engine (<b>111</b>) filters the video frames with clear visibility (i.e., video frames that have discernable objects) from the video based on the class confidence score. The frame filtering engine (<b>111</b>) identifies the objects in the filtered video frames. The frame filtering engine (<b>111</b>) calculates depth information for each object in the filtered video frames, based on the video data of the video frames, and estimates the depth score of each object from the filtered video frames based on the depth information of each object in the filtered video frames. The frame filtering engine (<b>111</b>) determines the optical flow rate of each object from the filtered video frames, where the optical flow rate of each object is a quantitative measurement of a velocity of movement of pixels corresponding to the object in the video. The segmentation engine (<b>112</b>) generates the segmented object from the filtered video frames by applying the optical flow rate and the depth score of each object to the semantic segmentation network.</p><p id="p-0082" num="0081">The interaction detector (<b>114</b>) determines an interaction between the segmented objects. In an embodiment, the interaction detector (<b>114</b>) determines the interaction between the segmented objects for a time duration by applying the optical flow rate of each segmented object from the filtered video frames and the depth score of each segmented object to a memory based neural network.</p><p id="p-0083" num="0082">The significance score determiner (<b>117</b>) determines the degree of slow motion effect to be used for each segmented object in the video based on the interaction between the segmented objects and at least one parameter associated with the scene importance in the video. In an embodiment, the cluster generator (<b>115</b>) clusters the segmented objects in each video frame of the video to form the object clusters based on the interaction between the segmented objects. The significance score determiner (<b>117</b>) determines the significance score of each object cluster based on the change in clustering of the segmented objects at the different instants of time in the video and based on the at least one parameter. The masked interpolation engine (<b>118</b>) interpolates each object cluster based on the significance score of each object cluster to apply the determined degree of slow motion effect to the corresponding segmented object.</p><p id="p-0084" num="0083">In an embodiment, the temporal proximity change detector (<b>116</b>) determines the cluster flow rate (&#x3b1;). The temporal proximity change detector (<b>116</b>) receives an indication on whether two objects or object clusters in the video are interacting or not. Further, the temporal proximity change detector (<b>116</b>) generates the hash table of the proximity change coefficient (&#x3b2;) using the change in clustering of the segmented objects at different instants of time in the video. In another embodiment, the cluster generator (<b>115</b>) determines the cluster flow rate (&#x3b1;) from the object clusters and provides the cluster flow rate (&#x3b1;) to the temporal proximity change detector (<b>116</b>) for generating the hash table. The cluster flow rate (&#x3b1;) for the object cluster i, with J objects in the object cluster at any timestamp is determined using the equation 1:</p><p id="p-0085" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mrow>      <mi>Cluster</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mi>flow</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mi>rate</mi>      <mo>&#x2062;</mo>      <mtext>   </mtext>      <mrow>       <mo>(</mo>       <mi>&#x3b1;</mi>       <mo>)</mo>      </mrow>     </mrow>     <mo>=</mo>     <mfrac>      <mrow>       <msubsup>        <mo>&#x2211;</mo>        <mrow>         <mi>j</mi>         <mo>=</mo>         <mn>1</mn>        </mrow>        <mi>J</mi>       </msubsup>       <mrow>        <mi>Flow</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mi>rate</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mi>Obj</mi>       </mrow>      </mrow>      <mi>j</mi>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>1</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0086" num="0084">where, flow rate Obj is the optical flow rate. The cluster flow rate (&#x3b1;) may be determined for K number of object clusters in the video frame.</p><p id="p-0087" num="0085">Since a fast moving object has a higher tendency towards slow motion effect generation, hence increasing the cluster flow rate would normally refer to an increase in the slow motion effect. For example, a car moving at 100 miles per hour would be slow motioned at 8&#xd7;, while a bicycle moving at 20 miles per hour would be slow motioned at 2&#xd7;.</p><p id="p-0088" num="0086">The cluster flow rate (&#x3b1;) (or a cluster speed) denotes the average of a motion magnitude of different objects in the object cluster. A slow motion factor may be nonlinear directly proportional to the cluster flow rate (&#x3b1;). The slow motion factor may also be called a slow motion rate which is a degree of slow motion effect or a quantitative measure of a magnitude of the slow motion effect. The terms &#x201c;slow motion factor&#x201d; and &#x201c;slow motion rate&#x201d; are interchangeably used and are related to the object's affinity for slow motion. The cluster flow rate (&#x3b1;) is calculated by averaging an optical flow of all the objects present in the object cluster. A fast moving object cluster has more affinity for the slow motion. For example, a man moving on a bike will have a higher slow motion rate as compared to a man running with a slower speed. Depending on the situation or the video frame of reference of recording the video, the cluster flow rate (&#x3b1;) may be different for a same object. In an example, a car is moving and a person is sitting inside the car. Consider, the person records the video of outside environment while moving along with the car. Now, everything in the outside environment appears to move backward for the person. Consider, a man is standing in a ground in the outside environment, but the man will appear to move along with the ground for the person inside the car. So, the value of the cluster flow rate (&#x3b1;) is the magnitude of a backward motion. The backward motion a relative motion of outside objects (not in the car from which the video is recorded). So all the objects outside the car will appear to move backward with respect to an object moving forward.</p><p id="p-0089" num="0087">If two objects are in close interaction, there is a problem in that it is difficult to find a different slow motion rate for each object, which introduces inconsistency. In order to address this problem, the temporal proximity change detector (<b>116</b>) monitors the motion of objects grouped in clusters of close interaction. With reference to video frames <b>705</b>-<b>707</b> in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, consider, &#x3b1;1 is the optical flow of the ball, &#x3b1;2 is the optical flow of the human in each case, &#x3b1;3 is optical flow of the goal post. In the video frame <b>705</b>, the ball and the human are in close interaction, where the ball and the human are grouped together in an object cluster (see yellow). Therefore, the temporal proximity change detector (<b>116</b>) determines the cluster flow rate (&#x3b1;) of the object cluster as (a1+&#x3b1;2)/2.</p><p id="p-0090" num="0088">In the video frame <b>706</b>, the ball and the human are each in a separate object cluster, where there are two clusters corresponding to the ball (red) and the human (green). Therefore, the temporal proximity change detector (<b>116</b>) determines the cluster flow rate (&#x3b1;) corresponding to the ball cluster as &#x3b1;1, and the cluster flow rate (&#x3b1;) corresponding to the human cluster as &#x3b1;2. In the video frame <b>707</b>, the ball and goalpost are in close interaction, wherein the ball and the goalpost are grouped together in a single object cluster (green), where there are two object clusters corresponding to the ball and goalpost (green), and human (yellow). Therefore, the temporal proximity change detector (<b>116</b>)) determines the cluster flow rate (&#x3b1;) corresponding to the cluster containing the ball and goalpost as &#x3b1;1+&#x3b1;3/2, and the cluster flow rate (&#x3b1;) corresponding to cluster containing the human as &#x3b1;2.</p><p id="p-0091" num="0089">In some embodiments, the proximity change coefficient (&#x3b2;) may denote the number of times the object interacts with different objects or object clusters. For example, in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, the ball interacts with different objects or a collection of objects (e.g., with the human in (<b>705</b>), with the goalpost in (<b>707</b>), etc.) or the object clusters. In other embodiments, the proximity change coefficient (&#x3b2;) may denote the number of times the object changes cluster. The slow motion factor is nonlinear directly proportional to the proximity change coefficient (&#x3b2;). The proximity change coefficient (&#x3b2;) is calculated by monitoring the change of the object cluster in the hash table. Consider, a man is jumping from a horse while playing Soccer ball, where the man changes cluster several times.</p><p id="p-0092" num="0090">When a human kicks the ball towards goalpost, then the ball changes cluster several times, etc. The proximity change coefficient (&#x3b2;) accounts for effect of the motion, specifically the number of times the cluster of the object changes due to its own motion or a motion of other objects. The proximity change coefficient (&#x3b2;) provides an importance of the motion of the object. A greater number of changes of the object cluster denotes that the object is more important and in turn contributes more towards the slow motion rate. The proximity change coefficient (&#x3b2;) may be used by the temporal proximity change detector (<b>116</b>) to decide whether the object is of central importance in the video.</p><p id="p-0093" num="0091">Returning to <figref idref="DRAWINGS">FIG. <b>2</b>C</figref>, the rigidness score estimator (<b>113</b>) estimates the rigidness score of the segmented object based on one or more of (a) the relative frequency of the segmented object occurring the video and (b) the perceptual aesthetic quality of the video frame. The significance score determiner (<b>117</b>) determines the significance score of each object cluster based on the rigidness score, and based on the cluster flow rate (&#x3b1;) and/or the proximity change coefficient (&#x3b2;), of the segmented objects included in the object cluster.</p><p id="p-0094" num="0092">The masked interpolation engine (<b>118</b>) determines a degree of slow motion effect corresponding to each object cluster based on the significance score of the object cluster, and generates the slow motion video by applying the determined degree of slow motion effect to corresponding segmented objects in the object cluster. In an embodiment, the masked interpolation engine (<b>118</b>) determines the interpolation rate of each section in the video frames based on the significance score of the object cluster in the section. The masked interpolation engine (<b>118</b>) interpolates the object cluster in each video frame based on the interpolation rate.</p><p id="p-0095" num="0093">Although the <figref idref="DRAWINGS">FIG. <b>2</b>C</figref> shows the hardware components of the sectional slow motion controller (<b>110</b>), it is to be understood that other embodiments are not limited thereon. In other embodiments, the sectional slow motion controller (<b>110</b>) may include fewer or more components. Further, the labels or names of the components are used only for illustrative purposes and do not limit the scope of the disclosure. One or more components may be combined together to perform a same or substantially similar function for generating the slow motion video by applying a different degree of slow motion effect to different objects in the video.</p><p id="p-0096" num="0094"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a flow diagram <b>300</b> illustrating a method for generating the multiple sectional slow motion effects in the video, according to an embodiment. At step <b>301</b>, the method includes segmenting the objects in the video. In an embodiment, the segmentation engine (<b>112</b>) segments the objects in the video. Segmentation engine (<b>112</b>) is configured to determine a class confidence score of each video frame of the video using a Machine Learning (ML) model, the class confidence score being a quantitative measurement of a quality of content in the video frame. Segmentation engine (<b>112</b>) is further configured to filter the video frames based on the class confidence scores. Segmentation engine (<b>112</b>) is further configured to identify by the electronic device (<b>100</b>) the objects in the filtered video frames. Segmentation engine (<b>112</b>) is further configured to estimate a depth score of each of the identified objects in the filtered video frames based on depth information in the filtered video frames. Segmentation engine (<b>112</b>) is further configured to determine an optical flow rate of each of the identified objects in the filtered video frames, the optical flow rate being a quantitative measurement of a velocity of movement of pixels corresponding to the identified objects in the filtered video frames. Segmentation engine (<b>112</b>) is further configured to generate the segmented objects from the filtered video frames by applying the optical flow rate and the depth score of each identified object to a semantic segmentation network.</p><p id="p-0097" num="0095">At step <b>302</b>, the method includes determining an interaction between the segmented objects. In an embodiment, the interaction detector (<b>114</b>) determines the interaction between the segmented objects. Interaction detector (<b>114</b>) is further configured to determine the interaction between the segmented objects for a time duration by applying the optical flow rate and the depth score of each of the segmented objects to a memory based neural network. In an embodiment, the determining of the interaction comprises categorizing the segmented objects to one of an interacting class in response to determining the interaction between the segmented objects, and a non-interacting class in response to not determining the interaction between the segmented objects. The memory based neural network may output a probability score indicating whether the segmented objects are interacting.</p><p id="p-0098" num="0096">At step <b>303</b>, the method includes clustering the segmented objects in the video to generate object clusters based on the interaction. Cluster generator (<b>115</b>) is further configured to cluster the segmented objects in the video to generate object clusters based on the interaction</p><p id="p-0099" num="0097">At step <b>304</b>, the method includes determining a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters. Wherein, the determining of the degree of slow motion effect may comprise determining a degree of slow motion effect to be used for clusters of objects in the video based on the interaction between the segmented objects and at least one parameter associated with the importance of the scene in the video.</p><p id="p-0100" num="0098">In an embodiment, the significance score determiner (<b>117</b>) determines a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters. In an embodiment, the significance score determiner (<b>117</b>) is configured to determine the significance score of each of the object clusters. The significance score determiner (<b>117</b>) is further configured to determine the degree of slow motion effect for each of the objects based on the corresponding the significance score for the object clusters. The significance score may be a measure of a relative importance of each of the object clusters in the video.</p><p id="p-0101" num="0099">In an embodiment, the determining of the significance score comprises determining a cluster flow rate (&#x3b1;) of each of the object clusters based on an optical flow rate of each segmented object in the corresponding object clusters, generating a hash table of a proximity change coefficient (&#x3b2;) based on the change in clustering of the segmented objects at the different instants of time in the video, the proximity change coefficient (&#x3b2;) being a number of times that the segmented object changes into and out of the object clusters within a time duration, and determining the significance score of each of the object clusters based on based on at least one of the cluster flow rate (&#x3b1;) and the proximity change coefficient (&#x3b2;).</p><p id="p-0102" num="0100">In an embodiment, the determining of the significance score comprises estimating, by the electronic device (<b>100</b>), a rigidness score of each of the segmented objects based on at least one of (a) a relative frequency of the segmented objects occurring in the video, (b) a perceptual aesthetic quality of the segmented objects and (c) a degree of movement of the segmented objects based on the object classes of the segmented objects, the rigidness score being a measure of static aspects of an affinity for slow motion of the segmented object, and determining the significance score of each of the object clusters based on the rigidness score. The rigidness score may be estimated using a weighted combination of (a) the relative frequency of the segmented objects occurring in the video, (b) the perceptual aesthetic quality of the segmented objects and (c) the degree of movement of the segmented objects based on the object classes of the segmented objects, weights thereof being determined using regression analysis. The perceptual aesthetic quality of the segmented objects may be determined as a probability using a neural network that is trained on a dataset of images labeled with a score of aesthetic values between 0 and 1.</p><p id="p-0103" num="0101">At step <b>305</b>, the method includes generating the slow motion video by applying the degree of slow motion effect to that has been determined to corresponding the object clusters. In an embodiment, the masked interpolation engine (<b>118</b>) generates the slow motion video by applying the degree of slow motion effect to that has been determined to corresponding the object clusters. The method may further include storing the slow motion video. In an embodiment, the memory (<b>120</b>) stores the slow motion video.</p><p id="p-0104" num="0102">In an embodiment, masked interpolation engine (<b>118</b>) is configured to determine an interpolation rate for each of a plurality of sections in the video frames that correspond to the object clusters, based on the significance score of the object cluster corresponding to the section, and to interpolate the object clusters in each video frame based on the interpolation rate of the corresponding sections.</p><p id="p-0105" num="0103">The various actions, acts, blocks, steps, or the like in the flow diagram <b>300</b> may be performed by the processor (<b>130</b>) in the order presented, in a different order or simultaneously. Further, in some embodiments, some of the actions, acts, blocks, steps, or the like may be omitted, added, modified, skipped, or the like without departing from the scope of the invention.</p><p id="p-0106" num="0104"><figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example scenario of generating the multiple sectional slow motion effects in the video, according to an embodiment. At <b>401</b>A, the filter frame engine (<b>111</b>) receives the video (e.g. a human kicking a ball to a goalpost) from the memory (<b>120</b>). At <b>401</b>B, the filter frame engine (<b>111</b>) decodes the video frames in the video. At <b>402</b>, the filter frame engine (<b>111</b>) determines the class confidence score of each video frame in the video. In an embodiment, the filter frame engine (<b>111</b>) determines the class confidence score of each video frame by adding a class confidence score of each object in the video frame. In the example, the class confidence scores of the ball, the human and the goalpost, are 0.9, 0.9, and 0.8 respectively. The filter frame engine (<b>111</b>) matches the class confidence score of each video frame with a class confidence score threshold for filtering the video frames. At <b>403</b>, the filter frame engine (<b>111</b>) chooses the video frames with a class confidence score that is greater than the class confidence score threshold as the video frames with clear visibility of the objects.</p><p id="p-0107" num="0105">The video frames filtered from the filter frame engine (<b>111</b>) are further processed by the segmentation engine (<b>112</b>). The segmentation engine (<b>112</b>) identifies the objects in the video frame. At <b>404</b>, the segmentation engine (<b>112</b>) segments the objects in each video frame (i.e., frame 1 to frame n) by applying the optical flow rate and the depth score of each object to the semantic segmentation network, where each pixel in the video frame is assigned to an object category to which the pixel belongs. In another embodiment, the segmentation engine (<b>112</b>) segments the objects in each video frame. At <b>405</b>, the rigidness score estimator (<b>113</b>) analyses the segmented objects in the video frames and determines the rigidness score of the objects. In the example, the rigidness scores of the ball, the human and the goalpost are 0.6, 0.5, and 0.01 respectively.</p><p id="p-0108" num="0106">At <b>406</b>, the interaction detector (<b>114</b>) determines the interaction between the segmented objects in the video frame in terms of an interaction probability of each object by applying the optical flow rate of each segmented object from the filtered video frames and the depth score of each segmented object to the memory based neural network. The interaction probability is calculated using the memory based neural network. The memory based neural network takes the video frames, the optical flow and the depth score as input, and outputs the interaction probability, i.e., the probability with which the objects in the video frames are interacting. In the example, the interaction probability of the ball and human is 0.95, the interaction probability of the human and goal post is 0.05, and the interaction probability of the goal post and ball is 0.05, in the video frame. In response to detecting that the interaction probability of ball and human are 0.95 and thus is the highest interaction probability among the interaction probabilities of the ball and human, the human and goal post, and the goal post and ball, the interaction detector (<b>114</b>) detects that the ball and human are interacting objects in the video frame.</p><p id="p-0109" num="0107">At <b>407</b>, in response to detecting the interacting objects in the video frame, the cluster generator (<b>115</b>) creates the object clusters of the interacting and non-interacting objects in the video frame. In the example, the ball and human are interacting objects, and the goalpost is the non-interacting object in the video frame. Therefore, the cluster generator (<b>115</b>) creates a first object cluster including the ball and human and a second object cluster including the goalpost. In response to clustering the objects, the cluster generator (<b>115</b>) determines the cluster flow rate (&#x3b1;) of each object cluster. In a case in which an object cluster contains multiple objects, the cluster generator (<b>115</b>) determines the cluster flow rate (&#x3b1;) of the object cluster containing the multiple objects by calculating the average optical flow rate of each segmented object in the object cluster. In the example, the cluster flow rate (&#x3b1;) of the object cluster that includes the ball and human is clustered_flow1, whereas the cluster flow rate (&#x3b1;) of the object cluster that includes the goalpost is clustered_flow2. The clustered_flow1 is determined by calculating the average optical flow rate of the ball and human in the first object cluster. The clustered_flow2 is determined by calculating the optical flow rate of the goalpost in the second object cluster.</p><p id="p-0110" num="0108">At <b>408</b>, the temporal proximity change detector (<b>116</b>) generates the hash table of the proximity change coefficient (&#x3b2;) using a change in clustering of the segmented objects at different instants of time in the video. At <b>409</b>, the significance score determiner (<b>117</b>) determines the significance score of each object or object cluster based on the rigidness score, and based on the cluster flow rate (&#x3b1;) and/or the proximity change coefficient (&#x3b2;). In the example, the significance score of the ball and the human are 0.6 and 0.3 respectively. Since goalpost does not move, the optical flow corresponding to the goalpost will be negligibly small. Hence, the significance score of the goalpost is zero. Further, the significance score determiner (<b>117</b>) determines the sectional interpolation rate of each section in the video frame containing the objects based on the significance score of the objects. At <b>410</b>, the masked interpolation engine (<b>118</b>) generates the sectional slow motion video by interpolating the sections in the video based on the sectional interpolation rate.</p><p id="p-0111" num="0109"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a flow diagram illustrating a method of segmenting objects in the video, according to an embodiment. In response to decoding the video frames in the video at <b>401</b>B, the filter frame engine (<b>111</b>) filters the video frames having a class confidence score that is greater than the class confidence score threshold as the video frames with clear visibility of the objects at <b>402</b>. The video frames filtered from the filter frame engine (<b>111</b>) are further processed by the segmentation engine (<b>112</b>). The segmentation engine (<b>112</b>) identifies the objects in the video frame. Further, the segmentation engine (<b>112</b>) applies the optical flow rate (<b>502</b>) and the depth score (<b>501</b>) of each object in the filtered video frames to the semantic segmentation network (<b>503</b>). The semantic segmentation network (<b>503</b>) outputs a probability of each pixel of the video frame belonging to object classes (e.g., ball, human, goalpost). Further, the segmentation engine (<b>112</b>) obtains the segmented objects (<b>404</b>) in each video as an output from the semantic segmentation network (<b>503</b>).</p><p id="p-0112" num="0110"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates an example scenario of determining a depth score of objects in the video frame, according to an embodiment. The electronic device (<b>100</b>) obtains the depth score from the camera sensor by capturing depth patterns along with the image of the objects. The depth patterns may correspond to the depth information discussed above. In another embodiment, the electronic device (<b>100</b>) determines the depth score using classical image processing or deep learning based methods. The electronic device (<b>100</b>) performs the classical image processing or the deep learning on the image (<b>504</b>) to generate the image map (<b>505</b>) and clarifies the objects in the image with their depth score as shown in <b>506</b>.</p><p id="p-0113" num="0111"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flow diagram illustrating a method for determining the interaction between the segmented objects, according to an embodiment. In response to segmenting the objects at <b>404</b>, the interaction detector (<b>114</b>) determines a context map (<b>602</b>), the depth score (<b>603</b>) of each segmented object, and the optical flow rate (<b>604</b>) of each segmented object at <b>601</b>. The context map is obtained using an intermediate activation of pre-trained networks that have been pre-trained to perform an image classification task. In this case the input image is passed through the pre-trained network which has been trained to perform the image classification task and one or more intermediate layers may be taken as output which represents the context map. At <b>605</b>, the interaction detector (<b>114</b>) provides a concatenate value of the context map (<b>602</b>), the depth score (<b>603</b>), and the optical flow rate (<b>604</b>) to a network built using the memory based neural cells (i.e., LSTM based neural network). At <b>606</b>, the interaction detector (<b>114</b>) detects whether or not the segmented objects are in interaction with each other in a time span using the network built using the memory based neural cells. At <b>607</b>, the interaction detector (<b>114</b>) categorizes the segmented objects into an interacting class in response to detecting interaction between the segmented objects. At <b>608</b>, the interaction detector (<b>114</b>) categorizes the segmented objects into a non-interacting class in response to detecting no interaction between the segmented objects.</p><p id="p-0114" num="0112">In an embodiment, the LSTM based neural network outputs a probability score denoting whether two objects are interacting or not. For example, if the ball and the human are interacting, the probability will be high (e.g., 0.99) and if the ball and human are not interacting, the probability will be low (e.g., 0.1).</p><p id="p-0115" num="0113"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a flow diagram illustrating a method for generating the hash table of the proximity change coefficient (&#x3b2;), according to an embodiment. In an embodiment the temporal proximity change detector (<b>116</b>) identifies the pixels of the object cluster at <b>701</b>. Further, the temporal proximity change detector (<b>116</b>) calculates the flow rate of pixels of each object cluster at <b>702</b>. At <b>703</b>, the temporal proximity change detector (<b>116</b>) prepares the table of an interaction score and the cluster flow rates. The interaction score is 1 if two objects are interacting, otherwise the interaction score is 0.</p><p id="p-0116" num="0114"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> illustrates an example scenario of generating the hash table of the proximity change coefficient (&#x3b2;), according to an embodiment. Item <b>704</b> represents the object clusters in the video frames (<b>705</b>-<b>707</b>) (see discussion above). The temporal proximity change detector (<b>116</b>) counts the number of times a proximity of the object changes from different object clusters in a given span of time from the object clusters in the video frames (<b>705</b>-<b>707</b>). Further, the temporal proximity change detector (<b>116</b>) dynamically creates the hash tables (<b>708</b>) arranged in Three Dimensionally (3D) for storing the proximity change of the objects in different object clusters at different time stamps. Item <b>710</b> is z axis direction of the hash tables arranged in 3D and denotes a timestamp &#x394;t at which the hash table is created. In another embodiment, the hash table may also store the cluster flow rate (&#x3b1;) and the object interactions among various clusters.</p><p id="p-0117" num="0115">Item <b>709</b> denotes a first Boolean in tuple for each cell tpcd[l][m][n] is equal to:</p><p id="p-0118" num="0116">1: if object <b>1</b> belongs to cluster m</p><p id="p-0119" num="0117">0: otherwise</p><p id="p-0120" num="0118">The second element in the tuple shows optical flow rate of object <b>1</b>. Here, n corresponds to the frame dimension or time dimension.</p><p id="p-0121" num="0119">In an example, tpcd[l] [m] [0] denotes a 2d table for a first video frame. Similarly, tpcd[l][m][1] denote a 2d table for second video frame etc. Also for 2d table say tpcd[m][n[0], m here denotes rows that indicate objects and n here denotes columns which denote object cluster number. So, tpcd[0][0][0] denotes a first object, a first object cluster and a first frame of the video.</p><p id="p-0122" num="0120">Tpcd[1][2][5] represent tuple for a second object, a third object cluster and a sixth frame of the video. Each 2D table stores information about a frame. The 2D tables when stacked for all the video frames forms the 3D table.</p><p id="p-0123" num="0121">In an example scenario, consider the tpcd[1][0][0] in the hash table <b>408</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref> and the video frames (<b>705</b>-<b>707</b>). Then the cluster flow rate3 (&#x3b1;) of an object cluster <b>3</b>=(flow of ball+flow of human)/2=(0+0.110)/2=0.055. where the object cluster <b>3</b> is the cluster of ball and human of the first video frame (<b>705</b>) of the video. Similarly each object cluster in each video frame has a cluster flow rate. Consider, the second frame (<b>706</b>) contains a cluster that contains only the ball. The cluster that contains only the ball has a cluster flow rate the same as that of the flow rate of the ball.</p><p id="p-0124" num="0122">The proximity change coefficient (&#x3b2;) is equal to the number of times the object changes cluster within the time &#x394;t. In the example, consider the ball has changed cluster four times. Initially the ball was alone prior to the human approaching the ball to kick the ball. Further, the ball was in cluster with human, as the human kicked the ball. Further, the ball was in the air and alone as a cluster. Further, the ball was in cluster with the goal post. Further, the ball is again alone after deflecting off of the goalpost, i.e., the same as cluster <b>1</b>. So, the ball changes cluster <b>4</b> times&#x2014;alone, with human, with goalpost, alone. Therefore, &#x3b2; of the ball is 4. In case of the human, initially the human was alone. Further, the human was in cluster with the ball. Further, the human was alone. So, total number of changes of human is 2 and hence of the human is 2.</p><p id="p-0125" num="0123">The hash table <b>408</b> in the <figref idref="DRAWINGS">FIG. <b>4</b></figref> belongs to the first frame. Similarly, a z axis direction of hash table <b>408</b> includes hash tables corresponds to other video frames. A first entry of the hash table <b>408</b> denotes whether the corresponding row, i.e., object (e.g. ball), is in the corresponding column, i.e. cluster of the ball and human. Further, the method provides the sectional SloMo effect to only that portion of the video where the object cluster change does not occur to maintain a time consistency.</p><p id="p-0126" num="0124"><figref idref="DRAWINGS">FIG. <b>8</b>A</figref> is a flow diagram illustrating a method for estimating the rigidness score of the object, according to an embodiment.</p><p id="p-0127" num="0125">At <b>801</b>, the rigidness score estimator (<b>113</b>) receives segmented objects from the segmentation engine (<b>112</b>).</p><p id="p-0128" num="0126">At <b>802</b>, the rigidness score estimator (<b>113</b>) may determine statistical features including (a) the relative frequency of the segmented objects occurring in the video, and (b) a perceptual aesthetic quality of the video frame, and (c) a degree of movement of the segmented objects based on the object classes of the segmented objects.</p><p id="p-0129" num="0127">At <b>803</b>, the rigidness score of each segmented object is calculated based on the statistical features, i.e., based on at least one of (a), (b), and (c).</p><p id="p-0130" num="0128">Generally, humans give more importance to living things such as pets over non-living things. Hence, the humans are more pleasant for recording the slow motion video of hierarchically advanced objects. The rigidness score is determined using an ML model trained on pre-recorded slow motion videos and the statistical features including (a) the relative frequency of the segmented objects occurring in the video, (b) a relative visually pleasing appearance of the objects from perspectives of users, and (c) a degree of movement of the segmented objects based on the object classes of the segmented objects. The relative visually pleasing appearance of objects from perspectives of users is a metric measurement of an aesthetic value of the video frame using a pre trained CNN networks on Aesthetic value Analysis (AVA) dataset of images labeled with a score of aesthetic values between 0 and 1. The pre trained CNN network may be trained on other publicly available datasets or it is possible to generate a custom dataset for aesthetic values to train the CNN network. The relative frequency of the segmented objects occurring in the video (a) may be determined using equations 2:</p><p id="p-0131" num="0000"><maths id="MATH-US-00002" num="00002"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>a</mi>     <mo>=</mo>     <mfrac>      <mtable>       <mtr>        <mtd>         <mrow>          <mi>Number</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>of</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>times</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>the</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>object</mi>         </mrow>        </mtd>       </mtr>       <mtr>        <mtd>         <mrow>          <mi>occurred</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>in</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>slow</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>motion</mi>          <mo>&#x2062;</mo>          <mtext>   </mtext>          <mi>video</mi>         </mrow>        </mtd>       </mtr>      </mtable>      <mrow>       <mi>Total</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>number</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>of</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>occurences</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>in</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>all</mi>       <mo>&#x2062;</mo>       <mtext>   </mtext>       <mi>videos</mi>      </mrow>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>2</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0132" num="0129">The rigidness score may be calculated using the equation 3 based on the statistical features, i.e., based on at least one of (a), (b), and (c):</p><p id="p-0133" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>Rigidness Score=&#x3bb;1<i>a+&#x3bb;</i>2<i>b+&#x3bb;</i>3<i>c</i>&#x2003;&#x2003;(3)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0134" num="0130">where &#x3bb;1, &#x3bb;2, &#x3bb;3 contribute a weightage to the parameters (a), (b), and (c) respectively in calculating the in the rigidness score. In an embodiment, &#x3bb;1, &#x3bb;2, &#x3bb;3 are used as normalization factors to the parameters (a), (b) and (c) respectively. Here, (a) denotes the relative frequency of the segmented objects occurring in the video and (b) denotes the relative visually pleasing appearance of the objects. (c) denotes a degree of movement of the segmented objects based on the object classes of the segmented objects.</p><p id="p-0135" num="0131">A statistical preference or the relative frequency of objects occurring in the video is a parameter showing the object category/class which has occurred most frequently in past recordings of slow motion videos. The object occur most number of times in slow motion videos suggests that object is more important for slow motion as compared to other objects in the video. The statistical preference (a) denotes the statistical aspects of preference towards the slow motion videos. The statistical preference (a) static properties of an object's motion affinity for movement. If the reference dataset is fixed then the value the statistical preference (a) remains as a fixed value. The slow motion factor is nonlinear directly proportional to the statistical preference (a). Consider, a dataset of 1000 normal slow motion videos recorded at uniform frame rates. The electronic device (<b>100</b>) checks for parts in the 1000 normal slow motion videos where the human is slow motioned occurs 420 times and total number of videos in which humans occurred is 860. Therefore, statistical preference of the human (ahuman)=420/860=0.488. Similarly each object class has a set of values for a video dataset. Generally, a user tends to convert to slow motion an image of jumping, running etc. containing the human. So the preferences are given for objects such as human.</p><p id="p-0136" num="0132">The perceptual aesthetic or visible quality (b) of the video frame denotes how good the video frame looks to human eyes. The slow motion factor is nonlinear directly proportional to the perceptual aesthetic or visible quality (b). The perceptual aesthetic quality (b) is calculated in terms of a probability of how good the video frame looks to the eye, using a neural network which is trained on a dataset of images labeled with score of aesthetic values between 0 and 1. Neat and clear videos with good aesthetic values are preferred for recording the slow motion videos. For example, a child playing on snow with mountains in the background will have more affinity for slow motion as compared to a child playing inside the building with a non-clear image or surroundings.</p><p id="p-0137" num="0133">Consider the first frame in the decoded frames (<b>401</b>B) (see <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>), doing a higher slow motion rate of the ball as compared to the human means the ball will be visible for relatively longer duration in the output sectional SloMo video. In this example a background, a lighting condition, a tone, and a contrast does not change very much so for this particular example the perceptual aesthetic quality (b) does not have a significant role. For this example the aesthetic value of the human or the ball remains almost constant throughout the video.</p><p id="p-0138" num="0134">Consider a scenario where a color of the ball is similar to the background, or the ball looks blurred and not very clear, or there is very little contrast to notice sharp textures of a section of the ball. Now the importance to the ball is low and, hence the value of b will be low which will contribute towards a lower significance score of the ball, making the ball appear for a relatively smaller time in the output sectional SloMo video.</p><p id="p-0139" num="0135"><figref idref="DRAWINGS">FIG. <b>8</b>B</figref> illustrates an example scenario of different type of objects in different video frames, according to an embodiment. The electronic device (<b>100</b>) classifies the different type of objects in <figref idref="DRAWINGS">FIG. <b>8</b>B</figref> to different object classes and determines the rigidness scores based on the parameters (a) and (b). As shown in <b>804</b> and <b>805</b>, an object class such as sports equipment is often presented along with the human in the video frame to be slow motion relevant. Aesthetic value of the scenes like fire (shown in <b>806</b>), waves, flora have higher aesthetic value than others. The video frame shown in <b>807</b> has lower aesthetic value for slow motion content.</p><p id="p-0140" num="0136">In an example, consider a dataset of 1000 normal slow motion videos, where a dog has been in large number in the videos with slow motion effect in the reference dataset and the dog belonging to an animal category. Therefore the electronic device (<b>100</b>) calculates the statistical preference (a) of the dog in the video frame <b>808</b>A as 0.422.</p><p id="p-0141" num="0137">In another example, consider a dataset of 1000 normal slow motion videos, where a building has been in less number in the videos with slow motion effect in reference dataset. Therefore, the electronic device (<b>100</b>) calculates the statistical preference (a) of the building in the video frame <b>808</b>B as 0.002.</p><p id="p-0142" num="0138">In an embodiment, an additional parameter (c) with a constant value may also be used for determining the rigidness score, if the reference dataset that are used to calculate the parameter (a) is inaccurate or biased. The parameter (c) is a manually annotated value using a degree of movement of some object classes. For example, object classes like, animals, birds, humans may have a greater degree of movement than other object classes such as buildings, trees etc. The parameter (c) is manually annotated for various fixed objects categories (e.g. <b>1000</b> majorly occurring objects in a video and used to reduce bias).</p><p id="p-0143" num="0139">In an example, the electronic device (<b>100</b>) calculates the perceptual aesthetic quality (b) of the video frame <b>809</b>A as 0.86, where the probability is higher because the video frame <b>809</b>A is more aesthetically pleasing to the human eyes.</p><p id="p-0144" num="0140">In another example, the electronic device (<b>100</b>) calculates the perceptual aesthetic quality (b) of the video frame <b>809</b>B as 0.14, where the probability is less because the video frame <b>809</b>A is not aesthetically pleasing to the human eyes.</p><p id="p-0145" num="0141"><figref idref="DRAWINGS">FIG. <b>8</b>C</figref> illustrates an example scenario of determining linear coefficients &#x3bb;1, &#x3bb;2, according to an embodiment. The dataset used for training the ML model contains normal (i.e., non-slow motion) videos and slow motion videos captured or converted by users. A first step for modelling includes extracting the object classes from each video frame. Using a Linear Regression taking RS as input and 0/1 as output (SloMo or normal video), the weights (&#x3bb;) to be assigned for each parameter may be obtained. Various (a) and (b) values for the objects are stored in the reference dataset (i.e., generated from 1000 videos). These values are stores as features in a file in the memory (<b>120</b>). A ground truth corresponding to this modelling is whether the video has been preferred for slow motion (or by how much affinity). Linear regression modelling is applied to predict the ground truth and coefficients of linear regression are stored as &#x3bb;1, &#x3bb;2, values.</p><p id="p-0146" num="0142"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a flow diagram illustrating a method for determining the Significance Score (SS) of the object cluster, according to an embodiment. In an embodiment, the significance score may be determined using the equation 4a or 4b:</p><p id="p-0147" num="0000"><maths id="MATH-US-00003" num="00003"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SS</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mo>(</mo>       <mrow>        <mi>a</mi>        <mo>&#xd7;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mi>&#x3b2;</mi>          <mo>+</mo>          <mrow>           <mi>Rigidness</mi>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>score</mi>          </mrow>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <msqrt>       <msubsup>        <mi>SS</mi>        <mi>i</mi>        <mn>2</mn>       </msubsup>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mi>a</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00003-2" num="00003.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SS</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mo>(</mo>       <mrow>        <mrow>         <mi>a</mi>         <mo>&#xd7;</mo>         <mi>&#x3b2;</mi>        </mrow>        <mo>+</mo>        <mrow>         <mi>Rigidness</mi>         <mo>&#x2062;</mo>         <mtext>   </mtext>         <mi>score</mi>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <msqrt>       <msubsup>        <mi>SS</mi>        <mi>i</mi>        <mn>2</mn>       </msubsup>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mrow>      <mn>4</mn>      <mo>&#x2062;</mo>      <mi>b</mi>     </mrow>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0148" num="0143">where, the rigidness score=&#x3bb;1a+&#x3bb;2b+&#x3bb;3c, &#x221a;{square root over (ss<sub>1</sub><sup>2</sup>)} is a normalization coefficient which is a square root of sum of squares of different object clusters for that timestamp i for a different object cluster. where,</p><p id="p-0149" num="0000"><maths id="MATH-US-00004" num="00004"><math overflow="scroll"> <mrow>  <mi>&#x3b1;</mi>  <mo>=</mo>  <mfrac>   <mrow>    <mi>Cluster</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>flow</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>rate</mi>    <mo>&#x2062;</mo>    <mi>i</mi>   </mrow>   <mrow>    <msubsup>     <mo>&#x2211;</mo>     <mrow>      <mi>i</mi>      <mo>=</mo>      <mn>1</mn>     </mrow>     <mi>T</mi>    </msubsup>    <mrow>     <mi>Cluster</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>flow</mi>     <mo>&#x2062;</mo>     <mtext>   </mtext>     <mi>rate</mi>     <mo>&#x2062;</mo>     <mi>i</mi>     <mo>/</mo>     <mi>T</mi>    </mrow>   </mrow>  </mfrac> </mrow></math></maths></p><p id="p-0150" num="0144">where, cluster flow rate i is the flow rate of given object at a particular time instant i, where T is the total time of the slow motion video.</p><p id="p-0151" num="0145">In another embodiment, the significance score may be determined using the equation 5:</p><p id="p-0152" num="0000"><maths id="MATH-US-00005" num="00005"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SS</mi>     <mo>&#x221d;</mo>     <mfrac>      <mrow>       <mo>(</mo>       <mrow>        <mi>&#x3b1;</mi>        <mo>&#xd7;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mi>&#x3b2;</mi>          <mo>+</mo>          <mrow>           <mi>Rigidness</mi>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>score</mi>          </mrow>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <msqrt>       <msubsup>        <mi>SS</mi>        <mi>i</mi>        <mn>2</mn>       </msubsup>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>5</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0153" num="0146">where, the rigidness score=&#x3bb;1a+&#x3bb;2b+&#x3bb;3c.</p><p id="p-0154" num="0147">In another embodiment, the significance score may be determined using the equation 6:</p><p id="p-0155" num="0000"><maths id="MATH-US-00006" num="00006"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SS</mi>     <mo>&#x221d;</mo>     <mfrac>      <mrow>       <mo>(</mo>       <mrow>        <mi>&#x3b2;</mi>        <mo>+</mo>        <mrow>         <mi>Rigidness</mi>         <mo>&#x2062;</mo>         <mtext>   </mtext>         <mi>score</mi>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <msqrt>       <msubsup>        <mi>SS</mi>        <mi>i</mi>        <mn>2</mn>       </msubsup>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>6</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0156" num="0148">where, the rigidness score=&#x3bb;1a+&#x3bb;2b.</p><p id="p-0157" num="0149">In another embodiment, the significance score may be determined using the equation 7:</p><p id="p-0158" num="0000"><maths id="MATH-US-00007" num="00007"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SS</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mo>(</mo>       <mrow>        <mi>&#x3b1;</mi>        <mo>&#xd7;</mo>        <mi>Rigidness</mi>        <mo>&#x2062;</mo>        <mtext>   </mtext>        <mi>score</mi>       </mrow>       <mo>)</mo>      </mrow>      <msqrt>       <msubsup>        <mi>SS</mi>        <mi>i</mi>        <mn>2</mn>       </msubsup>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>7</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0159" num="0150">where, the rigidness score=&#x3bb;1a+&#x3bb;2b.</p><p id="p-0160" num="0151">In another embodiment, the significance score may be determined using the equation 8:</p><p id="p-0161" num="0000"><maths id="MATH-US-00008" num="00008"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>SS</mi>     <mo>=</mo>     <mfrac>      <mrow>       <mo>(</mo>       <mrow>        <mi>&#x3b1;</mi>        <mo>&#xd7;</mo>        <mrow>         <mo>(</mo>         <mrow>          <mi>&#x3b2;</mi>          <mo>+</mo>          <mrow>           <mi>Rigidness</mi>           <mo>&#x2062;</mo>           <mtext>   </mtext>           <mi>score</mi>          </mrow>         </mrow>         <mo>)</mo>        </mrow>       </mrow>       <mo>)</mo>      </mrow>      <msqrt>       <msubsup>        <mi>SS</mi>        <mi>i</mi>        <mn>2</mn>       </msubsup>      </msqrt>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>8</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0162" num="0152">where, the rigidness score=&#x3bb;1a.</p><p id="p-0163" num="0153">In an example scenario, consider the segmented objects in the video frames (<b>705</b>-<b>707</b>) in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, the rigidness score of ball is 0.8 and the rigidness score of the human is 0.9. Initially, the electronic device (<b>100</b>) calculates the significance scores of each video frame (<b>705</b>-<b>707</b>) as follows. The significance score of the video frame (<b>707</b>)=(0.11/1.276)&#xd7;(2+0.9)=0.25, in which the flow rate for human is 0.11 and a summation of flow rates of the human for all the frames is 1.276, the proximity change coefficient for the human is 2 and the rigidness score for human is 0.9. Similarly, the significance score of the human in first 5 video frames in the video frames (<b>705</b>-<b>707</b>) are 0.28, 0.25, 0.24, 0.26, and 0.09. Similarly, the significance score of the ball in first 5 video frames in the video frames (<b>705</b>-<b>707</b>) are 0.52, 0.48, 0.44, 0.48, and 0.45.</p><p id="p-0164" num="0154">Further, the electronic device (<b>100</b>) calculates normalized significance scores. Consider, SS1h=(0.28+0.25+0.24+0.26+0.09)=1.12, and SS1b=(0.52+0.48+0.44+0.48+0.45)=2.37.</p><p id="p-0165" num="0000"><maths id="MATH-US-00009" num="00009"><math overflow="scroll"> <mrow>  <mrow>   <mi>The</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>significance</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>score</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>for</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>the</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>human</mi>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mrow>    <mi>in</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>the</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>timestamp</mi>   </mrow>   <mo>=</mo>   <mrow>    <mfrac>     <mrow>      <mi>SS</mi>      <mo>&#x2062;</mo>      <mn>1</mn>      <mo>&#x2062;</mo>      <mi>h</mi>     </mrow>     <msqrt>      <mrow>       <mrow>        <mi>SS</mi>        <mo>&#x2062;</mo>        <mn>1</mn>        <mo>&#x2062;</mo>        <msup>         <mi>b</mi>         <mn>2</mn>        </msup>       </mrow>       <mo>+</mo>       <mrow>        <mi>SS</mi>        <mo>&#x2062;</mo>        <mn>1</mn>        <mo>&#x2062;</mo>        <msup>         <mi>h</mi>         <mn>2</mn>        </msup>       </mrow>      </mrow>     </msqrt>    </mfrac>    <mo>=</mo>    <mn>0.43</mn>   </mrow>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mi>The</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>significance</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>score</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>for</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>the</mi>   <mo>&#x2062;</mo>   <mtext>   </mtext>   <mi>ball</mi>  </mrow>  <mo>&#x2062;</mo>  <mtext></mtext>  <mrow>   <mrow>    <mi>in</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>the</mi>    <mo>&#x2062;</mo>    <mtext>   </mtext>    <mi>timestamp</mi>   </mrow>   <mo>=</mo>   <mrow>    <mfrac>     <mrow>      <mi>SS</mi>      <mo>&#x2062;</mo>      <mn>1</mn>      <mo>&#x2062;</mo>      <mi>b</mi>     </mrow>     <msqrt>      <mrow>       <mrow>        <mi>SS</mi>        <mo>&#x2062;</mo>        <mn>1</mn>        <mo>&#x2062;</mo>        <msup>         <mi>b</mi>         <mn>2</mn>        </msup>       </mrow>       <mo>+</mo>       <mrow>        <mi>SS</mi>        <mo>&#x2062;</mo>        <mn>1</mn>        <mo>&#x2062;</mo>        <msup>         <mi>h</mi>         <mn>2</mn>        </msup>       </mrow>      </mrow>     </msqrt>    </mfrac>    <mo>=</mo>    <mn>0.9</mn>   </mrow>  </mrow> </mrow></math></maths></p><p id="p-0166" num="0155">Since, a ratio of significance scores, i.e., the significance score for the human to the significance score for the ball, is approximately 1:2, the electronic device (<b>100</b>) interpolates the ball at 2&#xd7; slower as compared to the human based on the ratio for generating the sectional slow motion video.</p><p id="p-0167" num="0156"><figref idref="DRAWINGS">FIG. <b>10</b>A</figref> is a flow diagram illustrating a method for generating the multi sectional slow motion video, according to an embodiment. The masked interpolation engine (<b>118</b>) receives the sectional interpolation rates (<b>1001</b>) from the significance score determiner (<b>117</b>) and the filtered video frames (<b>1002</b>) from the frame filtering engine (<b>111</b>). Further, the masked interpolation engine (<b>118</b>) interpolates each section in the filtered video frames based on the interpolation rate and post processes (<b>1003</b>) the sectionally interpolated segments in the filtered video frames.</p><p id="p-0168" num="0157"><figref idref="DRAWINGS">FIG. <b>10</b>B</figref> illustrates an example scenario of generating the multi sectional slow motion video, according to an embodiment. In response to determining the significance score (<b>1005</b>) (i.e. SS1-SS3) of the sections in the video frame (<b>1004</b>), the significance score determiner (<b>117</b>) determines the sectional interpolation rate (<b>1006</b>) (SIR1-SIR3) of each section based on the significance score (<b>1005</b>) of the sections, where the sectional interpolation rate is directly proportional to the significance score. Absolute value of the sectional interpolation rate is assigned to each section based on a difference of the optical flow rate of the section in between two successive video frames.</p><p id="p-0169" num="0158">Further, the significance score determiner (<b>117</b>) sends the sectional interpolation rates to the masked interpolation engine (<b>118</b>). The masked interpolation engine (<b>118</b>) is a neural network based model that takes the sectional interpolation rates and the filtered video frame as input and outputs the sectionally interpolated frames (<b>1007</b>-<b>1009</b>), where different object clusters of the video frame are interpolated at different sectional frame rates. The masked interpolation engine (<b>118</b>) creates a grid in which the masked interpolation engine (<b>118</b>) sequentially places the outputs of sectionally interpolated segments on the grid. First a segment corresponding to a first source frame is placed, then sectionally interpolated frames are placed subsequently, and after all sectionally interpolated clips are placed, a segment corresponding to a second source frame is placed on the grid. This is the process corresponding to sectional interpolation for a single object using two frames. Same process is repeated for multiple objects separately using multiple source frame pairs. Further, the masked interpolation engine (<b>118</b>) uses placeholders for placing the sectionally interpolated frames in between source frames for generating the multi sectional slow motion video (<b>1011</b>).</p><p id="p-0170" num="0159"><figref idref="DRAWINGS">FIGS. <b>11</b>A-<b>12</b>D</figref> illustrate a comparison of a related art method of interpolation and a masked interpolation in the method according to an embodiment. As shown in the <figref idref="DRAWINGS">FIG. <b>11</b>A</figref>, item <b>1101</b> represents two original video frames (i.e. source frame 1 (<b>1102</b>) and source frame 2 (<b>1103</b>)) recorded at 30 fps. The two original video frames representing 2/30 seconds of the video. Item <b>1104</b> represents three video frames (<b>1105</b>-<b>1107</b>) to be interpolated with the two original video frames at uniform frame interpolation of 4&#xd7; using the related art method of interpolation. The three video frames (<b>1105</b>-<b>1107</b>) represent 4/30 seconds of the video. As shown in the <figref idref="DRAWINGS">FIG. <b>11</b>B</figref>, in related art methods all the frames are interpolated at the uniform rate. In this example, a 30 fps video is being converted into 120 fps video. Interpolated frame sequence corresponds to 4/30s.</p><p id="p-0171" num="0160">By contrast, the sectionally interpolated segments are obtained from the masked interpolation engine (<b>118</b>) as shown in the <figref idref="DRAWINGS">FIG. <b>12</b>A</figref>. A different number of sectionally interpolated sectional frames are created for different objects or object clusters. After that, segments of sectional source frames are placed on the grid separated by a distance of sectional interpolation rate as shown in the <figref idref="DRAWINGS">FIG. <b>12</b>B</figref>. After this, sectionally interpolated frames are placed on the grids as shown in <figref idref="DRAWINGS">FIGS. <b>12</b>C-<b>12</b>D</figref>. Various smoothening techniques such as Gaussian smoothening or ML model based smoothening may also be applied after sectional components are arranged on the grid to reduce the boundary artifacts. After completion of grids, the frames are then combined and compressed to form the sectional slow motion video.</p><p id="p-0172" num="0161"><b>13</b>A illustrates an example scenario of generating interpolation segments corresponding to the object, according to an embodiment. Consider an example of a car and a human moving together shown in <b>1301</b>. Assume a slow motion at a rate of 4&#xd7; for car and 2&#xd7; for the human. Consider, a length of an initial video clip in which the car and the human coexist is of duration 2s at 32 fps. The electronic device (<b>100</b>) creates a video clip of duration of 4s with a cutoff corresponding to man because man is slow motioned at lower rate. The electronic device (<b>100</b>) generates the section slow motion effect for components where two object clusters not interacting with each other within that time duration. So there is no chance of inconsistency. Consider, the 4 source frames shown in <b>1302</b>. The green color segments in each source frame represent the portion of human in the video frame. The red color segments in each source frame represent the portion of car in the video frame. In this example, these 4 source frames are used to interpolate the output shown in <b>1303</b> and <b>1304</b>. In <b>1303</b>, the electronic device (<b>100</b>) generates 8 interpolated segments corresponding to the portion of the human in the video frame. In <b>1304</b>, the electronic device (<b>100</b>) generates 16 interpolated segments corresponding to the portion of the car in the video frame.</p><p id="p-0173" num="0162"><figref idref="DRAWINGS">FIG. <b>13</b>B</figref> illustrates an example scenario of generating sectional slow motion clips, according to an embodiment. The red color and green color blocks in <figref idref="DRAWINGS">FIG. <b>13</b>B</figref> represent the section of the car and the human in the video frames respectively in <b>1301</b> of <figref idref="DRAWINGS">FIG. <b>13</b>A</figref>. Consider, two object clusters (i.e., cluster <b>1</b> is human and cluster <b>2</b> is car) were interacting at a point. Then for a time &#x394;t1, the two object clusters are not interacting. Further, the two object clusters came together again to interact after the time &#x394;t1 as shown in <b>1306</b>. Consider, these two object clusters are again separated for a time &#x394;t2 and interact again after the time &#x394;t2 as shown in <b>1308</b>.</p><p id="p-0174" num="0163">Initially, the electronic device (<b>100</b>) detects the interaction points of different object clusters in the video frame. Further, the electronic device (<b>100</b>) produces the sectional SloMo video clips corresponding to the time interval for which these object clusters are not interacting as shown in <b>1305</b> and <b>1307</b>. The electronic device (<b>100</b>) generates different sectional SloMo clips for different time clusters for which these two object clusters are not interacting, i.e., for &#x394;t1 and &#x394;t2.</p><p id="p-0175" num="0164"><figref idref="DRAWINGS">FIGS. <b>14</b>-<b>15</b>B</figref> illustrate example scenarios for explaining inconsistency in interpolation, according to an embodiment. Consider, a person is moving slower than a ball in the video frame as shown in <b>1401</b>-<b>1404</b> of <figref idref="DRAWINGS">FIG. <b>14</b></figref>. Only one frame is displayed at time t=26 ms. That frame is neither N=2 related frame, nor is N=4 related frame. It is a hybrid of both the frames. So, for this new generated hybrid frame, the fast moving object which is slow motioned at a higher rate is taken from N=2. The slow moving object (i.e., object slow motioned at slower rate) uses the section from N=4.</p><p id="p-0176" num="0165">Six video frames of a video of the human kicking the ball to a goalpost are shown in the <figref idref="DRAWINGS">FIG. <b>15</b>A</figref>. At frame N=1, the human kicks the ball. At frame N=2, the ball is in air. At frame N=3, the ball hits goalpost. At frame N=4, the human runs toward the goalpost. At frame N=5, the human runs toward the goalpost which is far away from the human. At frame N=6, the human runs toward the goalpost which is near to the human.</p><p id="p-0177" num="0166">The interpolation of the six video frames in the <figref idref="DRAWINGS">FIG. <b>15</b>A</figref> are shown in the <figref idref="DRAWINGS">FIG. <b>15</b>B</figref>. Item <b>1501</b> represents the interpolated frames after the frame N=1. Item <b>1502</b> represents the interpolated frames after the frame N=2. Item <b>1503</b> represents the interpolated frames after the frame N=3. Item <b>1504</b> represents the interpolated frames after the frame N=4. Item <b>1505</b> represents the interpolated frames after the frame N=5. The electronic device (<b>100</b>) creates a sectional slomo video with the ball at 4&#xd7; and the human at 2&#xd7;. At the interpolation stage, the output from the masked interpolation engine (<b>118</b>) consist of 11 output frames (N&#x2032;=1 to N&#x2032;=11). The two type of arrows in <figref idref="DRAWINGS">FIG. <b>15</b>B</figref> indicate that information for any output frame (e.g., N&#x2032;=8) is taken from two frames. In an example, for creating the frame N=8, the electronic device (<b>100</b>) takes information from frames (2,3) and (4,2), where frame (2,3) contributes for the motion of ball and frame (4,2) contributes for motion of the human.</p><p id="p-0178" num="0167">For creating the output frame N&#x2032; (e.g., N&#x2032;=8), the information from multiple intermediate interpolated frames (e.g., (1,1), (1,2), (1,3) etc.) are taken by the electronic device (<b>100</b>). In case of the frame N&#x2032;=8, the frame N=8 consists of outputs from frames (2,3) and (4,2). Here, a section of frame belonging to ball is taken from (2,3) and a section of frame belonging to the human is taken from (4,2). The rest of the background remains the same. The electronic device (<b>100</b>) further uses frames until N=4 from the input frames to output sectional component of output frame (i.e., last output frame N&#x2032;=11 takes sections from (3,2) that uses information from input frames N=3 and N=4) and (6,0). Therefore, in the output video, the ball is not touching the goalpost. The output frame of the sectional slow motioned video includes different sections of objects at different interpolation rates belonging to different instances of time, which is the desired effect.</p><p id="p-0179" num="0168"><figref idref="DRAWINGS">FIG. <b>16</b></figref> illustrates an example scenario of generating the multiple sectional slow motion effects in the video, according to an embodiment. Consider, a video of a street is recorded by the electronic device (<b>100</b>). At <b>1601</b>, the electronic device (<b>100</b>) decodes the video to multiple video frames and filters out the best video frames. At <b>1602</b>, the electronic device (<b>100</b>) selects a video frame from the filtered video frames, where the video frame illustrates a scene in the street. A car (<b>1604</b>) is moving faster, a boy (<b>1605</b>) is cycling with a medium speed, and a kid (<b>1603</b>) is moving forward slowly with a skateboard are the objects and the motion information of the objects in the scene. At <b>1606</b>, the electronic device (<b>100</b>) segments the objects (<b>1603</b>-<b>1605</b>) in the video frame and clusters the segmented objects based on the interaction between the segmented objects.</p><p id="p-0180" num="0169">At <b>1607</b>, the electronic device (<b>100</b>) assigns a significance score to each of the segmented objects and each section of the video frame is made to have a different sectional flow rate based on the significance score of each section. In the example, the electronic device (<b>100</b>) sets the interpolation rate of the car (<b>1604</b>), the boy (<b>1605</b>) and the kid as 2&#xd7;, 4&#xd7; and 8&#xd7; respectively. Since the kid (<b>1603</b>) with the skateboard is in the front portion of the video at relatively lesser depth and is also looking aesthetically pleasing to the human eye, the electronic device (<b>100</b>) sets the interpolation rate of the kid (<b>1603</b>) as 8&#xd7;. Since the boy (<b>1605</b>) with cycle is in the rear or at much larger depth and not in focus hence intuitively it is of lesser importance, the electronic device (<b>100</b>) sets the interpolation rate of the boy (<b>1605</b>) as 4&#xd7;. Since, the car (<b>1604</b>) in this video is moving at much slower rate, the electronic device (<b>100</b>) sets the interpolation rate of the car (<b>1604</b>) as 2&#xd7;. At <b>1608</b>, the electronic device (<b>100</b>) interpolates the sections of the video containing the objects (<b>1603</b>-<b>1605</b>) according to the interpolation rate of the objects (<b>1603</b>-<b>1605</b>). Further, the electronic device (<b>100</b>) generates the sectional slow motion video clip by encoding the interpolated video frames.</p><p id="p-0181" num="0170"><figref idref="DRAWINGS">FIG. <b>17</b></figref> illustrates an example scenario of generating the multiple sectional slow motion effects for a specific duration in the video, according to an embodiment. At <b>1701</b>, consider a video of a boys (<b>1702</b>) playing with waves (<b>1703</b>) at a beach is recorded by the electronic device (<b>100</b>). In response to detecting the video, the electronic device (<b>100</b>) decodes the video to multiple video frames. Further, the electronic device (<b>100</b>) determines a start point and an end point for the sectional slow motion video based on a content information and relative flow rate of objects (i.e., boys (<b>1702</b>) and waves (<b>703</b>)) in the video frames. The electronic device (<b>100</b>) determines the significant score of the objects in each frame.</p><p id="p-0182" num="0171">For the first 3 seconds (<b>1706</b>) of the video, the electronic device (<b>100</b>) predicts same significance score (<b>1704</b>) for the wave and the boys. At <b>1710</b>, hence the electronic device (<b>100</b>) selects a uniform video frame interpolation at 2&#xd7; (<b>1708</b>) for the first 3 seconds of the video. For a subsequent section (<b>1707</b>) of the video starts from the 3.1 second (i.e., at second 3.1) and ends on the 9th second (i.e., at second 9) of the video, the electronic device (<b>100</b>) predicts different significance scores (<b>1705</b>) for the wave and the boy. At <b>1710</b>, therefore the electronic device (<b>100</b>) takes the video frames from 3.1 second to 9 second of the video as the start point and the end point of the sectional slow motion video respectively. The significance score of the wave is higher than the significance score of the kids, therefore, the electronic device (<b>100</b>) interpolates the section in the video containing the waves at 8&#xd7; and the section in the video containing the boys at 2&#xd7; while generating the sectional slow motion video. Thus, the method according to an embodiment enhances a user experience by automatically identifying the start and end points for the sectional slow motion video from the recorded videos, and producing a sectional slow motion video using the start and end points that are identified.</p><p id="p-0183" num="0172"><figref idref="DRAWINGS">FIG. <b>18</b></figref> illustrates an example scenario of selecting an animated clip based on a maximum significance score for generating the multiple sectional slow motion effects for in the video, according to an embodiment. At <b>1801</b>, consider the electronic device (<b>100</b>) stores a video of a scenery that includes a river (<b>1804</b>), a mountain (<b>1803</b>) and a sky (<b>1802</b>). The electronic device (<b>100</b>) receives an instruction (e.g., input by a user) to create the slow motion video of the scenery by including an appropriate animated clip in background of the video. In response to receiving the instruction, the electronic device (<b>100</b>) determines the significance score of the video frame of the video of the scenery. At <b>1805</b>, the electronic device (<b>100</b>) checks for the animated clip present in a repository of the electronic device (<b>100</b>) or in a cloud dataset. Further, the electronic device (<b>100</b>) identifies that the cloud animated clip (<b>1806</b>) and a fire animated clip (<b>1807</b>) are the animated clips present in the repository or the cloud dataset.</p><p id="p-0184" num="0173">At <b>1808</b>, the electronic device (<b>100</b>) determines a total significance score by adding the significance score of each animated clip with the significance score of the video frame. The electronic device (<b>100</b>) detects that the cloud animated clip (<b>1806</b>) and the video frame together gives a maximum of the total significance score (i.e., <b>1</b>). At <b>1809</b>, hence the electronic device (<b>100</b>) chooses the cloud animated clip (<b>1806</b>) to overlay on objects in the video frame. At <b>1810</b>, the electronic device (<b>100</b>) resizes pixels of the cloud animated clip (<b>1806</b>) and overlays the cloud animated clip (<b>1806</b>) on the scenery in the video. A movement of clouds in the in the cloud animated clip (<b>1806</b>) is relatively less than a movement of the river (<b>1804</b>) in the video. At <b>1811</b>, the electronic device (<b>100</b>) performs masked interpolation on the cloud animated clip (<b>1806</b>) and the river (<b>1804</b>) by sectionally slow motioned the cloud animated clip (<b>1806</b>) and the river (<b>1804</b>) at different interpolation rates to introduce a cinematographic experience in the slow motion video.</p><p id="p-0185" num="0174"><figref idref="DRAWINGS">FIG. <b>19</b></figref> illustrates an example scenario of early triggering for recording the sectional slow motion video using the multi-camera module, according to an embodiment. At <b>1901</b>, consider the user is recording wide angle video of a scene includes apartments, a river and clouds using the electronic device (<b>100</b>), where <b>1902</b> and <b>1903</b> represent a field of view (FOV) of the wide angle camera and the normal (i.e., non-wide angle) camera of the electronic device (<b>100</b>), respectively. Consider, a bird (<b>1904</b>) (i.e., a new object) enters to the FOV of the wide angle camera. The electronic device (<b>100</b>) determines the significance score of the video frame that includes the city and the bird. At <b>1905</b>, in response to determining that the significance score of the video frame is greater than a threshold significance score, the electronic device (<b>100</b>) prepares to trigger the normal camera for recording the sectional slow motion video once the bird (<b>1904</b>) enters to the FOV of the normal camera.</p><p id="p-0186" num="0175">At <b>1906</b>, in response to detecting an entry of the bird (<b>1904</b>) to the FOV of the normal camera as shown in <b>1907</b>, the electronic device (<b>100</b>) triggers the normal camera to record the sectional slow motion video by interpolating a section containing the bird, a section containing the cloud, a section containing the river in the video frames at different interpolation rates based on the significance scores of the bird, the clouds, the river, respectively. The electronic device (<b>100</b>) continuously calculates the significance scores of the objects in video frame and records the sectional slow motion video until the significance scores of the objects in the video frame are below the threshold significance score. At <b>1908</b>, the bird is still moving in the FOV of the normal camera and hence the significance score of at least one object in the video frame is above the threshold significance score.</p><p id="p-0187" num="0176">At <b>1909</b>, the bird exits from the FOV of the normal camera and hence the significance scores of the objects in the video frame are lower than the threshold significance score. At <b>1910</b>, the electronic device (<b>100</b>) detects that the significance scores of the objects in the video frame are lower than the threshold significance score. At <b>1911</b>, in response to detecting that the significance scores of the objects in the video frame are lower than the threshold significance score, the electronic device (<b>100</b>) stops recording of the sectional slow motion video using the normal camera, and stores the sectional slow motion video to the memory (<b>120</b>).</p><p id="p-0188" num="0177">In an embodiment, the electronic device (<b>100</b>) may pause or stop recording of the video using the wide angle camera in response to detecting that the normal camera starts recording of the sectional slow motion video. Further, in response to detecting that the normal camera stops recording of the sectional slow motion video, the electronic device (<b>100</b>) may resume or start the recording of the video using the wide angle camera. In another embodiment, the electronic device (<b>100</b>) continuously records the video while the normal camera may start or stop recording of the sectional slow motion video.</p><p id="p-0189" num="0178">Values of various parameters mentioned in the example scenarios are given for understanding purposes, which may be different in real world scenarios.</p><p id="p-0190" num="0179">According to a first aspect of an embodiment, there is provided a method for generating multiple sectional slow motion effects in a video, the method comprising segmenting, by an electronic device (<b>100</b>), a plurality of objects in the video; determining, by the electronic device (<b>100</b>), an interaction between the segmented objects; determining, by the electronic device (<b>100</b>), a degree of slow motion effect to be applied to each segmented object in the video based on the interaction and a parameter associated with an importance of a scene in the video; generating, by the electronic device (<b>100</b>), a slow motion video by applying the degree of slow motion effect to that has been determined to corresponding segmented objects; and storing, by the electronic device (<b>100</b>), the slow motion video.</p><p id="p-0191" num="0180">According to a second aspect of an embodiment, the segmenting may comprise determining, by the electronic device (<b>100</b>), a class confidence score of each video frame of the video using a Machine Learning (ML) model, the class confidence score being a quantitative measurement of a quality of content in the video frame; filtering, by the electronic device (<b>100</b>), the video frames based on the class confidence scores; identifying, by the electronic device (<b>100</b>), the plurality of objects in the filtered video frames; estimating, by the electronic device (<b>100</b>), a depth score of each identified object in the filtered video frames based on depth information in the filtered video frames; determining, by the electronic device (<b>100</b>), an optical flow rate of each identified object in the filtered video frames, the optical flow rate being a quantitative measurement of a velocity of movement of pixels corresponding to the identified object in the video; and generating, by the electronic device (<b>100</b>), the segmented objects from the filtered video frames by applying the optical flow rate and the depth score of each identified object to a semantic segmentation network.</p><p id="p-0192" num="0181">According to any of the above aspects of an embodiment, the determining the interaction may comprises determining, by the electronic device (<b>100</b>), the interaction between the segmented objects for a time duration by applying the optical flow rate of each segmented object from the filtered video frames and the depth score of each segmented object to a memory based neural network.</p><p id="p-0193" num="0182">According to any of the above aspects of an embodiment, the determining the degree of slow motion effect may comprise clustering, by the electronic device (<b>100</b>), the segmented objects in each video frame of the video to form object clusters based on the interaction between the segmented objects; determining, by the electronic device (<b>100</b>), a significance score of each object cluster based a change in clustering of the segmented objects at different instants of time in the video and based on the parameter, the significance score being a measure of a relative importance of the object cluster in the video; and determining the degree of slow motion effect for each object based on the significance score for the object cluster which includes the object.</p><p id="p-0194" num="0183">According to any of the above aspects of an embodiment, the determining the significance score of each object cluster may comprise determining, by the electronic device (<b>100</b>), a cluster flow rate (&#x3b1;) of each segmented object in the object cluster, the cluster flow rate (&#x3b1;) being an average optical flow rate of the segmented objects in the object cluster; generating, by the electronic device (<b>100</b>), a hash table of a proximity change coefficient (&#x3b2;) based on the change in clustering of the segmented objects at the different instants of time in the video, the proximity change coefficient (&#x3b2;) being a number of times that the segmented object changes into and out of the object cluster within a time duration; estimating, by the electronic device (<b>100</b>), a rigidness score of each segmented object based on (a) a relative frequency of the objects occurring in the video and (b) a perceptual aesthetic quality of the video frame including the segmented object, the rigidness score being a measure of static aspects of an affinity for slow motion of the segmented object; and determining, by the electronic device (<b>100</b>), the significance score of each object cluster based on the rigidness score (a, b), and based on one or more of the cluster flow rate (&#x3b1;) and the proximity change coefficient (&#x3b2;).</p><p id="p-0195" num="0184">According to any of the above aspects of an embodiment, the generating the slow motion video may comprise determining, by the electronic device (<b>100</b>), an interpolation rate for each of a plurality of sections in the video frames that correspond to the object clusters, based on the significance score of the object cluster corresponding to the section; and interpolating, by the electronic device (<b>100</b>), the object clusters in each video frame based on the interpolation rate of the corresponding sections.</p><p id="p-0196" num="0185">According to any of the above aspects of an embodiment, the electronic device (<b>100</b>) may categorize the segmented objects to one of an interacting class in response to determining the interaction between the segmented objects, and a non-interacting class in response to not determining the interaction between the segmented objects.</p><p id="p-0197" num="0186">According to any of the above aspects of an embodiment, the memory based neural network outputs a probability score indicating whether the segmented objects are interacting.</p><p id="p-0198" num="0187">According to any of the above aspects of an embodiment, the rigidness score may be estimated using a weighted combination of the relative frequency of the objects occurring in the video (a) and the perceptual aesthetic quality of the video frame (b), weights thereof being determined using regression analysis.</p><p id="p-0199" num="0188">According to any of the above aspects of an embodiment, the perceptual aesthetic quality of the video frame (b) may be determined as a probability using a neural network that is trained on a dataset of images labeled with a score of aesthetic values between 0 and 1.</p><p id="p-0200" num="0189">According to a first aspect of another embodiment, there is provided an electronic device (<b>100</b>) for generating multiple sectional slow motion effects in a video, the electronic device (<b>100</b>) comprising a memory (<b>120</b>); a processor (<b>130</b>); and a sectional slow motion controller (<b>110</b>), coupled to the memory (<b>120</b>) and the processor (<b>130</b>), the sectional slow motion controller configured to perform operations comprising segmenting a plurality of objects in the video, determining an interaction between the segmented objects, determining a degree of slow motion effect to be applied to each segmented object in the video based on the interaction and a parameter associated with an importance of a scene in the video, generating a slow motion video by applying the degree of slow motion effect that has been determined to corresponding segmented objects, and storing the slow motion video.</p><p id="p-0201" num="0190">According to a second aspect of another embodiment, the segmenting may comprise determining a class confidence score of each video frame of the video using a Machine Learning (ML) model, the class confidence score being a quantitative measurement of a quality of content in the video frame; filtering the video frames based on the class confidence score; identifying the plurality of objects in the filtered video frames; estimating a depth score of each identified object from the filtered video frames based on depth information of each identified object in the filtered video frames; determining an optical flow rate of each identified object in the filtered video frames, the optical flow rate being a quantitative measurement of a velocity of movement of pixels corresponding to the identified object in the video; and generating the segmented objects from the filtered video frames by applying the optical flow rate and the depth score of each identified object to a semantic segmentation network.</p><p id="p-0202" num="0191">According to any of the above aspects of another embodiment, the determining may comprise determining the interaction between the segmented objects for a time duration by applying the optical flow rate of each segmented object from the filtered video frames and the depth score of each segmented object to a memory based neural network.</p><p id="p-0203" num="0192">According to any of the above aspects of another embodiment, the determining the degree of slow motion effect may comprise clustering the segmented objects in each video frame of the video to form object clusters based on the interaction between the segmented objects; determining a significance score of each object cluster based a change in clustering of the segmented objects at different instants of time in the video and based on the parameter, the significance score being a measure of a relative importance of the object cluster in the video; and determining the degree of slow motion effect for each object based on the significance score for the object cluster which includes the object.</p><p id="p-0204" num="0193">According to any of the above aspects of another embodiment, the determining the significance score may comprise determining a cluster flow rate (&#x3b1;) of each segmented object in the object cluster, the cluster flow rate (&#x3b1;) being an average optical flow rate of the segmented objects in the object cluster; generating a hash table of a proximity change coefficient (&#x3b2;) i based on the change in clustering of the segmented objects at the different instants of time in the video, the proximity change coefficient being a number of times that the segmented object changes into and out of the object cluster within a time duration; estimating a rigidness score of each segmented object based on a relative frequency of the objects occurring in the video (a) and a perceptual aesthetic quality of the video frame including the segmented object (b), the rigidness score being a measure of static aspects of an affinity for slow motion of the segmented object; and determining the significance score of each object cluster based on the rigidness score (a, b), and based on one or more of the cluster flow rate (&#x3b1;) and the proximity change coefficient (&#x3b2;).</p><p id="p-0205" num="0194">According to any of the above aspects of another embodiment, the generating the slow motion video may comprise determining an interpolation rate for each of a plurality of sections in the video frames that correspond to the object clusters, based on the significance score of the object cluster corresponding to the section; and interpolating the object cluster in each video frame based on the interpolation rate of the corresponding sections.</p><p id="p-0206" num="0195">According to any of the above aspects of another embodiment, the electronic device (<b>100</b>) may categorize the segmented objects to one of an interacting class in response to determining the interaction between the segmented objects, and a non-interacting class in response to not determining the interaction between the segmented objects.</p><p id="p-0207" num="0196">According to any of the above aspects of another embodiment, the memory based neural network outputs a probability score indicating whether that the segmented objects are interacting.</p><p id="p-0208" num="0197">According to any of the above aspects of another embodiment, the rigidness score is estimated using a weighted combination of the relative frequency of the objects occurring in the video (a) and the perceptual aesthetic quality of the video frame (b), weights thereof being determined using regression analysis.</p><p id="p-0209" num="0198">According to any of the above aspects of another embodiment, the perceptual aesthetic quality of the video frame (b) is determined as a probability using a neural network that is trained on a dataset of images labeled with a score of aesthetic values between 0 and 1.</p><p id="p-0210" num="0199">The foregoing description of the specific embodiments will so fully reveal the general nature of the embodiments herein that others may, by applying current knowledge, readily modify and/or adapt for various applications such specific embodiments without departing from the generic concept, and, therefore, such adaptations and modifications should and are intended to be comprehended within the meaning and range of equivalents of the disclosed embodiments. It is to be understood that the phraseology or terminology employed herein is for the purpose of description and not of limitation. Therefore, while the embodiments herein have been described in terms of various embodiments, those skilled in the art will recognize that the embodiments herein may be practiced with modification within the scope of the embodiments as described herein.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001" nb-file="US20230005505A1-20230105-M00001.NB"><img id="EMI-M00001" he="8.13mm" wi="76.20mm" file="US20230005505A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00002" nb-file="US20230005505A1-20230105-M00002.NB"><img id="EMI-M00002" he="8.47mm" wi="76.20mm" file="US20230005505A1-20230105-M00002.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00003 MATH-US-00003-2" nb-file="US20230005505A1-20230105-M00003.NB"><img id="EMI-M00003" he="15.83mm" wi="76.20mm" file="US20230005505A1-20230105-M00003.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00004" nb-file="US20230005505A1-20230105-M00004.NB"><img id="EMI-M00004" he="8.13mm" wi="76.20mm" file="US20230005505A1-20230105-M00004.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00005" nb-file="US20230005505A1-20230105-M00005.NB"><img id="EMI-M00005" he="7.37mm" wi="76.20mm" file="US20230005505A1-20230105-M00005.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00006" nb-file="US20230005505A1-20230105-M00006.NB"><img id="EMI-M00006" he="7.37mm" wi="76.20mm" file="US20230005505A1-20230105-M00006.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00007" nb-file="US20230005505A1-20230105-M00007.NB"><img id="EMI-M00007" he="7.37mm" wi="76.20mm" file="US20230005505A1-20230105-M00007.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00008" nb-file="US20230005505A1-20230105-M00008.NB"><img id="EMI-M00008" he="7.37mm" wi="76.20mm" file="US20230005505A1-20230105-M00008.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-math idrefs="MATH-US-00009" nb-file="US20230005505A1-20230105-M00009.NB"><img id="EMI-M00009" he="22.61mm" wi="76.20mm" file="US20230005505A1-20230105-M00009.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for generating a slow motion video, the method comprising:<claim-text>segmenting, by an electronic device, objects in the video;</claim-text><claim-text>determining, by the electronic device, an interaction between the segmented objects;</claim-text><claim-text>clustering, by the electronic device, the segmented objects in the video to generate object clusters based on the interaction;</claim-text><claim-text>determining, by the electronic device, a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters; and</claim-text><claim-text>generating, by the electronic device, the slow motion video by applying the degree of slow motion effect to that has been determined to corresponding the object clusters.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the segmenting of the objects in the video comprises:<claim-text>determining, by the electronic device, a class confidence score of each video frame of the video using a Machine Learning model, the class confidence score being a quantitative measurement of a quality of content in the video frame;</claim-text><claim-text>filtering, by the electronic device, the video frames based on the class confidence scores;</claim-text><claim-text>identifying, by the electronic device, the objects in the filtered video frames;</claim-text><claim-text>estimating, by the electronic device, a depth score of each of the identified objects in the filtered video frames based on depth information in the filtered video frames;</claim-text><claim-text>determining, by the electronic device, an optical flow rate of each of the identified objects in the filtered video frames, the optical flow rate being a quantitative measurement of a velocity of movement of pixels corresponding to the identified objects in the filtered video frames; and</claim-text><claim-text>generating, by the electronic device, the segmented objects from the filtered video frames by applying the optical flow rate and the depth score of each identified object to a semantic segmentation network.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method as claimed in <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the determining of the interaction comprises:<claim-text>determining, by the electronic device, the interaction between the segmented objects for a time duration by applying the optical flow rate and the depth score of each of the segmented objects to a memory based neural network.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method as claimed in <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the determining of the degree of slow motion effect comprises:<claim-text>determining, by the electronic device, the significance score of each of the object clusters, the significance score being a measure of a relative importance of each of the object clusters in the video; and</claim-text><claim-text>determining the degree of slow motion effect for each of the objects based on the corresponding the significance score for the object clusters.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method as claimed in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the determining of the significance score comprises:<claim-text>determining, by the electronic device, a cluster flow rate of each of the object clusters based on an optical flow rate of each segmented object in the corresponding object clusters;</claim-text><claim-text>generating, by the electronic device, a hash table of a proximity change coefficient &#x3b2; based on the change in clustering of the segmented objects at the different instants of time in the video, the proximity change coefficient &#x3b2; being a number of times that the segmented object changes into and out of the object clusters within a time duration; and</claim-text><claim-text>determining, by the electronic device, the significance score of each of the object clusters based on based on at least one of the cluster flow rate and the proximity change coefficient &#x3b2;.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method as claimed in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the determining of the significance score comprises:<claim-text>estimating, by the electronic device, a rigidness score of each of the segmented objects based on at least one of a relative frequency of the segmented objects occurring in the video, a perceptual aesthetic quality of the segmented objects and a degree of movement of the segmented objects based on the object classes of the segmented objects, the rigidness score being a measure of static aspects of an affinity for slow motion of the segmented object; and</claim-text><claim-text>determining, by the electronic device, the significance score of each of the object clusters based on the rigidness score.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method as claimed in <claim-ref idref="CLM-00004">claim 4</claim-ref>, wherein the generating of the slow motion video comprises:<claim-text>determining, by the electronic device, an interpolation rate for each of a plurality of sections in the video frames that correspond to the object clusters, based on the significance score of the object cluster corresponding to the section; and</claim-text><claim-text>interpolating, by the electronic device, the object clusters in each video frame based on the interpolation rate of the corresponding sections.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method as claimed in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the determining of the interaction comprises:<claim-text>categorizing, by the electronic device, the segmented objects to one of an interacting class in response to determining the interaction between the segmented objects, and a non-interacting class in response to not determining the interaction between the segmented objects.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method as claimed in <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the memory based neural network outputs a probability score indicating whether the segmented objects are interacting.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the rigidness score is estimated using a weighted combination of the relative frequency of the segmented objects occurring in the video, the perceptual aesthetic quality of the segmented objects, and the degree of movement of the segmented objects based on the object classes of the segmented objects, weights thereof being determined using regression analysis.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The method as claimed in <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the perceptual aesthetic quality of the segmented objects is determined as a probability using a neural network that is trained on a dataset of images labeled with a score of aesthetic values between 0 and 1.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. An electronic device for generating a slow motion video, the electronic device comprising:<claim-text>a memory;</claim-text><claim-text>a processor; and</claim-text><claim-text>a sectional slow motion controller, coupled to the memory and the processor, the sectional slow motion controller configured to perform operations comprising:</claim-text><claim-text>segmenting objects in the video,</claim-text><claim-text>determining an interaction between the segmented objects,</claim-text><claim-text>clustering the segmented objects in the video to generate object clusters based on the interaction,</claim-text><claim-text>determining a degree of slow motion effect to be applied to each of the object clusters in the video based on a significance score of each of the object clusters,</claim-text><claim-text>generating the slow motion video by applying the degree of slow motion effect that has been determined to corresponding the object clusters.</claim-text></claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The electronic device as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the segmenting of the objects comprises:<claim-text>determining a class confidence score of each video frame of the video using a Machine Learning model, the class confidence score being a quantitative measurement of a quality of content in the video frame;</claim-text><claim-text>filtering the video frames based on the class confidence scores;</claim-text><claim-text>identifying the objects in the filtered video frames;</claim-text><claim-text>estimating a depth score of each of the identified objects in the filtered video frames based on depth information in the filtered video frames;</claim-text><claim-text>determining an optical flow rate of each of the identified objects in the filtered video frames, the optical flow rate being a quantitative measurement of a velocity of movement of pixels corresponding to the identified objects in the filtered video frames; and</claim-text><claim-text>generating the segmented objects from the filtered video frames by applying the optical flow rate and the depth score of each identified object to a semantic segmentation network.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The electronic device as claimed in <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the determining of the interaction comprises:<claim-text>determining the interaction between the segmented objects for a time duration by applying the optical flow rate and the depth score of each of the segmented objects to a memory based neural network.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. A non-transitory computer-readable storage medium, having a computer program stored thereon that performs, when executed by a processor, the method according to <claim-ref idref="CLM-00001">claim 1</claim-ref>.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The electronic device as claimed in <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the determining of the degree of slow motion effect comprises:<claim-text>determining the significance score of each of the object clusters, the significance score being a measure of a relative importance of each of the object clusters in the video; and</claim-text><claim-text>determining the degree of slow motion effect for each of the objects based on the corresponding the significance score for the object clusters.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The electronic device as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the determining of the significance score comprises:<claim-text>determining a cluster flow rate of each of the object clusters based on an optical flow rate of each segmented object in the corresponding object clusters,</claim-text><claim-text>generating a hash table of a proximity change coefficient based on the change in clustering of the segmented objects at the different instants of time in the video, the proximity change coefficient being a number of times that the segmented object changes into and out of the object clusters within a time duration, and</claim-text><claim-text>determining the significance score of each of the object clusters based on based on at least one of the cluster flow rate and the proximity change coefficient.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The electronic device as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the determining of the significance score comprises:<claim-text>estimating, by the electronic device, a rigidness score of each of the segmented objects based on at least one of a relative frequency of the segmented objects occurring in the video, a perceptual aesthetic quality of the segmented objects and a degree of movement of the segmented objects based on the object classes of the segmented objects, the rigidness score being a measure of static aspects of an affinity for slow motion of the segmented object, and</claim-text><claim-text>determining the significance score of each of the object clusters based on the rigidness score.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The electronic device as claimed in <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the generating of the slow motion video comprises determining an interpolation rate for each of a plurality of sections in the video frames that correspond to the object clusters, based on the significance score of the object cluster corresponding to the section and interpolating the object clusters in each video frame based on the interpolation rate of the corresponding sections.</claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The electronic device as claimed in <claim-ref idref="CLM-00014">claim 14</claim-ref>, wherein the determining of the interaction comprises:<claim-text>categorizing the segmented objects to one of an interacting class in response to determining the interaction between the segmented objects, and a non-interacting class in response to not determining the interaction between the segmented objects.</claim-text></claim-text></claim></claims></us-patent-application>