<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230001575A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230001575</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17931194</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>16</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>B</section><class>25</class><subclass>J</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>9</main-group><subgroup>163</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>8</main-group><subgroup>40</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>B</section><class>25</class><subclass>J</subclass><main-group>19</main-group><subgroup>00</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>Y</section><class>10</class><subclass>S</subclass><main-group>901</main-group><subgroup>09</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">Robotic Control</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>17183688</doc-number><date>20210224</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11472026</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17931194</doc-number></document-id></child-doc></relation></continuation><continuation><relation><parent-doc><document-id><country>US</country><doc-number>15963630</doc-number><date>20180426</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>10940584</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17183688</doc-number></document-id></child-doc></relation></continuation></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>X Development LLC</orgname><address><city>Mountain View</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Coe</last-name><first-name>Sarah</first-name><address><city>San Francisco</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Wu</last-name><first-name>Yunchen</first-name><address><city>M&#xfc;nchen</city><country>DE</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for receiving, by one or more non-real-time processors, data defining a light illumination pattern for a robotic device. Generating, by the one or more non-real-time processors and based on the data, a spline that represents the light illumination pattern, where a knot vector of the spline defines a timing profile of the light illumination pattern. Providing the spline to one or more real-time processors of the robotic system. Calculating, by the one or more real-time processors, an illumination value from the spline at each of a plurality of time steps. Controlling, by the one or more real-time processors, illumination of a lighting display of the robotic system in accordance with the illumination value of the spline at each respective time step.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="104.14mm" wi="158.75mm" file="US20230001575A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="237.57mm" wi="168.66mm" orientation="landscape" file="US20230001575A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="181.95mm" wi="158.24mm" file="US20230001575A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="228.35mm" wi="158.92mm" file="US20230001575A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="229.70mm" wi="158.92mm" file="US20230001575A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="219.88mm" wi="152.06mm" file="US20230001575A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATION</heading><p id="p-0002" num="0001">This application is a continuation of U.S. patent application Ser. No. 17/183,688, filed Feb. 24, 2021, which is a continuation of U.S. patent application Ser. No. 15/963,630, filed Apr. 26, 2018, which are incorporated herein by reference in their entirety.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Autonomous devices, such as robotic devices, are capable of producing animated effects on light displays and coordinating motions with the animated effects. Present development techniques to drive such functions can be complex and may be suitable for inclusion in higher-level application development for robotic devices. Furthermore, translating high-level animations into hardware level control signals can be resource intensive and may require more computation resources that exceed those of control processors for some robotic devices.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0004" num="0003">In general, the disclosure relates to a process for controlling lighting displays and motion of a robotic device. More specifically, the disclosure relates to a process controlling lighting displays and motion of a robotic device based on high-level program code or animation files (e.g., Adobe After Effects&#xae; files). For example, a robot can translate high-level programing data into a basis spline, e.g., a piecewise polynomial parametric curve, that represents a light illumination pattern that changes over time. The light illumination pattern may represent an animated effect produced by changing light values of individual LEDs on the robot. The robot can calculate lighting values at particular time steps over a duration of the lighting pattern using the basis spline. The robot can then control individual lights on the robot in accordance with the lighting values. For example, a control processor in the robot can use the calculated lighting values to drive a lighting display to produce an animation defined by the basis spline. In some implementations, a knot vector of the spline defines a timing profile of the light illumination pattern. For example, the knot vector may be a mathematical representation of the variation (e.g., degree of variation) in colors, brightness, and position within a display of the lighting effects over time.</p><p id="p-0005" num="0004">The basis spline may serve as an efficient data structure for translating between high-level programing data and lower-level real-time control operations of the robot. For example, the basis spline provides an efficient transition between non-real time operations of the robot performed by a first processor and real-time control operations performed by a second processor.</p><p id="p-0006" num="0005">In some implementations, a robot can implement a similar process to control motion. For example, the robot can translate high-level programing data into a basis spline that represents a pattern of motion executed over a period of time. The motion pattern may represent a series of positions the robot's limb must traverse to execute the motion. The robot can calculate position values at particular time steps over motion pattern using the basis spline. The robot can then control a limb or limbs to execute the motion in accordance with the position values. For example, a control processor in the robot can use the calculated lighting values to drive one or more motors to produce motion pattern defined by the basis spline. In some implementations, a knot vector of the spline is exposed to define a timing profile. For example, the knot vector may be a mathematical representation of variations in position of a robotic limb within space over time and movement of the limb as it transitions between positions. For example, the knot vector may mathematically describe aspects of the motion such as velocity, acceleration, and jerk of the limb as it transitions between positions.</p><p id="p-0007" num="0006">In general, innovative aspects of the subject matter described in this specification can be embodied in methods that include the actions of receiving, by one or more non-real-time processors, data defining a light illumination pattern for a robotic device; generating, by the one or more non-real-time processors and based on the data, a spline that represents the light illumination pattern, where a knot vector of the spline defines a timing profile of the light illumination pattern; providing the spline to one or more real-time processors of the robotic system; calculating, by the one or more real-time processors, an illumination value from the spline at each of a plurality of time steps; and controlling, by the one or more real-time processors, illumination of a lighting display of the robotic system in accordance with the illumination value of the spline at each respective time step. Other implementations of this aspect include corresponding systems, apparatus, and computer programs, configured to perform the actions of the methods, encoded on computer storage devices. These and other implementations can each optionally include one or more of the following features.</p><p id="p-0008" num="0007">Some implementations include the steps of: receiving, by one or more non-real-time processors, data defining a motion pattern for a robotic system; generating, by the one or more non-real-time processors and based on the data, a second spline that represents the motion pattern, wherein a knot vector of the second spline defines a second timing profile that represents timing of the motion pattern; synchronizing, by the one or more non-real-time processors, the timing profile of the light illumination pattern and the second timing profile; providing the second spline to one or more real-time processors; calculating, by the one or more real-time processors, a position value from the second spline at each of a plurality of time steps; and controlling, by the one or more real-time processors, a movement of the robotic device in accordance the position value of the second spline at each respective time step.</p><p id="p-0009" num="0008">In some implementations, the data defining the light illumination pattern is an animation file.</p><p id="p-0010" num="0009">In some implementations, the data defining the light illumination pattern includes programing code that describes the light illumination pattern by defining an illumination line segment for the lighting display.</p><p id="p-0011" num="0010">In some implementations, the programing code includes a start point and an end point for the line segment.</p><p id="p-0012" num="0011">In some implementations, the programing code includes an indication of a time dependent transition of the line segment.</p><p id="p-0013" num="0012">In some implementations, the knot vector of the spline mathematically represents the time dependent transition.</p><p id="p-0014" num="0013">In some implementations, the calculating an illumination value from the spline at each of a plurality of time steps includes determining that two or more illumination line segments overlap, and calculating illumination values for overlapping portions of the line segments based on blending colors in the overlapping portions.</p><p id="p-0015" num="0014">In some implementations, the plurality of time steps correspond with an operating frequency of the one or more real-time processors.</p><p id="p-0016" num="0015">Particular implementations of the subject matter described in this specification can be implemented so as to realize one or more of the following advantages. Implementations may provide standardized programing protocols that permit developers to seamlessly control detailed animation and motion aspects of robotic devices. For example, implementations provide a robotic system control architecture that can efficiently translate high-level programing data to specific control signals for driving robotic device displays and movements. Implementations may improve efficiency of complex robotic control operations. For example, implementations distribute processing operations between non-real-time and real-time processors to leverage translate high-level program data into control signals to perform real-time control of robotic operations. As another example, implementations generate splines to describe complex robotic control and pass control information between non-real-time and real-time processors. The splines represent the control information in a highly efficient structure and reduce the complexity of data that is transferred between processors. Implementations make programing lighting motion robust to the number of LEDs on a given robotic device. For example, implementations can process high-level programing data that is agnostic to the specific number of LEDs on a given robotic device. Thus, such implementations permit portability of application code across a wide range of robotic devices without adversely affecting the execution of the code on any particular robotic device.</p><p id="p-0017" num="0016">The details of one or more embodiments of the subject matter described in this specification are set forth in the accompanying drawings and the description below. Other features, aspects, and advantages of the subject matter will become apparent from the description, the drawings, and the claims.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram that illustrates an example of a system architecture for controlling aspects of a robotic device.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a diagram showing different types of example light arrays for a robotic device.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram that illustrates an example light animation process according to implementations of the present disclosure.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a diagram that illustrates another example light animation process according to implementations of the present disclosure.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram that illustrates an example of a process for controlling aspects of a robotic device.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0023" num="0022">Like reference numbers and designations in the various drawings indicate like elements.</p><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a diagram that illustrates an example architecture of a system <b>100</b> for controlling functions of a robotic device <b>110</b>. The system <b>100</b> includes a non-real-time processing system <b>102</b>, a real-time processing system <b>104</b>, and one or more output component control systems <b>106</b>. As discussed in more detail below, the non-real-time processing system <b>102</b> receives high-level programing data, e.g., animation files or high-level program code (e.g., Java code), that defines a time dependent function of a robotic device <b>110</b>. Example time dependent functions include, but are not limited to, lighting illumination patterns that vary over time to create animated effects and patterns of motion for one or more joints or limbs of a robotic device <b>110</b>. The non-real-time processing system <b>102</b> converts the high-level programing data into a spline that represents the robotic device function defined by the high-level programing data. The spline serves as an efficient data structure for translating the high-level description of robotic functions into mathematical functions that can be evaluated and executed by the real-time processing system <b>104</b>. The real-time processing system <b>104</b> uses the spline to segment the robotic function represented by the spline into discrete time steps. The real-time processing system <b>104</b> calculates output control values for each time step that indicate a desired state of one of the robotic device's output components, e.g., a light array illumination value or a limb position value, during the time step in order to execute the function. The real-time processing system <b>104</b> controls the robotic device's output component control systems <b>106</b> to drive one of more output components, e.g., individual motors or lighting elements, in accordance with the calculated control values at each time step, thereby, executing the robotic function.</p><p id="p-0025" num="0024">The non-real-time processing system <b>102</b> can include, but is not limited to, a processing system of a robotic device <b>110</b> or a separate computing device such as a server system <b>112</b> or a user computing device <b>114</b>. User computing devices <b>114</b> can include, but are not limited to, desktop computers, laptop computers, tablet computers, or smartphones. The real-time processing system <b>104</b> includes one or more processors, controllers, or both that are part of a robotic device <b>110</b>. The real-time processing system <b>104</b> provides outputs that directly control the real-time functions of one or more output components of the robotic device <b>110</b>. The output component control systems <b>106</b> include interfaces, controllers, or both that drive output components of a robotic device <b>110</b>. For example, output component control systems can include, but are not limited to, light driver firmware <b>116</b> for driving a light array <b>118</b>, e.g., an LED array, on robotic device <b>110</b>, or motor controllers <b>120</b> for driving a system of motors <b>122</b>, e.g., to control the motion of a mechanical joint on a robotic device <b>110</b>.</p><p id="p-0026" num="0025">The non-real-time processing system <b>102</b> performs operations that generally do not control real-time functions of a robotic device <b>110</b>. For example, the non-real-time processing system <b>102</b> does not produce output data that is necessarily time synchronized with function of a robotic device <b>110</b>. For example, the non-real-time processing system <b>102</b> output data from the non-real-time processing system <b>102</b> can be stored in memory for use by the real-time processing system <b>104</b> when needed to execute a function of the robotic device <b>110</b>. In some examples, operations performed by the non-real-time processing system <b>102</b> can be considered as pre-processing operations. In the depicted example, the non-real-time processing system <b>102</b> includes an application program interface (API) <b>130</b>, a light spline converter <b>132</b>, a motion spline converter <b>134</b>, and a synchronizer <b>136</b>. In some examples, the API <b>130</b>, light spline converter <b>132</b>, motion spline converter <b>134</b>, and synchronizer <b>136</b> are provided as one or more computer executable programs, which can be executed by the non-real-time processing system <b>102</b>. In some implementations, some or all of the components of the non-real-time processing system <b>102</b> can be implemented in hardware, e.g., some or all of the components can be implemented in an field programmable gate array (FPGA) or an application specific integrated circuit (ASIC).</p><p id="p-0027" num="0026">In some examples, the API <b>130</b> is a component of the non-real-time processing system <b>102</b> that receives high-level programing data for defining functions for robotic devices <b>110</b>. For example, high-level programing data defining robotic functions can include animation files or high-level programing code that define robot lighting effects or motion. In some examples, the API <b>130</b> permits robotic program developers to control finely tuned robotic functions using high-level code, animation data files, or both. For example, the API <b>130</b> can provide programing instructions, definitions, protocols, subroutines, or a combination thereof that permit developers to define functions, e.g., movements and lighting animations, for robotic devices. In some examples, the API <b>130</b> provides rules or other criteria for defining robotic functions using animation data files, e.g., Adobe After Effects&#xae; files.</p><p id="p-0028" num="0027">In some examples, the API <b>130</b> defines a set of robotic function attributes that a developer can vary in order to control time dependent robotic functions such as lighting animation patterns and motion. In some examples, the robotic function definitions of the API <b>130</b> are agnostic to the type or characteristics of the particular hardware on individual robotic devices that perform the desired functions. In other words, the API <b>130</b> definitions are independent of hardware characteristics such as the type of lighting elements that on a particular robot (e.g., LED arrays vs. LCD array), the number of lighting elements in a light array of a particular type of robot, motor drive characteristics for motors that operate robotic joints, the number of motors a particular type of robot uses to move a joint, etc.</p><p id="p-0029" num="0028">In some examples, the high-level programing protocols and definitions provided by the API <b>130</b> can serve as standardized programing protocols for controlling robotic joints or limbs. For example, a limb motion can be represented by a series of positions for the limb that describe the desired motion or part of the desired motion. In addition, transitions between the positions represented by motion transitions types that describe how the limb should move from one position to another. Motion transition types can describe characteristics of the movement from position to position that include, but are not limited to, velocity, acceleration, and jerk. The motion transition type can indicate a mathematical interpolation of motion between joint positions, which can be represented by a spline.</p><p id="p-0030" num="0029">For example, <figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrate two example light arrays of a robotic device. The first light array is an example of a linear light array <b>200</b> and the second light array is an example of a ring light array <b>250</b>. Each of the light arrays is made up of an array of lighting elements <b>202</b>. The lighting elements <b>202</b> can be, but are not limited to, LEDs, LCD segments or pixels, organic LEDs (OLED), carbon nanotube display elements, or quantum dot display elements. Furthermore, various types of robotic devices may include light arrays of different sizes with different numbers of lighting elements <b>202</b>.</p><p id="p-0031" num="0030">In some examples, the high-level programing protocols and definitions provided by the API <b>130</b> can serve as standardized programing protocols for driving the light arrays. For example, the API <b>130</b> can represent a light array as an indexed line in high-level programing code. For example, each light array <b>200</b>, <b>250</b> is illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> with a set of line indices that are normalized to range from [0] to [1] to cover the entire array. For the linear array <b>200</b> the indices may range from [0] to [1] moving left to right across the array. For the ring array <b>250</b> the indices may begin and end at the six o'clock position.</p><p id="p-0032" num="0031">In some examples, the API <b>130</b> can permit a developer to control the light array by defining line segments to be illuminated using the indices. For example, light array animation can be control by allowing the developer to define a lighting pattern by controlling API provided variables associated with one or more illumination line segments of the array. Line segment variables can include, but are not limited to, line segment start and end points, a time duration for illumination, an illumination color represented by an RGB value, a color transition type, an illumination intensity transition type, a sequence transition type, and an opacity value.</p><p id="p-0033" num="0032">In some examples, a line segment for illumination is defined by its start and end point as represented by two indices. For example, a line segment [0.25]-[0.75] on the linear array <b>200</b> would extend across the middle half of the array. The same line segment (e.g., [0.25]-[0.75]) on the ring array <b>250</b> is ambiguous and could extend along the upper or lower half of the ring. Thus, indices for a ring array may include positive and negative values to indicate a direction around the array, e.g., counter clockwise may be positive values and clockwise may be negative values. For example, the line segment defined by [0.25]-[0.75] would represent the upper half of the ring array <b>250</b> and the line segment defined by [&#x2212;0.25]-[0.25] would represent the lower half of the ring array <b>250</b>.</p><p id="p-0034" num="0033">In some examples, the start and end point can each be associated with a different illumination color. In such examples, the color transition type can indicate how the color of the line segment should transition from the color of the start point to the color of the end point. Color transition types can include, but are not limited to, linear transitions, exponential transitions, and abrupt transitions, e.g., step-functions. The transition type can indicate a mathematical interpolation of the line segment colors between the start and end points, which can be represented by a spline. In some examples, the color transition of a line segment over time can be referred to as a color trajectory.</p><p id="p-0035" num="0034">In some examples, temporal aspects of the line segment (e.g., animations) can be represented by an illumination duration variable, an illumination intensity transitions type variable, or both. For example, a duration variable can indicate how long the line segment is to be illuminated. The duration variable can also be paired with a transition type. For example, the duration can indicate a period of time over which a transition of the line segment should elapse. The transition type can give the appearance that the light segment is fading in or fading out. For example, the transition variable may indicate that the line segment is to increase in brightness (e.g., illumination intensity) linearly over a duration of two seconds. Illumination intensity transitions types can include, but are not limited to, linear transitions, exponential transitions, and abrupt transitions, e.g., step-functions. The transition type can indicate a mathematical interpolation of the manner in which the line segment is to be illuminated, which can be represented by a spline.</p><p id="p-0036" num="0035">In some examples, the illumination intensity transitions and duration can indicate a temporal sequence for illuminating individual lighting elements in the light array. The temporal sequence can give the appearance that the line segment is moving along the array. For example, the illumination intensity transitions can indicate a timing sequence for illuminating individual lighting elements from the start point to the end point. The duration can indicate the total time over which the transition should take place. Illumination sequence transition types can include, but are not limited to, linear sequential transitions, exponential sequential transitions, and abrupt sequential transitions, e.g., step-functions. The sequential transition type can indicate a mathematical interpolation of a sequence for illuminating light elements along a line segment, which can be represented by a spline.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>3</b></figref> illustrates an example light animation process <b>300</b> in a series using a series of diagrams (<b>302</b>-<b>306</b>) of the ring array <b>250</b>. For example, a line segment of the ring array <b>250</b> may be defined by a start point, an endpoint of, a sequential transition type of a linear sequence, and a duration for the transition of two seconds. Each start and end point can be indicated by a pair of light array indices, e.g., to indicate the length of the line segment. Further, intermediate line segment points in addition to the startpoint and endpoint also may be defined. This high-level definition of a line segment describes an illumination pattern that illuminates each light element in the ring array <b>250</b> sequentially in a counter clockwise direction over the duration of two seconds. For instance, diagram <b>302</b> represents the ring array <b>250</b> at a first time with the line segment animation just beginning. In diagram <b>302</b>, light element <b>310</b> is illuminated in red (R) at the start point of the line segment (e.g., index [0, 0.033]). Diagram <b>304</b> represents the ring array <b>250</b> at a second time in the animation of the line segment (e.g., index [0.067, 0.167]. In diagram <b>304</b>, three light elements <b>312</b>-<b>316</b> are illuminated in red as the segment begins to traverse the ring array <b>250</b>. For example, the leading light element <b>312</b> may be in the process of turning on, the middle light element <b>314</b> may be fully illuminated, and the trailing light element <b>316</b> may be dimming as it is turned off. In some implementations, the sequential transition type also describes how each light element energizes over time (e.g., how quickly it is faded on or off). Finally, diagram <b>306</b> represents the ring array <b>250</b> at a third time in the animation of the line segment (e.g., index [0.567, 0.667]). In diagram <b>306</b>, three light elements <b>320</b>-<b>324</b> are illuminated in red as the segment continues to traverse the ring array <b>250</b> in the counterclockwise direction.</p><p id="p-0038" num="0037">As another example, a second line segment of the ring array <b>250</b> may be defined by a start point of [&#x2212;1], an end point of [0], a sequence transition type of a linear sequence, and a duration for the transition of two seconds. This high-level definition of a second line segment describes an illumination pattern that illuminates each light element in the ring array <b>250</b> sequentially in a clockwise direction over the duration of two seconds.</p><p id="p-0039" num="0038">In some implementations, line segment points greater than 1 can indicate multiple rotations around a ring array <b>250</b>. As another example, a third line segment of the ring array <b>250</b> may be defined by a start point of [0, 0.1], an end point of [4.9, 5], a sequence transition type of a linear sequence, and a duration for the transition of two seconds. This high-level definition of a second line segment describes an illumination pattern that illuminates each light element in the ring array <b>250</b> sequentially in a counter clockwise direction for five revolutions around the array over the duration of two seconds.</p><p id="p-0040" num="0039">Referring again to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, in some implementations, the non-real-time processing system <b>102</b> preprocesses animation files. For example, some animation files represent time as a dependent variable in functions defining the animation. This can create excessive processing overhead, processing errors, or both when converting a robot lighting animation or robot motion defined in the animation file into a spline. In such implementations, the non-real-time processing system <b>102</b> can convert the time related variables in the animation file from dependent variables to independent variables. For example, the non-real-time processing system <b>102</b> can read the animation file and approximate a timing profile for the robotic function. Using an Adobe After Effects&#xae; Butterfly file as an example, the non-real-time processing system <b>102</b> can approximate a series of cubic Bezier curves defined in the file into a single spatial spline with knot vectors that represent the timing profile of the animation, and, by extension, the desired robotic function. For example, the non-real-time processing system <b>102</b> can use a linear least-squares regression to minimize errors in control points generated by the approximation. The non-real-time processing system <b>102</b> can increase the number of knots in the knot vector and recalculate the regression if the control point error exceeds a predefined error threshold.</p><p id="p-0041" num="0040">In some implementations, the API <b>130</b> can directly expose spline generation primitives. For example, the API <b>130</b> can provide programing protocols that permit a developer to specify robotic functions by providing a set of spline control points and knots. In such implementations, the spline converters <b>132</b>/<b>134</b> can pass the spline control points and knot vector to an appropriate one of the spline trajectory controllers <b>138</b>/<b>144</b> of the real-time processing system <b>104</b>. In some implementations, the spline converters <b>132</b>/<b>134</b> can check developer defined spline control points and knots for errors prior to passing them to the real-time processing system <b>104</b>.</p><p id="p-0042" num="0041">The light spline converter <b>132</b> generates a lighting spline <b>133</b> that represents a light illumination pattern defined by the high-level programing data. For example, the light spline converter <b>132</b> can translate the high-level programing data into a basis spline, e.g., a third-order basis spline, which represents changes in the light illumination pattern over time. For example, the light spline converter <b>132</b> can determine a timing profile for the light illumination pattern based on attributes of the illumination pattern including, but not limited to, the color transition type, the illumination intensity transitions type, and the sequence transition type. The light spline converter <b>132</b> can represent the timing profile as a set of piecewise functions that mathematically describe the desired lighting animation. In other words, light spline converter <b>132</b> can generate the timing profile based on high-level programing code that defines the lighting pattern. For example, the timing profile can be determined based on the lighting pattern transitions defined by the programing code. The timing profile may represent, e.g., the timing of changes in color, intensity, and illumination sequences for appropriate portions of one or more lighting displays on the robotic device. In some examples, the knot vector equations are a mathematical representation of the timing profile.</p><p id="p-0043" num="0042">In some examples, the light spline converter <b>132</b> can translate data from an animation file into a spline representation of lighting animation defined by the file. For example, the light spline converter <b>132</b> can extract key frames from the animation file and generate a spline based on interpolating transitions in a lighting pattern between the key frames.</p><p id="p-0044" num="0043">In some implementations, a knot vector of the lighting spline <b>133</b> can define the timing profile of the light illumination pattern to generate the animated effect. For example, the knot vector can serve as a mathematical representation of the variation in colors, brightness, and position (within a display) of the lighting effects as a function of time.</p><p id="p-0045" num="0044">The motion spline converter <b>134</b> generates a motion spline <b>135</b> that represents a motion pattern defined by the high-level programing data. For example, the motion spline converter <b>134</b> can translate the high-level programing data into a basis spline, e.g., a third-order basis spline, which represents a series of positions, which a robotic limb must traverse to execute the motion. For example, the motion spline converter <b>134</b> can determine a timing profile for the motion pattern based on the limb positions and motion transition types between positions. The motion spline converter <b>134</b> can represent the timing profile as a set of piecewise functions that mathematically describe the desired motion of the limb. In some examples, the motion spline converter <b>134</b> can translate data from an animation file into a motion spline <b>135</b> representation of lighting animation defined by the file. For example, the motion spline converter <b>134</b> can extract key frames from the animation file. The key frames may provide a series of discrete positions for the limb to execute the animation. The motion spline converter <b>134</b> can generate a motion spline <b>135</b> based on interpolating motion transitions between the key frames.</p><p id="p-0046" num="0045">In some implementations, a knot vector of the motion spline <b>135</b> can define the timing profile of the motion pattern. For example, the knot vector can serve as a mathematical representation of the variation in limb position and transitions between positions as a function of time.</p><p id="p-0047" num="0046">The synchronizer <b>136</b> synchronizes the timing of lighting and motion patterns for robotic functions that include both lighting and motion aspects. For example, the synchronizer <b>136</b> can synchronize light illumination and motion timing profiles between two splines to ensure that a robotic device's <b>110</b> motion and lighting patterns are executed properly. For instance, a particular animation sequence may require a robotic device to accelerate a limb faster than the motors of the device are capable of moving the limb. In such situations, the motion spline converter <b>134</b> can adjust the acceleration of the limb as represented by a motion spline <b>135</b> to a value that is within the capabilities of the robotic device's motors. In response, the synchronizer <b>136</b> can make similar adjustments to the lighting spline <b>133</b> to maintain synchronization between the desired motion and light animation.</p><p id="p-0048" num="0047">In some examples, a desired motion function for a robotic device <b>110</b> may require that a joint be moved to an initial starting position before beginning a motion. For example, a particular motion pattern for a limb may require the limb be driven to a starting position. If such a motion pattern is associated with a lighting animation, the synchronizer can delay the lighting animation while repositioning a limb to ensure that both the motion and the lighting animation begin at the same time. For example, the synchronizer <b>136</b> can insert or cause the light spline converter <b>132</b> to insert a delay period into the beginning of the lighting spline <b>133</b> that accounts for a period of time during which the limb is repositioned.</p><p id="p-0049" num="0048">The real-time processing system <b>104</b> controls the execution of robotic functions in real-time. For example, the real-time processing system <b>104</b> can include one or more control processors that process lighting and/or motions splines to control the illumination of light arrays and/or robot motion in real-time. In the depicted example, the real-time processing system <b>104</b> includes a lighting spline trajectory controller <b>138</b>, a light selection controller <b>140</b>, an, optional, reset controller <b>142</b>, a motion spline trajectory controller <b>144</b>, and a joint selection controller <b>146</b>. In some examples, the lighting spline trajectory controller <b>138</b>, light selection controller <b>140</b>, reset controller <b>142</b>, motion spline trajectory controller <b>144</b>, and joint selection controller <b>146</b> are provided as one or more computer executable programs, which can be executed by the real-time processing system <b>104</b>. In some examples, the real-time processing system <b>104</b> operates with a 250 Hz execution loop. In other words, instructions are processed and new commands or control signals are transmitted to output hardware of the robotic device <b>110</b> approximately every 4 ms. In other implementations, the real-time processing system <b>104</b> operates with a 1000 Hz to 100 Hz execution loop. In some implementations, some or all of the components of the real-time processing system <b>104</b> can be implemented in hardware, e.g., some or all of the components can be implemented in an FPGA or an ASIC.</p><p id="p-0050" num="0049">The lighting spline trajectory controller <b>138</b> receives lighting splines <b>133</b> from the non-real-time processing system <b>102</b> and determines discrete lighting values for each light element in a light array based on one or more lighting splines <b>133</b>. For example, the lighting spline trajectory controller <b>138</b> can calculate an illumination value for individual light elements of an array from a lighting spline <b>133</b>. In some examples, the spline may define lighting values for every light element in an array. That is the spline may evaluate to a zero illumination value for non-illuminated light elements. In some examples, the spline may only define lighting values for light elements that are within an illumination line segment, e.g., light elements that will be illuminated. In some examples, the spline may only define lighting values for key light elements of an illumination line segment of a light array. In such examples, the lighting spline trajectory controller <b>138</b> can interpolate illumination values for non-key light elements that are within the illuminated line segment from the illumination values of the key light elements. Illumination values can include, but are not limited to, a light intensity value that indicates a brightness level for a light element and a color value that indicates a color of a light element. The color value can be, e.g., an RGB (red-green-blue) array or an RGBW (red-green-blue-white) array.</p><p id="p-0051" num="0050">In some implementations, the lighting spline trajectory controller <b>138</b> calculates illumination values from the lighting spline <b>133</b> for light elements of a light array <b>118</b> at discrete time steps. In some examples, the time steps can correspond to the operating frequency of the real-time processing system <b>104</b>.</p><p id="p-0052" num="0051">In some implementations, the lighting spline trajectory controller <b>138</b> can process two or more light array line segments simultaneously. Multiple illumination line segments can be defined by multiple splines that share a knot vector and are processed concurrently. When multiple line segments overlap on a light array <b>118</b>, the lighting spline trajectory controller <b>138</b> can blend the colors of the overlapping line segments. For example, the lighting spline trajectory controller <b>138</b> can use alpha blending techniques to blend overlapping portions of line segments.</p><p id="p-0053" num="0052"><figref idref="DRAWINGS">FIG. <b>4</b></figref> provides an illustrative example of line segment blending. <figref idref="DRAWINGS">FIG. <b>4</b></figref> illustrates an example of a light animation and blending process <b>400</b> using a series of diagrams (<b>402</b>-<b>408</b>) of the linear array <b>200</b> (from <figref idref="DRAWINGS">FIG. <b>2</b></figref>). Each diagram represents the linear array as illuminated during a time step of the lighting spline trajectory controller <b>138</b>. Diagram <b>402</b> shows a red (R) line segment <b>410</b> traversing the linear array <b>200</b> in a left to right direction and a green (G) line segment <b>412</b> traversing the linear array <b>200</b> in a right to left direction. In diagram <b>404</b>, the red line segment <b>410</b> and the green line segment <b>412</b> begin to intersect at light elements <b>414</b>. The lighting spline trajectory controller <b>138</b> computes a yellow (Y) illumination value for light elements <b>414</b> by alpha blending illumination values computed for light elements <b>414</b> based on the lighting spline (or splines) that define the red line segment <b>410</b> and the green line segment <b>412</b>. In diagram <b>406</b>, the red line segment <b>410</b> and the green line segment <b>412</b> have passed each other as they traverse the linear array <b>200</b> in their respective directions. Thus, the green line segment <b>412</b> is now shown on the left side of the linear array <b>200</b> and the red line segment <b>410</b> is now shown on the right side of the linear array <b>200</b>. However, the two line segments still overlap at light element <b>416</b>, the illumination values of which are alpha blended to produce yellow. In diagram <b>408</b>, the red line segment <b>410</b> and the green line segment <b>412</b> no longer overlap; therefore, the lighting spline trajectory controller <b>138</b> no longer blends the line segments.</p><p id="p-0054" num="0053">Referring again to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the light selection controller <b>140</b> receives light element illumination values from the lighting spline trajectory controller <b>138</b> and provides control signals to the firmware <b>116</b> to drive the light arrays <b>118</b> on a robotic device <b>110</b>. For example, the light selection controller <b>140</b> receives illumination values from the lighting spline trajectory controller <b>138</b> during each time step. The light selection controller <b>140</b> parses the illumination values and can send control signals to firmware <b>116</b> to drive appropriate light elements of the light arrays <b>118</b>.</p><p id="p-0055" num="0054">Referring again to <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the reset controller <b>142</b> provides non-spline based control of the light arrays <b>118</b>. For example, the reset controller <b>142</b> can provide forced controls or manual controls for the light array <b>118</b>. The reset controller <b>142</b> can be used to reset one or more light arrays <b>118</b> by, e.g., turning all light elements off or to a predefined illumination value (e.g., white).</p><p id="p-0056" num="0055">The motion spline trajectory controller <b>144</b> receives motion splines <b>135</b> from the non-real-time processing system <b>102</b> and determines discrete position values for one or more limbs or joints of the robotic device <b>110</b>. For example, the motion spline trajectory controller <b>144</b> can calculate position values for individual limbs of the robotic device <b>110</b> from the motion spline <b>135</b>. In some implementations, the motion spline trajectory controller <b>144</b> calculates position values from the motion spline <b>135</b> for the robotic device <b>110</b> at discrete time steps. In some examples, the time steps can correspond to the operating frequency of the real-time processing system <b>104</b>.</p><p id="p-0057" num="0056">The joint selection controller <b>146</b> receives position values from the motion spline trajectory controller <b>144</b> and provides control signals to the motor controllers <b>120</b> to drive the motors <b>122</b> to move appropriate limbs of the robotic device <b>110</b>. For example, the joint selection controller <b>146</b> receives position values from the motion spline trajectory controller <b>144</b> during each time step. The joint selection controller <b>146</b> selects appropriate joints and motors to operate in order to move associated limbs from one position to another based on the position values. The joint selection controller <b>146</b> provides control signals to the motor controller <b>120</b> to drive the motors <b>122</b> that operate the robotic device's <b>110</b> limbs.</p><p id="p-0058" num="0057"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a flow diagram that illustrate a process <b>500</b> for controlling a robotic device or a robotic system. The process <b>500</b> can be performed by one or more processing systems. For example, a robotic system can include a non-real-time processing system and a real-time processing system. Both processing systems may reside on a single robotic device or the robotic system may include a robotic device with a real-time processing system in communication with a non-real-time processing system that resides on another computing device (e.g., server system or user commuting device). As discussed above, some of the operations may be performed by a non-real-time processing system and some operations may be performed by a real-time processing system. Each set of steps <b>502</b>-<b>508</b> and steps <b>510</b>-<b>516</b> represent separate sub-process that can be performed separately or together. Step <b>520</b> is performed to synchronize the sub-processes, e.g., when a light animation and motion need to be synchronized.</p><p id="p-0059" num="0058">The system receives data defining a light illumination pattern for a robotic device (<b>502</b>). For example, the light illumination pattern is a time dependent lighting pattern to produce an animated effect on a lighting display of the robotic device. The lighting display can include, e.g., one or more light arrays. The data can include high-level programing data that defines the light illumination pattern. For example, the data can be high-level programing code or an animation file. The data defines the characteristics of the lighting pattern including, but not limited to, illumination line segments, colors, opacity, illumination intensity, e.g., brightness, and lighting pattern transitions. Lighting pattern transitions can be time dependent or spatial. Lighting pattern transitions can include, but are not limited to, color transitions, illumination intensity transitions, and sequence transitions. Transitions can be defined by programing data indicating a desired transition type between states of the pattern and can include, but are not limited to, linear transitions, exponential transitions, geometric transitions, and abrupt transitions, e.g., step-functions.</p><p id="p-0060" num="0059">The system generates a lighting spline that represents the light illumination pattern (<b>504</b>). For example, the system can generate a spline that mathematically represents the light illumination pattern as defined by the data received in step <b>502</b>. For example, the system can translate the high-level programing data into a spline that mathematically defines the light illumination pattern over time. The system can determine a timing profile of the illumination pattern based on the high-level programing data and generate a spline with knot functions that represent the timing profile. For example, the system can generate the timing profile based on high-level programing code that defines the lighting pattern. For example, the timing profile can be determined based on the lighting pattern transitions defined by the programing code. The timing profile may represent the timing of changes in color, intensity, and illumination sequences for appropriate portions of one or more lighting displays on the robotic device. In some examples, the knot vector equations are a mathematical representation of the timing profile. As another example, the system extract key frames from the animation file and generate a spline based on interpolating transitions in the lighting pattern between the key frames.</p><p id="p-0061" num="0060">The system calculates illumination values from the lighting spline (<b>506</b>). For example, the system calculates illumination values for portions of the light array(s) at discrete time steps based on the lighting spline. The lighting values may include a color value and an intensity value. For example, the color value can be an RGB or an RGBW array. In some examples, the time steps correspond to an operation frequency of one or more real-time processors. For example, the system can calculate illumination values at intervals that correspond to an operational period of the one or more real-time processors (e.g., the inverse of the operation frequency).</p><p id="p-0062" num="0061">The system controls a lighting display on the robotic device to produce the lighting pattern (<b>508</b>). For example, the system can generate control signals for individual light elements of the light array(s) based on the illumination values. The system can provide the control signals to firmware that drives the light array(s) to produce a portion of illumination pattern during each time step.</p><p id="p-0063" num="0062">The system receives data defining a motion pattern for the robotic device (<b>510</b>). For example, the motion pattern is a time dependent pattern to control movement of a limb of the robotic device, a joint of the robotic device, movement of the robotic device itself, or a combination thereof. The data can include high-level programing data that defines the light illumination pattern. For example, the data can be high-level programing code or an animation file. The data defines the characteristics of the motion pattern including, but not limited to, positions of a limb, joint, or the robotic device and transitions between positions. For example, motion transitions can describe characteristics of the movement from position to position that include, but are not limited to, velocity, acceleration, and jerk. The motion transition can indicate a mathematical interpolation of motion between joint positions, which can be represented by a spline. Motion transitions can be defined by programing data indicating a desired transition type between positions of the motion pattern and can include, but are not limited to, linear transitions, exponential transitions, geometric transitions, and abrupt transitions, e.g., step-functions.</p><p id="p-0064" num="0063">The system generates a motion spline that represents the motion pattern (<b>512</b>). For example, the system can generate a spline that mathematically represents the motion pattern as defined by the data received in step <b>510</b>. For example, the system can translate the high-level programing data into a spline that mathematically defines the motion pattern over time. The system can determine a timing profile of the motion pattern based on the high-level programing data and generate a spline with knot functions that represent the timing profile. For example, the system can generate the timing profile based on high-level programing code that defines the motion pattern. For example, the timing profile can be determined based on the positions and the motion pattern transitions defined by the programing code. The timing profile may represent the timing of changes in position. In some examples, the knot vector equations are a mathematical representation of the timing profile. As another example, the system extracts key frames from the animation file and generate a spline based on interpolating transitions in the motion pattern between the key frames.</p><p id="p-0065" num="0064">The system calculates position values from the motion spline (<b>514</b>). For example, the system calculates position values for the robotic device at discrete time steps based on the motion spline. In some examples, the time steps correspond to an operation frequency of one or more real-time processors. For example, the system can calculate position values at intervals that correspond to an operational period of the one or more real-time processors (e.g., the inverse of the operation frequency).</p><p id="p-0066" num="0065">The system controls a movement of the robotic device (<b>516</b>). For example, the system can generate control signals for individual motors or actuators of the robotic device to produce the desired motion pattern. The system can provide the control signals to motor controllers and/or firmware that drives the limbs or joints of the robotic device.</p><p id="p-0067" num="0066">In some examples, the system synchronizes the lighting spline and the motion spline to synchronize the lighting animation with the motion of the robotic device (<b>520</b>). For example, the system can synchronize the timing of lighting and motion patterns for robotic functions that include both lighting and motion aspects. For example, the system can synchronize light illumination and motion timing profiles between two splines to ensure that a robotic device's motion and lighting patterns are executed in unison. For example, the system can adjust the timing of one or both of the splines to ensure that the lighting and motion patterns are performed by the robotic device in unison.</p><p id="p-0068" num="0067">Embodiments of the invention and all of the functional operations described in this specification may be implemented in digital electronic circuitry, or in computer software, firmware, or hardware, including the structures disclosed in this specification and their structural equivalents, or in combinations of one or more of them. Embodiments of the invention may be implemented as one or more computer program products, i.e., one or more modules of computer program instructions encoded on a computer-readable medium for execution by, or to control the operation of, data processing apparatus. The computer readable medium may be a non-transitory computer readable storage medium, a machine-readable storage device, a machine-readable storage substrate, a memory device, a composition of matter affecting a machine-readable propagated signal, or a combination of one or more of them. The term &#x201c;data processing apparatus&#x201d; encompasses all apparatus, devices, and machines for processing data, including by way of example a programmable processor, a computer, or multiple processors or computers. The apparatus may include, in addition to hardware, code that creates an execution environment for the computer program in question, e.g., code that constitutes processor firmware, a protocol stack, a database management system, an operating system, or a combination of one or more of them. A propagated signal is an artificially generated signal, e.g., a machine-generated electrical, optical, or electromagnetic signal that is generated to encode information for transmission to suitable receiver apparatus.</p><p id="p-0069" num="0068">A computer program (also known as a program, software, software application, script, or code) may be written in any form of programming language, including compiled or interpreted languages, and it may be deployed in any form, including as a stand-alone program or as a module, component, subroutine, or other unit suitable for use in a computing environment. A computer program does not necessarily correspond to a file in a file system. A program may be stored in a portion of a file that holds other programs or data (e.g., one or more scripts stored in a markup language document), in a single file dedicated to the program in question, or in multiple coordinated files (e.g., files that store one or more modules, sub programs, or portions of code). A computer program may be deployed to be executed on one computer or on multiple computers that are located at one site or distributed across multiple sites and interconnected by a communication network.</p><p id="p-0070" num="0069">The processes and logic flows described in this specification may be performed by one or more programmable processors executing one or more computer programs to perform functions by operating on input data and generating output. The processes and logic flows may also be performed by, and apparatus may also be implemented as, special purpose logic circuitry, e.g., an FPGA (field programmable gate array) or an ASIC (application specific integrated circuit).</p><p id="p-0071" num="0070">Processors suitable for the execution of a computer program include, by way of example, both general and special purpose microprocessors, and any one or more processors of any kind of digital computer. Generally, a processor will receive instructions and data from a read only memory or a random access memory or both. The essential elements of a computer are a processor for performing instructions and one or more memory devices for storing instructions and data. Generally, a computer will also include, or be operatively coupled to receive data from or transfer data to, or both, one or more mass storage devices for storing data, e.g., magnetic, magneto optical disks, or optical disks. However, a computer need not have such devices. Moreover, a computer may be embedded in another device, e.g., a tablet computer, a mobile telephone, a personal digital assistant (PDA), a mobile audio player, a Global Positioning System (GPS) receiver, to name just a few. Computer readable media suitable for storing computer program instructions and data include all forms of non-volatile memory, media, and memory devices, including by way of example semiconductor memory devices, e.g., EPROM, EEPROM, and flash memory devices; magnetic disks, e.g., internal hard disks or removable disks; magneto optical disks; and CD ROM and DVD-ROM disks. The processor and the memory may be supplemented by, or incorporated in, special purpose logic circuitry.</p><p id="p-0072" num="0071">To provide for interaction with a user, embodiments of the invention may be implemented on a computer having a display device, e.g., a CRT (cathode ray tube) or LCD (liquid crystal display) monitor, for displaying information to the user and a keyboard and a pointing device, e.g., a mouse or a trackball, by which the user may provide input to the computer. Other kinds of devices may be used to provide for interaction with a user as well; for example, feedback provided to the user may be any form of sensory feedback, e.g., visual feedback, auditory feedback, or tactile feedback; and input from the user may be received in any form, including acoustic, speech, or tactile input.</p><p id="p-0073" num="0072">Embodiments of the invention may be implemented in a computing system that includes a back end component, e.g., as a data server, or that includes a middleware component, e.g., an application server, or that includes a front end component, e.g., a client computer having a graphical user interface or a Web browser through which a user may interact with an implementation of the invention, or any combination of one or more such back end, middleware, or front end components. The components of the system may be interconnected by any form or medium of digital data communication, e.g., a communication network. Examples of communication networks include a local area network (&#x201c;LAN&#x201d;) and a wide area network (&#x201c;WAN&#x201d;), e.g., the Internet.</p><p id="p-0074" num="0073">The computing system may include clients and servers. A client and server are generally remote from each other and typically interact through a communication network. The relationship of client and server arises by virtue of computer programs running on the respective computers and having a client-server relationship to each other.</p><p id="p-0075" num="0074">While this specification contains many specifics, these should not be construed as limitations on the scope of the invention or of what may be claimed, but rather as descriptions of features specific to particular embodiments of the invention. Certain features that are described in this specification in the context of separate embodiments may also be implemented in combination in a single embodiment. Conversely, various features that are described in the context of a single embodiment may also be implemented in multiple embodiments separately or in any suitable subcombination. Moreover, although features may be described above as acting in certain combinations and even initially claimed as such, one or more features from a claimed combination may in some cases be excised from the combination, and the claimed combination may be directed to a subcombination or variation of a subcombination.</p><p id="p-0076" num="0075">Similarly, while operations are depicted in the drawings in a particular order, this should not be understood as requiring that such operations be performed in the particular order shown or in sequential order, or that all illustrated operations be performed, to achieve desirable results. In certain circumstances, multitasking and parallel processing may be advantageous. Moreover, the separation of various system components in the embodiments described above should not be understood as requiring such separation in all embodiments, and it should be understood that the described program components and systems may generally be integrated together in a single software product or packaged into multiple software products.</p><p id="p-0077" num="0076">In each instance where an HTML file is mentioned, other file types or formats may be substituted. For instance, an HTML file may be replaced by an XML, JSON, plain text, or other types of files. Moreover, where a table or hash table is mentioned, other data structures (such as spreadsheets, relational databases, or structured files) may be used.</p><p id="p-0078" num="0077">Thus, particular embodiments of the invention have been described. Other embodiments are within the scope of the following claims. For example, the actions recited in the claims may be performed in a different order and still achieve desirable results.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method of controlling a robotic system, the method comprising:<claim-text>receiving, by one or more processors of the robotic system, a spline that represents a motion pattern, wherein a knot vector of the spline defines a timing profile that represents timing of the motion pattern;</claim-text><claim-text>determining, by the one or more processors of the robotic system and based on the knot vector, a position value of the spline for each of a plurality of time steps; and</claim-text><claim-text>controlling, by the one or more processors of the robotic system, a movement of the robotic system in accordance with the position value of the spline determined for each of the plurality of time steps.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the spline is defined by an animation file.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the spline is defined by programing code describing the motion pattern by defining a series of positions for a component of the robotic system.</claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the programing code includes transition types indicating characteristics of how the robotic system should move the component between respective positions.</claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the programing code includes an indication of a time dependent transition between two respective position values of the spline.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the knot vector of the spline mathematically represents time dependent transitions between respective position values of the spline.</claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the knot vector of the spline mathematically represents characteristics of the movement of the robotic system between respective position values of the spline.</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the characteristics of the movement include one or more of a velocity, an acceleration, or a jerk of the robotic system.</claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising receiving a second spline that represents a lighting pattern synchronized with the motion pattern, and wherein the method further comprising controlling a lighting effect of the robotic system based on the second spline.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A robotic system comprising:<claim-text>a real-time processing system; and</claim-text><claim-text>one or more storage devices storing instructions that are operable, when executed</claim-text></claim-text><claim-text>by the real-time processing system, to cause real-time processing system to perform real-time</claim-text><claim-text>operations comprising:<claim-text>receiving a spline that represents a motion pattern, wherein a knot vector of the spline defines a timing profile that represents timing of the motion pattern;</claim-text><claim-text>determining, based on the knot vector, a position value of the spline for each of a plurality of time steps; and</claim-text><claim-text>controlling a movement of the robotic system in accordance with the position value of the spline determined for each of the plurality of time steps.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The robotic system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the spline is defined by an animation file.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The robotic system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the spline is defined by programing code describing the motion pattern by defining a series of positions for a component of the robotic system.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The robotic system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the programing code includes transition types indicating characteristics of how the robotic system should move the component between respective positions.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The robotic system of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the programing code includes an indication of a time dependent transition between two respective position values of the spline.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The robotic system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the knot vector of the spline mathematically represents time dependent transitions between respective position values of the spline.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The robotic system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, wherein the knot vector of the spline mathematically represents characteristics of the movement of the robotic system between respective position values of the spline.</claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The robotic system of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the characteristics of the movement include one or more of a velocity, an acceleration, or a jerk of the robotic system.</claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The robotic system of <claim-ref idref="CLM-00010">claim 10</claim-ref>, the operations further comprising receiving a second spline that represents a lighting pattern synchronized with the motion pattern, and wherein the operations further comprising controlling a lighting effect of the robotic system based on the second spline.</claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. One or more storage devices storing instructions that are operable, when executed by a real-time processing system, to cause real-time processing system to perform real-time</claim-text><claim-text>operations comprising:<claim-text>receiving a spline that represents a motion pattern, wherein a knot vector of the spline defines a timing profile that represents timing of the motion pattern;</claim-text><claim-text>determining, based on the knot vector, a position value of the spline for each of a plurality of time steps; and</claim-text><claim-text>controlling a movement of a robotic system in accordance with the position value of the spline determined for each of the plurality of time steps.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The one or more storage devices of <claim-ref idref="CLM-00019">claim 19</claim-ref>, the operations further comprising receiving a second spline that represents a lighting pattern synchronized with the motion pattern, and wherein the operations further comprising controlling a lighting effect of the robotic system based on the second spline.</claim-text></claim></claims></us-patent-application>