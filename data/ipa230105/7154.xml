<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230007155A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230007155</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17941177</doc-number><date>20220909</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><priority-claims><priority-claim sequence="01" kind="national"><country>JP</country><doc-number>2018-248373</doc-number><date>20181228</date></priority-claim></priority-claims><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>235</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2329</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23264</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>23254</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>2353</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232935</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20180801</date></cpc-version-indicator><section>H</section><class>04</class><subclass>N</subclass><main-group>5</main-group><subgroup>232941</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e61">NOTIFYING APPARATUS, IMAGE CAPTURING APPARATUS, NOTIFYING METHOD, IMAGE CAPTURING METHOD, AND STORAGE MEDIUM</invention-title><us-related-documents><division><relation><parent-doc><document-id><country>US</country><doc-number>16728434</doc-number><date>20191227</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11463623</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17941177</doc-number></document-id></child-doc></relation></division></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>CANON KABUSHIKI KAISHA</orgname><address><city>Tokyo</city><country>JP</country></address></addressbook><residence><country>JP</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Kubo</last-name><first-name>Yukiko</first-name><address><city>Yokohama-shi</city><country>JP</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Kaida</last-name><first-name>Hironori</first-name><address><city>Kawasaki-shi</city><country>JP</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">There is provided a notifying apparatus. A detecting unit detects a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time. A converting unit converts the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting. A notifying unit makes a notification of motion blur on the basis of the motion blur amount. The notifying unit changes a form of the notification in accordance with a magnitude of the motion blur amount.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="181.86mm" wi="131.83mm" file="US20230007155A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="230.21mm" wi="142.16mm" orientation="landscape" file="US20230007155A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="196.93mm" wi="133.86mm" file="US20230007155A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="235.71mm" wi="135.30mm" orientation="landscape" file="US20230007155A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="168.99mm" wi="107.78mm" file="US20230007155A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="204.30mm" wi="115.40mm" file="US20230007155A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="190.33mm" wi="126.75mm" file="US20230007155A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="208.70mm" wi="157.14mm" orientation="landscape" file="US20230007155A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="225.13mm" wi="158.33mm" file="US20230007155A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="120.31mm" wi="156.21mm" file="US20230007155A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="230.21mm" wi="142.16mm" orientation="landscape" file="US20230007155A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="204.39mm" wi="131.91mm" orientation="landscape" file="US20230007155A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="188.47mm" wi="111.25mm" file="US20230007155A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="238.93mm" wi="131.91mm" orientation="landscape" file="US20230007155A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="204.05mm" wi="109.47mm" file="US20230007155A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="233.34mm" wi="157.65mm" orientation="landscape" file="US20230007155A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="230.21mm" wi="142.16mm" orientation="landscape" file="US20230007155A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="238.93mm" wi="131.91mm" orientation="landscape" file="US20230007155A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="209.89mm" wi="110.66mm" file="US20230007155A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="225.13mm" wi="128.35mm" file="US20230007155A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00020" num="00020"><img id="EMI-D00020" he="154.60mm" wi="110.91mm" file="US20230007155A1-20230105-D00020.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00021" num="00021"><img id="EMI-D00021" he="179.24mm" wi="156.97mm" file="US20230007155A1-20230105-D00021.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00022" num="00022"><img id="EMI-D00022" he="232.75mm" wi="134.87mm" orientation="landscape" file="US20230007155A1-20230105-D00022.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00023" num="00023"><img id="EMI-D00023" he="214.29mm" wi="100.16mm" orientation="landscape" file="US20230007155A1-20230105-D00023.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application is a divisional of application Ser. No. 16/728,434, filed Dec. 27, 2019, the entire disclosure of which is hereby incorporated by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND OF THE INVENTION</heading><heading id="h-0003" level="1">Field of the Invention</heading><p id="p-0003" num="0002">The present invention relates to a notifying apparatus, an image capturing apparatus, a notifying method, an image capturing method, and a storage medium.</p><heading id="h-0004" level="1">Description of the Related Art</heading><p id="p-0004" num="0003">Recently, some commercial models of image capturing apparatuses such as digital still cameras are provided with a shooting mode called &#x201c;shutter speed priority mode&#x201d;. The shutter speed priority mode is a shooting mode in which the photographer sets a desired shutter speed, and the image capturing apparatus then automatically sets exposure setting values aside from the shutter speed, such as the aperture value, ISO sensitivity, and so on. The photographer can shoot an image at his or her preferred shutter speed by using the shutter speed priority mode in this manner. For example, setting a fast shutter speed before shooting an image and then shooting the image in the shutter speed priority mode makes it possible to shoot an image having little motion blur.</p><p id="p-0005" num="0004">Japanese Patent Laid-Open No. 2008-172667 discloses a technique that makes it possible for a photographer to visually confirm a motion region during preparatory shooting. &#x201c;Preparatory shooting&#x201d; refers to shooting carried out by the photographer to compose the shot, set the shooting conditions, and so on while looking at an electronic viewfinder or rear surface LCD of the image capturing apparatus. According to the technique disclosed in Japanese Patent Laid-Open No. 2008-172667, a motion region is detected between time-series images capturing during preparatory shooting, and that motion region is displayed in an emphasized manner.</p><p id="p-0006" num="0005">As described above, to shoot an image with little motion blur, it is necessary to shoot the image using a fast shutter speed. However, even if a fast shutter speed is set during preparatory shooting and the actual shooting is carried out using the shutter speed priority mode, there are situations where an object will be captured in a blurry state.</p><p id="p-0007" num="0006">For example, when shooting a footrace runner in the shutter speed priority mode so that little motion blur arises, the photographer estimates the speed at which the runner is moving during the preparatory shooting, and sets a shutter speed he or she thinks will result in little motion blur for the runner. However, it is difficult for the photographer to confirm whether or not motion blur is arising at the set shutter speed, even if he or she visually confirms the image displayed in an electronic viewfinder or rear surface LCD during preparatory shooting. Specifically, it is difficult for the photographer to visually confirm motion blur in small regions, such as the arms and legs of the runner, during the preparatory shooting. Furthermore, if the shutter speed differs between the actual shooting and the preparatory shooting, the motion blur will also be different between the actual shooting and the preparatory shooting, and it is therefore difficult to confirm motion blur in the actual shooting even when the image has been visually confirmed during the preparatory shooting. Thus when taking an actual shot at the set shutter speed, the runner will be blurred in the captured image if the shutter speed is too slow relative to the speed at which the runner is moving.</p><p id="p-0008" num="0007">Furthermore, in this conventional technique, it is not easy for the photographer to know how much motion blur will arise in the actual shooting in advance.</p><heading id="h-0005" level="1">SUMMARY OF THE INVENTION</heading><p id="p-0009" num="0008">Having been achieved in light of such circumstances, the present invention provides a technique that makes it easy for a photographer to know how much motion blur will arise in actual shooting in advance.</p><p id="p-0010" num="0009">According to a first aspect of the present invention, there is provided a notifying apparatus comprising at least one processor and/or at least one circuit which functions as: a detecting unit configured to detect a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time; a converting unit configured to convert the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting; and a notifying unit configured to make a notification of motion blur on the basis of the motion blur amount, wherein the notifying unit changes a form of the notification in accordance with a magnitude of the motion blur amount.</p><p id="p-0011" num="0010">According to a second aspect of the present invention, there is provided a notifying apparatus comprising at least one processor and/or at least one circuit which functions as: a detecting unit configured to detect, in a predetermined period, a motion amount of an image capturing apparatus carrying out first shooting that is carried out repeatedly at predetermined intervals of time; a converting unit configured to convert the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined period and an exposure time used in the second shooting; and a notifying unit configured to make a notification of motion blur on the basis of the motion blur amount, wherein the notifying unit changes a form of the notification in accordance with a magnitude of the motion blur amount.</p><p id="p-0012" num="0011">According to a third aspect of the present invention, there is provided a notifying apparatus comprising at least one processor and/or at least one circuit which functions as: a detecting unit configured to detect a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time; a converting unit configured to convert the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting; and a notifying unit configured to make a notification of motion blur on the basis of the motion blur amount, wherein the notifying unit changes a form of the notification in accordance with a divergence between the motion blur amount and a target motion blur amount.</p><p id="p-0013" num="0012">According to a fourth aspect of the present invention, there is provided an image capturing apparatus comprising: the notifying apparatus according to the first aspect; and an image sensor.</p><p id="p-0014" num="0013">According to a fifth aspect of the present invention, there is provided an image capturing apparatus comprising: the notifying apparatus according to the second aspect; and an image sensor.</p><p id="p-0015" num="0014">According to a sixth aspect of the present invention, there is provided an image capturing apparatus comprising: the notifying apparatus according to the third aspect; and an image sensor.</p><p id="p-0016" num="0015">According to a seventh aspect of the present invention, there is provided an image capturing apparatus comprising at least one processor and/or at least one circuit which functions as: an image capturing control unit configured to control an image sensor to carry out first shooting repeatedly at a predetermined framerate; a detecting unit configured to detect a motion amount of an object from an image obtained through the first shooting; a changing unit configured to change the predetermined framerate on the basis of a difference between the motion amount and a target motion blur amount; a converting unit configured to convert the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined framerate and an exposure time used in the second shooting; and a notifying unit configured to make a notification of motion blur on the basis of the motion blur amount.</p><p id="p-0017" num="0016">According to an eighth aspect of the present invention, there is provided a notifying method executed by a notifying apparatus, comprising: detecting a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount, wherein a form of the notification is changed in accordance with a magnitude of the motion blur amount.</p><p id="p-0018" num="0017">According to a ninth aspect of the present invention, there is provided a notifying method executed by a notifying apparatus, comprising: detecting, in a predetermined period, a motion amount of an image capturing apparatus carrying out first shooting that is carried out repeatedly at predetermined intervals of time; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined period and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount, wherein a form of the notification is changed in accordance with a magnitude of the motion blur amount.</p><p id="p-0019" num="0018">According to a tenth aspect of the present invention, there is provided a notifying method executed by a notifying apparatus, comprising: detecting a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount, wherein a form of the notification is changed in accordance with a divergence between the motion blur amount and a target motion blur amount.</p><p id="p-0020" num="0019">According to an eleventh aspect of the present invention, there is provided an image capturing method executed by an image capturing apparatus, comprising: controlling an image sensor to carry out first shooting repeatedly at a predetermined framerate; detecting a motion amount of an object from an image obtained through the first shooting; changing the predetermined framerate on the basis of a difference between the motion amount and a target motion blur amount; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined framerate and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount.</p><p id="p-0021" num="0020">According to a twelfth aspect of the present invention, there is provided a non-transitory computer-readable storage medium which stores a program for causing a computer to execute a notifying method comprising: detecting a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount, wherein a form of the notification is changed in accordance with a magnitude of the motion blur amount.</p><p id="p-0022" num="0021">According to a thirteenth aspect of the present invention, there is provided a non-transitory computer-readable storage medium which stores a program for causing a computer to execute a notifying method comprising: detecting, in a predetermined period, a motion amount of an image capturing apparatus carrying out first shooting that is carried out repeatedly at predetermined intervals of time; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined period and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount, wherein a form of the notification is changed in accordance with a magnitude of the motion blur amount.</p><p id="p-0023" num="0022">According to a fourteenth aspect of the present invention, there is provided a non-transitory computer-readable storage medium which stores a program for causing a computer to execute a notifying method comprising: detecting a motion amount of an object from an image obtained through first shooting, the first shooting being carried out repeatedly at predetermined intervals of time; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined intervals of time and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount, wherein a form of the notification is changed in accordance with a divergence between the motion blur amount and a target motion blur amount.</p><p id="p-0024" num="0023">According to a fifteenth aspect of the present invention, there is provided a non-transitory computer-readable storage medium which stores a program for causing a computer to execute an image capturing method comprising: controlling an image sensor to carry out first shooting repeatedly at a predetermined framerate; detecting a motion amount of an object from an image obtained through the first shooting; changing the predetermined framerate on the basis of a difference between the motion amount and a target motion blur amount; converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined framerate and an exposure time used in the second shooting; and making a notification of motion blur on the basis of the motion blur amount.</p><p id="p-0025" num="0024">Further features of the present invention will become apparent from the following description of exemplary embodiments with reference to the attached drawings.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating the configuration of an image capturing apparatus <b>100</b> including a notifying apparatus.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a flowchart illustrating a shooting process according to a first embodiment.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a diagram illustrating an example of the configuration of a notification image generating unit <b>300</b> included in an image processing unit <b>107</b> according to the first embodiment.</p><p id="p-0029" num="0028"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a flowchart illustrating a process by which the notification image generating unit <b>300</b> generates a motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a diagram illustrating an example of a preparatory shooting image.</p><p id="p-0031" num="0030"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a diagram illustrating an example of motion vectors in a preparatory shooting image.</p><p id="p-0032" num="0031"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating a motion vector calculation process (step S<b>402</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0033" num="0032"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a diagram illustrating the motion vector calculation process (step S<b>402</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>).</p><p id="p-0034" num="0033"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating a motion vector in preparatory shooting, and motion blur, in actual shooting, converted from the motion vector in the preparatory shooting (converted motion blur).</p><p id="p-0035" num="0034"><figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>E</figref> are diagrams illustrating three examples of a motion blur notification image.</p><p id="p-0036" num="0035"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a histogram of converted motion blur having a predetermined value or higher in the scene illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram illustrating the configuration of an image capturing apparatus <b>1100</b> including a notifying apparatus.</p><p id="p-0038" num="0037"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a diagram illustrating an example of the configuration of a notification image generating unit <b>1200</b> included in an image processing unit <b>1102</b>.</p><p id="p-0039" num="0038"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a flowchart illustrating a process by which the notification image generating unit <b>1200</b> generates a motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0040" num="0039"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a diagram illustrating an example of the configuration of a notification image generating unit <b>1400</b> included in the image processing unit <b>107</b> according to a third embodiment.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a flowchart illustrating a process by which the notification image generating unit <b>1400</b> generates a motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIGS. <b>16</b>A to <b>16</b>C</figref> are diagrams illustrating converted motion blur corresponding to the current shutter speed (exposure time) in actual shooting, and converted motion blur when the shutter speed (exposure time) has been changed.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a block diagram illustrating the configuration of an image capturing apparatus <b>1700</b> including a notifying apparatus.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a diagram illustrating an example of the configuration of a notification image generating unit <b>1800</b> included in an image processing unit <b>1701</b>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a flowchart illustrating a process by which the notification image generating unit <b>1800</b> generates a motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>).</p><p id="p-0046" num="0045"><figref idref="DRAWINGS">FIG. <b>20</b></figref> is a flowchart illustrating a shooting process according to a fifth embodiment.</p><p id="p-0047" num="0046"><figref idref="DRAWINGS">FIG. <b>21</b></figref> is a flowchart illustrating a process by which a control unit <b>101</b> determines shooting conditions for preparatory shooting (step S<b>2008</b> in <figref idref="DRAWINGS">FIG. <b>20</b></figref>).</p><p id="p-0048" num="0047"><figref idref="DRAWINGS">FIGS. <b>22</b>A and <b>22</b>B</figref> are diagrams illustrating a relationship between a motion amount of an object, calculated from a preparatory shooting image, and a target motion blur (permissible motion amount).</p><p id="p-0049" num="0048"><figref idref="DRAWINGS">FIGS. <b>23</b>A and <b>23</b>B</figref> are timing charts illustrating a process by which the control unit <b>101</b> determines shooting conditions for preparatory shooting (step S<b>2008</b> in <figref idref="DRAWINGS">FIG. <b>20</b></figref>).</p><p id="p-0050" num="0049"><figref idref="DRAWINGS">FIG. <b>24</b></figref> is a diagram illustrating a motion vector in preparatory shooting, and motion blur, in actual shooting, converted from the motion vector in the preparatory shooting (converted motion blur).</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0007" level="1">DESCRIPTION OF THE EMBODIMENTS</heading><p id="p-0051" num="0050">Hereinafter, embodiments of the present invention will be described with reference to the attached drawings. Elements that are given the same reference numerals throughout all of the attached drawings represent the same or similar elements, unless otherwise specified. Note that the technical scope of the present invention is defined by the claims, and is not limited by the following respective embodiments. Also, not all of the combinations of the aspects that are described in the embodiments are necessarily essential to the present invention. Also, the aspects that are described in the individual embodiments can be combined as appropriate.</p><heading id="h-0008" level="1">First Embodiment</heading><p id="p-0052" num="0051"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram illustrating the configuration of an image capturing apparatus <b>100</b> including a notifying apparatus. The first embodiment will describe a configuration that switches a motion blur notification on and off.</p><p id="p-0053" num="0052">A control unit <b>101</b> is a CPU, for example; the control unit <b>101</b> reads out control programs for controlling the various blocks of the image capturing apparatus <b>100</b> from ROM <b>102</b> (described later), loads the programs into RAM <b>103</b> (described later), and executes the programs. Through this, the control unit <b>101</b> can control the operations of the various blocks of the image capturing apparatus <b>100</b>. The ROM <b>102</b> is electrically erasable/recordable non-volatile memory, and stores parameters and the like necessary for each of the blocks of the image capturing apparatus <b>100</b> to function, in addition to the control programs for controlling those blocks. The RAM <b>103</b> is rewritable volatile memory, and is used for loading the control programs executed by the control unit <b>101</b> and the like, temporarily storing data generated through the operations of the blocks of the image capturing apparatus <b>100</b>, and the like.</p><p id="p-0054" num="0053">An optical system <b>104</b> is constituted by a lens group including a zoom lens and a focus lens, and forms a subject image on an image capturing surface of an image capturing unit <b>105</b>, which will be described later. The image capturing unit <b>105</b> is an image sensor such as a CCD or a CMOS sensor; the image capturing unit <b>105</b> photoelectrically converts an optical image formed on the image capturing surface of the image capturing unit <b>105</b> by the optical system <b>104</b> and outputs an analog image signal obtained as a result to an A/D conversion unit <b>106</b>. The A/D conversion unit <b>106</b> converts the input analog image signal into digital image data. The digital image data output from the A/D conversion unit <b>106</b> is temporarily stored in the RAM <b>103</b>.</p><p id="p-0055" num="0054">An image processing unit <b>107</b> applies various types of image processing, such as white balance adjustment, color interpolation, and gamma processing, to the image data stored in the RAM <b>103</b>. The image processing unit <b>107</b> also includes a notification image generating unit <b>300</b> (described later), which generates a motion blur notification image by superimposing an image plane enabling motion blur to be easily confirmed over an image stored in the RAM <b>103</b>.</p><p id="p-0056" num="0055">A recording unit <b>108</b> is a removable memory card or the like. The recording unit <b>108</b> records the image data processed by the image processing unit <b>107</b> as a recorded image, via the RAM <b>103</b>. A display unit <b>109</b> is a display device such as an LCD; the display unit <b>109</b> displays images stored in the RAM <b>103</b>, images recorded into the recording unit <b>108</b>, a user interface for operations for accepting instructions from the user, and so on. The display unit <b>109</b> also displays images captured by the image capturing unit <b>105</b>, for composing the shot and the like during preparatory shooting. An instruction input unit <b>110</b> is a touch panel, a mouse, or the like. The user inputs instructions to the image capturing apparatus <b>100</b> using the instruction input unit <b>110</b>.</p><p id="p-0057" num="0056">A shooting process executed by the image capturing apparatus <b>100</b> will be described next with reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>. Unless otherwise specified, the processes in the respective steps of this flowchart are realized by the control unit <b>101</b> executing the aforementioned control programs. The processing illustrated in this flowchart starts when the user turns the image capturing apparatus <b>100</b> on and an operating mode of the image capturing apparatus <b>100</b> enters a shooting mode.</p><p id="p-0058" num="0057">In step S<b>201</b>, the control unit <b>101</b> starts preparatory shooting (first shooting). During the period of the preparatory shooting, the image capturing apparatus <b>100</b> captures images in sequence, in the same manner as a moving image (a preparatory shooting image), and displays that image in the display unit <b>109</b>. In other words, during the period of the preparatory shooting, the preparatory shooting is carried out repeatedly at predetermined intervals of time. The user composes the shot and so on while viewing the display preparatory shooting image. Note that the processes of steps S<b>202</b> to S<b>206</b> (described hereinafter) are carried out during the period of preparatory shooting.</p><p id="p-0059" num="0058">In step S<b>202</b>, the control unit <b>101</b> sets shooting conditions for actual shooting (second shooting) in response to user instructions (user operations) made using the instruction input unit <b>110</b>. The shooting conditions include exposure conditions, and the exposure conditions include the shutter speed (exposure time), ISO sensitivity, F-stop value, and so on. Note that the shooting conditions for the actual shooting may be automatically set by the control unit <b>101</b>.</p><p id="p-0060" num="0059">In step S<b>203</b>, the control unit <b>101</b> determines whether to turn a motion blur notification on or off. Turning the motion blur notification on or off can be set by the user using the instruction input unit <b>110</b>, for example. When the user sets the motion blur notification on or off, a setting value indicating on or off is held in the RAM <b>103</b>. The control unit <b>101</b> determines whether the motion blur notification is on or off in accordance with this setting value. If it has been determined that the motion blur notification is on, the process moves to step S<b>204</b>, and if not, the process moves to step S<b>205</b>.</p><p id="p-0061" num="0060">In step S<b>204</b>, under the control of the control unit <b>101</b>, the notification image generating unit <b>300</b> generates the motion blur notification image by superimposing a motion blur notification plane onto the preparatory shooting image. In other words, the motion blur notification image is a preparatory shooting image with the motion blur notification plane superimposed thereon. The process of step S<b>204</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0062" num="0061">In step S<b>205</b>, the control unit <b>101</b> displays an image in the display unit <b>109</b>. Specifically, when the motion blur notification image has been generated in step S<b>204</b> (when it has been determined in step S<b>203</b> that the motion blur notification is on), the control unit <b>101</b> displays the motion blur notification image (the preparatory shooting image onto which the motion blur notification plane has been superimposed) in the display unit <b>109</b>. When the motion blur notification image has not been generated (when it has been determined in step S<b>203</b> that the motion blur notification is off), the control unit <b>101</b> displays the preparatory shooting image (the preparatory shooting image onto which the motion blur notification plane has not been superimposed) in the display unit <b>109</b>.</p><p id="p-0063" num="0062">In step S<b>206</b>, the control unit <b>101</b> determines whether or not the user has pressed a shutter button. The shutter button is included in the instruction input unit <b>110</b>, for example. If the shutter button has been pressed, the process moves to step S<b>207</b>, and if not, the process returns to step S<b>202</b>.</p><p id="p-0064" num="0063">While viewing the preparatory shooting image or the motion blur notification image displayed in the display unit <b>109</b>, the user can press the shutter button when he or she has the chance to take a shot. The user can easily confirm motion blur during the preparatory shooting when the motion blur notification image is being displayed in the display unit <b>109</b>. If the motion blur the user has confirmed is not motion blur that meets his or her preferences, the user can avoid pressing the shutter button, which returns the process to step S<b>202</b> and makes it possible to change (reset) the shutter speed (exposure time) for the actual shooting. In this manner, during the preparatory shooting, the user can repeatedly change the shutter speed (exposure time) for the actual shooting while confirming the motion blur notification image displayed in the display unit <b>109</b> until the motion blur that meets his or her preferences is achieved, and can then press the shutter button when there is a chance to take a shot.</p><p id="p-0065" num="0064">When the shutter button is pressed in step S<b>206</b>, the control unit <b>101</b> carries out actual shooting, and records the image from the actual shooting in the recording unit <b>108</b>, in step S<b>207</b>.</p><p id="p-0066" num="0065">An example of the configuration of the notification image generating unit <b>300</b> included in the image processing unit <b>107</b> will be described next with reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>. The notification image generating unit <b>300</b> includes a motion vector calculation unit <b>301</b>, a converted motion blur calculation unit <b>302</b>, a motion blur notification plane generation unit <b>303</b>, and an image superimposing unit <b>304</b>. The operations of the notification image generating unit <b>300</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0067" num="0066">Next, the process by which the notification image generating unit <b>300</b> generates the motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0068" num="0067">In step S<b>401</b>, the notification image generating unit <b>300</b> obtains the preparatory shooting image captured during the preparatory shooting by the image capturing apparatus <b>100</b>. The obtained preparatory shooting image is input to the motion vector calculation unit <b>301</b> and the image superimposing unit <b>304</b>.</p><p id="p-0069" num="0068"><figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is a diagram illustrating an example of the preparatory shooting image. The present embodiment will describe an example in which, as illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>, a scene is shot in which a dog <b>501</b> is running to the left and a dog <b>502</b> is standing still.</p><p id="p-0070" num="0069">In step S<b>402</b>, the motion vector calculation unit <b>301</b> calculates a motion vector between the preparatory shooting images as motion information. A &#x201c;motion vector&#x201d; expresses horizontal and vertical direction movement amounts of an object between preparatory shooting images as a vector. The method for calculating the motion vector will be described in detail later with reference to <figref idref="DRAWINGS">FIGS. <b>6</b> and <b>7</b></figref>.</p><p id="p-0071" num="0070"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a flowchart illustrating the motion vector calculation process carried out by the motion vector calculation unit <b>301</b> (step S<b>402</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>). Although the present embodiment will describe a block matching method as an example of the method for calculating the motion vector, the method for calculating the motion vector is not limited to this example, and may be gradient method instead, for example.</p><p id="p-0072" num="0071">In step S<b>601</b>, the motion vector calculation unit <b>301</b> obtains two preparatory shooting images adjacent with respect to time. The motion vector calculation unit <b>301</b> then sets the preparatory shooting image from an Mth frame as a base frame, and sets the preparatory shooting image from an M+1th frame as a reference frame.</p><p id="p-0073" num="0072">As illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in step S<b>602</b>, the motion vector calculation unit <b>301</b> arranges a base block <b>702</b>, made up of N&#xd7;N pixels, in a base frame <b>701</b>.</p><p id="p-0074" num="0073">Also as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, in step S<b>603</b>, the motion vector calculation unit <b>301</b> sets pixels (N+n)&#xd7;(N+n), which surround coordinates <b>704</b> that match the center coordinates of the base block <b>702</b> in the base frame <b>701</b>, in a reference frame <b>703</b> as a search range <b>705</b>.</p><p id="p-0075" num="0074">In step S<b>604</b>, the motion vector calculation unit <b>301</b> calculates the correlation between the base block <b>702</b> in the base frame <b>701</b>, and an N&#xd7;N-pixel reference block <b>706</b> at coordinates present within the search range <b>705</b> in the reference frame <b>703</b>, to calculate a correlation value. The correlation value is calculated on the basis of an inter-frame difference absolute value sum for the pixels in the base block <b>702</b> and the reference blocks <b>706</b>. In other words, the coordinates where the value of the inter-frame difference absolute value sum is lowest are the coordinates where the correlation value is the highest. Note that the method for calculating the correlation value is not limited to a method that finds the inter-frame difference absolute value sum, and may instead be a method for calculating the correlation value on the basis of an inter-frame difference sum of squares, a normal cross-correlation value, or the like, for example. The example in <figref idref="DRAWINGS">FIG. <b>7</b></figref> indicates that the reference blocks <b>706</b> has the highest correlation.</p><p id="p-0076" num="0075">In step S<b>605</b>, the motion vector calculation unit <b>301</b> calculates the motion vector on the basis of the reference block coordinates indicating the highest correlation value found in step S<b>604</b>. In the example in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the motion vector is found on the basis of the coordinates <b>704</b> corresponding to the center coordinates of the base block <b>702</b> in the base frame <b>701</b>, and the center coordinates of the reference block <b>706</b>, in the search range <b>705</b> of the reference frame <b>703</b>. In other words, the inter-coordinate distance and direction, from the coordinates <b>704</b> to the center coordinates of the reference block <b>706</b>, are found as the motion vector.</p><p id="p-0077" num="0076">In step S<b>606</b>, the motion vector calculation unit <b>301</b> determines whether or not a motion vector has been calculated for all of the pixels in the base frame <b>701</b>. If the motion vector calculation unit <b>301</b> has determined in step S<b>606</b> that a motion vector has not been calculated for all of the pixels, the process returns to step S<b>602</b>, whereas if the motion vector calculation unit <b>301</b> has determined that a motion vector has been calculated for all of the pixels, the process returns to the flowchart of <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0078" num="0077">When the process returns to step S<b>602</b>, the motion vector calculation unit <b>301</b> arranges an N&#xd7;N-pixel base block <b>702</b> in the aforementioned base frame <b>701</b>, central to a pixel for which a motion vector has not yet been calculated. The processing from steps S<b>603</b> to S<b>605</b> is then carried out in the same manner as described earlier. In other words, the motion vector calculation unit <b>301</b> calculates motion vectors for all of the pixels in the base frame <b>701</b> by repeating the processing from steps S<b>602</b> to S<b>605</b> while moving the base block <b>702</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>.</p><p id="p-0079" num="0078"><figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates an example of the motion vectors calculated in this manner. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> is a diagram illustrating an example of motion vectors in the preparatory shooting image indicated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. The preparatory shooting image in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref> is an example in which the dog <b>501</b> is running to the left. <figref idref="DRAWINGS">FIG. <b>5</b>B</figref> illustrates an example of the motion vectors in the case where an object is moving in this manner. In the example illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>B</figref>, leftward motion vectors are detected in the region corresponding to the running dog <b>501</b>, whereas &#x201c;0&#x201d; is detected as the motion vectors in other regions, such as the dog <b>502</b> that is standing still, the fence in the background, and so on. The motion vectors of &#x201c;0&#x201d; are not illustrated.</p><p id="p-0080" num="0079">Note that the motion vector calculation unit <b>301</b> may calculate a motion vector every predetermined number of pixels instead of calculating motion vectors for all of the pixels.</p><p id="p-0081" num="0080">The motion vector calculation unit <b>301</b> calculates the motion vectors between preparatory shooting images adjacent with respect to time through the foregoing processing.</p><p id="p-0082" num="0081">Returning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in step S<b>403</b>, the converted motion blur calculation unit <b>302</b> obtains the shutter speed (exposure time) for the actual shooting set in step S<b>202</b> of <figref idref="DRAWINGS">FIG. <b>2</b></figref>, and the time interval between the images in the preparatory shooting, as the shooting conditions.</p><p id="p-0083" num="0082">In step S<b>404</b>, the converted motion blur calculation unit <b>302</b> converts the motion vectors for each pixel, calculated in step S<b>402</b>, into motion blur in the actual shooting, on the basis of the exposure time for the actual shooting and the time interval between the images in the preparatory shooting, which were obtained in step S<b>403</b>. The method for converting the motion vectors from the preparatory shooting into the motion blur in the actual shooting will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0084" num="0083"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a diagram illustrating a motion vector in preparatory shooting, and motion blur, in actual shooting, converted from the motion vector in the preparatory shooting (converted motion blur). <figref idref="DRAWINGS">FIG. <b>8</b></figref> illustrates an example in which the time interval between the images in the preparatory shooting is 1/60 seconds, and the exposure time in the actual shooting is 1/120 seconds or 1/30 seconds.</p><p id="p-0085" num="0084">The converted motion blur calculation unit <b>302</b> converts the motion vector for each pixel into motion blur in the actual shooting on the basis of the following conversion equations (1) and (2).</p><p id="p-0086" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>CONV_GAIN=EXP_TIME/INT_TIME&#x2003;&#x2003;(1)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0087" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>CONV_BLUR=VEC_LEN&#xd7;CONV_GAIN&#x2003;&#x2003;(2)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0088" num="0085">Here, in Equation (1), CONV_GAIN represents a conversion gain for converting the motion vector in the preparatory shooting into a motion vector in the actual shooting, EXP_TIME represents the exposure time in the actual shooting, and INT_TIME represents the time interval between images in the preparatory shooting. In Equation (2), CONV_BLUR represents the converted motion blur in the actual shooting, and VEC_LEN indicates the length of the motion vector in the preparatory shooting.</p><p id="p-0089" num="0086">In Equation (1), the conversion gain CONV_GAIN is calculated by dividing the exposure time EXP_TIME in the actual shooting by the time interval INT_TIME between images in the preparatory shooting. In Equation (2), the converted motion blur CONV_BLUR in the actual shooting is calculated by multiplying the length VEC_LEN of the motion vector by the conversion gain CONV_GAIN.</p><p id="p-0090" num="0087">Specifically, as illustrated in <figref idref="DRAWINGS">FIG. <b>8</b></figref>, when the length VEC_LEN of the motion vector in the preparatory shooting is 10 pixels and the exposure time EXP_TIME in the actual shooting is 1/120 seconds, the conversion gain CONV_GAIN is &#xbd;x, and thus the converted motion blur is 5 pixels. Likewise, when the exposure time EXP_TIME in the actual shooting is 1/30 seconds, the conversion gain CONV_GAIN is 2&#xd7;, and thus the converted motion blur is 20 pixels.</p><p id="p-0091" num="0088">Returning to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, in step S<b>405</b>, the motion blur notification plane generation unit <b>303</b> creates an image plane for notifying the user of the motion blur (a motion blur notification plane) on the basis of the converted motion blur for each pixel, calculated in step S<b>404</b>.</p><p id="p-0092" num="0089">In step S<b>406</b>, the image superimposing unit <b>304</b> generates the motion blur notification image by superimposing the motion blur notification plane created in step S<b>405</b> onto the preparatory shooting image.</p><p id="p-0093" num="0090">Three examples of the motion blur notification image will be described here with reference to <figref idref="DRAWINGS">FIG. <b>9</b>A to <b>9</b>E</figref>. Displaying the motion blur notification image in the display unit <b>109</b> during the preparatory shooting makes it possible for the user to easily confirm the motion blur.</p><p id="p-0094" num="0091"><figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref> illustrate an example of notifying the user of the motion blur by displaying an icon. A method for generating the motion blur notification image by displaying an icon will be described here. In step S<b>405</b>, of the converted motion blur for each pixel, the motion blur notification plane generation unit <b>303</b> calculates the percentage of the number of pixels having a converted motion blur of a predetermined value or higher, with respect to the entire screen. When the percentage is greater than or equal to a predetermined percentage, the motion blur notification plane generation unit <b>303</b> determines the converted motion blur, from among the converted motion blur at greater than or equal to a predetermined value, that occupies the greatest percentage in the screen.</p><p id="p-0095" num="0092">The method for determining the converted motion blur that occupies the greatest percentage of the screen will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a histogram of converted motion blur having a predetermined value or higher in the scene illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. The motion blur notification plane generation unit <b>303</b> can generate a histogram such as that illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, and then divide ranges of the converted motion blur on the basis of the generated histogram. The number of classes in the histogram can be set in advance. For example, consider a case where peaks (local maximum points) are present in the frequency of the histogram, as indicated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In this case, the motion blur notification plane generation unit <b>303</b> can divide the range of the converted motion blur so that each divided range has one peak, with each peak serving as a representative motion blur for a corresponding part of the object. Note that the peaks in the histogram can be detected using a method similar to that used for detecting peaks in signal waveforms.</p><p id="p-0096" num="0093">For example, three peaks <b>1001</b> to <b>1003</b> are present in the frequency of the histogram <b>1000</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. As such, the motion blur notification plane generation unit <b>303</b> divides the range of the converted motion blur at the positions where the frequency of the converted motion blur first becomes a local minimum, on the lower sides from each peak, and then assigns display colors to each of the resulting divided ranges. As illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, when the percentage of the converted motion blur corresponding to the peak <b>1003</b> is the highest, the motion blur notification plane generation unit <b>303</b> creates a red motion blur notification icon <b>901</b>, as indicated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, as the motion blur notification plane. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> by superimposing the motion blur notification plane, including the motion blur notification icon <b>901</b>, onto the preparatory shooting image.</p><p id="p-0097" num="0094">When the peak <b>1002</b> is higher than the peak <b>1003</b> and the percentage of the converted motion blur corresponding to the peak <b>1002</b> is the highest, the motion blur notification plane generation unit <b>303</b> creates a green motion blur notification icon <b>902</b>, as indicated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, as the motion blur notification plane. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> by superimposing the motion blur notification plane, including the motion blur notification icon <b>902</b>, onto the preparatory shooting image.</p><p id="p-0098" num="0095">Likewise, when the peak <b>1001</b> is higher than the peak <b>1003</b> and the percentage of the converted motion blur corresponding to the peak <b>1001</b> is the highest, the motion blur notification plane generation unit <b>303</b> creates a blue motion blur notification icon <b>903</b>, as indicated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>, as the motion blur notification plane. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref> by superimposing the motion blur notification plane, including the motion blur notification icon <b>903</b>, onto the preparatory shooting image.</p><p id="p-0099" num="0096">Although the foregoing describes dividing the converted motion blur ranges using minimum points in the histogram, the converted motion blur ranges may be divided at a predetermined threshold instead. Alternatively, a configuration in which the division positions are varied in accordance with conditions may be employed as well. Even if such a division method is employed, the motion blur notification plane generation unit <b>303</b> can create, as the motion blur notification plane, a motion blur notification icon having a display color corresponding to the divided range including the converted motion blur that occupies the highest percentage in the screen.</p><p id="p-0100" num="0097"><figref idref="DRAWINGS">FIG. <b>9</b>D</figref> illustrates an example of notifying the user of the motion blur by displaying frames. A method for generating the motion blur notification image by displaying frames will be described here. In step S<b>405</b>, of the pixels within divided regions of the shooting screen, the motion blur notification plane generation unit <b>303</b> calculates the percentage of the number of pixels having a converted motion blur of a predetermined value or higher, with respect to the overall divided regions. For divided regions in which that percentage is greater than or equal to a predetermined percentage, the motion blur notification plane generation unit <b>303</b> creates motion blur notification frames, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>D</figref>, as the motion blur notification plane. At this time, the motion blur notification plane generation unit <b>303</b> determines the display color of the motion blur notification frames corresponding to each divided region in accordance with the magnitude of the converted motion blur in each divided region.</p><p id="p-0101" num="0098">For example, the motion blur notification plane generation unit <b>303</b> divides the converted motion blur range into three parts on the basis of three thresholds, and assigns blue, green, and red to the divided ranges in that order, starting with the lowest converted motion blur. In this case, the motion blur notification plane generation unit <b>303</b> creates red motion blur notification frames <b>904</b> for the parts corresponding to the legs, where there is a high amount of converted motion blur, and creates green motion blur notification frames <b>905</b> for parts corresponding to the head and tail, where there is a medium amount of converted motion blur. The motion blur notification plane generation unit <b>303</b> also creates blue motion blur notification frames <b>906</b> for parts corresponding to the trunk, where there is little converted motion blur. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>D</figref> by superimposing the motion blur notification plane, including the motion blur notification frames <b>904</b> to <b>906</b>, onto the preparatory shooting image.</p><p id="p-0102" num="0099"><figref idref="DRAWINGS">FIG. <b>9</b>E</figref> illustrates an example of notifying the user of the motion blur by displaying edges in which motion blur has arisen in an enhanced manner. A method for generating the motion blur notification image by displaying the edges in which motion blur has arisen in an enhanced manner will be described here. In step S<b>405</b>, the motion blur notification plane generation unit <b>303</b> detects the edge strength of the preparatory shooting image. It is assumed that the edge strength is calculated using a known technique such as a Sobel filter, so this will not be described here. The motion blur notification plane generation unit <b>303</b> then extracts pixels for which the edge strength is greater than or equal to a predetermined value and for which the converted motion blur is greater than or equal to a predetermined value. The motion blur notification plane generation unit <b>303</b> then creates a motion blur notification plane, in which the edge areas where motion blur has arisen are displayed in an enhanced manner for the extracted pixels, as indicated by motion blur notification edges <b>907</b> to <b>909</b> in <figref idref="DRAWINGS">FIG. <b>9</b>E</figref>. At this time, the motion blur notification plane generation unit <b>303</b> determines the display color of each pixel in the motion blur notification edges in accordance with the amount of converted motion blur in those pixels.</p><p id="p-0103" num="0100">For example, the motion blur notification plane generation unit <b>303</b> divides the converted motion blur range into three parts on the basis of three thresholds, and assigns blue, green, and red to the divided ranges in that order, starting with the lowest converted motion blur. In this case, the motion blur notification plane generation unit <b>303</b> creates red motion blur notification edges <b>907</b> for the pixels in the parts corresponding to the legs, where there is a high amount of converted motion blur, and creates green motion blur notification edges <b>908</b> for the pixels in the parts corresponding to the head and tail, where there is a medium amount of converted motion blur. The motion blur notification plane generation unit <b>303</b> also creates blue motion blur notification edges <b>909</b> for the pixels in the parts corresponding to the trunk, where there is little converted motion blur. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>E</figref> by superimposing the motion blur notification plane, including the motion blur notification edges <b>907</b> to <b>909</b>, onto the preparatory shooting image.</p><p id="p-0104" num="0101">Although <figref idref="DRAWINGS">FIG. <b>9</b>E</figref> illustrates an example in which the edges are displayed in an enhanced manner by varying the display colors of the motion blur notification edges in accordance with the amount of converted motion blur, the enhanced display method is not limited thereto. A method of displaying the edges in an enhanced manner by varying the thicknesses of the motion blur notification edges in accordance with the amount of converted motion blur can be given as another example of an enhanced display method.</p><p id="p-0105" num="0102">According to the first embodiment as described thus far, the image capturing apparatus <b>100</b> detects the motion amount of an object from an image obtained through preparatory shooting, and converts the motion amount to a motion blur amount arising during actual shooting. The image capturing apparatus <b>100</b> then notifies the user of the motion blur on the basis of the motion blur amount. At this time, the image capturing apparatus <b>100</b> varies the form of the notification in accordance with the amount of the motion blur. This makes it possible for the photographer to easily know, in advance, how much motion blur will arise in the actual shooting.</p><p id="p-0106" num="0103">The present embodiment describes, with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>E</figref>, an example in which the user is notified of the motion blur when the converted motion blur is greater than or equal to a predetermined value. However, a configuration in which the user is notified of the motion blur even when the converted motion blur is less than the predetermined value may be employed. This makes it easy for the user to confirm whether the motion blur is insufficient during the preparatory shooting period, such as in long-exposure shooting, where the user wishes to use motion blur to express a sense of motion.</p><p id="p-0107" num="0104">Additionally, although the present embodiment describes an example in which the user is notified of the motion blur when the converted motion blur is greater than or equal to a predetermined value, a configuration may be employed in which the user is notified having set a range for the converted motion blur in which a notification is to be made. As an example of a way for setting the range in which a notification is to be made, if the user has decided to view the shot image in the display, a range is set in which motion blur can be confirmed visually when the image is displayed in the designated display at actual size. A method in which the range at which motion blur can be recognized in the display is converted to a range corresponding to the size of the shot image on the basis of the difference between the number of pixels of the shot image and the number of pixels of the display can be used as a method for calculating the range. The image capturing apparatus <b>100</b> then notifies the user of the converted motion blur when the blur is within the converted range.</p><p id="p-0108" num="0105">Additionally, although the present embodiment describes an example in which the user is notified of the motion blur when the converted motion blur is greater than or equal to a predetermined value, a configuration may be employed in which the predetermined value is determined on the basis of information from an angular velocity sensor. When the user takes a shot while holding the image capturing apparatus <b>100</b> in his or her hand, blur resulting from his or her hand shaking, camerawork, and the like is included as converted motion blur in the calculated range. If a notification is to be made only for the converted motion blur of the object, while excluding blur caused by hand shake and camerawork, removing converted motion blur within the same range as the movements detected by the angular velocity sensor makes it possible to notify the user only of motion blur arising in the object.</p><p id="p-0109" num="0106">Additionally, although the present embodiment describes the motion blur notification icons <b>901</b> to <b>903</b>, the motion blur notification frames <b>904</b> to <b>906</b>, and the motion blur notification edges <b>907</b> to <b>909</b> as three examples of the motion blur notification plane, the types of the motion blur notification planes are not limited thereto. For example, a configuration may be employed in which all regions where motion blur is arising, including both the edge region and a flat region, are displayed in an enhanced manner. Specifically, the motion blur notification plane generation unit <b>303</b> carries out an enhanced display in which the pixels where the per-pixel converted motion blur is greater than or equal to a predetermined value are colored with a color based on the amount of converted motion blur. Carrying out the enhanced display for both the edge region and regions aside from the edge region in this manner ensure the entire object is displayed in an enhanced manner, which makes it easier to confirm motion blur.</p><p id="p-0110" num="0107">Additionally, although the present embodiment describes a configuration in which the motion blur notification image is displayed in the display unit <b>109</b> as the method for notifying the user of motion blur, the method for notifying the user of motion blur is not limited thereto. For example, a configuration in which the user is notified of motion blur by outputting sound may be employed. In this case, for example, the control unit <b>101</b> may output a motion blur notification sound from a speaker (not shown) when, of the converted motion blur for each pixel, the percentage of the number of pixels having a converted motion blur of a predetermined value or higher, with respect to the entire screen, is greater than or equal to a predetermined percentage. Then, the control unit <b>101</b> may divide the range of the converted motion blur using the same method as that described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, assign a different volume to each divided range, and then output motion blur notification sound at a volume corresponding to the divided range including the converted motion blur that occupies the highest percentage within the screen. Alternatively, the control unit <b>101</b> may assign a different type of sound to each divided range, and output the type of motion blur notification sound corresponding to the divided range including the converted motion blur that occupies the highest percentage in the screen. In other words, the control unit <b>101</b> can change at least one of the type of the sound and the volume in accordance with the amount of converted motion blur.</p><p id="p-0111" num="0108">Additionally, although the present embodiment describes, as a method of making the motion blur notification, an example in which the display color of the motion blur notification icon is changed in accordance with the amount of converted motion blur, the item to be varied in accordance with the amount of converted motion blur is not limited to the display color. For example, a configuration may be employed in which the size of the motion blur notification icon is varied in accordance with the amount of converted motion blur. For example, a configuration may be employed in which the icon size is increased when there is more converted motion blur, and the icon size is reduced when there is less converted motion blur. Varying the icon size in accordance with the amount of converted motion blur in this manner makes it possible for the user to easily confirm the intensity of the motion blur.</p><p id="p-0112" num="0109">Accordingly, the notification image generating unit <b>300</b> can carry out control for displaying blur notification icons, which have different appearances depending on the amount of converted motion blur, along with the preparatory shooting image. For example, the notification image generating unit <b>300</b> can vary at least one of the color and size of the blur notification icon in accordance with the amount of converted motion blur.</p><p id="p-0113" num="0110">Additionally, although the present embodiment describes, as a method of making the motion blur notification, an example in which the display color of the motion blur notification frames or the motion blur notification edges is changed in accordance with the amount of converted motion blur, the item to be varied in accordance with the amount of converted motion blur is not limited to the display color. For example, a configuration may be employed in which the thickness of the lines of the motion blur notification frames or the motion blur notification edges is varied in accordance with the amount of converted motion blur. For example, a configuration may be employed in which the frames or edges are displayed thicker when there is more converted motion blur, and thinner when there is less converted motion blur. Varying the line thickness in accordance with the amount of converted motion blur in this manner makes it possible for the user to easily confirm the intensity of the motion blur.</p><p id="p-0114" num="0111">Accordingly, the notification image generating unit <b>300</b> can carry out control for displaying a plurality of blur notification frames, which surround a plurality of divided regions in the preparatory shooting image where motion blur is arising, along with the preparatory shooting image. The notification image generating unit <b>300</b> can then vary the appearance of each blur notification frame in accordance with the amount of converted motion blur in the divided region corresponding to the blur notification frame. For example, the notification image generating unit <b>300</b> can vary at least one of the color and size of each blur notification frame in accordance with the amount of converted motion blur in the divided region corresponding to the blur notification frame. Note that the region where the blur notification frame is displayed is not limited to the divided region, and a motion blur notification can be made using a blur notification frame for any desired partial region in the preparatory shooting image where motion blur is arising. Additionally, the notification image generating unit <b>300</b> can display a plurality of edge regions where motion blur is arising in the preparatory shooting image, in an enhanced manner with different forms depending on the amount of converted motion blur in each of the edge regions. The &#x201c;enhanced display&#x201d; includes at least one of displaying the plurality of edge regions with different colors depending on the amount of converted motion blur in each of the edge regions, and displaying the plurality of edge regions using lines of different thicknesses depending on the amount of converted motion blur in each of the edge regions.</p><p id="p-0115" num="0112">The present embodiment describes a configuration in which the motion amount of an object is detected from an image obtained through preparatory shooting, and by converting that motion amount to a motion blur amount that will arise in the actual shooting, the motion blur amount that will arise in the actual shooting is estimated. However, the method for estimating the motion blur amount that will arise in the actual shooting is not limited thereto. Additionally, the two instances of shooting, i.e., the preparatory shooting and the actual shooting, may be any two types of shooting (first shooting and second shooting). For example, the image capturing apparatus <b>100</b> may obtain a first shot image obtained through the first shooting under first shooting conditions, and motion information of an object in the first shot image. The motion information obtained here is the speed of the object in the first shot image (e.g., a movement amount in a unit of time, expressed as a number of pixels). Then, on the basis of the motion information and second shooting conditions, the image capturing apparatus <b>100</b> may estimate the motion blur amount of the object in a second shot image obtained when the second shooting is carried out under the second shooting conditions. The second shooting conditions are set independent from the first shooting conditions. This point also applies to the second to fifth embodiments, which will be described hereinafter.</p><heading id="h-0009" level="1">Second Embodiment</heading><p id="p-0116" num="0113">Although the first embodiment described a configuration in which a motion amount detected from a preparatory shooting image is converted into a motion blur amount arising in actual shooting, the second embodiment will describe a configuration in which a motion amount of an image capturing apparatus that is carrying out preparatory shooting is converted into a motion blur amount arising in actual shooting. Although the second embodiment will be described with reference to an image capturing apparatus <b>1100</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref>, instead of the image capturing apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the configuration and operations of the image capturing apparatus <b>1100</b> share some common parts with the image capturing apparatus <b>100</b>. The following will primarily describe points that are different from the first embodiment.</p><p id="p-0117" num="0114">The image capturing apparatus <b>1100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>11</b></figref> differs from the image capturing apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in that an angular velocity detection unit <b>1101</b> has been added, and the image processing unit <b>107</b> has been replaced with an image processing unit <b>1102</b>.</p><p id="p-0118" num="0115">The angular velocity detection unit <b>1101</b> is an angular velocity sensor, for example, which detects the angular velocity of the image capturing apparatus <b>1100</b> itself, caused by the user's hand shaking, the user's camerawork, or the like, in a yaw direction and a pitch direction. Any desired known method can be used as the method by which the angular velocity detection unit <b>1101</b> detects the angular velocity.</p><p id="p-0119" num="0116">The image processing unit <b>1102</b> includes a notification image generating unit <b>1200</b> (described later), which generates a motion blur notification image by superimposing an image plane enabling motion blur to be easily confirmed over an image stored in the RAM <b>103</b>.</p><p id="p-0120" num="0117">An example of the configuration of the notification image generating unit <b>1200</b> included in the image processing unit <b>1102</b> will be described next with reference to <figref idref="DRAWINGS">FIG. <b>12</b></figref>. The notification image generating unit <b>1200</b> illustrated in <figref idref="DRAWINGS">FIG. <b>12</b></figref> differs from the notification image generating unit <b>300</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> in that the motion vector calculation unit <b>301</b> has been replaced with an image motion information conversion unit <b>1201</b>. The operations of the notification image generating unit <b>1200</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0121" num="0118">Next, the process by which the notification image generating unit <b>1200</b> generates the motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>.</p><p id="p-0122" num="0119">In step S<b>1301</b>, the image motion information conversion unit <b>1201</b> converts the angular velocity detected by the angular velocity detection unit <b>1101</b> into information corresponding to motion vectors in the image. Equations (3) and (4) indicate approximate transformations for converting the angular velocity into information corresponding to motion vectors in the image.</p><p id="p-0123" num="0000"><maths id="MATH-US-00001" num="00001"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>MOV_yaw</mi>     <mo>&#x2248;</mo>     <mfrac>      <mrow>       <mi>f</mi>       <mo>&#x2062;</mo>       <mrow>        <mi>tan</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>-</mo>         <mfrac>          <mi>&#x3c9;_yaw</mi>          <mi>fps</mi>         </mfrac>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mi>pp</mi>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>3</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths><maths id="MATH-US-00001-2" num="00001.2"><math overflow="scroll"> <mtable>  <mtr>   <mtd>    <mrow>     <mi>MOV_pitch</mi>     <mo>&#x2248;</mo>     <mfrac>      <mrow>       <mi>f</mi>       <mo>&#x2062;</mo>       <mrow>        <mi>tan</mi>        <mo>&#x2061;</mo>        <mo>(</mo>        <mrow>         <mo>-</mo>         <mfrac>          <mi>&#x3c9;_pitch</mi>          <mi>fps</mi>         </mfrac>        </mrow>        <mo>)</mo>       </mrow>      </mrow>      <mi>pp</mi>     </mfrac>    </mrow>   </mtd>   <mtd>    <mrow>     <mo>(</mo>     <mn>4</mn>     <mo>)</mo>    </mrow>   </mtd>  </mtr> </mtable></math></maths></p><p id="p-0124" num="0120">Here, MOV_yaw represents the amount of movement in the yaw direction, and MOV_pitch represents the amount of movement in the pitch direction. f represents the focal length, &#x3c9;_yaw represents the angular velocity in the yaw direction, &#x3c9;_pitch represents the angular velocity in the pitch direction, fps represents the framerate of the preparatory shooting, and pp represents the pixel pitch of the image capturing unit <b>105</b>.</p><p id="p-0125" num="0121">According to the approximate transformation equations indicated in Equations (3) and (4), the amount of movement in the image capturing plane is calculated on the basis of the angle of movement in the time interval between images in the preparatory shooting, and the focal length; the amount of movement in the image (the number of pixels of movement) is then calculated by dividing the amount of movement in the image capturing plane by the pixel pitch. Here, the calculated amount of movement in the image does not differ from pixel to pixel, but is instead uniform for all of the pixels.</p><p id="p-0126" num="0122">The image motion information conversion unit <b>1201</b> treats the amount of movement in the yaw direction as an amount of movement in the horizontal direction and the amount of movement in the pitch direction as an amount of movement in the vertical direction, and then outputs the combination of these movement amounts to the converted motion blur calculation unit <b>302</b> as motion vectors uniform for all the pixels.</p><p id="p-0127" num="0123">In step S<b>1302</b>, the motion blur notification plane generation unit <b>303</b> creates an image plane for notifying the user of the motion blur (the motion blur notification plane) on the basis of the converted motion blur calculated in step S<b>404</b>. The process of step S<b>1302</b> is the same as in the first embodiment (step S<b>405</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>) in that the motion blur notification plane is created with different forms depending on the amount of the converted motion blur. However, in the second embodiment, the motion vectors are uniform for all of the pixels, and thus the converted motion blur is also uniform for all of the pixels, which means that a histogram such as that illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> cannot be generated. Accordingly, the motion blur notification plane generation unit <b>303</b> determines the form of the motion blur notification plane by, for example, comparing the amount of converted motion blur that is uniform for all of the pixels to one or more thresholds. For example, assume that A represents the amount of the converted motion blur, and T1, T2, and T3 represent three thresholds (T1&#x3c;T2&#x3c;T3). If T1&#x3c;A&#x3c;T2, the motion blur notification plane generation unit <b>303</b> creates the blue motion blur notification icon <b>903</b> (<figref idref="DRAWINGS">FIG. <b>9</b>C</figref>) as the motion blur notification plane. If T2&#x3c;A&#x3c;T3, the motion blur notification plane generation unit <b>303</b> creates the green motion blur notification icon <b>902</b> (<figref idref="DRAWINGS">FIG. <b>9</b>B</figref>) as the motion blur notification plane. If T3&#x3c;A, the motion blur notification plane generation unit <b>303</b> creates the red motion blur notification icon <b>901</b> (<figref idref="DRAWINGS">FIG. <b>9</b>A</figref>) as the motion blur notification plane.</p><p id="p-0128" num="0124">According to the second embodiment as described thus far, the image capturing apparatus <b>1100</b> converts the motion amount of the image capturing apparatus <b>1100</b> carrying out preparatory shooting into the motion blur amount that will arise in the actual shooting. This makes it possible to obtain the converted motion blur with a comparatively light processing load.</p><p id="p-0129" num="0125">The second embodiment describes an example in which the amount of movement in the image capturing plane is calculated on the basis of the angle of movement in the time interval between images in the preparatory shooting, and the focal length, and the amount of movement in the image is then calculated by dividing the amount of movement in the image capturing plane by the pixel pitch. However, the method for calculating the amount of movement in the image is not limited thereto. A configuration may be employed in which the amount of movement in the image capturing plane is calculated on the basis of the angle of movement in an exposure period of the image in the preparatory shooting, and the focal length, and the amount of movement in the image is then calculated by dividing the amount of movement in the image capturing plane by the pixel pitch. Specifically, the angular velocity detection unit <b>1101</b> detects the angular velocity during the exposure period of the image during the preparatory shooting, and the notification image generating unit <b>1200</b> generates the motion blur notification image on the basis of that angular velocity. Note that in this case, the converted motion blur calculation unit <b>302</b> converts motion vectors uniform for all the pixels into motion blur arising in the actual shooting, on the basis of the exposure time for the actual shooting and the exposure time for the image during the preparatory shooting. In this manner, the image capturing apparatus <b>1100</b> can calculate the motion blur arising during actual shooting from the motion amount of the image capturing apparatus <b>1100</b> (e.g., the angle of movement) in a predetermined period (e.g., the time interval between images in the preparatory shooting or the exposure period of the image in the preparatory shooting).</p><heading id="h-0010" level="1">Third Embodiment</heading><p id="p-0130" num="0126">The third embodiment will describe a configuration in which the motion blur notification plane is created with different forms depending on divergence between the converted motion blur and a target motion blur. In the third embodiment, the basic configuration of the image capturing apparatus <b>100</b> is the same as in the first embodiment (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The following will primarily describe points that are different from the first embodiment.</p><p id="p-0131" num="0127">In the present embodiment, the image processing unit <b>107</b> of the image capturing apparatus <b>100</b> includes a notification image generating unit <b>1400</b> (<figref idref="DRAWINGS">FIG. <b>14</b></figref>) instead of the notification image generating unit <b>300</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>). The notification image generating unit <b>1400</b> illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> differs from the notification image generating unit <b>300</b> illustrated in <figref idref="DRAWINGS">FIG. <b>3</b></figref> in that a divergence calculation unit <b>1401</b> has been added, and the motion blur notification plane generation unit <b>303</b> has been replaced with a motion blur notification plane generation unit <b>1402</b>. The operations of the notification image generating unit <b>1400</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0132" num="0128">Next, the process by which the notification image generating unit <b>1400</b> generates the motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0133" num="0129">In step S<b>1501</b>, the divergence calculation unit <b>1401</b> calculates a target value for the converted motion blur in the actual shooting (the target motion blur), and finds the divergence between the target motion blur and the converted motion blur for each pixel calculated in step S<b>404</b>. The divergence calculation unit <b>1401</b> calculates a converted motion blur that is permissible within the screen as the target motion blur. For example, if the user has decided to view shot images in a display, the &#x201c;converted motion blur that is permissible within the screen&#x201d; corresponds to the upper limit of a range at which the motion blur cannot be seen visually when the image is displayed in the designated display at actual size.</p><p id="p-0134" num="0130">The method for calculating the divergence between the converted motion blur and the target motion blur will be described with reference to <figref idref="DRAWINGS">FIGS. <b>16</b>A to <b>16</b>C</figref>. <figref idref="DRAWINGS">FIGS. <b>16</b>A to <b>16</b>C</figref> are diagrams illustrating converted motion blur corresponding to the current shutter speed (exposure time) in actual shooting, and converted motion blur when the shutter speed (exposure time) has been changed. <figref idref="DRAWINGS">FIGS. <b>16</b>A to <b>16</b>C</figref> illustrate examples in which the exposure time in the actual shooting is 1/125 seconds, and the exposure time after the change is 1/250 seconds, 1/500 seconds, and 1/1000 seconds, as the shooting conditions.</p><p id="p-0135" num="0131">The divergence calculation unit <b>1401</b> calculates the divergence between the converted motion blur and the target motion blur for each pixel on the basis of the following Equations (5) to (7).</p><p id="p-0136" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>DIFF=TARG_BLUR/CONV_BLUR&#x2003;&#x2003;(5)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0137" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>TARG_TIME=EXP_TIME&#xd7;DIFF&#x2003;&#x2003;(6)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0138" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>STEP=LOG<sub>2</sub>(DIFF)&#x2003;&#x2003;(7)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0139" num="0000">In Equation (5), TARG_BLUR represents the target motion blur, and DIFF represents the difference between the target motion blur and the converted motion blur (the ratio of the target motion blur to the converted motion blur, here). In Equation (6), TARG_TIME represents a target exposure time, and EXP_TIME represents the current exposure time. In Equation (7), STEP represents the number of shutter speed steps required to change from the current exposure time to the target exposure time.</p><p id="p-0140" num="0132">In Equation (5), the difference between the target motion blur and the converted motion blur is calculated by dividing the target motion blur by a converted motion vector. Next, in Equation (6), the target exposure time is calculated by multiplying the current exposure time by the divergence. The number of shutter speed steps to change from the current exposure time in order to achieve the target exposure time calculated through Equation (6) is calculated using Equation (7).</p><p id="p-0141" num="0133"><figref idref="DRAWINGS">FIG. <b>16</b>A</figref> illustrates a case where the current converted motion blur is 40 pixels. Assuming the target motion blur is 5 pixels, DIFF is &#x215b;x; thus for the current exposure time of 1/125 seconds, the target exposure time is 1/1000 seconds, meaning that the number of steps the shutter speed is changed is &#x2212;3 steps.</p><p id="p-0142" num="0134"><figref idref="DRAWINGS">FIG. <b>16</b>B</figref> illustrates a case where the current converted motion blur is 20 pixels. Assuming the target motion blur is 5 pixels, DIFF is &#xbc;x; thus for the current exposure time of 1/125 seconds, the target exposure time is 1/500 seconds, meaning that the number of steps the shutter speed is changed is &#x2212;2 steps.</p><p id="p-0143" num="0135"><figref idref="DRAWINGS">FIG. <b>16</b>C</figref> illustrates a case where the current converted motion blur is 10 pixels. Assuming the target motion blur is 5 pixels, DIFF is &#xbd;x; thus for the current exposure time of 1/125 seconds, the target exposure time is 1/250 seconds, meaning that the number of steps the shutter speed is changed is &#x2212;1 steps.</p><p id="p-0144" num="0136">Returning to <figref idref="DRAWINGS">FIG. <b>15</b></figref>, in step S<b>1502</b>, the motion blur notification plane generation unit <b>1402</b> creates an image plane for notifying the user of the motion blur (a motion blur notification plane) on the basis of the divergence between the converted motion blur and the target motion blur for each pixel (the divergence for each pixel) calculated in step S<b>1501</b>.</p><p id="p-0145" num="0137">Three examples of the motion blur notification image will be described here with reference to <figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>E</figref>. Displaying the motion blur notification image in the display unit <b>109</b> during the preparatory shooting makes it possible for the user to easily confirm the motion blur.</p><p id="p-0146" num="0138"><figref idref="DRAWINGS">FIGS. <b>9</b>A to <b>9</b>C</figref> illustrate an example of notifying the user of the motion blur by displaying an icon. A method for generating the motion blur notification image by displaying an icon will be described here. In step S<b>1502</b>, of the converted motion blur for each pixel, the motion blur notification plane generation unit <b>1402</b> calculates the percentage of the number of pixels having a converted motion blur of a predetermined value or higher, with respect to the entire screen. When the percentage is greater than or equal to a predetermined percentage, the motion blur notification plane generation unit <b>1402</b> determines the converted motion blur, from among the converted motion blur at greater than or equal to a predetermined value, that occupies the greatest percentage in the screen.</p><p id="p-0147" num="0139">The method for determining the converted motion blur that occupies the greatest percentage of the screen will be described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>. <figref idref="DRAWINGS">FIG. <b>10</b></figref> is a diagram illustrating a histogram of converted motion blur having a predetermined value or higher in the scene illustrated in <figref idref="DRAWINGS">FIG. <b>5</b>A</figref>. The motion blur notification plane generation unit <b>1402</b> can generate a histogram such as that illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, and then divide ranges of the converted motion blur on the basis of the generated histogram. The number of classes in the histogram can be set in advance. For example, consider a case where peaks (local maximum points) are present in the frequency of the histogram, as indicated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. In this case, the motion blur notification plane generation unit <b>1402</b> can divide the range of the converted motion blur so that each divided range has one peak, with each peak serving as a representative motion blur for a corresponding part of the object. Note that the peaks in the histogram can be detected using a method similar to that used for detecting peaks in signal waveforms.</p><p id="p-0148" num="0140">For example, three peaks <b>1001</b> to <b>1003</b> are present in the frequency of the histogram <b>1000</b> illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>. As such, the motion blur notification plane generation unit <b>1402</b> divides the range of the converted motion blur at the positions where the frequency of the converted motion blur first becomes a local minimum, from each peak. The motion blur notification plane generation unit <b>1402</b> then assigns a display color to each divided range on the basis of the divergences corresponding to the peaks <b>1001</b> to <b>1003</b>. For example, the motion blur notification plane generation unit <b>1402</b> assigns red to the divided range including the peak <b>1003</b>, where three steps' (or more) worth of a shutter speed change is required, as illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>A</figref>. Likewise, the motion blur notification plane generation unit <b>1402</b> assigns green to the divided range including the peak <b>1002</b>, where two steps' worth of a shutter speed change is required, as illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>B</figref>. Finally, the motion blur notification plane generation unit <b>1402</b> assigns blue to the divided range including the peak <b>1001</b>, where one step's worth of a shutter speed change is required, as illustrated in <figref idref="DRAWINGS">FIG. <b>16</b>C</figref>.</p><p id="p-0149" num="0141">When the display colors have been assigned in this manner, and the percentage of the converted motion blur corresponding to the peak <b>1003</b> is the highest as illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref>, the motion blur notification plane generation unit <b>1402</b> creates the red motion blur notification icon <b>901</b>, as indicated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref>, as the motion blur notification plane. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> by superimposing the motion blur notification plane, including the motion blur notification icon <b>901</b>, onto the preparatory shooting image.</p><p id="p-0150" num="0142">When the peak <b>1002</b> is higher than the peak <b>1003</b> and the percentage of the converted motion blur corresponding to the peak <b>1002</b> is the highest, the motion blur notification plane generation unit <b>1402</b> creates a green motion blur notification icon <b>902</b>, as indicated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, as the motion blur notification plane. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref> by superimposing the motion blur notification plane, including the motion blur notification icon <b>902</b>, onto the preparatory shooting image.</p><p id="p-0151" num="0143">Likewise, when the peak <b>1001</b> is higher than the peak <b>1003</b> and the percentage of the converted motion blur corresponding to the peak <b>1001</b> is the highest, the motion blur notification plane generation unit <b>1402</b> creates a blue motion blur notification icon <b>903</b>, as indicated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>, as the motion blur notification plane. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref> by superimposing the motion blur notification plane, including the motion blur notification icon <b>903</b>, onto the preparatory shooting image.</p><p id="p-0152" num="0144">Although the foregoing describes dividing the converted motion blur ranges using local minimum points in the histogram, the converted motion blur ranges may be divided at a predetermined threshold instead. Alternatively, a configuration in which the division points are varied in accordance with conditions may be employed as well. Even if such a division method is employed, the motion blur notification plane generation unit <b>1402</b> can create, as the motion blur notification plane, a motion blur notification icon having a display color corresponding to the divided range including the converted motion blur that occupies the highest percentage in the screen.</p><p id="p-0153" num="0145"><figref idref="DRAWINGS">FIG. <b>9</b>D</figref> illustrates an example of notifying the user of the motion blur by displaying frames. A method for generating the motion blur notification image by displaying frames will be described here. In step S<b>1502</b>, of the pixels within divided regions of the shooting screen, the motion blur notification plane generation unit <b>1402</b> calculates the percentage of the number of pixels having a converted motion blur of a predetermined value or higher, with respect to the overall divided regions. For divided regions in which that percentage is greater than or equal to a predetermined percentage, the motion blur notification plane generation unit <b>1402</b> creates motion blur notification frames, as illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>D</figref>, as the motion blur notification plane. At this time, the motion blur notification plane generation unit <b>1402</b> determines the display color of the motion blur notification frames corresponding to each divided region in accordance with the divergence in each divided region.</p><p id="p-0154" num="0146">For example, the motion blur notification plane generation unit <b>1402</b> assigns red to divided regions having a divergence that corresponds to a change of three steps (or more) in the shutter speed. Likewise, the motion blur notification plane generation unit <b>1402</b> assigns green to divided regions having a divergence that corresponds to a change of two steps in the shutter speed. Finally, the motion blur notification plane generation unit <b>1402</b> assigns blue to divided regions having a divergence that corresponds to a change of one step in the shutter speed. In this case, the motion blur notification plane generation unit <b>1402</b> creates the red motion blur notification frames <b>904</b> for the parts corresponding to the legs, where there is a high amount of converted motion blur and thus a higher divergence, and creates the green motion blur notification frames <b>905</b> for parts corresponding to the head and tail, where there is a medium amount of converted motion blur and thus a medium divergence. The motion blur notification plane generation unit <b>1402</b> also creates blue motion blur notification frames <b>906</b> for parts corresponding to the trunk, where there is little converted motion blur and thus little divergence. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>D</figref> by superimposing the motion blur notification plane, including the motion blur notification frames <b>904</b> to <b>906</b>, onto the preparatory shooting image.</p><p id="p-0155" num="0147"><figref idref="DRAWINGS">FIG. <b>9</b>E</figref> illustrates an example of notifying the user of the motion blur by displaying edges in which motion blur has arisen in an enhanced manner. A method for generating the motion blur notification image by displaying the edges in which motion blur has arisen in an enhanced manner will be described here. In step S<b>1502</b>, the motion blur notification plane generation unit <b>1402</b> detects the edge strength of the preparatory shooting image. It is assumed that the edge strength is calculated using a known technique such as a Sobel filter, so this will not be described here. The motion blur notification plane generation unit <b>1402</b> then extracts pixels for which the edge strength is greater than or equal to a predetermined value and for which the converted motion blur is greater than or equal to a predetermined value. The motion blur notification plane generation unit <b>1402</b> then creates a motion blur notification plane, in which the edge areas where motion blur has arisen are displayed in an enhanced manner for the extracted pixels, as indicated by motion blur notification edges <b>907</b> to <b>909</b> in <figref idref="DRAWINGS">FIG. <b>9</b>C</figref>. At this time, the motion blur notification plane generation unit <b>1402</b> determines the display color of each pixel in the motion blur notification edges in accordance with the divergence in those pixels.</p><p id="p-0156" num="0148">For example, the motion blur notification plane generation unit <b>1402</b> assigns red to pixels having a divergence that corresponds to a change of three steps (or more) in the shutter speed. Likewise, the motion blur notification plane generation unit <b>1402</b> assigns green to pixels having a divergence that corresponds to a change of two steps in the shutter speed. Finally, the motion blur notification plane generation unit <b>1402</b> assigns blue to pixels having a divergence that corresponds to a change of one step in the shutter speed. In this case, the motion blur notification plane generation unit <b>1402</b> creates the red motion blur notification edges <b>907</b> for pixels in the parts corresponding to the legs, where there is a high amount of converted motion blur and thus a higher divergence, and creates the green motion blur notification edges <b>908</b> for pixels in the parts corresponding to the head and tail, where there is a medium amount of converted motion blur and thus a medium divergence. The motion blur notification plane generation unit <b>1402</b> also creates blue motion blur notification edges <b>909</b> for the pixels in the parts corresponding to the trunk, where there is little converted motion blur and thus little divergence. Then, in step S<b>406</b>, the image superimposing unit <b>304</b> generates a motion blur notification image such as that illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>E</figref> by superimposing the motion blur notification plane, including the motion blur notification edges <b>907</b> to <b>909</b>, onto the preparatory shooting image.</p><p id="p-0157" num="0149">Although <figref idref="DRAWINGS">FIG. <b>9</b>E</figref> illustrates an example in which the edges are displayed in an enhanced manner by varying the display colors of the motion blur notification edges in accordance with the divergence, the enhanced display method is not limited thereto. A method of displaying the edges in an enhanced manner by varying the thicknesses of the motion blur notification edges in accordance with the divergence can be given as another example of an enhanced display method.</p><p id="p-0158" num="0150">According to the third embodiment as described thus far, the image capturing apparatus <b>100</b> varies the form of the motion blur notification in accordance with the divergence between the motion blur amount that will arise in the actual shooting, and the target motion blur amount. This makes it possible for the photographer to easily know, in advance, how much motion blur will arise in the actual shooting.</p><p id="p-0159" num="0151">Although the present embodiment describes a configuration in which the motion blur notification image is displayed in the display unit <b>109</b> as the method for notifying the user of motion blur, the method for notifying the user of motion blur is not limited thereto. For example, a configuration in which the user is notified of motion blur by outputting sound may be employed. In this case, for example, the control unit <b>101</b> may output a motion blur notification sound from a speaker (not shown) when, of the converted motion blur for each pixel, the percentage of the number of pixels having a converted motion blur of a predetermined value or higher, with respect to the entire screen, is greater than or equal to a predetermined percentage. In this case, the control unit <b>101</b> may divide the range of the converted motion blur using the same method as that described with reference to <figref idref="DRAWINGS">FIG. <b>10</b></figref>, and assign a different volume to each divided range depending on the divergence. The control unit <b>101</b> may then output the motion blur notification sound at a volume corresponding to the divided range including the converted motion blur that occupies the highest percentage in the screen. Alternatively, the control unit <b>101</b> may assign a different type of sound to each divided range depending on the divergence, and output the type of motion blur notification sound corresponding to the divided range including the converted motion blur that occupies the highest percentage in the screen. In other words, the control unit <b>101</b> can change at least one of the type of the sound and the volume in accordance with the divergence.</p><p id="p-0160" num="0152">Additionally, although the present embodiment describes, as a method of making the motion blur notification, an example in which the display color of the motion blur notification icon is changed in accordance with the divergence, the item to be varied in accordance with the divergence is not limited to the display color. For example, a configuration may be employed in which the size of the motion blur notification icon is varied in accordance with the divergence. For example, a configuration may be employed in which the icon size is increased when the divergence is higher, and the icon size is reduced when the divergence is lower. Varying the icon size in accordance with the divergence in this manner makes it possible for the user to easily confirm the intensity of the motion blur.</p><p id="p-0161" num="0153">Accordingly, the notification image generating unit <b>1400</b> can carry out control for displaying blur notification icons, which have different appearances depending on the divergence, along with the preparatory shooting image. For example, the notification image generating unit <b>1400</b> can vary at least one of the color and size of the blur notification icon in accordance with the divergence.</p><p id="p-0162" num="0154">Additionally, although the present embodiment describes, as a method of making the motion blur notification, an example in which the display color of the motion blur notification frames or the motion blur notification edges is changed in accordance with the divergence, the item to be varied in accordance with the divergence is not limited to the display color. For example, a configuration may be employed in which the thickness of the lines of the motion blur notification frames or the motion blur notification edges is varied in accordance with the divergence. For example, a configuration may be employed in which the frames or edges are displayed thicker when there is a higher divergence, and thinner when there is a lower divergence. Varying the line thicknesses in accordance with the divergence in this manner makes it possible for the user to easily confirm the intensity of the motion blur.</p><p id="p-0163" num="0155">Accordingly, the notification image generating unit <b>1400</b> can carry out control for displaying a plurality of blur notification frames, which surround a plurality of divided regions in the preparatory shooting image where motion blur is arising, along with the preparatory shooting image. The notification image generating unit <b>1400</b> can then vary the appearance of each blur notification frame in accordance with the divergence in the divided region corresponding to the blur notification frame. For example, the notification image generating unit <b>1400</b> can vary at least one of the color and thickness of each blur notification frame in accordance with the divergence in the divided region corresponding to the blur notification frame. Note that the region where the blur notification frame is displayed is not limited to the divided region, and a motion blur notification can be made using a blur notification frame for any desired partial region in the preparatory shooting image where motion blur is arising. Additionally, the notification image generating unit <b>1400</b> can display a plurality of edge regions where motion blur is arising in the preparatory shooting image, in an enhanced manner with different forms depending on the divergence in each of the edge regions. The &#x201c;enhanced display&#x201d; includes at least one of displaying the plurality of edge regions with different colors depending on the divergence in each of the edge regions, and displaying the plurality of edge regions using lines of different thicknesses depending on the divergence in each of the edge regions.</p><heading id="h-0011" level="1">Fourth Embodiment</heading><p id="p-0164" num="0156">Although the third embodiment described a configuration in which a motion amount detected from a preparatory shooting image is converted into a motion blur amount arising in actual shooting, the fourth embodiment will describe a configuration in which a motion amount of an image capturing apparatus that is carrying out preparatory shooting is converted into a motion blur amount arising in actual shooting. Although the fourth embodiment will be described with reference to an image capturing apparatus <b>1700</b>, illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref>, instead of the image capturing apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the configuration and operations of the image capturing apparatus <b>1700</b> share some common parts with the image capturing apparatus <b>100</b> according to the third embodiment. The following will primarily describe points that are different from the third embodiment.</p><p id="p-0165" num="0157">The image capturing apparatus <b>1700</b> illustrated in <figref idref="DRAWINGS">FIG. <b>17</b></figref> differs from the image capturing apparatus <b>100</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b></figref> in that the angular velocity detection unit <b>1101</b> has been added, and the image processing unit <b>107</b> has been replaced with an image processing unit <b>1701</b>.</p><p id="p-0166" num="0158">The angular velocity detection unit <b>1101</b> is an angular velocity sensor, for example, which detects the angular velocity of the image capturing apparatus <b>1700</b> itself, caused by the user's hand shaking, the user's camerawork, or the like, in the yaw direction and the pitch direction. Any desired known method can be used as the method by which the angular velocity detection unit <b>1101</b> detects the angular velocity.</p><p id="p-0167" num="0159">The image processing unit <b>1701</b> includes a notification image generating unit <b>1800</b> (described later), which generates a motion blur notification image by superimposing an image plane enabling motion blur to be easily confirmed over an image stored in the RAM <b>103</b>.</p><p id="p-0168" num="0160">An example of the configuration of the notification image generating unit <b>1800</b> included in the image processing unit <b>1701</b> will be described next with reference to <figref idref="DRAWINGS">FIG. <b>18</b></figref>. The notification image generating unit <b>1800</b> illustrated in <figref idref="DRAWINGS">FIG. <b>18</b></figref> differs from the notification image generating unit <b>1400</b> illustrated in <figref idref="DRAWINGS">FIG. <b>14</b></figref> in that the motion vector calculation unit <b>301</b> has been replaced with the image motion information conversion unit <b>1201</b>. The operations of the notification image generating unit <b>1800</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIG. <b>19</b></figref>.</p><p id="p-0169" num="0161">Next, the process by which the notification image generating unit <b>1800</b> generates the motion blur notification image (step S<b>204</b> in <figref idref="DRAWINGS">FIG. <b>2</b></figref>) will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>19</b></figref>. In <figref idref="DRAWINGS">FIG. <b>19</b></figref>, the processes of steps S<b>401</b>, S<b>403</b>, S<b>404</b>, and S<b>406</b> are the same as the processes described in the first embodiment with reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>. Additionally, the process of step S<b>1301</b> is the same as the process described in the second embodiment with reference to <figref idref="DRAWINGS">FIG. <b>13</b></figref>. The process of step S<b>1501</b> is the same as the process described in the third embodiment with reference to <figref idref="DRAWINGS">FIG. <b>15</b></figref>.</p><p id="p-0170" num="0162">In step S<b>1901</b>, the motion blur notification plane generation unit <b>1402</b> creates an image plane for notifying the user of the motion blur (the motion blur notification plane) on the basis of the converted motion blur calculated in step S<b>404</b>. The process of step S<b>1901</b> is the same as in the third embodiment (step S<b>1502</b> in <figref idref="DRAWINGS">FIG. <b>15</b></figref>) in that the motion blur notification plane is created with different forms depending on the divergence between the converted motion blur and the target motion blur. However, in the fourth embodiment, the motion vectors are uniform for all of the pixels, and thus the divergence between the converted motion blur and the target motion blur is also uniform for all of the pixels, which means that a histogram such as that illustrated in <figref idref="DRAWINGS">FIG. <b>10</b></figref> cannot be generated. Accordingly, the motion blur notification plane generation unit <b>1402</b> determines the form of the motion blur notification plane by, for example, comparing the divergence that is uniform for all of the pixels to one or more thresholds. For example, assume that D represents the divergence, and T1, T2, and T3 represent three thresholds (T1&#x3c;T2&#x3c;T3). If T1&#x2264;D&#x3c;T2, the motion blur notification plane generation unit <b>1402</b> creates the blue motion blur notification icon <b>903</b> (<figref idref="DRAWINGS">FIG. <b>9</b>C</figref>) as the motion blur notification plane. If T2&#x2264;D&#x3c;T3, the motion blur notification plane generation unit <b>1402</b> creates the green motion blur notification icon <b>902</b> (<figref idref="DRAWINGS">FIG. <b>9</b>B</figref>) as the motion blur notification plane. If T3&#x2264;D, the motion blur notification plane generation unit <b>1402</b> creates the red motion blur notification icon <b>901</b> (<figref idref="DRAWINGS">FIG. <b>9</b>A</figref>) as the motion blur notification plane.</p><p id="p-0171" num="0163">According to the fourth embodiment as described thus far, the image capturing apparatus <b>1700</b> converts the motion amount of the image capturing apparatus <b>1700</b> carrying out preparatory shooting into the motion blur amount that will arise in the actual shooting. This makes it possible to obtain the converted motion blur with a comparatively light processing load.</p><heading id="h-0012" level="1">Fifth Embodiment</heading><p id="p-0172" num="0164">According to Japanese Patent Laid-Open No. 2008-172667, detecting a region of motion between images captured in time series during preparatory shooting, and emphasizing that region of motion, makes it easy to prompt the user to reduce blur.</p><p id="p-0173" num="0165">However, Japanese Patent Laid-Open No. 2008-172667 does not take into account shooting an image at an appropriate framerate, shutter speed, and so on in accordance with the speed, movement amount, and so on of the object, as a way to extract the region of motion between images in time series. For example, detecting motion accurately requires that the moving object be shot without cumulative blur, and to accomplish this, it is necessary to increase the shutter speed. Furthermore, for objects moving at high speeds, the range in which the moving object can be detected is limited as well, and it is therefore necessary to increase the framerate so as to reduce the amount of movement between images. However, carrying out preparatory shooting at a high framerate reduces the quality of the preparatory shooting image, and also consumes more power.</p><p id="p-0174" num="0166">In light of such circumstances, the present embodiment will describe the configuration for striking a desirable balance between the accuracy of motion detection and improvement in the quality of the preparatory shooting image.</p><p id="p-0175" num="0167">In the fifth embodiment, the basic configuration of the image capturing apparatus <b>100</b> is the same as in the first embodiment (see <figref idref="DRAWINGS">FIG. <b>1</b></figref>). The following will primarily describe points that are different from the first embodiment.</p><p id="p-0176" num="0168">A shooting process executed by the image capturing apparatus <b>100</b> will be described with reference to <figref idref="DRAWINGS">FIG. <b>20</b></figref>. Unless otherwise specified, the processes in the respective steps of this flowchart are realized by the control unit <b>101</b> executing the aforementioned control programs. The processing illustrated in this flowchart starts when the user turns the image capturing apparatus <b>100</b> on and an operating mode of the image capturing apparatus <b>100</b> enters a shooting mode.</p><p id="p-0177" num="0169">In step S<b>2008</b>, the control unit <b>101</b> determines shooting conditions for the preparatory shooting in order to detect the motion amount of an object in the composition. Specifically, the control unit <b>101</b> calculates shooting conditions suited to the detection of the motion amount of the object in the composition on the basis of a relationship between the motion amount of the object detected from a preparatory shooting image shot under initial shooting conditions, and a target blur amount (target motion blur) at which a motion blur notification is to be made. The process of step S<b>2008</b> will be described in detail later with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref>.</p><p id="p-0178" num="0170">Next, the process by which the control unit <b>101</b> determines the shooting conditions for preparatory shooting (step S<b>2008</b> in <figref idref="DRAWINGS">FIG. <b>20</b></figref>) will be described in detail with reference to <figref idref="DRAWINGS">FIG. <b>21</b></figref>.</p><p id="p-0179" num="0171">In step S<b>2101</b>, the control unit <b>101</b> sets the initial shooting conditions in the image capturing unit <b>105</b>, and captures the preparatory shooting image in a continuous manner. Under the initial shooting conditions, the framerate and the shutter speed are set to the highest (fastest) values within a range that does not affect processing for calculating evaluation values used in the auto function control carried out by a typical camera, such as automatic exposure (AE) and autofocus (AF) control. An appropriate aperture value and ISO sensitivity are also set so that the image can be shot under appropriate exposure conditions using the fast shutter speed which has been set. Accordingly, a moving object in the preparatory shooting image captured under the initial shooting conditions has a comparatively low amount of cumulative blur. There is also a comparatively low amount of movement in the object between the consecutive images. As such, the motion amount can be detected with a high level of accuracy, even for an object moving at high speed. However, the ISO sensitivity is set to a comparatively high value in order to achieve the appropriate exposure conditions, which means that the preparatory shooting image contains a comparatively large amount of noise.</p><p id="p-0180" num="0172">In step S<b>2102</b>, the notification image generating unit <b>300</b> calculates motion vectors between the preparatory shooting images shot under the initial shooting conditions, under the control of the control unit <b>101</b>, so as to calculate a motion amount of the object (the magnitude of the motion vectors). The method for calculating the motion vectors is the same as the calculation method described in the first embodiment with reference to step S<b>402</b> in <figref idref="DRAWINGS">FIG. <b>4</b></figref>.</p><p id="p-0181" num="0173">In step S<b>2103</b>, the control unit <b>101</b> specifies a relationship between the motion amount of the object, calculated in step S<b>2102</b>, and the target motion blur. The &#x201c;target motion blur&#x201d; is a permissible motion amount at which the motion amount of the object in the captured image does not produce motion blur that stands out. The permissible motion amount is determined on the basis of the size and the number of pixels of the image sensor, which is a CCD, a CMOS sensor, or the like, and the resolution of the display in which the shot image is displayed. For example, if the size of the image sensor is APS-C and the number of pixels is 200,000 pixels, and the resolution of the display is full HD (1920&#xd7;1080 pixels), the permissible motion amount is assumed to be 5 pixels or less. In this case, adjusting the framerate so that motion less than or equal to 5 pixels can be detected between images makes it possible to detect the motion of the object with an appropriate level of accuracy.</p><p id="p-0182" num="0174">A specific example of the relationship between the motion amount of the object and the target motion blur will be described with reference to <figref idref="DRAWINGS">FIGS. <b>22</b>A and <b>22</b>B</figref>. <figref idref="DRAWINGS">FIGS. <b>22</b>A and <b>22</b>B</figref> express the relationship between the motion amount of the object, calculated from the preparatory shooting image, and the target motion blur (permissible motion amount), using the magnitude (number of pixels) of motion vectors. <figref idref="DRAWINGS">FIG. <b>22</b>A</figref> illustrates a state in which the relationship between the motion amount of the object and the target motion blur is not appropriate, whereas <figref idref="DRAWINGS">FIG. <b>22</b>B</figref> illustrates a state in which the relationship between the motion amount of the object and the target motion blur is appropriate.</p><p id="p-0183" num="0175">The state illustrated in <figref idref="DRAWINGS">FIG. <b>22</b>A</figref> is a state in which image capturing control for the preparatory shooting image is carried out using a framerate of 60 fps as the initial shooting conditions. At this time, a motion vector <b>2201</b> of the object is 10 pixels, and a motion vector <b>2202</b> corresponding to the permissible motion amount is 5 pixels, which means that the motion vector <b>2201</b> of the object is larger than the motion vector <b>2202</b> corresponding to the permissible motion amount. In this case, despite the motion vector <b>2202</b> corresponding to the permissible motion amount being 5 pixels, the motion amount of the object can only be detected in units of 10 pixels. This means that there is too much movement in the object for the framerate of 60 fps in the preparatory shooting. In particular, when the object is soft or is undergoing rotational movement, if the motion amount for detection is too coarse (the unit is too great), it becomes difficult to detect the correct motion, which in turn makes it difficult to make a correct motion blur notification and the process for notifying the user of motion blur for the actual shooting (described later).</p><p id="p-0184" num="0176">The state illustrated in <figref idref="DRAWINGS">FIG. <b>22</b>B</figref> is a state in which image capturing control for the preparatory shooting image is carried out using a framerate of 120 fps as the shooting conditions. At this time, a motion vector <b>2203</b> of the object is 5 pixels, and a motion vector <b>2204</b> corresponding to the permissible motion amount is 5 pixels, which means that the motion vector <b>2203</b> of the object indicates the same movement amount as the motion vector <b>2204</b> corresponding to the permissible motion amount. In other words, it can be seen that the motion amount can be detected at the appropriate unit (that is, units of 5 pixels) at 120 fps. Thus if the motion vectors can be detected at units less than or equal to the permissible motion amount in this manner, whether or not the motion blur in the actual shooting will exceed the permissible motion amount can be determined with a high level of accuracy.</p><p id="p-0185" num="0177">Expressing the conditions for detecting motion with a high level of accuracy as a relational expression between the motion blur of the object and the target motion amount results in the following Equation (8).</p><p id="p-0186" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?><i>n</i>=magnitude of motion vector/permissible motion amount&#x2003;&#x2003;(8)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0187" num="0178">Here, n being greater than 1 indicates that the unit at which the motion amount can be detected is too high, whereas n being less than or equal to 1 indicates that the motion amount can be detected at the appropriate unit.</p><p id="p-0188" num="0179">In addition to a method in which the permissible motion amount is set in advance in consideration of the accuracy of the detection of the motion blur during the actual shooting, the permissible motion amount can also be determined in response to user instructions. Alternatively, the permissible motion amount can be determined in accordance with the amount of motion blur after converting the length of the detected motion vectors into a magnitude of motion vectors corresponding to the shutter speed for the actual shooting, which will be described later.</p><p id="p-0189" num="0180">Returning to <figref idref="DRAWINGS">FIG. <b>21</b></figref>, in step S<b>2104</b>, the control unit <b>101</b> calculates the shooting conditions at which the motion amount can be detected at the appropriate unit, on the basis of the relationship between the motion amount of the object and the target motion blur specified in step S<b>2103</b>; then, the shooting conditions for the preparatory shooting are changed to the calculated shooting conditions. The control unit <b>101</b> then controls the image capturing unit <b>105</b> to carry out the preparatory shooting under the post-change shooting conditions.</p><p id="p-0190" num="0181">Note that the appropriate shooting conditions for detecting motion are not necessarily achieved simply by increasing the framerate. If the framerate is too high, the exposure time will be shortened, resulting in images that contain a high amount of noise, particularly for dark locations; in this case, it is possible that adverse effects will arise with respect to detecting motion, such as the inability to accurately determine the signals from the object. As such, the &#x201c;appropriate shooting conditions for detecting motion&#x201d; correspond to an exposure that is as long as possible, within an exposure time at which the motion amount can be detected at a unit less than or equal to the permissible motion amount, and at which that motion amount can be detected.</p><p id="p-0191" num="0182">Accordingly, the control unit <b>101</b> calculates the framerate and the shutter speed, which are the primary factors in the shooting conditions, using Equations (9) and (10).</p><p id="p-0192" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>framerate(fps)=initial framerate&#xd7;<i>n</i>&#x2003;&#x2003;(9)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0193" num="0000"><br/><?in-line-formulae description="In-line Formulae" end="lead"?>shutter speed(<i>s</i>)=initial shutter speed&#xd7;1/<i>n</i>&#x2003;&#x2003;(10)<?in-line-formulae description="In-line Formulae" end="tail"?></p><p id="p-0194" num="0000">Here, n is a value determined through the aforementioned Equation (8).</p><p id="p-0195" num="0183">The framerate and shutter speed calculated in this manner are important shooting conditions for detecting movement. Additionally, to capture an image of the appropriate brightness, the control unit <b>101</b> also carries out control for changing the aperture value and the ISO sensitivity in accordance with the changes made to the framerate and the shutter speed, so that the exposure value does not change.</p><p id="p-0196" num="0184">According to Equations (9) and (10), when n is less than 1, as in the examples illustrated in <figref idref="DRAWINGS">FIGS. <b>22</b>A and <b>22</b>B</figref>, the framerate and shutter speed both increase. Accordingly, the motion amount can be detected at a unit necessary for detecting the motion with a high level of accuracy. Additionally, according to Equations (9) and (10), when n is less than 1, the framerate and shutter speed both decrease. Accordingly, situations where the framerate and shutter speed are increased more than is necessary, resulting in a drop in the quality of the preparatory shooting image, can be suppressed.</p><p id="p-0197" num="0185">Note that there are cases where the framerates that can be set by the control unit <b>101</b> have discrete values. Thus, to be more exact, the control unit <b>101</b> does not need to set the initial framerate to strictly n times, but rather may set the initial framerate to approximately n times (approximately (motion amount)/(target motion blur amount) times). The range indicated by &#x201c;approximately&#x201d; is determined as appropriate in accordance with the framerate values that can be set by the control unit <b>101</b>. The same applies to the shutter speed, and thus the control unit <b>101</b> may set the initial shutter speed to approximately 1/n times.</p><p id="p-0198" num="0186">Furthermore, the control unit <b>101</b> may change the initial framerate to a value higher (or lower) that approximately n times. Even in this case, changing the framerate of the preparatory shooting on the basis of a difference between the magnitude of the motion vectors (the motion amount) and the permissible motion amount (the target motion blur amount) makes it possible to achieve the desired balance between improving the accuracy of the motion detection and improving the quality of the preparatory shooting image.</p><p id="p-0199" num="0187"><figref idref="DRAWINGS">FIGS. <b>23</b>A and <b>23</b>B</figref> are timing charts illustrating a process by which the control unit <b>101</b> determines shooting conditions for preparatory shooting (step S<b>2008</b> in <figref idref="DRAWINGS">FIG. <b>20</b></figref>). <figref idref="DRAWINGS">FIGS. <b>23</b>A and <b>23</b>B</figref> indicate the timings of the preparatory shooting and the actual shooting. <figref idref="DRAWINGS">FIG. <b>23</b>A</figref> illustrates the timings of the preparatory shooting and the actual shooting when the process of step S<b>2008</b> is not carried out, whereas <figref idref="DRAWINGS">FIG. <b>23</b>B</figref> illustrates the timings of the preparatory shooting and the actual shooting when the process of step S<b>2008</b> is carried out. Reference signs <b>2301</b> to <b>2318</b> indicate pairs of sequential and adjacent images necessary for detecting motion.</p><p id="p-0200" num="0188">In the case of <figref idref="DRAWINGS">FIG. <b>23</b>A</figref> (when the process of step S<b>2008</b> is not carried out), the control unit <b>101</b> carries out preparatory shooting continuously at a fixed, high framerate, for example. In other words, the control unit <b>101</b> continuously detects the motion amount of the object between the adjacent preparatory shooting images shot under the initial shooting conditions (reference signs <b>2301</b> to <b>2312</b>), until the actual shooting is carried out. Upon detecting that the user has depressed the shutter button, the control unit <b>101</b> carries out the actual shooting (reference sign <b>2321</b>).</p><p id="p-0201" num="0189">On the other hand, in the case illustrated in <figref idref="DRAWINGS">FIG. <b>23</b>B</figref> (when the process of step S<b>2008</b> is carried out), the preparatory shooting is carried out under the initial shooting conditions until the timings indicated by reference signs <b>2313</b> to <b>2316</b>, in the same manner as the timings indicated by the reference signs <b>2301</b> to <b>2304</b>. However, due to the process of step S<b>2008</b>, the shooting conditions for the preparatory shooting are changed on the basis of the relationship between the motion amount of the object and the target motion blur, and thus the preparatory shooting is carried out under the post-change shooting conditions at the timings indicated by reference signs <b>2317</b> and <b>2318</b>. Although <figref idref="DRAWINGS">FIG. <b>22</b>B</figref> illustrates an example in which the frame rate increases, <figref idref="DRAWINGS">FIG. <b>23</b>B</figref> illustrates an example in which the framerate decreases. The actual shooting (reference sign <b>2322</b>) is carried out thereafter.</p><p id="p-0202" num="0190">The processes following step S<b>2008</b> are the same as those described in the first embodiment (<figref idref="DRAWINGS">FIG. <b>2</b></figref>). However, in the process for converting the motion vectors from the preparatory shooting into the motion blur that will arise in the actual shooting (step S<b>404</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>, which is a part of step S<b>204</b>), the motion vectors from the preparatory shooting are motion vectors corresponding to the shooting conditions changed by the process of step S<b>2008</b>. As such, when, for example, the change has been made as indicated in <figref idref="DRAWINGS">FIG. <b>22</b>B</figref>, the conversion is carried out as indicated in <figref idref="DRAWINGS">FIG. <b>24</b></figref> (instead of <figref idref="DRAWINGS">FIG. <b>8</b></figref>). If it is determined in step S<b>206</b> that the shutter button has not been pressed, the process returns to step S<b>2008</b>. In this case, in step S<b>2101</b> of <figref idref="DRAWINGS">FIG. <b>21</b></figref>, the shooting conditions that have been changed as a result of the processing in the previous step S<b>2008</b> are used instead of the initial shooting conditions.</p><p id="p-0203" num="0191">Note that with respect to the determination (change) of the shooting conditions in step S<b>2008</b>, the control unit <b>101</b> may be configured to change the conditions upon detecting that the motion of the object is in a stable state. Additionally, the control unit <b>101</b> may, on the basis of changes in the motion vectors, determine whether the motion of the object is in a stable state after the shooting conditions for the preparatory shooting have been changed once, and may then change the shooting conditions for the preparatory shooting again if the amount of change in the motion vectors per unit of time has increased. In other words, if a change has occurred in the motion amount detected from the preparatory shooting image after the shooting conditions such as the framerate have been changed, the control unit <b>101</b> may change the shooting conditions such as the framerate again.</p><p id="p-0204" num="0192">According to the fifth embodiment as described above, the image capturing apparatus <b>100</b> changes the framerate for the preparatory shooting on the basis of a difference between the motion amount detected from the preparatory shooting image, and the target motion blur amount. This makes it possible to achieve the desired balance between improving the accuracy of the motion detection and improving the quality of the preparatory shooting image.</p><p id="p-0205" num="0193">Note that by combining the configuration of the third embodiment with the configuration of the present embodiment, the user can be notified of motion blur using three colors (red, green, and blue) depending on the amount of converted motion blur. In this case, the control unit <b>101</b> sets the permissible motion amount so that the narrowest range of the motion amount can be detected, so that it is possible to express the range of the motion amount represented by each color. For example, if the colors representing the motion blur relative to the magnitude of the motion vectors are 0 to 10 pixels for blue, 11 to 15 pixels for green, and 16 pixels or more for red, the range for green is the narrowest section as a motion amount. As such, the control unit <b>101</b> sets 5 pixels, i.e., the range for green (11 to 15), as the permissible motion amount.</p><p id="p-0206" num="0194">Although the present embodiment describes an example in which the framerate and the shutter speed are changed as the shooting conditions for the preparatory shooting in order to detect motion, it is also possible to change only one of these conditions.</p><p id="p-0207" num="0195">Additionally, a configuration may be employed in which the permissible motion amount is set as the magnitude of motion vectors within a range that does not exceed a limit range for motion detection, in a situation where the user has not intentionally determined the permissible motion amount. At this time, it is necessary to determine whether the motion of the object has exceeded the range for motion detection. Using an SAD value during motion detection to detect whether an amount of change has dropped below a predetermined value, measuring motion of the image capturing apparatus <b>100</b> itself using gyro information of the like, and so on can be used as methods to make this determination. If it is determined that the permissible motion amount has been exceeded, the control unit <b>101</b> carries out control for increasing the shutter speed, the framerate, or the like.</p><p id="p-0208" num="0196">Furthermore, if the object is moving quickly and the framerate for the preparatory shooting cannot be changed, the control unit <b>101</b> may reduce the shutter speed only, within a range that does not lead to a drop in the framerate.</p><heading id="h-0013" level="1">OTHER EMBODIMENTS</heading><p id="p-0209" num="0197">Embodiment(s) of the present invention can also be realized by a computer of a system or apparatus that reads out and executes computer executable instructions (e.g., one or more programs) recorded on a storage medium (which may also be referred to more fully as a &#x2018;non-transitory computer-readable storage medium&#x2019;) to perform the functions of one or more of the above-described embodiment(s) and/or that includes one or more circuits (e.g., application specific integrated circuit (ASIC)) for performing the functions of one or more of the above-described embodiment(s), and by a method performed by the computer of the system or apparatus by, for example, reading out and executing the computer executable instructions from the storage medium to perform the functions of one or more of the above-described embodiment(s) and/or controlling the one or more circuits to perform the functions of one or more of the above-described embodiment(s). The computer may comprise one or more processors (e.g., central processing unit (CPU), micro processing unit (MPU)) and may include a network of separate computers or separate processors to read out and execute the computer executable instructions. The computer executable instructions may be provided to the computer, for example, from a network or the storage medium. The storage medium may include, for example, one or more of a hard disk, a random-access memory (RAM), a read only memory (ROM), a storage of distributed computing systems, an optical disk (such as a compact disc (CD), digital versatile disc (DVD), or Blu-ray Disc (BD)&#x2122;), a flash memory device, a memory card, and the like.</p><p id="p-0210" num="0198">While the present invention has been described with reference to exemplary embodiments, it is to be understood that the invention is not limited to the disclosed exemplary embodiments. The scope of the following claims is to be accorded the broadest interpretation so as to encompass all such modifications and equivalent structures and functions.</p><p id="p-0211" num="0199">This application claims the benefit of Japanese Patent Application No. 2018-248373, filed Dec. 28, 2018 which is hereby incorporated by reference herein in its entirety.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-math idrefs="MATH-US-00001 MATH-US-00001-2" nb-file="US20230007155A1-20230105-M00001.NB"><img id="EMI-M00001" he="19.73mm" wi="76.20mm" file="US20230007155A1-20230105-M00001.TIF" alt="embedded image " img-content="math" img-format="tif"/></us-math><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A notifying apparatus comprising at least one processor and/or at least one circuit which functions as:<claim-text>a detecting unit configured to detect, in a predetermined period, a motion amount of an image capturing apparatus carrying out first shooting that is carried out repeatedly at predetermined intervals of time;</claim-text><claim-text>a converting unit configured to convert the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined period and an exposure time used in the second shooting; and</claim-text><claim-text>a notifying unit configured to make a notification of motion blur on the basis of the motion blur amount,</claim-text><claim-text>wherein the notifying unit changes a form of the notification in accordance with a magnitude of the motion blur amount.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The notifying apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor and/or at least one circuit is further configured to function as:<claim-text>a display unit configured to display the image obtained from the first shooting,</claim-text><claim-text>wherein the notifying unit controls the display unit to display an icon having a different appearance depending on the magnitude of the motion blur amount along with the image.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The notifying apparatus according to <claim-ref idref="CLM-00002">claim 2</claim-ref>,<claim-text>wherein the notifying unit changes at least one of a color and size of the icon in accordance with the magnitude of the motion blur amount.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The notifying apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor and/or at least one circuit is further configured to function as:<claim-text>a display unit configured to display the image obtained from the first shooting,</claim-text><claim-text>wherein the notifying unit controls the display unit to display a plurality of frames surrounding a plurality of partial regions where motion blur is arising in the image, along with the image, and changes an appearance of each frame in accordance with the magnitude of the motion blur amount in the partial region corresponding to that frame.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The notifying apparatus according to <claim-ref idref="CLM-00004">claim 4</claim-ref>,<claim-text>wherein the notifying unit changes at least one of a color and a thickness of each frame in accordance with the magnitude of the motion blur amount in the partial region corresponding to that frame.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The notifying apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the at least one processor and/or at least one circuit is further configured to function as:<claim-text>a display unit configured to display the image obtained from the first shooting,</claim-text><claim-text>wherein the notifying unit controls the display unit to display a plurality of edge regions where motion blur is arising in the image in an enhanced manner with different appearances depending on the magnitude of the motion blur amount in each of those edge regions.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The notifying apparatus according to <claim-ref idref="CLM-00006">claim 6</claim-ref>,<claim-text>wherein the enhanced display includes at least one of displaying the plurality of edge regions with different colors depending on the magnitude of the motion blur amount in each of those edge regions, and displaying the plurality of edge regions with lines of different thicknesses depending on the magnitude of the motion blur amount in each of those edge regions.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. An image capturing apparatus comprising:<claim-text>the notifying apparatus according to <claim-ref idref="CLM-00001">claim 1</claim-ref>; and</claim-text><claim-text>an image sensor.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. A notifying method executed by a notifying apparatus, comprising:<claim-text>detecting, in a predetermined period, a motion amount of an image capturing apparatus carrying out first shooting that is carried out repeatedly at predetermined intervals of time;</claim-text><claim-text>converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined period and an exposure time used in the second shooting; and</claim-text><claim-text>making a notification of motion blur on the basis of the motion blur amount,</claim-text><claim-text>wherein a form of the notification is changed in accordance with a magnitude of the motion blur amount.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. A non-transitory computer-readable storage medium which stores a program for causing a computer to execute a notifying method comprising:<claim-text>detecting, in a predetermined period, a motion amount of an image capturing apparatus carrying out first shooting that is carried out repeatedly at predetermined intervals of time;</claim-text><claim-text>converting the motion amount into a motion blur amount that will arise in second shooting, on the basis of the predetermined period and an exposure time used in the second shooting; and</claim-text><claim-text>making a notification of motion blur on the basis of the motion blur amount,</claim-text><claim-text>wherein a form of the notification is changed in accordance with a magnitude of the motion blur amount.</claim-text></claim-text></claim></claims></us-patent-application>