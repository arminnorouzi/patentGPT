<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004166A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004166</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942804</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>02</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0221</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0274</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0234</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>05</class><subclass>D</subclass><main-group>1</main-group><subgroup>0287</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">SYSTEMS AND METHODS FOR ROUTE SYNCHRONIZATION FOR ROBOTIC DEVICES</invention-title><us-related-documents><continuation><relation><parent-doc><document-id><country>US</country><doc-number>PCT/US21/22125</doc-number><date>20210312</date></document-id><parent-status>PENDING</parent-status></parent-doc><child-doc><document-id><country>US</country><doc-number>17942804</doc-number></document-id></child-doc></relation></continuation><us-provisional-application><document-id><country>US</country><doc-number>62989026</doc-number><date>20200313</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>Brain Corporation</orgname><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Sackinger</last-name><first-name>Dan</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Cannon</last-name><first-name>Jarad</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Klein</last-name><first-name>Josh</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Woodlands</last-name><first-name>Daniel</first-name><address><city>San Diego</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Systems and methods for route synchronization between two or more robots to allow for a single training run of a route to effectively train multiple robots to follow the route.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="141.14mm" wi="144.10mm" file="US20230004166A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="262.04mm" wi="156.29mm" orientation="landscape" file="US20230004166A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="177.88mm" wi="144.27mm" orientation="landscape" file="US20230004166A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="230.72mm" wi="147.24mm" orientation="landscape" file="US20230004166A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="145.46mm" wi="148.25mm" orientation="landscape" file="US20230004166A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="146.73mm" wi="152.15mm" orientation="landscape" file="US20230004166A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="146.13mm" wi="155.02mm" orientation="landscape" file="US20230004166A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="242.06mm" wi="152.57mm" orientation="landscape" file="US20230004166A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="242.06mm" wi="152.57mm" orientation="landscape" file="US20230004166A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="213.19mm" wi="149.69mm" orientation="landscape" file="US20230004166A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="209.80mm" wi="141.22mm" orientation="landscape" file="US20230004166A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="194.65mm" wi="149.35mm" orientation="landscape" file="US20230004166A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="268.48mm" wi="151.81mm" orientation="landscape" file="US20230004166A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="229.36mm" wi="149.86mm" orientation="landscape" file="US20230004166A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">PRIORITY</heading><p id="p-0002" num="0001">This application is a continuation of International Patent Application No. PCT/US21/22125 filed Mar. 12, 2021 and claims the benefit of U.S. Provisional Patent Application Ser. No. 62/989,026 filed on Mar. 13, 2020 under 35 U.S.C. &#xa7; 119, the entire disclosure of each is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">COPYRIGHT</heading><p id="p-0003" num="0002">A portion of the disclosure of this patent document contains material that is subject to copyright protection. The copyright owner has no objection to the facsimile reproduction by anyone of the patent document or the patent disclosure, as it appears in the Patent and Trademark Office patent files or records, but otherwise reserves all copyright rights whatsoever.</p><heading id="h-0003" level="1">BACKGROUND</heading><heading id="h-0004" level="1">Technological Field</heading><p id="p-0004" num="0003">The present application relates generally to robotics, and more specifically to systems and methods for route synchronization for robotic devices.</p><heading id="h-0005" level="1">SUMMARY</heading><p id="p-0005" num="0004">The foregoing needs are satisfied by the present disclosure, which provides for, inter alia, systems and methods for route synchronization for robotic devices. The systems and methods herein are directed towards a practical application of data collection, management, and robotic path navigation to drastically reduce time spent by human operators training multiple robots to follow multiple routes.</p><p id="p-0006" num="0005">Exemplary embodiments described herein have innovative features, no single one of which is indispensable or solely responsible for their desirable attributes. Without limiting the scope of the claims, some of the advantageous features will now be summarized. One skilled in the art would appreciate that as used herein, the term robot may generally refer to an autonomous vehicle or object that travels a route, executes a task, or otherwise moves automatically upon executing or processing computer-readable instructions.</p><p id="p-0007" num="0006">According to at least one non-limiting exemplary embodiment, a method, a non-transitory computer-readable medium or a system for causing a succeeding robot to navigate a route previously navigated by a preceding robot is disclosed. The method comprising the succeeding robot receiving a computer-readable map, the computer-readable map being produced based on data collected by at least one sensor of the preceding robot during navigation of the route by the preceding robot at a preceding instance in time; and navigating the route at a second instance in time by the succeeding robot based on the computer-readable map, the preceding instance in time being before the succeeding instance in time.</p><p id="p-0008" num="0007">According to at least one non-limiting exemplary embodiment, the preceding robot, upon completing the route, communicates the computer-readable map to a server communicatively coupled to both the first robot and preceding robot.</p><p id="p-0009" num="0008">According to at least one non-limiting exemplary embodiment, the preceding robot is navigating the route for an initial time in a training mode during the preceding instance in time, and the succeeding robot navigates the route for the succeeding time by recreating the route executed by the preceding robot during the preceding instance in time.</p><p id="p-0010" num="0009">According to at least one non-limiting exemplary embodiment, the route begins and ends proximate to a landmark or feature recognizable by sensors of the succeeding and preceding robots.</p><p id="p-0011" num="0010">According to at least one non-limiting exemplary embodiment, the computer-readable map comprises a pose graph indicative of positions of the robot during navigation of the route.</p><p id="p-0012" num="0011">According to at least one non-limiting exemplary embodiment, the method may further comprise synchronizing data with a server upon initializing the succeeding robot from an idle or off state, the synchronized data comprising at least the computer-readable map of the route.</p><p id="p-0013" num="0012">These and other objects, features, and characteristics of the present disclosure, as well as the methods of operation and functions of the related elements of structure and the combination of parts and economies of manufacture, will become more apparent upon consideration of the following description and the appended claims with reference to the accompanying drawings, all of which form a part of this specification, wherein like reference numerals designate corresponding parts in the various figures. It is to be expressly understood, however, that the drawings are for the purpose of illustration and description only and are not intended as a definition of the limits of the disclosure. As used in the specification and in the claims, the singular form of &#x201c;a,&#x201d; &#x201c;an,&#x201d; and &#x201c;the&#x201d; include plural referents unless the context clearly dictates otherwise.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0006" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0014" num="0013">The disclosed aspects will hereinafter be described in conjunction with the appended drawings, provided to illustrate and not to limit the disclosed aspects, wherein like designations denote like elements.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a functional block diagram of a robot in accordance with some embodiments of this disclosure.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is a functional block diagram of a controller or processor in accordance with some embodiments of this disclosure.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a functional block diagram of a cloud server and communicatively coupled to devices thereto, in accordance with some embodiments of this disclosure.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a process flow diagram illustrating a method for a controller of a robot to initialize the robot to facilitate route synchronization, according to an exemplary embodiment.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a process flow diagram illustrating a method for a controller of a robot to navigate a route to facilitate route synchronization, according to an exemplary embodiment.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a process flow diagram illustrating a method for a controller of a robot to learn a new route and synchronize the new route with other robots within its environment, according to an exemplary embodiment.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIGS. <b>6</b>A-B</figref> is a top down view of two robots utilizing route synchronization to navigate a new route, according to an exemplary embodiment.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a top down or birds-eye view of a map comprising a route navigated by a preceding robot, according to an exemplary embodiment.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a top down view of a map comprising a succeeding, larger robot modifying a route received from a preceding small robot to enable the succeeding robot to follow the route of the preceding robot without collisions with objects, according to an exemplary embodiment.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a process flow diagram illustrating a method for a controller of a robot to determine if a route received from a different type or sized robot is navigable, according to an exemplary embodiment.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIGS. <b>9</b>A-B</figref> illustrate two robots synchronizing binary data from a server using metadata stored in ledgers, according to an exemplary embodiment.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates binding trees used to synchronize routes between two robots as illustrated in <figref idref="DRAWINGS">FIGS. <b>9</b>A-B</figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><p id="p-0027" num="0026">All Figures disclosed herein are &#xa9; Copyright <b>2021</b> Brain Corporation. All rights reserved.</p><heading id="h-0007" level="1">DETAILED DESCRIPTION</heading><p id="p-0028" num="0027">Currently, many robots navigate along predetermined routes or paths, only deviating slightly from the routes to avoid obstacles. Many robots may operate within a single environment, such as a warehouse, department store, airport, and the like. Training multiple robots to follow multiple different routes can become very time consuming for operators of these robots. Training a robot typically comprises pushing, leading, or otherwise indicating a path for the robot to follow and requires human input. The time required to train multiple robots to follow multiple routes scales multiplicatively with the number of robots and number of routes, thereby causing human operators to spend a substantial amount of time training the robots for each route. Alternatively, separate robots may be designated separate routes, however this limits the utility of each individual robot to a select few routes. Accordingly, there is a need in the art for systems and methods for route synchronization between two or more robots to allow for a single training run of a route to effectively train multiple robots to follow the route.</p><p id="p-0029" num="0028">Various aspects of the novel systems, apparatuses, and methods disclosed herein are described more fully hereinafter with reference to the accompanying drawings. This disclosure can, however, be embodied in many different forms and should not be construed as limited to any specific structure or function presented throughout this disclosure. Rather, these aspects are provided so that this disclosure will be thorough and complete, and will fully convey the scope of the disclosure to those skilled in the art. Based on the teachings herein, one skilled in the art would appreciate that the scope of the disclosure is intended to cover any aspect of the novel systems, apparatuses, and methods disclosed herein, whether implemented independently of, or combined with, any other aspect of the disclosure. For example, an apparatus may be implemented or a method may be practiced using any number of the aspects set forth herein. In addition, the scope of the disclosure is intended to cover such an apparatus or method that is practiced using other structure, functionality, or structure and functionality in addition to or other than the various aspects of the disclosure set forth herein. It should be understood that any aspect disclosed herein may be implemented by one or more elements of a claim.</p><p id="p-0030" num="0029">Although particular aspects are described herein, many variations and permutations of these aspects fall within the scope of the disclosure. Although some benefits and advantages of the preferred aspects are mentioned, the scope of the disclosure is not intended to be limited to particular benefits, uses, and/or objectives. The detailed description and drawings are merely illustrative of the disclosure rather than limiting, the scope of the disclosure being defined by the appended claims and equivalents thereof.</p><p id="p-0031" num="0030">The present disclosure provides for systems and methods for route synchronization for robotic devices. As used herein, a robot may include mechanical and/or virtual entities configurable to carry out a complex series of tasks or actions autonomously. In some exemplary embodiments, robots may be machines that are guided and/or instructed by computer programs and/or electronic circuitry. In some exemplary embodiments, robots may include electro-mechanical components that are configurable for navigation, where the robot may move from one location to another. Such robots may include autonomous and/or semi-autonomous cars, floor cleaners, rovers, drones, planes, boats, carts, trams, wheelchairs, industrial equipment, stocking machines, mobile platforms, personal transportation devices (e.g., hover boards, scooters, self-balancing vehicles such as manufactured by Segway, etc.), trailer movers, vehicles, and the like. Robots may also include any autonomous and/or semi-autonomous machine for transporting items, people, animals, cargo, freight, objects, luggage, and/or anything desirable from one location to another.</p><p id="p-0032" num="0031">The present disclosure provides for systems and methods for route synchronization among a plurality of robotic devices in a shared environment. The plurality of robotic devices may travel through the shared environment using a plurality of routes. As used herein, the term &#x201c;route&#x201d; refers to a general path that a robot or plurality of robots may use to travel or navigate through the environment, such as from a starting point to an endpoint past one or more landmarks or objects in the environment. Without limitation, the starting point and the endpoint may be at the same location, providing a closed loop route. Alternatively, the starting point and the endpoint may be at different locations, providing an open-ended route. Further, a plurality of routes may be combined to provide a larger route. The term &#x201c;run&#x201d; is a single instance of a robot traveling along a route. The route does not necessarily comprise an identical track or path through the environment from run to run, but may be modified depending on factors such as a change of conditions encountered during a run by a robot, a different robot executing a run, etc. Each run may be timestamped to provide a listing of runs in chronological order. The term &#x201c;route synchronization&#x201d; refers to sharing information about a given route among the plurality of robots determined during a plurality of runs executed by the plurality of robots for the given route in the shared environment.</p><p id="p-0033" num="0032">Because route synchronization involves sharing information among a plurality of robots gathered during a plurality of runs, the information is gathered at different time points. As used herein, the term &#x201c;initial&#x201d; refers to the chronologically earliest time or run that any robot of the plurality of robots travels a given route in the shared environment. The terms &#x201c;preceding,&#x201d; &#x201c;precedes&#x201d; and variations thereof refer to a chronological time earlier than other times or runs in which the plurality of robots operates in the shared environment. These terms also are used to describe a robot traveling a route (i.e. a run) earlier in time than the same or a different robot travels the route. As such, the initial time or initial run is chronologically earlier than all other times or runs. The terms &#x201c;succeeding,&#x201d; &#x201c;succeeds&#x201d; and variations thereof refer to a chronological time later than other times in which the plurality of robots operates in the shared environment, and also refer to robots executing runs later than other runs. By way of illustration but not limitation, a route through the shared environment may be traveled by the plurality of robots for a plurality of n runs, wherein n is a range of integers starting at 1, such as 1, 2, 3, up to n. An initial run is a run wherein n is 1, and the initial robot is the robot that executes the initial run. For a plurality of runs wherein n is greater than 1, the initial run (i.e. Run<sub>1</sub>), is a preceding run to all other runs in the plurality of n runs, and all runs wherein n is greater than 1 are succeeding runs to the initial run. Further, Run<sub>n-1 </sub>is a preceding run to Run<sub>n</sub>, which is a succeeding run to Run<sub>n-1</sub>. An additional run (i.e. Run<sub>+1</sub>) is a succeeding run to Run<sub>n</sub>, which is a preceding run to Run<sub>+n</sub>. Similar nomenclature is used herein to refer to a robot executing a run in the plurality of runs. Notably, the robots executing the plurality of runs may be the same or different, in any combination or order.</p><p id="p-0034" num="0033">As used herein, a feature may comprise one or more numeric values (e.g., floating point, decimal, a tensor of values, etc.) characterizing an input from a sensor unit <b>114</b> including, but not limited to, detection of an object, parameters of the object (e.g., size, shape color, orientation, edges, etc.), color values of pixels of an image, depth values of pixels of a depth image, brightness of an image, the image as a whole, changes of features over time (e.g., velocity, trajectory, etc. of an object), sounds, spectral energy of a spectrum bandwidth, motor feedback (i.e., encoder values), sensor values (e.g., gyroscope, accelerometer, GPS, magnetometer, etc. readings), a binary categorical variable, an enumerated type, a character/string, or any other characteristic of a sensory input.</p><p id="p-0035" num="0034">As used herein, network interfaces may include any signal, data, or software interface with a component, network, or process including, without limitation, those of the FireWire (e.g., FW400, FW800, FWS800T, FWS1600, FWS3200, etc.), universal serial bus (&#x201c;USB&#x201d;) (e.g., USB 1.X, USB 2.0, USB 3.0, USB Type-C, etc.), Ethernet (e.g., 10/100, 10/100/1000 (Gigabit Ethernet), 10-Gig-E, etc.), multimedia over coax alliance technology (&#x201c;MoCA&#x201d;), Coaxsys (e.g., TVNET&#x2122;), radio frequency tuner (e.g., in-band or OOB, cable modem, etc.), Wi-Fi (802.11), WiMAX (e.g., WiMAX (802.16)), PAN (e.g., PAN/802.15), cellular (e.g., 3G, LTE/LTE-A/TD-LTE/TD-LTE, GSM, etc.), IrDA families, etc. As used herein, Wi-Fi may include one or more of IEEE-Std. 802.11, variants of IEEE-Std. 802.11, standards related to IEEE-Std. 802.11 (e.g., 802.11 a/b/g/n/ac/ad/af/ah/ai/aj/aq/ax/ay), and/or other wireless standards.</p><p id="p-0036" num="0035">As used herein, the term &#x201c;processing device&#x201d; refers to any processor, microprocessor, and/or digital processor and may include any type of digital processing device such as, without limitation, digital signal processors (&#x201c;DSPs&#x201d;), reduced instruction set computers (&#x201c;RISC&#x201d;), general-purpose (&#x201c;CISC&#x201d;) processors, microprocessors, gate arrays (e.g., field programmable gate arrays (&#x201c;FPGAs&#x201d;)), programmable logic device (&#x201c;PLDs&#x201d;), reconfigurable computer fabrics (&#x201c;RCFs&#x201d;), array processors, secure microprocessors, and application-specific integrated circuits (&#x201c;ASICs&#x201d;). Such digital processors may be contained on a single unitary integrated circuit die or distributed across multiple components. The term &#x201c;processor&#x201d; may be used herein as shorthand for any one or more processing devices described above.</p><p id="p-0037" num="0036">As used herein, computer program and/or software may include any sequence or human- or machine-cognizable steps which perform a function. Such computer program and/or software may be rendered in any programming language or environment including, for example, C/C++, C#, Fortran, COBOL, MATLABTM, PASCAL, GO, RUST, SCALA, Python, assembly language, markup languages (e.g., HTML, SGML, XML, VoXML), and the like, as well as object-oriented environments such as the Common Object Request Broker Architecture (&#x201c;CORBA&#x201d;), JAVATM (including J2ME, Java Beans, etc.), Binary Runtime Environment (e.g., &#x201c;BREW&#x201d;), and the like.</p><p id="p-0038" num="0037">As used herein, connection, link, and/or wireless link may include a causal link between any two or more entities (whether physical or logical/virtual), which enables information exchange between the entities.</p><p id="p-0039" num="0038">As used herein, computer and/or computing device may include, but are not limited to, personal computers (&#x201c;PCs&#x201d;) and minicomputers, whether desktop, laptop, or otherwise, mainframe computers, workstations, servers, personal digital assistants (&#x201c;PDAs&#x201d;), handheld computers, embedded computers, programmable logic devices, personal communicators, tablet computers, mobile devices, portable navigation aids, J2ME equipped devices, cellular telephones, smart phones, personal integrated communication or entertainment devices, and/or any other device capable of executing a set of instructions and processing an incoming data signal.</p><p id="p-0040" num="0039">As used herein, a computer-readable map may comprise any 2-dimensional or 3-dimensional structure representative of an environment in a computer-readable format or data structure, the map being generated at least in part by sensors on a robotic device during navigation along a route. Such formats may include 3-dimensional point cloud structures, birds-eye view maps, maps stitched together using a plurality of images, pixelated maps, and/or any other digital representation of an environment using data collected by at least one sensor in which a robot operates. Computer-readable maps may further comprise at least one route for a robot to follow superimposed thereon or associated with the maps. Some computer-readable maps may comprise additional data encoded therein in addition to two- or three-dimensional representations of objects; the additional encoded data may include color data, temperature data, Wi-Fi signal strength data, and so forth.</p><p id="p-0041" num="0040">Detailed descriptions of the various embodiments of the system and methods of the disclosure are now provided. While many examples discussed herein may refer to specific exemplary embodiments, it will be appreciated that the described systems and methods contained herein are applicable to any kind of robot. Myriad other embodiments or uses for the technology described herein would be readily envisaged by those having ordinary skill in the art, given the contents of the present disclosure.</p><p id="p-0042" num="0041">Advantageously, the systems and methods of this disclosure at least: (i) drastically reduce time spent by humans training a plurality of robots to follow a plurality of routes, (ii) allow for rapid integration of new robots in environments comprising robots, and (iii) increase utility of existing robots by enabling existing robots to quickly exchange routes and tasks between each other. Other advantages are readily discernible by one having ordinary skill in the art given the contents of the present disclosure.</p><p id="p-0043" num="0042"><figref idref="DRAWINGS">FIG. <b>1</b>A</figref> is a functional block diagram of a robot <b>102</b> in accordance with some principles of this disclosure. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, robot <b>102</b> may include controller <b>118</b>, memory <b>120</b>, user interface unit <b>112</b>, sensor units <b>114</b>, navigation units <b>106</b>, actuator unit <b>108</b>, and communications unit <b>116</b>, as well as other components and subcomponents (e.g., some of which may not be illustrated). Although a specific embodiment is illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, it is appreciated that the architecture may be varied in certain embodiments as would be readily apparent to one of ordinary skill given the contents of the present disclosure. As used herein, robot <b>102</b> may be representative at least in part of any robot described in this disclosure.</p><p id="p-0044" num="0043">Controller <b>118</b> may control the various operations performed by robot <b>102</b>. Controller <b>118</b> may include and/or comprise one or more processors (e.g., microprocessors) and other peripherals. As previously mentioned and used herein, processor, microprocessor, and/or digital processor may include any type of digital processing device such as, without limitation, digital signal processors (&#x201c;DSPs&#x201d;), reduced instruction set computers (&#x201c;RISC&#x201d;), general-purpose (&#x201c;CISC&#x201d;) processors, microprocessors, gate arrays (e.g., field programmable gate arrays (&#x201c;FPGAs&#x201d;)), programmable logic device (&#x201c;PLDs&#x201d;), reconfigurable computer fabrics (&#x201c;RCFs&#x201d;), array processors, secure microprocessors, and application-specific integrated circuits (&#x201c;ASICs&#x201d;). Peripherals may include hardware accelerators configured to perform a specific function using hardware elements such as, without limitation, encryption/description hardware, algebraic processing devices (e.g., tensor processing units, quadradic problem solvers, multipliers, etc.), data compressors, encoders, arithmetic logic units (&#x201c;ALU&#x201d;), and the like. Such digital processors may be contained on a single unitary integrated circuit die, or distributed across multiple components.</p><p id="p-0045" num="0044">Controller <b>118</b> may be operatively and/or communicatively coupled to memory <b>120</b>. Memory <b>120</b> may include any type of integrated circuit or other storage device configurable to store digital data including, without limitation, read-only memory (&#x201c;ROM&#x201d;), random access memory (&#x201c;RAM&#x201d;), non-volatile random access memory (&#x201c;NVRAM&#x201d;), programmable read-only memory</p><p id="p-0046" num="0045">(&#x201c;PROM&#x201d;), electrically erasable programmable read-only memory (&#x201c;EEPROM&#x201d;), dynamic random-access memory (&#x201c;DRAM&#x201d;), Mobile DRAM, synchronous DRAM (&#x201c;SDRAM&#x201d;), double data rate SDRAM (&#x201c;DDR/<b>2</b> SDRAM&#x201d;), extended data output (&#x201c;EDO&#x201d;) RAM, fast page mode RAM (&#x201c;FPM&#x201d;), reduced latency DRAM (&#x201c;RLDRAM&#x201d;), static RAM (&#x201c;SRAM&#x201d;), flash memory (e.g., NAND/NOR), memristor memory, pseudostatic RAM (&#x201c;PSRAM&#x201d;), etc. Memory <b>120</b> may provide instructions and data to controller <b>118</b>. For example, memory <b>120</b> may be a non-transitory, computer-readable storage apparatus and/or medium having a plurality of instructions stored thereon, the instructions being executable by a processing apparatus (e.g., controller <b>118</b>) to operate robot <b>102</b>. In some cases, the instructions may be configurable to, when executed by the processing apparatus, cause the processing apparatus to perform the various methods, features, and/or functionality described in this disclosure. Accordingly, controller <b>118</b> may perform logical and/or arithmetic operations based on program instructions stored within memory <b>120</b>. In some cases, the instructions and/or data of memory <b>120</b> may be stored in a combination of hardware, some located locally within robot <b>102</b>, and some located remote from robot <b>102</b> (e.g., in a cloud, server, network, etc.).</p><p id="p-0047" num="0046">It should be readily apparent to one of ordinary skill in the art that a processor may be external to robot <b>102</b> and be communicatively coupled to controller <b>118</b> of robot <b>102</b> utilizing communication units <b>116</b> wherein the external processor may receive data from robot <b>102</b>, process the data, and transmit computer-readable instructions back to controller <b>118</b>. In at least one non-limiting exemplary embodiment, the processor may be on a remote server (not shown).</p><p id="p-0048" num="0047">In some exemplary embodiments, memory <b>120</b>, shown in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, may store a library of sensor data. In some cases, the sensor data may be associated at least in part with objects and/or people. In exemplary embodiments, this library may include sensor data related to objects and/or people in different conditions, such as sensor data related to objects and/or people with different compositions (e.g., materials, reflective properties, molecular makeup, etc.), different lighting conditions, angles, sizes, distances, clarity (e.g., blurred, obstructed/occluded, partially off frame, etc.), colors, surroundings, and/or other conditions. The sensor data in the library may be taken by a sensor (e.g., a sensor of sensor units <b>114</b> or any other sensor) and/or generated automatically, such as with a computer program that is configurable to generate/simulate (e.g., in a virtual world) library sensor data (e.g., which may generate/simulate these library data entirely digitally and/or beginning from actual sensor data) from different lighting conditions, angles, sizes, distances, clarity (e.g., blurred, obstructed/occluded, partially off frame, etc.), colors, surroundings, and/or other conditions. The number of images in the library may depend at least in part on one or more of the amount of available data, the variability of the surrounding environment in which robot <b>102</b> operates, the complexity of objects and/or people, the variability in appearance of objects, physical properties of robots, the characteristics of the sensors, and/or the amount of available storage space (e.g., in the library, memory <b>120</b>, and/or local or remote storage). In exemplary embodiments, at least a portion of the library may be stored on a network (e.g., cloud, server, distributed network, etc.) and/or may not be stored completely within memory <b>120</b>. As yet another exemplary embodiment, various robots (e.g., that are commonly associated, such as robots by a common manufacturer, user, network, etc.) may be networked so that data captured by individual robots are collectively shared with other robots. In such a fashion, these robots may be configurable to learn and/or share sensor data in order to facilitate the ability to readily detect and/or identify errors and/or assist events.</p><p id="p-0049" num="0048">Still referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, operative units <b>104</b> may be coupled to controller <b>118</b>, or any other controller, to perform the various operations described in this disclosure. One, more, or none of the modules in operative units <b>104</b> may be included in some embodiments. Throughout this disclosure, reference may be to various controllers and/or processors. In some embodiments, a single controller (e.g., controller <b>118</b>) may serve as the various controllers and/or processors described. In other embodiments different controllers and/or processors may be used, such as controllers and/or processors used particularly for one or more operative units <b>104</b>. Controller <b>118</b> may send and/or receive signals, such as power signals, status signals, data signals, electrical signals, and/or any other desirable signals, including discrete and analog signals to operative units <b>104</b>. Controller <b>118</b> may coordinate and/or manage operative units <b>104</b>, and/or set timings (e.g., synchronously or asynchronously), turn off/on control power budgets, receive/send network instructions and/or updates, update firmware, send interrogatory signals, receive and/or send statuses, and/or perform any operations for running features of robot <b>102</b>.</p><p id="p-0050" num="0049">Returning to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, operative units <b>104</b> may include various units that perform functions for robot <b>102</b>. For example, operative units <b>104</b> includes at least navigation units <b>106</b>, actuator units <b>108</b>, user interface units <b>112</b>, sensor units <b>114</b>, and communication units <b>116</b>. Operative units <b>104</b> may also comprise other units that provide the various functionality of robot <b>102</b>. In exemplary embodiments, operative units <b>104</b> may be instantiated in software, hardware, or both software and hardware. For example, in some cases, units of operative units <b>104</b> may comprise computer-implemented instructions executed by a controller. In exemplary embodiments, units of operative unit <b>104</b> may comprise hardcoded logic (e.g., ASICS). In exemplary embodiments, units of operative units <b>104</b> may comprise both computer-implemented instructions executed by a controller and hardcoded logic. Where operative units <b>104</b> are implemented in part in software, operative units <b>104</b> may include units/modules of code configurable to provide one or more functionalities.</p><p id="p-0051" num="0050">In exemplary embodiments, navigation units <b>106</b> may include systems and methods that may computationally construct and update a map of an environment, localize robot <b>102</b> (e.g., find the position) in a map, and navigate robot <b>102</b> to/from destinations. The mapping may be performed by imposing data obtained in part by sensor units <b>114</b> into a computer-readable map representative at least in part of the environment. In exemplary embodiments, a map of an environment may be uploaded to robot <b>102</b> through user interface units <b>112</b>, uploaded wirelessly or through wired connection, or taught to robot <b>102</b> by a user.</p><p id="p-0052" num="0051">In exemplary embodiments, navigation units <b>106</b> may include components and/or software configurable to provide directional instructions for robot <b>102</b> to navigate. Navigation units <b>106</b> may process maps, routes, and localization information generated by mapping and localization units, data from sensor units <b>114</b>, and/or other operative units <b>104</b>.</p><p id="p-0053" num="0052">Still referring to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>, actuator units <b>108</b> may include actuators such as electric motors, gas motors, driven magnet systems, solenoid/ratchet systems, piezoelectric systems (e.g., inchworm motors), magnetostrictive elements, gesticulation, and/or any way of driving an actuator known in the art. According to exemplary embodiments, actuator unit <b>108</b> may include systems that allow movement of robot <b>102</b>, such as motorized propulsion. For example, motorized propulsion may move robot <b>102</b> in a forward or backward direction, and/or be used at least in part in turning robot <b>102</b> (e.g., left, right, and/or any other direction). By way of illustration, actuator unit <b>108</b> may control if robot <b>102</b> is moving or is stopped and/or allow robot <b>102</b> to navigate from one location to another location. By way of illustration, such actuators may actuate the wheels for robot <b>102</b> to navigate a route, navigate around obstacles or move the robot as it conducts a task. Other actuators may rotate cameras and sensors. According to exemplary embodiments, actuator unit <b>108</b> may include systems that allow in part for task execution by the robot <b>102</b> such as, for example, actuating features of robot <b>102</b> (e.g., moving a robotic arm feature to manipulate objects within an environment).</p><p id="p-0054" num="0053">According to exemplary embodiments, sensor units <b>114</b> may comprise systems and/or methods that may detect characteristics within and/or around robot <b>102</b>. Sensor units <b>114</b> may comprise a plurality and/or a combination of sensors. Sensor units <b>114</b> may include sensors that are internal to robot <b>102</b> or external, and/or have components that are partially internal and/or partially external. In some cases, sensor units <b>114</b> may include one or more exteroceptive sensors, such as sonars, light detection and ranging (&#x201c;LiDAR&#x201d;) sensors, radars, lasers, cameras (including video cameras (e.g., red-blue-green (&#x201c;RBG&#x201d;) cameras, infrared cameras, three-dimensional (&#x201c;3D&#x201d;) cameras, thermal cameras, etc.), time of flight (&#x201c;TOF&#x201d;) cameras, structured light cameras, antennas, motion detectors, microphones, and/or any other sensor known in the art. According to some exemplary embodiments, sensor units <b>114</b> may collect raw measurements (e.g., currents, voltages, resistances, gate logic, etc.) and/or transformed measurements (e.g., distances, angles, detected points in obstacles, etc.). In some cases, measurements may be aggregated and/or summarized. Sensor units <b>114</b> may generate data based at least in part on distance or height measurements. Such data may be stored in data structures, such as matrices, arrays, queues, lists, stacks, bags, etc.</p><p id="p-0055" num="0054">According to exemplary embodiments, sensor units <b>114</b> may include sensors that may measure internal characteristics of robot <b>102</b>. For example, sensor units <b>114</b> may measure temperature, power levels, statuses, and/or any characteristic of robot <b>102</b>. In some cases, sensor units <b>114</b> may be configurable to determine the odometry of robot <b>102</b>. For example, sensor units <b>114</b> may include proprioceptive sensors, which may comprise sensors such as accelerometers, inertial measurement units (&#x201c;IMU&#x201d;), odometers, gyroscopes, speedometers, cameras (e.g. using visual odometry), clock/timer, and the like. Odometry may facilitate autonomous navigation and/or autonomous actions of robot <b>102</b>. This odometry may include robot <b>102</b>'s position (e.g., where position may include robot's location, displacement and/or orientation, and may sometimes be interchangeable with the term pose as used herein) relative to the initial location. Such data may be stored in data structures, such as matrices, arrays, queues, lists, stacks, bags, etc. According to exemplary embodiments, the data structure of the sensor data may be called an image.</p><p id="p-0056" num="0055">According to exemplary embodiments, user interface units <b>112</b> may be configurable to enable a user to interact with robot <b>102</b>. For example, user interface units <b>112</b> may include touch panels, buttons, keypads/keyboards, ports (e.g., universal serial bus (&#x201c;USB&#x201d;), digital visual interface (&#x201c;DVI&#x201d;), Display Port, E-Sata, Firewire, PS/2, Serial, VGA, SCSI, audioport, high-definition multimedia interface (&#x201c;HDMI&#x201d;), personal computer memory card international association (&#x201c;PCMCIA&#x201d;) ports, memory card ports (e.g., secure digital (&#x201c;SD&#x201d;) and miniSD), and/or ports for computer-readable medium), mice, rollerballs, consoles, vibrators, audio transducers, and/or any interface for a user to input and/or receive data and/or commands, whether coupled wirelessly or through wires. Users may interact through voice commands or gestures. User interface units <b>218</b> may include a display, such as, without limitation, liquid crystal display (&#x201c;LCDs&#x201d;), light-emitting diode (&#x201c;LED&#x201d;) displays, LED LCD displays, in-plane-switching (&#x201c;IPS&#x201d;) displays, cathode ray tubes, plasma displays, high definition (&#x201c;HD&#x201d;) panels, <b>4</b>K displays, retina displays, organic LED displays, touchscreens, surfaces, canvases, and/or any displays, televisions, monitors, panels, and/or devices known in the art for visual presentation. According to exemplary embodiments user interface units <b>112</b> may be positioned on the body of robot <b>102</b>. According to exemplary embodiments, user interface units <b>112</b> may be positioned away from the body of robot <b>102</b> but may be communicatively coupled to robot <b>102</b> (e.g., via communication units including transmitters, receivers, and/or transceivers) directly or indirectly (e.g., through a network, server, and/or a cloud). According to exemplary embodiments, user interface units <b>112</b> may include one or more projections of images on a surface (e.g., the floor) proximally located to the robot, e.g., to provide information to the occupant or to people around the robot. The information could be the direction of future movement of the robot, such as an indication of moving forward, left, right, back, at an angle, and/or any other direction. In some cases, such information may utilize arrows, colors, symbols, etc.</p><p id="p-0057" num="0056">According to exemplary embodiments, communications unit <b>116</b> may include one or more receivers, transmitters, and/or transceivers. Communications unit <b>116</b> may be configurable to send/receive a transmission protocol, such as BLUETOOTH&#xae;, ZIGBEE&#xae;, Wi-Fi, induction wireless data transmission, radio frequencies, radio transmission, radio-frequency identification (&#x201c;RFID&#x201d;), near-field communication (&#x201c;NFC&#x201d;), infrared, network interfaces, cellular technologies such as 3G (3GPP/3GPP2), high-speed downlink packet access (&#x201c;HSDPA&#x201d;), high-speed uplink packet access (&#x201c;HSUPA&#x201d;), time division multiple access (&#x201c;TDMA&#x201d;), code division multiple access (&#x201c;CDMA&#x201d;) (e.g., IS-95A, wideband code division multiple access (&#x201c;WCDMA&#x201d;), etc.), frequency hopping spread spectrum (&#x201c;FHSS&#x201d;), direct sequence spread spectrum (&#x201c;DSSS&#x201d;), global system for mobile communication (&#x201c;GSM&#x201d;), Personal Area Network (&#x201c;PAN&#x201d;) (e.g., PAN/802.15), worldwide interoperability for microwave access (&#x201c;WiMAX&#x201d;), 802.20, long term evolution (&#x201c;LTE&#x201d;) (e.g., LTE/LTE-A), time division LTE (&#x201c;TD-LTE&#x201d;), global system for mobile communication (&#x201c;GSM&#x201d;), narrowband/frequency-division multiple access (&#x201c;FDMA&#x201d;), orthogonal frequency-division multiplexing (&#x201c;OFDM&#x201d;), analog cellular, cellular digital packet data (&#x201c;CDPD&#x201d;), satellite systems, millimeter wave or microwave systems, acoustic, infrared (e.g., infrared data association (&#x201c;IrDA&#x201d;)), and/or any other form of wireless data transmission.</p><p id="p-0058" num="0057">Communications unit <b>116</b> may also be configurable to send/receive signals utilizing a transmission protocol over wired connections, such as any cable that has a signal line and ground. For example, such cables may include Ethernet cables, coaxial cables, Universal Serial Bus (&#x201c;USB&#x201d;), FireWire, and/or any connection known in the art. Such protocols may be used by communications unit <b>116</b> to communicate to external systems, such as computers, smart phones, tablets, data capture systems, mobile telecommunications networks, clouds, servers, or the like. Communications unit <b>116</b> may be configurable to send and receive signals comprising numbers, letters, alphanumeric characters, and/or symbols. In some cases, signals may be encrypted, using algorithms such as 128-bit or 256-bit keys and/or other encryption algorithms complying with standards such as the Advanced Encryption Standard (&#x201c;AES&#x201d;), RSA, Data Encryption Standard (&#x201c;DES&#x201d;), Triple DES, and the like. Communications unit <b>116</b> may be configurable to send and receive statuses, commands, and other data/information. For example, communications unit <b>116</b> may communicate with a user operator to allow the user to control robot <b>102</b>. Communications unit <b>116</b> may communicate with a server/network (e.g., a network) in order to allow robot <b>102</b> to send data, statuses, commands, and other communications to the server. The server may also be communicatively coupled to computer(s) and/or device(s) that may be used to monitor and/or control robot <b>102</b> remotely. Communications unit <b>116</b> may also receive updates (e.g., firmware or data updates), data, statuses, commands, and other communications from a server for robot <b>102</b>.</p><p id="p-0059" num="0058">In exemplary embodiments, operating system <b>110</b> may be configurable to manage memory <b>120</b>, controller <b>118</b>, power supply <b>122</b>, modules in operative units <b>104</b>, and/or any software, hardware, and/or features of robot <b>102</b>. For example, and without limitation, operating system <b>110</b> may include device drivers to manage hardware resources for robot <b>102</b>.</p><p id="p-0060" num="0059">In exemplary embodiments, power supply <b>122</b> may include one or more batteries, including, without limitation, lithium, lithium ion, nickel-cadmium, nickel-metal hydride, nickel-hydrogen, carbon-zinc, silver-oxide, zinc-carbon, zinc-air, mercury oxide, alkaline, or any other type of battery known in the art. Certain batteries may be rechargeable, such as wirelessly (e.g., by resonant circuit and/or a resonant tank circuit) and/or plugging into an external power source. Power supply <b>122</b> may also be any supplier of energy, including wall sockets and electronic devices that convert solar, wind, water, nuclear, hydrogen, gasoline, natural gas, fossil fuels, mechanical energy, steam, and/or any power source into electricity.</p><p id="p-0061" num="0060">One or more of the units described with respect to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> (including memory <b>120</b>, controller <b>118</b>, sensor units <b>114</b>, user interface unit <b>112</b>, actuator unit <b>108</b>, communications unit <b>116</b>, mapping and localization unit <b>126</b>, and/or other units) may be integrated onto robot <b>102</b>, such as in an integrated system. However, according to some exemplary embodiments, one or more of these units may be part of an attachable module. This module may be attached to an existing apparatus to automate so that it behaves as a robot. Accordingly, the features described in this disclosure with reference to robot <b>102</b> may be instantiated in a module that may be attached to an existing apparatus and/or integrated onto robot <b>102</b> in an integrated system. Moreover, in some cases, a person having ordinary skill in the art would appreciate from the contents of this disclosure that at least a portion of the features described in this disclosure may also be run remotely, such as in a cloud, network, and/or server.</p><p id="p-0062" num="0061">As used herein below, a robot <b>102</b>, a controller <b>118</b>, or any other controller, processor, or robot performing a task illustrated in the figures below comprises a controller executing computer-readable instructions stored on a non-transitory computer-readable storage apparatus, such as memory <b>120</b>, as would be appreciated by one skilled in the art.</p><p id="p-0063" num="0062">Next referring to <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the architecture of a processor or processing device <b>138</b> is illustrated according to an exemplary embodiment. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, the processing device <b>138</b> includes a data bus <b>128</b>, a receiver <b>126</b>, a transmitter <b>134</b>, at least one processor <b>130</b>, and a memory <b>132</b>. The receiver <b>126</b>, the processor <b>130</b> and the transmitter <b>134</b> all communicate with each other via the data bus <b>128</b>. The processor <b>130</b> is configurable to access the memory <b>132</b> which stores computer code or computer-readable instructions in order for the processor <b>130</b> to execute the specialized algorithms. As illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>, memory <b>132</b> may comprise some, none, different, or all of the features of memory <b>120</b> previously illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. The algorithms executed by the processor <b>130</b> are discussed in further detail below. The receiver <b>126</b> as shown in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> is configurable to receive input signals <b>124</b>. The input signals <b>124</b> may comprise signals from a plurality of operative units <b>104</b> illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref> including, but not limited to, sensor data from sensor units <b>114</b>, user inputs, motor feedback, external communication signals (e.g., from a remote server), and/or any other signal from an operative unit <b>104</b> requiring further processing. The receiver <b>126</b> communicates these received signals to the processor <b>130</b> via the data bus <b>128</b>. As one skilled in the art would appreciate, the data bus <b>128</b> is the means of communication between the different components&#x2014;receiver, processor, and transmitter&#x2014;in the processing device. The processor <b>130</b> executes the algorithms, as discussed below, by accessing specialized computer-readable instructions from the memory <b>132</b>. Further detailed description as to the processor <b>130</b> executing the specialized algorithms in receiving, processing and transmitting of these signals is discussed above with respect to <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. The memory <b>132</b> is a storage medium for storing computer code or instructions. The storage medium may include optical memory (e.g., CD, DVD, HD-DVD, Blu-Ray Disc, etc.), semiconductor memory (e.g., RAM, EPROM, EEPROM, etc.), and/or magnetic memory (e.g., hard-disk drive, floppy-disk drive, tape drive, MRAM, etc.), among others. Storage medium may include volatile, nonvolatile, dynamic, static, read/write, read-only, random-access, sequential-access, location-addressable, file-addressable, and/or content-addressable devices. The processor <b>130</b> may communicate output signals to transmitter <b>134</b> via data bus <b>128</b> as illustrated. The transmitter <b>134</b> may be configurable to further communicate the output signals to a plurality of operative units <b>104</b> illustrated by signal output <b>136</b>.</p><p id="p-0064" num="0063">One of ordinary skill in the art would appreciate that the architecture illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> may illustrate an external server architecture configurable to effectuate the control of a robotic apparatus from a remote location, such as server <b>202</b> illustrated next in <figref idref="DRAWINGS">FIG. <b>2</b></figref>. That is, the server may also include a data bus, a receiver, a transmitter, a processor, and a memory that stores specialized computer-readable instructions thereon.</p><p id="p-0065" num="0064">One of ordinary skill in the art would appreciate that a controller <b>118</b> of a robot <b>102</b> may include one or more processing devices <b>138</b> and may further include other peripheral devices used for processing information, such as ASICS, DPS, proportional-integral-derivative (&#x201c;PID&#x201d;) controllers, hardware accelerators (e.g., encryption/decryption hardware), and/or other peripherals (e.g., analog to digital converters) described above in <figref idref="DRAWINGS">FIG. <b>1</b>A</figref>. The other peripheral devices when instantiated in hardware are commonly used within the art to accelerate specific tasks (e.g., multiplication, encryption, etc.) which may alternatively be performed using the system architecture of <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. In some instances, peripheral devices are used as a means for intercommunication between the controller <b>118</b> and operative units <b>104</b> (e.g., digital to analog converters and/or amplifiers for producing actuator signals). Accordingly, as used herein, the controller <b>118</b> executing computer-readable instructions to perform a function may include one or more processing devices <b>138</b> thereof executing computer-readable instructions and, in some instances, the use of any hardware peripherals known within the art. Controller <b>118</b> may be illustrative of various processing devices <b>138</b> and peripherals integrated into a single circuit die or distributed to various locations of the robot <b>102</b> which receive, process, and output information to/from operative units <b>104</b> of the robot <b>102</b> to effectuate control of the robot <b>102</b> in accordance with instructions stored in a memory <b>120</b>, <b>132</b>. For example, controller <b>118</b> may include a plurality of processing devices <b>138</b> for performing high-level tasks (e.g., planning a route to avoid obstacles) and processing devices <b>138</b> for performing low-level tasks (e.g., producing actuator signals in accordance with the route).</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>2</b></figref> illustrates a server <b>202</b> and communicatively coupled components thereof in accordance with some exemplary embodiments of this disclosure. The server <b>202</b> may comprise one or more processing devices depicted in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref> above, each processing device comprising at least one processor <b>130</b> and memory <b>132</b> therein in addition to, without limitation, any other components illustrated in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. The processing devices may be centralized at a location or distributed among a plurality of devices across many locations (e.g., a cloud server, distributed network, or dedicated server). Communication links between the server <b>202</b> and coupled devices may comprise wireless and/or wired communications, wherein the server <b>202</b> may further comprise one or more coupled antenna, relays, and/or routers to effectuate the wireless communication. The server <b>202</b> may be coupled to a host <b>204</b>, wherein the host <b>204</b> may correspond to a high-level entity (e.g., an admin) of the server <b>202</b>. The host <b>204</b> may include computerized and/or human entities. The host <b>204</b> may, for example, upload software and/or firmware updates for the server <b>202</b> and/or coupled devices <b>208</b> and <b>210</b> via a user interface or terminal. External data sources <b>206</b> may comprise any publicly available data sources (e.g., public databases such as weather data from the national oceanic and atmospheric administration (NOAA), satellite topology data, public records, etc.) and/or any other databases (e.g., private databases with paid or restricted access) of which the server <b>202</b> may access data therein. Edge devices <b>208</b> may comprise any device configurable to perform a task at an edge of the server <b>202</b>. These devices may include, without limitation, internet of things (IoT) devices (e.g., stationary CCTV cameras, smart locks, smart thermostats, etc.), external processors (e.g., external CPUs or GPUs), and/or external memories configurable to receive and execute a sequence of computer-readable instructions, which may be provided at least in part by the server <b>202</b>, and/or store large amounts of data.</p><p id="p-0067" num="0066">Lastly, the server <b>202</b> may be coupled to a plurality of robot networks <b>210</b>, each robot network <b>210</b> comprising at least one robot <b>102</b>. In some embodiments, each network <b>210</b> may comprise one or more robots <b>102</b> operating within separate environments from other robots <b>102</b> of other robot networks <b>210</b>. An environment may comprise, for example, a section of a building (e.g., a floor or room), an entire building, a street block, or any enclosed and defined space in which the robots <b>102</b> operate. In some embodiments, each robot network <b>210</b> may comprise a different number of robots <b>102</b> and/or may comprise different types of robot <b>102</b>. For example, network <b>210</b>-<b>1</b> may only comprise a robotic wheelchair, and network <b>210</b>-<b>1</b> may operate in a home of an owner of the robotic wheelchair or a hospital, whereas network <b>210</b>-<b>2</b> may comprise a scrubber robot <b>102</b>, vacuum robot <b>102</b>, and a gripper arm robot <b>102</b>, wherein network <b>210</b>-<b>2</b> may operate within a retail store. Alternatively or additionally, in some embodiments, the robot networks <b>210</b> may be organized around a common function or type of robot <b>102</b>. For example, a network <b>210</b>-<b>3</b> may comprise a plurality of security or surveillance robots that may or may not operate in a single environment, but are in communication with a central security network linked to server <b>202</b>. Alternatively or additionally, in some embodiments, a single robot <b>102</b> may be a part of two or more networks <b>210</b>. That is, robot networks <b>210</b> are illustrative of any grouping or categorization of a plurality of robots <b>102</b> coupled to the server. The relationships between individual robots <b>102</b>, robot networks <b>210</b>, and server <b>202</b> may be defined using binding trees or similar data structures, as discussed below in regards to <figref idref="DRAWINGS">FIG. <b>10</b></figref>.</p><p id="p-0068" num="0067">Each robot network <b>210</b> may communicate data including, but not limited to, sensor data (e.g., RGB images captured, LiDAR scan points, network signal strength data from sensors <b>202</b>, etc.), IMU data, navigation and route data (e.g., which routes were navigated), localization data of objects within each respective environment, and metadata associated with the sensor, IMU, navigation, and localization data. Each robot <b>102</b> within each network <b>210</b> may receive communication from the server <b>202</b> including, but not limited to, a command to navigate to a specified area, a command to perform a specified task, a request to collect a specified set of data, a sequence of computer-readable instructions to be executed on respective controllers <b>118</b> of the robots <b>102</b>, software updates, and/or firmware updates. One skilled in the art may appreciate that a server <b>202</b> may be further coupled to additional relays and/or routers to effectuate communication between the host <b>204</b>, external data sources <b>206</b>, edge devices <b>208</b>, and robot networks <b>210</b> which have been omitted for clarity. It is further appreciated that a server <b>202</b> may not exist as a single hardware entity, rather may be illustrative of a distributed network of non-transitory memories and processors. In some embodiments, a robot network <b>210</b>, such as network <b>210</b>-<b>1</b>, may communicate data, e.g. share route and map information, with other networks <b>210</b>-<b>2</b> and/or <b>210</b>-<b>3</b>. In some embodiments, a robot <b>102</b> in one network may communicate sensor, route or map information with a robot in a different network. Communication among networks <b>210</b> and/or individual robots <b>102</b> may be facilitated via server <b>202</b>, but direct device-to-device communication at any level may also be envisioned. For example, a device <b>208</b> may be directly coupled to a robot <b>102</b> to enable the device <b>208</b> to provide instructions for the robot <b>102</b> (e.g., command the robot <b>102</b> to navigate a route).</p><p id="p-0069" num="0068">One skilled in the art may appreciate that any determination or calculation described herein may comprise one or more processors of the server <b>202</b>, edge devices <b>208</b>, and/or robots <b>102</b> of networks <b>210</b> performing the determination or calculation by executing computer-readable instructions. The instructions may be executed by a processor of the server <b>202</b> and/or may be communicated to robot networks <b>210</b> and/or edge devices <b>208</b> for execution on their respective controllers/processors in part or in entirety. Advantageously, use of a centralized server <b>202</b> may enhance a speed at which parameters may be measured, analyzed, and/or calculated by executing the calculations (i.e., computer-readable instructions) on a distributed network of processors on robots <b>102</b> and edge devices <b>208</b>. Use of a distributed network of controllers <b>118</b> of robots <b>102</b> may further enhance functionality of the robots <b>102</b> as the robots <b>102</b> may execute instructions on their respective controllers <b>118</b> during times when the robots <b>102</b> are not in use by operators of the robots <b>102</b>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a process flow diagram illustrating a method <b>300</b> for powering on or initiating a robot <b>102</b> coupled to a server <b>202</b> for later use in route synchronization, according to an exemplary embodiment. Method <b>300</b> configures a controller <b>118</b> of a robot <b>102</b> to, upon being powered on or initiated, receive and store (in memory <b>120</b>) up-to-the moment route and map data as well as software updates, firmware updates, and the like. Steps of method <b>300</b> may be effectuated by the controller <b>118</b> executing computer-readable instructions from a memory <b>120</b> as appreciated by one skilled in the art.</p><p id="p-0071" num="0070">Block <b>302</b> comprises powering on of the robot <b>102</b>. Powering on may comprise, for example, a human pressing an &#x201c;ON&#x201d; button of the robot <b>102</b> or a server <b>202</b> activating the robot <b>102</b> from an idle or off state. Powering on of the robot <b>102</b> may comprise, without limitation, activation of the robot <b>102</b> for a first (i.e., initial) time in a new environment or for a subsequent time within a familiar environment to the robot <b>102</b>.</p><p id="p-0072" num="0071">Block <b>304</b> comprises the controller <b>118</b> of the robot <b>102</b> checking for a connection to a server <b>202</b>. Controller <b>118</b> may utilize communication units <b>116</b> to communicate via wired or wireless communication (e.g., using Wi-Fi or <b>4</b>G, <b>5</b>G, etc.) to the server <b>202</b>. The server <b>202</b> may send and receive communications from the robot <b>102</b> and other robots <b>102</b> within the same or different locations. To verify the connection to the server <b>202</b>, the controller <b>118</b> may, for example, send at least one transmission of data to the server <b>202</b> and await a response (i.e., a handshake verification).</p><p id="p-0073" num="0072">Upon the controller <b>118</b> utilizing communications units <b>116</b> to successfully communicate with the server <b>202</b>, the controller <b>118</b> moves to block <b>306</b>.</p><p id="p-0074" num="0073">Upon the controller <b>118</b> failing to communicate successfully with the server <b>202</b>, the controller <b>118</b> moves to block <b>310</b>.</p><p id="p-0075" num="0074">According to at least one non-limiting exemplary embodiment, connection with the server <b>202</b> in block <b>304</b> may comprise connection to a local robot network <b>210</b>, the local robot network <b>210</b> comprising at least one robot <b>102</b> within an environment. The local robot network <b>210</b> being a portion of the server <b>202</b> structure illustrated in <figref idref="DRAWINGS">FIG. <b>2</b></figref> above local to the environment of the robot <b>102</b> (e.g., within the same building, using the same Wi-Fi, etc.). That is, connection to the server <b>202</b> is not limited to coupling of the robot <b>102</b> with all robots <b>102</b> of all robot networks <b>210</b>, all data sources <b>206</b>, the host <b>204</b>, or edge devices <b>208</b>.</p><p id="p-0076" num="0075">Block <b>306</b> comprises the controller <b>118</b> checking if data is available to be synchronized (&#x201c;syncing&#x201d;) with the server <b>202</b>. The server <b>202</b> may store computer-readable maps of an environment of the robot <b>102</b>, for example, generated by the robot <b>102</b> in the past (e.g., during prior navigation of routes) or generated by at least one other robot <b>102</b> within the environment in one or more preceding runs of one or more routes. Data to be synchronized may comprise without limitation, software updates, firmware updates, updates to computer-readable maps, updates to routes (e.g., new routes from other robots <b>102</b>, as discussed below), and/or any other data stored by the server <b>202</b> which may be of use for later navigation. Synchronizing of data may comprise the controller <b>118</b>, via communications units <b>116</b>, uploading and/or downloading data to/from the server <b>202</b>. The controller <b>118</b> may communicate with the server <b>202</b> to determine if there is data to be synchronized with the server <b>202</b>. Data may be pulled from the server <b>202</b> by the controller <b>118</b> or pushed from the server <b>202</b> to the controller <b>118</b>, or any combination thereof.</p><p id="p-0077" num="0076">For example, a preceding robot <b>102</b> may have a route stored within its memory <b>120</b>, wherein the route was last completed by the preceding robot <b>102</b> at 5:00 AM, for instance, wherein the preceding robot <b>102</b> may synchronize data collected during navigation of the route with the server <b>202</b>. At 6:00 AM the same day, for instance, a succeeding robot <b>102</b> (of the same make/model) may have navigated the same route and observed substantially the same objects with slight variations or, in some instances, substantial changes in the objects (e.g., changes in position, orientation, size, shape, presence, etc. of the objects). Accordingly, any time after <b>6</b>:<b>00</b> AM, both robots <b>102</b> may utilize data (e.g., sensor data and/or computer-readable maps) collected by the succeeding robot <b>102</b> as the data from the succeeding robot <b>102</b> is more up to date. The preceding robot <b>102</b>, at any time after <b>6</b>:<b>00</b> AM, may synchronize with the server <b>202</b> to download the data from the succeeding robot <b>102</b>.</p><p id="p-0078" num="0077">Upon the controller <b>118</b> receiving communications from the server <b>202</b> indicative of data available to be synchronized, the controller <b>118</b> moves to block <b>308</b>. Upon the controller <b>118</b> receiving communications from the server <b>202</b> indicative that all map and route data is up-to-the moment (i.e., no new data to be uploaded or downloaded), the controller <b>118</b> moves to block <b>310</b>.</p><p id="p-0079" num="0078">A more thorough discussion on how the controller <b>118</b> of the robot <b>102</b> and processors <b>130</b> of the server <b>202</b> know when data is available to be synchronized is shown and described in <figref idref="DRAWINGS">FIG. <b>9</b>-<b>10</b></figref> below. In short, if a robot <b>102</b> detects any change to its routes (e.g., creation, edits, or deletions), the robot <b>102</b> may ping or transmit a signal to the server <b>202</b> communicating that there has been a change in its traveled route, which needs to be synchronized. Following this ping, the synchronization may occur as described next in block <b>308</b>.</p><p id="p-0080" num="0079">Block <b>308</b> comprises the controller <b>118</b> synchronizing data with the server <b>202</b>. The data synchronized may comprise route data (e.g., pose graphs indicative of a path to be followed, a target location to navigate to and a shortest path thereto, etc.), map data (e.g., LiDAR scan maps, 3-dimensional points, 2-dimensional bird's eye view maps, etc.), software, and/or firmware updates. The route data may comprise updates to existing routes, for example, using data collected by other robots <b>102</b> within the same environment. The route data may include a pose graph, a cost map, a sequence of motion commands (e.g., motion primitives), pixels on a computer-readable map, filters (e.g., areas to avoid), and/or any other method of representing a route or path followed by a robot <b>102</b>. The route data may further comprise new routes navigated by the other robots <b>102</b> within the same environment. The map data may comprise any computer-readable maps generated by one or more sensors from one or more robots <b>102</b> within the environment; the map data communicated may comprise the most up-to-the-moment map of the environment. In some embodiments, the map data may comprise a single large map or a plurality of smaller maps of the environment. In some embodiments, the map data may comprise the route data superimposed thereon. In some embodiments, the map data may include cost maps. In some embodiments, the map data may include multiple maps (i.e., representations of an environment) for a same route, for example, a point cloud map and a cost map.</p><p id="p-0081" num="0080">Route synchronization may be tailored to the robot receiving the information. As part of the connection with the server <b>202</b>, characterization of the robot may be conducted. Characterization of the robot may include information related to, for example, its size, capability, executable tasks, and/or assigned functionality in the environments; its location (e.g., store number); and routes available to the robot <b>102</b>, once synchronized. These characteristics may be defined using a binding tree or similar structure as shown and described in <figref idref="DRAWINGS">FIG. <b>10</b></figref> below. In some embodiments, a robot <b>102</b> new to the environment may receive all route and map information related to the environment in <b>308</b>. In other embodiments a robot <b>102</b> new to the environment may only receive route and map data in <b>308</b> relative to its function in the environment. For example, a floor-cleaning robot in a warehouse environment may receive map information related to cleaning floors but not receive information related to locations for stocking storage shelves. Following the same example, the floor-cleaning robot may receive route and map data from other floor-cleaning robots within the environment and not from shelf-stocking robots. In other embodiments, a robot may receive route information relevant to its size. See the discussion related to <figref idref="DRAWINGS">FIGS. <b>7</b>A and <b>7</b>B</figref> below for embodiments related to methods of modifying preceding route information to enable a robot to navigate a route based on its footprint.</p><p id="p-0082" num="0081">Synchronizing of data between a robot <b>102</b> and a server <b>202</b> may comprise a delta update. A delta update, as used herein, occurs when a file, bundle, or component is updated by being provided with only new information. For example, a route file may be edited such that a segment of route is removed. To synchronize this update to the route from one robot <b>102</b> to another robot <b>102</b>, only the update (i.e., removed segment) may be communicated rather than the entire map file. Advantageously, delta updates reduce communications bandwidth needed to update and synchronize files between robots <b>102</b> and server <b>202</b>.</p><p id="p-0083" num="0082">According to at least one non-limiting exemplary embodiment, the data available to be synced may include the deletion of a route. For example, a first robot <b>102</b> and a second robot <b>102</b> may operate in a single environment and/or be included in a robot network <b>210</b>, both robots <b>102</b> having synchronized data with the server <b>202</b> such that both robots <b>102</b> comprise a set of routes stored in their respective memories <b>120</b>. The first robot <b>102</b> may receive input from an operator, e.g., via user interface units <b>116</b>, to delete a route from the set of routes stored in memory <b>120</b>. Accordingly, the same route may be deleted from the memory <b>120</b> of the second robot <b>102</b> of the two robots <b>102</b> upon the second robot <b>102</b> being powered on (step <b>302</b>) and synchronizing data with the server <b>202</b> following method <b>300</b>.</p><p id="p-0084" num="0083">According to at least one non-limiting exemplary embodiment, data synchronization may be specific to the environment of the robot <b>102</b>. For example, a first robot network <b>210</b>, comprising a plurality of robots <b>102</b>, may operate within a first environment (e.g., a grocery store) and a second robot network <b>210</b>, comprising a plurality of different robots <b>102</b>, may operate within a different second environment (e.g., a warehouse). Upon a robot <b>102</b> of the first robot network <b>210</b> being initialized following method <b>300</b>, the robot <b>102</b> may receive up-to-the-moment route and map data corresponding only to the first environment. In some instances, robots <b>102</b> of the first robot network <b>210</b> may be moved into the second environment of the second robot network <b>210</b>. Accordingly, the robots <b>102</b> which have moved from the first environment to the second environment, and subsequently coupled to the second robot network <b>210</b>, may receive data corresponding to the second environment upon reaching step <b>308</b>, wherein data corresponding to the first environment may be deleted from their respective memories <b>120</b>.</p><p id="p-0085" num="0084">Block <b>310</b> comprises the controller <b>118</b> awaiting user input. The controller <b>118</b> may, for example, utilize user interface units <b>112</b> to display options to a human operator of the robot <b>102</b> such as &#x201c;select a route to navigate,&#x201d; &#x201c;teach a route,&#x201d; or other settings (e.g., delete a route, configuration settings, diagnostics, etc.). Methods <b>400</b>, <b>500</b> below illustrate methods for the robot <b>102</b> and server <b>202</b> to maintain up-to-the-moment route and map data for later use in route synchronizing between two robots <b>102</b>. Upon the controller <b>118</b> reaching step <b>310</b> following method <b>300</b>, memory <b>120</b> of the robot <b>102</b> comprises some or all routes within the environment navigated by the robot <b>102</b> or navigated by other robots <b>102</b> in the past.</p><p id="p-0086" num="0085">According to at least one non-limiting exemplary embodiment, while robot <b>102</b> is awaiting user input in block <b>310</b>, controller <b>118</b> may communicate with the server <b>202</b> to determine (i) that connection to the server <b>202</b> still exists and, if so, (ii) if any new data is available to be synchronized. For example, upon following method <b>300</b> and awaiting a user input, the controller <b>118</b> may check if any new route or map data is available from the server <b>202</b> (e.g., from another robot <b>102</b> (i.e. a preceding robot <b>102</b>) which had just completed its route while the succeeding robot <b>102</b> is being initialized) periodically, such as every 30 seconds, 1 minute, 5 minutes, etc. This may enable a robot <b>102</b> to receive up-to-the-moment route and map data from the server <b>202</b> even if after powering on the robot <b>102</b>, the user becomes occupied and cannot provide the robot <b>102</b> with further instructions in block <b>310</b>.</p><p id="p-0087" num="0086"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a process flow diagram illustrating a method <b>400</b> for a robot <b>102</b> to navigate a route and synchronize the route data with a server <b>202</b>, according to an exemplary embodiment. Method <b>400</b> begins at block <b>310</b> (i.e., after initialization of the robot <b>102</b> using method <b>300</b>) and proceeds to block <b>402</b> upon a server <b>202</b> or human operator indicating to the robot <b>102</b> (e.g., via user interface units <b>112</b>) to navigate a route. Method <b>400</b> describes the process of synchronizing one component of a route, its map, wherein one skilled in the art may appreciate other components of a route, such as masks (e.g., no-go areas), tasks to complete at certain locations, pose graph information, etc., which may be synchronized in a substantially similar manner.</p><p id="p-0088" num="0087">Block <b>402</b> comprises a controller <b>118</b> of the robot <b>102</b> receiving an input to navigate a route. The input may comprise a human operator selecting the route to navigate on a user interface unit <b>112</b> coupled to the robot <b>102</b>. In some instances, the server <b>202</b> may configure the controller <b>118</b> to begin navigating the route in response to a human in a remote location indicating to the server <b>202</b> (e.g., via a device <b>208</b> or user interface) that the robot <b>102</b> is to navigate the route. In some instances, the server <b>202</b> may configure the controller <b>118</b> to navigate the route on a predetermined schedule or at specified time intervals. In some instances, the memory <b>120</b> of the robot <b>102</b> may include the predetermined schedule or time intervals for navigating the route, e.g., set by an operator of the robot <b>102</b>. In some instances, the robot <b>102</b> may be trained to learn a route under user guided control (e.g., via an operator pushing, leading, pulling, driving, or moving the robot <b>102</b> along the route), as further discussed in <figref idref="DRAWINGS">FIG. <b>5</b></figref> below.</p><p id="p-0089" num="0088">Block <b>404</b> comprises the controller <b>118</b> navigating the route. The controller <b>118</b> may utilize any conventional method known in the art for navigating the route such as, for example, following a pose graph comprising positions for the robot <b>102</b> as a function of time or distance which, when executed properly, configure the robot <b>102</b> to follow the route. Navigation of the route may be effectuated by the controller <b>118</b> providing signals to one or more actuator units <b>108</b>.</p><p id="p-0090" num="0089">Block <b>406</b> comprises the controller <b>118</b> collecting data from at least one sensor unit <b>114</b> during navigation of the route to create a computer-readable map of the route and surrounding environment. The computer-readable map may comprise a plurality of LiDAR scans or points joined or merged to create a point cloud representative of objects within an environment of the robot <b>102</b> during navigation of the route. In some embodiments of a computer-readable map, the computer-readable map may comprise a plurality of greyscale or colorized images merged to produce the map. In at least one non-limiting embodiment of robot <b>102</b>, sensor units <b>114</b> may further comprise gyroscopes, accelerometers and other odometry units configurable to enable the robot <b>102</b> to localize itself with respect to a fixed starting location and thereby accurately map its path during execution of the route. A plurality of methods for mapping a route navigated by the robot <b>102</b> may be utilized to produce the computer-readable map, wherein the method used in block <b>406</b> may depend on the types of sensors of sensor units <b>114</b>, resolution of the sensor units <b>114</b>, and/or computing capabilities of controller <b>118</b> as should be readily apparent to one skilled in the art.</p><p id="p-0091" num="0090">According to at least one non-limiting exemplary embodiment, the computer-readable map of the environment may comprise a starting location, an ending location, landmark(s) and object(s) therebetween detected by sensor units <b>114</b> of the robot <b>102</b> or different robot <b>102</b> during prior navigation along or nearby the objects.</p><p id="p-0092" num="0091">Block <b>408</b> comprises the controller <b>118</b>, upon completion of the route, uploading the computer-readable map generated in block <b>408</b> to the server <b>202</b> via communications units <b>116</b>. The computer-readable map uploaded to the server <b>202</b> may comprise route data (e.g., pose graphs, gyro meter data, accelerometer data, a path superimposed on the computer-readable map, etc.) and/or localization data of objects detected by sensor units <b>114</b> during navigation of the route.</p><p id="p-0093" num="0092">According to at least one non-limiting exemplary embodiment, block <b>408</b> may comprise the controller <b>118</b> uploading summary information corresponding to the navigated route. The summary information may include data such as the runtime of the route, number of obstacles encountered, deviation from the route to avoid objects, a number of requests for human assistance issued during the navigation, timestamps, and/or performance metrics (e.g., square footage of cleaned floor if robot <b>102</b> is a floor-cleaning robot). That is, uploading of the computer-readable map is not intended to be limiting as computer-readable maps produced in large environments may comprise a substantial amount of data (e.g., <b>100</b>kB to GB) as compared to metadata associated with navigation of the route. For example, robots <b>102</b> may be coupled to the server <b>202</b> using a cellular connection (e.g., 4G, 5G, or other LTE networks), wherein reduction in communications bandwidth may be desirable to reduce costs in operating the robots <b>102</b>. The binary data of the computer-readable map may be kept locally on memory <b>120</b> on the robot <b>102</b> until the server <b>202</b> determines that another robot <b>102</b> may utilize the same map, wherein the binary data is uploaded to the server <b>202</b> such that the server <b>202</b> may provide the route and map data to the other robot <b>102</b>.</p><p id="p-0094" num="0093">According to at least one non-limiting exemplary embodiment, the controller <b>118</b> may upload metadata associated with the run of the route. The metadata may include, for example, a site identifier (e.g., an identifier which denotes the environment and/or network <b>210</b> of the robot <b>102</b>), a timestamp, a route identifier (e.g., an identifier which denotes a specific route within the environment), and/or other metadata associated with the run of the route. The utility of metadata for determining if there is data available to be synchronized for the next step <b>410</b> is further illustrated in <figref idref="DRAWINGS">FIG. <b>9</b>A-B</figref> below.</p><p id="p-0095" num="0094">Block <b>410</b> comprises the controller <b>118</b> communicating with the server <b>202</b> to determine if there is data to be synchronized, similar to block <b>306</b> discussed in <figref idref="DRAWINGS">FIG. <b>3</b></figref> above. The synchronization upon completion of the route enables the robot <b>102</b> to receive updated route data collected by another robot <b>102</b>-A. For example, a first robot <b>102</b> may follow method <b>400</b> up to block <b>410</b> to navigate a first route while another, second robot <b>102</b> navigates and completes another route following method <b>400</b> and accordingly uploads a computer-readable map and route data to the server <b>202</b>. The second robot <b>102</b> completes its respective route while the first robot <b>102</b> is still navigating its respective route, wherein the first robot <b>102</b> is pre-occupied with its task and cannot synchronize. Upon the second robot <b>102</b> completing its route, new route data may be synchronized between the second robot <b>102</b> and the server <b>202</b>. Server <b>202</b>, upon detecting a change to route data (e.g., creation, deletion, or edits to route data) from the second robot <b>102</b> may ping the first robot <b>102</b> indicating that new data is available to be synchronized. The receipt of this ping may indicate to the controller <b>118</b> of the first robot <b>102</b> that data is available to be synchronized. In other embodiments, the controller <b>118</b> may issue a ping to the server <b>202</b>, wherein the server <b>202</b> may reply indicating that data is or is not available to be synchronized.</p><p id="p-0096" num="0095">Upon the one robot <b>102</b> reaching block <b>410</b>, the server <b>202</b> may have received the computer-readable map from the other preceding robot <b>102</b>-A of the other route and may provide the computer-readable map to the one robot <b>102</b>, thereby ensuring the computer-readable map of the other route stored in memory <b>120</b> of the one robot <b>102</b> is up-to-the-moment based on data collected by the other, preceding robot <b>102</b>-A. A similar example is further illustrated below in <figref idref="DRAWINGS">FIG. <b>6</b>A-B</figref>.</p><p id="p-0097" num="0096">Upon the controller <b>118</b> receiving communication from the server <b>202</b> indicating there is no data to be synchronized with the server <b>202</b>, the controller <b>118</b> returns to block <b>310</b> and awaits a user input.</p><p id="p-0098" num="0097">Upon the controller <b>118</b> receiving communication from the server <b>202</b> indicating there is data to be synchronized, the controller <b>118</b> moves to block <b>412</b>.</p><p id="p-0099" num="0098">Block <b>412</b> comprises the controller <b>118</b> synchronizing data with the server <b>202</b>. As mentioned previously, synchronizing data with the server <b>202</b> may comprise the robot <b>102</b> receiving software updates, firmware updates, updated computer-readable maps, updated or new routes (e.g., collected by other robots <b>102</b>), and/or other data useful for navigation within its environment.</p><p id="p-0100" num="0099">In some embodiments, the robot new to an environment may be the initial robot in the environment and no route and/or map information is available to be synchronized. In other embodiments, the robot may not be new to an environment but needs to learn a new route. Accordingly, the robot is the initial robot for the route and no route and/or map information for that route is available to be synchronized. In such embodiments, the robot is configurable to learn one or more routes taught by a user to the robot in a training mode as described in more detail below in relation to <figref idref="DRAWINGS">FIG. <b>5</b></figref>. The robot may also gather map and route data in an exploration mode also as described in more detail below.</p><p id="p-0101" num="0100">New route or map data may comprise an entirely new route through the environment, or it may comprise an existing route modified to address one or more new conditions, such as a task being added or deleted, landmark(s) being added, deleted or moved, object(s) being added and/or different environmental conditions requiring which may necessitate the entirely new route. One of skill in the art can appreciate that the amount of human user input needed would be less for modifying an existing route than for teaching an entirely new route.</p><p id="p-0102" num="0101">For illustration, a preceding route may include locations A, B, D and E, tasks a, b and d at locations A, B and D, and object C at location C<b>1</b>. A new route may comprise one in which locations A, B and E are unchanged, task b is deleted, location D and task d are deleted, object C is moved to new location C<b>2</b> and location F and associated taskfare added. As a consequence the existing preceding route may be modified to define a new succeeding route to skip locations B and D, navigate around object C at new position C<b>2</b>, navigate to new location F and perform new task f at location F. In some embodiments, these changes to a preceding route may be effectuated by a human operator providing input to user interface units <b>116</b> or may require the human operator to navigate the robot <b>102</b> through the modified route.</p><p id="p-0103" num="0102">In some embodiments, the entire new route can be taught to a robot in learning mode directed by a human user in an initial run. In other embodiments, a new succeeding route may be learned by a robot in training and/or exploration mode by navigating a preceding route with changes inputted by a human user as it navigates the preceding route. In still other embodiments, a new succeeding route can be configured into a robot <b>102</b> by modifying an existing preceding route by a processing device at the level of the controller <b>118</b>, the network level <b>210</b> or the server level <b>202</b> by a combination of user inputs designating desired changes to the route and sensor data gathered during exploration mode of the robot.</p><p id="p-0104" num="0103"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a process flow diagram illustrating a method <b>500</b> for a robot <b>102</b> to learn a route and upload the learned route to a server <b>202</b>, according to an exemplary embodiment. Method <b>500</b> begins at block <b>310</b> (i.e., after initialization of the robot <b>102</b> using method <b>300</b> and/or after execution of method <b>400</b>) and proceeds to block <b>502</b> upon a human operator indicating to the robot <b>102</b> (e.g., via user interface units <b>112</b>) to navigate a route.</p><p id="p-0105" num="0104">Block <b>502</b> comprises the controller <b>118</b> receiving an input which configures the robot <b>102</b> to learn a route. The input may be received from a human operator via user interface units <b>112</b> coupled to the robot <b>102</b>.</p><p id="p-0106" num="0105">Block <b>504</b> comprises the controller <b>118</b> navigating the route in a training mode. The training mode may configure the controller <b>118</b> to learn the route as a human operator moves the robot <b>102</b> through the route. The robot <b>102</b> may be pushed, driven, directed, steered, remotely controlled, or led through the route by the operator. As the human operator moves the robot <b>102</b> through the route, the controller <b>118</b> may store position data (e.g., measured by sensor units <b>114</b>) of the robot <b>102</b> over time to, for example, generate a pose graph of the robot <b>102</b> indicative of the route.</p><p id="p-0107" num="0106">According to at least one non-limiting exemplary embodiment, learning of a route may comprise the robot <b>102</b> operating in an exploration mode to (i) detect and localize objects within its environment, and (ii) find a shortest and safest (i.e., collision free) path to its destination. The exploration mode may be executed using, for example, an area fill algorithm which configures the robot <b>102</b> to explore its entire area and subsequently calculate a shortest path. Exploration mode for use in learning or discovering an optimal route from a first location to another may be advantageous if ample time is provided, human assistance is undesired, and the environment comprises few dynamic or changing objects (e.g., warehouses, stores after they have closed to the public, etc.).</p><p id="p-0108" num="0107">Block <b>506</b> comprises the controller <b>118</b> collecting data from sensor units <b>114</b> during navigation of the route to produce a computer-readable map of the route and surrounding environment. For example, the human operator may drive the robot <b>102</b> along the route, such as by remote control via user interface units <b>112</b> and communication units <b>116</b>. As the robot <b>102</b> is being driven through the route, controller <b>118</b> may collect and store data from sensor units <b>114</b>. The data collected may comprise any data useful for producing the computer-readable map and for later navigation of the route such as, without limitation, position data over time of the robot <b>102</b>, LiDAR scans or point clouds of nearby objects, colorized or greyscale images, and/or depth images from depth cameras.</p><p id="p-0109" num="0108">Block <b>508</b> comprises the controller <b>118</b> saving the computer-readable map and route data collected during navigation of the training route in blocks <b>504</b>-<b>506</b> in memory <b>120</b>.</p><p id="p-0110" num="0109">Block <b>510</b> comprises the controller <b>118</b>, upon completing the route, uploading the route data and computer-readable map to the server <b>202</b>. The route data and computer-readable map may be communicated to the server <b>202</b> via communications units <b>116</b> of the robot <b>102</b>. According to at least one non-limiting exemplary embodiment, the computer-readable map and route data may be communicated via communications units <b>116</b> to a robot network <b>210</b> and thereafter relayed to the server <b>202</b>.</p><p id="p-0111" num="0110">Block <b>512</b> comprises the controller <b>118</b> communicating with the server <b>202</b> to determine if there is any data to be synchronized. Data to be synchronized may comprise computer-readable maps produced by other robots <b>102</b> during navigation of the training route, other routes, software updates, and/or firmware updates.</p><p id="p-0112" num="0111">Upon the controller <b>118</b> receiving communication from the server <b>202</b> indicating there is no data to be synchronized with the server <b>202</b>, the controller <b>118</b> returns to block <b>310</b> and awaits a user input.</p><p id="p-0113" num="0112">Upon the controller <b>118</b> receiving communication from the server <b>202</b> indicating there is data to be synchronized, the controller <b>118</b> moves to block <b>514</b>.</p><p id="p-0114" num="0113">Block <b>514</b> comprises the controller <b>118</b> synchronizing with the server <b>202</b>. Synchronizing with the server <b>202</b> may comprise the server <b>202</b> communicating any new route data, computer-readable maps (e.g., produced by other robots <b>102</b> in the same environment), software updates, and/or firmware updates. The steps illustrated in blocks <b>512</b>-<b>514</b> ensure all routes and computer-readable maps stored in memory <b>120</b> of the robot <b>102</b> are up-to-the-moment based on data received by other robots <b>102</b>, external data sources <b>206</b>, and/or edge devices <b>208</b>.</p><p id="p-0115" num="0114">Although uploading route and map data is described in blocks <b>408</b> and <b>512</b> as being after completion of a route, alternatively or additionally in some embodiments, such data may be uploaded continuously, periodically (such as every 30 seconds, 1 minute, 5 minutes, etc.), or occasionally (such as after encountering an object or landmark along the route) as the robot <b>102</b> travels along a route. This may enable synchronizing data among a plurality of robots <b>102</b> traveling through a shared environment. This may be advantageous if the uploaded data may be used to inform other (succeeding) robots of new conditions discovered by a (preceding) robot that might influence the ability of the other robots to travel along the routes they are navigating. This embodiment may be most advantageous for robots <b>102</b> with ample communications bandwidth. Such data synchronization in (near-)real time may be particularly useful in environments where a plurality of robots is operating contemporaneously.</p><p id="p-0116" num="0115">An occasion wherein a robot <b>102</b> may upload route and map data prior to completion of a route may be when the robot <b>102</b> encounters a condition that prevents it or another robot <b>102</b> from completing a route. For illustration, a shelf-stocking robot navigating a route may encounter a spill. The shelf-stocking robot can upload data regarding the type and location of the spill to its network <b>210</b> and/or server <b>202</b> (e.g., a location of the spill on a computer-readable map). Based on that data, a determination can be made to activate a cleaning robot (see <figref idref="DRAWINGS">FIG. <b>3</b></figref>, block <b>302</b>) and download data (block <b>308</b>) to the spill location into the cleaning robot. In some embodiments, data about the condition, such as a spill, may also be the basis for a determination that a notification, sensor data etc. are to be sent to a higher-level controller, such as a server <b>202</b> or human user. After uploading data regarding the spill, the shelf-stocking robot may, for example, resume navigation of its route if possible (<figref idref="DRAWINGS">FIG. <b>4</b></figref>, block <b>404</b>) or may revert to block <b>310</b> and wait for user input. After receiving data about the spill at block <b>308</b>, the cleaning robot may wait for user input (block <b>310</b>) or navigate to the spill location (block <b>404</b>). Once the cleaning robot reaches the spill location, it may communicate with the server to upload and synchronize data (blocks <b>408</b>, <b>410</b> and <b>412</b>). The cleaning robot, depending on its instructions, may then wait for user input (block <b>310</b>) or autonomously clean up the spill. Accordingly, both the shelf-stocking robot and cleaning robot may no longer localize the spill on their respective computer-readable maps upon the cleaning robot synchronizing data with the server <b>202</b> subsequent to cleaning of the spill.</p><p id="p-0117" num="0116"><figref idref="DRAWINGS">FIG. <b>6</b>A-B</figref> illustrate the methods <b>300</b>, <b>400</b>, and <b>500</b> for synchronizing routes between two robots <b>102</b>-<b>1</b> and <b>102</b>-<b>2</b>, according to an exemplary embodiment. First, in <figref idref="DRAWINGS">FIG. <b>6</b>A</figref>, one robot <b>102</b>-<b>1</b> is being taught a new route <b>606</b>-<b>1</b> by a human operator <b>610</b>. Concurrently, a different robot <b>102</b>-<b>2</b> is navigating a route <b>606</b>-<b>2</b> which has been stored in the memory <b>120</b> of the robot <b>102</b>-<b>2</b> and synchronized with a server <b>202</b>. Both routes <b>606</b>-<b>1</b>, <b>606</b>-<b>2</b> may begin at their respective starting points <b>602</b>-<b>1</b>, <b>602</b>-<b>2</b>. The starting points <b>602</b>-<b>1</b>, <b>602</b>-<b>2</b> may comprise a landmark (e.g., an image or feature) or any predetermined point within environment <b>600</b> such as, for example, a barcode or quick response (&#x201c;QR&#x201d;) code. Each landmark <b>602</b> may correspond to one or more routes <b>606</b> beginning and ending at the respective landmarks <b>602</b>. In some embodiments, routes <b>606</b> may start and end at different landmarks <b>602</b>. While the robot <b>102</b>-<b>1</b> is being taught the route <b>606</b>-<b>1</b> (e.g., by the operator <b>610</b> driving, pushing, leading, pulling, or otherwise demonstrating the route <b>606</b>-<b>1</b>), the other robot <b>102</b>-<b>2</b> may have completed the route <b>606</b>-<b>2</b>. Following method <b>400</b>, the other robot <b>102</b>-<b>2</b> communicates with the server <b>202</b> a computer-readable map produced by data from sensor units <b>114</b> during navigation of the route <b>606</b>-<b>2</b>, as illustrated by dashed arrow <b>608</b>. The server <b>202</b> may utilize the computer-readable map to determine updates to the route <b>606</b>-<b>2</b> such as, for example, updating a position of one or more of objects <b>604</b>, which may change over time.</p><p id="p-0118" num="0117">Next, in <figref idref="DRAWINGS">FIG. <b>6</b>B</figref>, robot <b>102</b>-<b>1</b> has completed its training of route <b>606</b>-<b>1</b> and, following method <b>500</b> above, uploads a computer-readable map generated by sensor data collected during navigation of route <b>606</b>-<b>1</b> to the server <b>202</b>. This data will be synchronized with route and map data of the other robot <b>102</b>-<b>2</b> to provide the other robot <b>102</b>-<b>2</b> with the new route and corresponding computer-readable map produced by the robot <b>102</b>-<b>1</b> during training. At a later time, the operator <b>610</b> may configure the other robot <b>102</b>-<b>2</b> to navigate the route <b>606</b>-<b>1</b>. Advantageously, due to the synchronization illustrated in methods <b>300</b>, <b>400</b>, <b>500</b> above, the other robot <b>102</b>-<b>2</b> may now comprise a computer-readable map and data for route <b>606</b>-<b>1</b> stored within its memory <b>120</b>, the map for route <b>606</b>-<b>1</b> having been generated by robot <b>102</b>-<b>1</b>. Similarly, robot <b>102</b>-<b>1</b> may also receive a map of route <b>606</b>-<b>2</b> after synchronizing with the server <b>202</b> following method <b>500</b>. Accordingly, the robot <b>102</b>-<b>2</b> may begin navigating the second route <b>606</b>-<b>1</b> without a need for the operator <b>610</b> to teach it the new route <b>602</b>-<b>1</b>. The robot <b>102</b>-<b>1</b> may, for example, navigate the route <b>606</b>-<b>2</b> or be powered off for later use. In the scenario depicted in <figref idref="DRAWINGS">FIGS. <b>6</b>A and <b>6</b>B</figref>, robot <b>102</b>-<b>1</b> is the preceding robot for route <b>606</b>-<b>1</b> and the succeeding robot for route <b>606</b>-<b>2</b>. Likewise, robot <b>102</b>-<b>2</b> is the preceding robot for route <b>606</b>-<b>2</b> and the succeeding robot for route <b>606</b>-<b>1</b>. Both robots <b>102</b> are able to navigate both routes <b>606</b>-<b>1</b> and <b>606</b>-<b>2</b> despite each robot <b>102</b>-<b>1</b> and <b>102</b>-<b>2</b> having only navigated different ones of the two routes <b>606</b>-<b>1</b>, <b>606</b>-<b>2</b>.</p><p id="p-0119" num="0118"><figref idref="DRAWINGS">FIGS. <b>7</b>A-B</figref> illustrate a method for synchronizing a route between two robots <b>102</b>-<b>1</b> and <b>102</b>-<b>2</b> of different types or comprising different footprints, according to an exemplary embodiment. The methods <b>300</b>, <b>400</b>, <b>500</b> above are applicable, however additional optimizations to a synchronized route may be required if the two robots <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b> between which the route is being synchronized comprise different sizes, tracks (e.g., differential drives, tricycle, four wheels), makes/models, etc. Further, one skilled in the art may appreciate that not all routes are capable of being navigated by all types of robots <b>102</b> and in some instances, route synchronization may not be possible.</p><p id="p-0120" num="0119"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> illustrates a robot <b>102</b>-<b>1</b> navigating a first route <b>702</b> beginning at a starting location <b>704</b>-<b>0</b>, the first route <b>702</b> comprising a path around an object <b>706</b>, the first route <b>702</b> comprising a pose graph, according to an exemplary embodiment. The starting location <b>704</b>-<b>0</b> being proximate to a landmark <b>700</b>, the landmark <b>700</b> comprising a feature which denotes the start of route <b>702</b> (e.g., a QR code, an infrared beacon, audio beacon, light, and/or any other feature of an environment). The pose graph may comprise any of (x, y, z, yaw, pitch, roll) coordinates which denote a position of the robot <b>102</b> at each point <b>704</b>, each point <b>704</b> being a predetermined distance or time along the route <b>702</b> (e.g., every 5 seconds robot <b>102</b>-<b>1</b> moves from (x<sub>1</sub>, y<sub>1</sub>, yawi) to (x<sub>2</sub>, y<sub>2</sub>, yawn)). Each point <b>704</b> may be illustrative of a pose of the pose graph. Illustrated for clarity is the footprint <b>708</b> (i.e., area occupied) of the robot <b>102</b>-<b>1</b> at each respective point <b>704</b> illustrated on a two-dimensional birds-eye view computer-readable map. Upon completion of the first route <b>702</b>, the robot <b>102</b>-<b>1</b> may upload the resulting pose graph formed by points <b>704</b> as well as any additional localization data of the nearby object <b>706</b> to the server <b>202</b>, following methods <b>400</b> or <b>500</b>. Alternatively, the pose graph may be uploaded after each point <b>704</b> is reached by the robot <b>102</b>.</p><p id="p-0121" num="0120">Next, in <figref idref="DRAWINGS">FIG. <b>7</b>B</figref>, a larger robot <b>102</b>-<b>2</b> may begin at the starting location <b>704</b>-<b>0</b> proximate to the same landmark <b>700</b> used to start the route <b>702</b> by the smaller robot <b>102</b> illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, according to an exemplary embodiment. Following methods <b>300</b>, <b>400</b>, or <b>500</b> above, large robot <b>102</b>-<b>2</b> may receive data for the route <b>702</b> including the pose graph executed by robot <b>102</b>-<b>1</b> and a computer-readable map which, in part, localizes object <b>706</b>. The large robot <b>102</b>-<b>2</b> may perform a test to determine if the pose graph of route <b>702</b> is navigable by the large robot <b>102</b>-<b>2</b> without colliding with the object <b>706</b>. The test comprises, for each point <b>704</b> (open circles) of the pose graph of route <b>702</b> (dashed line) controller <b>118</b> superimposing a simulated footprint <b>712</b> of the large robot <b>102</b>-<b>2</b> and determining if the footprint <b>712</b> intersects with object <b>706</b>. If a potential collision (i.e., overlap between a simulated footprint <b>712</b> and object <b>710</b> on the map as shown by footprint <b>716</b> corresponding to the position of the large robot <b>102</b>-<b>2</b> at point <b>704</b>-<b>1</b>) is detected, controller <b>118</b> may calculate changes to the route <b>702</b>, such as moving the robot <b>102</b>-<b>2</b> farther from object <b>706</b>, until no overlap between object <b>706</b> and footprints <b>712</b> occur to provide a new route <b>710</b>. The calculated changes are represented by route <b>710</b> (solid line) comprising a pose graph containing poses <b>714</b> (closed circles) which causes the robot <b>102</b>-<b>2</b> to navigate farther from the object <b>706</b> than the robot <b>102</b>-<b>1</b>. The initial pose <b>708</b>-<b>0</b> being the same as the initial pose <b>704</b>-<b>0</b> of the route <b>702</b>. In this manner the controller <b>118</b> of the robot <b>102</b>-<b>2</b>, or a processor on server <b>202</b> may project a footprint <b>712</b> at each point <b>704</b> of the pose graph of route <b>706</b> on the computer-readable map to determine if the route is navigable without collisions and if collisions occur determine any changes to the route to avoid collisions, such as prior to beginning the route.</p><p id="p-0122" num="0121">In some embodiments, a controller <b>118</b> of the robot <b>102</b>-<b>2</b>, or a processor on server <b>202</b>, may modify and update all routes stored for robot <b>102</b>-<b>2</b> received from other robots (e.g., <b>102</b>-<b>1</b>) to navigate through the environment and avoid collisions. In other embodiments, modification of routes for robot <b>102</b>-<b>2</b> may be made only as needed for each specific route.</p><p id="p-0123" num="0122">One skilled in the art may appreciate that not all routes are navigable by all types of robots <b>102</b>. For example, a small differential drive robot may navigate almost all routes navigable by a large tricycle robot; however, the large tricycle robot may not navigate all routes the smaller differential drive robot is able to navigate. Similarly, large robot <b>102</b>-<b>2</b> may find route <b>706</b> to be unnavigable without collisions, despite changes thereto, using footprints <b>712</b>. For example, a path between two objects <b>706</b> (not shown) may be impassable using a large robot having a footprint <b>712</b>.</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a process flow diagram illustrating a method <b>800</b> for a robot <b>102</b> to synchronize a route received from another robot <b>102</b> of a different type and/or size and determine if the route is navigable, according to an exemplary embodiment.</p><p id="p-0125" num="0124">Block <b>802</b> comprises a controller <b>118</b> of a robot <b>102</b> receiving a computer-readable map comprising a route. The computer-readable map is received from a server <b>202</b> and produced by a different robot <b>102</b> of a different type, size, and/or shape.</p><p id="p-0126" num="0125">Block <b>804</b> comprises the controller <b>118</b> superimposing at least one simulated robot footprint <b>712</b> along the received route. The robot footprint <b>712</b> comprises a projection (e.g., 2-dimensional top view projection or 3-dimensional projection) of an area occupied by the robot <b>102</b> on the computer-readable map. According to at least one non-limiting exemplary embodiment, the received computer-readable map and route may comprise in part a pose graph, wherein the footprint <b>712</b> is projected at each point of the pose graph to detect collisions as illustrated in <figref idref="DRAWINGS">FIG. <b>7</b>A-B</figref> above. According to at least one non-limiting exemplary embodiment, the route may comprise a continuous path or line, wherein the at least one footprint <b>712</b> may be virtually (i.e., simulated) moved along the route on the computer-readable map. According to at least one non-limiting exemplary embodiment, a plurality of footprints <b>712</b> is positioned along the route separated by a fixed distance (e.g., every 2 meters along the route).</p><p id="p-0127" num="0126">Block <b>806</b> comprises the controller <b>118</b> detecting collisions along the route using the footprints <b>712</b>. Detection of a collision comprises at least one of the footprints <b>712</b> superimposed on the computer-readable map overlapping at least in part with one or more objects.</p><p id="p-0128" num="0127">Upon the controller <b>118</b> determining at least one footprint <b>712</b> overlaps at least in part with an object on the computer-readable map, the controller <b>118</b> moves to block <b>808</b>.</p><p id="p-0129" num="0128">Upon the controller <b>118</b> determining the entire route causes no overlap between a footprint <b>712</b> and objects, the controller <b>118</b> may move to block <b>814</b>.</p><p id="p-0130" num="0129">Block <b>808</b> comprises the controller <b>118</b> modifying the route. According to at least one non-limiting exemplary embodiment, modifications of the route may comprise an iterative process of moving a point of a pose graph, checking for a collision using a footprint <b>712</b>, and repeating until no collision occurs. According to at least one non-limiting exemplary embodiment, modifications of the route may comprise rubber banding or stretching of the route to cause the robot <b>102</b> to execute larger turns or navigate further away from obstacles. According to at least one non-limiting exemplary embodiment, modifications to the route may comprise a use of a cost map, wherein the lowest cost solution (if possible, without collisions) is chosen. A cost map may at least associate a high cost with object collision, a high cost for excessively long routes, and a low cost for a collision-free short route. Other cost parameters may be considered such as tightness of turns or costs for abrupt movements.</p><p id="p-0131" num="0130">Block <b>810</b> comprises the controller <b>118</b> determining if a collision-free route is possible. If the controller <b>118</b> is unable to determine a modification to the route which is, for example, collision free or below a specified cost threshold, the controller <b>118</b> may determine no modifications to the route may enable the robot <b>102</b> to navigate the route.</p><p id="p-0132" num="0131">Upon the controller <b>118</b> determining no modifications to the route enable the robot <b>102</b> to execute the route, the controller <b>118</b> moves to block <b>812</b>.</p><p id="p-0133" num="0132">Upon the controller <b>118</b> determining a modification to the route which enables the robot <b>102</b> to execute the route without collisions, the controller <b>118</b> returns to block <b>806</b>.</p><p id="p-0134" num="0133">Block <b>812</b> comprises the controller <b>118</b> determining the route is unnavigable without collision with objects. According to at least one non-limiting exemplary embodiment, the controller <b>118</b> may communicate this determination to a robot network <b>210</b> and/or server <b>202</b>. Thereafter, the server <b>202</b> or network <b>210</b> will avoid providing the same route to the robot <b>102</b>.</p><p id="p-0135" num="0134">Block <b>814</b> comprises the controller <b>118</b> saving the route data in memory <b>120</b> along with any modifications made thereto; and thereafter wait for user input for additional tasks for the robot to complete as reflected in block <b>310</b> in <figref idref="DRAWINGS">FIGS. <b>3</b>-<b>5</b></figref> and discussed above.</p><p id="p-0136" num="0135">Advantageously, the method <b>800</b> may enable a robot <b>102</b> to verify that a received route is navigable without the robot <b>102</b> navigating the route itself and, if not, any modifications required to configure the route to become navigable. That is, a succeeding robot <b>102</b> may independently verify that a route received from a preceding, different robot <b>102</b> is navigable using the received computer-readable map and footprints <b>712</b> superimposed thereon.</p><p id="p-0137" num="0136">One skilled in the art would appreciate that in some instances, the most recent preceding route information may be informative, but may not include all information useful for a succeeding route. For illustration, the most recent preceding run of a route may have been at 11:30 PM on Friday and the succeeding route may be executed at 6:00 AM on Saturday. One or more processors may, according to methods described herein, determine that information related to another preceding route executed on a previous Saturday at 6:00 AM may be more indicative of conditions likely to be encountered than information collected in the most recent preceding run at 11:30 PM on Friday. In another example, for a robot of a specific type, size, or capability, selection by one or more processors of a preceding route executed by a robot of the same type, size or capability may be preferable to the most recent preceding run of a route by a robot of a different type, size or capability. One or more processors may, according to methods described herein, compare the most recent preceding route and map data with route and map data of a different preceding route and determine that the most recent route and map data does not impact the ability of a succeeding robot to execute the route for a succeeding run of the different preceding route. Alternatively, the one or more processors may determine that the most recent preceding route and map data does impact the ability of a succeeding robot to execute the route for a succeeding run of the different preceding route. In those instances, one or more processors may, according to methods described herein, modify a preceding route to reflect the route and map data synchronized from the most recent preceding route. For example, a portion of a preceding route may be unchanged and a different portion of that preceding route may be changed to address the new conditions found in the most recent route synchronization. The modified route would then be used for a succeeding run of the route.</p><p id="p-0138" num="0137"><figref idref="DRAWINGS">FIGS. <b>9</b>A-B</figref> illustrates two robots <b>102</b>, a first robot <b>102</b>-<b>1</b> may be uploading data to the server <b>202</b> while the other robot <b>102</b>-<b>2</b> is checking if data is available to synchronize with the server <b>202</b>, according to an exemplary embodiment. Data received from the robot <b>102</b>-<b>1</b> may include metadata and binary data. Metadata may include timestamps, an environment ID (i.e., an identification number or code which corresponds to an environment of robot <b>102</b>-<b>1</b>), a network <b>210</b> ID (i.e., an identifier which specifies a robot network <b>210</b> which includes robot <b>102</b>-<b>1</b>), and/or other metadata (e.g., robot ID, route type (e.g., new route or replayed route), etc.). Binary data may include data from sensor units <b>114</b>, computer-readable maps produced during navigation of a route, performance metrics (e.g., average deviation from the route to avoid obstacles), route data (i.e., the path traveled), and the like. The server <b>202</b> may receive communications <b>902</b>, <b>904</b> representing the robot <b>102</b>-<b>1</b> communicating the binary data and metadata, respectively, to the server <b>202</b>, the binary data and metadata correspond to a run of a route (e.g., execution of methods <b>400</b> or <b>500</b> above). The two communications <b>902</b>, <b>904</b> may be received by the server <b>202</b> contemporaneously or sequentially, wherein communicating binary data at a later time may reduce network bandwidth occupied by the robot <b>102</b>-<b>1</b> (e.g., robot <b>102</b>-<b>1</b> may wait for a Wi-Fi signal to issue communications <b>902</b> but may issue communications <b>904</b> using LTE or cellular networks).</p><p id="p-0139" num="0138">Server <b>202</b> may store binary data <b>906</b> and metadata <b>908</b> in a memory, such as memory <b>130</b> described in <figref idref="DRAWINGS">FIG. <b>1</b>B</figref>. The server <b>202</b> may store the binary data <b>906</b> and metadata <b>908</b> in a same or separate memory <b>130</b>. The metadata stored on the server <b>202</b> may include, in part, a list of routes corresponding to the environment of robots <b>102</b>-<b>1</b> and <b>102</b>-<b>2</b>, wherein the list of routes may correspond to one or more computer-readable maps of the binary data <b>906</b>.</p><p id="p-0140" num="0139">Robot <b>102</b>-<b>2</b> may have completed a route, learned a new route, or may have been initialized for a first time following the methods illustrated in <figref idref="DRAWINGS">FIG. <b>3</b>-<b>5</b></figref> above. Accordingly, the robot <b>102</b>-<b>2</b> may synchronize data with the server <b>202</b>. For robot <b>102</b>-<b>2</b> to check if data is available to be synchronized with the server <b>202</b>, the robot <b>102</b> may communicate with the server <b>202</b> to receive metadata <b>910</b> associated with the environment. The metadata may include, for example, a list of routes associated with the environment and timestamps corresponding to the routes. The controller <b>118</b> of the robot <b>102</b>-<b>2</b> may compare the metadata received via communications <b>910</b> to determine if the routes stored within memory <b>120</b> of the robot <b>102</b>-<b>2</b> matche the routes stored in memory of the server <b>202</b>. In some embodiments, the server <b>202</b> may receive metadata from the robot <b>102</b>-<b>2</b>, such as a ledger <b>918</b> shown in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>, wherein processing devices <b>138</b> of the server <b>202</b> may perform the comparison. That is, server <b>202</b> may store a list of routes (i.e., metadata <b>908</b>) corresponding to the environment of robots <b>102</b>-<b>1</b> and <b>102</b>-<b>2</b>, wherein the controller <b>118</b> of the robot <b>102</b>-<b>2</b> may compare its list (stored locally on memory <b>120</b>) with the list stored on the server <b>202</b>. If the controller <b>118</b> detects a discrepancy between the two lists, the controller <b>118</b> may synchronize the binary data <b>906</b> with the server shown by communications <b>912</b>. Accordingly, the controller <b>118</b> may receive up-to-the-moment route and map information corresponding to its environment upon verifying that route and map information stored locally within its memory <b>120</b> comprises discrepancies with the route and map information stored on the server <b>202</b> by comparing metadata. This comparison is further illustrated next in <figref idref="DRAWINGS">FIG. <b>9</b>B</figref>.</p><p id="p-0141" num="0140"><figref idref="DRAWINGS">FIG. <b>9</b>B</figref> illustrates local metadata ledgers <b>914</b> and <b>918</b> stored in respective memories <b>120</b> of the robots <b>102</b>-<b>1</b> and <b>102</b>-<b>2</b> shown in <figref idref="DRAWINGS">FIG. <b>9</b>A</figref> above and a metadata ledger <b>916</b> of the server <b>202</b>, according to an exemplary embodiment. As robots <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b> learn, navigate (i.e., replays), or delete routes stored in their respective memories <b>120</b>, the controllers <b>118</b> may keep a ledger <b>914</b>, <b>918</b> to document the behavior of the robot <b>102</b> with respect to routes learned, navigated, or deleted. For example, for a route identified by a route ID &#x201c;AAAA,&#x201d; the metadata associated with the route ID &#x201c;AAAA&#x201d; may include the creation or training of the route (i.e., entry &#x201c;NEW ROUTE&#x201d;, wherein the route may be learned in accordance with method <b>500</b> above), the replay or navigation of the existing route (i.e., entry &#x201c;REPLAY&#x201d; which includes a timestamp denoted by a date), and/or deletion of the route (i.e., entry &#x201c;DELETE&#x201d;).</p><p id="p-0142" num="0141">By way of illustration, an operator of robot <b>102</b>-<b>1</b> may train a route associated with route ID &#x201c;AAAA&#x201d; at a first instance in time. Subsequently, following method <b>400</b>, the controller <b>118</b> may synchronize data with the server <b>202</b> which includes providing metadata associated with the new route such as the route ID, a timestamp, an environment or network <b>210</b> ID, and/or other metadata not shown (e.g., route length). Accordingly, the server <b>202</b> may store the route ID &#x201c;AAAA&#x201d; and corresponding metadata which represents that route &#x201c;AAAA&#x201d; is a new route in its respective ledger <b>916</b>. Binary data, such as computer-readable maps, sensor data, route data, and the like associated with the new route &#x201c;AAAA&#x201d; may be communicated and stored in a separate memory or in a different location in memory. The server <b>202</b> may further provide the same route ID and metadata associated thereto to the second robot <b>102</b>, wherein the second robot <b>102</b> may store the route ID and metadata in its ledger <b>918</b>. Binary data associated with the route &#x201c;AAAA&#x201d; may be communicated to the robot <b>102</b>-<b>2</b> and stored in its memory <b>120</b> to enable the robot <b>102</b>-<b>2</b> to replay the route, as shown in <figref idref="DRAWINGS">FIG. <b>6</b>A-B</figref>.</p><p id="p-0143" num="0142">At a second instance in time subsequent to the first instance in time, either the robot <b>102</b>-<b>1</b> or <b>102</b>-<b>2</b> may navigate the same route of route ID &#x201c;AAAA,&#x201d; wherein the respective controller <b>118</b> stores the metadata associated with the run of the route in its respective ledger <b>914</b> or <b>918</b>. Accordingly, the server <b>202</b> and both robots <b>102</b>-<b>1</b>, <b>102</b>-<b>2</b> may, upon synchronization, store the metadata associated with the run of the route in their respective ledgers <b>914</b>, <b>916</b>, <b>918</b> as shown by the second entries comprising a &#x201c;REPLAY&#x201d; and a date and/or time of the replay. Replay corresponds to a robot replaying or renavigation the route for a second, third, fourth, etc. time.</p><p id="p-0144" num="0143">At a third instance in time subsequent to the second instance in time, the robot <b>102</b>-<b>1</b> may receive an indication from an operator via its user interface units <b>112</b> to delete the route associated with the route ID &#x201c;AAAA.&#x201d; Accordingly, the deletion of the route may be denoted in the ledger <b>914</b> as shown by the metadata &#x201c;DELETE&#x201d; corresponding to the route ID &#x201c;AAAA.&#x201d; The robot <b>102</b>-<b>1</b> may delete binary data associated with the route from its memory <b>120</b>. In accordance with methods <b>300</b>, <b>400</b>, <b>500</b> above, the controller <b>118</b> of the robot <b>102</b>-<b>1</b> may communicate with the server <b>202</b> (via communications <b>920</b>) to synchronize its ledger <b>914</b> with the ledger <b>916</b> stored on the server <b>202</b> such that the ledger of the server <b>916</b> includes deletion of the route associated with the route ID &#x201c;AAAA.&#x201d; At a fourth instance in time, subsequent to the third instance, the controller <b>118</b> of the second robot <b>102</b>-<b>2</b> may compare its ledger <b>918</b> with the ledger <b>916</b> of the server <b>202</b>. Alternatively, a processing device <b>138</b> of the server <b>202</b> may compare its ledger <b>916</b> with a ledger <b>918</b> received from the robot <b>102</b>-<b>2</b>. The controller <b>118</b> of the robot <b>102</b>-<b>2</b> may identify that its ledger <b>918</b> differs from the ledger <b>916</b> of the server <b>202</b> (i.e., checks if data is available to be synchronized) and, upon identifying the discrepancy, the controller <b>118</b> synchronizes its ledger <b>918</b> with the ledger <b>916</b> of the server <b>202</b>, as shown by arrows <b>924</b>. Accordingly, the route associated with the route ID &#x201c;AAAA&#x201d; may be deleted from memory <b>120</b> of the robot <b>102</b>-<b>2</b> upon the controller <b>118</b> receiving the metadata corresponding to the deletion of the route.</p><p id="p-0145" num="0144"><figref idref="DRAWINGS">FIG. <b>10</b></figref> illustrates binding trees <b>1000</b>, <b>1014</b> used to synchronize routes between robots <b>102</b>, according to an exemplary embodiment. A binding, as used herein, represents a relationship between two devices, components, or things. A component may comprise a file or other granular piece of data or metadata (e.g., a map for a route). The binding tree represents the relationship between a device, such as a given robot <b>102</b>, and components such as routes executable by the robot <b>102</b>. Bindings are represented by the arrows shown in binding tree <b>1000</b> and components are akin to the functional blocks thereof, although one skilled in the art may appreciate that the functional blocks shown herein may include numerous components. The binding tree may be stored on both the robot <b>102</b> and server <b>202</b> to ensure both entities agree upon the current state of the components thereof, wherein any discrepancies may be corrected via synchronization. Each block shown in the binding tree may represent data synchronized between a server <b>202</b> and robots <b>102</b>. That is, both the server <b>202</b> and robot <b>102</b> continuously synchronize their respective binding trees.</p><p id="p-0146" num="0145">The binding trees illustrated may correspond to two separate environments or sites A and B. Within site A, two robots <b>102</b> A and <b>102</b> B operate while only one robot operates in site B. Beginning at the device level of robot A, the robot A may be identified by the server <b>202</b> using a unique identifier, such as an alphanumeric code. Continuing along the binding tree <b>1000</b> the robot A may be bound to a product block <b>1002</b> comprising &#x201c;Product A.&#x201d; Product A may comprise an identifier for a product, or type of robot. For example, product A may correspond to a floor-sweeping robot, an item-transport robot, a floor-scrubbing robot, and so forth. Stated differently, the product block <b>1002</b> may identify a shelf-keeping unit (&#x201c;SKU&#x201d;), universal product code (&#x201c;UPC&#x201d;), or other unique identifier for a specific robot type. The specific value represented by the product blocks <b>1002</b> may be pre-determined by a manufacturer of the robot <b>102</b>.</p><p id="p-0147" num="0146">The robot <b>102</b>, now bound to a specific product type, is bound to an activation block <b>1004</b>. The activation block <b>1004</b> may include customer information used to indicate that the robot <b>102</b> is activated by the manufacturer of the robot <b>102</b>. Robots <b>102</b> produced by a manufacturer may be left inactivated until they are purchased by a consumer, wherein the activation block <b>1004</b> binds the robot A to the consumer. In some embodiments, the consumer may pay a recurring service fee for maintenance and autonomy services of the robot <b>102</b>, wherein the activation data may be used to create billing information for the consumer.</p><p id="p-0148" num="0147">If the consumer, later, no longer desires to utilize the robot <b>102</b> and pay the service fees, the data in activation A block <b>1004</b> may be changed from &#x201c;Active&#x201d; to &#x201c;Deactivate.&#x201d; The change may be performed on the robot <b>102</b> via user interface units <b>112</b> or on the server <b>202</b> via a device <b>208</b>, such as an admin terminal. In either case, and based on method <b>300</b>, the update to the binding tree <b>1000</b> will be synchronized between the robot A, server <b>202</b>, and robot B such that both the server <b>202</b> and robot B include a binding tree <b>1000</b> with no robot A or at least a deactivated robot A.</p><p id="p-0149" num="0148">Continuing along the binding tree <b>1000</b>, the robot A (now associated with a product type and consumer activation) may now be bound to a site <b>1006</b>. The site <b>1006</b> block may represent a unique identifier, or other metadata, for the environment the consumer would desire the robot <b>102</b> to operate in. In addition to robot A, robot B (also bound to its own product type and consumer activation, which may be the same or different from robot A) is also bound to the site A indicating that both robots <b>102</b> operate within this environment.</p><p id="p-0150" num="0149">Site and activation blocks <b>1004</b>, <b>1006</b> are denoted as separate blocks of information to facilitate transfer of a robot <b>102</b> from site A to another site owned by the same consumer. That is, the activation <b>1004</b> of the robot <b>102</b> may be the same in the new environment while the site <b>1006</b> is updated.</p><p id="p-0151" num="0150">In some instances, ownership of the robot <b>102</b> may change while the robot <b>102</b> continues to operate at site A.</p><p id="p-0152" num="0151">Further down the binding tree <b>1000</b>, the robot <b>102</b>, being now bound to a product type, activation information, and site information, is further bound to various home codes <b>1008</b> A, B, and C. The home codes <b>1008</b> may represent three landmarks recognizable by the robot <b>102</b> as a start of a route, such as landmarks <b>602</b> or <b>700</b> shown in <figref idref="DRAWINGS">FIG. <b>6</b>-<b>7</b></figref> above. A home code <b>1008</b> may be bound to the site A <b>1006</b> upon robot A or B <b>102</b>, bound to the site <b>1006</b>, detecting a home code <b>1008</b> before, during, or after learning a route <b>1010</b>. Each home code <b>1008</b> may denote the start, end, or midpoint of one or many routes associated with the home code <b>1008</b>. Looking further at home code A <b>1008</b>, home code A <b>1008</b> is bound to two routes <b>1010</b>-Al and <b>1010</b>-A<b>2</b>. The routes <b>1010</b> may be bound to the home code A <b>1008</b> by an operator training either robot A or robot B to learn the routes <b>1010</b>-Al and <b>1010</b>-A<b>2</b>, wherein the training of the routes <b>1010</b>-Al and <b>1010</b>-A<b>2</b> begins, ends, or includes the robot <b>102</b> detecting the home code A <b>1008</b>. Similarly, home codes B and C are also bound to site A and their respective routes, wherein the number of routes bound to the home codes <b>1008</b> is not intended to be limited to two as shown and may be more or fewer.</p><p id="p-0153" num="0152">Each route <b>1010</b> may comprise route components <b>1012</b> needed by the robot <b>102</b> to recreate the route autonomously; only one set of route components <b>1012</b> for route <b>1010</b>-A<b>2</b> is shown for clarity. The route components <b>1012</b> may include binary data and may include pose graphs, route information, computer-readable maps, and/or any other data needed by the robot <b>102</b> to recreate the route autonomously. Assuming robot A learned route <b>1010</b>-A<b>2</b> and generated the route components <b>1012</b>, the route components <b>1012</b> may be synchronized with robot B following method <b>300</b>, wherein the server <b>202</b> synchronizes its binding tree <b>1000</b> stored in its memory to include route components <b>1012</b> from robot A which is subsequently transferred to robot B. Shared data <b>1016</b> illustrates the data shared between robots <b>102</b> A and B, wherein the shared data includes the site data <b>1006</b> and route data (i.e., home code data <b>1008</b> and route components <b>1012</b>). The binding tree <b>1000</b> may indicate to the server <b>202</b> which robots <b>102</b> connected to the server <b>202</b> should receive the route components <b>1012</b>. Specifically, the server <b>202</b> only synchronizes binary route components <b>1012</b> with robot B since robot B is within the same site A <b>1006</b>. Robot C, shown in binding tree <b>1014</b>, does not receive the route A<b>2</b> components <b>1012</b>, or any components <b>1012</b> of any routes <b>1010</b> associated with Site A.</p><p id="p-0154" num="0153">Assuming no further updates are made to the binary route components <b>1012</b>, such as changes to the shape of the route (e.g., as provided to a user interface <b>112</b> of a robot <b>102</b>), the binary data remains static without a need for synchronization. If a route component is changed, a discrepancy between the binding tree <b>1000</b> of the robot <b>102</b> and the binding tree <b>1000</b> stored in the server <b>202</b> arises. When a route component <b>1012</b> is created, edited, or deleted, the robot <b>102</b> may note the change as a change to site A. For example, a parameter stored in memory <b>120</b> on the robot <b>102</b> may change value from 0 (no change) to 1 (change) upon one or more home codes <b>1008</b>, routes <b>1010</b>, and/or route components being created, deleted, or edited. Upon the parameter changing to a value indicating a change to the binding tree at the site A <b>1006</b> level or below, the robot <b>102</b> may ping the server <b>202</b> with an indication that the site data <b>1006</b> has changed locally on the device, thereby requiring synchronization.</p><p id="p-0155" num="0154">The server <b>202</b>, in response to the ping, may issue communications to other robots <b>102</b> bound to the same site <b>1006</b>. Such communication may enable the other robots <b>102</b> to know that data is available to sync before the binary data is synchronized. By way of an illustrative example, robot A may issue a ping to the server <b>202</b> to indicate a change to any component of the shared data <b>1016</b>. In response to this ping, the server <b>202</b> issues a communication to robot B indicating the change occurred and that new data is available to be synchronized. In some instances, robot B may display on its user interface <b>112</b> that data is available to be synchronized. An operator of robot B may, upon noticing that data is available to be synchronized, pause autonomous operation of robot B until after the data is synchronized. In other embodiments, the data is synchronized automatically upon robot B receiving indication of a change to the shared data <b>1016</b>, provided robot B includes a secure connection to the server <b>202</b> and is not pre-occupied with other tasks.</p><p id="p-0156" num="0155">Upon detecting the update to the shared data <b>1016</b> from the robot <b>102</b> via the received ping, the server <b>202</b> will update its binding tree <b>1000</b> using binary data shared from the robot <b>102</b>. This binary data is subsequently synchronized to the remaining robots <b>102</b> at site A such that the remaining robots <b>102</b> include the modified shared data <b>1016</b>. The server <b>202</b> further updates the metadata, such as timestamps, of route components <b>1012</b> stored in its memory (e.g., ledger <b>916</b>) and on the robot <b>102</b> memory <b>120</b> (e.g., ledger <b>914</b>, <b>918</b>) such that each robot <b>102</b> includes an up-to-date ledger <b>914</b>, <b>918</b> and up-to-date binding tree <b>1000</b> locally.</p><p id="p-0157" num="0156">A binding tree may be generated for each robot <b>102</b> coupled to the server <b>202</b> to enable the server <b>202</b> to determine relationships between a given robot <b>102</b> and its various operating parameters, such as the types of robots, the site information <b>1006</b>, activation information <b>1004</b>, route information, and the like. With reference to <figref idref="DRAWINGS">FIG. <b>2</b></figref>, the binding tree <b>1000</b> may define parameters and relationship between the server <b>202</b> and any given robot <b>102</b> of a robot network <b>210</b>. Binding trees enable the server <b>202</b> to determine which robots <b>102</b> synchronize databy ensuring binary data is only synchronized between robots <b>102</b> bound to a same site and, in some embodiments, robots <b>102</b> of a same product type <b>1002</b>.</p><p id="p-0158" num="0157">Advantageously, by tracking changes to the binding tree, the server <b>202</b> and robots <b>102</b> coupled to a site may be aware of any changes to be synchronized before the binary data is synchronized which may indicate to users of the robots <b>102</b> that data can be synchronized for more efficient usage of their robots <b>102</b>. Further, by detecting a change to the binding tree <b>1000</b> locally on the robot <b>102</b> via determining if a change to shared data <b>1016</b> occurred, the query time taken by the server <b>202</b> to detect if a change to shared data <b>1016</b> occurred is reduced.</p><p id="p-0159" num="0158">It will be recognized that while certain aspects of the disclosure are described in terms of a specific sequence of steps of a method, these descriptions are only illustrative of the broader methods of the disclosure and may be modified as required by the particular application. Certain steps may be rendered unnecessary or optional under certain circumstances. Additionally, certain steps or functionality may be added to the disclosed embodiments, or the order of performance of two or more steps permuted. All such variations are considered to be encompassed within the disclosure disclosed and claimed herein.</p><p id="p-0160" num="0159">While the above detailed description has shown, described, and pointed out novel features of the disclosure as applied to various exemplary embodiments, it will be understood that various omissions, substitutions, and changes in the form and details of the device or process illustrated may be made by those skilled in the art without departing from the disclosure. The foregoing description is of the best mode presently contemplated of carrying out the disclosure. This description is in no way meant to be limiting, but rather should be taken as illustrative of the general principles of the disclosure. The scope of the disclosure should be determined with reference to the claims.</p><p id="p-0161" num="0160">While the disclosure has been illustrated and described in detail in the drawings and foregoing description, such illustration and description are to be considered illustrative or exemplary and not restrictive. The disclosure is not limited to the disclosed embodiments. Variations to the disclosed embodiments and/or implementations may be understood and effected by those skilled in the art in practicing the claimed disclosure, from a study of the drawings, the disclosure and the appended claims.</p><p id="p-0162" num="0161">It should be noted that the use of particular terminology when describing certain features or aspects of the disclosure should not be taken to imply that the terminology is being re-defined herein to be restricted to include any specific characteristics of the features or aspects of the disclosure with which that terminology is associated. Terms and phrases used in this application, and variations thereof, especially in the appended claims, unless otherwise expressly stated, should be construed as open-ended as opposed to limiting. As examples of the foregoing, the term &#x201c;including&#x201d; should be read to mean &#x201c;including, without limitation,&#x201d; &#x201c;including but not limited to,&#x201d; or the like; the term &#x201c;comprising&#x201d; as used herein is synonymous with &#x201c;including,&#x201d; &#x201c;containing,&#x201d; or &#x201c;characterized by,&#x201d; and is inclusive or open-ended and does not exclude additional, unrecited elements or method steps; the term &#x201c;having&#x201d; should be interpreted as &#x201c;having at least;&#x201d; the term &#x201c;such as&#x201d; should be interpreted as &#x201c;such as, without limitation&#x201d;; the term &#x201c;includes&#x201d; should be interpreted as &#x201c;includes but is not limited to&#x201d;; the term &#x201c;example&#x201d; or the abbreviation &#x201c;e.g.&#x201d; is used to provide exemplary instances of the item in discussion, not an exhaustive or limiting list thereof, and should be interpreted as &#x201c;example, but without limitation&#x201d;; the term &#x201c;illustration&#x201d; is used to provide illustrative instances of the item in discussion, not an exhaustive or limiting list thereof, and should be interpreted as &#x201c;illustration, but without limitation&#x201d;; adjectives such as &#x201c;known,&#x201d; &#x201c;normal,&#x201d; &#x201c;standard,&#x201d; and terms of similar meaning should not be construed as limiting the item described to a given time period or to an item available as of a given time, but instead should be read to encompass known, normal, or standard technologies that may be available or known now or at any time in the future; and use of terms like &#x201c;preferably,&#x201d; &#x201c;preferred,&#x201d; &#x201c;desired,&#x201d; or &#x201c;desirable,&#x201d; and words of similar meaning should not be understood as implying that certain features are critical, essential, or even important to the structure or function of the present disclosure, but instead as merely intended to highlight alternative or additional features that may or may not be utilized in a particular embodiment. Likewise, a group of items linked with the conjunction &#x201c;and&#x201d; should not be read as requiring that each and every one of those items be present in the grouping, but rather should be read as &#x201c;and/or&#x201d; unless expressly stated otherwise. Similarly, a group of items linked with the conjunction &#x201c;or&#x201d; should not be read as requiring mutual exclusivity among that group, but rather should be read as &#x201c;and/or&#x201d; unless expressly stated otherwise. The terms &#x201c;about&#x201d; or &#x201c;approximate&#x201d; and the like are synonymous and are used to indicate that the value modified by the term has an understood range associated with it, where the range may be &#xb1;20%, &#xb1;15%, &#xb1;10%, &#xb1;5%, or &#xb1;1%. The term &#x201c;substantially&#x201d; is used to indicate that a result (e.g., measurement value) is close to a targeted value, where close may mean, for example, the result is within 80% of the value, within 90% of the value, within 95% of the value, or within 99% of the value. Also, as used herein &#x201c;defined&#x201d; or &#x201c;determined&#x201d; may include &#x201c;predefined&#x201d; or &#x201c;predetermined&#x201d; and/or otherwise determined values, conditions, thresholds, measurements, and the like.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A method for causing a succeeding robot to navigate a route, comprising:<claim-text>receiving a computer readable map, the computer readable map being produced based on data collected by at least one sensor of a preceding robot during navigation of the route by the preceding robot at a preceding instance in time; and</claim-text><claim-text>navigating, by the succeeding robot, the route at a succeeding instance in time based on the computer readable map, the succeeding instance in time being after the preceding instance in time.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>communicating by the preceding robot the computer readable map to a server upon completion of the route by the succeeding route, the server communicatively coupled to both the succeeding robot and the preceding robot.</claim-text></claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>synchronizing data with a server upon initializing the succeeding robot from an idle or off state, the synchronized data comprising at least the computer readable map of the route, the server being communicatively coupled to both the succeeding robot and the preceding robot.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein,<claim-text>the preceding robot navigates the route for an initial time in a training mode during the preceding instance in time; and</claim-text><claim-text>the succeeding robot navigates the route for the succeeding time by recreating the route executed by the preceding robot during the preceding instance in time.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein,<claim-text>the route begins and ends proximate to a landmark or feature identified by sensors of the succeeding and preceding robots.</claim-text></claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein,<claim-text>the computer readable map comprises a pose graph indicative of positions of the preceding robot during navigation of the route.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. A system, comprising:<claim-text>a non-transitory computer readable storage medium comprising computer readable instructions embodied thereon; and</claim-text><claim-text>at least one processor configured to execute the computer readable instructions to,<claim-text>receive a computer readable map, the computer readable map being generated based on data collected by at least one sensor of a preceding robot during navigation of a route by the preceding robot at a preceding instance in time; and</claim-text><claim-text>configure a succeeding robot to navigate the route at a succeeding instance in time based on the computer readable map, the succeeding instance in time being after the preceding instance in time.</claim-text></claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein,<claim-text>the preceding robot, upon completing the route, communicates the computer readable map to a server communicatively coupled to both the succeeding robot and the preceding robot.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the at least one processor is further configured to execute the computer readable instructions to,<claim-text>synchronize data with the succeeding robot upon the succeeding robot being initialized from an idle or off state, the synchronized data comprises at least the computer readable map of the route produced by the preceding robot.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein,<claim-text>the preceding robot navigates the route for an initial time in a training mode during the preceding instance in time; and</claim-text><claim-text>the succeeding robot navigates the route for the succeeding time by recreating</claim-text><claim-text>the route executed by the preceding robot during the preceding instance in time.</claim-text></claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the route begins and ends proximate to a landmark or feature identified by sensors of the succeeding and preceding robots.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The system of <claim-ref idref="CLM-00007">claim 7</claim-ref>, wherein the computer readable map comprises a pose graph indicative of positions of the preceding robot during navigation of the route.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. A non-transitory computer readable medium comprising computer readable instructions stored thereon that when executed by at least one processor configure the at least one processor to,<claim-text>receive a computer readable map, the computer readable map being produced based on data collected by at least one sensor of a preceding robot during navigation of the route by the preceding robot at a preceding instance in time; and</claim-text><claim-text>navigate, by the succeeding robot, the route at a succeeding instance in time based on the computer readable map, the succeeding instance in time being after the preceding instance in time,</claim-text><claim-text>wherein,<claim-text>the preceding robot navigates the route for an initial time in a training mode during the preceding instance in time, and</claim-text><claim-text>the succeeding robot navigates the route for the succeeding time by recreating the route executed by the preceding robot during the preceding instance in time.</claim-text></claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one processor is further configured to execute the computer readable instructions to,<claim-text>communicate by the preceding robot the computer readable map to a server upon completion of the route by the succeeding route, the server communicatively coupled to both the succeeding robot and the preceding robot.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The non-transitory computer readable medium of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the at least one processor is further configured to execute the computer readable instructions to,<claim-text>synchronize data with a server upon initializing the succeeding robot from an idle or off state, the synchronized data comprising at least the computer readable map of the route, the server being communicatively coupled to both the succeeding robot and the preceding robot.</claim-text></claim-text></claim></claims></us-patent-application>