<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004522A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004522</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17942816</doc-number><date>20220912</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>15</main-group><subgroup>80</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>38</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>15</main-group><subgroup>8015</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3887</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>3001</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>9</main-group><subgroup>30047</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>F</subclass><main-group>1</main-group><subgroup>32</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">COMPUTATIONAL MEMORY WITH COOPERATION AMONG ROWS OF PROCESSING ELEMENTS AND MEMORY THEREOF</invention-title><us-related-documents><division><relation><parent-doc><document-id><country>US</country><doc-number>17187082</doc-number><date>20210226</date></document-id><parent-grant-document><document-id><country>US</country><doc-number>11468002</doc-number></document-id></parent-grant-document></parent-doc><child-doc><document-id><country>US</country><doc-number>17942816</doc-number></document-id></child-doc></relation></division><us-provisional-application><document-id><country>US</country><doc-number>62983076</doc-number><date>20200228</date></document-id></us-provisional-application></us-related-documents><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UNTETHER AI CORPORATION</orgname><address><city>Toronto</city><country>CA</country></address></addressbook><residence><country>CA</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>SNELGROVE</last-name><first-name>William Martin</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>SCOBBIE</last-name><first-name>Jonathan</first-name><address><city>Toronto</city><country>CA</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">A computing device includes an array of processing elements mutually connected to perform single instruction multiple data (SIMD) operations, memory cells connected to each processing element to store data related to the SIMD operations, and a cache connected to each processing element to cache data related to the SIMD operations. Caches of adjacent processing elements are connected. The same or another computing device includes rows of mutually connected processing elements to share data. The computing device further includes a row arithmetic logic unit (ALU) at each row of processing elements. The row ALU of a respective row is configured to perform an operation with processing elements of the respective row.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="220.05mm" wi="158.75mm" file="US20230004522A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="236.56mm" wi="164.25mm" file="US20230004522A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="225.98mm" wi="172.30mm" orientation="landscape" file="US20230004522A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="229.11mm" wi="167.05mm" orientation="landscape" file="US20230004522A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="155.62mm" wi="89.15mm" file="US20230004522A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="195.66mm" wi="135.13mm" file="US20230004522A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="243.67mm" wi="149.94mm" file="US20230004522A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="214.97mm" wi="146.64mm" file="US20230004522A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="204.47mm" wi="143.51mm" file="US20230004522A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="236.90mm" wi="143.51mm" file="US20230004522A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="147.66mm" wi="154.69mm" file="US20230004522A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="225.98mm" wi="127.59mm" file="US20230004522A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="217.85mm" wi="175.94mm" file="US20230004522A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="230.63mm" wi="175.34mm" file="US20230004522A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00014" num="00014"><img id="EMI-D00014" he="233.76mm" wi="184.83mm" file="US20230004522A1-20230105-D00014.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00015" num="00015"><img id="EMI-D00015" he="151.21mm" wi="162.56mm" file="US20230004522A1-20230105-D00015.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00016" num="00016"><img id="EMI-D00016" he="219.12mm" wi="174.50mm" file="US20230004522A1-20230105-D00016.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00017" num="00017"><img id="EMI-D00017" he="237.41mm" wi="174.84mm" file="US20230004522A1-20230105-D00017.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00018" num="00018"><img id="EMI-D00018" he="229.62mm" wi="166.20mm" file="US20230004522A1-20230105-D00018.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00019" num="00019"><img id="EMI-D00019" he="198.04mm" wi="175.26mm" file="US20230004522A1-20230105-D00019.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="lead"?><heading id="h-0001" level="1">CROSS-REFERENCE TO RELATED APPLICATIONS</heading><p id="p-0002" num="0001">This application claims priority to U.S. provisional patent application Ser. No. 62/983,076 (filed Feb. 28, 2020), which is incorporated herein by reference.</p><?cross-reference-to-related-applications description="Cross Reference To Related Applications" end="tail"?><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0002" level="1">BACKGROUND</heading><p id="p-0003" num="0002">Deep learning has proven to be a powerful technique for performing functions that have long resisted other artificial intelligence approaches. For example, deep learning may be applied to recognition of objects in cluttered images, speech understanding and translation, medical diagnosis, gaming, and robotics. Deep learning techniques typically apply many layers (hence &#x201c;deep&#x201d;) of neural networks that are trained (hence &#x201c;learning&#x201d;) on the tasks of interest. Once trained, a neural network may perform &#x201c;inference&#x201d;, that is, inferring from new input data an output consistent with what it has learned.</p><p id="p-0004" num="0003">Neural networks, which may also be called neural nets, perform computations analogous to the operations of biological neurons, typically computing weighted sums (or dot products) and modifying the results with a memoryless nonlinearity. However, it is often the case that more general functionality, such as memory, multiplicative nonlinearities, and &#x201c;pooling&#x201d;, are also required.</p><p id="p-0005" num="0004">In many types of computer architecture, power consumption due to physically moving data between memory and processing elements is non-trivial and is frequently the dominant use of power. This power consumption is typically due to the energy required to charge and discharge the capacitance of wiring, which is roughly proportional to the length of the wiring and hence to distance between memory and processing elements. As such, processing a large number of computations in such architectures, as generally required for deep learning and neural networks, often requires a relatively large amount of power. In architectures that are better suited to handle deep learning and neural networks, other inefficiencies may arise, such as increased complexity, increased processing time, and larger chip area requirements.</p><heading id="h-0003" level="1">SUMMARY</heading><p id="p-0006" num="0005">According to one aspect of this disclosure, a computing device includes an array of processing elements mutually connected to perform single instruction multiple data (SIMD) operations, memory cells connected to each processing element to store data related to the SIMD operations, and a cache connected to each processing element to cache data related to the SIMD operations. A first cache of a first processing element is connected to a second cache of a second processing element that is adjacent the first processing element in the array of processing elements.</p><p id="p-0007" num="0006">According to another aspect of this disclosure, a computing device includes a plurality of rows of processing elements to perform SIMD operations. The processing elements of each row are mutually connected to share data. The computing device further includes a row arithmetic logic unit (ALU) at each row of the plurality of rows of processing elements. The row ALU of a respective row is configured to perform an operation with processing elements of the respective row.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a block diagram of an example computing device that includes banks of processing elements.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a block diagram of an example array of processing elements.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a block diagram of an example array of processing elements with a controller.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a block diagram of an example array of processing elements with a controller and memory.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>5</b></figref> is a schematic diagram of example processing elements and related memory cells.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is an equation for an example matrix multiplication carried out by the processing elements and memory cells of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>7</b>A</figref> is a schematic diagram of an example state sequence of the processing elements and memory cells of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> is a schematic diagram of an example state sequence of the processing elements and memory cells of <figref idref="DRAWINGS">FIG. <b>5</b></figref>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>7</b>C</figref> is a schematic diagram of an example generalized solution to movement of input vector components among a set of processing elements.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a flowchart of an example method of performing operations using processing elements and memory cells.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a block diagram of an example processing element and related memory cells.</p><p id="p-0019" num="0018"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a block diagram of an example of the neighbor processing element interconnect control of <figref idref="DRAWINGS">FIG. <b>9</b></figref>.</p><p id="p-0020" num="0019"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a block diagram of an example processing element and associated memory arrangement.</p><p id="p-0021" num="0020"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a block diagram of an example two-dimensional array of processing banks connected to an interface.</p><p id="p-0022" num="0021"><figref idref="DRAWINGS">FIG. <b>13</b></figref> is a block diagram of an example processing element with zero detection and disabling functionality.</p><p id="p-0023" num="0022"><figref idref="DRAWINGS">FIG. <b>14</b></figref> is a block diagram of memory cells associated with a processing element, where the memory cells are configured in blocks and related caches.</p><p id="p-0024" num="0023"><figref idref="DRAWINGS">FIG. <b>15</b></figref> is a block diagram of an arrangement of memory cells and associated processing elements of <figref idref="DRAWINGS">FIG. <b>14</b></figref> connected by memory-sharing switches.</p><p id="p-0025" num="0024"><figref idref="DRAWINGS">FIG. <b>16</b></figref> is a block diagram of an example zero disable circuit.</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>17</b></figref> is a block diagram of a cache arrangement to facilitate communications among memory allocated to different processing elements.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram of a computing device including an array of processing elements with cache and connections there-between.</p><p id="p-0028" num="0027"><figref idref="DRAWINGS">FIG. <b>19</b></figref> is a block diagram of a bank of rows of processing devices to provide row-based and bank-based computations.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0029" num="0028">The techniques described herein aim to improve computational memory to handle large numbers of dot-product and neural-network computations with flexible low-precision arithmetic, provide power-efficient communications, and provide local storage and decoding of instructions and coefficients. The parallel processing described herein is suitable for neural networks, particularly where power consumption is a concern, such as in battery-powered devices, portable computers, smartphones, wearable computers, smart watches, and the like.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>1</b></figref> shows a computing device <b>100</b>. The computing device <b>100</b> includes a plurality of banks <b>102</b> of processing elements. The banks <b>102</b> may be operated in a cooperative manner to implement a parallel processing scheme, such as a single instruction, multiple data (SIMD) scheme.</p><p id="p-0031" num="0030">The banks <b>102</b> may be arranged in a regular rectangular grid-like pattern, as illustrated. For sake of explanation, relative directions mentioned herein will be referred to as up, down, vertical, left, right, horizontal, and so on. However, it is understood that such directions are approximations, are not based on any particular reference direction, and are not to be considered limiting.</p><p id="p-0032" num="0031">Any practical number of banks <b>102</b> may be used. Limitations in semiconductor fabrication techniques may govern. In some examples, 512 banks <b>102</b> are arranged in a 32-by-16 grid.</p><p id="p-0033" num="0032">A bank <b>102</b> may include a plurality of rows <b>104</b> of processing elements (PEs) <b>108</b> and a controller <b>106</b>. A bank <b>102</b> may include any practical number of PE rows <b>104</b>. For example, eight rows <b>104</b> may be provided for each controller <b>106</b>. In some examples, all banks <b>102</b> may be provided with the same or similar arrangement of rows. In other examples, substantially all banks <b>102</b> are substantially identical. In still other examples, a bank <b>102</b> may be assigned a special purpose in the computing device and may have a different architecture, which may omit PE rows <b>104</b> and/or a controller <b>106</b>.</p><p id="p-0034" num="0033">Any practical number of PEs <b>108</b> may be provided to a row <b>104</b>. For example, 256 PEs may be provided to each row <b>104</b>. Continuing the numerical example above, 256 PEs provided to each of eight rows <b>104</b> of 512 banks <b>102</b> means the computing device <b>100</b> includes about 1.05 million PEs <b>108</b>, less any losses due to imperfect semiconductor manufacturing yield.</p><p id="p-0035" num="0034">A PE <b>108</b> may be configured to operate at any practical bit size, such as one, two, four, or eight bits. PEs may be operated in pairs to accommodate operations requiring wider bit sizes.</p><p id="p-0036" num="0035">Instructions and/or data may be communicated to/from the banks <b>102</b> via an input/output (I/O) bus <b>110</b>. The I/O bus <b>110</b> may include a plurality of segments.</p><p id="p-0037" num="0036">A bank <b>102</b> may be connected to the I/O bus <b>110</b> by a vertical bus <b>112</b>. Additionally or alternatively, a vertical bus <b>112</b> may allow communication among banks <b>102</b> in a vertical direction. Such communication may be restricted to immediately vertically adjacent banks <b>102</b> or may extend to further banks <b>102</b>.</p><p id="p-0038" num="0037">A bank <b>102</b> may be connected to a horizontally neighboring bank <b>102</b> by a horizontal bus <b>114</b> to allow communication among banks <b>102</b> in a horizontal direction. Such communication may be restricted to immediately horizontally adjacent banks <b>102</b> or may extend to further banks <b>102</b>.</p><p id="p-0039" num="0038">Communications through any or all of the busses <b>110</b>, <b>112</b>, <b>114</b> may include direct memory access (DMA) to memory of the rows <b>104</b> of the PEs <b>108</b>. Additionally or alternatively, such communications may include memory access performed through the processing functionality of the PEs <b>108</b>.</p><p id="p-0040" num="0039">The computing device <b>100</b> may include a main processor (not shown) to communicate instructions and/or data with the banks <b>102</b> via the I/O bus <b>110</b>, manage operations of the banks <b>102</b>, and/or provide an I/O interface for a user, network, or other device. The I/O bus <b>110</b> may include a Peripheral Component Interconnect Express (PCIe) interface or similar.</p><p id="p-0041" num="0040"><figref idref="DRAWINGS">FIG. <b>2</b></figref> shows an example row <b>104</b> including an array of processing elements <b>108</b>, which may be physically arranged in a linear pattern (e.g., a physical row). Each PE <b>108</b> includes an arithmetic logic unit (ALU) to perform an operation, such as addition, multiplication, and so on.</p><p id="p-0042" num="0041">The PEs <b>108</b> are mutually connected to share or communicate data. For example, interconnections <b>200</b> may be provided among the array of PEs <b>108</b> to provide direct communication among neighboring PEs <b>108</b>.</p><p id="p-0043" num="0042">A PE <b>108</b> (e.g., indicated at &#x201c;n&#x201d;) is connected to a first neighbor PE <b>108</b> (i.e., n+1) that is immediately adjacent the PE <b>108</b>. Likewise, the PE <b>108</b> (n) is further connected to a second neighbor PE <b>108</b> (n+2) that is immediately adjacent the first neighbor PE <b>108</b> (n+1). A plurality of PEs <b>108</b> may be connected to neighboring processing elements in the same relative manner, where n merely indicates an example PE <b>108</b> for explanatory purposes. That is, the first neighbor PE <b>108</b> (n+1) may be connected to its respective first and second neighbors (n+2 and n+3).</p><p id="p-0044" num="0043">A given PE <b>108</b> (e.g., n+5) may also be connected to an opposite first neighbor PE <b>108</b> (n+4) that is immediately adjacent the PE <b>108</b> (n+5) on a side opposite the first neighbor PE <b>108</b> (n+6). Similarly, the PE <b>108</b> (n+5) may further be connected to an opposite second neighbor PE <b>108</b> (n+3) that is immediately adjacent the opposite first neighbor PE <b>108</b> (n+4).</p><p id="p-0045" num="0044">Further, a PE <b>108</b> may be connected to a fourth neighbor PE <b>108</b> that is immediately adjacent a third neighbor PE <b>108</b> that is immediately adjacent the second neighbor PE <b>108</b>. For example, the PE <b>108</b> designated at n may be connected to the PE designated at n+4. A connection of the PE <b>108</b> (n) to its third neighbor PE <b>108</b> (n+3) may be omitted. The fourth-neighbor connection may also be provided in the opposite direction, so that the PE <b>108</b> (n) connects to its fourth neighbor PE <b>108</b> at n&#x2212;4 (not shown).</p><p id="p-0046" num="0045">Still further, a PE <b>108</b> may be connected to a sixth neighbor PE <b>108</b> that is immediately adjacent a fifth neighbor PE <b>108</b> that is immediately adjacent the fourth neighbor PE <b>108</b>. For example, the PE <b>108</b> designated at n may be connected to the PE designated at n+6. A connection of the PE <b>108</b> (n) to its fifth neighbor PE <b>108</b> (n+5) may be omitted. The sixth-neighbor connection may also be provided in the opposite direction, so that the PE <b>108</b> (n) connects to its sixth neighbor PE <b>108</b> at n&#x2212;6 (not shown).</p><p id="p-0047" num="0046">Again, a plurality of PEs <b>108</b> may be connected to neighboring processing elements in the above relative manner. The designation of a PE <b>108</b> as n may be considered arbitrary for non-endmost PEs <b>108</b>. PEs <b>108</b> at the ends of the array may omit certain connections by virtue of the array terminating. In the example of each PE <b>108</b> being connected to its first, second, fourth, and sixth neighbor PEs <b>108</b> in both directions, the six endmost PEs <b>108</b> have differing connections.</p><p id="p-0048" num="0047">With reference to <figref idref="DRAWINGS">FIG. <b>3</b></figref>, endmost PEs <b>108</b> at one end of a row <b>104</b> may have connections <b>300</b> to a controller <b>106</b>. Further, endmost PEs <b>108</b> at the opposite end of the row <b>104</b> may have a reduced number of connections <b>302</b>. Additionally or alternatively, end-most PEs <b>108</b> of one bank <b>102</b> may connect in the same relative manner through the controller <b>106</b> and to PEs <b>108</b> of an adjacent bank <b>102</b>. That is, the controller <b>106</b> may be connected between two rows <b>104</b> of PEs <b>108</b> in adjacent banks <b>102</b>, where the two rows <b>104</b> of PEs <b>108</b> are connected in the same manner as shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref></p><p id="p-0049" num="0048">With reference to <figref idref="DRAWINGS">FIG. <b>4</b></figref>, a row <b>104</b> of PEs <b>108</b> may include memory <b>400</b> to store data for the row <b>104</b>. A PE <b>108</b> may have a dedicated space in the memory <b>400</b>. For example, each PE <b>108</b> may be connected to a different range of memory cells <b>402</b>. Any practical number of memory cells <b>402</b> may be used. In one example, <b>144</b> memory cells <b>402</b> are provided to each PE <b>108</b>. Note that in <figref idref="DRAWINGS">FIG. <b>4</b></figref> the interconnections <b>200</b> among the PEs <b>108</b> and with the controller <b>106</b> are shown schematically for sake of explanation.</p><p id="p-0050" num="0049">The controller <b>106</b> may control the array of PEs <b>108</b> to perform a SIMD operation with data in the memory <b>400</b>. For example, the controller <b>106</b> may trigger the PEs <b>108</b> to simultaneously add two numbers stored in respective cells <b>402</b>.</p><p id="p-0051" num="0050">The controller <b>106</b> may communicate data to and from the memory <b>400</b> though the PEs <b>108</b>. For example, the controller <b>106</b> may load data into the memory <b>400</b> by directly loading data into connected PEs <b>108</b> and controlling PEs <b>108</b> to shift the data to PEs <b>108</b> further in the array. PEs <b>108</b> may load such data into their respective memory cells <b>402</b>. For example, data destined for rightmost PEs <b>108</b> may first be loaded into leftmost PEs and then communicated rightwards by interconnections <b>200</b> before being stored in rightmost memory cells <b>402</b>. Other methods of I/O with the memory, such as direct memory access by the controller <b>106</b>, are also contemplated. The memory cells <b>402</b> of different PEs <b>108</b> may have the same addresses, so that address decoding may be avoided to the extent possible.</p><p id="p-0052" num="0051">Data stored in memory cells <b>402</b> may be any suitable data, such as operands, operators, coefficients, vector components, mask data, selection data, and similar. Mask data may be used to select portions of a vector. Selection data may be used to make/break connections among neighboring PEs <b>108</b>.</p><p id="p-0053" num="0052">Further, the controller <b>106</b> may perform a rearrangement of data within the array of PEs <b>108</b> by controlling communication of data through the interconnections <b>200</b> among the array of PEs <b>108</b>. A rearrangement of data may include a rotation or cycling that reduces or minimizes a number of memory accesses while increasing or maximizing operational throughput. Other examples of rearrangements of data include reversing, interleaving, and duplicating.</p><p id="p-0054" num="0053">In other examples, a set of interconnections <b>200</b> may be provided to connect PEs <b>108</b> in up-down (column-based) connections, so that information may be shared directly between PEs <b>108</b> that are in adjacent rows. In this description, interconnections <b>200</b> and related components that are discussed with regard to left-right (row-based) connections among PEs apply in principle to up-down (column-based) connections among PEs.</p><p id="p-0055" num="0054"><figref idref="DRAWINGS">FIG. <b>5</b></figref> shows an array of PEs <b>108</b> and related memory cells <b>402</b>. Each PE <b>108</b> may include local registers <b>500</b>, <b>502</b> to hold data undergoing an operation. Memory cells <b>402</b> may also hold data contributing to the operation. For example, the PEs <b>108</b> may carry out a matrix multiplication, as shown in <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0056" num="0055">A matrix multiplication may be a generalized matrix-vector multiply (GEMV). A matrix multiplication may use a coefficient matrix and an input vector to obtain a resultant vector. In this example, the coefficient matrix is a four-by-four matrix and the vectors are of length four. In other examples, matrices and vectors of any practical size may be used. In other examples, a matrix multiplication may be a generalized matrix-matrix multiply (GEMM).</p><p id="p-0057" num="0056">As matrix multiplication involves sums of products, the PEs <b>108</b> may additively accumulate resultant vector components d<sub>0 </sub>to d<sub>3 </sub>in respective registers <b>500</b>, while input vector components a<sub>0 </sub>to a<sub>3 </sub>are multiplied by respective coefficients c<sub>00 </sub>to c<sub>33</sub>. That is, one PE <b>108</b> may accumulate a resultant vector component d<sub>0</sub>, a neighbor PE <b>108</b> may accumulate another resultant vector component d<sub>1</sub>, and so on. Resultant vector components d<sub>0 </sub>to d<sub>3 </sub>may be considered dot products. Generally, a GEMV may be considered a collection of dot products of a vector with a set of vectors represented by the rows of a matrix.</p><p id="p-0058" num="0057">To facilitate matrix multiplication, the contents of registers <b>500</b> and/or registers <b>502</b> may be rearranged among the PEs <b>108</b>. A rearrangement of resultant vector components d<sub>0 </sub>to d<sub>3 </sub>and/or input vector components a<sub>0 </sub>to a<sub>3 </sub>may use the direct interconnections among neighbor PEs <b>108</b>, as discussed above. In this example, resultant vector components d<sub>0 </sub>to d<sub>3 </sub>remain fixed and input vector components a<sub>0 </sub>to a<sub>3 </sub>are moved. Further, coefficients c<sub>00 </sub>to c<sub>33 </sub>may be loaded into memory cells to optimize memory accesses.</p><p id="p-0059" num="0058">In the example illustrated in <figref idref="DRAWINGS">FIG. <b>5</b></figref>, the input vector components a<sub>0 </sub>to a<sub>3 </sub>are loaded into a sequence of PEs <b>108</b> that are to accumulate resultant vector components d<sub>0 </sub>to d<sub>3 </sub>in the same sequence. The relevant coefficients c<sub>00</sub>, c<sub>11</sub>, c<sub>22</sub>, c<sub>33 </sub>are accessed and multiplied by the respective input vector components a<sub>0 </sub>to a<sub>3</sub>. That is, a<sub>0 </sub>and c<sub>00 </sub>are multiplied and then accumulated as d<sub>0</sub>, a<sub>1 </sub>and c<sub>11 </sub>are multiplied and then accumulated as d<sub>1</sub>, and so on.</p><p id="p-0060" num="0059">The input vector components a<sub>0 </sub>to a<sub>3 </sub>are then rearranged, as shown in the PE state sequence of <figref idref="DRAWINGS">FIG. <b>7</b>A</figref>, so that a remaining contribution of each input vector components a<sub>0 </sub>to a<sub>3 </sub>to a respective resultant vector components d<sub>0 </sub>to d<sub>3 </sub>may be accumulated. In this example, input vector components a<sub>0 </sub>to a<sub>2 </sub>are moved one PE <b>108</b> to the right and input vector components a<sub>3 </sub>is moved three PEs <b>108</b> to the left. With reference to the first and second neighbor connections shown in <figref idref="DRAWINGS">FIG. <b>2</b></figref>, this rearrangement of input vector components a<sub>0 </sub>to a<sub>3 </sub>may be accomplished by swapping a<sub>0 </sub>with a<sub>1 </sub>and simultaneously swapping a<sub>2 </sub>with a<sub>3</sub>, using first neighbor connections, and then by swapping a<sub>1 </sub>with a<sub>3 </sub>using second neighbor connections. The result is that a next arrangement of input vector components a<sub>3</sub>, a<sub>0</sub>, a<sub>1</sub>, a<sub>2 </sub>at the PEs <b>108</b> is achieved, where each input vector component is located at a PE <b>108</b> that it has not yet occupied during the present matrix multiplication.</p><p id="p-0061" num="0060">Appropriate coefficients c<sub>03</sub>, c<sub>10</sub>, c<sub>21</sub>, c<sub>32 </sub>in memory cells <b>402</b> are then accessed and multiplied by the respective input vector components a<sub>3</sub>, a<sub>0</sub>, a<sub>1</sub>, a<sub>2</sub>. That is, a<sub>3 </sub>and c<sub>03 </sub>are multiplied and then accumulated as d<sub>0</sub>, a<sub>0 </sub>and c<sub>10 </sub>are multiplied and then accumulated as d<sub>1</sub>, and so on.</p><p id="p-0062" num="0061">The input vector components a<sub>0 </sub>to a<sub>3 </sub>are then rearranged twice more, with multiplying accumulation being performed with the input vector components and appropriate coefficients at each new arrangement. At the conclusion of four sets of multiplying accumulation and three intervening rearrangements, the accumulated resultant vector components d<sub>0 </sub>to d<sub>3 </sub>represent the final result of the matrix multiplication.</p><p id="p-0063" num="0062">Rearrangement of the input vector components a<sub>0 </sub>to a<sub>3 </sub>allows each input vector component to be used to the extent needed when it is located at a particular PE <b>108</b>. This is different from traditional matrix multiplication where each resultant vector component is computed to finality prior to moving to the next. The present technique simultaneously accumulates all resultant vector components using sequenced arrangements of input vector components.</p><p id="p-0064" num="0063">Further, such rearrangements of data at the PEs <b>108</b> using the PE neighbor interconnections (<figref idref="DRAWINGS">FIG. <b>2</b></figref>) may be optimized to reduce or minimize processing cost. The example given above of two simultaneous first neighbor swaps followed by a second neighbor swap is merely one example. Additional examples are contemplated for matrices and vectors of various dimensions.</p><p id="p-0065" num="0064">Further, the arrangements of coefficients c<sub>00 </sub>to c<sub>33 </sub>in the memory cells <b>402</b> may be predetermined, so that each PE <b>108</b> may access the next coefficient needed without requiring coefficients to be moved among memory cells <b>402</b>. The coefficients c<sub>00 </sub>to c<sub>33 </sub>may be arranged in the memory cells <b>402</b> in a diagonalized manner, such that a first row of coefficients is used for a first arrangement of input vector components, a second row of coefficients is used for a second arrangement of input vector components, and so on. Hence, the respective memory addresses referenced by the PEs <b>108</b> after a rearrangement of input vector components may be incremented or decremented identically. For example, with a first arrangement of input vector components, each PE <b>108</b> may reference its respective memory cell at address <b>0</b> for the appropriate coefficient. Likewise, with a second arrangement of input vector components, each PE <b>108</b> may reference its respective memory cell at address <b>1</b> for the appropriate coefficient, and so on.</p><p id="p-0066" num="0065"><figref idref="DRAWINGS">FIG. <b>7</b>B</figref> shows another example sequence. Four states of a set of PEs <b>108</b> are shown with four sets of selected coefficients. Input vector components a<sub>0 </sub>to a<sub>3 </sub>are rotated so that each component a<sub>0 </sub>to a<sub>3 </sub>is used exactly once to contribute to the accumulation at each resultant vector component d<sub>0 </sub>to d<sub>3</sub>. The coefficients c<sub>00 </sub>to c<sub>33 </sub>are arranged so that the appropriate coefficient c<sub>00 </sub>to c<sub>33 </sub>is selected for each combination of input vector component a<sub>0 </sub>to a<sub>3 </sub>and resultant vector component d<sub>0 </sub>to d<sub>3</sub>. In this example, the input vector components a<sub>0 </sub>to a<sub>3 </sub>are subject to the same rearrangement three times to complete a full rotation. Specifically, the input vector component of an n<sup>th </sup>PE <b>108</b> is moved right to the second neighbor PE <b>108</b> (i.e., n+2), the input vector component of the PE <b>108</b> n+1 is moved left (opposite) to its first neighbor PE <b>108</b> (i.e., n) in that direction, the input vector component of the PE <b>108</b> n+2 is moved right to the first neighbor PE <b>108</b> (i.e., n+3), and the input vector component of the PE <b>108</b> n+3 is moved left to the second neighbor PE <b>108</b> (i.e., n+1).</p><p id="p-0067" num="0066"><figref idref="DRAWINGS">FIG. <b>7</b>C</figref> shows a generalized solution, which is implicit from the examples discussed herein, to movement of input vector components among a set of PEs <b>108</b>. As shown by the row-like arrangement <b>700</b> of input vector components a<sub>0 </sub>to a<sub>i</sub>, which may be held by a row <b>104</b> of PEs <b>108</b>, rotating information may require many short paths <b>702</b>, between adjacent components a<sub>0 </sub>to a<sub>i</sub>, and a long path <b>704</b> between end-most components a<sub>i </sub>and a<sub>0</sub>. The short paths are not a concern. However, the long path <b>704</b> may increase latency and consume additional electrical power because charging and charging a conductive trace takes time and is not lossless. The longer the trace, the greater the time/loss. The efficiency of a row <b>104</b> of PEs <b>108</b> is limited by its long path <b>704</b>, in that power is lost and other PEs <b>108</b> may need to wait while data is communicated over the long path <b>704</b>.</p><p id="p-0068" num="0067">As shown at <b>710</b>, a circular arrangement of PEs <b>108</b> may avoid a long path <b>704</b>. All paths <b>712</b> may be segments of a circle and may be made the same length. A circular arrangement <b>710</b> of PEs <b>108</b> may be considered an ideal case. However, a circular arrangement <b>710</b> is impractical for manufacturing purposes.</p><p id="p-0069" num="0068">Accordingly, the circular arrangement <b>720</b> may be rotated slightly and flattened (or squashed), while preserving the connections afforded by circular segment paths <b>712</b> and the relative horizontal (X) positions of the PEs, to provide for an efficient arrangement <b>720</b>, in which paths <b>722</b>, <b>724</b> connect adjacent PEs or skip one intermediate PE. As such, PEs <b>108</b> may be connected by a set of first-neighbor paths <b>722</b> (e.g., two end-arriving paths) and a set of second neighbor paths <b>724</b> (e.g., four intermediate and two end-leaving paths) that are analogous to circular segment paths <b>712</b> of a circular arrangement <b>710</b>. The paths <b>722</b>, <b>724</b> have much lower variance than the short and long paths <b>702</b>, <b>704</b>, so power may be saved and latency reduced. Hence, the arrangement <b>720</b> represents a readily manufacturable implementation of an ideal circular arrangement of PEs <b>108</b>.</p><p id="p-0070" num="0069"><figref idref="DRAWINGS">FIG. <b>8</b></figref> shows a method <b>900</b> that generalizes the above example. The method <b>900</b> may be performed with the computing device <b>100</b> or a similar device. The method may be implemented by instructions executable by the device <b>100</b> or a controller <b>106</b> thereof. The instructions may be stored in a non-transitory computer-readable medium integral to the device <b>100</b> or controller <b>106</b>.</p><p id="p-0071" num="0070">At block <b>902</b>, operands (e.g., matrix coefficients) are loaded into PE memory cells. The arrangement of operands may be predetermined with the constraint that moving operands is to be avoided where practical. An operand may be duplicated at several cells to avoid moving an operand between such cells.</p><p id="p-0072" num="0071">At block <b>904</b>, operands (e.g., input vector components) are loaded into PE registers. The operands to be loaded into PE registers may be distinguished from the operands to be loaded into PE memory cells, in that there may be fewer PE registers than PE memory cells. Hence, in the example of a matrix multiplication, it may be more efficient to load the smaller matrix/vector to the into PE registers and load the larger matrix into the PE memory cells. In other applications, other preferences may apply.</p><p id="p-0073" num="0072">At block <b>906</b>, a set of memory cells may be selected for use in an operation. The set may be a row of memory cells. For example, a subset of coefficients of a matrix to be multiplied may be selected, one coefficient per PE.</p><p id="p-0074" num="0073">At block <b>908</b>, the same operation is performed by the PEs on the contents of the selected memory cells and respective PE registers. The operation may be performed substantially simultaneously with all relevant PEs. All relevant PEs may be all PEs of a device or a subset of PEs assigned to perform the operation. An example operation is a multiplication (e.g., multiplying PE register content with memory cell content) and accumulation (e.g., accumulating the resulting product with a running total from a previous operation).</p><p id="p-0075" num="0074">Then, if a subsequent operation is to be performed, via block <b>910</b>, operands in the PE registers may be rearranged, at block <b>912</b>, to obtain a next arrangement. A next set of memory cells is then selected at block <b>906</b>, and a next operation is performed at block <b>908</b>. For example, a sequence of memory cells may be selected during each cycle and operands in the PE registers may be rearranged to correspond to the sequence of memory cells, so as to perform a matrix multiplication. In other examples, other operations may be performed.</p><p id="p-0076" num="0075">Hence, a sequence or cycle or operations may be performed on the content of selected memory cells using the content of PE registers that may be rearranged as needed. The method <b>900</b> ends after the last operation, via block <b>910</b>.</p><p id="p-0077" num="0076">The method <b>900</b> may be varied. In various examples, selection of the memory cells need not be made by selection of a contiguous row. Arranging data in the memory cells according to rows may simplify the selection process. For example, a single PE-relative memory address may be referenced (e.g., all PEs refer to their local memory cell with the same given address). That said, it is not strictly necessary to arrange the data in rows. In addition or alternatively, a new set of memory cells need not be selected for each operation. The same set may be used in two or more consecutive cycles. Further, overlapping sets may be used, in that a memory cell used in a former operation may be deselected and a previously unselected memory cell may be selected for a next operation, while another memory cell may remain selected for both operations. In addition or alternatively, the operands in the PE registers need not be rearranged each cycle. Operands may remain in the same arrangement for two or more consecutive cycles. Further, operand rearrangement does not require each operand to change location, in that a given operand may be moved while another operand may remain in place.</p><p id="p-0078" num="0077"><figref idref="DRAWINGS">FIG. <b>9</b></figref> shows an example PE <b>108</b> schematically. The PE <b>108</b> includes an ALU <b>1000</b>, registers <b>1002</b>, a memory interface <b>1004</b>, and neighbor PE interconnect control <b>1006</b>.</p><p id="p-0079" num="0078">The ALU <b>1000</b> performs the operational function of the PE. The ALU <b>1000</b> may include an adder, multiplier, accumulator, or similar. In various examples, the ALU <b>1000</b> is a multiplying accumulator. The ALU <b>1000</b> may be connected to the memory interface <b>1004</b>, directly or indirectly, through the registers <b>1002</b> to share information with the memory cells <b>402</b>. In this example, the ALU <b>1000</b> is connected to the memory interface <b>1004</b> though the registers <b>1002</b> and a bus interface <b>1008</b>.</p><p id="p-0080" num="0079">The registers <b>1002</b> are connected to the ALU <b>1000</b> and store data used by the PE <b>108</b>. The registers <b>1002</b> may store operands, results, or other data related to operation of the ALU <b>1000</b>, where such data may be obtained from or provided to the memory cells <b>402</b> or other PEs <b>108</b> via the neighbor PE interconnect control <b>1006</b>.</p><p id="p-0081" num="0080">The memory interface <b>1004</b> is connected to the memory cells <b>402</b> and allows for reading/writing at the memory cells <b>402</b> to communicate data with the registers <b>1002</b>, ALU <b>1000</b>, and/or other components of the PE <b>108</b>.</p><p id="p-0082" num="0081">The neighbor PE interconnect control <b>1006</b> connects to the registers <b>1002</b> and controls communication of data between the registers <b>1002</b> and like registers of neighboring PEs <b>108</b>, for example via interconnections <b>200</b> (<figref idref="DRAWINGS">FIG. <b>2</b></figref>), and/or between a controller (see <b>106</b> in <figref idref="DRAWINGS">FIG. <b>3</b></figref>). The neighbor PE interconnect control <b>1006</b> may include a logic/switch array to selectively communicate the registers <b>1002</b> to the registers <b>1002</b> of neighboring PEs <b>108</b>, such as first, second, fourth, or sixth neighbor PEs. The neighbor PE interconnect control <b>1006</b> may designate a single neighbor PE <b>108</b> from which to obtain data. That is, the interconnections <b>200</b> may be restricted so that a PE <b>108</b> only at most listens to one selected neighbor PE <b>108</b>. The neighbor PE interconnect control <b>1006</b> may connect PEs <b>108</b> that neighbor each other in the same row. Additionally or alternatively, a neighbor PE interconnect control <b>1006</b> may be provided to connect PEs <b>108</b> that neighbor each other in the same column.</p><p id="p-0083" num="0082">The PE may further include a bus interface <b>1008</b> to connect the PE <b>108</b> to a bus <b>1010</b>, such as a direct memory access bus. The bus interface <b>1008</b> may be positioned between the memory interface <b>1004</b> and registers <b>1002</b> and may selectively communicate data between the memory interface <b>1004</b> and either a component outside the PE <b>108</b> connected to the bus <b>1010</b> (e.g., a main processor via direct memory access) or the registers <b>1002</b>. The bus interface <b>1008</b> may control whether the memory <b>402</b> is connected to the registers <b>1002</b> or the bus <b>1010</b>.</p><p id="p-0084" num="0083">The PE may further include a shifter circuit <b>1012</b> connected to the ALU <b>1000</b> and a wide-add bus <b>1014</b> to perform shifts to facilitate performing operations in conjunction with one or more neighbor PEs <b>108</b>.</p><p id="p-0085" num="0084"><figref idref="DRAWINGS">FIG. <b>10</b></figref> shows an example of the neighbor PE interconnect control <b>1006</b>. The neighbor PE interconnect control <b>1006</b> includes a multiplexer <b>1100</b> or similar switch/logic array and a listen register <b>1102</b>.</p><p id="p-0086" num="0085">The multiplexer <b>1100</b> selectively communicates one interconnection <b>200</b> to a neighbor PE <b>108</b> to a register <b>1002</b> used for operations of the PE <b>108</b> to which the neighbor PE interconnect control <b>1006</b> belongs. Hence, a PE <b>108</b> listens to one neighbor PE <b>108</b>.</p><p id="p-0087" num="0086">The listen register <b>1102</b> controls the output of the multiplexer <b>1100</b>, that is, the listen register <b>1102</b> selects a neighbor PE <b>108</b> as source of input to the PE <b>108</b>. The listen register <b>1102</b> may be set by an external component, such as a controller <b>106</b> (<figref idref="DRAWINGS">FIG. <b>3</b></figref>), or by the PE <b>108</b> itself.</p><p id="p-0088" num="0087"><figref idref="DRAWINGS">FIG. <b>11</b></figref> shows a diagram of an example PE <b>1500</b> and its associated memory <b>1502</b>. The memory <b>1502</b> may be arranged into blocks, so that the PE <b>1500</b> may access one block at the same time that an external process, such as direct memory access, accesses another block. Such simultaneous access may allow for faster overall performance of a row, bank, or other device containing the PE, as the PE and external process can perform operations with different blocks of memory at the same time and there will be fewer occasions of the PE or external process having to wait for the other to complete its memory access. In general, PE access to memory is faster than outside access, so it is expected that the PE <b>1500</b> will be able to perform N memory operations to one block per one outside operation to the other block.</p><p id="p-0089" num="0088">The memory <b>1502</b> includes two blocks <b>1504</b>, <b>1506</b>, each containing an array of memory cells <b>1508</b>. Each block <b>1504</b>, <b>1506</b> may also include a local I/O circuit <b>1510</b> to handle reads/writes to the cells of the block <b>1504</b>, <b>1506</b>. In other examples, more than two blocks may be used.</p><p id="p-0090" num="0089">The memory <b>1502</b> further includes a global I/O circuit <b>1512</b> to coordinate access by the PE and external process to the blocks <b>1504</b>, <b>1506</b>.</p><p id="p-0091" num="0090">The PE <b>1500</b> may include memory access circuits <b>1520</b>-<b>1526</b>, such as a most-significant nibble (MSN) read circuit <b>1520</b>, a least-significant nibble (LSN) read circuit <b>1522</b>, an MSN write circuit <b>1524</b>, and an LSN write circuit <b>1526</b>. The memory access circuits <b>1520</b>-<b>1526</b> are connected to the global I/O circuit <b>1512</b> of the memory <b>1502</b>.</p><p id="p-0092" num="0091">The memory address schema of the blocks <b>1504</b>, <b>1506</b> of memory <b>1502</b> may be configured to reduce latency. In this example, block <b>1504</b> contains cells <b>1508</b> with even addresses and the block <b>1506</b> contains cells <b>1508</b> with odd addresses. As such, when the PE <b>1500</b> is to write to a series of addresses, the global I/O circuit <b>1512</b> connects the PE <b>1500</b> in an alternating fashion to the blocks <b>1504</b>, <b>1506</b>. That is, the PE <b>1500</b> switches between accessing the blocks <b>1504</b>, <b>1506</b> for a sequence of memory addresses. This reduces the chance that the PE <b>1500</b> will have to wait for a typically slower external memory access. Timing between block access can overlap. For example, one block can still be finishing latching data into an external buffer while the other block is concurrently providing data to the PE <b>1500</b>.</p><p id="p-0093" num="0092"><figref idref="DRAWINGS">FIG. <b>12</b></figref> shows an example two-dimensional array <b>1600</b> of processing banks <b>102</b> connected to an interface <b>1602</b> via I/O busses <b>1604</b>. The array <b>1600</b> may be grid-like with rows and columns of banks <b>102</b>. Rows need not have the same number of banks <b>102</b>, and columns need not have the same number of banks <b>102</b>.</p><p id="p-0094" num="0093">The interface <b>1602</b> may connect the I/O busses <b>1604</b> to a main processor, such as a CPU of a device that contains the array <b>1600</b>. The interface <b>1602</b> may be a PCIe interface.</p><p id="p-0095" num="0094">The interface <b>1602</b> and buses <b>1604</b> may be configured to communicate data messages <b>1606</b> with the banks <b>102</b>. The interface <b>1602</b> may pump messages through the busses <b>1604</b> with messages becoming accessible to banks <b>102</b> via bus connections <b>1608</b>. A bank <b>102</b> may read/write data from/to a message <b>1606</b> via a bus connection <b>1608</b>.</p><p id="p-0096" num="0095">Each bus <b>1604</b> includes two legs <b>1610</b>, <b>1612</b>. Each leg <b>1610</b>, <b>1612</b> may run between two adjacent columns of banks <b>102</b>. Depending on its column, a given bank <b>102</b> may have bus connections <b>1608</b> to both legs <b>1610</b>, <b>1612</b> of the same bus <b>1604</b> or may have bus connections <b>1608</b> to opposite legs <b>1610</b>, <b>1612</b> of adjacent busses. In this example, even columns (e.g., 0<sup>th</sup>, 2<sup>nd</sup>, 4<sup>th</sup>) are connected to the legs <b>1610</b>, <b>1612</b> of the same bus <b>1604</b> and odd columns (e.g., 1<sup>st</sup>, 3<sup>rd</sup>) are connected to different legs <b>1610</b>, <b>1612</b> of adjacent busses <b>1604</b>.</p><p id="p-0097" num="0096">In each bus <b>1604</b>, one end of each leg <b>1610</b>, <b>1612</b> is connected to the interface <b>1602</b>, and the opposite end of each leg <b>1610</b>, <b>1612</b> is connected to a reversing segment <b>1620</b>. Further, concerning the direction of movement of messages on the bus <b>1604</b>, one leg <b>1610</b> may be designated as outgoing from the interface <b>1602</b> and the other leg <b>1612</b> may be designated as incoming to the interface <b>1602</b>. As such, a message <b>1606</b> put onto the bus <b>1604</b> by the interface <b>1602</b> may be pumped along the leg <b>1610</b>, through the reversing segment <b>1620</b>, and back towards the interface <b>1602</b> along the other leg <b>1612</b>.</p><p id="p-0098" num="0097">The reversing segment <b>1620</b> reverses an ordering of content for each message <b>1606</b>, such that the orientation of the content of each message <b>1606</b> remains the same relative to the PEs of the banks <b>102</b>, regardless of which side of the bank <b>102</b> the message <b>1606</b> is on. This is shown schematically as message packets &#x201c;A,&#x201d; &#x201c;B,&#x201d; and &#x201c;C,&#x201d; which are discrete elements of content of a message <b>1606</b>. As can be seen, the orientation of the packets of the message <b>1606</b> whether on the leg <b>1610</b> or the leg <b>1612</b> is the same due to the reversing segment <b>1620</b>. Without the reversing segment, i.e., with a simple loop bus, the orientation of the message <b>1606</b> on the return leg <b>1612</b> would be opposite.</p><p id="p-0099" num="0098"><figref idref="DRAWINGS">FIG. <b>13</b></figref> shows another example of a PE <b>2100</b> that may be used with any of the example banks of processing elements discussed herein.</p><p id="p-0100" num="0099">The PE <b>2100</b> includes an ALU <b>2102</b>, an array of resultant registers <b>2104</b>, a resultant selector <b>2105</b>, a hold register <b>2106</b>, a zero disable <b>2110</b>, a switch <b>2112</b>, an input vector register <b>2114</b>, an input zero detector <b>2116</b>, neighbor PE interconnect control <b>2118</b>, and a listen register <b>2120</b>.</p><p id="p-0101" num="0100">The ALU <b>2102</b> implements an operation on data in the input vector register <b>2114</b> and data in memory cells <b>2130</b> associated with the PE <b>2100</b>. Examples of such operations include multiplying accumulation as discussed elsewhere herein. This may include, for example, multiplying matrix coefficients, which may be stored in memory cells <b>2130</b>, by an activation vector, which may be stored in input vector register <b>2114</b>. During such operation, the array of resultant registers <b>2104</b> may accumulate resultant vector components. The ALU <b>2102</b> may include one or more levels of multiplexor and/or a multiplier <b>2108</b>.</p><p id="p-0102" num="0101">Accumulation of results in resultant registers <b>2104</b> may be performed. That is, at a given time, the input vector register <b>2114</b> may be multiplied with selected coefficients from memory cells <b>2130</b> and the products may be accumulated at the resultant registers <b>2104</b> (e.g., a product is added to a value already in a resultant register). As such, for a particular value in the input vector register <b>2114</b>, an appropriate value may be selected from memory cells <b>2130</b> for multiplication and an appropriate resultant register <b>2104</b> may perform the accumulation. This may implement any of the input vector cycling/shuffling described herein, such as discussed with respect to <figref idref="DRAWINGS">FIGS. <b>7</b>A-<b>7</b>C</figref>.</p><p id="p-0103" num="0102">The resultant selector <b>2105</b> selects a resultant register <b>2104</b> to write to the memory cells <b>2130</b>.</p><p id="p-0104" num="0103">The neighbor PE interconnect control <b>2118</b> may communicate values between the input vector register <b>2114</b> of the PE <b>2100</b> and an input vector register <b>2114</b> of a neighboring PE <b>2100</b>. As such, the neighbor PE interconnect control <b>2118</b> is connected to a like element in neighboring PEs <b>2100</b> via interconnections <b>2132</b>. For example, the neighbor PE interconnect control <b>2118</b> may be connected to first neighbor PEs <b>2100</b> on each side of the PE <b>2100</b>, second neighbor PEs <b>2100</b> on each side of the PE <b>2100</b>, fourth neighbor PEs <b>2100</b> on each side of the PE <b>2100</b>, and/or sixth neighbor PEs <b>2100</b> on each side of the PE <b>2100</b>. When no such neighbor PE exists in the bank to which the PE <b>2100</b> is provided, the neighbor PE interconnect control <b>2118</b> may be connected to respective PEs <b>2100</b> of a neighboring bank and/or to a controller. The neighbor PE interconnect control <b>2118</b> may be configured to rotate or shuffle input vector values as discussed elsewhere herein. The neighbor PE interconnect control <b>2118</b> may connect neighboring PEs <b>2100</b> in a row (left-right) of PEs <b>2100</b>. Additionally or alternatively, a neighbor PE interconnect control <b>2118</b> may connect neighboring PEs <b>2100</b> in a column (up-down) of PEs <b>2100</b>.</p><p id="p-0105" num="0104">The neighbor PE interconnect control <b>2118</b> may include a logic/switch array to selectively communicate values among PEs <b>2100</b>, such as the logic/switch arrays discussed elsewhere herein.</p><p id="p-0106" num="0105">The neighbor PE interconnect control <b>2118</b> may designate a single neighbor PE <b>2100</b> from which to obtain data. That is, interconnections <b>2132</b> with neighbor PEs <b>2100</b> may be restricted so that a PE <b>2100</b> only at most listens to one selected neighbor PE <b>2100</b>. The listen register <b>2120</b> controls from which, if any, PE <b>2100</b> that the neighbor PE interconnect control <b>2118</b> obtains data. That is, the listen register <b>2120</b> selects a neighbor PE <b>2100</b> as the source of input to the PE <b>2100</b>.</p><p id="p-0107" num="0106">The hold register <b>2106</b> may be set to disable computation by the ALU <b>2102</b>. That is, data may be selected from memory <b>2130</b> and moved into/out of input vector register <b>2114</b> while the hold register <b>2106</b> at the same time ensures that the computation is not performed by the PE <b>2100</b>, but may be performed by other PEs in the same row/column.</p><p id="p-0108" num="0107">The zero disable <b>2110</b> controls the inputs to the multiplier <b>2108</b> to be unchanged when detecting that one or both intended inputs to the multiplier <b>2108</b> are zero. That is, should the intended inputs include a zero value for multiplication and accumulation, the zero disable <b>2110</b> holds the present input values as unchanged instead of providing the actual inputs that include the zero value. Multiplication by zero produces a zero product which does not need to be accumulated. As such, the zero disable <b>2110</b> saves energy, as the ALU <b>2102</b> uses significantly more energy when an input changes as opposed to when the inputs do not change.</p><p id="p-0109" num="0108">The switch <b>2112</b> allows a selected resultant register <b>2104</b> or the input vector register <b>2114</b>, via the input zero detector <b>2116</b>, to be written to memory cells <b>2130</b>. The switch <b>2112</b> allows data from the memory cells <b>2130</b> to be written to the listen register <b>2120</b>. The switch <b>2112</b> allows one bit of data to be written to the hold register <b>2106</b>. The switch <b>2112</b> allows data to be written to the input vector register <b>2114</b> through the input zero detector <b>2116</b>. If switch <b>2112</b> is open, then the memory cells <b>2130</b> are connected to the multiplier <b>2108</b>, without being loaded down by inputs of the input vector register <b>2114</b> and input zero detector <b>2116</b> or the resultant selector <b>2105</b>.</p><p id="p-0110" num="0109">The input zero detector <b>2116</b> detects whether the input vector register <b>2114</b> contains a value that is zero. Likewise, the memory cells <b>2130</b> may include OR logic <b>2134</b> to determine whether the selected value in the memory cells <b>2130</b> is zero. The OR logic <b>2134</b> provides an indication of a zero value. As such, when either (or both) of the input vector register <b>2114</b> and the selected value in the memory cells <b>2130</b> is zero, the zero disable <b>2110</b> controls both inputs from the input vector register <b>2114</b> and the selected value in the memory cells <b>2130</b> to appear to the ALU <b>2102</b> to be unchanged, thereby refraining from performing a needless multiplication and accumulation and saving power at the ALU <b>2102</b>.</p><p id="p-0111" num="0110">With reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, the memory cells <b>2130</b> associated with a PE <b>2100</b> may include blocks <b>2200</b>, <b>2202</b> of memory cells and related caches <b>2204</b>, <b>2206</b>. In this example, a main memory block <b>2200</b> is associated with a cache <b>2204</b> and secondary memory blocks <b>2202</b> are each associated with a cache <b>2206</b>. The caches <b>2204</b>, <b>2206</b> communicate with the PE <b>2100</b> rather than the PE communicating with the memory blocks <b>2200</b>, <b>2202</b> directly.</p><p id="p-0112" num="0111">A cache <b>2204</b>, <b>2206</b> may read/write to its memory block <b>2200</b>, <b>2202</b> an amount of data that is larger than the amount communicated with the PE <b>2100</b>. For example, a cache <b>2204</b>, <b>2206</b> may read/write to its memory block <b>2200</b>, <b>2202</b> in 16-bit units and may communicate with the PE <b>2100</b> in 4-bit units. As such, timing of read/write operations at the memory blocks <b>2200</b>, <b>2202</b> may be relaxed. Thus, it is contemplated that the processing speed of the PE <b>2100</b> will govern operations of the PE <b>2100</b> with the memory cells <b>2130</b>. Clocking of the memory cells <b>2130</b> can be increased to meet the needs of the PE <b>2100</b>.</p><p id="p-0113" num="0112">An isolation switch <b>2208</b> may be provided to isolate secondary memory blocks <b>2202</b> and their caches <b>2206</b> from the PE <b>2100</b>. As such, the PE <b>2100</b> may be selectably connected to a smaller set of memory cells or a larger set of memory cells. When the isolation switch <b>2208</b> is closed, the PE <b>2100</b> may access the main memory block <b>2200</b> and the secondary memory blocks <b>2202</b>, through the caches <b>2204</b>, <b>2206</b>, as a contiguous range of addresses. When the isolation switch <b>2208</b> is opened, the PE <b>2100</b> may only access the main memory block <b>2200</b>, though its cache <b>2204</b>, with the respective reduced range of addresses. Opening the isolation switch <b>2208</b> to reduce the amount of available memory to the PE <b>2100</b> may save energy. The isolation switch <b>2208</b> may be implemented by a switchable bus that connects the secondary caches <b>2206</b> to the PE <b>2100</b>.</p><p id="p-0114" num="0113">In this example, each cache <b>2204</b>, <b>2206</b> includes OR logic to inform the PE <b>2100</b> as to whether the memory value selected by the PE <b>2100</b> is zero. As such, the above-discussed technique of refraining from changing ALU input values may be used to save power.</p><p id="p-0115" num="0114">Further, in this example, the memory cells <b>2130</b> include working or scratch registers <b>2210</b> connected to the PE <b>2100</b> to provide temporary space for intermediate or larger-bit computations.</p><p id="p-0116" num="0115">A memory-sharing switch <b>2214</b>, <b>2218</b> may be provided to connect memory blocks <b>2200</b>, <b>2202</b> of a PE <b>2100</b> to memory blocks <b>2200</b>, <b>2202</b> of a neighboring PE <b>2100</b>. The memory-sharing switch <b>2214</b>, <b>2218</b> may be implemented as a switchable bus that connects the caches <b>2204</b>, <b>2206</b> to respective caches <b>2204</b>, <b>2206</b> of a set of memory cells <b>2130</b> associated with a neighboring PE <b>2100</b>.</p><p id="p-0117" num="0116">As shown in <figref idref="DRAWINGS">FIG. <b>15</b></figref>, a left-right (row-based) memory-sharing switch <b>2214</b> may connect PE memory <b>2130</b> to a left/right neighbor PE memory <b>2130</b>. Similarly, an up-down (column-based) memory-sharing switch <b>2218</b> may connect PE memory <b>2130</b> to an up/down neighbor PE memory <b>2130</b>. Any number and combination of such memory-sharing switches <b>2214</b>, <b>2218</b> may be provided to combine memory cells <b>2130</b> associated with individual PEs <b>2100</b> into a combined pool of memory cells <b>2130</b> associated with a group of PEs <b>2100</b>.</p><p id="p-0118" num="0117">Memory-sharing switches <b>2214</b>, <b>2218</b> may be provided between groups of memory cells <b>2130</b> so that a maximum pool of memory cells <b>2130</b> is determined by hardware. Alternatively, memory-sharing switches <b>2214</b>, <b>2218</b> may be provided between all memory cells <b>2130</b>, and firmware or software may govern a maximum pool size, if any.</p><p id="p-0119" num="0118">PEs <b>2100</b> can share memory in groups of two or four, in this example. If a PE fails, it can be skipped. An entire column of PEs can be labelled as bad if a particular PE in the column is bad, so as to avoid having to move data laterally around the bad PE. In <figref idref="DRAWINGS">FIG. <b>15</b></figref>, for example, if the top center PE <b>2100</b> had failed, it and both the top and bottom center PEs can be labeled &#x201c;bad&#x201d; and be skipped. However, if the rightmost PEs were supposed to be grouped with the center PEs to share memory, this memory can no longer be shared due to the bad PEs. The rightmost PEs can also skip the bad column of PEs and share with the leftmost PEs. This can be achieved by each PE having a DUD register that can be written from SRAM. An application can then initially detect the bad or failed PEs, and set the DUD bits for the entire column of PEs of the broken PE. When SRAM is then to be shared in a group that contains a DUD column, the controller can read the DUD bit and skip over the PE in that column. Hence, if the center column of PEs had their DUD bit set, the rightmost PEs could still share SRAM with the leftmost PEs.</p><p id="p-0120" num="0119">In other examples, if size and power restrictions are a concern, a maximum size of shared PEs may be enforced such as a two-by-two arrangement of four PEs. Groups of four PEs may have hardwired interconnections. In this example, the DUD bit disables an entire block of four PEs.</p><p id="p-0121" num="0120">In still other examples, the hardware may be further simplified and memory sharing may not be omitted. The DUD bit may provide a way of turning off a PE to save power.</p><p id="p-0122" num="0121">With reference back to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, caches <b>2204</b>, <b>2206</b> may provide I/O capabilities, such as addressing and address mapping for a pool of memory cells <b>2130</b> as enabled by memory-sharing switches <b>2214</b>, <b>2218</b>. Further, caches <b>2204</b>, <b>2206</b> may be configured to perform copy, move, or other operations as facilitated by memory-sharing switches <b>2214</b>, <b>2218</b>. That is, caches <b>2204</b>, <b>2206</b> may be I/O enabled so as to communicate information with the respective PE <b>2100</b> and further to communicate information with any other caches <b>2204</b>, <b>2206</b> connected by memory-sharing switches <b>2214</b>, <b>2218</b>. As such, data in memory cells <b>2130</b> may be copied, moved, or undergo other operation independent of operation of a PE <b>2100</b>.</p><p id="p-0123" num="0122">Further, the isolation switch <b>2208</b> may allow a PE <b>2100</b> to access its main memory block <b>2200</b> while memory-to-memory operations, enabled by secondary caches <b>2206</b> and memory-sharing switches <b>2214</b>, <b>2218</b>, are performed to share information in secondary memory blocks <b>2202</b> with neighboring memory. This may allow for greater operational flexibility and for reading/writing data to memory <b>2130</b> while allowing PEs <b>2100</b> to continue with their assigned computations.</p><p id="p-0124" num="0123"><figref idref="DRAWINGS">FIG. <b>16</b></figref> shows a zero disable circuit <b>2110</b> that may be used in PEs discussed elsewhere herein, such as the PE <b>2100</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>. The zero disable circuit <b>2110</b> reduces power consumption used by a multiplier and/or accumulator of the PE. Such power may be saved by disabling the multiplier and/or the accumulator when an input value &#x201c;a&#x201d; of an adjacent PE is zero and/or when a coefficient &#x201c;c&#x201d; at the present PE is zero. Power savings may be substantial when the PEs are part of a device that processes a neural network, as it is often the case that a significant number (e.g., 50%) of values in such processing are zeros. Further, neural network can be trained to have coefficients &#x201c;c&#x201d; that tend to be zero, such that specific training can enhance power savings.</p><p id="p-0125" num="0124">In addition to input values &#x201c;a&#x201d; taken from an adjacent PE at <b>2132</b>A and coefficients &#x201c;c&#x201d; taken from memory, the zero disable circuit <b>2110</b> also takes respective indicators of whether or not such values &#x201c;a&#x201d; and &#x201c;c&#x201d; are zero, as indicated by &#x201c;a=0&#x201d; and &#x201c;c=0&#x201d;, respectively.</p><p id="p-0126" num="0125">Various components of the zero disable circuit <b>2110</b> operate according to a clock (&#x201c;clk&#x201d;).</p><p id="p-0127" num="0126">The zero disable circuit <b>2110</b> includes a multiplier <b>2402</b> to multiply input value &#x201c;a&#x201d; and coefficient &#x201c;c&#x201d;. The multiplier <b>2402</b> includes cascading logic, operates asynchronously, and therefore does not operate according to the clock. The multiplier <b>2402</b> is triggered only if either or both of its inputs, i.e., the input value &#x201c;a&#x201d; and the coefficient &#x201c;c&#x201d;, change. Hence, if either or both of the inputs &#x201c;a&#x201d; and &#x201c;c&#x201d; is zero, then the inputs to the multiplier <b>2402</b> are held unchanged, so as to prevent the multiplier <b>2402</b> from computing a zero result that unnecessarily consumes power.</p><p id="p-0128" num="0127">The zero disable circuit <b>2110</b> includes a transparent latch <b>2404</b>. Input to the transparent latch <b>2404</b> includes coefficients &#x201c;c&#x201d; from memory associated with the PE (e.g., memory cells <b>2130</b> of <figref idref="DRAWINGS">FIG. <b>13</b></figref>). The transparent latch <b>2404</b> acts as a pass-through when its select line stays high, requiring minimal power. When the select line of the transparent latch <b>2404</b> is lowered, it latches the current value into a fixed state. This uses power, but occurs less frequently than a latch that is set at each clock cycle. The transparent latch <b>2404</b> is set if coefficient &#x201c;c&#x201d; or input value &#x201c;a&#x201d; is zero, and the multiplier <b>2402</b> consequently does not receive a change in the coefficient &#x201c;c&#x201d; value. Rather, the multiplier <b>2402</b> still receives the previous value of the coefficient &#x201c;c&#x201d;.</p><p id="p-0129" num="0128">The input value &#x201c;a&#x201d; is handled similarly. The zero disable circuit <b>2110</b> includes a register <b>2114</b> to store an input value &#x201c;a&#x201d;, as may be received with data from an adjacent PE at <b>2132</b>A. The zero disable circuit <b>2110</b> receives a value for &#x201c;a&#x201d; from an adjacent PE and does not latch this value of &#x201c;a&#x201d; if either &#x201c;a&#x201d; or &#x201c;c&#x201d; is zero. If the coefficient &#x201c;c&#x201d; is zero and the input value &#x201c;a&#x201d; is not zero (a!=0), then the input value &#x201c;a&#x201d; is stored, as the input value &#x201c;a&#x201d; may still need to be passed to the next adjacent PE at <b>2132</b>B. The zero disable circuit <b>2110</b> includes a shadow register <b>2408</b> to transfer the input value &#x201c;a&#x201d; to the next PE at <b>2132</b>B. The shadow register <b>2408</b> has a path parallel to the main register <b>2114</b> that stores the input value &#x201c;a&#x201d;. The shadow register <b>2408</b> is used to latch in the input value &#x201c;a&#x201d; if either &#x201c;a&#x201d; or &#x201c;c&#x201d; is zero.</p><p id="p-0130" num="0129">The zero disable circuit <b>2110</b> further includes a demultiplexer <b>2414</b> at the outputs of the main register <b>2114</b> and shadow register <b>2408</b> to select which value to pass to the next PE at <b>2132</b>B.</p><p id="p-0131" num="0130">The shadow register <b>2408</b> is useful when the input value &#x201c;a&#x201d; is not zero (a!=0) and the coefficient &#x201c;c&#x201d; is zero. However, in this example, signal timing is simplified by also using the shadow register <b>2408</b> for the case where the input value &#x201c;a&#x201d; is zero and the coefficient &#x201c;c&#x201d; is zero.</p><p id="p-0132" num="0131">When neither the input value &#x201c;a&#x201d; nor the coefficient &#x201c;c&#x201d; is zero, then the coefficient &#x201c;c&#x201d; flows from memory, through the transparent latch <b>2404</b>, and to the multiplier <b>2402</b>. Further, the input value &#x201c;a&#x201d; flows from the previous adjacent PE at <b>2132</b>A through the main registers <b>2114</b>, to the multiplier <b>2402</b>, and also to the next adjacent PE at <b>2132</b>B.</p><p id="p-0133" num="0132">When either &#x201c;a&#x201d; or &#x201c;c&#x201d; are zero, the previous coefficient &#x201c;c&#x201d; from memory is held in the transparent latch <b>2404</b>, and the previous input value &#x201c;a&#x201d; is held in the main register <b>2114</b>. As such, the inputs to the multiplier <b>2402</b> do not change, and the multiplier <b>2402</b> is therefore not triggered, saving power.</p><p id="p-0134" num="0133">The &#x201c;a=0&#x201d; signal from the previous adjacent PE at <b>2132</b>A is held for one clock cycle, and then passed on to the next adjacent PE at <b>2132</b>B.</p><p id="p-0135" num="0134">When the input value &#x201c;a&#x201d; is not zero and the coefficient &#x201c;c&#x201d; is zero, then the contents of the shadow register <b>2408</b> is selected to be passed on to the next PE at <b>2132</b>B. It is selected by a signal, delayed by one clock cycle, by a delay flipflop <b>2420</b>, which holds the signal for the duration of one clock.</p><p id="p-0136" num="0135">Further, in this example if the input value &#x201c;a&#x201d; is zero and the coefficient &#x201c;c&#x201d; is also zero, the input value &#x201c;a&#x201d; value of zero is latched to the shadow register <b>2408</b>, although it is never used. This simplifies signal timing issues and may further save power. Such a zero value is not passed on to the next PE at <b>2132</b>B, since the demultiplexer signal that selects between the main register <b>2114</b> and the shadow register <b>2408</b> is only triggered for the case where &#x201c;c=<b>0</b> and a!=0&#x201d;. However, the &#x201c;a=0&#x201d; signal is passed on. This may save some power, since the previous value of the main register <b>2114</b> is passed on to the next PE at <b>2132</b>B (along with the &#x201c;a=0&#x201d; signal that tells the next PE to ignore the actual input value &#x201c;a&#x201d;), and since the value has not changed, the signals in the conductors connecting the PEs do not have to change, which would otherwise cost power.</p><p id="p-0137" num="0136">There are various examples where refraining from triggering an ALU can reduce computations and save power. In neural networks, layers of convolutions are often used. After each convolution, a scale factor (e.g., multiplying all the results by a common factor) is often applied to normalize data or to shift the data into a useable range. When using integer math, the scale factor may be performed as two steps: multiplication by a factor, and then a bit-shift. A bit-shift shifts the bits of a result rightwards, discarding the lower least significant bits.</p><p id="p-0138" num="0137">Multiplication and shift values are often known in advance, as with the coefficients. If the multiplier value and shift value are such that some or many of the lower bits will be discarded, this means that some of the least significant terms are never used. Hence, two approaches may be used to save power: (1) a part of the output may be skipped (e.g., the lowest &#x201c;little &#x2018;a&#x2019;&#xd7;little &#x2018;c&#x2019;&#x201d; term) and/or (2) refrain from calculating the lowest term (e.g., &#x201c;little &#x2018;a&#x2019;&#xd7;little &#x2018;c&#x2019;&#x201d;) of the convolution at all. The first may save some cycles. The second may save many cycles. In an example that uses 4-bit PEs to perform 8-bit computations, up to &#xbc; of the convolution computation may be saved.</p><p id="p-0139" num="0138">With regard to all examples herein, a power-saving floating point representation may be used. Power may be saved when input values &#x201c;a&#x201d; and/or coefficients &#x201c;c&#x201d; are zero. Such values &#x201c;a&#x201d; and &#x201c;c&#x201d; may be represented by 4-bit nibbles. For 8-bit (Aa) by 8-bit (Cc) multiplications, multiplications may be performed in four stages, A*C, A*c, a*C, a*c. It is contemplated that Aa and Cc are distributed in some kind of distribution (such as a Gaussian distribution), where values near zero are most common, and the values farther away from zero are less common. Accordingly, values may be quantized, such that if a value is greater or equal to +/&#x2212;16, it is rounded to the nearest multiple of 16. In this way, a lower nibble for such numbers will always be zero and power will be saved when processing this nibble. In this way, all small values, less than 16, will have their MSNs=0. All large values, greater or equal to 16 will have their LSNs=0. When multiplication is performed on the basis of nibbles, this kind of rounding can be used to force a significant number of nibbles to zero, thereby saving power at the cost of some accuracy. If rounding on the basis of +/&#x2212;16 causes too much loss in accuracy, then quantization at +/&#x2212;32 or other value may be used.</p><p id="p-0140" num="0139"><figref idref="DRAWINGS">FIG. <b>17</b></figref> shows a cache arrangement <b>2500</b> to facilitate communications among memory allocated to different PEs. The cache arrangement <b>2500</b> includes a plurality of caches <b>2502</b>, each associated with a different set of memory blocks (not shown). The selection of which cache <b>2502</b> and/or memory cells to read/write from/to may be made by activation of row and column lines.</p><p id="p-0141" num="0140">For example, with reference to <figref idref="DRAWINGS">FIG. <b>14</b></figref>, each cache <b>2502</b> may be used as a cache <b>2204</b>, <b>2206</b> associated with any suitable number of blocks <b>2200</b>, <b>2202</b> of memory cells. Communications provided for by switches <b>2208</b>, <b>2218</b> in the example of <figref idref="DRAWINGS">FIG. <b>15</b></figref> may be replaced or augmented by the components discussed below.</p><p id="p-0142" num="0141">The caches <b>2502</b> are in mutual communication via a write bus <b>2504</b> and a read bus <b>2506</b>. A write multiplexer <b>2508</b> puts signals onto the write bus <b>2504</b> and a read multiplexer <b>2510</b> takes signals from the read bus <b>2506</b>.</p><p id="p-0143" num="0142">The write multiplexer <b>2508</b> selectively takes input from the read bus <b>2506</b>, a read bus of an adjacent cache arrangement of an adjacent PE above the present PE, at <b>2512</b>, and a read bus of an adjacent cache arrangement of an adjacent PE below the present PE, at <b>2514</b>. As such, the write multiplexer <b>2508</b> may be controlled to write to a cache <b>2502</b> from another cache <b>2502</b> of the same arrangement <b>2500</b> or a cache of a PE in an adjacent row.</p><p id="p-0144" num="0143">The write multiplexer <b>2508</b> also selectively takes data from the PE registers, at <b>2516</b>, so that accumulated results &#x201c;d&#x201d; and/or input values &#x201c;a&#x201d; may be written to memory.</p><p id="p-0145" num="0144">The write multiplexer <b>2508</b> may be controlled, at selection input <b>2518</b>, by a controller associated with the row or bank of PEs.</p><p id="p-0146" num="0145">The read bus <b>2506</b> takes input from the caches <b>2502</b> and provides same to the write multiplexer <b>2508</b>, a write bus of the adjacent cache arrangement of the adjacent PE above the present PE, at <b>2512</b>, and a write bus of the adjacent cache arrangement of the adjacent PE below the present PE, at <b>2514</b>.</p><p id="p-0147" num="0146">The read multiplexer <b>2510</b> may provide input to the PE registers, at <b>2520</b>, so that the PE may read coefficients &#x201c;c&#x201d; and/or write input values &#x201c;a&#x201d; from memory.</p><p id="p-0148" num="0147">The cache arrangement <b>2500</b> allows for cache-to-cache communications between memory blocks associated with different PEs as well as blocks associated with the same PE. The cache arrangements <b>2500</b> of a top and/or bottom row of PEs in a bank <b>102</b> (<figref idref="DRAWINGS">FIG. <b>1</b></figref>) of PEs may be connected to respective cache arrangements <b>2500</b> of a bottom and/or top row of an adjacent bank <b>102</b> of PEs, so as to facilitate communications among different banks <b>102</b> of PEs. Additionally or alternatively, the cache arrangements <b>2500</b> of a top and/or bottom row of PEs in a bank <b>102</b> may be connected to an I/O bus, such as bus <b>1604</b> (<figref idref="DRAWINGS">FIG. <b>12</b></figref>), so as to facilitate communications among different banks of PEs.</p><p id="p-0149" num="0148">The cache arrangement <b>2500</b> may additionally provide redundancy in case of bad memory cells. Each cache <b>2502</b> may serve a number of rows (e.g., 32) of memory cells. A given memory block may be assigned to compensate for bad memory cells in another block. This memory block may have a number of rows (e.g., 2) reserved to replace rows containing bad cells.</p><p id="p-0150" num="0149">A table of virtual registers may be maintained for each cache <b>2502</b>. The table may map logical addresses to caches <b>2502</b>.</p><p id="p-0151" num="0150"><figref idref="DRAWINGS">FIG. <b>18</b></figref> is a block diagram of a computing device <b>1800</b> including an array of PEs <b>1802</b> with memory cells <b>1804</b> and cache memory <b>1806</b> and connections there-between. The computing device <b>1800</b> may include any number of PEs <b>1802</b>, each with its own connected blocks of memory cells <b>1804</b> and cache memory <b>1806</b>. Examples of PEs with memory cells and cache have been described with regard to <figref idref="DRAWINGS">FIGS. <b>14</b>, <b>15</b>, and <b>17</b></figref>, which may be referenced for further description. The techniques described below may be used in the other examples discussed herein, such as those of <figref idref="DRAWINGS">FIGS. <b>14</b>, <b>15</b>, and <b>17</b></figref>, and vice versa. The PEs <b>1802</b> may implement a row <b>104</b> of PEs in a bank <b>102</b>, where a plurality of banks <b>102</b> form a computing device, such as discussed elsewhere herein (see <figref idref="DRAWINGS">FIG. <b>1</b></figref> for example).</p><p id="p-0152" num="0151">The PEs <b>1802</b> are mutually connected to share data, such as described elsewhere herein, so as to perform SIMD operations, such as multiplying accumulations. Blocks of memory cells <b>1804</b> are connected to each PE <b>1802</b> to store data related to the SIMD operations, such as coefficients, input/activation values, and accumulated results, performed by the PEs <b>1802</b>. A cache <b>1806</b> is connected to each PE <b>1802</b> to cache data of a respective block of memory cells <b>1804</b>. A PE <b>1802</b> may be connected to its memory cells <b>1804</b> through its cache <b>1806</b>. In this example, each PE <b>1802</b> includes a plurality of caches <b>1806</b> each associated with a different block of memory cells <b>1804</b>. Any number of caches <b>1806</b> and blocks of memory cells <b>1804</b> may be used for a respective PE <b>1802</b>.</p><p id="p-0153" num="0152">A cache (first cache) <b>1806</b> of a given PE (first PE) <b>1802</b> is connected to an adjacent cache (second or third cache) <b>1806</b> of a PE <b>1802</b> that is adjacent the given PE <b>1802</b>. Adjacency may include any one or combination of immediately adjacent, second adjacent, third adjacent, and so on. Such connections may have special cases at or near the ends of the array of PEs. An end of the array at or near a controller <b>1810</b> may have connections with the controller <b>1810</b>.</p><p id="p-0154" num="0153">The connections of caches <b>1806</b> of adjacent PEs <b>1802</b> allow sharing of recently or frequently used data among adjacent PEs <b>1802</b>. While the PEs <b>1802</b> may be mutually connected to share data, such as data stored in PE registers, and described elsewhere herein, it may be useful to provide direct memory communication via the caches <b>1806</b>.</p><p id="p-0155" num="0154">The computing device <b>1800</b> may further include a multiplexer <b>1812</b> connecting each PE <b>1802</b> to its caches <b>1806</b>. The multiplexer <b>1812</b> may implement cache read and/or write functionality between the PE <b>1802</b> and its caches <b>1806</b> and between the caches <b>1806</b> of adjacent PEs <b>1802</b>. The multiplexer <b>1812</b> may include a write multiplexer and/or a read multiplexer, as discussed above with regard to <figref idref="DRAWINGS">FIG. <b>17</b></figref>.</p><p id="p-0156" num="0155">Regarding writing, the multiplexer <b>1812</b> (or write multiplexer <b>2508</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>) may include an output connected to the cache <b>1806</b> of the PE <b>1802</b> to which the multiplexer <b>1812</b> belongs. The multiplexer <b>1812</b> (or write multiplexer <b>2508</b>) may further include selectable inputs connected to a register <b>1814</b> of the same PE <b>1802</b> and to a cache <b>1806</b> of an adjacent PE <b>1802</b>.</p><p id="p-0157" num="0156">Regarding reading, the multiplexer <b>1812</b> (or read multiplexer <b>2510</b> of <figref idref="DRAWINGS">FIG. <b>17</b></figref>) may include an output connected to a register <b>1814</b> of the PE <b>1802</b> to which the multiplexer <b>1812</b> belongs. The multiplexer <b>1812</b> (or read multiplexer <b>2510</b>) may further include selectable inputs connected to the cache <b>1806</b> and to a cache <b>1806</b> of an adjacent PE <b>1802</b>.</p><p id="p-0158" num="0157">As such the multiplexer <b>1812</b> of a respective PE <b>1802</b> may read from its (first) cache <b>1806</b> and (second, third) caches <b>1806</b> of adjacent PEs <b>1802</b> and provide such data to its PE's registers <b>1814</b>. The multiplexer <b>1812</b> may further write to its (first) cache <b>1806</b> from its PE's registers <b>1814</b> and from (second, third) caches <b>1806</b> of adjacent PEs <b>1802</b>.</p><p id="p-0159" num="0158">The multiplexer <b>1812</b> selection input that determines cache read source and/or write destination may be controlled by a controller <b>1810</b>, which may be a SIMD controller of the row or bank of PEs <b>1802</b>.</p><p id="p-0160" num="0159">The controller <b>1810</b> may control the PEs <b>1802</b> to perform a multiplying accumulation that uses coefficients, input/activation values, and accumulated results. Accordingly, the controller <b>1810</b> may be configured to control a multiplexer <b>1812</b> to write to its (first) cache <b>1806</b> accumulated results and/or input values of the multiplying accumulation. The controller <b>1810</b> may further be configured to control the multiplexer <b>1812</b> to read from its (first) cache coefficients and/or input values of the multiplying accumulation. Input values and/or coefficients, for example, may be shared among adjacent PEs <b>1802</b> via the cache connections provided by the multiplexers <b>1812</b> of the PEs <b>1802</b> and as controlled by the controller <b>1810</b>.</p><p id="p-0161" num="0160"><figref idref="DRAWINGS">FIG. <b>19</b></figref> shows a bank <b>2600</b> of PEs <b>108</b> arranged in rows <b>104</b> and connected to a controller <b>2602</b> that may be used to control SIMD operations of the PEs <b>108</b>. The controller <b>2602</b> may be connected to end-most PEs <b>108</b> of each row <b>104</b>, for example, two adjacent end-most PEs <b>108</b>.</p><p id="p-0162" num="0161">The controller <b>2602</b> may be configured to control the PEs <b>108</b> as discussed elsewhere herein.</p><p id="p-0163" num="0162">An ALU <b>2604</b> may be provided to each row <b>104</b> of PEs <b>108</b>. The ALU may be connected to end-most PEs <b>108</b>, for example, two adjacent end-most PEs <b>108</b>. Local registers <b>2606</b> may be provided to each ALU <b>2604</b>.</p><p id="p-0164" num="0163">Each ALU <b>2604</b> may be configured to perform an operation on the respective row <b>104</b> of PEs <b>108</b>. Example operations include move, add, argmax, and maximum/minimum determination. The intermediate and final results of such operation may be stored in the associated registers <b>2606</b>. The ALU <b>2604</b> may be purposefully limited in its operational capacity, so as to reduce complexity, and addition and argmax operations are contemplated to be quite important for neural networks.</p><p id="p-0165" num="0164">Operations on a row <b>104</b> of PEs <b>108</b>, as performed by the ALU <b>2604</b>, may be facilitated by copying data between neighboring PEs <b>108</b>, as discussed elsewhere herein. For example, operations may be performed on data as the data is shifted towards and into the end-most PEs <b>108</b> at the ALU <b>2604</b>.</p><p id="p-0166" num="0165">The controller <b>2602</b> may control the ALUs <b>2604</b> in a SIMD fashion, so that each ALU <b>2604</b> performs the same operation at the same time.</p><p id="p-0167" num="0166">The local registers <b>2606</b> may also be used as staging for reading and writing from/to the respective row <b>104</b> of PEs <b>108</b>.</p><p id="p-0168" num="0167">The controller <b>2602</b> may further include an ALU <b>2608</b> to perform an operation on the results obtained by the row-based ALUs <b>2604</b>. Example operations include move, add, argmax, and maximum/minimum determination. The bank-based ALU <b>2608</b> may include registers <b>2610</b> to store intermediate and final results. As such, results obtained by individual PEs <b>108</b> may be distilled to row-based results, by ALUs <b>2604</b>, that may be further distilled to a bank-based result, by the ALU <b>2608</b>.</p><p id="p-0169" num="0168">As should be apparent from the above discussion, the techniques discussed herein are suitable for low-power neural-network computations and applications. Further, the techniques are capable of handling a large number of computations with flexibility and configurability.</p><p id="p-0170" num="0169">It should be recognized that features and aspects of the various examples provided above can be combined into further examples that also fall within the scope of the present disclosure. In addition, the figures are not to scale and may have size and shape exaggerated for illustrative purposes.</p><?detailed-description description="Detailed Description" end="tail"?></description><claims id="claims"><claim id="CLM-001-7" num="001-7"><claim-text><b>1</b>-<b>7</b>. (canceled)</claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. A computing device comprising:<claim-text>a plurality of rows of processing elements to perform single instruction multiple data (SIMD) operations, wherein the processing elements of each row are mutually connected to share data; and</claim-text><claim-text>a row arithmetic logic unit (ALU) at each row of the plurality of rows of processing elements, the row ALU of a respective row being configured to perform an operation with processing elements of the respective row;</claim-text><claim-text>wherein the row ALU is connected to a plurality of end-most processing elements of the respective row.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the row ALU is connected to two end-most processing elements of the respective row.</claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the row ALU is configured to perform addition with data contained in the processing elements of the respective row.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, wherein the row ALU is configured to perform argmax with data contained in the processing elements of the respective row.</claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising registers connected to the row ALU to store a result of the operation.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The computing device of <claim-ref idref="CLM-00008">claim 8</claim-ref>, further comprising a bank ALU connected to the row ALU of each row of processing elements, the bank ALU being configured to perform an additional operation with results obtained by row ALUs of the plurality of rows of processing elements.</claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The computing device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the bank ALU is configured to perform addition with results obtained by the row ALUs.</claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The computing device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the bank ALU is configured to perform argmax with results obtained by the row ALUs.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The computing device of <claim-ref idref="CLM-00013">claim 13</claim-ref>, further comprising registers connected to the bank ALU to store a result of the additional operation.</claim-text></claim></claims></us-patent-application>