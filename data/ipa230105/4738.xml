<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE us-patent-application SYSTEM "us-patent-application-v46-2022-02-17.dtd" [ ]><us-patent-application lang="EN" dtd-version="v4.6 2022-02-17" file="US20230004739A1-20230105.XML" status="PRODUCTION" id="us-patent-application" country="US" date-produced="20221221" date-publ="20230105"><us-bibliographic-data-application lang="EN" country="US"><publication-reference><document-id><country>US</country><doc-number>20230004739</doc-number><kind>A1</kind><date>20230105</date></document-id></publication-reference><application-reference appl-type="utility"><document-id><country>US</country><doc-number>17364743</doc-number><date>20210630</date></document-id></application-reference><us-application-series-code>17</us-application-series-code><classifications-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>46</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr><classification-ipcr><ipc-version-indicator><date>20060101</date></ipc-version-indicator><classification-level>A</classification-level><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>73</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source></classification-ipcr></classifications-ipcr><classifications-cpc><main-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>00362</subgroup><symbol-position>F</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></main-cpc><further-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>K</subclass><main-group>9</main-group><subgroup>4671</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20170101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>7</main-group><subgroup>74</subgroup><symbol-position>L</symbol-position><classification-value>I</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>30196</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10024</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc><classification-cpc><cpc-version-indicator><date>20130101</date></cpc-version-indicator><section>G</section><class>06</class><subclass>T</subclass><main-group>2207</main-group><subgroup>10028</subgroup><symbol-position>L</symbol-position><classification-value>A</classification-value><action-date><date>20230105</date></action-date><generating-office><country>US</country></generating-office><classification-status>B</classification-status><classification-data-source>H</classification-data-source><scheme-origination-code>C</scheme-origination-code></classification-cpc></further-cpc></classifications-cpc><invention-title id="d2e43">HUMAN POSTURE DETERMINATION METHOD AND MOBILE MACHINE USING THE SAME</invention-title><us-parties><us-applicants><us-applicant sequence="00" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UBTECH NORTH AMERICA RESEARCH AND DEVELOPMENT CENTER CORP</orgname><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook><residence><country>US</country></residence></us-applicant><us-applicant sequence="01" app-type="applicant" designation="us-only" applicant-authority-category="assignee"><addressbook><orgname>UBTECH ROBOTICS CORP LTD</orgname><address><city>Shenzhen</city><country>CN</country></address></addressbook><residence><country>CN</country></residence></us-applicant></us-applicants><inventors><inventor sequence="00" designation="us-only"><addressbook><last-name>Dong</last-name><first-name>Chuqiao</first-name><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="01" designation="us-only"><addressbook><last-name>Shao</last-name><first-name>Dan</first-name><address><city>San Gabriel</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="02" designation="us-only"><addressbook><last-name>Xiu</last-name><first-name>Zhen</first-name><address><city>Chino Hills</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="03" designation="us-only"><addressbook><last-name>Guo</last-name><first-name>Dejun</first-name><address><city>San Gabriel</city><state>CA</state><country>US</country></address></addressbook></inventor><inventor sequence="04" designation="us-only"><addressbook><last-name>Tan</last-name><first-name>Huan</first-name><address><city>Pasadena</city><state>CA</state><country>US</country></address></addressbook></inventor></inventors></us-parties></us-bibliographic-data-application><abstract id="abstract"><p id="p-0001" num="0000">Human posture determination is disclosed. Human posture is determined by obtaining range image(s) through a range camera, detecting key points of an estimated skeleton of a human in color data of the range image(s) and calculating positions of the detected key points based on depth data of the range image(s), choosing a feature map from a set of predefined feature maps based on the detected key points among a set of predefined key points, obtaining two features of a body of the human corresponding to the chosen feature map based on the positions of the detected key points, and determining a posture of the human according to the two features in the chosen feature map.</p></abstract><drawings id="DRAWINGS"><figure id="Fig-EMI-D00000" num="00000"><img id="EMI-D00000" he="186.27mm" wi="128.95mm" file="US20230004739A1-20230105-D00000.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00001" num="00001"><img id="EMI-D00001" he="182.88mm" wi="143.76mm" file="US20230004739A1-20230105-D00001.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00002" num="00002"><img id="EMI-D00002" he="186.44mm" wi="110.83mm" orientation="landscape" file="US20230004739A1-20230105-D00002.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00003" num="00003"><img id="EMI-D00003" he="167.30mm" wi="145.37mm" file="US20230004739A1-20230105-D00003.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00004" num="00004"><img id="EMI-D00004" he="199.22mm" wi="130.98mm" file="US20230004739A1-20230105-D00004.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00005" num="00005"><img id="EMI-D00005" he="54.78mm" wi="133.43mm" orientation="landscape" file="US20230004739A1-20230105-D00005.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00006" num="00006"><img id="EMI-D00006" he="185.59mm" wi="134.37mm" orientation="landscape" file="US20230004739A1-20230105-D00006.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00007" num="00007"><img id="EMI-D00007" he="139.62mm" wi="149.44mm" file="US20230004739A1-20230105-D00007.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00008" num="00008"><img id="EMI-D00008" he="131.83mm" wi="146.39mm" file="US20230004739A1-20230105-D00008.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00009" num="00009"><img id="EMI-D00009" he="157.31mm" wi="141.14mm" file="US20230004739A1-20230105-D00009.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00010" num="00010"><img id="EMI-D00010" he="132.16mm" wi="144.86mm" file="US20230004739A1-20230105-D00010.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00011" num="00011"><img id="EMI-D00011" he="164.68mm" wi="128.19mm" orientation="landscape" file="US20230004739A1-20230105-D00011.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00012" num="00012"><img id="EMI-D00012" he="176.36mm" wi="109.22mm" orientation="landscape" file="US20230004739A1-20230105-D00012.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure><figure id="Fig-EMI-D00013" num="00013"><img id="EMI-D00013" he="175.26mm" wi="110.15mm" orientation="landscape" file="US20230004739A1-20230105-D00013.TIF" alt="embedded image" img-content="drawing" img-format="tif"/></figure></drawings><description id="description"><?summary-of-invention description="Summary of Invention" end="lead"?><heading id="h-0001" level="1">BACKGROUND</heading><heading id="h-0002" level="1">1. Technical Field</heading><p id="p-0002" num="0001">The present disclosure relates to human posture determination, and particularly to a human posture determination method and a mobile machine using the same.</p><heading id="h-0003" level="1">2. Description of Related Art</heading><p id="p-0003" num="0002">With the help of flourishing artificial intelligence (AI) techniques, mobile robots have been used in various scenes of daily life to provide various services such as healthcare, housework, and transportation. Taking healthcare as an example, mobility aid robots are often designed as devices like walkers or wheelchairs to assist walking and otherwise so as to improve the mobility of people with a mobility impairment.</p><p id="p-0004" num="0003">In addition to automatic navigation which can assist a user in a more automatic and convenient way, in order to serve the user in a more appropriate manner, a mobility aid robot is inevitable to detect the user's posture so as to serve accordingly. Skeleton-based posture detection is a popular technique for realizing human posture detections, which detects the posture of a human according to the identified key points of an estimated skeleton of the human.</p><p id="p-0005" num="0004">In the case that there have enough identified key points, it will be effective and can detect accurately; otherwise, in the case that the key points can be identified are not enough because, for example, the body of the human is occluded by obstacles or cloths, the efficiency of the detections will be affected. Especially when the human sits behind a furniture or lies down in a bed and covered with a quilt, the furniture or the quilt may occlude the body and affect the effect of detection. Therefore, a human posture determination method that is adaptable to different sets of detected key points is need.</p><?summary-of-invention description="Summary of Invention" end="tail"?><?brief-description-of-drawings description="Brief Description of Drawings" end="lead"?><description-of-drawings><heading id="h-0004" level="1">BRIEF DESCRIPTION OF THE DRAWINGS</heading><p id="p-0006" num="0005">In order to more clearly illustrate the technical solutions in this embodiment, the drawings used in the embodiments or the description of the prior art will be briefly introduced below. In the drawing(s), like reference numerals designate corresponding parts throughout the figures. It should be understood that, the drawings in the following description are only examples of the present disclosure. For those skilled in the art, other drawings can be obtained based on these drawings without creative works.</p><p id="p-0007" num="0006"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a scenario of detecting human posture using a mobile machine according to some embodiments of the present disclosure.</p><p id="p-0008" num="0007"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of using a camera of the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to detect the posture of a human.</p><p id="p-0009" num="0008"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic block diagram illustrating the mobile machine of FIC <b>1</b>.</p><p id="p-0010" num="0009"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic block diagram of an example of detecting human posture using the mobile machine of <figref idref="DRAWINGS">FIG. <b>1</b></figref>.</p><p id="p-0011" num="0010"><figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> is a flow chart of a posture determination process on the basis of detected key points.</p><p id="p-0012" num="0011"><figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of scenario <b>1</b> of the posture determination of <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>.</p><p id="p-0013" num="0012"><figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of feature map <b>1</b> for the scenario of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0014" num="0013"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of scenario <b>2</b> of the posture determination of <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>R</figref>.</p><p id="p-0015" num="0014"><figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of feature map <b>2</b> for the scenario of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0016" num="0015"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic diagram of an example of obtaining a detected internal angle and a detected body ratio in the scenario of <figref idref="DRAWINGS">FIG. <b>6</b></figref>.</p><p id="p-0017" num="0016"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram of an example of obtaining a detected body ratio and a detected upper body angle in the scenario of <figref idref="DRAWINGS">FIG. <b>8</b></figref>.</p><p id="p-0018" num="0017"><figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of scenario <b>3</b> of the posture determination of <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>.</p></description-of-drawings><?brief-description-of-drawings description="Brief Description of Drawings" end="tail"?><?detailed-description description="Detailed Description" end="lead"?><heading id="h-0005" level="1">DETAILED DESCRIPTION</heading><p id="p-0019" num="0018">In order to make the objects, features and advantages of the present disclosure more obvious and easy to understand, the technical solutions in this embodiment will be clearly and completely described below with reference to the drawings. Apparently, the described embodiments are part of the embodiments of the present disclosure, not all of the embodiments. All other embodiments obtained by those skilled in the art based on the embodiments of the present disclosure without creative efforts are within the scope of the present disclosure.</p><p id="p-0020" num="0019">It is to be understood that, when used in the description and the appended claims of the present disclosure, the terms &#x201c;including&#x201d;, &#x201c;comprising&#x201d;, &#x201c;having&#x201d; and their variations indicate the presence of stated features, integers, steps, operations, elements and/or components, but do not preclude the presence or addition of one or a plurality of other features, integers, steps, operations, elements, components and/or combinations thereof.</p><p id="p-0021" num="0020">It is also to be understood that, the terminology used in the description of the present disclosure is only for the purpose of describing particular embodiments and is not intended to limit the present disclosure. As used in the description and the appended claims of the present disclosure, the singular forms &#x201c;one&#x201d;, &#x201c;a&#x201d;, and &#x201c;the&#x201d; are intended to include the plural forms as well, unless the context clearly indicates otherwise.</p><p id="p-0022" num="0021">It is also to be further understood that the term &#x201c;and/or&#x201d; used in the description and the appended claims of the present disclosure refers to any combination of one or more of the associated listed items and all possible combinations, and includes such combinations.</p><p id="p-0023" num="0022">In the present disclosure, the terms &#x201c;first&#x201d;. &#x201c;second&#x201d;, and &#x201c;third&#x201d; are for descriptive purposes only, and are not to be comprehended as indicating or implying the relative importance or implicitly indicating the amount of technical features indicated. Thus, the feature limited by &#x201c;first&#x201d;, &#x201c;second&#x201d;, and &#x201c;third&#x201d; may include at least one of the feature either explicitly or implicitly. In the description of the present disclosure, the meaning of &#x201c;a plurality&#x201d; is at least two, for example, two, three, and the like, unless specifically defined otherwise.</p><p id="p-0024" num="0023">In the present disclosure, the descriptions of &#x201c;one embodiment&#x201d;, &#x201c;some embodiments&#x201d; or the like described in the specification mean that one or more embodiments of the present disclosure can include particular features, structures, or characteristics which are related to the descriptions of the descripted embodiments. Therefore, the sentences &#x201c;in one embodiment&#x201d;, &#x201c;in some embodiments&#x201d;, &#x201c;in other embodiments&#x201d;, &#x201c;in other embodiments&#x201d; and the like that appear in different places of the specification do not mean that descripted embodiments should be referred by all other embodiments, but instead be referred by &#x201c;one or more but not all other embodiments&#x201d; unless otherwise specifically emphasized.</p><p id="p-0025" num="0024">The present disclosure relates to mobile machine navigating. As used herein, the term &#x201c;human&#x201d; refers to the most populous and widespread species of primates in the earth. A human has a body including a head, a neck, a trunk, arms, hands, legs and feet. The term &#x201c;posture&#x201d; refers to a human position such as standing, sitting, and lying. The term &#x201c;determination&#x201d; refers to the judgment of the state (e.g., the posture) of a specific objective (e.g., a human) by calculating based on the related data (e.g., the image including the human). The term &#x201c;mobile machine&#x201d; refers to a machine such as a mobile robot or a vehicle that has the capability to move around in its environment. The term &#x201c;sensor&#x201d; refers to a device, module, machine, or subsystem such as ambient light sensor and image sensor (e.g., camera) whose purpose is to detect events or changes in its environment and send the information to other electronics (e.g., processor).</p><p id="p-0026" num="0025"><figref idref="DRAWINGS">FIG. <b>1</b></figref> is a schematic diagram of a scenario of detecting human posture using a mobile machine <b>100</b> according to some embodiments of the present disclosure. As shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref>, the mobile machine <b>100</b> that is navigated in its environment (e.g., a room) detects the posture of a human, that is, the user U. The mobile machine <b>100</b> is a mobile robot (e.g., a mobility aid robot), which includes a camera C and wheels E. The camera C may be disposed toward a forward direction D<sub>f </sub>which the mobile machine <b>100</b> straightly moves such that lens of the camera C straightly face toward the forward direction D<sub>f</sub>. The camera C has a camera coordinate system, and the coordinates of the mobile machine <b>100</b> are consistent with the coordinates of the camera C. In the camera coordinate system, the x-axis is parallel to the horizon, the y-axis is perpendicular to the horizon, and the z-axis is consistent with the forward direction D<sub>f</sub>. It should be noted that, the mobile machine <b>100</b> is only one example of mobile machine, and the mobile machine <b>100</b> may have more, fewer, or different parts than shown in above or below (e.g., have legs rather than the wheels E), or may have a different configuration or arrangement of the parts (e.g., have the camera C dispose on the top of the mobile machine <b>100</b>). In other embodiments, the mobility machine <b>100</b> may be another kind of mobile machine such as a vehicle.</p><p id="p-0027" num="0026"><figref idref="DRAWINGS">FIG. <b>2</b></figref> is a schematic diagram of using the camera C of the mobile machine <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref> to detect the posture of a human. A field of view (FOV) V of the camera C covers both the user U, a quilt Q covering on the user U, and a bench B on which the user U is sitting. The height (e.g., 1 meter) of the camera C on the mobile machine <b>100</b> may be changed according to actual needs (e.g., larger height to have the larger field of view V and smaller height to have the smaller field of view V), and the pitch angle of the camera C with respect to the floor F may also be changed according to actual needs (e.g., larger pitch angle to have the nearer field of view V and smaller pitch angle to have the farer field of view V). Based on the height and the pitch angle of the camera C, a relative position of the user U (and/or the bench B) near the mobile machine <b>100</b> can be obtained, and then the posture of the user U can be determined.</p><p id="p-0028" num="0027">In some embodiments, the mobile machine <b>100</b> is navigated in the environment while dangerous situations such as collisions and unsafe conditions (e.g., falling, extreme temperature, radiation, and exposure) may be prevented. In this indoor navigation, the mobile machine <b>100</b> is navigated from a starting point (e.g., the location where the mobile machine <b>100</b> originally locates) to a destination (e.g., the location of the goal of navigation which is indicated by the user U or the navigation/operation system of the mobile machine <b>100</b>), and obstacles (e.g., walls, furniture, humans, pets, and garbage) may be avoided so as to prevent the above-mentioned dangerous situations. In the navigation, target finding (e.g., human finding or user identifying) may be considered to help to find the user U, and occlusion (e.g., furniture and cloth) avoidance may also be considered to improve the efficiency of human posture determination. The trajectory (e.g., trajectory T) for the mobile machine <b>100</b> to move from the starting point to the destination has to be planned so as to move the mobile machine <b>100</b> according to the trajectory. The trajectory includes a sequence of poses (e.g., poses S<sub>n&#x2212;1</sub>-S<sub>n </sub>of trajectory T). In some embodiments, for realizing the navigation of the mobile machine <b>100</b>, the map for the environment has to be built, the current position of the mobile machine <b>100</b> in the environment may have to be determined (using, for example, the IMU <b>1331</b>), and trajectories may be planned based on the built map and the determined current position of the mobile machine <b>100</b>. The desired pose S<sub>d </sub>is the last of the sequence of poses S in a trajectory T (only shown partially in the figure), that is, the end of the trajectory T. The trajectory T is planned according to, for example, a shortest path in the built map to the user U. In addition, the collision avoidance to obstacles in the built map (e.g., walls and furniture) or that detected in real time (e.g., humans and pets) may also be considered when planning, so as to accurately and safely navigate the mobile machine <b>100</b>. It should be noted that, the starting point and the destination only refer to the locations of the mobile machine <b>100</b>, rather than the real beginning and end of the trajectory (the real beginning and end of a trajectory should be a pose, respectively). In addition, the trajectory T shown in <figref idref="DRAWINGS">FIG. <b>1</b></figref> is only a part of the planned trajectory <b>1</b>.</p><p id="p-0029" num="0028">In some embodiments, the navigation of the mobile machine <b>100</b> may be actuated through the mobile machine <b>100</b> itself (e.g., a control interface on the mobile machine <b>100</b>) or a control device such as a remote control, a smart phone, a tablet computer, a notebook computer, a desktop computer, or other electronic device by, for example, providing a request for the navigation of the mobile machine <b>100</b>. The mobile machine <b>100</b> and the control device may communicate over a network which may include, for example, the Internet, intranet, extranet, local area network (LAN), wide area network (WAN), wired network, wireless networks (e.g., Wi-Fi network, Bluetooth network, and mobile network), or other suitable networks, or any combination of two or more such networks.</p><p id="p-0030" num="0029"><figref idref="DRAWINGS">FIG. <b>3</b></figref> is a schematic block diagram illustrating the mobile machine <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. The mobile machine <b>100</b> may include a processing unit <b>110</b>, a storage unit <b>120</b>, and a control unit <b>130</b> that communicate over one or more communication buses or signal lines L. It should be noted that, the mobile machine <b>100</b> is only one example of mobile machine, and the mobile machine <b>100</b> may have more or fewer components (e.g., unit, subunits, and modules) than shown in above or below, may combine two or more components, or may have a different configuration or arrangement of the components. The processing unit <b>110</b> executes various (sets of) instructions stored in the storage unit <b>120</b> that may be in form of software programs to perform various functions for the mobile machine <b>100</b> and to process related data, which may include one or more processors (e.g., CPU). The storage unit <b>120</b> may include one or more memories (e.g., high-speed random access memory (RAM) and non-transitory memory), one or more memory controllers, and one or more non-transitory computer readable storage media (e.g., solid-state drive (SSD) or hard disk drive). The control unit <b>130</b> may include various controllers (e.g., camera controller, display controller, and physical button controller) and peripherals interface for coupling the input and output peripheral of the mobile machine <b>100</b>, for example, external port (e.g., USB), wireless communication circuit (e.g., RF communication circuit), audio circuit (e.g., speaker circuit), sensor (e.g., inertial measurement unit (IMU)), and the like, to the processing unit <b>110</b> and the storage unit <b>120</b>. In some embodiments, the storage unit <b>120</b> may include a navigation module <b>121</b> for implementing navigation functions (e.g., map building and trajectory planning) related to the navigation (and trajectory planning) of the mobile machine <b>100</b>, which may be stored in the one or more memories (and the one or more non-transitory computer readable storage media).</p><p id="p-0031" num="0030">The navigation module <b>121</b> in the storage unit <b>120</b> of the mobile machine <b>10</b> may be a software module (of the operation system of the mobile machine <b>100</b>), which has instructions I<sub>n </sub>(e.g., instructions for actuating motor(s) <b>1321</b> of the wheels E of the mobile machine <b>100</b> to move the mobile machine <b>100</b>) for implementing the navigation of the mobile machine <b>100</b>, a map builder <b>1211</b>, and trajectory planner(s) <b>1212</b>. The map builder <b>1211</b> may be a software module having instructions I<sub>b </sub>for building map for the mobile machine <b>100</b>. The trajectory planner(s) <b>1212</b> may be software module(s) having instructions I<sub>p </sub>for planning trajectories for the mobile machine <b>100</b>. The trajectory planner(s) <b>1212</b> may include a global trajectory planner for planning global trajectories (e.g., trajectory T) for the mobile machine <b>100</b> and a local trajectory planner for planning local trajectories (e.g., the part of the trajectory T in <figref idref="DRAWINGS">FIG. <b>1</b></figref>) for the mobile machine <b>100</b>. The global trajectory planner may be, for example, a trajectory planner based on Dijkstra's algorithm, which plans global trajectories based on map(s) built by the map builder <b>1211</b> through, for example, simultaneous localization and mapping (SLAM). The local trajectory planner may be, for example, a trajectory planner based on TEB (timed elastic band) algorithm, which plans local trajectories based on the global trajectory, and other data collected by the mobile machine <b>100</b>. For example, images may be collected through the camera C of the mobile machine <b>100</b>, and the collected images may be analyzed so as to identify obstacles, so that the local trajectory can be planned with reference to the identified obstacles, and the obstacles can be avoided by moving the mobile machine <b>10</b> according to the planned local trajectory.</p><p id="p-0032" num="0031">Each of the map builder <b>1211</b> and the trajectory planner(s) <b>1212</b> may be a submodule separated from the instructions I<sub>n </sub>or other submodules of the navigation module <b>121</b>, or a part of the instructions I<sub>n </sub>for implementing the navigation of the mobile machine <b>100</b>. The trajectory planner(s) <b>1212</b> may further have data (e.g., input/output data and temporary data) related to the trajectory planning of the mobile machine <b>100</b> which may be stored in the one or more memories and accessed by the processing unit <b>110</b>. In some embodiments, each of the trajectory planner(s) <b>1212</b> may be a module in the storage unit <b>120</b> that is separated from the navigation module <b>121</b>.</p><p id="p-0033" num="0032">In some embodiments, the instructions I<sub>n </sub>may include instructions for implementing collision avoidance of the mobile machine <b>100</b> (e.g., obstacle detection and trajectory replanning). In addition, the global trajectory planner may replan the global trajectory(s) (i.e., plan new global trajectory(s)) to detour in response to, for example, the original global trajectory(s) being blocked (e.g., blocked by an unexpected obstacle) or inadequate for collision avoidance (e.g., impossible to avoid a detected obstacle when adopted). In other embodiments, the navigation module <b>121</b> may be a navigation unit communicating with the processing unit <b>110</b>, the storage unit <b>120</b>, and the control unit <b>130</b> over the one or more communication buses or signal lines L, and may further include one or more memories (e.g., high-speed random access memory (RAM) and non-transitory memory) for storing the instructions I<sub>n</sub>, the map builder <b>1211</b>, and the trajectory planner(s) <b>1212</b>, and one or more processors (e.g., MPU and MCU) for executing the stored instructions I<sub>n</sub>, I<sub>b </sub>and I<sub>p </sub>to implement the navigation of the mobile machine <b>100</b>.</p><p id="p-0034" num="0033">The mobile machine <b>100</b> may further include a communication subunit <b>131</b> and an actuation subunit <b>132</b>. The communication subunit <b>131</b> and the actuation subunit <b>132</b> communicate with the control unit <b>130</b> over one or more communication buses or signal lines that may be the same or at least partially different from the above-mentioned one or more communication buses or signal lines L. The communication subunit <b>131</b> is coupled to communication interfaces of the mobile machine <b>100</b>, for example, network interface(s) <b>1311</b> for the mobile machine <b>100</b> to communicate with the control device via the network, I/O interface(s) <b>1312</b> (e.g., a physical button), and the like. The actuation subunit <b>132</b> is coupled to component(s)/device(s) for implementing the motions of the mobile machine <b>100</b> by, for example, actuating motor(s) <b>1321</b> of the wheels E and/or joints of the mobile machine <b>100</b>. The communication subunit <b>131</b> may include controllers for the above-mentioned communication interfaces of the mobile machine <b>100</b>, and the actuation subunit <b>132</b> may include controller(s) for the above-mentioned component(s)/device(s) for implementing the motions of the mobile machine <b>100</b>. In other embodiments, the communication subunit <b>131</b> and/or actuation subunit <b>132</b> may just abstract component for representing the logical relationships between the components of the mobile machine <b>100</b>.</p><p id="p-0035" num="0034">The mobile machine <b>100</b> may further include a sensor subunit <b>133</b> which may include a set of sensor(s) and related controller(s), for example, the camera C. and an IMU <b>1331</b> (or an accelerometer and a gyroscope), for detecting the environment in which it is located to realize its navigation. The camera C may be a range camera producing range images. The range image(s) include color data for representing the colors of pixels in the image and depth data for representing the distance to the scene object in the image. In some embodiments, the camera C is an RGB-D camera, which produces RGB-D image pairs each including an RGB image and a depth image, where the RGB image includes pixels each represented as red, green and blue colors, and the depth image includes pixels each having the value for representing the distance to the scene object (e.g., a human or a furniture). The sensor subunit <b>133</b> communicates with the control unit <b>130</b> over one or more communication buses or signal lines that may be the same or at least partially different from the above-mentioned one or more communication buses or signal lines L. In other embodiments, in the case that the navigation module <b>121</b> is the above-mentioned navigation unit, the sensor subunit <b>133</b> may communicate with the navigation unit over one or more communication buses or signal lines that may be the same or at least partially different from the above-mentioned one or more communication buses or signal lines L. In addition, the sensor subunit <b>133</b> may just abstract component for representing the logical relationships between the components of the mobile machine <b>100</b>.</p><p id="p-0036" num="0035">In some embodiments, the map builder <b>1211</b>, the trajectory planner(s) <b>1212</b>, the sensor subunit <b>133</b>, and the motor(s) <b>1321</b> (and the wheels E and/or joints of the mobile machine <b>100</b> coupled to the motor(s) <b>1321</b>) jointly compose a (navigation) system which implements map building, (global and local) trajectory planning, and motor actuating so as to realize the navigation of the mobile machine <b>100</b>. In addition, the various components shown in <figref idref="DRAWINGS">FIG. <b>2</b>B</figref> may be implemented in hardware, software or a combination of both hardware and software. Two or more of the processing unit <b>110</b>, the storage unit <b>120</b>, the control unit <b>130</b>, the navigation module <b>121</b>, and other units/subunits/modules may be implemented on a single chip or a circuit. In other embodiments, at least a part of them may be implemented on separate chips or circuits.</p><p id="p-0037" num="0036"><figref idref="DRAWINGS">FIG. <b>4</b></figref> is a schematic block diagram of an example of detecting human posture using the mobile machine <b>100</b> of <figref idref="DRAWINGS">FIG. <b>1</b></figref>. In some embodiments, a human posture determination method maybe implemented in the mobile machine <b>100</b> by, for example, storing (sets of) the instructions I<sub>n </sub>corresponding to the human posture determination method as the navigation module <b>121</b> in the storage unit <b>120</b> and executing the stored instructions I<sub>n </sub>through the processing unit <b>110</b>, and then the mobile machine <b>100</b> can detect using the camera C so as to determine the posture of the user U. The human posture determination method may be performed in response to, for example, a request for detecting the posture of the user U from, for example, (the navigation/operation system of) the mobile machine <b>100</b> itself or the control device, then it may also be re-performed, for example, in every predetermined time interval (e.g., 1 second) to detect the change of the posture of the user U. According to the human posture determination method, the processing unit <b>110</b> may obtain RGB-D image pair(s) G through the camera C (block <b>410</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). The camera C captures image pair(s) G each including an RGB image G<sub>f </sub>and a depth image G<sub>d</sub>. The RGB image G<sub>r </sub>includes color information for representing the colors making up an image, and the depth image G<sub>d </sub>includes depth information for representing a distance to the scene object (e.g., the user U or the bench B) in the image. In some embodiments, a plurality of image pairs G may be obtained so as to select one image pair G (e.g., the image pair G that meets a certain quality) for use.</p><p id="p-0038" num="0037">The processing unit <b>110</b> may further detect key points P of an estimated skeleton N (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>) of the user U in the RGB image G<sub>r </sub>(block <b>420</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). The key points P in the RGB image G<sub>r </sub>may be identified to obtain two dimensional (2D) positions of the key points P on the estimated skeleton N of the user U. The estimated skeleton N is a pseudo human skeleton for determining human postures (e.g., standing, sitting, and lying), which has a set of predefined key points P each representing a joint (e.g., knee) or an organ (e.g., ear). In some embodiments, the predefined key points P may include 18 key points, that is, two eye key points, a nose key point, two ear key points, a neck key point, two shoulder key points P<sub>s </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>), two elbow key points, two wrist key points, two hip key points P<sub>h </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>), two knee key points P<sub>k </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>), and two foot key points, where the two eye key points, the nose key point, and the two ear key points are also referred to as head key points P<sub>d </sub>(not shown. The processing unit <b>110</b> may further calculate the positions of the detected key points P based on the depth image G<sub>d </sub>(block <b>430</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). The depth information in the depth image G<sub>d </sub>that corresponds to the key points P may be combined with the obtained 2D positions of the key points P so to obtain the three dimensional (3D) positions of the key points P.</p><p id="p-0039" num="0038">The processing unit <b>110</b> may further choose a feature map M from a set of predefined feature maps M based on the detected key points P among the predefined key points P (block <b>440</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). A set of predefined feature maps M may be stored in the storage unit <b>120</b> in advance. Each feature map M (e.g., feature map <b>1</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>) is a self-defined map which includes values corresponding to features (e.g., internal angle and body ratio in <figref idref="DRAWINGS">FIG. <b>7</b></figref>) for distinguishing postures, which can be used to classify the detected key points P so as to realize the distinguishing of postures. A clear separation of postures in the feature map M may be reached with the maximum margin from each posture region in the feature map M. <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> are a flow chart of a posture determination process on the basis of the detected key points P. In the posture determination process, the posture of the user U is determined on the basis of detected key points P. In addition to distinguishing postures by using a feature map M to classify the detected key points P in the case that all or a specific part of the predefined key points are detected as in the above-mentioned human posture determination method (the process of classifying postures by using a feature map M on the key points P in individual image pair G is called &#x201c;classifier&#x201d;) (steps <b>441</b>-<b>446</b>, <b>452</b>-<b>453453</b>, and <b>462</b>-<b>463463</b>), the posture determination process further distinguishes postures using the detected head key points P<sub>d </sub>in the case that all or the specific part of the predefined key points are not detected but the head key points P<sub>d </sub>are detected (steps <b>470</b>-<b>490</b>). The posture determination process may be incorporated into the above-mentioned human posture determination method.</p><p id="p-0040" num="0039">In some embodiments, for choosing a feature map M (block <b>440</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>), at step <b>441</b>, a determination is made whether or not the detected key points P include at least a shoulder key point P<sub>s</sub>, at least a hip key point P<sub>h</sub>, and at least a knee key point P<sub>k </sub>among the predefined key points P. If the detected key points P include at least a shoulder key point P<sub>s</sub>, at least a hip key point P<sub>h</sub>, and at least a knee key point P<sub>k</sub>, step <b>442</b> will be performed; otherwise, the method will be ended. At step <b>442</b>, feature map <b>2</b> is chosen from the set of predefined feature maps M. <figref idref="DRAWINGS">FIG. <b>9</b></figref> is a schematic diagram of feature map <b>2</b> for the scenario of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. Feature map <b>2</b> is for the two features of a body ratio and an upper body angle, and includes threshold curve &#x3b2; which is a polynormal curve for distinguishing a standing posture and a sitting posture, and threshold curve &#x3b3; which is a polynormal curve for distinguishing a lying posture, the standing posture, and the sitting posture. Since the internal angle is no more valid in this scenario and lying postures overlap with standing/siting postures in the values of the body ratio, the upper body angle is introduced in feature map <b>2</b>. It should be noted that, for better performance of posture determination, the feature values shown in feature map <b>2</b> are also normalized value with mean and scale. After step <b>442</b>, step <b>451</b> and step <b>461</b> will be performed sequentially to determine the posture of the user U. At step <b>443</b>, a determination is made whether or not the determined posture is the lying posture. If no, step <b>444</b> will be performed; otherwise, the method will be ended.</p><p id="p-0041" num="0040">At step <b>444</b>, a determination is made whether or not the detected key points P include all the predefined key points P. <figref idref="DRAWINGS">FIG. <b>6</b></figref> is a schematic diagram of scenario <b>1</b> of the posture determination of <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>. In scenario <b>1</b>, all the predefined key points P are detected, and steps <b>445</b>, <b>452453</b>, and <b>462463</b> in <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> will be performed to determine the posture of the user U. For instance, because the user U in <figref idref="DRAWINGS">FIG. <b>6</b></figref> stands at attention before the camera C of the mobile machine <b>100</b> and there is no obstacle therebetween, all the above-mentioned 18 predefined key points P can be detected. It should be noted that, the 5 head key points P<sub>d </sub>(i.e., the two eye key points, the nose key point, and the two ear key points) are not shown in the figure. If the detected key points P include all the predefined key points P (e.g., the detected key points P include all the above-mentioned 18 predefined key points), step <b>445</b> will be performed; otherwise, step <b>446</b> will be performed. At step <b>445</b>, feature map <b>1</b> is chosen from the set of predefined feature maps M. <figref idref="DRAWINGS">FIG. <b>7</b></figref> is a schematic diagram of feature map <b>1</b> for the scenario of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. Feature map <b>1</b> is for the two features of an internal angle and a body ratio, and includes threshold curve &#x3b1; which is a polynormal curve for distinguishing a standing posture and a sitting posture. Because a much higher possibility of occlusion in lying postures than standing and sitting postures, only standing and sitting postures are considered in scenario <b>1</b> that all the predefined key points P are detected. It should be noted that, for better performance of posture determination, the feature values shown in feature map <b>1</b> are normalized value with mean and scale. At step <b>446</b>, a determination is made whether or not the detected key points P include at least a shoulder key point P<sub>x</sub>, at least a hip key point P<sub>h</sub>, and at least a knee key point P<sub>k </sub>among the predefined key points P.</p><p id="p-0042" num="0041"><figref idref="DRAWINGS">FIG. <b>8</b></figref> is a schematic diagram of scenario <b>2</b> of the posture determination of <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>. In scenario <b>2</b>, one or two shoulder key points P<sub>s</sub>, one or two hip key points P<sub>h</sub>, and one or two knee key points P<sub>k </sub>rather than all the predefined key points P are detected, and steps <b>453453</b> and <b>463463</b> in <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref> will be performed to determine the posture of the user U. For instance, because the user U in <figref idref="DRAWINGS">FIG. <b>8</b></figref> sits on a chair H before the camera C of the mobile machine <b>100</b> and there is a table T that occlude a part of the body of the user U, only the two shoulder key points P<sub>s</sub>, the two hip key points P<sub>h</sub>, and the two knee key points P<sub>k </sub>(and the head key points P<sub>d</sub>) among the above-mentioned 18 predefined key points P can detected. In one embodiment, if only one shoulder, hip or knee key point is detected, the position of the other key point may be obtained based on the position of the detected key point by, for example, using the position of the detected key point (and the position of the other detected key points) to calculate the position of the undetected key point. It should be noted that, the 5 head key points P<sub>d </sub>(i.e., the two eye key points, the nose key point, and the two ear key points) are not shown in the figure. If the detected key points P include at least a shoulder key point P<sub>s</sub>, at least a hip key point P<sub>h</sub>, and at least a knee key point P<sub>k</sub>, step <b>453</b> will be performed; otherwise, step <b>470</b> will be performed.</p><p id="p-0043" num="0042">The processing unit <b>110</b> may further obtain two features (i.e., feature <b>1</b> and feature <b>2</b>) of a body of the human that correspond to the chosen feature map M based on the positions of the detected key points P (block <b>450</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). In some embodiments, for obtaining the feature corresponding to feature map <b>2</b>, at step <b>451</b> of <figref idref="DRAWINGS">FIGS. <b>5</b>A and <b>5</b>B</figref>, a detected upper body angle (i.e., feature <b>2</b>) of the body of the user U is obtained based on the positions of the detected key points P; for obtaining the two feature corresponding to feature map <b>1</b>, at step <b>452</b>, a detected internal angle A<sub>i </sub>(i.e., feature <b>1</b>, see <figref idref="DRAWINGS">FIG. <b>10</b></figref>) of the body of the user U and a detected body ratio R (i.e., feature <b>2</b>, see <figref idref="DRAWINGS">FIG. <b>10</b></figref>) of the body of the user U am obtained based on the positions of the detected key points P; and for obtaining the feature corresponding to feature map <b>2</b>, at step <b>453</b>, the detected body ratio R (i.e., feature <b>1</b>) of the body of the user U is obtained based on the positions of the detected key points P.</p><p id="p-0044" num="0043"><figref idref="DRAWINGS">FIG. <b>10</b></figref> is a schematic diagram of an example of obtaining the detected internal angle A<sub>i </sub>and the detected body ratio R in the scenario (i.e., scenario <b>1</b>) of <figref idref="DRAWINGS">FIG. <b>6</b></figref>. In some embodiments, for obtaining the detected internal angle A<sub>i </sub>(i.e., feature <b>1</b>) and the detected body ratio R (i.e., feature <b>2</b>), the processing unit <b>110</b> may obtain a middle position p<sub>h </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>) between the positions of the two hip key points P<sub>h </sub>among the detected key points P (block <b>4521</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>). The processing unit <b>110</b> may further obtain an upper body plane L<sub>u </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>) based on the middle position p<sub>h </sub>and the positions of the two shoulder key points P<sub>s </sub>among the detected key points (block <b>4522</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>). The processing unit <b>110</b> may further obtain a lower body plane L<sub>1 </sub>based on the middle position p<sub>h </sub>with the positions of two knee key points among the detected key points (block <b>4523</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>). The processing unit <b>110</b> may further obtain an angle between the upper body plane and the lower body plane to take as the internal angle A<sub>i </sub>(block <b>4524</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>). In one embodiment, the upper body plane L<sub>u </sub>is a plane with a normal vector of {right arrow over (n)}<sub>up</sub>, and the lower body plane L<sub>1 </sub>is a plane with a normal vector of {right arrow over (n)}<sub>low</sub>, and the included angle between the normal vector {right arrow over (n)}<sub>up </sub>and the normal vector of {right arrow over (n)}<sub>low </sub>is take as the internal angle A<sub>i</sub>. The processing unit <b>110</b> may further obtain a ratio between a lower body height h<sub>low </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>) and an upper body height h<sub>up </sub>(see <figref idref="DRAWINGS">FIG. <b>6</b></figref>) to take as the body ratio R (block <b>4525</b> of <figref idref="DRAWINGS">FIG. <b>10</b></figref>). In one embodiment, the upper body height h<sub>up</sub>=p<sub>s</sub>&#xb7;y&#x2212;p<sub>h</sub>&#xb7;y and the lower body height h<sub>low</sub>=p<sub>h</sub>&#xb7;y&#x2212;p<sub>k</sub>&#xb7;y, where p<sub>s </sub>is a middle position between the positions of the two shoulder key points P<sub>s</sub>, p<sub>s</sub>&#xb7;y is a y coordinate of p<sub>s</sub>, p<sub>h</sub>&#xb7;y is a y coordinate of p<sub>h</sub>, p<sub>k </sub>is a middle position between the positions of the two knee key points P<sub>k</sub>, and p<sub>k</sub>&#xb7;y is a y coordinate of p<sub>k</sub>.</p><p id="p-0045" num="0044"><figref idref="DRAWINGS">FIG. <b>11</b></figref> is a schematic diagram of an example of obtaining the detected body ratio R and the detected upper body angle A<sub>u </sub>in the scenario (i.e., scenario <b>2</b>) of <figref idref="DRAWINGS">FIG. <b>8</b></figref>. In some embodiments, for obtaining the detected body ratio R (i.e., feature <b>1</b>) and the detected upper body angle A<sub>u </sub>(i.e., feature <b>2</b>), the processing unit <b>110</b> may obtain a middle position p between the positions of two hip key points P<sub>h </sub>among the detected key points P (block <b>4531</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>). The processing unit <b>110</b> may further obtain a ratio between a lower body height h<sub>low </sub>and an upper body height h<sub>up </sub>to take as the body ratio R (block <b>4532</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>), where h<sub>up</sub>=p<sub>s</sub>&#xb7;y&#x2212;p<sub>h</sub>&#xb7;y and h<sub>low</sub>=p<sub>h</sub>&#xb7;y&#x2212;p<sub>k</sub>&#xb7;y, p<sub>s </sub>is a middle position between the positions of two shoulder key points P<sub>s</sub>, p<sub>s</sub>&#xb7;y is a y coordinate of p<sub>s</sub>, p<sub>h</sub>&#xb7;y is a y coordinate of p<sub>h</sub>, p<sub>k </sub>is a middle position between the positions of two knee key points P<sub>k</sub>, and p<sub>k</sub>&#xb7;y is a y coordinate of p<sub>k</sub>. The processing unit <b>110</b> may further obtain an included angle between a vector {right arrow over (v)}<sub>hs </sub>that is between the middle position p<sub>h </sub>and the middle position p<sub>s </sub>and a unit vector &#x177; in y-direction to take as the upper body angle A<sub>u </sub>(block <b>4533</b> of <figref idref="DRAWINGS">FIG. <b>11</b></figref>).</p><p id="p-0046" num="0045">The processing unit <b>110</b> may further determines the posture of the user U according to the two features (i.e., feature <b>1</b> and feature <b>2</b>) in the chosen feature map M (block <b>460</b> of <figref idref="DRAWINGS">FIG. <b>4</b></figref>). In some embodiments, for determining the posture of the user U according to the upper body angle in feature map <b>2</b> (only distinguishing the lying posture here), at step <b>461</b>, the posture of the user U is determined based on the detected upper body angle A<sub>u </sub>(i.e., feature <b>2</b>). For determining the posture of the user U according to the two features (i.e., the detected internal angle A<sub>i </sub>and the detected body ratio R) of feature map <b>1</b>, at step <b>462</b>, the posture of the user U is determined based on a position of an intersection of the detected internal angle A, (i.e., feature <b>1</b>) and the detected body ratio R (i.e., feature <b>2</b>) in feature map <b>1</b>. For instance, for the standing user U of <figref idref="DRAWINGS">FIG. <b>6</b></figref>, since the position of the intersection of the detected internal angle A, (e.g., 0.613) and the detected body ratio R (e.g., 0.84) (both are normalized values) is at a point above the threshold curve a of feature map <b>1</b> in <figref idref="DRAWINGS">FIG. <b>7</b></figref>, the user U will be determined as at the standing posture. For determining the posture of the user U according to the two features (i.e., the detected body ratio R and the detected upper body angle A<sub>u</sub>) in feature map <b>2</b>, at step <b>463</b>, the posture of the user U is determined based on a position of an intersection of the detected body ratio R (i.e., feature <b>1</b>) in feature map <b>1</b>. For instance, for the sitting user U of <figref idref="DRAWINGS">FIG. <b>8</b></figref>, since the position of the intersection of the detected body ratio R (e.g., &#x2212;0.14) and the detected upper body angle A. (e.g., 0.44) (both are normalized values) is at a point that is on the left the threshold curve &#x3b2; and below the threshold curve &#x3b3; of feature map <b>2</b>, the user U will be determined as at the sitting posture.</p><p id="p-0047" num="0046">In the posture determination process of <figref idref="DRAWINGS">FIGS. <b>5</b>A</figref> and SB, at step <b>470</b>, a determination is made whether or not the detected key points P include a plurality of head key points P<sub>d </sub>among the predefined key points P. <figref idref="DRAWINGS">FIG. <b>12</b></figref> is a schematic diagram of scenario <b>3</b> of the posture determination of <figref idref="DRAWINGS">FIGS. <b>5</b>A</figref> and SB. In scenario <b>3</b> that may have more than half of the body of the user U is occluded or does not locate within the field of view V of the camera C but (some or all of) the head key points P<sub>d </sub>are detected, steps <b>480</b> and <b>490</b> in <figref idref="DRAWINGS">FIGS. <b>5</b>A</figref> and SB will be performed to determine the posture of the user U. For instance, because the user U lies on the bench B before the camera C of the mobile machine <b>100</b> and there is a quilt Q that occlude a part of the body of the user U, only the head key points P<sub>d </sub>(e.g., two or more of the two eye key points, the nose key point, and the two ear key points) (and the two shoulder key points P<sub>s</sub>, the two elbow key points, and the two hand key points) among the above-mentioned 18 predefined key points P can detected. It should be noted that, the 5 head key points P<sub>d </sub>(i.e., the two eye key points, the nose key point, and the two ear key points) are not shown in the figure. If the detected key points P include a plurality of head key points P<sub>d</sub>, step <b>480</b> will be performed; otherwise, the method will be ended. At step <b>480</b>, a head height H<sub>head </sub>is calculated based on the head key points P<sub>d</sub>. The head height H<sub>head </sub>is the height of (the eyes on) the head with respect to the ground. Because the relative height and angle of the camera C with respect to the ground are known, the head height H<sub>head </sub>can be calculated based on the position of the head in the camera coordinate system. For example, when the user U is sitting on the chair C, the head height U is the height of the head with respect to the floor F (see <figref idref="DRAWINGS">FIG. <b>8</b></figref>), and when the user U is standing on the floor F, the head height H<sub>head </sub>is also the height of the head with respect to the floo (see <figref idref="DRAWINGS">FIG. <b>6</b></figref>).</p><p id="p-0048" num="0047">At step <b>490</b>, the posture of the user U is determined by comparing the head height H<sub>head </sub>with a first head height threshold (e.g., 120 cm) for distinguishing a standing posture and a sitting posture and a second head height threshold (e.g., 50 cm) for distinguishing the sitting posture and a lying posture. In the case that head height H<sub>head </sub>is larger than the first head height threshold, the user U will be determined as at the standing posture; in the case that head height H<sub>head </sub>is between the first head height threshold and the second head height threshold, the user U will be determined as at the sitting posture; and in the case that head height H<sub>head </sub>is smaller than the second head height threshold, the user U will be determined as at the lying posture. For instance, for the lying user U of <figref idref="DRAWINGS">FIG. <b>12</b></figref>, since the head height H<sub>head </sub>(e.g., 30 cm) is smaller than the second head height threshold, the user U will be determined as at the lying posture. In some embodiments, the first head height threshold and the second head height threshold may be changed according to, for example, the height of the user U. In other embodiments, all the steps for the above-mentioned three scenarios (i.e., steps <b>445</b>, <b>452</b> and <b>462</b> for scenario <b>1</b>, steps <b>444</b>, <b>453</b> and <b>463</b> for scenario <b>2</b>, and steps <b>480</b> and <b>490</b> for scenario <b>3</b>) may be performed without the determination steps (i.e., steps <b>441</b>, <b>446</b>, and <b>470</b>). Then, the determination results of the three scenarios that correspond to each frame (i.e., image pair G) may be combined with a maximum vote to determine the posture of the user U.</p><p id="p-0049" num="0048">In other embodiments, in the human posture determination method (incorporated with the posture determination process), since human would not frequently change postures within millisecond time scale, a time-window (e.g., 1 second) may be added for filtering out the invalid results to realize a more accurate and robust posture determination. For instance, for a plurality of adjacent frames within a time-window, the determination results of the above-mentioned three scenarios that correspond to each frame with a maximum vote are combined to make a decision of human posture, and then the posture of the user U is determined in the case that all the decisions within the time-window are the same. It should be noted that the size of the time-window (which represents how many adjacent frames will be accumulated) may be defined according to actual needs. For example, the size of the time-window may be defined as dynamically adapted based on how close the current feature values to the corresponding threshold curve, and the closer to the threshold curve, the smaller the size and vice versa.</p><p id="p-0050" num="0049">The human posture determination method is adaptable to different sets of detected key points because it uses predefined feature maps for describing humane postures to determine human posture on the basis of detected key points. By using unique pre-defined features and feature maps, it is capable of determining human postures in an accurate way. The human posture determination method can be realized in a real-time manner while only a few computation resources are need, and is economic and efficient because only an RGB-D camera rather than a plurality of sensors are need for detection. In the case that a mobile machine realizing the human posture determination meth is a mobility aid robot, it can determine the posture of a human so as to choose a suitable way to interact with the human accordingly. For example, when the human is an old person who is determined as having lied down, the mobile machine can ask the human to sit first before providing further aids.</p><p id="p-0051" num="0050">It can be understood by those skilled in the art that, all or part of the method in the above-mentioned embodiment(s) can be implemented by one or more computer programs to instruct related hardware. In addition, the one or more programs can be stored in a non-transitory computer readable storage medium. When the one or more programs are executed, all or part of the corresponding method in the above-mentioned embodiment(s) is performed. Any reference to a storage, a memory, a database or other medium may include non-transitory and/or transitory memory. Non-transitory memory may include read only memory (ROM), programmable ROM (PROM), electrically programmable ROM (EPROM), electrically erasable programmable ROM (EEPROM), flash memory, solid-state drive (SSD), or the like. Volatile memory may include random access memory (RAM), external cache memory, or the like.</p><p id="p-0052" num="0051">The processing unit <b>110</b> (and the above-mentioned processor) may include central processing unit (CPU), or be other general purpose processor, digital signal processor (DSP), application specific integrated circuit (ASIC), field-programmable gate array (FPGA), or be other programmable logic device, discrete gate, transistor logic device, and discrete hardware component. The general purpose processor may be microprocessor, or the processor may also be any conventional processor. The storage unit <b>120</b> (and the above-mentioned memory) may include internal storage unit such as hard disk and internal memory. The storage unit <b>120</b> may also include external storage device such as plug-in hard disk, smart media card (SMC), secure digital (SD) card, and flash card.</p><p id="p-0053" num="0052">The exemplificative units/modules and methods/steps described in the embodiments may be implemented through software, hardware, or a combination of software and hardware. Whether these functions are implemented through software or hardware depends on the specific application and design constraints of the technical schemes. The above-mentioned human posture determination method and mobile machine <b>100</b> may be implemented in other manners. For example, the division of units/modules is merely a logical functional division, and other division manner may be used in actual implementations, that is, multiple units/modules may be combined or be integrated into another system, or some of the features may be ignored or not performed. In addition, the above-mentioned mutual coupling/connection may be direct coupling/connection or communication connection, and may also be indirect coupling/connection or communication connection through some interfaces/devices, and may also be electrical, mechanical or in other forms.</p><p id="p-0054" num="0053">The above-mentioned embodiments are merely intended for describing but not for limiting the technical schemes of the present disclosure. Although the present disclosure is described in detail with reference to the above-mentioned embodiments, the technical schemes in each of the above-mentioned embodiments may still be modified, or some of the technical features may be equivalently replaced, so that these modifications or replacements do not make the essence of the corresponding technical schemes depart from the spirit and scope of the technical schemes of each of the embodiments of the present disclosure, and should be included within the scope of the present disclosure.</p><?detailed-description description="Detailed Description" end="tail"?></description><us-claim-statement>What is claimed is:</us-claim-statement><claims id="claims"><claim id="CLM-00001" num="00001"><claim-text><b>1</b>. A human posture determination method, comprising:<claim-text>obtaining, through a range camera, one or more range images, wherein the one or more range images include color data and depth data;</claim-text><claim-text>detecting key points of an estimated skeleton of a human in the color data and calculating positions of the detected key points based on the depth data, wherein the estimated skeleton has a set of predefined key points;</claim-text><claim-text>choosing a feature map from a set of predefined feature maps based on the detected key points among the predefined key points;</claim-text><claim-text>obtaining two features of a body of the human corresponding to the chosen feature map based on the positions of the detected key points; and</claim-text><claim-text>determining a posture of the human according to the two features in the chosen feature map.</claim-text></claim-text></claim><claim id="CLM-00002" num="00002"><claim-text><b>2</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the set of predefined feature maps includes a first feature map with the two features of an internal angle and a body ratio, the first feature map includes a threshold curve for distinguishing a standing posture and a sitting posture.</claim-text></claim><claim id="CLM-00003" num="00003"><claim-text><b>3</b>. The method of <claim-ref idref="CLM-00002">claim 2</claim-ref>, wherein the choosing the feature map from the set of predefined feature maps based on the detected key points among the predefined key points comprises:<claim-text>in response to the detected key points including all the predefined key points, choosing the first feature map from the set of predefined feature maps;</claim-text><claim-text>the obtaining the two features of the body of the human corresponding to the chosen feature map based on the positions of the detected key points comprises:</claim-text><claim-text>in response to the detected key points including all the predefined key points, obtaining a detected internal angle of the body of the human and a detected body ratio of the body of the human based on the positions of the detected key points; and</claim-text><claim-text>the determining the posture of the human according to the two features in the chosen feature map comprises:</claim-text><claim-text>in response to the detected key points including all the predefined key points, determining the posture of the human based on a position of an intersection of the detected internal angle and the detected body ratio in the first feature map.</claim-text></claim-text></claim><claim id="CLM-00004" num="00004"><claim-text><b>4</b>. The method of <claim-ref idref="CLM-00003">claim 3</claim-ref>, wherein the obtaining the internal angle and the body ratio based on the positions of the key points comprises:<claim-text>obtaining a middle position p<sub>h </sub>between the positions of two hip key points among the detected key points;</claim-text><claim-text>obtaining an upper body plane based on the middle position p<sub>h </sub>and the positions of two shoulder key points among the detected key points;</claim-text><claim-text>obtaining a lower body plane based on the middle position p<sub>h </sub>and the positions of two knee key points among the detected key points;</claim-text><claim-text>obtaining an angle between the upper body plane and the lower body plane to take as the internal angle; and</claim-text><claim-text>obtaining a ratio between a lower body height h<sub>low </sub>and an upper body height h<sub>up </sub>to take as the body ratio, wherein h<sub>up</sub>=p<sub>s</sub>&#xb7;y&#x2212;p<sub>s</sub>&#xb7;y and h<sub>low</sub>=p<sub>h</sub>&#xb7;y&#x2212;p<sub>k</sub>&#xb7;y, p<sub>s </sub>is a middle position between the positions of the two shoulder key points, p<sub>s</sub>&#xb7;y is a y coordinate of p<sub>s</sub>, p<sub>h</sub>&#xb7;y is a y coordinate of p<sub>h</sub>, p<sub>k </sub>is a middle position between the positions of the two knee key points, and p<sub>k</sub>&#xb7;y is a y coordinate of p<sub>k</sub>; and</claim-text><claim-text>the determining the posture of the human based on the position of the intersection of the internal angle and the body ratio in the first feature map comprises:</claim-text><claim-text>determining the posture of the human according to the position of the intersection of the internal angle and the body ratio in the first feature map with respect to the threshold curve for distinguishing the standing posture and the sitting posture.</claim-text></claim-text></claim><claim id="CLM-00005" num="00005"><claim-text><b>5</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the set of predefined feature maps includes a second feature map with the two features of a body ratio and an upper body angle, the second feature map includes a first threshold curve for distinguishing a standing posture and a sitting posture and a second threshold curve for distinguishing a lying posture, the standing posture, and the sitting posture.</claim-text></claim><claim id="CLM-00006" num="00006"><claim-text><b>6</b>. The method of <claim-ref idref="CLM-00005">claim 5</claim-ref>, wherein the choosing the feature map from the set of predefined feature maps based on the detected key points among the predefined key points comprises:<claim-text>in response to the detected key points including at least a shoulder key point, at least a hip key point, and at least a knee key point among the predefined key points, choosing the second feature map from the set of predefined feature maps;</claim-text><claim-text>the obtaining the two features of the body of the human corresponding to the chosen feature map based on the positions of the detected key points comprises:</claim-text><claim-text>in response to the detected key points including at least a shoulder key point, at least a hip key point, and at least a knee key point among the predefined key points, obtaining a detected body ratio of the body of the human and a detected upper body angle of the body of the human based on the positions of the detected key points; and</claim-text><claim-text>the determining the posture of the human according to the two features in the chosen feature map comprises:</claim-text><claim-text>in response to the detected key points including at least a shoulder key point, at least a hip key point, and at least a knee key point among the predefined key points, determining the posture of the human based on a position of an intersection of the detected body ratio and the detected upper body angle in the second feature map.</claim-text></claim-text></claim><claim id="CLM-00007" num="00007"><claim-text><b>7</b>. The method of <claim-ref idref="CLM-00006">claim 6</claim-ref>, wherein the obtaining the body ratio and the upper body angle based on the positions of the detected key points comprises:<claim-text>obtaining a middle position p<sub>h </sub>between the positions of two hip key points among the detected key points;</claim-text><claim-text>obtaining a ratio between a lower body height h<sub>low </sub>and an upper body height h<sub>up </sub>to take as the body ratio, wherein h<sub>up</sub>=p<sub>s</sub>&#xb7;y&#x2212;p<sub>h</sub>&#xb7;y and h<sub>low</sub>=p<sub>h</sub>&#xb7;y&#x2212;p<sub>k</sub>&#xb7;y, p<sub>s </sub>is a middle position between the positions of two shoulder key points, p<sub>s</sub>&#xb7;y is a y coordinate of p<sub>s</sub>, p<sub>h</sub>&#xb7;y is a y coordinate of p<sub>h</sub>, p<sub>k </sub>is a middle position between the positions of two knee key points, and p<sub>k</sub>&#xb7;y is a y coordinate of p<sub>k</sub>; and</claim-text><claim-text>obtaining an angle between a vector {right arrow over (v)}<sub>hs </sub>between the middle position p<sub>h </sub>and the middle position p<sub>s </sub>and a unit vector {right arrow over (y)} in y-direction to take as the upper body angle; and</claim-text><claim-text>the determining the posture of the human based on the position of the intersection of the body ratio and the upper body angle in the second feature map comprises:</claim-text><claim-text>determining the posture of the human according to the position of the intersection of the body ratio and the upper body angle in the second feature map with respect to the first threshold curve for distinguishing the standing posture and the sitting posture and the second threshold curve for distinguishing the lying posture, the standing posture, and the sitting posture.</claim-text></claim-text></claim><claim id="CLM-00008" num="00008"><claim-text><b>8</b>. The method of <claim-ref idref="CLM-00007">claim 7</claim-ref>, further comprising:<claim-text>in response to the detected key points including one shoulder key point, obtaining the position of another of the two shoulder key points based on the position of the detected shoulder key point;</claim-text><claim-text>in response to the detected key points including one hip key point, obtaining the position of another of the two hip key points based on the position of the detected hip key point; and</claim-text><claim-text>in response to the detected key points including one knee key point, obtaining the position of another of the two knee key points based on the position of the detected knee key point.</claim-text></claim-text></claim><claim id="CLM-00009" num="00009"><claim-text><b>9</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, further comprising:<claim-text>in response to the detected key points including a plurality of head key points among the predefined key points, calculating a head height H<sub>head </sub>based on the head key points; and</claim-text><claim-text>determining the posture of the human by comparing the head height H<sub>head </sub>with a first head height threshold for distinguishing a standing posture and a sitting posture and a second head height threshold for distinguishing the sitting posture and a lying posture.</claim-text></claim-text></claim><claim id="CLM-00010" num="00010"><claim-text><b>10</b>. The method of <claim-ref idref="CLM-00001">claim 1</claim-ref>, wherein the predefined key points include two eye key points, a nose key point, two ear key points, a neck key point, two shoulder key points, two elbow key points, two hand key points, two hip key points, two knee key points, and two foot key points.</claim-text></claim><claim id="CLM-00011" num="00011"><claim-text><b>11</b>. A mobile machine, comprising:<claim-text>a range camera;</claim-text><claim-text>one or more processors; and</claim-text><claim-text>a memory storing one or more programs configured to be executed by the one or more processors, wherein the one or more programs include instructions to:</claim-text><claim-text>obtain, through the range camera, one or more range images, wherein the one or more range images include color data and depth data;</claim-text><claim-text>detect key points of an estimated skeleton of a human in the color data and calculating positions of the detected key points based on the depth data, wherein the estimated skeleton has a set of predefined key points;</claim-text><claim-text>choose a feature map from a set of predefined feature maps based on the detected key points among the predefined key points;</claim-text><claim-text>obtain two features of a body of the human corresponding to the chosen feature map based on the positions of the detected key points; and</claim-text><claim-text>determine a posture of the human according to the two features in the chosen feature map.</claim-text></claim-text></claim><claim id="CLM-00012" num="00012"><claim-text><b>12</b>. The mobile machine of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the set of predefined feature maps includes a first feature map with the two features of an internal angle and a body ratio, the first feature map includes a threshold curve for distinguishing a standing posture and a sitting posture.</claim-text></claim><claim id="CLM-00013" num="00013"><claim-text><b>13</b>. The mobile machine of <claim-ref idref="CLM-00012">claim 12</claim-ref>, wherein the choosing the feature map from the set of predefined feature maps based on the detected key points among the predefined key points comprises:<claim-text>in response to the detected key points including all the predefined key points, choosing the first feature map from the set of predefined feature maps;</claim-text><claim-text>the obtaining the two features of the body of the human corresponding to the chosen feature map based on the positions of the detected key points comprises:</claim-text><claim-text>in response to the detected key points including all the predefined key points, obtaining a detected internal angle of the body of the human and a detected body ratio of the body of the human based on the positions of the detected key points; and</claim-text><claim-text>the determining the posture of the human according to the two features in the chosen feature map comprises:</claim-text><claim-text>in response to the detected key points including all the predefined key points, determining the posture of the human based on a position of an intersection of the detected internal angle and the detected body ratio in the first feature map.</claim-text></claim-text></claim><claim id="CLM-00014" num="00014"><claim-text><b>14</b>. The mobile machine of <claim-ref idref="CLM-00013">claim 13</claim-ref>, wherein the obtaining the internal angle and the body ratio based on the positions of the key points comprises:<claim-text>obtaining a middle position p<sub>h </sub>between the positions of two hip key points among the detected key points;</claim-text><claim-text>obtaining an upper body plane based on the middle position p<sub>h </sub>and the positions of two shoulder key points among the detected key points:</claim-text><claim-text>obtaining a lower body plane based on the middle position p<sub>h </sub>and the positions of two knee key points among the detected key points;</claim-text><claim-text>obtaining an angle between the upper body plane and the lower body plane to take as the internal angle; and</claim-text><claim-text>obtaining a ratio between a lower body height h<sub>low </sub>and an upper body height h<sub>up </sub>to take as the body ratio, wherein h<sub>up</sub>=p<sub>s</sub>&#xb7;y&#x2212;p<sub>h</sub>&#xb7;y and h<sub>low</sub>=p<sub>h</sub>&#xb7;y&#x2212;p<sub>k</sub>&#xb7;y, p<sub>s </sub>is a middle position between the positions of the two shoulder key points, p<sub>s</sub>&#xb7;y is a y coordinate of p<sub>s</sub>, p<sub>h</sub>&#xb7;y is a y coordinate of p<sub>h</sub>, p<sub>k </sub>is a middle position between the positions of the two knee key points, and p<sub>k</sub>&#xb7;y is a y coordinate of p<sub>k</sub>; and</claim-text><claim-text>the determining the posture of the human based on the position of the intersection of the internal angle and the body ratio in the first feature map comprises:</claim-text><claim-text>determining the posture of the human according to the position of the intersection of the internal angle and the body ratio in the first feature map with respect to the threshold curve for distinguishing the standing posture and the sitting posture.</claim-text></claim-text></claim><claim id="CLM-00015" num="00015"><claim-text><b>15</b>. The mobile machine of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the set of predefined feature maps includes a second feature map with the two features of a body ratio and an upper body angle, the second feature map includes a first threshold curve for distinguishing a standing posture and a sitting posture and a second threshold curve for distinguishing a lying posture, the standing posture, and the sitting posture.</claim-text></claim><claim id="CLM-00016" num="00016"><claim-text><b>16</b>. The mobile machine of <claim-ref idref="CLM-00015">claim 15</claim-ref>, wherein the choosing the feature map from the set of predefined feature maps based on the detected key points among the predefined key points comprises:<claim-text>in response to the detected key points including at least a shoulder key point, at least a hip key point, and at least a knee key point among the predefined key points, choosing the second feature map from the set of predefined feature maps;</claim-text><claim-text>the obtaining the two features of the body of the human corresponding to the chosen feature map based on the positions of the detected key points comprises:</claim-text><claim-text>in response to the detected key points including at least a shoulder key point, at least a hip key point, and at least a knee key point among the predefined key points, obtaining a detected body ratio of the body of the human and a detected upper body angle of the body of the human based on the positions of the detected key points; and</claim-text><claim-text>the determining the posture of the human according to the two features in the chosen feature map comprises:</claim-text><claim-text>in response to the detected key points including at least a shoulder key point, at least a hip key point, and at least a knee key point among the predefined key points, determining the posture of the human based on a position of an intersection of the detected body ratio and the detected upper body angle in the second feature map.</claim-text></claim-text></claim><claim id="CLM-00017" num="00017"><claim-text><b>17</b>. The mobile machine of <claim-ref idref="CLM-00016">claim 16</claim-ref>, wherein the obtaining the body ratio and the upper body angle based on the positions of the detected key points comprises:<claim-text>obtaining a middle position p<sub>h </sub>between the positions of two hip key points among the detected key points;</claim-text><claim-text>obtaining a ratio between a lower body height h<sub>k </sub>and an upper body height h<sub>up </sub>to take as the body ratio, wherein h<sub>up</sub>=p<sub>s</sub>&#xb7;y&#x2212;p<sub>h</sub>&#xb7;y and h<sub>low</sub>=p<sub>h</sub>&#xb7;y&#x2212;p<sub>k</sub>&#xb7;y, p<sub>s </sub>is a middle position between the positions of two shoulder key points, p<sub>s</sub>&#xb7;y is a y coordinate of p<sub>s</sub>, p<sub>h</sub>&#xb7;y is a y coordinate of p<sub>h</sub>, p<sub>k </sub>is a middle position between the positions of two knee key points, and p<sub>k</sub>&#xb7;y is a y coordinate of p<sub>k</sub>; and</claim-text><claim-text>obtaining an angle between a vector {right arrow over (v)}<sub>hs </sub>between the middle position p<sub>h </sub>and the middle position p<sub>s </sub>and a unit vector {right arrow over (y)} in y-direction to take as the upper body angle; and</claim-text><claim-text>the determining the posture of the human based on the position of the intersection of the body ratio and the upper body angle in the second feature map comprises:</claim-text><claim-text>determining the posture of the human according to the position of the intersection of the body ratio and the upper body angle in the second feature map with respect to the first threshold curve for distinguishing the standing posture and the sitting posture and the second threshold curve for distinguishing the lying posture, the standing posture, and the sitting posture.</claim-text></claim-text></claim><claim id="CLM-00018" num="00018"><claim-text><b>18</b>. The mobile machine of <claim-ref idref="CLM-00017">claim 17</claim-ref>, wherein the one or more programs further include instructions to:<claim-text>in response to the detected key points including one shoulder key point, obtain the position of another of the two shoulder key points based on the position of the detected shoulder key point;</claim-text><claim-text>in response to the detected key points including one hip key point, obtain the position of another of the two hip key points based on the position of the detected hip key point; and</claim-text><claim-text>in response to the detected key points including one knee key point, obtain the position of another of the two knee key points based on the position of the detected knee key point.</claim-text></claim-text></claim><claim id="CLM-00019" num="00019"><claim-text><b>19</b>. The mobile machine of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the one or more programs further include instructions to:<claim-text>in response to the detected key points including a plurality of head key points among the predefined key points, calculate a head height H<sub>head </sub>based on the head key points; and</claim-text><claim-text>determine the posture of the human by comparing the head height H<sub>head </sub>with a first head height threshold for distinguishing a standing posture and a sitting posture and a second head height threshold for distinguishing the sitting posture and a lying posture.</claim-text></claim-text></claim><claim id="CLM-00020" num="00020"><claim-text><b>20</b>. The mobile machine of <claim-ref idref="CLM-00011">claim 11</claim-ref>, wherein the predefined key points include two eye key points, a nose key point, two ear key points, a neck key point, two shoulder key points, two elbow key points, two hand key points, two hip key points, two knee key points, and two foot key points.</claim-text></claim></claims></us-patent-application>